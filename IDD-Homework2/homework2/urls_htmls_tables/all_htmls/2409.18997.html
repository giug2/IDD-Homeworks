<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent</title>
<!--Generated on Thu Sep 19 06:22:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.18997v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S1" title="In PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S2" title="In PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span><span class="ltx_text ltx_font_italic">PropaInsight</span>: A Propaganda Analysis Framework</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S2.SS0.SSS0.Px1" title="In 2 PropaInsight: A Propaganda Analysis Framework ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Propaganda Techniques</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S2.SS0.SSS0.Px2" title="In 2 PropaInsight: A Propaganda Analysis Framework ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Arousal Appeals</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S2.SS0.SSS0.Px3" title="In 2 PropaInsight: A Propaganda Analysis Framework ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Underlying Intent</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S2.SS0.SSS0.Px4" title="In 2 PropaInsight: A Propaganda Analysis Framework ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Propaganda Analysis Task</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S3" title="In PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span><span class="ltx_text ltx_font_italic">PropaGaze</span>: A Dataset for Systematically Analyzing Propaganda</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S3.SS1" title="In 3 PropaGaze: A Dataset for Systematically Analyzing Propaganda ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span><span class="ltx_text ltx_font_italic">PTC-Gaze</span>: Human-Annotated Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S3.SS2" title="In 3 PropaGaze: A Dataset for Systematically Analyzing Propaganda ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span><span class="ltx_text ltx_font_italic">RUWA-Gaze</span> and <span class="ltx_text ltx_font_italic">Politifact-Gaze</span>: Synthetic Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S3.SS2.SSS0.Px1" title="In 3.2 RUWA-Gaze and Politifact-Gaze: Synthetic Datasets ‣ 3 PropaGaze: A Dataset for Systematically Analyzing Propaganda ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Data Generation Pipeline</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S4" title="In PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S4.SS1" title="In 4 Experiments ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S4.SS1.SSS0.Px1" title="In 4.1 Experimental Setup ‣ 4 Experiments ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Sub-Tasks and Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S4.SS1.SSS0.Px2" title="In 4.1 Experimental Setup ‣ 4 Experiments ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S4.SS1.SSS0.Px3" title="In 4.1 Experimental Setup ‣ 4 Experiments ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Data-Rich and Data-Sparse Training Settings</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S4.SS2" title="In 4 Experiments ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>How Do Off-the-Shelf LLMs Perform on Propaganda Analysis Tasks?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S4.SS3" title="In 4 Experiments ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>How Much Does <span class="ltx_text ltx_font_italic">PropaGaze</span> Enhance Model Performance?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S4.SS4" title="In 4 Experiments ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Is Propaganda Analysis Transferable Across Domains?</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S5" title="In PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S5.SS1" title="In 5 Discussion ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Discrepancy between Human-Annotated and Synthetic Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S5.SS2" title="In 5 Discussion ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Further Challenges of Propaganda Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S6" title="In PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S6.SS0.SSS0.Px1" title="In 6 Related Work ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Propaganda Detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S6.SS0.SSS0.Px2" title="In 6 Related Work ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Propaganda Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S6.SS0.SSS0.Px3" title="In 6 Related Work ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">User Intent Detection</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S7" title="In PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A1" title="In PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Details of a Propaganda Frame</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A2" title="In PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Data Generation Prompt Templates</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A2.SS0.SSS0.Px1" title="In Appendix B Data Generation Prompt Templates ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Step1: News Summarization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A2.SS0.SSS0.Px2" title="In Appendix B Data Generation Prompt Templates ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Step 2: Intent Creation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A2.SS0.SSS0.Px3" title="In Appendix B Data Generation Prompt Templates ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Step 4: Techniques Insertion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A2.SS0.SSS0.Px4" title="In Appendix B Data Generation Prompt Templates ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Step 4: Appeals Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A3" title="In PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Templates and Prompts We Used for Propaganda Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A3.SS0.SSS0.Px1" title="In Appendix C Templates and Prompts We Used for Propaganda Analysis ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Template for Composing the Predicted Elements into a Descriptive Sentence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A3.SS0.SSS0.Px2" title="In Appendix C Templates and Prompts We Used for Propaganda Analysis ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Prompt Template for the Language Models to Analyze Propaganda in a Zero-shot Manner</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A3.SS0.SSS0.Px3" title="In Appendix C Templates and Prompts We Used for Propaganda Analysis ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Template for Instruction Tuning with Llama2-Chat-7B on Tactics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A3.SS0.SSS0.Px4" title="In Appendix C Templates and Prompts We Used for Propaganda Analysis ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Template for Instruction Tuning with Llama2-Chat-7B on Appeals</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A3.SS0.SSS0.Px5" title="In Appendix C Templates and Prompts We Used for Propaganda Analysis ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Template for Instruction Tuning with Llama2-Chat-7B on intent</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A4" title="In PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Details of the <span class="ltx_text ltx_font_italic">PropaGaze</span> Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A4.SS0.SSS0.Px1" title="In Appendix D Details of the PropaGaze Dataset ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">RUWA-Gaze</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A4.SS0.SSS0.Px2" title="In Appendix D Details of the PropaGaze Dataset ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Politifact-Gaze</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A4.SS0.SSS0.Px3" title="In Appendix D Details of the PropaGaze Dataset ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Generated News Article:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A4.SS0.SSS0.Px4" title="In Appendix D Details of the PropaGaze Dataset ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_text ltx_font_italic">PTC-Gaze</span>:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A4.SS0.SSS0.Px5" title="In Appendix D Details of the PropaGaze Dataset ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Real-World News Article:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A4.SS0.SSS0.Px6" title="In Appendix D Details of the PropaGaze Dataset ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Technique annotation 1:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A4.SS0.SSS0.Px7" title="In Appendix D Details of the PropaGaze Dataset ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Appeal annotation 1:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A4.SS0.SSS0.Px8" title="In Appendix D Details of the PropaGaze Dataset ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Technique annotation 2:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A4.SS0.SSS0.Px9" title="In Appendix D Details of the PropaGaze Dataset ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Appeal annotation 2:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A4.SS0.SSS0.Px10" title="In Appendix D Details of the PropaGaze Dataset ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Technique annotation 3:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A4.SS0.SSS0.Px11" title="In Appendix D Details of the PropaGaze Dataset ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Appeal annotation 3:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A4.SS0.SSS0.Px12" title="In Appendix D Details of the PropaGaze Dataset ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Intent Annotation:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A4.SS0.SSS0.Px13" title="In Appendix D Details of the PropaGaze Dataset ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Analysis for Annotation Quality</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A5" title="In PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Experimental Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A6" title="In PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Case Study: Bottleneck of Propaganda Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A6.SS0.SSS0.Px1" title="In Appendix F Case Study: Bottleneck of Propaganda Analysis ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Input Example Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A6.SS0.SSS0.Px2" title="In Appendix F Case Study: Bottleneck of Propaganda Analysis ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Ground Truth Answers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A6.SS0.SSS0.Px3" title="In Appendix F Case Study: Bottleneck of Propaganda Analysis ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_title">Analysis for the Presented Result</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">PropaInsight: Toward Deeper Understanding of Propaganda
<br class="ltx_break"/>in Terms of Techniques, Appeals, and Intent</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="id1.1.id1">Jiateng Liu<sup class="ltx_sup" id="id1.1.id1.1">1, </sup></span>,
<span class="ltx_text ltx_font_bold" id="id2.2.id2">Lin Ai<sup class="ltx_sup" id="id2.2.id2.1">2, <span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex1.1.1.1">1</span></span></span></span></span></sup></span>,
<span class="ltx_text ltx_font_bold" id="id3.3.id3">Zizhou Liu<sup class="ltx_sup" id="id3.3.id3.1">2</sup></span>,
<span class="ltx_text ltx_font_bold" id="id4.4.id4">Payam Karisani<sup class="ltx_sup" id="id4.4.id4.1">1</sup></span>,
<span class="ltx_text ltx_font_bold" id="id5.5.id5">Zheng Hui<sup class="ltx_sup" id="id5.5.id5.1">2</sup></span>,

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id6.6.id6">May Fung<sup class="ltx_sup" id="id6.6.id6.1">1</sup></span>,
<span class="ltx_text ltx_font_bold" id="id7.7.id7">Preslav Nakov<sup class="ltx_sup" id="id7.7.id7.1">3</sup></span>,
<span class="ltx_text ltx_font_bold" id="id8.8.id8">Julia Hirschberg<sup class="ltx_sup" id="id8.8.id8.1">2</sup></span>,
<span class="ltx_text ltx_font_bold" id="id9.9.id9">Heng Ji<sup class="ltx_sup" id="id9.9.id9.1">1</sup></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id10.10.id10">1</sup>University of Illinois Urbana-Champaign
   
<sup class="ltx_sup" id="id11.11.id11">2</sup>Columbia University

<br class="ltx_break"/><sup class="ltx_sup" id="id12.12.id12">3</sup>Mohamed bin Zayed University of Artificial Intelligence

<br class="ltx_break"/>{jiateng5, hengji}@illinois.edu,
{lin.ai, julia}@cs.columbia.edu
</span><span class="ltx_author_notes"><span class="ltx_text ltx_font_bold" id="id13.13.id1">These authors contribute to this work equally.</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id14.id1">Propaganda plays a critical role in shaping public opinion and fueling disinformation. While existing research primarily focuses on identifying propaganda techniques, it lacks the ability to capture the broader motives and the impacts of such content. To address these challenges, we introduce <span class="ltx_text ltx_font_bold ltx_font_italic" id="id14.id1.1">PropaInsight</span>, a conceptual framework grounded in foundational social science research, which systematically dissects propaganda into techniques, arousal appeals, and underlying intent. <span class="ltx_text ltx_font_italic" id="id14.id1.2">PropaInsight</span> offers a more granular understanding of how propaganda operates across different contexts. Additionally, we present <span class="ltx_text ltx_font_bold ltx_font_italic" id="id14.id1.3">PropaGaze</span>, a novel dataset that combines human-annotated data with high-quality synthetic data generated through a meticulously designed pipeline.</p>
<p class="ltx_p" id="id15.id2">Our experiments show that off-the-shelf LLMs struggle with propaganda analysis, but training with <span class="ltx_text ltx_font_italic" id="id15.id2.1">PropaGaze</span> significantly improves performance.
Fine-tuned Llama-7B-Chat achieves 203.4% higher text span IoU in technique identification and 66.2% higher BertScore in appeal analysis compared to 1-shot GPT-4-Turbo. Moreover, <span class="ltx_text ltx_font_italic" id="id15.id2.2">PropaGaze</span> complements limited human-annotated data in data-sparse and cross-domain scenarios, showing its potential for comprehensive and generalizable propaganda analysis.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_text ltx_font_italic" id="footnote1.1">PropaGaze</span> and code will be released upon acceptance.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">PropaInsight: Toward Deeper Understanding of Propaganda
<br class="ltx_break"/>in Terms of Techniques, Appeals, and Intent</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tr" id="p1.1.2.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1">
Jiateng Liu<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1">1, <span class="ltx_note ltx_role_thanks" id="p1.1.2.1.1.1.1.1.1.1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>These authors contribute to this work equally.</span></span></span></sup>,
Lin Ai<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.2">2, <span class="ltx_note ltx_role_footnotemark" id="footnotex2"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex2.1.1.1">1</span></span></span></span></span></sup>,
Zizhou Liu<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.3">2</sup>,
Payam Karisani<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.4">1</sup>,
Zheng Hui<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.5">2</sup>,</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.1.1">May Fung<sup class="ltx_sup" id="p1.1.2.1.1.2.1.1.1">1</sup></span>,
<span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.1.2">Preslav Nakov<sup class="ltx_sup" id="p1.1.2.1.1.2.1.2.1">3</sup></span>,
<span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.1.3">Julia Hirschberg<sup class="ltx_sup" id="p1.1.2.1.1.2.1.3.1">2</sup></span>,
<span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.1.4">Heng Ji<sup class="ltx_sup" id="p1.1.2.1.1.2.1.4.1">1</sup></span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.1"><sup class="ltx_sup" id="p1.1.2.1.1.3.1.1">1</sup>University of Illinois Urbana-Champaign
   
<sup class="ltx_sup" id="p1.1.2.1.1.3.1.2">2</sup>Columbia University</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.4.1"><sup class="ltx_sup" id="p1.1.2.1.1.4.1.1">3</sup>Mohamed bin Zayed University of Artificial Intelligence</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.5">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.5.1">{jiateng5, hengji}@illinois.edu,
{lin.ai, julia}@cs.columbia.edu</span></span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="217" id="S1.F1.g1" src="extracted/5863894/figures/propainsight.png" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>We abstract key elements of propaganda from social science literature. A comprehensive propaganda frame includes the techniques employed, the appeals evoked in readers, and the author’s underlying intent.</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In an era of unbounded digital information, the deliberate dissemination of propaganda has proliferated, shaping public opinion and influencing political events <cite class="ltx_cite ltx_citemacro_cite">Stanley (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib49" title="">2015</a>)</cite>. Propaganda is also a key component of disinformation, where false information is intentionally crafted and distributed to deceive or mislead <cite class="ltx_cite ltx_citemacro_cite">Da San Martino et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib11" title="">2020</a>)</cite>. Detecting and analyzing propaganda is essential to maintain the integrity of public discourse and to ensure that individuals make informed, unbiased decisions <cite class="ltx_cite ltx_citemacro_cite">Da San Martino et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib11" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Most current research on propaganda detection focuses on identifying and categorizing the specific techniques used to persuade the audience <cite class="ltx_cite ltx_citemacro_cite">Da San Martino et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib10" title="">2019</a>); Martino et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib37" title="">2020a</a>)</cite>. However, simply recognizing these techniques does not fully capture the motives behind the propaganda or its broader impact. As noted by <cite class="ltx_cite ltx_citemacro_citet">Hobbs (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib25" title="">2020</a>); Lord and Vogt (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib36" title="">2021</a>)</cite>, understanding the sources and the intentions behind the information is critical to effectively combat disinformation. Therefore, there is a growing need for moving beyond simply recognizing techniques to deepen understanding of propaganda.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Moreover, existing studies often rely on expert annotations because non-expert annotators struggle with separating personal biases from their assessments of propaganda <cite class="ltx_cite ltx_citemacro_cite">Da San Martino et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib10" title="">2019</a>)</cite>. This dependence on expert annotations leads to relatively small datasets, which may be insufficient for training large, generalizable models, and limits their applicability in cross-domain contexts where propaganda usage varies. For instance, strategies in military content can differ greatly from such in political content, highlighting the need for broader, more diverse datasets.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To address these challenges, we build on foundational social science research on propaganda <cite class="ltx_cite ltx_citemacro_cite">Nelson (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib41" title="">1997</a>); Jowett and O’donnell (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib28" title="">2018</a>); Ellul (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib14" title="">2021</a>)</cite> and identify three key elements behind propaganda attempts: <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">propaganda techniques</span>, <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">arousal appeals</span>, and <span class="ltx_text ltx_font_italic" id="S1.p4.1.3">underlying intent</span>. Consequently, we introduce a new conceptual framework, <span class="ltx_text ltx_font_italic" id="S1.p4.1.4">PropaInsight</span>, that systematically analyzes these elements. Additionally, we leverage the strong context understanding ability of large language models (LLMs) to generate synthetic data, resulting in <span class="ltx_text ltx_font_italic" id="S1.p4.1.5">PropaGaze</span>, a novel dataset for propaganda analysis. Our motivations are twofold: <span class="ltx_text ltx_font_bold" id="S1.p4.1.6">(1)</span> to develop a comprehensive framework that goes beyond identifying techniques, and <span class="ltx_text ltx_font_bold" id="S1.p4.1.7">(2)</span> to explore the use of synthetic data to supplement limited human-annotated data. Our contributions are as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.ix1.p1">
<p class="ltx_p" id="S1.I1.ix1.p1.1">We propose <span class="ltx_text ltx_font_italic" id="S1.I1.ix1.p1.1.1">PropaInsight</span>, a conceptual framework for granular and comprehensive propaganda analysis that identifies propaganda techniques, arousal appeals, and underlying intent in news articles.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.ix2.p1">
<p class="ltx_p" id="S1.I1.ix2.p1.1">We introduce <span class="ltx_text ltx_font_italic" id="S1.I1.ix2.p1.1.1">PropaGaze</span>, a novel dataset for fine-grained propaganda analysis, consisting of a human-annotated news sub-dataset and two high-quality synthetic sub-datasets: one focused on the Russia-Ukraine conflict and one on the political domain. </p>
</div>
</li>
<li class="ltx_item" id="S1.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.ix3.p1">
<p class="ltx_p" id="S1.I1.ix3.p1.1">We demonstrate that <span class="ltx_text ltx_font_italic" id="S1.I1.ix3.p1.1.1">PropaGaze</span> enhances LLMs’ ability to analyze propaganda within the <span class="ltx_text ltx_font_italic" id="S1.I1.ix3.p1.1.2">PropaInsight</span> framework, paving the way for more nuanced and generalizable propaganda analysis methods.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span class="ltx_text ltx_font_italic" id="S2.1.1">PropaInsight</span>: A Propaganda Analysis Framework</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">We introduce <span class="ltx_text ltx_font_italic" id="S2.p1.1.1">PropaInsight</span>, a new conceptual framework for comprehensive propaganda analysis. In contrast to previous methods which ignore the underlying purposes and only focus on techniques, <span class="ltx_text ltx_font_italic" id="S2.p1.1.2">PropaInsight</span> delves into the more subtle and hidden elements of propaganda. Drawing from foundational social science research on propaganda <cite class="ltx_cite ltx_citemacro_cite">Nelson (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib41" title="">1997</a>); Jowett and O’donnell (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib28" title="">2018</a>); Ellul (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib14" title="">2021</a>)</cite>, we identify three key elements of each propaganda attempt: <span class="ltx_text ltx_font_italic" id="S2.p1.1.3">propaganda techniques</span>, <span class="ltx_text ltx_font_italic" id="S2.p1.1.4">arousal appeals</span>, and <span class="ltx_text ltx_font_italic" id="S2.p1.1.5">underlying intent</span>. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">1</span></a>, for a given article, we first identify and classify the techniques used. We then infer the arousal appeals these techniques evoke, and we further deduce the underlying intent of the article. To ensure interpretability and consistency, we consolidate these elements into a clear, structured natural language paragraph using a descriptive template, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">1</span></a>. Below, we provide a detailed explanation of each element of our proposed framework.</p>
</div>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Propaganda Techniques</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Propaganda techniques are systematic, deliberate strategies used to craft persuasive content <cite class="ltx_cite ltx_citemacro_cite">Jowett and O’Donnell (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib29" title="">2012</a>)</cite>. Domain experts typically define these techniques as pre-defined labels like ‘loaded language’. While the specific techniques may vary across different shared tasks <cite class="ltx_cite ltx_citemacro_cite">Torok (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib53" title="">2015</a>)</cite>, we follow the set of propaganda techniques defined in <cite class="ltx_cite ltx_citemacro_cite">Da San Martino et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib10" title="">2019</a>)</cite>, where each technique can be evaluated intrinsically. The full list of the 16 propaganda techniques we use is provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A3" title="Appendix C Templates and Prompts We Used for Propaganda Analysis ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Arousal Appeals</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Appeals directly influence a reader’s emotions, opinions, and actions after consuming propagandistic content <cite class="ltx_cite ltx_citemacro_cite">Nelson (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib41" title="">1997</a>); Jowett and O’Donnell (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib29" title="">2012</a>)</cite>. A common propaganda device is to evoke strong emotions, such as hate or fear, in readers <cite class="ltx_cite ltx_citemacro_cite">Miller (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib39" title="">1937</a>)</cite>. Another approach involves selectively presenting evidence and facts to shape the audience’s perception <cite class="ltx_cite ltx_citemacro_cite">Walton (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib59" title="">1997</a>); O’Shaughnessy (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib43" title="">2004</a>)</cite>. To capture these effects, we design three templates (detailed in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A3" title="Appendix C Templates and Prompts We Used for Propaganda Analysis ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">C</span></a>) that identify the emotions evoked and the aspects readers are guided toward or distracted from while reading an article.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Underlying Intent</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">Intent represents the ideological, political, or other underlying goal the author seeks to convey or achieve. To handle diverse real-world scenarios, we frame intent prediction as a free-text generation task, similar to approaches used for open intent generation in dialogue systems <cite class="ltx_cite ltx_citemacro_cite">Şimşek and Fensel (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib47" title="">2018</a>); Wagner (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib58" title="">2022</a>)</cite>. The advantage of this novel formulation in propaganda intent analysis is its flexibility in capturing complex, nuanced intent that predefined labels cannot easily categorize, allowing greater freedom to generate more detailed and context-specific interpretations of intent.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Propaganda Analysis Task</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px4.p1.1">The design of <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px4.p1.1.1">PropaInsight</span> introduces a new propaganda analysis task: generating a descriptive natural language paragraph explaining the techniques used, the appeals aroused, and the underlying intent. To avoid overlooking individual elements and to simplify evaluation, we divide the task into three sub-tasks:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.ix1.p1">
<p class="ltx_p" id="S2.I1.ix1.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.I1.ix1.p1.1.1">Propaganda Technique Identification:</span> Detect the spans where propaganda techniques are applied and which specific technique(s) correspond to each span, following prior task settings <cite class="ltx_cite ltx_citemacro_cite">Martino et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib37" title="">2020a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib38" title="">b</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.ix2.p1">
<p class="ltx_p" id="S2.I1.ix2.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.I1.ix2.p1.1.1">Appeal Analysis:</span> Generate the descriptions of emotions and feelings evoked using a template-based approach (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A3" title="Appendix C Templates and Prompts We Used for Propaganda Analysis ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">C</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I1.ix3.p1">
<p class="ltx_p" id="S2.I1.ix3.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.I1.ix3.p1.1.1">Intent Analysis:</span> Generate a free-form explanation of the article’s underlying intent.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span class="ltx_text ltx_font_italic" id="S3.1.1">PropaGaze</span>: A Dataset for Systematically Analyzing Propaganda</h2>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="220" id="S3.F2.g1" src="extracted/5863894/figures/pipeline.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Partially controlled data generation pipeline: We first collect real-world news articles and derive an objective summary to extract events. Then we generate event-based intent, and randomly sample specific propaganda techniques to insert into the event descriptions. Lastly, we generate appeals from a reader’s perspective, aiming at making the appeals grounded to the text.</figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Existing propaganda datasets <cite class="ltx_cite ltx_citemacro_cite">Martino et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib37" title="">2020a</a>); Heppell et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib24" title="">2023</a>)</cite> primarily focus on identifying propaganda techniques and their associated text spans, but lack insights into appeal and intent. We introduce <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">PropaGaze</span>, a new dataset specifically designed for comprehensive propaganda analysis, consisting of three sub-datasets: <span class="ltx_text ltx_font_italic" id="S3.p1.1.2">PTC-Gaze</span>, <span class="ltx_text ltx_font_italic" id="S3.p1.1.3">RUWA-Gaze</span>, and <span class="ltx_text ltx_font_italic" id="S3.p1.1.4">Politifact-Gaze</span>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span class="ltx_text ltx_font_italic" id="S3.SS1.1.1">PTC-Gaze</span>: Human-Annotated Dataset</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">PTC-Gaze</span> builds on the existing PTC dataset <cite class="ltx_cite ltx_citemacro_cite">Martino et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib37" title="">2020a</a>)</cite>, which includes human-written news articles annotated for propaganda techniques and spans. We reannotate this dataset by hiring human annotators to label appeals and intent independently. For appeals, annotators review propaganda-containing sentences along with their context and describe the feelings evoked. To reduce cognitive load, we provide GPT-4 generated candidate annotations for assistance. Annotators then evaluate whether the generated candidates accurately reflect their interpretations and reactions, and if not, they rewrite the descriptions based on the template in § <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S2" title="2 PropaInsight: A Propaganda Analysis Framework ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">2</span></a> and Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A3" title="Appendix C Templates and Prompts We Used for Propaganda Analysis ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">C</span></a>. For intent, annotators read the full article and infer its underlying intent in a single free-form sentence, and we leave the multi-intent scenarios for future work.As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S3.T1" title="Table 1 ‣ 3.1 PTC-Gaze: Human-Annotated Dataset ‣ 3 PropaGaze: A Dataset for Systematically Analyzing Propaganda ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">1</span></a>, this annotated sub-dataset contains 79 articles, with an average of 12.77 propaganda techniques per article. Additional information, data examples, and analysis of the annotation quality are given in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A4" title="Appendix D Details of the PropaGaze Dataset ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.1" style="width:208.1pt;height:61.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-56.8pt,16.8pt) scale(0.64696955942139,0.64696955942139) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.1">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S3.T1.1.1.1.1.1">Sub-Dataset <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.1.1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T1.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.1.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">#Articles</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.3">
<span class="ltx_text" id="S3.T1.1.1.1.3.1"></span> <span class="ltx_text" id="S3.T1.1.1.1.3.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.1.3.2.1">
<span class="ltx_tr" id="S3.T1.1.1.1.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.1.3.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.2.1.1.1.1">Avg. Article</span></span></span>
<span class="ltx_tr" id="S3.T1.1.1.1.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.1.3.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.2.1.2.1.1">Length (words)</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.1.1.1.3.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.4">
<span class="ltx_text" id="S3.T1.1.1.1.4.1"></span> <span class="ltx_text" id="S3.T1.1.1.1.4.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.1.4.2.1">
<span class="ltx_tr" id="S3.T1.1.1.1.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.1.4.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.4.2.1.1.1.1">Avg. Propa</span></span></span>
<span class="ltx_tr" id="S3.T1.1.1.1.4.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.1.4.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.4.2.1.2.1.1">Usage / Article</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.1.1.1.4.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.2">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.2.1"><span class="ltx_text ltx_font_italic" id="S3.T1.1.1.2.1.1">PTC-Gaze</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2.2">79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2.3">885.16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2.4">12.77</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.3">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.3.1"><span class="ltx_text ltx_font_italic" id="S3.T1.1.1.3.1.1">RUWA-Gaze</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.3.2">497</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.3.3">266.04</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.3.4">3.46</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.1.4.1"><span class="ltx_text ltx_font_italic" id="S3.T1.1.1.4.1.1">Politifact-Gaze</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.1.4.2">593</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.1.4.3">339.05</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.1.4.4">3.47</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Statistics about the <span class="ltx_text ltx_font_italic" id="S3.T1.3.1">PropaGaze</span> dataset.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span class="ltx_text ltx_font_italic" id="S3.SS2.1.1">RUWA-Gaze</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.2.2">Politifact-Gaze</span>: Synthetic Datasets</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">One limitation of the fully human-annotated dataset is that its usualy expensive, due to the challenging nature of the annotation tasks. This makes it insufficient for training large, generalizable models and limits its cross-domain applicability. Sparse data is a common issue in propaganda analysis research. To address this, we leverage LLMs such as LLaMA <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib54" title="">2023</a>)</cite> and GPT <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib65" title="">2023</a>); OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib42" title="">2023</a>)</cite> to synthesize data, using their strong prior knowledge and context understanding. These models have shown effectiveness in data augmentation for tasks like propaganda techniques identification, such as fallacy recognition <cite class="ltx_cite ltx_citemacro_cite">Alhindi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib4" title="">2024</a>)</cite>. These synthetic datasets are created mainly for training and can also serve as silver-standard benchmarks for propaganda analysis.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">We construct <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.1">RUWA-Gaze</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.2">Politifact-Gaze</span> using a partially controlled data generation pipeline, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S3.F2" title="Figure 2 ‣ 3 PropaGaze: A Dataset for Systematically Analyzing Propaganda ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">2</span></a>. Specifically, <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.3">RUWA-Gaze</span> is built upon a dataset of real-world news articles focused on the Russia-Ukraine War <cite class="ltx_cite ltx_citemacro_cite">Khairova et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib30" title="">2023</a>)</cite>, while <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.4">Politifact-Gaze</span> is constructed using the PolitiFact partition of the FakeNewsNet dataset <cite class="ltx_cite ltx_citemacro_cite">Shu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib46" title="">2020</a>)</cite>.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Data Generation Pipeline</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S3.F2" title="Figure 2 ‣ 3 PropaGaze: A Dataset for Systematically Analyzing Propaganda ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">2</span></a> shows the data creation pipeline. Initially, we use GPT-3.5 to summarize human-written, published news articles and to identify key events and objective facts. These summaries are intended to be objective, as the original articles may reflect various biases that could influence the creation of new propaganda pieces. Following this, we use GPT-3.5 to extract all focal entities involved in the events. We then randomly select one entity’s perspective and set an intent to guide the revision of the article. We also randomly choose a set of propaganda techniques to be inserted into the article, reshaping its narrative. </p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p2.1">Subsequently, we use GPT-4 as an intermediary author to craft intentional propaganda articles based on real-world events by injecting sampled propaganda techniques into an objective summary. We also ask the model to self-analyze the appeals the rewritten article might evoke to ensure alignment with the established intent. Human readers then verify the data quality for any obvious errors. The prompts for each step are provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A2" title="Appendix B Data Generation Prompt Templates ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p3">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p3.1">As illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S3.T1" title="Table 1 ‣ 3.1 PTC-Gaze: Human-Annotated Dataset ‣ 3 PropaGaze: A Dataset for Systematically Analyzing Propaganda ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">1</span></a>, <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px1.p3.1.1">RUWA-Gaze</span> consists of 497 articles, and <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px1.p3.1.2">Politifact-Gaze</span> consists of 593 articles. While we generated moderate data due to the computational cost. We believe the data generation pipeline is generalizable. The language models can be replaced with cheaper or open-source LLMs to reduce costs and, in turn, generate larger-scale datasets. In addition, we identify that these two subsets come from different domains (Military &amp; War and Politics), and they differ significantly in both content and the use of propaganda techniques.
</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">LLMs have strong prior knowledge and advanced context understanding, which makes them ideal for synthesizing propaganda-rich datasets and potentially effective for analyzing propaganda. In this section, we explore three research questions: <span class="ltx_text ltx_font_bold" id="S4.p1.1.1">(1)</span> how off-the-shelf LLMs perform on propaganda analysis, <span class="ltx_text ltx_font_bold" id="S4.p1.1.2">(2)</span> how much the <span class="ltx_text ltx_font_italic" id="S4.p1.1.3">PropaGaze</span> dataset improves performance when used for training or fine-tuning, and <span class="ltx_text ltx_font_bold" id="S4.p1.1.4">(3)</span> whether propaganda analysis is transferable across domains.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Sub-Tasks and Metrics</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">As outlined in § <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S2" title="2 PropaInsight: A Propaganda Analysis Framework ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">2</span></a>, <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px1.p1.1.1">PropaInsight</span> makes it possible to break the propaganda analysis task into three sub-tasks to ensure detailed evaluation and capture key elements:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.ix1.p1">
<p class="ltx_p" id="S4.I1.ix1.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.I1.ix1.p1.1.1">Propaganda Techniques Identification:</span>
We use Intersection over Union (<span class="ltx_text ltx_font_bold" id="S4.I1.ix1.p1.1.2">IoU</span>) to measure the overlap between the identified text spans and the actual propaganda spans, and <span class="ltx_text ltx_font_bold" id="S4.I1.ix1.p1.1.3">F1</span> scores to evaluate propaganda technique classification, following prior task settings <cite class="ltx_cite ltx_citemacro_cite">Martino et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib37" title="">2020a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib38" title="">b</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.ix2.p1">
<p class="ltx_p" id="S4.I1.ix2.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.I1.ix2.p1.1.1">Appeals Analysis:</span>
We evaluate the quality of the generated responses using <span class="ltx_text ltx_font_bold" id="S4.I1.ix2.p1.1.2">BertScore</span> <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib69" title="">2019</a>)</cite> to measure semantic similarity.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I1.ix3.p1">
<p class="ltx_p" id="S4.I1.ix3.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.I1.ix3.p1.1.1">intent Analysis:</span>
Similarly, we use <span class="ltx_text ltx_font_bold" id="S4.I1.ix3.p1.1.2">BertScore</span> for this sub-task.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Models</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">We experiment with the following:</p>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I2.ix1.p1">
<p class="ltx_p" id="S4.I2.ix1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.ix1.p1.1.1">GPT-4-Turbo</span>: One of the top-performing OpenAI models for various tasks. We use it in both zero-shot and few-shot prompting settings across all sub-tasks. The specific prompts used for each sub-task are given in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A3" title="Appendix C Templates and Prompts We Used for Propaganda Analysis ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I2.ix2.p1">
<p class="ltx_p" id="S4.I2.ix2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.ix2.p1.1.1">Llama-7B-Chat</span>: A popular open-source LLM. Due to its smaller size and relatively worse performance compared to GPT-4-Turbo, we fine-tune it for our sub-tasks. Specifically, we instruction-tune it to predict whether each sentence contains propaganda, and if so, identify the techniques and the appeals used, and predict the article’s intent. See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A3" title="Appendix C Templates and Prompts We Used for Propaganda Analysis ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">C</span></a> for the fine-tuning prompts. </p>
</div>
</li>
<li class="ltx_item" id="S4.I2.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I2.ix3.p1">
<p class="ltx_p" id="S4.I2.ix3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.ix3.p1.1.1">Multi-Granularity Neural Networks (MGNN model)</span> <cite class="ltx_cite ltx_citemacro_cite">Da San Martino et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib10" title="">2019</a>)</cite>: A benchmark method for the propaganda techniques identification sub-task. We train MGNN from scratch for this specific task, as it is not designed for text generation and cannot be applied to the other two sub-tasks.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Data-Rich and Data-Sparse Training Settings</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.1">In real-world scenarios, obtaining a large volume of well-annotated data for analyzing propaganda is challenging, as discussed in § <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S3" title="3 PropaGaze: A Dataset for Systematically Analyzing Propaganda ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">3</span></a>. To simulate this limitation, we use both data-rich and data-sparse environments to evaluate the impact of larger, generalized synthetic datasets on model performance.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p2">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p2.1">For all <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px3.p2.1.1">PropaGaze</span> sub-datasets, we split the articles into training and testing sets using a 70:30 ratio. <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px3.p2.1.2">PTC-Gaze</span>, with only 79 articles, represents a data-sparse condition. In contrast, the synthetic sub-datasets, <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px3.p2.1.3">RUWA-Gaze</span> and <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px3.p2.1.4">Politifact-Gaze</span>, contain a total of over 1,000 articles. To simulate data-sparse scenarios with these two sub-datasets, we sample subsets matching the size of the full <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px3.p2.1.5">PTC-Gaze</span> training set. For data-rich conditions, we use the full training sets of <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px3.p2.1.6">RUWA-Gaze</span> and <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px3.p2.1.7">Politifact-Gaze</span>, reserving one-seventh as the validation set.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>How Do Off-the-Shelf LLMs Perform on Propaganda Analysis Tasks?</h3>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.7" style="width:411.9pt;height:110.1pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-109.2pt,29.0pt) scale(0.65345944264279,0.65345944264279) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.7.7">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T2.1.1.1.2"></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1">Dataset <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T2.1.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.1.m1.1d">→</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T2.1.1.1.3"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T2.1.1.1.3.1">RUWA-Gaze</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.1.1.1.4"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T2.1.1.1.5"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T2.1.1.1.5.1">Politifact-Gaze</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.1.1.1.6"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T2.1.1.1.7"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T2.1.1.1.7.1">PTC-Gaze</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3.3">
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.1.1">Data Setting <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.2.2.2.1.1.m1.1"><semantics id="S4.T2.2.2.2.1.1.m1.1a"><mo id="S4.T2.2.2.2.1.1.m1.1.1" stretchy="false" xref="S4.T2.2.2.2.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.1.m1.1b"><ci id="S4.T2.2.2.2.1.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.1.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.3.3.3.2"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.2.1">Model <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.3.3.3.2.1.m1.1"><semantics id="S4.T2.3.3.3.2.1.m1.1a"><mo id="S4.T2.3.3.3.2.1.m1.1.1" stretchy="false" xref="S4.T2.3.3.3.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.2.1.m1.1b"><ci id="S4.T2.3.3.3.2.1.m1.1.1.cmml" xref="S4.T2.3.3.3.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.2.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3">
<span class="ltx_text" id="S4.T2.3.3.3.3.1"></span> <span class="ltx_text" id="S4.T2.3.3.3.3.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.3.3.3.3.2.1">
<span class="ltx_tr" id="S4.T2.3.3.3.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.3.2.1.1.1">Span</span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.3.2.1.2.1">Avg. IoU</span></span>
</span></span><span class="ltx_text" id="S4.T2.3.3.3.3.3"></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.4">
<span class="ltx_text" id="S4.T2.3.3.3.4.1"></span> <span class="ltx_text" id="S4.T2.3.3.3.4.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.3.3.3.4.2.1">
<span class="ltx_tr" id="S4.T2.3.3.3.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.4.2.1.1.1">Techniques</span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.4.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.4.2.1.2.1">Macro F1</span></span>
</span></span><span class="ltx_text" id="S4.T2.3.3.3.4.3"></span></td>
<td class="ltx_td" id="S4.T2.3.3.3.5"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.6">
<span class="ltx_text" id="S4.T2.3.3.3.6.1"></span> <span class="ltx_text" id="S4.T2.3.3.3.6.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.3.3.3.6.2.1">
<span class="ltx_tr" id="S4.T2.3.3.3.6.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.6.2.1.1.1">Span</span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.6.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.6.2.1.2.1">Avg. IoU</span></span>
</span></span><span class="ltx_text" id="S4.T2.3.3.3.6.3"></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.7">
<span class="ltx_text" id="S4.T2.3.3.3.7.1"></span> <span class="ltx_text" id="S4.T2.3.3.3.7.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.3.3.3.7.2.1">
<span class="ltx_tr" id="S4.T2.3.3.3.7.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.7.2.1.1.1">Techniques</span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.7.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.7.2.1.2.1">Macro F1</span></span>
</span></span><span class="ltx_text" id="S4.T2.3.3.3.7.3"></span></td>
<td class="ltx_td" id="S4.T2.3.3.3.8"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.9">
<span class="ltx_text" id="S4.T2.3.3.3.9.1"></span> <span class="ltx_text" id="S4.T2.3.3.3.9.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.3.3.3.9.2.1">
<span class="ltx_tr" id="S4.T2.3.3.3.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.9.2.1.1.1">Span</span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.9.2.1.2.1">Avg. IoU</span></span>
</span></span><span class="ltx_text" id="S4.T2.3.3.3.9.3"></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.10">
<span class="ltx_text" id="S4.T2.3.3.3.10.1"></span> <span class="ltx_text" id="S4.T2.3.3.3.10.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.3.3.3.10.2.1">
<span class="ltx_tr" id="S4.T2.3.3.3.10.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.10.2.1.1.1">Techniques</span></span>
<span class="ltx_tr" id="S4.T2.3.3.3.10.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.3.10.2.1.2.1">Macro F1</span></span>
</span></span><span class="ltx_text" id="S4.T2.3.3.3.10.3"></span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.4">
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.2" rowspan="2"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T2.4.4.4.2.1">No Training Data</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.4.4.4.1">GPT-4-Turbo<sub class="ltx_sub" id="S4.T2.4.4.4.1.1"><span class="ltx_text ltx_font_italic" id="S4.T2.4.4.4.1.1.1">0s</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.4.3">0.073</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.4.4">0.097</td>
<td class="ltx_td" id="S4.T2.4.4.4.5"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.4.6">0.152</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.4.7">0.226</td>
<td class="ltx_td" id="S4.T2.4.4.4.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.4.9">0.124</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.4.10">0.068</td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.5.5">
<td class="ltx_td ltx_align_left" id="S4.T2.5.5.5.1">GPT-4-Turbo<sub class="ltx_sub" id="S4.T2.5.5.5.1.1"><span class="ltx_text ltx_font_italic" id="S4.T2.5.5.5.1.1.1">1s</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.5.2">0.132</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.5.3">0.145</td>
<td class="ltx_td" id="S4.T2.5.5.5.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.5.5">0.183</td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.5.6">0.269</td>
<td class="ltx_td" id="S4.T2.5.5.5.7"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.5.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.5.5.5.8.1">0.165</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.5.9">0.171</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.7.8">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.7.8.1" rowspan="2"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T2.7.7.8.1.1">Data-Sparse Training</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.7.7.8.2">MGNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.7.8.3">0.089</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.7.8.4">0.139</td>
<td class="ltx_td ltx_border_t" id="S4.T2.7.7.8.5"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.7.8.6">0.160</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.7.8.7">0.159</td>
<td class="ltx_td ltx_border_t" id="S4.T2.7.7.8.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.7.8.9">0.140</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.7.8.10"><span class="ltx_text ltx_font_bold" id="S4.T2.7.7.8.10.1">0.206</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.6.6">
<td class="ltx_td ltx_align_left" id="S4.T2.6.6.6.1">Llama-7B-Chat<sub class="ltx_sub" id="S4.T2.6.6.6.1.1"><span class="ltx_text ltx_font_italic" id="S4.T2.6.6.6.1.1.1">ft</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.6.2">0.230</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.6.3">0.210</td>
<td class="ltx_td" id="S4.T2.6.6.6.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.6.5">0.253</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.6.6">0.281</td>
<td class="ltx_td" id="S4.T2.6.6.6.7"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.6.8"><span class="ltx_text ltx_font_bold" id="S4.T2.6.6.6.8.1">0.179</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.6.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.6.6.6.9.1">0.191</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.7.9">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.7.7.9.1" rowspan="2"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T2.7.7.9.1.1">Data-Rich Training</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.7.7.9.2">MGNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.7.9.3"><span class="ltx_text ltx_font_bold" id="S4.T2.7.7.9.3.1">0.545</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.7.9.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.7.7.9.4.1">0.591</span></td>
<td class="ltx_td ltx_border_t" id="S4.T2.7.7.9.5"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.7.9.6"><span class="ltx_text ltx_font_bold" id="S4.T2.7.7.9.6.1">0.449</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.7.9.7"><span class="ltx_text ltx_font_bold" id="S4.T2.7.7.9.7.1">0.461</span></td>
<td class="ltx_td ltx_border_t" id="S4.T2.7.7.9.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.7.9.9">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.7.9.10">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.7.7">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.7.7.7.1">Llama-7B-Chat<sub class="ltx_sub" id="S4.T2.7.7.7.1.1"><span class="ltx_text ltx_font_italic" id="S4.T2.7.7.7.1.1.1">ft</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.7.7.7.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.7.7.7.2.1">0.506</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.7.7.7.3"><span class="ltx_text ltx_font_bold" id="S4.T2.7.7.7.3.1">0.607</span></td>
<td class="ltx_td ltx_border_bb" id="S4.T2.7.7.7.4"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.7.7.7.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.7.7.7.5.1">0.409</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.7.7.7.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.7.7.7.6.1">0.453</span></td>
<td class="ltx_td ltx_border_bb" id="S4.T2.7.7.7.7"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.7.7.7.8">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.7.7.7.9">-</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Model performance on the propaganda technique identification sub-task under different training data settings. We report the performance of trained MGNN model and both k-shot (<math alttext="ks" class="ltx_Math" display="inline" id="S4.T2.10.m1.1"><semantics id="S4.T2.10.m1.1b"><mrow id="S4.T2.10.m1.1.1" xref="S4.T2.10.m1.1.1.cmml"><mi id="S4.T2.10.m1.1.1.2" xref="S4.T2.10.m1.1.1.2.cmml">k</mi><mo id="S4.T2.10.m1.1.1.1" xref="S4.T2.10.m1.1.1.1.cmml">⁢</mo><mi id="S4.T2.10.m1.1.1.3" xref="S4.T2.10.m1.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.10.m1.1c"><apply id="S4.T2.10.m1.1.1.cmml" xref="S4.T2.10.m1.1.1"><times id="S4.T2.10.m1.1.1.1.cmml" xref="S4.T2.10.m1.1.1.1"></times><ci id="S4.T2.10.m1.1.1.2.cmml" xref="S4.T2.10.m1.1.1.2">𝑘</ci><ci id="S4.T2.10.m1.1.1.3.cmml" xref="S4.T2.10.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.m1.1d">ks</annotation><annotation encoding="application/x-llamapun" id="S4.T2.10.m1.1e">italic_k italic_s</annotation></semantics></math>) and fine-tuned (<math alttext="ft" class="ltx_Math" display="inline" id="S4.T2.11.m2.1"><semantics id="S4.T2.11.m2.1b"><mrow id="S4.T2.11.m2.1.1" xref="S4.T2.11.m2.1.1.cmml"><mi id="S4.T2.11.m2.1.1.2" xref="S4.T2.11.m2.1.1.2.cmml">f</mi><mo id="S4.T2.11.m2.1.1.1" xref="S4.T2.11.m2.1.1.1.cmml">⁢</mo><mi id="S4.T2.11.m2.1.1.3" xref="S4.T2.11.m2.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.11.m2.1c"><apply id="S4.T2.11.m2.1.1.cmml" xref="S4.T2.11.m2.1.1"><times id="S4.T2.11.m2.1.1.1.cmml" xref="S4.T2.11.m2.1.1.1"></times><ci id="S4.T2.11.m2.1.1.2.cmml" xref="S4.T2.11.m2.1.1.2">𝑓</ci><ci id="S4.T2.11.m2.1.1.3.cmml" xref="S4.T2.11.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.m2.1d">ft</annotation><annotation encoding="application/x-llamapun" id="S4.T2.11.m2.1e">italic_f italic_t</annotation></semantics></math>) LLMs.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.6" style="width:411.9pt;height:93.9pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-84.5pt,19.1pt) scale(0.708973415414335,0.708973415414335) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.6.6">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1">Dataset <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T3.1.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.1.m1.1d">→</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T3.1.1.1.2"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T3.1.1.1.2.1">RUWA-Gaze</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T3.1.1.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T3.1.1.1.4"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T3.1.1.1.4.1">Politifact-Gaze</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T3.1.1.1.5"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T3.1.1.1.6"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T3.1.1.1.6.1">PTC-Gaze</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.2">
<td class="ltx_td ltx_align_left" id="S4.T3.2.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.2.1.1">Model <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.2.2.2.1.1.m1.1"><semantics id="S4.T3.2.2.2.1.1.m1.1a"><mo id="S4.T3.2.2.2.1.1.m1.1.1" stretchy="false" xref="S4.T3.2.2.2.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.1.m1.1b"><ci id="S4.T3.2.2.2.1.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.1.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.2">
<span class="ltx_text" id="S4.T3.2.2.2.2.1"></span> <span class="ltx_text" id="S4.T3.2.2.2.2.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.2.2.2.2.2.1">
<span class="ltx_tr" id="S4.T3.2.2.2.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.2.2.2.2.1.1.1">Appeals</span></span>
<span class="ltx_tr" id="S4.T3.2.2.2.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.2.2.2.2.1.2.1">BertScore</span></span>
</span></span><span class="ltx_text" id="S4.T3.2.2.2.2.3"></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.3">
<span class="ltx_text" id="S4.T3.2.2.2.3.1"></span> <span class="ltx_text" id="S4.T3.2.2.2.3.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.2.2.2.3.2.1">
<span class="ltx_tr" id="S4.T3.2.2.2.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.2.2.3.2.1.1.1">Intents</span></span>
<span class="ltx_tr" id="S4.T3.2.2.2.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.2.2.3.2.1.2.1">BertScore</span></span>
</span></span><span class="ltx_text" id="S4.T3.2.2.2.3.3"></span></td>
<td class="ltx_td" id="S4.T3.2.2.2.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.5">
<span class="ltx_text" id="S4.T3.2.2.2.5.1"></span> <span class="ltx_text" id="S4.T3.2.2.2.5.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.2.2.2.5.2.1">
<span class="ltx_tr" id="S4.T3.2.2.2.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.2.2.5.2.1.1.1">Appeals</span></span>
<span class="ltx_tr" id="S4.T3.2.2.2.5.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.2.2.5.2.1.2.1">BertScore</span></span>
</span></span><span class="ltx_text" id="S4.T3.2.2.2.5.3"></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.6">
<span class="ltx_text" id="S4.T3.2.2.2.6.1"></span> <span class="ltx_text" id="S4.T3.2.2.2.6.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.2.2.2.6.2.1">
<span class="ltx_tr" id="S4.T3.2.2.2.6.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.2.2.6.2.1.1.1">Intents</span></span>
<span class="ltx_tr" id="S4.T3.2.2.2.6.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.2.2.6.2.1.2.1">BertScore</span></span>
</span></span><span class="ltx_text" id="S4.T3.2.2.2.6.3"></span></td>
<td class="ltx_td" id="S4.T3.2.2.2.7"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.8">
<span class="ltx_text" id="S4.T3.2.2.2.8.1"></span> <span class="ltx_text" id="S4.T3.2.2.2.8.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.2.2.2.8.2.1">
<span class="ltx_tr" id="S4.T3.2.2.2.8.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.2.2.8.2.1.1.1">Appeals</span></span>
<span class="ltx_tr" id="S4.T3.2.2.2.8.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.2.2.8.2.1.2.1">BertScore</span></span>
</span></span><span class="ltx_text" id="S4.T3.2.2.2.8.3"></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.9">
<span class="ltx_text" id="S4.T3.2.2.2.9.1"></span> <span class="ltx_text" id="S4.T3.2.2.2.9.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.2.2.2.9.2.1">
<span class="ltx_tr" id="S4.T3.2.2.2.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.2.2.9.2.1.1.1">Intents</span></span>
<span class="ltx_tr" id="S4.T3.2.2.2.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.2.2.9.2.1.2.1">BertScore</span></span>
</span></span><span class="ltx_text" id="S4.T3.2.2.2.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.3.3">
<td class="ltx_td ltx_align_left" id="S4.T3.3.3.3.1">GPT-4-Turbo<sub class="ltx_sub" id="S4.T3.3.3.3.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.3.3.3.1.1.1">0s</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.3.3.2">0.282</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.3.3.3">0.849</td>
<td class="ltx_td" id="S4.T3.3.3.3.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.3.3.5">0.298</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.3.3.6">0.863</td>
<td class="ltx_td" id="S4.T3.3.3.3.7"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.3.3.8">0.228</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.3.3.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.3.3.3.9.1">0.869</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.4.4">
<td class="ltx_td ltx_align_left" id="S4.T3.4.4.4.1">GPT-4-Turbo<sub class="ltx_sub" id="S4.T3.4.4.4.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.4.4.4.1.1.1">1s</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.4.4.4.2.1">0.324</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.3"><span class="ltx_text ltx_font_bold" id="S4.T3.4.4.4.3.1">0.879</span></td>
<td class="ltx_td" id="S4.T3.4.4.4.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.4.4.4.5.1">0.345</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.6"><span class="ltx_text ltx_font_bold" id="S4.T3.4.4.4.6.1">0.875</span></td>
<td class="ltx_td" id="S4.T3.4.4.4.7"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.8"><span class="ltx_text ltx_font_bold" id="S4.T3.4.4.4.8.1">0.331</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.9"><span class="ltx_text ltx_font_bold" id="S4.T3.4.4.4.9.1">0.881</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.5.5">
<td class="ltx_td ltx_align_left" id="S4.T3.5.5.5.1">Llama-7B-Chat<sub class="ltx_sub" id="S4.T3.5.5.5.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.5.5.5.1.1.1">ft</span></sub> <span class="ltx_text ltx_font_italic" id="S4.T3.5.5.5.1.2">(Data-Sparse)</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.2">0.313</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.3">0.851</td>
<td class="ltx_td" id="S4.T3.5.5.5.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.5">0.342</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.6">0.860</td>
<td class="ltx_td" id="S4.T3.5.5.5.7"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.5.5.5.8.1">0.249</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.9">0.843</td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.6.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.6.6.6.1">Llama-7B-Chat<sub class="ltx_sub" id="S4.T3.6.6.6.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.6.6.6.1.1.1">ft</span></sub> <span class="ltx_text ltx_font_italic" id="S4.T3.6.6.6.1.2">(Data-Rich)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.6.6.2"><span class="ltx_text ltx_font_bold" id="S4.T3.6.6.6.2.1">0.612</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.6.6.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.6.6.6.3.1">0.861</span></td>
<td class="ltx_td ltx_border_bb" id="S4.T3.6.6.6.4"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.6.6.5"><span class="ltx_text ltx_font_bold" id="S4.T3.6.6.6.5.1">0.495</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.6.6.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.6.6.6.6.1">0.864</span></td>
<td class="ltx_td ltx_border_bb" id="S4.T3.6.6.6.7"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.6.6.8">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.6.6.9">-</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Model performance on appeal and intent analysis sub-tasks under different training data settings. We report the performance of zero-shot (<math alttext="0s" class="ltx_Math" display="inline" id="S4.T3.9.m1.1"><semantics id="S4.T3.9.m1.1b"><mrow id="S4.T3.9.m1.1.1" xref="S4.T3.9.m1.1.1.cmml"><mn id="S4.T3.9.m1.1.1.2" xref="S4.T3.9.m1.1.1.2.cmml">0</mn><mo id="S4.T3.9.m1.1.1.1" xref="S4.T3.9.m1.1.1.1.cmml">⁢</mo><mi id="S4.T3.9.m1.1.1.3" xref="S4.T3.9.m1.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.9.m1.1c"><apply id="S4.T3.9.m1.1.1.cmml" xref="S4.T3.9.m1.1.1"><times id="S4.T3.9.m1.1.1.1.cmml" xref="S4.T3.9.m1.1.1.1"></times><cn id="S4.T3.9.m1.1.1.2.cmml" type="integer" xref="S4.T3.9.m1.1.1.2">0</cn><ci id="S4.T3.9.m1.1.1.3.cmml" xref="S4.T3.9.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.m1.1d">0s</annotation><annotation encoding="application/x-llamapun" id="S4.T3.9.m1.1e">0 italic_s</annotation></semantics></math>) and fine-tuned (<math alttext="ft" class="ltx_Math" display="inline" id="S4.T3.10.m2.1"><semantics id="S4.T3.10.m2.1b"><mrow id="S4.T3.10.m2.1.1" xref="S4.T3.10.m2.1.1.cmml"><mi id="S4.T3.10.m2.1.1.2" xref="S4.T3.10.m2.1.1.2.cmml">f</mi><mo id="S4.T3.10.m2.1.1.1" xref="S4.T3.10.m2.1.1.1.cmml">⁢</mo><mi id="S4.T3.10.m2.1.1.3" xref="S4.T3.10.m2.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.10.m2.1c"><apply id="S4.T3.10.m2.1.1.cmml" xref="S4.T3.10.m2.1.1"><times id="S4.T3.10.m2.1.1.1.cmml" xref="S4.T3.10.m2.1.1.1"></times><ci id="S4.T3.10.m2.1.1.2.cmml" xref="S4.T3.10.m2.1.1.2">𝑓</ci><ci id="S4.T3.10.m2.1.1.3.cmml" xref="S4.T3.10.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.10.m2.1d">ft</annotation><annotation encoding="application/x-llamapun" id="S4.T3.10.m2.1e">italic_f italic_t</annotation></semantics></math>) LLMs.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">As shown in Tables <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S4.T2" title="Table 2 ‣ 4.2 How Do Off-the-Shelf LLMs Perform on Propaganda Analysis Tasks? ‣ 4 Experiments ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S4.T3" title="Table 3 ‣ 4.2 How Do Off-the-Shelf LLMs Perform on Propaganda Analysis Tasks? ‣ 4 Experiments ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">3</span></a>, <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">zero-shot LLMs struggle with propaganda analysis</span>. For example, in identifying propaganda techniques, zero-shot GPT-4-Turbo underperforms compared to the trained MGNN, even in data-sparse conditions, despite MGNN being much smaller in size. Zero-shot LLMs often struggle to pinpoint sentences containing propaganda. Similarly, in appeal analysis, zero-shot GPT-4-Turbo achieves relatively low BertScores. However, these models perform better at inferring intent, as shown by their stronger performance in the intent analysis sub-task (Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S4.T3" title="Table 3 ‣ 4.2 How Do Off-the-Shelf LLMs Perform on Propaganda Analysis Tasks? ‣ 4 Experiments ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Few-shot prompting improves LLM performance in analyzing propaganda elements</span>. Specifically, in identifying propaganda techniques, one-shot GPT-4-Turbo shows an 80.8% improvement in average IoU on <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.2">RUWA-Gaze</span>, a 20.4% increase on <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.3">Politifact-Gaze</span>, and a 33.1% higher IoU on <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.4">PTC-Gaze</span> compared to a zero-shot setting. Similarly, in appeal analysis, one-shot GPT-4-Turbo achieves 14.9% higher BertScore on <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.5">RUWA-Gaze</span>, 15.8% higher on <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.6">Politifact-Gaze</span>, and 45.2% higher on <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.7">PTC-Gaze</span>. In intent analysis, zero-shot GPT-4-Turbo already performs well. The improvements compared to one-shot prompting are minor, with the highest increase being 3.5% on <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.8">RUWA-Gaze</span>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>How Much Does <span class="ltx_text ltx_font_italic" id="S4.SS3.1.1">PropaGaze</span> Enhance Model Performance?</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS3.p1.1.1">PropaGaze<span class="ltx_text ltx_font_upright" id="S4.SS3.p1.1.1.1"> substantially improves the overall propaganda analysis performance</span></span>, especially in identifying propaganda techniques, under both data-sparse and data-rich training conditions. In the data-sparse setting, fine-tuned LLaMA-7B-Chat outperforms one-shot GPT-4-Turbo, achieving an average of 65.8% higher text span IoU and 33.7% higher technique identification F1 score, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S4.T2" title="Table 2 ‣ 4.2 How Do Off-the-Shelf LLMs Perform on Propaganda Analysis Tasks? ‣ 4 Experiments ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">2</span></a>. In the data-rich setting, the performance increases even further, with LLaMA-7B-Chat showing 90.9% higher text span IoU and 125.1% higher F1 score compared to the data-sparse results.Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S4.T3" title="Table 3 ‣ 4.2 How Do Off-the-Shelf LLMs Perform on Propaganda Analysis Tasks? ‣ 4 Experiments ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">3</span></a> shows similar improvements in appeals and intent analysis. For the appeals sub-task, data-rich fine-tuning leads to an average 70.1% increase in BertScore, while for intent analysis there is a smaller 8.5% gain compared to data-sparse training. This is likely due to the already high baseline performance. These results demonstrate that the synthetic sub-datasets effectively complement the limited human-annotated data, significantly improving the model’s performance in analyzing propaganda elements.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">We also compare the performance of LLaMA-7B-Chat with the baseline benchmark MGNN on propaganda identification. In the data-sparse setting, fine-tuned LLaMA-7B-Chat substantially outperforms trained MGNN, achieving 158.43% higher IoU on <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.1">RUWA-Gaze</span> and 58.1% higher IoU on PolitiFact-Gaze. However, in data-rich scenarios, MGNN, benefiting from the larger amount of training data, surpassing LLaMA-7B-Chat. This may be due to the fact that smaller models, such as MGNN, can overfit when exposed to excessive training data, while larger LLMs, such as LLaMA-7B-Chat, generalize better in data-sparse conditions. These findings suggest that <span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.2">LLMs are more suited for the task with limited training data, while smaller, dedicated models like MGNN could benefit more from the synthetic sub-datasets provided by <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.2.1">PropaGaze</span> in data-rich environments</span>. This is consistent with the findings of <cite class="ltx_cite ltx_citemacro_citet">Alhindi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib4" title="">2024</a>)</cite>. Thus, with sufficient training data, we can implement a pipeline that first localizes and identifies propaganda techniques using MGNN, followed by appeals and intent analysis based on MGNN’s output. This approach could potentially enhance the overall quality of the model’s output for the entire propaganda analysis task.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.45" style="width:411.9pt;height:113.3pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-100.3pt,27.4pt) scale(0.672575154132939,0.672575154132939) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.45.45">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T4.1.1.1.2"></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.1">Eval Dataset <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T4.1.1.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T4.1.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.1.1.m1.1d">→</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.1.1.1.3"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T4.1.1.1.3.1">RUWA-Gaze</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T4.1.1.1.4"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.1.1.1.5"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T4.1.1.1.5.1">Politifact-Gaze</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T4.1.1.1.6"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.1.1.1.7"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T4.1.1.1.7.1">PTC-Gaze</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.3.3">
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.2.1">
<span class="ltx_text" id="S4.T4.2.2.2.1.2"></span><span class="ltx_text" id="S4.T4.2.2.2.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.2.2.2.1.1.1">
<span class="ltx_tr" id="S4.T4.2.2.2.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.2.2.2.1.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.2.1.1.1.2.1.1">Additional</span></span></span>
<span class="ltx_tr" id="S4.T4.2.2.2.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.2.2.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.2.1.1.1.1.1.1">Train Data</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.2.2.2.1.1.1.1.1.m1.1"><semantics id="S4.T4.2.2.2.1.1.1.1.1.m1.1a"><mo id="S4.T4.2.2.2.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T4.2.2.2.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.1.1.1.1.1.m1.1b"><ci id="S4.T4.2.2.2.1.1.1.1.1.m1.1.1.cmml" xref="S4.T4.2.2.2.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.1.1.1.1.1.m1.1d">↓</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="S4.T4.2.2.2.1.3"></span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.3.3.3.2"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.3.2.1">Model <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.3.3.3.2.1.m1.1"><semantics id="S4.T4.3.3.3.2.1.m1.1a"><mo id="S4.T4.3.3.3.2.1.m1.1.1" stretchy="false" xref="S4.T4.3.3.3.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.2.1.m1.1b"><ci id="S4.T4.3.3.3.2.1.m1.1.1.cmml" xref="S4.T4.3.3.3.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.3.2.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.3.3">
<span class="ltx_text" id="S4.T4.3.3.3.3.1"></span> <span class="ltx_text" id="S4.T4.3.3.3.3.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.3.3.3.3.2.1">
<span class="ltx_tr" id="S4.T4.3.3.3.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.3.3.3.3.2.1.1.1">Span</span></span>
<span class="ltx_tr" id="S4.T4.3.3.3.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.3.3.3.3.2.1.2.1">Avg. IoU</span></span>
</span></span><span class="ltx_text" id="S4.T4.3.3.3.3.3"></span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.3.4">
<span class="ltx_text" id="S4.T4.3.3.3.4.1"></span> <span class="ltx_text" id="S4.T4.3.3.3.4.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.3.3.3.4.2.1">
<span class="ltx_tr" id="S4.T4.3.3.3.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.3.3.3.4.2.1.1.1">Techniques</span></span>
<span class="ltx_tr" id="S4.T4.3.3.3.4.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.3.3.3.4.2.1.2.1">Macro F1</span></span>
</span></span><span class="ltx_text" id="S4.T4.3.3.3.4.3"></span></td>
<td class="ltx_td" id="S4.T4.3.3.3.5"></td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.3.6">
<span class="ltx_text" id="S4.T4.3.3.3.6.1"></span> <span class="ltx_text" id="S4.T4.3.3.3.6.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.3.3.3.6.2.1">
<span class="ltx_tr" id="S4.T4.3.3.3.6.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.3.3.3.6.2.1.1.1">Span</span></span>
<span class="ltx_tr" id="S4.T4.3.3.3.6.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.3.3.3.6.2.1.2.1">Avg. IoU</span></span>
</span></span><span class="ltx_text" id="S4.T4.3.3.3.6.3"></span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.3.7">
<span class="ltx_text" id="S4.T4.3.3.3.7.1"></span> <span class="ltx_text" id="S4.T4.3.3.3.7.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.3.3.3.7.2.1">
<span class="ltx_tr" id="S4.T4.3.3.3.7.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.3.3.3.7.2.1.1.1">Techniques</span></span>
<span class="ltx_tr" id="S4.T4.3.3.3.7.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.3.3.3.7.2.1.2.1">Macro F1</span></span>
</span></span><span class="ltx_text" id="S4.T4.3.3.3.7.3"></span></td>
<td class="ltx_td" id="S4.T4.3.3.3.8"></td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.3.9">
<span class="ltx_text" id="S4.T4.3.3.3.9.1"></span> <span class="ltx_text" id="S4.T4.3.3.3.9.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.3.3.3.9.2.1">
<span class="ltx_tr" id="S4.T4.3.3.3.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.3.3.3.9.2.1.1.1">Span</span></span>
<span class="ltx_tr" id="S4.T4.3.3.3.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.3.3.3.9.2.1.2.1">Avg. IoU</span></span>
</span></span><span class="ltx_text" id="S4.T4.3.3.3.9.3"></span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.3.10">
<span class="ltx_text" id="S4.T4.3.3.3.10.1"></span> <span class="ltx_text" id="S4.T4.3.3.3.10.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.3.3.3.10.2.1">
<span class="ltx_tr" id="S4.T4.3.3.3.10.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.3.3.3.10.2.1.1.1">Techniques</span></span>
<span class="ltx_tr" id="S4.T4.3.3.3.10.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.3.3.3.10.2.1.2.1">Macro F1</span></span>
</span></span><span class="ltx_text" id="S4.T4.3.3.3.10.3"></span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.10.10.10">
<td class="ltx_td ltx_align_center" id="S4.T4.10.10.10.8" rowspan="2"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T4.10.10.10.8.1">RUWA-Gaze</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.4.4.4.1">MGNN<sub class="ltx_sub" id="S4.T4.4.4.4.1.1"><span class="ltx_text ltx_font_italic" id="S4.T4.4.4.4.1.1.1">ft</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.5.5.5.2">0.089 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.5.5.5.2.m1.1"><semantics id="S4.T4.5.5.5.2.m1.1a"><mo fence="false" id="S4.T4.5.5.5.2.m1.1.1" stretchy="false" xref="S4.T4.5.5.5.2.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.2.m1.1b"><ci id="S4.T4.5.5.5.2.m1.1.1.cmml" xref="S4.T4.5.5.5.2.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.2.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.5.5.5.2.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S4.T4.5.5.5.2.1">0.545</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.6.6.6.3">0.139 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.6.6.6.3.m1.1"><semantics id="S4.T4.6.6.6.3.m1.1a"><mo fence="false" id="S4.T4.6.6.6.3.m1.1.1" stretchy="false" xref="S4.T4.6.6.6.3.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.3.m1.1b"><ci id="S4.T4.6.6.6.3.m1.1.1.cmml" xref="S4.T4.6.6.6.3.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.3.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.6.6.6.3.m1.1d">|</annotation></semantics></math> 0.591</td>
<td class="ltx_td" id="S4.T4.10.10.10.9"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.7.7.7.4">0.243 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.7.7.7.4.m1.1"><semantics id="S4.T4.7.7.7.4.m1.1a"><mo fence="false" id="S4.T4.7.7.7.4.m1.1.1" stretchy="false" xref="S4.T4.7.7.7.4.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.7.4.m1.1b"><ci id="S4.T4.7.7.7.4.m1.1.1.cmml" xref="S4.T4.7.7.7.4.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.7.4.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.7.7.7.4.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S4.T4.7.7.7.4.1">0.471</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.8.8.8.5">0.251 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.8.8.8.5.m1.1"><semantics id="S4.T4.8.8.8.5.m1.1a"><mo fence="false" id="S4.T4.8.8.8.5.m1.1.1" stretchy="false" xref="S4.T4.8.8.8.5.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.8.8.8.5.m1.1b"><ci id="S4.T4.8.8.8.5.m1.1.1.cmml" xref="S4.T4.8.8.8.5.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.8.8.5.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.8.8.8.5.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S4.T4.8.8.8.5.1">0.475</span>
</td>
<td class="ltx_td" id="S4.T4.10.10.10.10"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.9.9.9.6">0.157 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.9.9.9.6.m1.1"><semantics id="S4.T4.9.9.9.6.m1.1a"><mo fence="false" id="S4.T4.9.9.9.6.m1.1.1" stretchy="false" xref="S4.T4.9.9.9.6.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.9.9.9.6.m1.1b"><ci id="S4.T4.9.9.9.6.m1.1.1.cmml" xref="S4.T4.9.9.9.6.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.9.9.9.6.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.9.9.9.6.m1.1d">|</annotation></semantics></math> 0.224</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.10.10.7">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.10.10.10.7.1">0.212</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.10.10.10.7.m1.1"><semantics id="S4.T4.10.10.10.7.m1.1a"><mo fence="false" id="S4.T4.10.10.10.7.m1.1.1" stretchy="false" xref="S4.T4.10.10.10.7.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.10.10.10.7.m1.1b"><ci id="S4.T4.10.10.10.7.m1.1.1.cmml" xref="S4.T4.10.10.10.7.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.10.10.10.7.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.10.10.10.7.m1.1d">|</annotation></semantics></math> 0.272</td>
</tr>
<tr class="ltx_tr" id="S4.T4.17.17.17">
<td class="ltx_td ltx_align_left" id="S4.T4.11.11.11.1">Llama-7B-Chat<sub class="ltx_sub" id="S4.T4.11.11.11.1.1"><span class="ltx_text ltx_font_italic" id="S4.T4.11.11.11.1.1.1">ft</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.12.12.12.2">0.230 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.12.12.12.2.m1.1"><semantics id="S4.T4.12.12.12.2.m1.1a"><mo fence="false" id="S4.T4.12.12.12.2.m1.1.1" stretchy="false" xref="S4.T4.12.12.12.2.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.12.12.12.2.m1.1b"><ci id="S4.T4.12.12.12.2.m1.1.1.cmml" xref="S4.T4.12.12.12.2.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.12.12.12.2.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.12.12.12.2.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.12.12.12.2.1">0.506</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.13.13.13.3">0.210 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.13.13.13.3.m1.1"><semantics id="S4.T4.13.13.13.3.m1.1a"><mo fence="false" id="S4.T4.13.13.13.3.m1.1.1" stretchy="false" xref="S4.T4.13.13.13.3.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.13.13.13.3.m1.1b"><ci id="S4.T4.13.13.13.3.m1.1.1.cmml" xref="S4.T4.13.13.13.3.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.13.13.13.3.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.13.13.13.3.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S4.T4.13.13.13.3.1">0.607</span>
</td>
<td class="ltx_td" id="S4.T4.17.17.17.8"></td>
<td class="ltx_td ltx_align_center" id="S4.T4.14.14.14.4">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.14.14.14.4.1">0.262</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.14.14.14.4.m1.1"><semantics id="S4.T4.14.14.14.4.m1.1a"><mo fence="false" id="S4.T4.14.14.14.4.m1.1.1" stretchy="false" xref="S4.T4.14.14.14.4.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.14.14.14.4.m1.1b"><ci id="S4.T4.14.14.14.4.m1.1.1.cmml" xref="S4.T4.14.14.14.4.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.14.14.14.4.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.14.14.14.4.m1.1d">|</annotation></semantics></math> 0.379</td>
<td class="ltx_td ltx_align_center" id="S4.T4.15.15.15.5">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.15.15.15.5.1">0.274</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.15.15.15.5.m1.1"><semantics id="S4.T4.15.15.15.5.m1.1a"><mo fence="false" id="S4.T4.15.15.15.5.m1.1.1" stretchy="false" xref="S4.T4.15.15.15.5.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.15.15.15.5.m1.1b"><ci id="S4.T4.15.15.15.5.m1.1.1.cmml" xref="S4.T4.15.15.15.5.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.15.15.15.5.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.15.15.15.5.m1.1d">|</annotation></semantics></math> 0.418</td>
<td class="ltx_td" id="S4.T4.17.17.17.9"></td>
<td class="ltx_td ltx_align_center" id="S4.T4.16.16.16.6">
<span class="ltx_text ltx_font_bold" id="S4.T4.16.16.16.6.1">0.215</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.16.16.16.6.m1.1"><semantics id="S4.T4.16.16.16.6.m1.1a"><mo fence="false" id="S4.T4.16.16.16.6.m1.1.1" stretchy="false" xref="S4.T4.16.16.16.6.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.16.16.16.6.m1.1b"><ci id="S4.T4.16.16.16.6.m1.1.1.cmml" xref="S4.T4.16.16.16.6.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.16.16.16.6.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.16.16.16.6.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.16.16.16.6.2">0.243</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.17.17.17.7">
<span class="ltx_text ltx_font_bold" id="S4.T4.17.17.17.7.1">0.220</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.17.17.17.7.m1.1"><semantics id="S4.T4.17.17.17.7.m1.1a"><mo fence="false" id="S4.T4.17.17.17.7.m1.1.1" stretchy="false" xref="S4.T4.17.17.17.7.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.17.17.17.7.m1.1b"><ci id="S4.T4.17.17.17.7.m1.1.1.cmml" xref="S4.T4.17.17.17.7.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.17.17.17.7.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.17.17.17.7.m1.1d">|</annotation></semantics></math> 0.258</td>
</tr>
<tr class="ltx_tr" id="S4.T4.24.24.24">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.24.24.24.8" rowspan="2"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T4.24.24.24.8.1">Politifact-Gaze</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.18.18.18.1">MGNN<sub class="ltx_sub" id="S4.T4.18.18.18.1.1"><span class="ltx_text ltx_font_italic" id="S4.T4.18.18.18.1.1.1">ft</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.19.19.19.2">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.19.19.19.2.1">0.246</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.19.19.19.2.m1.1"><semantics id="S4.T4.19.19.19.2.m1.1a"><mo fence="false" id="S4.T4.19.19.19.2.m1.1.1" stretchy="false" xref="S4.T4.19.19.19.2.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.19.19.19.2.m1.1b"><ci id="S4.T4.19.19.19.2.m1.1.1.cmml" xref="S4.T4.19.19.19.2.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.19.19.19.2.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.19.19.19.2.m1.1d">|</annotation></semantics></math> 0.456</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.20.20.20.3">
<span class="ltx_text ltx_font_bold" id="S4.T4.20.20.20.3.1">0.281</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.20.20.20.3.m1.1"><semantics id="S4.T4.20.20.20.3.m1.1a"><mo fence="false" id="S4.T4.20.20.20.3.m1.1.1" stretchy="false" xref="S4.T4.20.20.20.3.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.20.20.20.3.m1.1b"><ci id="S4.T4.20.20.20.3.m1.1.1.cmml" xref="S4.T4.20.20.20.3.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.20.20.20.3.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.20.20.20.3.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.20.20.20.3.2">0.593</span>
</td>
<td class="ltx_td ltx_border_t" id="S4.T4.24.24.24.9"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.21.21.21.4">0.160 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.21.21.21.4.m1.1"><semantics id="S4.T4.21.21.21.4.m1.1a"><mo fence="false" id="S4.T4.21.21.21.4.m1.1.1" stretchy="false" xref="S4.T4.21.21.21.4.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.21.21.21.4.m1.1b"><ci id="S4.T4.21.21.21.4.m1.1.1.cmml" xref="S4.T4.21.21.21.4.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.21.21.21.4.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.21.21.21.4.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.21.21.21.4.1">0.449</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.22.22.22.5">0.159 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.22.22.22.5.m1.1"><semantics id="S4.T4.22.22.22.5.m1.1a"><mo fence="false" id="S4.T4.22.22.22.5.m1.1.1" stretchy="false" xref="S4.T4.22.22.22.5.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.22.22.22.5.m1.1b"><ci id="S4.T4.22.22.22.5.m1.1.1.cmml" xref="S4.T4.22.22.22.5.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.22.22.22.5.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.22.22.22.5.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.22.22.22.5.1">0.461</span>
</td>
<td class="ltx_td ltx_border_t" id="S4.T4.24.24.24.10"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.23.23.23.6">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.23.23.23.6.1">0.203</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.23.23.23.6.m1.1"><semantics id="S4.T4.23.23.23.6.m1.1a"><mo fence="false" id="S4.T4.23.23.23.6.m1.1.1" stretchy="false" xref="S4.T4.23.23.23.6.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.23.23.23.6.m1.1b"><ci id="S4.T4.23.23.23.6.m1.1.1.cmml" xref="S4.T4.23.23.23.6.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.23.23.23.6.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.23.23.23.6.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S4.T4.23.23.23.6.2">0.252</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.24.24.24.7">0.210 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.24.24.24.7.m1.1"><semantics id="S4.T4.24.24.24.7.m1.1a"><mo fence="false" id="S4.T4.24.24.24.7.m1.1.1" stretchy="false" xref="S4.T4.24.24.24.7.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.24.24.24.7.m1.1b"><ci id="S4.T4.24.24.24.7.m1.1.1.cmml" xref="S4.T4.24.24.24.7.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.24.24.24.7.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.24.24.24.7.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S4.T4.24.24.24.7.1">0.298</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.31.31.31">
<td class="ltx_td ltx_align_left" id="S4.T4.25.25.25.1">Llama-7B-Chat<sub class="ltx_sub" id="S4.T4.25.25.25.1.1"><span class="ltx_text ltx_font_italic" id="S4.T4.25.25.25.1.1.1">ft</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.26.26.26.2">
<span class="ltx_text ltx_font_bold" id="S4.T4.26.26.26.2.1">0.271</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.26.26.26.2.m1.1"><semantics id="S4.T4.26.26.26.2.m1.1a"><mo fence="false" id="S4.T4.26.26.26.2.m1.1.1" stretchy="false" xref="S4.T4.26.26.26.2.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.26.26.26.2.m1.1b"><ci id="S4.T4.26.26.26.2.m1.1.1.cmml" xref="S4.T4.26.26.26.2.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.26.26.26.2.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.26.26.26.2.m1.1d">|</annotation></semantics></math> 0.443</td>
<td class="ltx_td ltx_align_center" id="S4.T4.27.27.27.3">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.27.27.27.3.1">0.265</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.27.27.27.3.m1.1"><semantics id="S4.T4.27.27.27.3.m1.1a"><mo fence="false" id="S4.T4.27.27.27.3.m1.1.1" stretchy="false" xref="S4.T4.27.27.27.3.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.27.27.27.3.m1.1b"><ci id="S4.T4.27.27.27.3.m1.1.1.cmml" xref="S4.T4.27.27.27.3.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.27.27.27.3.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.27.27.27.3.m1.1d">|</annotation></semantics></math> 0.582</td>
<td class="ltx_td" id="S4.T4.31.31.31.8"></td>
<td class="ltx_td ltx_align_center" id="S4.T4.28.28.28.4">0.253 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.28.28.28.4.m1.1"><semantics id="S4.T4.28.28.28.4.m1.1a"><mo fence="false" id="S4.T4.28.28.28.4.m1.1.1" stretchy="false" xref="S4.T4.28.28.28.4.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.28.28.28.4.m1.1b"><ci id="S4.T4.28.28.28.4.m1.1.1.cmml" xref="S4.T4.28.28.28.4.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.28.28.28.4.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.28.28.28.4.m1.1d">|</annotation></semantics></math> 0.409</td>
<td class="ltx_td ltx_align_center" id="S4.T4.29.29.29.5">
<span class="ltx_text ltx_font_bold" id="S4.T4.29.29.29.5.1">0.281</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.29.29.29.5.m1.1"><semantics id="S4.T4.29.29.29.5.m1.1a"><mo fence="false" id="S4.T4.29.29.29.5.m1.1.1" stretchy="false" xref="S4.T4.29.29.29.5.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.29.29.29.5.m1.1b"><ci id="S4.T4.29.29.29.5.m1.1.1.cmml" xref="S4.T4.29.29.29.5.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.29.29.29.5.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.29.29.29.5.m1.1d">|</annotation></semantics></math> 0.453</td>
<td class="ltx_td" id="S4.T4.31.31.31.9"></td>
<td class="ltx_td ltx_align_center" id="S4.T4.30.30.30.6">0.196 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.30.30.30.6.m1.1"><semantics id="S4.T4.30.30.30.6.m1.1a"><mo fence="false" id="S4.T4.30.30.30.6.m1.1.1" stretchy="false" xref="S4.T4.30.30.30.6.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.30.30.30.6.m1.1b"><ci id="S4.T4.30.30.30.6.m1.1.1.cmml" xref="S4.T4.30.30.30.6.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.30.30.30.6.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.30.30.30.6.m1.1d">|</annotation></semantics></math> 0.237</td>
<td class="ltx_td ltx_align_center" id="S4.T4.31.31.31.7">0.204 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.31.31.31.7.m1.1"><semantics id="S4.T4.31.31.31.7.m1.1a"><mo fence="false" id="S4.T4.31.31.31.7.m1.1.1" stretchy="false" xref="S4.T4.31.31.31.7.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.31.31.31.7.m1.1b"><ci id="S4.T4.31.31.31.7.m1.1.1.cmml" xref="S4.T4.31.31.31.7.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.31.31.31.7.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.31.31.31.7.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.31.31.31.7.1">0.273</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.38.38.38">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.38.38.38.8" rowspan="2"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T4.38.38.38.8.1">PTC-Gaze</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.32.32.32.1">MGNN<sub class="ltx_sub" id="S4.T4.32.32.32.1.1"><span class="ltx_text ltx_font_italic" id="S4.T4.32.32.32.1.1.1">ft</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.33.33.33.2">0.189 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.33.33.33.2.m1.1"><semantics id="S4.T4.33.33.33.2.m1.1a"><mo fence="false" id="S4.T4.33.33.33.2.m1.1.1" stretchy="false" xref="S4.T4.33.33.33.2.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.33.33.33.2.m1.1b"><ci id="S4.T4.33.33.33.2.m1.1.1.cmml" xref="S4.T4.33.33.33.2.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.33.33.33.2.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.33.33.33.2.m1.1d">|</annotation></semantics></math> - - - -</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.34.34.34.3">0.226 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.34.34.34.3.m1.1"><semantics id="S4.T4.34.34.34.3.m1.1a"><mo fence="false" id="S4.T4.34.34.34.3.m1.1.1" stretchy="false" xref="S4.T4.34.34.34.3.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.34.34.34.3.m1.1b"><ci id="S4.T4.34.34.34.3.m1.1.1.cmml" xref="S4.T4.34.34.34.3.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.34.34.34.3.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.34.34.34.3.m1.1d">|</annotation></semantics></math> - - - -</td>
<td class="ltx_td ltx_border_t" id="S4.T4.38.38.38.9"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.35.35.35.4">0.224 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.35.35.35.4.m1.1"><semantics id="S4.T4.35.35.35.4.m1.1a"><mo fence="false" id="S4.T4.35.35.35.4.m1.1.1" stretchy="false" xref="S4.T4.35.35.35.4.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.35.35.35.4.m1.1b"><ci id="S4.T4.35.35.35.4.m1.1.1.cmml" xref="S4.T4.35.35.35.4.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.35.35.35.4.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.35.35.35.4.m1.1d">|</annotation></semantics></math> - - - -</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.36.36.36.5">0.237 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.36.36.36.5.m1.1"><semantics id="S4.T4.36.36.36.5.m1.1a"><mo fence="false" id="S4.T4.36.36.36.5.m1.1.1" stretchy="false" xref="S4.T4.36.36.36.5.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.36.36.36.5.m1.1b"><ci id="S4.T4.36.36.36.5.m1.1.1.cmml" xref="S4.T4.36.36.36.5.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.36.36.36.5.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.36.36.36.5.m1.1d">|</annotation></semantics></math> - - - -</td>
<td class="ltx_td ltx_border_t" id="S4.T4.38.38.38.10"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.37.37.37.6">0.140 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.37.37.37.6.m1.1"><semantics id="S4.T4.37.37.37.6.m1.1a"><mo fence="false" id="S4.T4.37.37.37.6.m1.1.1" stretchy="false" xref="S4.T4.37.37.37.6.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.37.37.37.6.m1.1b"><ci id="S4.T4.37.37.37.6.m1.1.1.cmml" xref="S4.T4.37.37.37.6.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.37.37.37.6.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.37.37.37.6.m1.1d">|</annotation></semantics></math> - - - -</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.38.38.38.7">0.206 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.38.38.38.7.m1.1"><semantics id="S4.T4.38.38.38.7.m1.1a"><mo fence="false" id="S4.T4.38.38.38.7.m1.1.1" stretchy="false" xref="S4.T4.38.38.38.7.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.38.38.38.7.m1.1b"><ci id="S4.T4.38.38.38.7.m1.1.1.cmml" xref="S4.T4.38.38.38.7.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.38.38.38.7.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.38.38.38.7.m1.1d">|</annotation></semantics></math> - - - -</td>
</tr>
<tr class="ltx_tr" id="S4.T4.45.45.45">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.39.39.39.1">Llama-7B-Chat<sub class="ltx_sub" id="S4.T4.39.39.39.1.1"><span class="ltx_text ltx_font_italic" id="S4.T4.39.39.39.1.1.1">ft</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.40.40.40.2">0.215 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.40.40.40.2.m1.1"><semantics id="S4.T4.40.40.40.2.m1.1a"><mo fence="false" id="S4.T4.40.40.40.2.m1.1.1" stretchy="false" xref="S4.T4.40.40.40.2.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.40.40.40.2.m1.1b"><ci id="S4.T4.40.40.40.2.m1.1.1.cmml" xref="S4.T4.40.40.40.2.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.40.40.40.2.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.40.40.40.2.m1.1d">|</annotation></semantics></math> - - - -</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.41.41.41.3">0.239 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.41.41.41.3.m1.1"><semantics id="S4.T4.41.41.41.3.m1.1a"><mo fence="false" id="S4.T4.41.41.41.3.m1.1.1" stretchy="false" xref="S4.T4.41.41.41.3.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.41.41.41.3.m1.1b"><ci id="S4.T4.41.41.41.3.m1.1.1.cmml" xref="S4.T4.41.41.41.3.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.41.41.41.3.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.41.41.41.3.m1.1d">|</annotation></semantics></math> - - - -</td>
<td class="ltx_td ltx_border_bb" id="S4.T4.45.45.45.8"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.42.42.42.4">
<span class="ltx_text ltx_font_bold" id="S4.T4.42.42.42.4.1">0.265</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.42.42.42.4.m1.1"><semantics id="S4.T4.42.42.42.4.m1.1a"><mo fence="false" id="S4.T4.42.42.42.4.m1.1.1" stretchy="false" xref="S4.T4.42.42.42.4.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.42.42.42.4.m1.1b"><ci id="S4.T4.42.42.42.4.m1.1.1.cmml" xref="S4.T4.42.42.42.4.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.42.42.42.4.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.42.42.42.4.m1.1d">|</annotation></semantics></math> - - - -</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.43.43.43.5">0.261 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.43.43.43.5.m1.1"><semantics id="S4.T4.43.43.43.5.m1.1a"><mo fence="false" id="S4.T4.43.43.43.5.m1.1.1" stretchy="false" xref="S4.T4.43.43.43.5.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.43.43.43.5.m1.1b"><ci id="S4.T4.43.43.43.5.m1.1.1.cmml" xref="S4.T4.43.43.43.5.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.43.43.43.5.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.43.43.43.5.m1.1d">|</annotation></semantics></math> - - - -</td>
<td class="ltx_td ltx_border_bb" id="S4.T4.45.45.45.9"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.44.44.44.6">0.179 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.44.44.44.6.m1.1"><semantics id="S4.T4.44.44.44.6.m1.1a"><mo fence="false" id="S4.T4.44.44.44.6.m1.1.1" stretchy="false" xref="S4.T4.44.44.44.6.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.44.44.44.6.m1.1b"><ci id="S4.T4.44.44.44.6.m1.1.1.cmml" xref="S4.T4.44.44.44.6.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.44.44.44.6.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.44.44.44.6.m1.1d">|</annotation></semantics></math> - - - -</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.45.45.45.7">0.191 <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.45.45.45.7.m1.1"><semantics id="S4.T4.45.45.45.7.m1.1a"><mo fence="false" id="S4.T4.45.45.45.7.m1.1.1" stretchy="false" xref="S4.T4.45.45.45.7.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.45.45.45.7.m1.1b"><ci id="S4.T4.45.45.45.7.m1.1.1.cmml" xref="S4.T4.45.45.45.7.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.45.45.45.7.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.45.45.45.7.m1.1d">|</annotation></semantics></math> - - - -</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Model performance (<span class="ltx_text ltx_font_bold" id="S4.T4.47.1">data-sparse <math alttext="|" class="ltx_Math" display="inline" id="S4.T4.47.1.m1.1"><semantics id="S4.T4.47.1.m1.1b"><mo fence="false" id="S4.T4.47.1.m1.1.1" stretchy="false" xref="S4.T4.47.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T4.47.1.m1.1c"><ci id="S4.T4.47.1.m1.1.1.cmml" xref="S4.T4.47.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.47.1.m1.1d">|</annotation><annotation encoding="application/x-llamapun" id="S4.T4.47.1.m1.1e">|</annotation></semantics></math> data-rich</span>) on the propaganda techniques identification sub-task under cross-domain training. The <span class="ltx_text ltx_font_bold" id="S4.T4.50.2">best result</span> and <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.51.3">runner-up result</span> are highlighted per column for the data-sparse and data-rich settings, respectively. Diagonal cells show in-domain training only, without cross-domain training, and are included for reference.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T5.20" style="width:411.9pt;height:85.3pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-70.1pt,14.4pt) scale(0.746188211729939,0.746188211729939) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T5.20.20">
<tr class="ltx_tr" id="S4.T5.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.1.1">Eval Dataset <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T5.1.1.1.1.1.m1.1"><semantics id="S4.T5.1.1.1.1.1.m1.1a"><mo id="S4.T5.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T5.1.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.1.m1.1b"><ci id="S4.T5.1.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T5.1.1.1.1.1.m1.1d">→</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T5.1.1.1.2"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T5.1.1.1.2.1">RUWA-Gaze</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T5.1.1.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T5.1.1.1.4"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T5.1.1.1.4.1">Politifact-Gaze</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T5.1.1.1.5"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T5.1.1.1.6"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T5.1.1.1.6.1">PTC-Gaze</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2.2">
<td class="ltx_td ltx_align_center" id="S4.T5.2.2.2.1">
<span class="ltx_text ltx_font_bold" id="S4.T5.2.2.2.1.1">Additional Train Data</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T5.2.2.2.1.m1.1"><semantics id="S4.T5.2.2.2.1.m1.1a"><mo id="S4.T5.2.2.2.1.m1.1.1" stretchy="false" xref="S4.T5.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.1.m1.1b"><ci id="S4.T5.2.2.2.1.m1.1.1.cmml" xref="S4.T5.2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T5.2.2.2.1.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.2.2.2">
<span class="ltx_text" id="S4.T5.2.2.2.2.1"></span> <span class="ltx_text" id="S4.T5.2.2.2.2.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T5.2.2.2.2.2.1">
<span class="ltx_tr" id="S4.T5.2.2.2.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.2.2.2.2.2.1.1.1">Appeals</span></span>
<span class="ltx_tr" id="S4.T5.2.2.2.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.2.2.2.2.2.1.2.1">BertScore</span></span>
</span></span><span class="ltx_text" id="S4.T5.2.2.2.2.3"></span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.2.2.3">
<span class="ltx_text" id="S4.T5.2.2.2.3.1"></span> <span class="ltx_text" id="S4.T5.2.2.2.3.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T5.2.2.2.3.2.1">
<span class="ltx_tr" id="S4.T5.2.2.2.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.2.2.2.3.2.1.1.1">Intents</span></span>
<span class="ltx_tr" id="S4.T5.2.2.2.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.2.2.2.3.2.1.2.1">BertScore</span></span>
</span></span><span class="ltx_text" id="S4.T5.2.2.2.3.3"></span></td>
<td class="ltx_td" id="S4.T5.2.2.2.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.2.2.5">
<span class="ltx_text" id="S4.T5.2.2.2.5.1"></span> <span class="ltx_text" id="S4.T5.2.2.2.5.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T5.2.2.2.5.2.1">
<span class="ltx_tr" id="S4.T5.2.2.2.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.2.2.2.5.2.1.1.1">Appeals</span></span>
<span class="ltx_tr" id="S4.T5.2.2.2.5.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.2.2.2.5.2.1.2.1">BertScore</span></span>
</span></span><span class="ltx_text" id="S4.T5.2.2.2.5.3"></span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.2.2.6">
<span class="ltx_text" id="S4.T5.2.2.2.6.1"></span> <span class="ltx_text" id="S4.T5.2.2.2.6.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T5.2.2.2.6.2.1">
<span class="ltx_tr" id="S4.T5.2.2.2.6.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.2.2.2.6.2.1.1.1">Intents</span></span>
<span class="ltx_tr" id="S4.T5.2.2.2.6.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.2.2.2.6.2.1.2.1">BertScore</span></span>
</span></span><span class="ltx_text" id="S4.T5.2.2.2.6.3"></span></td>
<td class="ltx_td" id="S4.T5.2.2.2.7"></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.2.2.8">
<span class="ltx_text" id="S4.T5.2.2.2.8.1"></span> <span class="ltx_text" id="S4.T5.2.2.2.8.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T5.2.2.2.8.2.1">
<span class="ltx_tr" id="S4.T5.2.2.2.8.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.2.2.2.8.2.1.1.1">Appeals</span></span>
<span class="ltx_tr" id="S4.T5.2.2.2.8.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.2.2.2.8.2.1.2.1">BertScore</span></span>
</span></span><span class="ltx_text" id="S4.T5.2.2.2.8.3"></span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.2.2.9">
<span class="ltx_text" id="S4.T5.2.2.2.9.1"></span> <span class="ltx_text" id="S4.T5.2.2.2.9.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T5.2.2.2.9.2.1">
<span class="ltx_tr" id="S4.T5.2.2.2.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.2.2.2.9.2.1.1.1">Intents</span></span>
<span class="ltx_tr" id="S4.T5.2.2.2.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.2.2.2.9.2.1.2.1">BertScore</span></span>
</span></span><span class="ltx_text" id="S4.T5.2.2.2.9.3"></span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.8.8.8">
<td class="ltx_td ltx_align_center" id="S4.T5.8.8.8.7"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T5.8.8.8.7.1">RUWA-Gaze</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.3.3.1">0.313 <math alttext="|" class="ltx_Math" display="inline" id="S4.T5.3.3.3.1.m1.1"><semantics id="S4.T5.3.3.3.1.m1.1a"><mo fence="false" id="S4.T5.3.3.3.1.m1.1.1" stretchy="false" xref="S4.T5.3.3.3.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.3.1.m1.1b"><ci id="S4.T5.3.3.3.1.m1.1.1.cmml" xref="S4.T5.3.3.3.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.3.1.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T5.3.3.3.1.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S4.T5.3.3.3.1.1">0.612</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.4.4.4.2">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.4.4.4.2.1">0.851</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T5.4.4.4.2.m1.1"><semantics id="S4.T5.4.4.4.2.m1.1a"><mo fence="false" id="S4.T5.4.4.4.2.m1.1.1" stretchy="false" xref="S4.T5.4.4.4.2.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T5.4.4.4.2.m1.1b"><ci id="S4.T5.4.4.4.2.m1.1.1.cmml" xref="S4.T5.4.4.4.2.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.4.4.4.2.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T5.4.4.4.2.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S4.T5.4.4.4.2.2">0.861</span>
</td>
<td class="ltx_td" id="S4.T5.8.8.8.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.5.5.5.3">
<span class="ltx_text ltx_font_bold" id="S4.T5.5.5.5.3.1">0.362</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T5.5.5.5.3.m1.1"><semantics id="S4.T5.5.5.5.3.m1.1a"><mo fence="false" id="S4.T5.5.5.5.3.m1.1.1" stretchy="false" xref="S4.T5.5.5.5.3.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T5.5.5.5.3.m1.1b"><ci id="S4.T5.5.5.5.3.m1.1.1.cmml" xref="S4.T5.5.5.5.3.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.5.5.5.3.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T5.5.5.5.3.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.5.5.5.3.2">0.452</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.6.6.6.4">0.858 <math alttext="|" class="ltx_Math" display="inline" id="S4.T5.6.6.6.4.m1.1"><semantics id="S4.T5.6.6.6.4.m1.1a"><mo fence="false" id="S4.T5.6.6.6.4.m1.1.1" stretchy="false" xref="S4.T5.6.6.6.4.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T5.6.6.6.4.m1.1b"><ci id="S4.T5.6.6.6.4.m1.1.1.cmml" xref="S4.T5.6.6.6.4.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.6.6.6.4.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T5.6.6.6.4.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S4.T5.6.6.6.4.1">0.865</span>
</td>
<td class="ltx_td" id="S4.T5.8.8.8.9"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.7.7.7.5">
<span class="ltx_text ltx_font_bold" id="S4.T5.7.7.7.5.1">0.293</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T5.7.7.7.5.m1.1"><semantics id="S4.T5.7.7.7.5.m1.1a"><mo fence="false" id="S4.T5.7.7.7.5.m1.1.1" stretchy="false" xref="S4.T5.7.7.7.5.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T5.7.7.7.5.m1.1b"><ci id="S4.T5.7.7.7.5.m1.1.1.cmml" xref="S4.T5.7.7.7.5.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.7.7.7.5.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T5.7.7.7.5.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S4.T5.7.7.7.5.2">0.352</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.8.8.8.6">0.839 <math alttext="|" class="ltx_Math" display="inline" id="S4.T5.8.8.8.6.m1.1"><semantics id="S4.T5.8.8.8.6.m1.1a"><mo fence="false" id="S4.T5.8.8.8.6.m1.1.1" stretchy="false" xref="S4.T5.8.8.8.6.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T5.8.8.8.6.m1.1b"><ci id="S4.T5.8.8.8.6.m1.1.1.cmml" xref="S4.T5.8.8.8.6.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.8.8.8.6.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T5.8.8.8.6.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.8.8.8.6.1">0.841</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T5.14.14.14">
<td class="ltx_td ltx_align_center" id="S4.T5.14.14.14.7"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T5.14.14.14.7.1">Politifact-Gaze</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.9.9.9.1">
<span class="ltx_text ltx_font_bold" id="S4.T5.9.9.9.1.1">0.373</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T5.9.9.9.1.m1.1"><semantics id="S4.T5.9.9.9.1.m1.1a"><mo fence="false" id="S4.T5.9.9.9.1.m1.1.1" stretchy="false" xref="S4.T5.9.9.9.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T5.9.9.9.1.m1.1b"><ci id="S4.T5.9.9.9.1.m1.1.1.cmml" xref="S4.T5.9.9.9.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.9.9.9.1.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T5.9.9.9.1.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.9.9.9.1.2">0.584</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T5.10.10.10.2">
<span class="ltx_text ltx_font_bold" id="S4.T5.10.10.10.2.1">0.855</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T5.10.10.10.2.m1.1"><semantics id="S4.T5.10.10.10.2.m1.1a"><mo fence="false" id="S4.T5.10.10.10.2.m1.1.1" stretchy="false" xref="S4.T5.10.10.10.2.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T5.10.10.10.2.m1.1b"><ci id="S4.T5.10.10.10.2.m1.1.1.cmml" xref="S4.T5.10.10.10.2.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.10.10.10.2.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T5.10.10.10.2.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.10.10.10.2.2">0.860</span>
</td>
<td class="ltx_td" id="S4.T5.14.14.14.8"></td>
<td class="ltx_td ltx_align_center" id="S4.T5.11.11.11.3">0.342 <math alttext="|" class="ltx_Math" display="inline" id="S4.T5.11.11.11.3.m1.1"><semantics id="S4.T5.11.11.11.3.m1.1a"><mo fence="false" id="S4.T5.11.11.11.3.m1.1.1" stretchy="false" xref="S4.T5.11.11.11.3.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T5.11.11.11.3.m1.1b"><ci id="S4.T5.11.11.11.3.m1.1.1.cmml" xref="S4.T5.11.11.11.3.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.11.11.11.3.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T5.11.11.11.3.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S4.T5.11.11.11.3.1">0.495</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T5.12.12.12.4">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.12.12.12.4.1">0.860</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T5.12.12.12.4.m1.1"><semantics id="S4.T5.12.12.12.4.m1.1a"><mo fence="false" id="S4.T5.12.12.12.4.m1.1.1" stretchy="false" xref="S4.T5.12.12.12.4.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T5.12.12.12.4.m1.1b"><ci id="S4.T5.12.12.12.4.m1.1.1.cmml" xref="S4.T5.12.12.12.4.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.12.12.12.4.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T5.12.12.12.4.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.12.12.12.4.2">0.864</span>
</td>
<td class="ltx_td" id="S4.T5.14.14.14.9"></td>
<td class="ltx_td ltx_align_center" id="S4.T5.13.13.13.5">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.13.13.13.5.1">0.267</span><math alttext="|" class="ltx_Math" display="inline" id="S4.T5.13.13.13.5.m1.1"><semantics id="S4.T5.13.13.13.5.m1.1a"><mo fence="false" id="S4.T5.13.13.13.5.m1.1.1" stretchy="false" xref="S4.T5.13.13.13.5.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T5.13.13.13.5.m1.1b"><ci id="S4.T5.13.13.13.5.m1.1.1.cmml" xref="S4.T5.13.13.13.5.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.13.13.13.5.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T5.13.13.13.5.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.13.13.13.5.2">0.310</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T5.14.14.14.6">
<span class="ltx_text ltx_font_bold" id="S4.T5.14.14.14.6.1">0.845</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T5.14.14.14.6.m1.1"><semantics id="S4.T5.14.14.14.6.m1.1a"><mo fence="false" id="S4.T5.14.14.14.6.m1.1.1" stretchy="false" xref="S4.T5.14.14.14.6.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T5.14.14.14.6.m1.1b"><ci id="S4.T5.14.14.14.6.m1.1.1.cmml" xref="S4.T5.14.14.14.6.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.14.14.14.6.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T5.14.14.14.6.m1.1d">|</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S4.T5.14.14.14.6.2">0.847</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T5.20.20.20">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.20.20.20.7"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T5.20.20.20.7.1">PTC-Gaze</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.15.15.15.1">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.15.15.15.1.1">0.366</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T5.15.15.15.1.m1.1"><semantics id="S4.T5.15.15.15.1.m1.1a"><mo fence="false" id="S4.T5.15.15.15.1.m1.1.1" stretchy="false" xref="S4.T5.15.15.15.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T5.15.15.15.1.m1.1b"><ci id="S4.T5.15.15.15.1.m1.1.1.cmml" xref="S4.T5.15.15.15.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.15.15.15.1.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T5.15.15.15.1.m1.1d">|</annotation></semantics></math> - - - -</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.16.16.16.2">0.848 <math alttext="|" class="ltx_Math" display="inline" id="S4.T5.16.16.16.2.m1.1"><semantics id="S4.T5.16.16.16.2.m1.1a"><mo fence="false" id="S4.T5.16.16.16.2.m1.1.1" stretchy="false" xref="S4.T5.16.16.16.2.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T5.16.16.16.2.m1.1b"><ci id="S4.T5.16.16.16.2.m1.1.1.cmml" xref="S4.T5.16.16.16.2.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.16.16.16.2.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T5.16.16.16.2.m1.1d">|</annotation></semantics></math> - - - -</td>
<td class="ltx_td ltx_border_bb" id="S4.T5.20.20.20.8"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.17.17.17.3">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.17.17.17.3.1">0.350</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T5.17.17.17.3.m1.1"><semantics id="S4.T5.17.17.17.3.m1.1a"><mo fence="false" id="S4.T5.17.17.17.3.m1.1.1" stretchy="false" xref="S4.T5.17.17.17.3.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T5.17.17.17.3.m1.1b"><ci id="S4.T5.17.17.17.3.m1.1.1.cmml" xref="S4.T5.17.17.17.3.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.17.17.17.3.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T5.17.17.17.3.m1.1d">|</annotation></semantics></math> - - - -</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.18.18.18.4">
<span class="ltx_text ltx_font_bold" id="S4.T5.18.18.18.4.1">0.863</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T5.18.18.18.4.m1.1"><semantics id="S4.T5.18.18.18.4.m1.1a"><mo fence="false" id="S4.T5.18.18.18.4.m1.1.1" stretchy="false" xref="S4.T5.18.18.18.4.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T5.18.18.18.4.m1.1b"><ci id="S4.T5.18.18.18.4.m1.1.1.cmml" xref="S4.T5.18.18.18.4.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.18.18.18.4.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T5.18.18.18.4.m1.1d">|</annotation></semantics></math> - - - -</td>
<td class="ltx_td ltx_border_bb" id="S4.T5.20.20.20.9"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.19.19.19.5">0.249 <math alttext="|" class="ltx_Math" display="inline" id="S4.T5.19.19.19.5.m1.1"><semantics id="S4.T5.19.19.19.5.m1.1a"><mo fence="false" id="S4.T5.19.19.19.5.m1.1.1" stretchy="false" xref="S4.T5.19.19.19.5.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T5.19.19.19.5.m1.1b"><ci id="S4.T5.19.19.19.5.m1.1.1.cmml" xref="S4.T5.19.19.19.5.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.19.19.19.5.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T5.19.19.19.5.m1.1d">|</annotation></semantics></math> - - - -</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.20.20.20.6">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.20.20.20.6.1">0.843</span> <math alttext="|" class="ltx_Math" display="inline" id="S4.T5.20.20.20.6.m1.1"><semantics id="S4.T5.20.20.20.6.m1.1a"><mo fence="false" id="S4.T5.20.20.20.6.m1.1.1" stretchy="false" xref="S4.T5.20.20.20.6.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T5.20.20.20.6.m1.1b"><ci id="S4.T5.20.20.20.6.m1.1.1.cmml" xref="S4.T5.20.20.20.6.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.20.20.20.6.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.T5.20.20.20.6.m1.1d">|</annotation></semantics></math> - - - -</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Fine-tuned Llama-7B-Chat model performance (<span class="ltx_text ltx_font_bold" id="S4.T5.22.1">data-sparse <math alttext="|" class="ltx_Math" display="inline" id="S4.T5.22.1.m1.1"><semantics id="S4.T5.22.1.m1.1b"><mo fence="false" id="S4.T5.22.1.m1.1.1" stretchy="false" xref="S4.T5.22.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T5.22.1.m1.1c"><ci id="S4.T5.22.1.m1.1.1.cmml" xref="S4.T5.22.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.22.1.m1.1d">|</annotation><annotation encoding="application/x-llamapun" id="S4.T5.22.1.m1.1e">|</annotation></semantics></math> data-rich</span>) on the appeals and intent analysis sub-tasks under cross-domain training. The <span class="ltx_text ltx_font_bold" id="S4.T5.25.2">best result</span> and <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T5.26.3">runner-up result</span> are highlighted per column for the data-sparse and data-rich settings, respectively. Diagonal cells show in-domain training only, without cross-domain training, and are included for reference.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Is Propaganda Analysis Transferable Across Domains?</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">In the real world, propaganda spans various domains, including military and war, politics, economics, science, environmental issues, and more. Although the specific use of propaganda may differ across these domains, we are particularly interested in determining whether the general patterns of propaganda are transferable between domains. Additionally, high-quality human-annotated data is scarce, prompting us to investigate whether leveraging data from other domains can improve propaganda analysis in a target domain.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">As outlined in § <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S3" title="3 PropaGaze: A Dataset for Systematically Analyzing Propaganda ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">3</span></a>, our dataset consists of three subsets: <span class="ltx_text ltx_font_italic" id="S4.SS4.p2.1.1">RUWA-Gaze</span> (military and war), <span class="ltx_text ltx_font_italic" id="S4.SS4.p2.1.2">Politifact-Gaze</span> (politics), and <span class="ltx_text ltx_font_italic" id="S4.SS4.p2.1.3">PTC-Gaze</span> (general news). To explore cross-domain transferability, we perform additional training on each target sub-dataset using data from the other two sub-datasets after the in-domain training. For instance, in a data-sparse scenario, if <span class="ltx_text ltx_font_italic" id="S4.SS4.p2.1.4">RUWA-Gaze</span> is the target, cross-domain training on <span class="ltx_text ltx_font_italic" id="S4.SS4.p2.1.5">Politifact-Gaze</span> involves first training the model on the sparse <span class="ltx_text ltx_font_italic" id="S4.SS4.p2.1.6">RUWA-Gaze</span> data, followed by further training with sparse <span class="ltx_text ltx_font_italic" id="S4.SS4.p2.1.7">Politifact-Gaze</span> data. In a data-rich scenario, the model is trained on the full in-domain <span class="ltx_text ltx_font_italic" id="S4.SS4.p2.1.8">RUWA-Gaze</span> data, then further trained on the entire <span class="ltx_text ltx_font_italic" id="S4.SS4.p2.1.9">Politifact-Gaze</span> dataset. The results are presented in Tables <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S4.T4" title="Table 4 ‣ 4.3 How Much Does PropaGaze Enhance Model Performance? ‣ 4 Experiments ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S4.T5" title="Table 5 ‣ 4.3 How Much Does PropaGaze Enhance Model Performance? ‣ 4 Experiments ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">In data-sparse settings, we observe that models benefit substantially from incorporating cross-domain data. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S4.T4" title="Table 4 ‣ 4.3 How Much Does PropaGaze Enhance Model Performance? ‣ 4 Experiments ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">4</span></a>, when evaluated on <span class="ltx_text ltx_font_italic" id="S4.SS4.p3.1.1">RUWA-Gaze</span>, models trained on additional data from <span class="ltx_text ltx_font_italic" id="S4.SS4.p3.1.2">Politifact-Gaze</span> and <span class="ltx_text ltx_font_italic" id="S4.SS4.p3.1.3">PTC-Gaze</span> achieve higher performance than those trained solely on sparse in-domain data. Specifically, LLaMA-7B-Chat fine-tuned with additional <span class="ltx_text ltx_font_italic" id="S4.SS4.p3.1.4">Politifact-Gaze</span> data achieves the highest text span IoU of 0.271, while MGNN trained with additional <span class="ltx_text ltx_font_italic" id="S4.SS4.p3.1.5">Politifact-Gaze</span> data reaches the highest technique F1 score of 0.281. This pattern is consistent across other sub-datasets and holds true for appeal analysis as well, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S4.T5" title="Table 5 ‣ 4.3 How Much Does PropaGaze Enhance Model Performance? ‣ 4 Experiments ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">5</span></a>. This is expected, as models trained in data-sparse conditions tend to benefit from cross-domain data due to the need for a larger pool of training examples. Access to additional data from related domains enables models to learn generalized patterns of propaganda usage more effectively, leading to improved performance even on tasks outside of their original training domain.</p>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1">However, in data-rich scenarios, the benefit of cross-domain training diminishes. For example, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S4.T4" title="Table 4 ‣ 4.3 How Much Does PropaGaze Enhance Model Performance? ‣ 4 Experiments ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">4</span></a>, models trained on additional <span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.1">Politifact-Gaze</span> data underperform those trained solely on in-domain data when evaluated on <span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.2">RUWA-Gaze</span>. Similarly, when evaluated on <span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.3">Politifact-Gaze</span>, adding <span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.4">RUWA-Gaze</span> data sometimes leads to performance improvements, but the gains are relatively small. This holds for appeal analysis as well, as we can see in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S4.T5" title="Table 5 ‣ 4.3 How Much Does PropaGaze Enhance Model Performance? ‣ 4 Experiments ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">5</span></a>. These results suggest that <span class="ltx_text ltx_font_bold" id="S4.SS4.p4.1.5">when there is sufficient training data, the quality of the data has a greater impact on performance than its quantity</span>. We further observe that training on both <span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.6">RUWA-Gaze</span> and <span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.7">Politifact-Gaze</span> improves the performance on the human-annotated <span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.8">PTC-Gaze</span> across all sub-tasks. While this is partly due to the data-sparse nature of <span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.9">PTC-Gaze</span>, making extra training samples valuable, it also highlights that our synthetic data effectively complements the limited human-annotated data.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Discrepancy between Human-Annotated and Synthetic Datasets</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We acknowledge the discrepancy between the synthetic sub-datasets and the human-annotated sub-dataset in <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">PropaGaze</span>. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S3.T1" title="Table 1 ‣ 3.1 PTC-Gaze: Human-Annotated Dataset ‣ 3 PropaGaze: A Dataset for Systematically Analyzing Propaganda ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">1</span></a>, the average number of propaganda techniques per article in <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.2">PTC-Gaze</span> is 12.77, which is about 3.7 times higher than in the synthetic <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.3">RUWA-Gaze</span> and <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.4">Politifact-Gaze</span>. This occurs due to the way we generate the synthetic data, where we inject three propaganda techniques per article, with GPT-4-Turbo sometimes reusing techniques. However, we believe this is less of an issue, as <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.5">PTC-Gaze</span> articles are on average 3.3 times longer than those in the other sub-datasets. Moreover, since we treat the injected techniques as silver labels, we have not yet checked whether other sentences in the articles also use propaganda techniques. See the Limitations section for more details. Finally, we note the inherent difference in writing styles between synthetic and human-written articles, which is a common challenge with synthetic datasets.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Further Challenges of Propaganda Analysis</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We identified that accurately pinpointing the occurrence of propaganda is a major challenge in propaganda analysis. As highlighted in the case study (Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A6" title="Appendix F Case Study: Bottleneck of Propaganda Analysis ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">F</span></a>), LLMs often misclassify non-propagandistic sentences as propagandistic, leading to a high false positives rate. This issue may be partially attributed to hallucination or failing to account for subtle contextual differences. Although less frequent, similar errors occur with MGNN, indicating that the problem lies not only in the models themselves, but also in the training methodologies and the underlying algorithms. This underscores the need for improvements in both model development and in the training approaches to better distinguish propagandistic content from neutral text.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related Work</h2>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Propaganda Detection</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p1.1">Propaganda detection has long been a focus in both Natural Language Processing, with most work focusing on identifying propaganda usage and specific techniques. Various learning-based approaches have improved performance <cite class="ltx_cite ltx_citemacro_cite">Da San Martino et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib10" title="">2019</a>); Yoosuf and Yang (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib66" title="">2019</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib32" title="">2019</a>); Vorakitphan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib57" title="">2022</a>)</cite> and interpretability <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib67" title="">2021</a>)</cite> in detecting propaganda in news articles <cite class="ltx_cite ltx_citemacro_cite">Vlad et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib56" title="">2019</a>); Da San Martino et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib10" title="">2019</a>); Gupta et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib22" title="">2019</a>); Yu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib67" title="">2021</a>)</cite> and tweets <cite class="ltx_cite ltx_citemacro_cite">Vijayaraghavan and Vosoughi (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib55" title="">2022</a>); Khanday et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib31" title="">2021</a>); Guarino et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib19" title="">2020</a>)</cite>. Recent efforts have also applied LLMs to this task <cite class="ltx_cite ltx_citemacro_cite">Sprenkamp et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib48" title="">2023</a>); Jones (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib27" title="">2024</a>)</cite>. While these studies focus on identifying propaganda techniques, further research is needed to understand the appeals and intent behind them.</p>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p2.1">Following the escalation of the Russo-Ukrainian conflict in 2022, research has focused on analyzing propaganda campaigns, particularly from Russia. <cite class="ltx_cite ltx_citemacro_citet">Chen and Ferrara (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib8" title="">2023</a>); Fung and Ji (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib15" title="">2022</a>)</cite> collected user content and opinions from social media platforms such as X and Weibo, while <cite class="ltx_cite ltx_citemacro_citet">Golovchenko (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib18" title="">2022</a>)</cite> examined censorship of Ukrainian content on Russian platforms. <cite class="ltx_cite ltx_citemacro_citet">Geissler et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib16" title="">2023</a>)</cite> studied pro-Russian sentiment on social media and the role of bots, and <cite class="ltx_cite ltx_citemacro_citet">Patrona (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib44" title="">2022</a>)</cite> explored intertextuality and rhetoric in political performances during the war. However, few studies developed frameworks to analyze the specific intent behind propagandistic efforts. <cite class="ltx_cite ltx_citemacro_citet">Ai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib3" title="">2024</a>)</cite> examined two specific propaganda narrative intentions, but failed short of proposing a generalizable framework for propaganda analysis.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Propaganda Generation</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p1.1">Compared to propaganda detection, research on propaganda generation is sparse. <cite class="ltx_cite ltx_citemacro_citet">Zellers et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib68" title="">2019</a>)</cite> explores generating propaganda to spread targeted disinformation, while <cite class="ltx_cite ltx_citemacro_citet">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib26" title="">2023</a>)</cite> focuses on incorporating emotional and non-emotional propaganda techniques into generated articles. <cite class="ltx_cite ltx_citemacro_citet">Goldstein et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib17" title="">2024</a>)</cite> find that GPT-3 can generate highly persuasive propaganda. Our data generation pipeline goes further by allowing a broader range of propaganda techniques to be inserted into generated articles to evoke specific intent, while allowing for more granular analysis of the appeals behind their use.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">User Intent Detection</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px3.p1.1">Previous methods on intent detection concentrated primarily on understanding user queries in human-machine dialogue systems <cite class="ltx_cite ltx_citemacro_cite">Brenes et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib6" title="">2009</a>); Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib33" title="">2019</a>); Weld et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib63" title="">2022</a>)</cite>. This research facilitated the construction of more robust search engines and virtual assistants. The similarity of this task to ours is that both tasks require strong natural language understanding. However, detecting user query intent is relatively superficial compared to the intent behind a propaganda tactic, which could be highly concealed and hard to recognize <cite class="ltx_cite ltx_citemacro_cite">Jowett and O’Donnell (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib29" title="">2012</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion and Future Work</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">We proposed a comprehensive approach to propaganda analysis that goes beyond simply identifying techniques and addresses the common challenge of obtaining high-quality human-annotated data. We further introduced <span class="ltx_text ltx_font_italic" id="S7.p1.1.1">PropaInsight</span>, a conceptual framework for granular propaganda analysis that identifies propaganda techniques, arousal appeals, and underlying intent, grounded in foundational social science research. Moreover, we presented <span class="ltx_text ltx_font_italic" id="S7.p1.1.2">PropaGaze</span>, a novel dataset for fine-grained propaganda analysis that includes both human-annotated and high-quality synthetic sub-datasets. Our experiments showed that models fine-tuned on <span class="ltx_text ltx_font_italic" id="S7.p1.1.3">PropaGaze</span> outperform one-shot GPT-4-Turbo by a margin. <span class="ltx_text ltx_font_italic" id="S7.p1.1.4">PropaGaze</span> proved highly beneficial in data-sparse and cross-domain scenarios, serving as an effective complement to limited human-annotated data.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">Furthermore, <span class="ltx_text ltx_font_italic" id="S7.p2.1.1">PropaInsight</span> has broader implications beyond propaganda analysis. It enhances tasks such as disinformation detection <cite class="ltx_cite ltx_citemacro_cite">Guess and Lyons (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib20" title="">2020</a>); Ai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib2" title="">2021</a>); Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib26" title="">2023</a>)</cite>, sentiment analysis <cite class="ltx_cite ltx_citemacro_cite">Ahmad et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib1" title="">2019</a>)</cite>, narrative framing <cite class="ltx_cite ltx_citemacro_cite">Colley (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib9" title="">2019</a>); Andersen and Sandberg (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib5" title="">2020</a>)</cite>, media bias analysis <cite class="ltx_cite ltx_citemacro_cite">Nakov and Da San Martino (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib40" title="">2021</a>); Zollmann (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib70" title="">2019</a>)</cite>, and social media monitoring <cite class="ltx_cite ltx_citemacro_cite">Chaudhari and Pawar (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib7" title="">2021</a>)</cite>, offering deeper insights into manipulative content and coordinated disinformation campaigns, making the framework applicable to a wide range of areas.In the future, we plan to expand <span class="ltx_text ltx_font_italic" id="S7.p2.1.2">PropaGaze</span> into more diverse domains and genres, which will further broaden the scope of propaganda analysis. We will also explore how <span class="ltx_text ltx_font_italic" id="S7.p2.1.3">PropaInsight</span> can improve downstream applications and contribute to a deeper understanding of propaganda.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We reflect on the limitations of our work below:</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<ol class="ltx_enumerate" id="Sx1.I1">
<li class="ltx_item" id="Sx1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="Sx1.I1.i1.p1">
<p class="ltx_p" id="Sx1.I1.i1.p1.1">Although our dataset PropaGaze is novel and reliable, its size is relatively small due to the computational costs associated with GPT-4 and the high expense of human annotation. Consequently, we are uncertain about the dataset’s ability to generalize across a broad range of domains when models are fine-tuned exclusively on it.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Sx1.I1.i2.p1">
<p class="ltx_p" id="Sx1.I1.i2.p1.1">While we aimed to include diverse domains and construct a cross-domain dataset, the vast range of real-world scenarios exceeds what we could capture. The extent of the domain gap where propaganda thrives remains unclear, and therefore, the cross-domain performance we tested across our paper might not generalize well under varied conditions.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="Sx1.I1.i3.p1">
<p class="ltx_p" id="Sx1.I1.i3.p1.1">Despite our careful calibration of the proposed propaganda framework, the real-world responses such as reader engagement and ultimate impact can vary significantly. Personalized appeals may emerge, influencing the effectiveness of propaganda; however, our study did not account for these individual differences. We did not take this into consideration and leave this part for future work.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="Sx1.I1.i4.p1">
<p class="ltx_p" id="Sx1.I1.i4.p1.1">Although we use a partially controlled pipeline to generate synthetic data and have basic human reviewers skim the content to ensure its overall quality, a more fine-grained review is necessary. Specifically, we need to assess whether the sampled propaganda techniques are contextually appropriate within each article. Additionally, while we treat the injected techniques as silver labels for our experiments, we do not check whether other sentences in the article, beyond those explicitly marked, also employ propaganda techniques. This means that our synthetic sub-datasets have high precision in labeled techniques but have not been evaluated for recall. It is likely that in reshaping the articles, additional sentences may also use propaganda techniques not explicitly labeled. Further evaluation, potentially involving more comprehensive human annotation, is needed for a more granular assessment of the dataset’s quality.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">Based on this, we propose several promising future directions to further push for the success of combating misinformation.</p>
</div>
<div class="ltx_para" id="Sx1.p4">
<ol class="ltx_enumerate" id="Sx1.I2">
<li class="ltx_item" id="Sx1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="Sx1.I2.i1.p1">
<p class="ltx_p" id="Sx1.I2.i1.p1.1">A larger dataset developed using our propaganda framework could be constructed to further evaluate how synthetic data enhances the misinformation detection capabilities of large language models.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Sx1.I2.i2.p1">
<p class="ltx_p" id="Sx1.I2.i2.p1.1">Collect data from various domains where significant domain gaps typically exist, and investigate whether cross-domain data substantially influence the accuracy of models’ understanding and detection capabilities.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="Sx1.I2.i3.p1">
<p class="ltx_p" id="Sx1.I2.i3.p1.1">Consider personalized responses <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib50" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib51" title="">b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib52" title="">2024</a>)</cite> to propaganda. Conduct an in-depth analysis of how propaganda articles truly affect their readers and explore how these effects differ from the author’s original perspective. In terms of solutions, a mixture of experts can probably lead to a better result. It is also worth considering solutions with LLM-based agents <cite class="ltx_cite ltx_citemacro_cite">Guo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib21" title="">2024</a>)</cite>, which typically include multi-round of planning <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib34" title="">2023</a>)</cite> and interactions <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib62" title="">2024b</a>)</cite> or code execution <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib61" title="">2023</a>); Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib64" title="">2024</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib60" title="">2024a</a>)</cite> to work out a reasoning based solution.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="Sx1.I2.i4.p1">
<p class="ltx_p" id="Sx1.I2.i4.p1.1">Enhancing and shaping the knowledge of LLMs may help to cultivate a better understanding of propaganda articles from different perspectives. It is promising to combine perspective-based datasets to state-of-the-art knowledge control approaches <cite class="ltx_cite ltx_citemacro_cite">Han et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib23" title="">2024</a>); Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib35" title="">2024</a>); Deng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib12" title="">2024</a>)</cite> to alter the perspective and standpoints of LLMs.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="Sx1.I2.i5.p1">
<p class="ltx_p" id="Sx1.I2.i5.p1.1">It would also be valuable to explore incorporating this deeper understanding of propaganda techniques, appeals, and intent to enhance situation understanding and improve the comprehensiveness of situation report generation <cite class="ltx_cite ltx_citemacro_cite">Reddy et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib45" title="">2024</a>)</cite>.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Ethical Considerations</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">Our paper introduces a pipeline capable of generating news articles with strong intent and the potential for propaganda use. While our primary goal is to leverage this synthetic data to combat misinformation, it is important to acknowledge that the same technology could be misused to produce high-quality deceptive news content that could mislead public opinion. Consequently, the use of this pipeline must be approached with caution and safeguarded to prevent exploitation by malicious actors.</p>
</div>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">This research was done with funding from the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR001120C0123. The views, opinions, and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government. We also thank Kitware.Inc. and Rapidata.Inc. for their help in the data annotation process.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahmad et al. (2019)</span>
<span class="ltx_bibblock">
Siti Rohaidah Ahmad, Muhammad Zakwan Muhammad Rodzi, Nurlaila Syafira Shapiei, Nurhafizah Moziyana Mohd Yusop, and Suhaila Ismail. 2019.

</span>
<span class="ltx_bibblock">A review of feature selection and sentiment analysis technique in issues of propaganda.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">International Journal of Advanced Computer Science and Applications</em>, 10(11).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ai et al. (2021)</span>
<span class="ltx_bibblock">
Lin Ai, Run Chen, Ziwei Gong, Julia Guo, Shayan Hooshmand, Zixiaofan Yang, and Julia Hirschberg. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.36190/2021.31" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1.1">Exploring New Methods for Identifying False Information and the Intent Behind It on Social Media: COVID-19 Tweets</em></a>.

</span>
<span class="ltx_bibblock">ICWSM, virtual.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ai et al. (2024)</span>
<span class="ltx_bibblock">
Lin Ai, Sameer Gupta, Shreya Oak, Zheng Hui, Zizhou Liu, and Julia Hirschberg. 2024.

</span>
<span class="ltx_bibblock">Tweetintent@ crisis: A dataset revealing narratives of both sides in the russia-ukraine crisis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the International AAAI Conference on Web and Social Media</em>, volume 18, pages 1872–1887.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alhindi et al. (2024)</span>
<span class="ltx_bibblock">
Tariq Alhindi, Smaranda Muresan, and Preslav Nakov. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2024.findings-acl.732" title="">Large language models are few-shot training example generators: A case study in fallacy recognition</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Findings of the Association for Computational Linguistics ACL 2024</em>, pages 12323–12334, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andersen and Sandberg (2020)</span>
<span class="ltx_bibblock">
Jan Christoffer Andersen and Sveinung Sandberg. 2020.

</span>
<span class="ltx_bibblock">Islamic state propaganda: Between social movement framing and subcultural provocation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Terrorism and Political Violence</em>, 32(7):1506–1526.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brenes et al. (2009)</span>
<span class="ltx_bibblock">
David J Brenes, Daniel Gayo-Avello, and Kilian Pérez-González. 2009.

</span>
<span class="ltx_bibblock">Survey and evaluation of query intent detection methods.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 2009 Workshop on Web Search Click Data</em>, pages 1–7.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chaudhari and Pawar (2021)</span>
<span class="ltx_bibblock">
Deptii Devendra Chaudhari and Ambika Vishal Pawar. 2021.

</span>
<span class="ltx_bibblock">Propaganda analysis in social media: a bibliometric review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Information Discovery and Delivery</em>, 49(1):57–70.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Ferrara (2023)</span>
<span class="ltx_bibblock">
Emily Chen and Emilio Ferrara. 2023.

</span>
<span class="ltx_bibblock">Tweets in time of conflict: A public dataset tracking the twitter discourse on the war between ukraine and russia.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the International AAAI Conference on Web and social media</em>, volume 17, pages 1006–1013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Colley (2019)</span>
<span class="ltx_bibblock">
Thomas Colley. 2019.

</span>
<span class="ltx_bibblock">Strategic narratives and war propaganda.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">The SAGE Handbook of propaganda</em>, pages 38–54.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Da San Martino et al. (2019)</span>
<span class="ltx_bibblock">
Giovanni Da San Martino, Yu Seunghak, Alberto Barrón-Cedeno, Rostislav Petrov, Preslav Nakov, et al. 2019.

</span>
<span class="ltx_bibblock">Fine-grained analysis of propaganda in news article.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</em>, pages 5636–5646. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Da San Martino et al. (2020)</span>
<span class="ltx_bibblock">
Giovanni Da San Martino, Shaden Shaar, Yf Zhang, Yu Sh, Alberto Barrón-Cedeno, Preslav Nakov, et al. 2020.

</span>
<span class="ltx_bibblock">Prta: A system to support the analysis of propaganda techniques in the news.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</em>, pages 287–293. ASSOC COMPUTATIONAL LINGUISTICS-ACL.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2024)</span>
<span class="ltx_bibblock">
Jingcheng Deng, Zihao Wei, Liang Pang, Hanxing Ding, Huawei Shen, and Xueqi Cheng. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2405.15349" title="">Unke: Unstructured knowledge editing in large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Preprint</em>, arXiv:2405.15349.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diao et al. (2023)</span>
<span class="ltx_bibblock">
Shizhe Diao, Rui Pan, Hanze Dong, Ka Shun Shum, Jipeng Zhang, Wei Xiong, and Tong Zhang. 2023.

</span>
<span class="ltx_bibblock">Lmflow: An extensible toolkit for finetuning and inference of large foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2306.12420</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ellul (2021)</span>
<span class="ltx_bibblock">
Jacques Ellul. 2021.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Propaganda: The formation of men’s attitudes</em>.

</span>
<span class="ltx_bibblock">Vintage.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fung and Ji (2022)</span>
<span class="ltx_bibblock">
Yi R. Fung and Heng Ji. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2203.05967" title="">A weibo dataset for the 2022 russo-ukrainian crisis</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Preprint</em>, arXiv:2203.05967.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geissler et al. (2023)</span>
<span class="ltx_bibblock">
Dominique Geissler, Dominik Bär, Nicolas Pröllochs, and Stefan Feuerriegel. 2023.

</span>
<span class="ltx_bibblock">Russian propaganda on social media during the 2022 invasion of ukraine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">EPJ Data Science</em>, 12(1):35.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldstein et al. (2024)</span>
<span class="ltx_bibblock">
Josh A Goldstein, Jason Chao, Shelby Grossman, Alex Stamos, and Michael Tomz. 2024.

</span>
<span class="ltx_bibblock">How persuasive is ai-generated propaganda?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">PNAS nexus</em>, 3(2):pgae034.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Golovchenko (2022)</span>
<span class="ltx_bibblock">
Yevgeniy Golovchenko. 2022.

</span>
<span class="ltx_bibblock">Fighting propaganda with censorship: A study of the ukrainian ban on russian social media.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">The Journal of Politics</em>, 84(2):639–654.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guarino et al. (2020)</span>
<span class="ltx_bibblock">
Stefano Guarino, Noemi Trino, Alessandro Celestini, Alessandro Chessa, and Gianni Riotta. 2020.

</span>
<span class="ltx_bibblock">Characterizing networks of propaganda on twitter: a case study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Applied Network Science</em>, 5(1):1–22.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guess and Lyons (2020)</span>
<span class="ltx_bibblock">
Andrew M Guess and Benjamin A Lyons. 2020.

</span>
<span class="ltx_bibblock">Misinformation, disinformation, and online propaganda.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Social media and democracy: The state of the field, prospects for reform</em>, 10.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2024)</span>
<span class="ltx_bibblock">
Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. 2024.

</span>
<span class="ltx_bibblock">Large language model based multi-agents: A survey of progress and challenges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2402.01680</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al. (2019)</span>
<span class="ltx_bibblock">
Pankaj Gupta, Khushbu Saxena, Usama Yaseen, Thomas Runkler, and Hinrich Schütze. 2019.

</span>
<span class="ltx_bibblock">Neural architectures for fine-grained propaganda detection in news.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:1909.06162</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. (2024)</span>
<span class="ltx_bibblock">
Chi Han, Jialiang Xu, Manling Li, Yi Fung, Chenkai Sun, Nan Jiang, Tarek Abdelzaher, and Heng Ji. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2305.12798" title="">Word embeddings are steers for language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Preprint</em>, arXiv:2305.12798.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heppell et al. (2023)</span>
<span class="ltx_bibblock">
Freddy Heppell, Kalina Bontcheva, and Carolina Scarton. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-main.349" title="">Analysing state-backed propaganda websites: a new dataset and linguistic study</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 5729–5741, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hobbs (2020)</span>
<span class="ltx_bibblock">
Renee Hobbs. 2020.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Mind over media: Propaganda education for a digital age</em>.

</span>
<span class="ltx_bibblock">WW Norton &amp; Company.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2023)</span>
<span class="ltx_bibblock">
Kung-Hsiang Huang, Kathleen McKeown, Preslav Nakov, Yejin Choi, and Heng Ji. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.815" title="">Faking fake news for real fake news detection: Propaganda-loaded training data generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 14571–14589, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jones (2024)</span>
<span class="ltx_bibblock">
Daniel Gordon Jones. 2024.

</span>
<span class="ltx_bibblock">Detecting propaganda in news articles using large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Eng OA</em>, 2(1):01–12.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jowett and O’donnell (2018)</span>
<span class="ltx_bibblock">
Garth S Jowett and Victoria O’donnell. 2018.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Propaganda &amp; persuasion</em>.

</span>
<span class="ltx_bibblock">Sage publications.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jowett and O’Donnell (2012)</span>
<span class="ltx_bibblock">
Garth S Jowett and Victoria O’Donnell. 2012.

</span>
<span class="ltx_bibblock">What is propaganda, and how does it differ from persuasion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Propaganda &amp; persuasion</em>, pages 1–48.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khairova et al. (2023)</span>
<span class="ltx_bibblock">
Nina Khairova, Bogdan Ivasiuk, Fabrizio Lo Scudo, Carmela Comito, and Andrea Galassi. 2023.

</span>
<span class="ltx_bibblock">A first attempt to detect misinformation in russia-ukraine war news through text similarity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 4th Conference on Language, Data and Knowledge (LDK)</em>, pages 559–564.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khanday et al. (2021)</span>
<span class="ltx_bibblock">
Akib Mohi Ud Din Khanday, Qamar Rayees Khan, and Syed Tanzeel Rabani. 2021.

</span>
<span class="ltx_bibblock">Detecting textual propaganda using machine learning techniques.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Baghdad Science Journal</em>, 18(1):0199–0199.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2019)</span>
<span class="ltx_bibblock">
Jinfen Li, Zhihao Ye, and Lu Xiao. 2019.

</span>
<span class="ltx_bibblock">Detection of propaganda using logistic regression.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda</em>, pages 119–124.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Jiao Liu, Yanling Li, and Min Lin. 2019.

</span>
<span class="ltx_bibblock">Review of intent detection methods in the human-machine dialogue system.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Journal of physics: conference series</em>, volume 1267, page 012059. IOP Publishing.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Jiateng Liu, Sha Li, Zhenhailong Wang, Manling Li, and Heng Ji. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-acl.122" title="">A language-first approach for procedure planning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 1941–1954, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024)</span>
<span class="ltx_bibblock">
Jiateng Liu, Pengfei Yu, Yuji Zhang, Sha Li, Zixuan Zhang, and Heng Ji. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2402.11324" title="">Evedit: Event-based knowledge editing with deductive editing boundaries</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Preprint</em>, arXiv:2402.11324.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lord and Vogt (2021)</span>
<span class="ltx_bibblock">
Kristin M Lord and Katya Vogt. 2021.

</span>
<span class="ltx_bibblock">Strengthen media literacy to win the fight against misinformation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martino et al. (2020a)</span>
<span class="ltx_bibblock">
G Martino, Alberto Barrón-Cedeno, Henning Wachsmuth, Rostislav Petrov, and Preslav Nakov. 2020a.

</span>
<span class="ltx_bibblock">Semeval-2020 task 11: Detection of propaganda techniques in news articles.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2009.02696</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martino et al. (2020b)</span>
<span class="ltx_bibblock">
Giovanni Da San Martino, Stefano Cresci, Alberto Barrón-Cedeño, Seunghak Yu, Roberto Di Pietro, and Preslav Nakov. 2020b.

</span>
<span class="ltx_bibblock">A survey on computational propaganda detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2007.08024</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller (1937)</span>
<span class="ltx_bibblock">
Clyde Raymond Miller. 1937.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">How to detect propaganda</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakov and Da San Martino (2021)</span>
<span class="ltx_bibblock">
Preslav Nakov and Giovanni Da San Martino. 2021.

</span>
<span class="ltx_bibblock">Fake news, disinformation, propaganda, and media bias.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</em>, pages 4862–4865.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nelson (1997)</span>
<span class="ltx_bibblock">
Richard Alan Nelson. 1997.

</span>
<span class="ltx_bibblock">A chronology and glossary of propaganda in the united states (book review).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Journalism and Mass Communication Quarterly</em>, 74(3):645.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2303.08774" title="">Gpt-4 technical report</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Preprint</em>, arXiv:2303.08774.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">O’Shaughnessy (2004)</span>
<span class="ltx_bibblock">
Nicholas J O’Shaughnessy. 2004.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Politics and propaganda: Weapons of mass seduction</em>.

</span>
<span class="ltx_bibblock">Manchester University Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patrona (2022)</span>
<span class="ltx_bibblock">
Marianna Patrona. 2022.

</span>
<span class="ltx_bibblock">Snapshots from an information war: Propaganda, intertextuality, and audience design in the russia–ukraine conflict.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Violence: An International Journal</em>, 3(2):253–280.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddy et al. (2024)</span>
<span class="ltx_bibblock">
Revanth Gangi Reddy, Daniel Lee, Yi R. Fung, Khanh Duy Nguyen, Qi Zeng, Manling Li, Ziqi Wang, Clare Voss, and Heng Ji. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2303.14337" title="">Smartbook: Ai-assisted situation report generation for intelligence analysts</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Preprint</em>, arXiv:2303.14337.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shu et al. (2020)</span>
<span class="ltx_bibblock">
Kai Shu, Deepak Mahudeswaran, Suhang Wang, Dongwon Lee, and Huan Liu. 2020.

</span>
<span class="ltx_bibblock">Fakenewsnet: A data repository with news content, social context, and spatiotemporal information for studying fake news on social media.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Big data</em>, 8(3):171–188.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Şimşek and Fensel (2018)</span>
<span class="ltx_bibblock">
Umutcan Şimşek and Dieter Fensel. 2018.

</span>
<span class="ltx_bibblock">Intent generation for goal-oriented dialogue systems based on schema. org annotations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:1807.01292</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sprenkamp et al. (2023)</span>
<span class="ltx_bibblock">
Kilian Sprenkamp, Daniel Gordon Jones, and Liudmila Zavolokina. 2023.

</span>
<span class="ltx_bibblock">Large language models for propaganda detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2310.06422</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stanley (2015)</span>
<span class="ltx_bibblock">
Jason Stanley. 2015.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">How propaganda works</em>.

</span>
<span class="ltx_bibblock">Princeton University Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2023a)</span>
<span class="ltx_bibblock">
Chenkai Sun, Jinning Li, Hou Pong Chan, ChengXiang Zhai, and Heng Ji. 2023a.

</span>
<span class="ltx_bibblock">Measuring the effect of influential messages on varying personas.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proc. The 61st Annual Meeting of the Association for Computational Linguistics (ACL2023)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2023b)</span>
<span class="ltx_bibblock">
Chenkai Sun, Jinning Li, Yi Fung, Hou Pong Chan, Tarek Abdelzaher, ChengXiang Zhai, and Heng Ji. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.13297" title="">Decoding the silent majority: Inducing belief augmented social graph with large language model for response forecasting</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Preprint</em>, arXiv:2310.13297.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2024)</span>
<span class="ltx_bibblock">
Chenkai Sun, Ke Yang, Revanth Gangi Reddy, Yi R. Fung, Hou Pong Chan, ChengXiang Zhai, and Heng Ji. 2024.

</span>
<span class="ltx_bibblock">Persona-db: Efficient large language model personalization for response prediction with collaborative data refinement.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arxiv</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Torok (2015)</span>
<span class="ltx_bibblock">
Robyn Torok. 2015.

</span>
<span class="ltx_bibblock">Symbiotic radicalisation strategies: Propaganda tools and neuro linguistic programming.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2302.13971" title="">Llama: Open and efficient foundation language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Preprint</em>, arXiv:2302.13971.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vijayaraghavan and Vosoughi (2022)</span>
<span class="ltx_bibblock">
Prashanth Vijayaraghavan and Soroush Vosoughi. 2022.

</span>
<span class="ltx_bibblock">Tweetspin: Fine-grained propaganda detection in social media using multi-view representations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 3433–3448.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vlad et al. (2019)</span>
<span class="ltx_bibblock">
George-Alexandru Vlad, Mircea-Adrian Tanase, Cristian Onose, and Dumitru-Clementin Cercel. 2019.

</span>
<span class="ltx_bibblock">Sentence-level propaganda detection in news articles with transfer learning and bert-bilstm-capsule model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Proceedings of the second workshop on natural language processing for internet freedom: censorship, disinformation, and propaganda</em>, pages 148–154.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vorakitphan et al. (2022)</span>
<span class="ltx_bibblock">
Vorakit Vorakitphan, Elena Cabrio, and Serena Villata. 2022.

</span>
<span class="ltx_bibblock">Protect: A pipeline for propaganda detection and classification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">CLiC-it 2021-Italian Conference on Computational Linguistics</em>, pages 352–358.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wagner (2022)</span>
<span class="ltx_bibblock">
Julia N Wagner. 2022.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Open Intent Generation Through Unsupervised Semantic Clustering of Task-Oriented Dialog</em>.

</span>
<span class="ltx_bibblock">Ph.D. thesis, Massachusetts Institute of Technology.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Walton (1997)</span>
<span class="ltx_bibblock">
Douglas Walton. 1997.

</span>
<span class="ltx_bibblock">What is propaganda, and what exactly is wrong with it.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Public Affairs Quarterly</em>, 11(4):383–413.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024a)</span>
<span class="ltx_bibblock">
Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. 2024a.

</span>
<span class="ltx_bibblock">Executable code actions elicit better llm agents.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proc. The Forty-first International Conference on Machine Learning (ICML2024)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Xingyao Wang, Hao Peng, Reyhaneh Jabbarvand, and Heng Ji. 2023.

</span>
<span class="ltx_bibblock">Leti: Learning to generate from textual interactions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">arxiv</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024b)</span>
<span class="ltx_bibblock">
Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. 2024b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2309.10691" title="">Mint: Evaluating llms in multi-turn interaction with tools and language feedback</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Preprint</em>, arXiv:2309.10691.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weld et al. (2022)</span>
<span class="ltx_bibblock">
Henry Weld, Xiaoqi Huang, Siqu Long, Josiah Poon, and Soyeon Caren Han. 2022.

</span>
<span class="ltx_bibblock">A survey of joint intent detection and slot filling models in natural language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">ACM Computing Surveys</em>, 55(8):1–38.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024)</span>
<span class="ltx_bibblock">
Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R. Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, Heng Ji, and Chengxiang Zhai. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2401.00812" title="">If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Preprint</em>, arXiv:2401.00812.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2023)</span>
<span class="ltx_bibblock">
Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, et al. 2023.

</span>
<span class="ltx_bibblock">A comprehensive capability analysis of gpt-3 and gpt-3.5 series models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2303.10420</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoosuf and Yang (2019)</span>
<span class="ltx_bibblock">
Shehel Yoosuf and Yin Yang. 2019.

</span>
<span class="ltx_bibblock">Fine-grained propaganda detection with fine-tuned bert.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Proceedings of the second workshop on natural language processing for internet freedom: censorship, disinformation, and propaganda</em>, pages 87–91.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2021)</span>
<span class="ltx_bibblock">
Seunghak Yu, Giovanni Da San Martino, Mitra Mohtarami, James Glass, and Preslav Nakov. 2021.

</span>
<span class="ltx_bibblock">Interpretable propaganda detection in news articles.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">arXiv preprint arXiv:2108.12802</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et al. (2019)</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock">Defending against neural fake news.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Advances in neural information processing systems</em>, 32.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2019)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019.

</span>
<span class="ltx_bibblock">Bertscore: Evaluating text generation with bert.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:1904.09675</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zollmann (2019)</span>
<span class="ltx_bibblock">
Florian Zollmann. 2019.

</span>
<span class="ltx_bibblock">Bringing propaganda back into news media studies.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Critical Sociology</em>, 45(3):329–345.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Details of a Propaganda Frame</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">We list the closed set of propaganda techniques that are used in the paper in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A1.T6" title="Table 6 ‣ Appendix A Details of a Propaganda Frame ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">6</span></a>. We also included the full template that we used to describe appeals and intent. Note that (1) The set of propaganda techniques included here can be freely extended with any other techniques. (2) We made the templates for Appeals and intent with a valid rationale, as detailed in Section§ <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S2" title="2 PropaInsight: A Propaganda Analysis Framework ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">2</span></a>. However, we are not claiming that this is the optimal template design among all other possible designs. We believe that prompt engineering and further human assessment are necessary for discovering the optimal template for this task. And we leave this part for future work.</p>
</div>
<figure class="ltx_table" id="A1.T6">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.T6.1">
<tr class="ltx_tr" id="A1.T6.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A1.T6.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1">Propaganda Technique Set</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.2">
<td class="ltx_td ltx_align_left" id="A1.T6.1.2.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.2.1.1">Loaded Language</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="A1.T6.1.2.2" style="width:94.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T6.1.2.2.1">
<span class="ltx_p" id="A1.T6.1.2.2.1.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.2.2.1.1.1">Name Calling / labeling</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.3">
<td class="ltx_td ltx_align_left" id="A1.T6.1.3.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.3.1.1">Repetition</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="A1.T6.1.3.2" style="width:94.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T6.1.3.2.1">
<span class="ltx_p" id="A1.T6.1.3.2.1.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.3.2.1.1.1">Obfuscation</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.4">
<td class="ltx_td ltx_align_left" id="A1.T6.1.4.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.4.1.1">Doubt</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="A1.T6.1.4.2" style="width:94.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T6.1.4.2.1">
<span class="ltx_p" id="A1.T6.1.4.2.1.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.4.2.1.1.1">Straw man</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.5">
<td class="ltx_td ltx_align_left" id="A1.T6.1.5.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.5.1.1">Flag-waving</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="A1.T6.1.5.2" style="width:94.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T6.1.5.2.1">
<span class="ltx_p" id="A1.T6.1.5.2.1.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.5.2.1.1.1">Causal oversimplification</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.6">
<td class="ltx_td ltx_align_left" id="A1.T6.1.6.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.6.1.1">Slogans</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="A1.T6.1.6.2" style="width:94.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T6.1.6.2.1">
<span class="ltx_p" id="A1.T6.1.6.2.1.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.6.2.1.1.1">Black-and-white fallacy</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.7">
<td class="ltx_td ltx_align_left" id="A1.T6.1.7.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.7.1.1">Appeal to authority</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="A1.T6.1.7.2" style="width:94.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T6.1.7.2.1">
<span class="ltx_p" id="A1.T6.1.7.2.1.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.7.2.1.1.1">thought-terminating cliche</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.8">
<td class="ltx_td ltx_align_left" id="A1.T6.1.8.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.8.1.1">Whataboutism</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="A1.T6.1.8.2" style="width:94.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T6.1.8.2.1">
<span class="ltx_p" id="A1.T6.1.8.2.1.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.8.2.1.1.1">Reductio and Hitlerum</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.9">
<td class="ltx_td ltx_align_left" id="A1.T6.1.9.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.9.1.1">Smears</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="A1.T6.1.9.2" style="width:94.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T6.1.9.2.1">
<span class="ltx_p" id="A1.T6.1.9.2.1.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.9.2.1.1.1">Glittering Generalities</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.10">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A1.T6.1.10.1"><span class="ltx_text ltx_font_bold" id="A1.T6.1.10.1.1">Appeals Raised by Propaganda Usage</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.11">
<td class="ltx_td ltx_align_center" colspan="2" id="A1.T6.1.11.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.11.1.1">1. Make the readers feel [Emotion] about [Sth that is related]</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.12">
<td class="ltx_td ltx_align_center" colspan="2" id="A1.T6.1.12.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.12.1.1">2. Make the readers realize that [Sth that is related]</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.13">
<td class="ltx_td ltx_align_center" colspan="2" id="A1.T6.1.13.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.13.1.1">3. Make the readers ignore that [Sth that is related]</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.14">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A1.T6.1.14.1"><span class="ltx_text ltx_font_bold" id="A1.T6.1.14.1.1">Ultimate Intent Generation</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.15">
<td class="ltx_td ltx_align_center ltx_border_bb" colspan="2" id="A1.T6.1.15.1"><span class="ltx_text ltx_font_italic" id="A1.T6.1.15.1.1">[Anything that is applicable for explaining the intent]</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>The complete formulation for each component during propaganda analysis. The parts marked by ‘[]’ indicates the allowance for free generation.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Data Generation Prompt Templates</h2>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Step1: News Summarization</h4>
<div class="ltx_para" id="A2.SS0.SSS0.Px1.p1">
<blockquote class="ltx_quote" id="A2.SS0.SSS0.Px1.p1.1">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p1.1.1"><span class="ltx_text ltx_font_italic" id="A2.SS0.SSS0.Px1.p1.1.1.1">System Message: You are a helpful assistant.</span></p>
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p1.1.2">News: {news}</p>
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p1.1.3">Provide an objective summary of the news article, ensuring to present information in neutral terms. Avoid using language or phrases that show bias towards either party involved.</p>
</blockquote>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Step 2: Intent Creation</h4>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p1">
<blockquote class="ltx_quote" id="A2.SS0.SSS0.Px2.p1.1">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p1.1.1"><span class="ltx_text ltx_font_italic" id="A2.SS0.SSS0.Px2.p1.1.1.1">System Message: You are a helpful assistant.</span></p>
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p1.1.2">Article: {article}</p>
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p1.1.3">Identify all parties mentioned in the article. Select one party randomly and create an intent narrative to potentially reshape the article.</p>
</blockquote>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Step 4: Techniques Insertion</h4>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p1">
<blockquote class="ltx_quote" id="A2.SS0.SSS0.Px3.p1.1">
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p1.1.1"><span class="ltx_text ltx_font_italic" id="A2.SS0.SSS0.Px3.p1.1.1.1">System Message: You are a skilled journalist, proficient in composing short brief news pieces.</span></p>
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p1.1.2">Article: {article}</p>
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p1.1.3">Rewrite the article into a short news piece to {intent}. Convey the intent narrative effectively by applying the following rhetorical tactics, once or more as needed. The revision must be concise, with a clear emphasis on using these tactics to communicate the intended message. Avoid generating non-factual information.</p>
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p1.1.4">1. {appeal tactic 1}
<br class="ltx_break"/>Example: {appeal tactic 1 example}</p>
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p1.1.5">2. {appeal tactic 2}
<br class="ltx_break"/>Example: {appeal tactic 2 example}</p>
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p1.1.6">3. {appeal tactic 3}
<br class="ltx_break"/>Example: {appeal tactic 3 example}</p>
</blockquote>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Step 4: Appeals Generation</h4>
<div class="ltx_para" id="A2.SS0.SSS0.Px4.p1">
<blockquote class="ltx_quote" id="A2.SS0.SSS0.Px4.p1.1">
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p1.1.1"><span class="ltx_text ltx_font_italic" id="A2.SS0.SSS0.Px4.p1.1.1.1">System Message: You are a helpful assistant that identifies how the writer of a news article wants the readers of the article to feel after reading some sentences.</span></p>
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p1.1.2">In this task, the input will be a news article, then some sentence in the article will be provided and you need to identify how the specific sentence raise appeals among the readers, the propaganda tactics used in these sentences will also be proivded as a hint. Also remeber that your response should be aware of the main goal of the whole article. For each sentence, you only need to output a sentence describing the feelings in one of the following two templates:</p>
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p1.1.3">Make the readers feel [Positive &amp; Negative Emotions] about [Something that is related]
<br class="ltx_break"/>Make the readers realize/Ignore [Something that is related]</p>
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p1.1.4">Here is an example indicating the input and output format:</p>
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p1.1.5">Input:
News article: {article}</p>
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p1.1.6">Sentence: {first sentence}
<br class="ltx_break"/>Tactic: {the tactic that is used in the sentence}</p>
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p1.1.7">Sentence: {second sentence}
<br class="ltx_break"/>Tactic: {the tactic that is used in the sentence}</p>
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p1.1.8">…</p>
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p1.1.9">Output:</p>
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p1.1.10">[1] Your response for the first sentence: Make the readers feel [Positive &amp; Negative Emotions] about [Something that is related]</p>
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p1.1.11">[2] Your response for the second sentence: Make the readers realize/Ignore [Something that is related]</p>
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p1.1.12">…</p>
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p1.1.13">Now let’s begin!</p>
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p1.1.14">Now given the following news article:</p>
</blockquote>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Templates and Prompts We Used for Propaganda Analysis</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">This section describes each generation and prompt used in this paper. While these prompts could be enhanced through prompt engineering and additional human evaluation, we use them here as proof of concept.</p>
</div>
<section class="ltx_paragraph" id="A3.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Template for Composing the Predicted Elements into a Descriptive Sentence</h4>
<div class="ltx_para" id="A3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A3.SS0.SSS0.Px1.p1.1">We use the following template to compose the predicted elements into a descriptive sentence as the final output for the propaganda analysis task:</p>
<blockquote class="ltx_quote" id="A3.SS0.SSS0.Px1.p1.2">
<p class="ltx_p" id="A3.SS0.SSS0.Px1.p1.2.1">This article uses propaganda to further its author’s ultimate intent of {<span class="ltx_text ltx_font_italic" id="A3.SS0.SSS0.Px1.p1.2.1.1">The ultimate intent that is predicted by the model</span>}. Specifically, the author uses {<span class="ltx_text ltx_font_italic" id="A3.SS0.SSS0.Px1.p1.2.1.2">The first identified propaganda technique</span>} in the sentence: "{<span class="ltx_text ltx_font_italic" id="A3.SS0.SSS0.Px1.p1.2.1.3">The first sentence that is identified to use propaganda</span>}" to make the readers {<span class="ltx_text ltx_font_italic" id="A3.SS0.SSS0.Px1.p1.2.1.4">The first appeal that is raised among the readers</span>}. The author also uses {<span class="ltx_text ltx_font_italic" id="A3.SS0.SSS0.Px1.p1.2.1.5">The second identified propaganda technique</span>} in the sentence: "{<span class="ltx_text ltx_font_italic" id="A3.SS0.SSS0.Px1.p1.2.1.6">The second sentence that is identified to use propaganda</span>}" to make the readers {<span class="ltx_text ltx_font_italic" id="A3.SS0.SSS0.Px1.p1.2.1.7">The second appeal that is raised among the readers</span>}…</p>
</blockquote>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Prompt Template for the Language Models to Analyze Propaganda in a Zero-shot Manner</h4>
<div class="ltx_para" id="A3.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A3.SS0.SSS0.Px2.p1.1">We use the following prompt to encourage language models to correctly predict the elements with in a propaganda frame, this prompt also enables simple parsing to obtain the results.</p>
</div>
<div class="ltx_para" id="A3.SS0.SSS0.Px2.p2">
<blockquote class="ltx_quote" id="A3.SS0.SSS0.Px2.p2.1">
<p class="ltx_p" id="A3.SS0.SSS0.Px2.p2.1.1">News article: {The news article that needs to be analyzed}</p>
<p class="ltx_p" id="A3.SS0.SSS0.Px2.p2.1.2">Given the news article above, you should detect the major intent of the article. The intent is conveyed by using certain tactics and raise appeals in some text spans. You are also going to output all the text spans and the corresponding tactics and appeals.</p>
<p class="ltx_p" id="A3.SS0.SSS0.Px2.p2.1.3">The tactics that maybe used are listed here: loaded language, flag waving, slogans, repetition, straw man, red herring, whataboutism, obfuscation, causal oversimplification, false dilemma, thought terminating cliche, appeal to authority, bandwagon, glittering generalities, name calling, doubt, smears, reducito ad hitlerum</p>
<p class="ltx_p" id="A3.SS0.SSS0.Px2.p2.1.4">You should also formulate the generated appeals in the following format, choose one of the following template to fill in the appeals:</p>
<p class="ltx_p" id="A3.SS0.SSS0.Px2.p2.1.5">Make the readers feel [Some Emotion] about [Something that is related]
Make the readers realize about [Something that is related]
Make the readers ignore [Something that is related]</p>
<p class="ltx_p" id="A3.SS0.SSS0.Px2.p2.1.6">Your should firstly output the ultimate intent, then sequentially output all the text spans within the original article that contains tacic and appeals related to this intent and the corresponding tactics and appeals. You should only output one appeal for each text span. Here is an example:</p>
<p class="ltx_p" id="A3.SS0.SSS0.Px2.p2.1.7">{Ultimate intent} The intent detected {Ultimate intent}</p>
<p class="ltx_p" id="A3.SS0.SSS0.Px2.p2.1.8">{Text Span} Text Span 1 {Text Span}
{Tactic} Tactic 1 {Tactic}
{Appeal} Appeal 1 {Appeal}</p>
<p class="ltx_p" id="A3.SS0.SSS0.Px2.p2.1.9">{Text Span} Text Span 2 {Text Span}
{Tactic} Tactic 2 {Tactic}
{Appeal} Appeal 1 {Appeal}</p>
<p class="ltx_p" id="A3.SS0.SSS0.Px2.p2.1.10">…</p>
<p class="ltx_p" id="A3.SS0.SSS0.Px2.p2.1.11">Now, output your answer with the given News article!</p>
</blockquote>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Template for Instruction Tuning with Llama2-Chat-7B on Tactics</h4>
<div class="ltx_para" id="A3.SS0.SSS0.Px3.p1">
<blockquote class="ltx_quote" id="A3.SS0.SSS0.Px3.p1.1">
<p class="ltx_p" id="A3.SS0.SSS0.Px3.p1.1.1">User: Is the sentence below using propaganda techniques? Answer with [Yes] [Propaganda Technique] or [No] [None], candidate techniques are: Providing the candidate techniques Sentence: {The sentence that needs to be identified. }</p>
<p class="ltx_p" id="A3.SS0.SSS0.Px3.p1.1.2">Assistant: {The Templated Answers}</p>
</blockquote>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Template for Instruction Tuning with Llama2-Chat-7B on Appeals</h4>
<div class="ltx_para" id="A3.SS0.SSS0.Px4.p1">
<blockquote class="ltx_quote" id="A3.SS0.SSS0.Px4.p1.1">
<p class="ltx_p" id="A3.SS0.SSS0.Px4.p1.1.1">What is the appeal that the author tries to arouse in the following sentence ? Answer with ’Make the readers …’, Sentence: {The sentence that needs to be identified. }</p>
<p class="ltx_p" id="A3.SS0.SSS0.Px4.p1.1.2">Assistant: {The Templated Answers}</p>
</blockquote>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS0.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">Template for Instruction Tuning with Llama2-Chat-7B on intent</h4>
<div class="ltx_para" id="A3.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="A3.SS0.SSS0.Px5.p1.1">What is the intent that the author tries to convey in the following article ? Answer with a paragraph of intent. Article: {The article that needs detection. }</p>
</div>
<div class="ltx_para" id="A3.SS0.SSS0.Px5.p2">
<p class="ltx_p" id="A3.SS0.SSS0.Px5.p2.1">Assistant: {The Answers}</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Details of the <span class="ltx_text ltx_font_italic" id="A4.1.1">PropaGaze</span> Dataset</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">As introduced in Section § <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S3" title="3 PropaGaze: A Dataset for Systematically Analyzing Propaganda ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">3</span></a>, the <span class="ltx_text ltx_font_italic" id="A4.p1.1.1">PropaGaze</span> dataset comprises three subsets: <span class="ltx_text ltx_font_italic" id="A4.p1.1.2">RUWA-Gaze</span>, <span class="ltx_text ltx_font_italic" id="A4.p1.1.3">Politifact-Gaze</span>, and <span class="ltx_text ltx_font_italic" id="A4.p1.1.4">PTC-Gaze</span>. More details and data examples are provided in this section.</p>
</div>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_font_italic ltx_title_paragraph">RUWA-Gaze</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px1.p1.1">The <span class="ltx_text ltx_font_italic" id="A4.SS0.SSS0.Px1.p1.1.1">RUWA-Gaze</span> dataset is constructed focusing on the Ukraine-Russia War. The original news dataset was from <cite class="ltx_cite ltx_citemacro_cite">Khairova et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib30" title="">2023</a>)</cite>. After human verification on the construction steps, we keep 497 articles, with each article having an average of 3.46 times propaganda usage. We provide an example piece of data from the constructed <span class="ltx_text ltx_font_italic" id="A4.SS0.SSS0.Px1.p1.1.2">RUWA-Gaze</span> below, the article has three times of propaganda usage:</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="A4.SS0.SSS0.Px1.p2.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px1.p2.1.1">Generated News Article:</span> In light of recent intelligence reports from Ukraine detailing a covert Russian operation to recover classified remains from a sunken cruiser in Crimea, the international call for transparency has never been louder. The Moskva missile cruiser, a symbol of strength and resilience, met its fate in the Black Sea on April 14, igniting a flurry of claims and counterclaims between Ukraine and Russia regarding the circumstances of its sinking. Ukraine has steadfastly maintained that the cruiser was struck by two of its Neptune missiles, a claim seemingly bolstered by the Pentagon’s confirmation, while Russia vehemently denies such an event. In this critical moment of geopolitical tension, the need for concrete evidence from Ukraine to substantiate its claims cannot be overstated. As we stand united in our pursuit of truth and justice, it becomes imperative for us to rally around the call for transparency, ensuring that every claim made is backed by irrefutable proof. This is not just about a sunken ship; it’s about maintaining the delicate balance of peace and preventing any further escalation that could lead our brothers and sisters into an unwarranted conflict. Some may argue, why this focus on Ukraine’s need to present evidence when there are other pressing issues at hand that demand our attention. However, this moment offers a unique opportunity to address underlying issues of trust and accountability in a world already beset by misinformation and conflict. By embracing a posture of openness, Ukraine can lead by example, joining the chorus of nations that have chosen the path of transparency and responsibility. Such a move would not only vindicate Ukraine’s claims but also strengthen international confidence in its commitment to integrity and truth. As the situation develops, the world watches closely. The presentation of solid evidence will be a pivotal step in resolving the current standoff, soothing tensions, and charting a course towards resolution and understanding between nations.</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="A4.SS0.SSS0.Px1.p3.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px1.p3.1.1">The sentence that uses propaganda 1:</span> As we stand united in our pursuit of truth and justice, it becomes imperative for us to rally around the call for transparency, ensuring that every claim made is backed by irrefutable proof.</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px1.p4">
<p class="ltx_p" id="A4.SS0.SSS0.Px1.p4.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px1.p4.1.1">Technique annotation 1:</span> Flag-waving</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px1.p5">
<p class="ltx_p" id="A4.SS0.SSS0.Px1.p5.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px1.p5.1.1">Appeal annotation 1:</span> Make the readers feel positive about the demonstration of unity and transparency.</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px1.p6">
<p class="ltx_p" id="A4.SS0.SSS0.Px1.p6.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px1.p6.1.1">The sentence that uses propaganda 2:</span> Some may argue, why this focus on Ukraine’s need to present evidence when there are other pressing issues at hand that demand our attention.</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px1.p7">
<p class="ltx_p" id="A4.SS0.SSS0.Px1.p7.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px1.p7.1.1">Technique annotation 2:</span> Whataboutism</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px1.p8">
<p class="ltx_p" id="A4.SS0.SSS0.Px1.p8.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px1.p8.1.1">Appeal annotation 2:</span> Make the readers ignore the distraction of other issues and focus on Ukraine’s need to present evidence.</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px1.p9">
<p class="ltx_p" id="A4.SS0.SSS0.Px1.p9.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px1.p9.1.1">The sentence that uses propaganda 3:</span> By embracing a posture of openness, Ukraine can lead by example, joining the chorus of nations that have chosen the path of transparency and responsibility.</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px1.p10">
<p class="ltx_p" id="A4.SS0.SSS0.Px1.p10.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px1.p10.1.1">Technique annotation 3:</span> Bandwagon</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px1.p11">
<p class="ltx_p" id="A4.SS0.SSS0.Px1.p11.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px1.p11.1.1">Appeal annotation 3:</span> Make the readers realize the opportunity Ukraine has to set a strong example of transparency and responsibility.</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px1.p12">
<p class="ltx_p" id="A4.SS0.SSS0.Px1.p12.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px1.p12.1.1">Intent annotation:</span> This article urges Ukraine to provide concrete evidence to support their claim that the Moskva missile cruiser was hit by Ukrainian-made Neptune missiles, in order to maintain transparency and prevent further escalation of tensions with Russia.</p>
</div>
</section>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_font_italic ltx_title_paragraph">Politifact-Gaze</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px2.p1.1">Similar to the <span class="ltx_text ltx_font_italic" id="A4.SS0.SSS0.Px2.p1.1.1">RUWA-Gaze</span> dataset, this subset is also constructed with our partially controlled generation pipeline. This dataset is based on the PolitiFact partition of the FakeNewsNet dataset <cite class="ltx_cite ltx_citemacro_cite">Shu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib46" title="">2020</a>)</cite> with a focus on political status across countries. We keep 593 generated articles with with each article having an average of 3.47 times propaganda usage. We provide an example piece of data from the constructed <span class="ltx_text ltx_font_italic" id="A4.SS0.SSS0.Px2.p1.1.2">Politifact-Gaze</span> below, the article has four times of propaganda usage:</p>
</div>
</section>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Generated News Article:</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px3.p1.1">Alabama Attorney General John Simmons has taken a bold stand for truth and justice by filing charges against Mary Lynne Davies, the accuser of Roy Moore, who now faces allegations of falsification, a glaring first-degree misdemeanor. In a shocking twist, Davies, a Democrat, has been exposed as a fabricator of claims against Moore, specifically relating to an alleged incident that supposedly occurred when she was 14 and he was in his early 30s. Legal experts confirm the charges could result in a year of imprisonment and $10,000 in fines for Davies. This sordid tale of deceit came to a head after the yearbook inscription that Moore purportedly wrote in the 1970s was definitively debunked as a forgery. Davies, in a desperate bid for attention, concocted a story so heinous, it has now backfired, forcing her into hiding following her release on $500 bail. Despite the unraveling of her narrative, her attorney maintains, without evidence, that she is a victim, not a perpetrator of falsehood. The root of this entire scandal lies solely in the vindictive actions of an individual who sought to manipulate public opinion for political gain2̆014disregarding the serious harm inflicted on Roy Moore’s reputation and life. Moore’s attorney has extended an olive branch, stating that Moore harbors no ill will and remains committed to championing women’s rights, a testament to his character and integrity. In a move that restores faith in the justice system, Attorney General Simmons is not only holding Davies accountable but is also investigating other dubious claims against Moore. This encompasses looking into malicious allegations made by employees of a mall, who labeled him with reprehensible titles without a shred of proof. Through the exposing of these falsehoods and the pursuit of accountability, Roy Moore stands vindicated. Legal and moral authorities alike have rallied to his defense, recognizing the travesty that nearly cost a steadfast advocate for American values and family principles his reputation and career. The pursuit of justice for Moore sends a strong message against the weaponization of false accusations in political warfare.</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="A4.SS0.SSS0.Px3.p2.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px3.p2.1.1">The sentence that uses propaganda 1:</span> Legal experts confirm the charges could result in a year of imprisonment and $10,000 in fines for Davies.</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px3.p3">
<p class="ltx_p" id="A4.SS0.SSS0.Px3.p3.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px3.p3.1.1">Technique annotation 1:</span> Appeal to authority</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px3.p4">
<p class="ltx_p" id="A4.SS0.SSS0.Px3.p4.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px3.p4.1.1">Appeal annotation 1:</span> Make the readers feel anxious and sympathetic towards Davies about her potential legal consequences.</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px3.p5">
<p class="ltx_p" id="A4.SS0.SSS0.Px3.p5.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px3.p5.1.1">The sentence that uses propaganda 2:</span> This sordid tale of deceit came to a head after the yearbook inscription that Moore purportedly wrote in the 1970s was definitively debunked as a forgery.</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px3.p6">
<p class="ltx_p" id="A4.SS0.SSS0.Px3.p6.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px3.p6.1.1">Technique annotation 2:</span> Loaded Language</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px3.p7">
<p class="ltx_p" id="A4.SS0.SSS0.Px3.p7.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px3.p7.1.1">Appeal annotation 2:</span> Make the readers feel disgusted about the forgery that Moore was accused of.</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px3.p8">
<p class="ltx_p" id="A4.SS0.SSS0.Px3.p8.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px3.p8.1.1">The sentence that uses propaganda 3:</span> The root of this entire scandal lies solely in the vindictive actions of an individual who sought to manipulate public opinion for political gain2̆014disregarding the serious harm inflicted on Roy Moore’s reputation and life.</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px3.p9">
<p class="ltx_p" id="A4.SS0.SSS0.Px3.p9.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px3.p9.1.1">Technique annotation 3:</span> Causal Oversimplification</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px3.p10">
<p class="ltx_p" id="A4.SS0.SSS0.Px3.p10.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px3.p10.1.1">Appeal annotation 3:</span> Make the readers feel anger and resentment about the sole individual who manipulated public opinion.</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px3.p11">
<p class="ltx_p" id="A4.SS0.SSS0.Px3.p11.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px3.p11.1.1">The sentence that uses propaganda 4:</span> Legal and moral authorities alike have rallied to his defense, recognizing the travesty that nearly cost a steadfast advocate for American values and family principles his reputation and career</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px3.p12">
<p class="ltx_p" id="A4.SS0.SSS0.Px3.p12.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px3.p12.1.1">Technique annotation 4:</span> Appeal to authority</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px3.p13">
<p class="ltx_p" id="A4.SS0.SSS0.Px3.p13.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px3.p13.1.1">Appeal annotation 4:</span> Make the readers feel relief and satisfaction about the support Moore is receiving from legal and moral authorities.</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px3.p14">
<p class="ltx_p" id="A4.SS0.SSS0.Px3.p14.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px3.p14.1.1">Intent annotation:</span> This article urges Ukraine to provide concrete evidence to support their claim that the Moskva missile cruiser was hit by Ukrainian-made Neptune missiles, in order to maintain transparency and prevent further escalation of tensions with Russia.</p>
</div>
</section>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">
<span class="ltx_text ltx_font_italic" id="A4.SS0.SSS0.Px4.1.1">PTC-Gaze</span>:</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px4.p1.1">The <span class="ltx_text ltx_font_italic" id="A4.SS0.SSS0.Px4.p1.1.1">PTC-Gaze</span> subset is constructed based on the propaganda techniques corpus <cite class="ltx_cite ltx_citemacro_cite">Martino et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib37" title="">2020a</a>)</cite>, the propaganda articles within the PTC dataset are from real-world news articles and the usage of propaganda together with propaganda techniques annotation is done by human annotators. To simulate the propaganda frame usage in the real world, we further hire human annotators from kitware to annotate each propaganda usage with further information of appeals, and conclude the article with intent. To alleviate the annotation burden, we firstly let GPT-4 models to generate a synthetic annotation, and then let human annotators to just the generated parts of this synthetic annotation and further rewrite into their own annotations. We collected 79 long articles with each article has an average number of 12.77 times of propaganda usage. We notice that in the real-world propaganda corpus, the times of propaganda usage for each article can be far exceeding that of synthetic data. We attribute this difference to a domain gap existing between synthetic articles and real articles. We give an example of the annotated article from <span class="ltx_text ltx_font_italic" id="A4.SS0.SSS0.Px4.p1.1.2">PTC-Gaze</span> as below:</p>
</div>
</section>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">Real-World News Article:</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px5.p1.1">Ex-Sailor Pardoned By Trump Says He’s SUING Obama And Comey A former Navy sailor, who is one of five people to receive a pardon from President Donald Trump, is planning to file a lawsuit against Obama administration officials. Kristian Saucier, who served a year in federal prison for taking photos of classified sections of the submarine on which he worked, says he was subject to unequal protection by the law. Saucier said that he realizes he had erred in taking the photos, which he said he wanted to show only to his family to show them where he worked. He has also lashed out at Obama officials, saying that his prosecution was politically motivated, prompted by sensitivity about classified information amid the scandal involving Clinton’s emails. According to Fox News, Saucier argues that the same officials who sought out punishment to Saucier for his actions chose to be lenient with Hillary Clinton in her use of a private email server and mishandling of classified information. Saucier’s lawyer, Ronald Daigle, told Fox News on Monday that the lawsuit, which he expects to file soon in Manhattan, will name the U.S. Department of Justice, former FBI Director James Comey and former President Barack Obama as defendants, among others. Saucier, who lives in Vermont, pleaded guilty in 2016 to taking photos inside the USS Alexandria while it was stationed in Groton, Connecticut, in 2009. He said he only wanted service mementos, but federal prosecutors argued he was a disgruntled sailor who had put national security at risk by taking photos showing the submarine’s propulsion system and reactor compartment and then obstructed justice by destroying a laptop and camera. Fox News They interpreted the law in my case to say it was criminal, Saucier told Fox News, referring to prosecuting authorities in his case, but they didn’t prosecute Hillary Clinton. Hillary is still walking free. Two guys on my ship did the same thing and weren’t treated as criminals. We want them to correct the wrong. Daigle said that a notice about the pending lawsuit was sent to the Department of Justice and others included in it in December. There is usually a six-month period that must elapse before the lawsuit actually is actually filed. My case was usually something handled by military courts, he said. They used me as an example because of [the backlash over] Hillary Clinton, he continued, alleging his life was ruined for political reasons. With a pardon, there’s no magic wand that gets waved and makes everything right, Saucier said, But I try to stay positive and look forward. Saucier has had cars repossessed and is in debt due to the loss of income after having a felony on his record. The government actively destroyed his life and made it all but impossible for his family to get back on track. But Hillary Clinton is running around free, to this day. And that is what Saucier is so burnt about, with good reason.</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px5.p2">
<p class="ltx_p" id="A4.SS0.SSS0.Px5.p2.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px5.p2.1.1">The sentence that uses propaganda 1:</span> Fox News: They interpreted the law in my case to say it was criminal, Saucier told Fox News, referring to prosecuting authorities in his case, but they didn’t prosecute Hillary Clinton.</p>
</div>
</section>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px6">
<h4 class="ltx_title ltx_title_paragraph">Technique annotation 1:</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px6.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px6.p1.1">Whataboutism</p>
</div>
</section>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px7">
<h4 class="ltx_title ltx_title_paragraph">Appeal annotation 1:</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px7.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px7.p1.1">Make the readers feel indignant about the contrasting legal treatments toward Saucier and Hillary Clinton.</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px7.p2">
<p class="ltx_p" id="A4.SS0.SSS0.Px7.p2.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px7.p2.1.1">The sentence that uses propaganda 2:</span> Two guys on my ship did the same thing and weren’t treated as criminals.</p>
</div>
</section>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px8">
<h4 class="ltx_title ltx_title_paragraph">Technique annotation 2:</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px8.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px8.p1.1">Whataboutism</p>
</div>
</section>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px9">
<h4 class="ltx_title ltx_title_paragraph">Appeal annotation 2:</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px9.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px9.p1.1">Make the readers feel unjust about the inequality in punitive measures for similar actions.</p>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px9.p2">
<p class="ltx_p" id="A4.SS0.SSS0.Px9.p2.1"><span class="ltx_text ltx_font_bold" id="A4.SS0.SSS0.Px9.p2.1.1">The sentence that uses propaganda 3:</span> They used me as an example because of [the backlash over] Hillary Clinton, he continued, alleging his life was ruined for political reasons.</p>
</div>
</section>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px10">
<h4 class="ltx_title ltx_title_paragraph">Technique annotation 3:</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px10.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px10.p1.1">Causal oversimplification</p>
</div>
</section>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px11">
<h4 class="ltx_title ltx_title_paragraph">Appeal annotation 3:</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px11.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px11.p1.1">Make the readers feel sympathetic towards Saucier’s inopportune life circumstances allegedly resulting from political motivations.</p>
</div>
</section>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px12">
<h4 class="ltx_title ltx_title_paragraph">Intent Annotation:</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px12.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px12.p1.1">The news intends to inform the public about Kristian Saucier’s plans to sue Obama administration officials.</p>
</div>
</section>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px13">
<h4 class="ltx_title ltx_title_paragraph">Analysis for Annotation Quality</h4>
<figure class="ltx_figure" id="A4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="422" id="A4.F3.g1" src="extracted/5863894/figures/UI_intent.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The user interface we used in Label Studio to annotate intent based on a given article.</figcaption>
</figure>
<figure class="ltx_figure" id="A4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="556" id="A4.F4.g1" src="extracted/5863894/figures/UI_appeal.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The user interface we used in Label Studio to annotate appeals based on a context. The highlighted part will be the sentence to be annotated, while other parts of ’Target Article’ provide related context.</figcaption>
</figure>
<div class="ltx_para" id="A4.SS0.SSS0.Px13.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px13.p1.1">We provide an analysis of the annotation quality of our <span class="ltx_text ltx_font_italic" id="A4.SS0.SSS0.Px13.p1.1.1">PTC-Gaze</span> dataset. We used Label Studio for design the annotation interface. We present the user interface design of the intent annotation and appeal annotation tasks in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A4.F3" title="Figure 3 ‣ Analysis for Annotation Quality ‣ Appendix D Details of the PropaGaze Dataset ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">3</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#A4.F4" title="Figure 4 ‣ Analysis for Annotation Quality ‣ Appendix D Details of the PropaGaze Dataset ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">4</span></a>. Two professional annotators from Kitware.Inc is in charge of the annotation task. Annotators choose to utilize the candidate annotation generated by GPT-4 under 59.8% annotated intent data points and 75.1% annotated appeal data points. This demonstrates the high quality of GPT4-provided annotation in terms of appeals and intent, further enhanced our points in § <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S5.SS2" title="5.2 Further Challenges of Propaganda Analysis ‣ 5 Discussion ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Experimental Details</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">We provide experimental details for fine-tuning with Llama-Chat-7B and MGNN. For Llama-Chat-7B model, we used the LMFlow <cite class="ltx_cite ltx_citemacro_cite">Diao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#bib.bib13" title="">2023</a>)</cite> framework for fine-tuning. We used four A100 GPUs for training, we set the learning rate to 0.00002 and batch size to 4. We tune the model for 3 epochs with our training data. During inference, we always set the inference temperature of the Llama-Chat-7B model to 1.0. For GPT-4-turbo, we used the default temperature for generation. In terms of tuning MGNN, we set batch size to 16 as MGNN takes a smaller memory space, and we set the learning rate to 0.00003. We then tune the model for 20 epochs.</p>
</div>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Case Study: Bottleneck of Propaganda Analysis</h2>
<div class="ltx_para" id="A6.p1">
<p class="ltx_p" id="A6.p1.1">As discussed in § <a class="ltx_ref" href="https://arxiv.org/html/2409.18997v1#S5.SS2" title="5.2 Further Challenges of Propaganda Analysis ‣ 5 Discussion ‣ PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent"><span class="ltx_text ltx_ref_tag">5.2</span></a>, we find that the bottleneck of propaganda analysis lies in identifying the correct propagandistic sentences. In this section, we give a case study on LLMs doing propaganda analysis to explain the cause further.</p>
</div>
<section class="ltx_paragraph" id="A6.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Input Example Data</h4>
<div class="ltx_para" id="A6.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A6.SS0.SSS0.Px1.p1.1">In a riveting instance of journalism that pierced through the veil of political spin, Fox News host Shepard Smith launched into a fervent condemnation of Donald Trump Jr.’s misleading explanations about his meeting with a Russian lawyer. During a segment that left audiences grappling with notions of truth and integrity, Smith vociferously questioned the incessant l̈iesänd d̈eceptionẗhat seem to shroud the Trump administration’s dealings, implying the audience’s own complacency in the face of such deceit unless they demand accountability. Smith’s critique, grounded in an urgent plea for transparency, resonated strongly in an era where allegations of Russian collusion loom over the presidential election2̆014a matter of paramount significance under investigation by multiple congressional committees and a special counsel. Ẅhy all these lies? Why is it lie after lie after lie? If you’re clean, come on clean,S̈mith implored on S̈hepard Smith Reporting,ëmphasizing the sheer implausibility of the evolving narrative woven by Trump Jr. and, by extension, the administration. In a moment that laid bare the discomforting truths surrounding this saga, Smith’s acrimonious outburst was underscored by an on-air exchange with fellow anchor Chris Wallace, whose own speechlessness served as a testament to the gravity of Smith’s statements. The interaction, a compelling dramatization of the inner turmoil gripping the nation, amplified the weight of Smith’s words as he navigated the treacherous waters of political discourse. Yet, amid this pursuit of clarity and honesty, voices emerged calling for Smith’s removal from the network2̆014a diversion that starkly contrasts the core issue at hand: the integrity of democratic institutions and the transparency of those in power. Smith’s unapologetic defense of the press earlier in the year, where he rebuked claims against CNN as n̈ot fake news,f̈urther cements his role as a stalwart advocate for journalistic integrity in the face of political adversity. As the narrative of Donald Trump Jr.’s Russian rendezvous unfolds, Shepard Smith’s impassioned critique serves as a poignant reminder of the media’s critical role in dissecting the complex web of political narratives, urging the public to remain vigilant, question narratives, and demand nothing short of the truth.</p>
</div>
</section>
<section class="ltx_paragraph" id="A6.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Ground Truth Answers</h4>
<div class="ltx_para" id="A6.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p1.1">We list the ground truth propaganda identification and their related appeals and intent below:</p>
</div>
<div class="ltx_para" id="A6.SS0.SSS0.Px2.p2">
<blockquote class="ltx_quote" id="A6.SS0.SSS0.Px2.p2.1">
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p2.1.1"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p2.1.1.1">Sentence:</span> During a segment that left audiences grappling with notions of truth and integrity, Smith vociferously questioned the incessant l̈iesänd d̈eceptionẗhat seem to shroud the Trump administration’s dealings, implying the audience’s own complacency in the face of such deceit unless they demand accountability.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p2.1.2"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p2.1.2.1">Technique:</span> loaded language</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p2.1.3"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p2.1.3.1">Appeal:</span> Make the readers realize the serious implications of Smith’s statements, as emphasized by Chris Wallace’s speechlessness.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p2.1.4"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p2.1.4.1">Sentence:</span> In a moment that laid bare the discomforting truths surrounding this saga, Smith’s acrimonious outburst was underscored by an on-air exchange with fellow anchor Chris Wallace, whose own speechlessness served as a testament to the gravity of Smith’s statements.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p2.1.5"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p2.1.5.1">Technique:</span> appeal to authority</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p2.1.6"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p2.1.6.1">Appeal:</span> Make the readers realize the serious implications of Smith’s statements, as emphasized by Chris Wallace’s speechlessness.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p2.1.7"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p2.1.7.1">Sentence:</span> Yet, amid this pursuit of clarity and honesty, voices emerged calling for Smith’s removal from the network2̆014a diversion that starkly contrasts the core issue at hand: the integrity of democratic institutions and the transparency of those in power.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p2.1.8"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p2.1.8.1">Technique:</span> red herring</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p2.1.9"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p2.1.9.1">Appeal:</span> Make the readers ignore calls for Smith’s removal and focus instead on the integrity of democratic institutions and transparency of those in power.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p2.1.10"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p2.1.10.1">Intent:</span> Highlight Fox News host Shepard Smith’s criticism of Donald Trump Jr.’s l̈iesänd d̈eceptionr̈egarding his meeting with a Russian lawyer, questioning the ongoing narrative of dishonesty and secrecy within the Trump administration.</p>
</blockquote>
</div>
<div class="ltx_para" id="A6.SS0.SSS0.Px2.p3">
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p3.1"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p3.1.1">Predictions from LLMs</span></p>
</div>
<div class="ltx_para" id="A6.SS0.SSS0.Px2.p4">
<blockquote class="ltx_quote" id="A6.SS0.SSS0.Px2.p4.1">
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.1"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.1.1">Sentence:</span> In a riveting instance of journalism that pierced through the veil of political spin, Fox News host Shepard Smith launched into a fervent condemnation of Donald Trump Jr.’s misleading explanations about his meeting with a Russian lawyer.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.2"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.2.1">Technique:</span> loaded language</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.3"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.3.1">Appeal:</span> Make the readers feel inspired by Shepard Smith’s courage to confront political deceit.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.4"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.4.1">Sentence:</span> Smith vociferously questioned the incessant l̈iesänd d̈eceptionẗhat seem to shroud the Trump administration’s dealings.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.5"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.5.1">Technique:</span> repetition</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.6"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.6.1">Appeal:</span> Make the readers realize about the persistent dishonesty in the Trump administration.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.7"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.7.1">Sentence:</span> Implying the audience’s own complacency in the face of such deceit unless they demand accountability.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.8"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.8.1">Technique:</span> false dilemma</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.9"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.9.1">Appeal:</span> Make the readers feel guilty for any lack of action against political deception.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.10"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.10.1">Sentence:</span> “Why all these lies? Why is it lie after lie after lie? If you’re clean, come on clean,”</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.11"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.11.1">Technique:</span> repetition</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.12"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.12.1">Appeal:</span> Make the readers feel outraged at the continuous lies by encouraging them to demand honesty.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.13"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.13.1">Sentence:</span> Smith’s critique, grounded in an urgent plea for transparency,</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.14"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.14.1">Technique:</span> appeal to authority</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.15"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.15.1">Appeal:</span> Make the readers trust in the necessity of transparency for integrity.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.16"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.16.1">Sentence:</span> the sheer implausibility of the evolving narrative woven by Trump Jr. and, by extension, the administration.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.17"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.17.1">Technique:</span> doubt</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.18"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.18.1">Appeal:</span> Make the readers skeptical about any explanations from the Trump administration.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.19"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.19.1">Sentence:</span> voices emerged calling for Smith’s removal from the network2̆014a diversion that starkly contrasts the core issue at hand: the integrity of democratic institutions and the transparency of those in power.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.20"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.20.1">Technique:</span> red herring</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.21"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.21.1">Appeal:</span> Make the readers ignore the calls for Smith’s removal and focus on the larger issues of integrity and transparency.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.22"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.22.1">Sentence:</span> " Smith’s unapologetic defense of the press earlier in the year, where he rebuked claims against CNN as n̈ot fake news,"̈</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.23"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.23.1">Technique:</span> appeal to authority</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.24"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.24.1">Appeal:</span> Make the readers feel supportive of Smith’s stance on defending press integrity.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.25"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.25.1">Sentence:</span> urging the public to remain vigilant, question narratives, and demand nothing short of the truth.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.26"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.26.1">Technique:</span> glittering generalities</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.27"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.27.1">Appeal:</span> Make the readers realize about their role in ensuring political honesty and transparency.</p>
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p4.1.28"><span class="ltx_text ltx_font_bold" id="A6.SS0.SSS0.Px2.p4.1.28.1">Intent:</span> The intent detected is to criticize the Trump administration’s lack of transparency and integrity, particularly focusing on Donald Trump Jr., while highlighting Shepard Smith’s role as a defender of truth and journalistic integrity.</p>
</blockquote>
</div>
</section>
<section class="ltx_paragraph" id="A6.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Analysis for the Presented Result</h4>
<div class="ltx_para" id="A6.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="A6.SS0.SSS0.Px3.p1.1">In the example above, LLMs are making reasonable predictions in terms of
general intent, with Bertscore over 0.9, however, the model gives poor predictions on the sentences which are propagandistic. Though only three sentences are identified as propagandistic, the LLM predicts there is nine. Despite the techniques included covers the ground truth techniques, its low grounding rate causes these predictions to be useless.</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 19 06:22:22 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
