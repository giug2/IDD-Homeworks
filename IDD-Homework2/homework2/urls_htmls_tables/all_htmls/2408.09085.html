<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.09085] Segment Anything with Multiple Modalities</title><meta property="og:description" content="Robust and accurate segmentation of scenes has become one core functionality in various visual recognition and navigation tasks. This has inspired the recent development of Segment Anything Model (SAM), a foundation moâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Segment Anything with Multiple Modalities">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Segment Anything with Multiple Modalities">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.09085">

<!--Generated on Thu Sep  5 17:17:39 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Segment Anything with Multiple Modalities</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aoran Xiao<sup id="id11.11.id1" class="ltx_sup"><span id="id11.11.id1.1" class="ltx_text ltx_font_italic">1âˆ—</span></sup>, Weihao Xuan<sup id="id12.12.id2" class="ltx_sup"><span id="id12.12.id2.1" class="ltx_text ltx_font_italic">2,3âˆ—</span></sup>, Heli Qi<sup id="id13.13.id3" class="ltx_sup">4</sup>, Yun Xing<sup id="id14.14.id4" class="ltx_sup">1</sup>, Naoto Yokoya<sup id="id15.15.id5" class="ltx_sup"><span id="id15.15.id5.1" class="ltx_text ltx_font_italic">2,3â€ </span></sup>, Shijian Lu<sup id="id16.16.id6" class="ltx_sup"><span id="id16.16.id6.1" class="ltx_text ltx_font_italic">1â€ </span></sup>
<br class="ltx_break"><sup id="id17.17.id7" class="ltx_sup">1</sup> Nanyang Technological University, Singapore   <sup id="id18.18.id8" class="ltx_sup">2</sup> The University of Tokyo, Japan 
<br class="ltx_break"><sup id="id19.19.id9" class="ltx_sup">3</sup> RIKEN AIP, Japan   <sup id="id20.20.id10" class="ltx_sup">4</sup> Nara Institute of Science and Technology, Japan 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">\textsuperscript{}</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">\textsuperscript{}</sup><span class="ltx_note_type">footnotetext: </span>* Co-first authors with equal contributions. <sup id="footnotex1.1" class="ltx_sup">â€ </sup> Co-corresponding authors.</span></span></span>
<p id="id21.id1" class="ltx_p">Robust and accurate segmentation of scenes has become one core functionality in various visual recognition and navigation tasks. This has inspired the recent development of Segment Anything Model (SAM), a foundation model for general mask segmentation. However, SAM is largely tailored for single-modal RGB images, limiting its applicability to multi-modal data captured with widely-adopted sensor suites, such as LiDAR plus RGB, depth plus RGB, thermal plus RGB, etc. We develop MM-SAM, an extension and expansion of SAM that supports cross-modal and multi-modal processing for robust and enhanced segmentation with different sensor suites. MM-SAM features two key designs, namely, unsupervised cross-modal transfer and weakly-supervised multi-modal fusion, enabling label-efficient and parameter-efficient adaptation toward various sensor modalities. It addresses three main challenges: 1) adaptation toward diverse non-RGB sensors for single-modal processing, 2) synergistic processing of multi-modal data via sensor fusion, and 3) mask-free training for different downstream tasks. Extensive experiments show that MM-SAM consistently outperforms SAM by large margins, demonstrating its effectiveness and robustness across various sensors and data modalities. Project page: <a target="_blank" href="https://xiaoaoran.github.io/projects/MM-SAM" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://xiaoaoran.github.io/projects/MM-SAM</a></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure ltx_align_floatright"><img src="/html/2408.09085/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">The proposed MM-SAM extends and expands SAM towards multi-modal data with various sensor suites, facilitating cross-modal and multi-modal segmentation without requiring mask annotations in different downstream tasks.</span></figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Leveraging flexible geometric prompts with points, boxes, or coarse masks, the recent Segment Anything Model (SAM)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> has emerged as a state-of-the-art visual foundation model for general mask segmentation.
However, trained over billion-scale RGB image masks, SAM is tailored to optical RGB cameras and it often struggles or even fails while handling other visual sensor modalities.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">This limitation constrains the applicability of SAM, as we are facing increasing multi-modal data and sensor suites that integrate multiple sensors to capture complementary and paired data. It is crucial to extend and expand SAMâ€™s capabilities beyond RGB cameras, to allow leveraging the distinct advantages of different sensors and enhancing perception robustness and accuracy under complicated and dynamic situations.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">This paper presents MM-SAM, a Multi-Modal SAM that extends and expands SAM toward multi-modal data captured with various sensor suites. Our goal, as illustrated in Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, is to adapt pre-trained SAM with lightweight modules to enable cross-modal segmentation for individual sensor modalities and multi-modal segmentation with sensor fusion. To this end, MM-SAM addresses several major challenges while adapting SAM toward multi-modal data:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Adapting SAM for cross-sensor heterogeneous data. We design <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Unsupervised Cross-Modal Transfer</span> (UCMT) that incorporates modal-specific patch embedding module and parameter-efficient tuning into SAMâ€™s image encoder, facilitating the extraction of modal-specific sensor features. UCMT includes an embedding unification loss that enforces unified representations across sensor modalities within the output latent space of SAMâ€™s image encoder, ensuring segmentation compatibility with the prompt encoder and mask decoder. This simple and lightweight design empowers MM-SAM with superior segmentation ability on individual modalities, as demonstrated in FigureÂ <a href="#footnotex3" title="footnote 1 â€£ Figure 2 â€£ 1 Introduction â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text" style="font-size:111%;">1</span></span></a>.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Adapting SAM for synergistic sensor fusion. We design <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Weakly-supervised Multi-Modal Fusion</span> (WMMF), featuring a lightweight selective fusion gate for adaptive fusion of multi-modal embeddings. As illustrated in FigureÂ <a href="#footnotex3" title="footnote 1 â€£ Figure 2 â€£ 1 Introduction â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text" style="font-size:111%;">1</span></span></a>, the selective fusion gate enables effective sensor fusion under complicated and dynamic situations, greatly enhancing segmentation robustness and accuracy as compared with using individual modality alone.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Label-efficient SAM adaptation towards different sensors. MM-SAM requires no mask annotations for adaptation. Specifically, UCMT leverages unlabeled multi-modal data from sensor suites, while WMMF introduces multi-modal pseudo-labeling to train the selective fusion gate with given geometric prompts. The label-efficient adaptation expands MM-SAMâ€™s applicability significantly.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2408.09085/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="121" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.12.5.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.8.4" class="ltx_text" style="font-size:90%;">MM-SAM extends and expands SAM effectively. (a) Activation heatmap and mask predictions for an example from RGB-DÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> with a box prompt in the 1st column. MM-SAM performs clearly better for cross-modal segmentation of depth, and it also enables superb multi-modal segmentation with modality fusion. (b) MM-SAM demonstrate superior robustness and accuracy across seven multi-modal datasets, each featured by RGB plus a non-RGB modality (SAM on RGBÂ [<math id="S1.F2.5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.F2.5.1.m1.1b"><mo mathcolor="#EA3323" id="S1.F2.5.1.m1.1.1" xref="S1.F2.5.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S1.F2.5.1.m1.1c"><ci id="S1.F2.5.1.m1.1.1.cmml" xref="S1.F2.5.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.5.1.m1.1d">\bullet</annotation></semantics></math>], SAM on non-RGB <span id="S1.F2.8.4.1" class="ltx_text ltx_font_italic">X*</span> [<math id="S1.F2.6.2.m2.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.F2.6.2.m2.1b"><mo mathcolor="#68349A" id="S1.F2.6.2.m2.1.1" xref="S1.F2.6.2.m2.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S1.F2.6.2.m2.1c"><ci id="S1.F2.6.2.m2.1.1.cmml" xref="S1.F2.6.2.m2.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.6.2.m2.1d">\bullet</annotation></semantics></math>], MM-SAM on non-RGB <span id="S1.F2.8.4.2" class="ltx_text ltx_font_italic">X</span> with cross-modal adaptation [<math id="S1.F2.7.3.m3.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.F2.7.3.m3.1b"><mo mathcolor="#1A6B23" id="S1.F2.7.3.m3.1.1" xref="S1.F2.7.3.m3.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S1.F2.7.3.m3.1c"><ci id="S1.F2.7.3.m3.1.1.cmml" xref="S1.F2.7.3.m3.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.7.3.m3.1d">\bullet</annotation></semantics></math>] and MM-SAM on RGB + non-RGB with multi-modal fusion [<math id="S1.F2.8.4.m4.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.F2.8.4.m4.1b"><mo mathcolor="#0770C0" id="S1.F2.8.4.m4.1.1" xref="S1.F2.8.4.m4.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S1.F2.8.4.m4.1c"><ci id="S1.F2.8.4.m4.1.1.cmml" xref="S1.F2.8.4.m4.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.8.4.m4.1d">\bullet</annotation></semantics></math>]). The symbol * denotes false-color images<span id="footnotex3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnotex3.1.1.1" class="ltx_text" style="font-size:111%;">1</span></span>Non-RGB modality data are converted into false-color images with three channels to meet SAMâ€™s input requirements for zero-shot segmentation. See AppendixÂ <a href="#A2.SS1" title="B.1 Data Representations â€£ Appendix B Datasets and Metrics â€£ Segment Anything with Multiple Modalities" class="ltx_ref" style="font-size:111%;"><span class="ltx_text ltx_ref_tag">B.1</span></a> for details.</span></span></span> transformed from each non-RGB modality. The radius is normalized by MM-SAMâ€™s multi-modal segmentation scores. Bigger area coverage indicates better segmentation. Best viewed in color.</span></figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The contribution of this study could be summarized into three major aspects. <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">First</span>, we design MM-SAM, an extension and expansion of SAM toward multi-modal sensor suites that tackles three challenges in cross-sensor adaptation, multi-sensor fusion, and mask-free adaptation effectively. To the best of our knowledge, this is the first work that explores visual foundation models for sensor suites. <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">Second</span>, we design unsupervised cross-modal transfer and weakly-supervised multi-modal fusion, facilitating parameter-efficient and label-efficient adaptation to various downstream tasks and sensors. <span id="S1.p4.1.3" class="ltx_text ltx_font_italic">Third</span>, serving as a general and versatile adaptation pipeline, MM-SAM demonstrates superior cross-modal and multi-modal segmentation performance over multiple widely adopted sensor suites.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Image Segmentation Foundation Model.</span> Scaling up deep neural networks has led to impressive advancements across various recognition tasks, inspiring the development of language and vision-language foundation models pre-trained on web-scale datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>, as well as vision foundation models such as SAMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> and DINO v2Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>. Among these foundation models, SAM is notable for its ability to perform zero-shot mask segmentation with flexible geometric prompts. Several studies explore adapting SAM to various specialized domainsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> such as medical images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>, camouflaged objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>, thin structures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>, and optical RGB remote sensing images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>. Additionally, some research expands SAMâ€™s capabilities beyond binary mask segmentation such as semantic recognitionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>, <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>, <a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite> and pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>. Efforts have also been made to enhance SAM towards more efficient and lightweight modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>, <a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">On the other hand, SAM is constrained to RGB cameras due to its training on large-scale RGB image masks. Recent studies attempt to mitigate this limitation by transforming non-RGB data into false-color images to align with SAMâ€™s input requirements <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> or re-training SAM with newly annotated data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>. However, data transformation can result in information loss and discrepancies with SAMâ€™s training distribution, leading to suboptimal segmentation performance. Re-training the model, on the other hand, is labor-intensive due to the significant effort in data collection and annotation. Given the prevalence of various sensor suites in perception tasks, it is crucial to extend SAMâ€™s capabilities to handle non-RGB and multi-modal data. MM-SAM is designed to fill this gap, enabling seamless integration of SAM with various sensor suites.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Efficient Tuning</span> of foundation models has become more critical due to their growing size and high costs of deploying separate models for each task. Two primary approaches have been explored. The first is <span id="S2.p3.1.2" class="ltx_text ltx_font_italic">parameter-efficient tuning</span>, such as Low-Rank Adaptation (LoRA)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>, prompt tuningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>, and adaptersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>, which work by freezing the core model and introducing a minimal number of learnable parameters. The second is <span id="S2.p3.1.3" class="ltx_text ltx_font_italic">data-efficient tuning</span>, such as few-shot learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> and weakly-supervised domain adaptationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>, which aims to achieve desired accuracy with minimal training data or annotations. While existing studies primarily focus on single-modal efficient tuning, the proposed MM-SAM aims to adapt for cross-modal and multi-modal processing while being parameter-efficient and label-efficient concurrently.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Multi-Modal Fusion.</span> Fusing multi-modal data holds great promise due to the complementary nature of their captured information. However, it is a challenging task as multi-modal data are often heterogeneous <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> and involve complicated calibration and alignment in capturing and processingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>. Though multi-modal fusion has demonstrated clear advantages in various visual detection and recognition tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>, <a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>, <a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>, how to exploit it with visual foundation models remains largely unexplored. We design MM-SAM to tackle this challenge by extending and expanding SAM toward effective fusion of multi-modal data. One of its unique features is it requires no ground-truth annotations for data of various new modalities, broadening its applicability significantly in various downstream tasks.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Preliminaries</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">Segment Anything Model.</span> SAMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> consists of three key modules for image mask segmentation: a heavyweight <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">image encoder</span> (i.e., ViTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>) that encodes input images into image embeddings, a lightweight <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_italic">prompt encoder</span> that encodes geometric prompts (such as points, boxes, or coarse masks) into prompt embeddings, and a lightweight <span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_italic">mask decoder</span> that combines these embeddings to predict segmentation masks. SAM is trained on the SA-1B dataset, which includes over 11 million RGB images with 1.1 billion mask annotations. More details about SAM are described inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>. This work aims to extend and expand SAM toward cross-modal and multi-modal segmentation tasks, addressing the challenge of deploying SAM for various sensor suites.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Sensor Suites with Modality Pairs.</span> A sensor suite is a collection of sensors deployed together within a system to capture data from different modalities for comprehensive sensing. This paper focuses on visual sensors such as RGB cameras and LiDAR scanners, widely used in visual recognition and navigation tasks. The multi-modal data captured by these sensors is naturally paired in space. We cover two main categories of sensor suites: 1) <span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_italic">Time-synchronized</span> suites, where multiple sensors are calibrated on a unified platform for simultaneous data collection; and 2) <span id="S3.SS1.p2.1.3" class="ltx_text ltx_font_italic">Time-asynchronous</span> suites, where sensors are mounted on disparate platforms, capturing data at different times and perspectives but aligned through geographic coordinates. Representative examples include remote sensing sensors for earth observation. More details on the datasets are provided in Section <a href="#S4.SS1" title="4.1 Experiment Setup â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2408.09085/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="208" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.4.2.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.2.1" class="ltx_text" style="font-size:90%;">Overview of MM-SAM. MM-SAM freezes the entire SAM architecture while tuning it with multi-modal pairs (RGB and non-RGB modal X) for achieving cross-modal and multi-modal segmentation. It consists of two novel tuning modules: 1) Unsupervised Cross-Modal Transfer (UCMT) introduces modality-specific patch embedding module and low-rank (LoRA) structures into SAMâ€™s image encoder for extracting modality-specific X embeddings. An embedding unification loss (<math id="S3.F3.2.1.m1.1" class="ltx_Math" alttext="L_{U}" display="inline"><semantics id="S3.F3.2.1.m1.1b"><msub id="S3.F3.2.1.m1.1.1" xref="S3.F3.2.1.m1.1.1.cmml"><mi id="S3.F3.2.1.m1.1.1.2" xref="S3.F3.2.1.m1.1.1.2.cmml">L</mi><mi id="S3.F3.2.1.m1.1.1.3" xref="S3.F3.2.1.m1.1.1.3.cmml">U</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F3.2.1.m1.1c"><apply id="S3.F3.2.1.m1.1.1.cmml" xref="S3.F3.2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F3.2.1.m1.1.1.1.cmml" xref="S3.F3.2.1.m1.1.1">subscript</csymbol><ci id="S3.F3.2.1.m1.1.1.2.cmml" xref="S3.F3.2.1.m1.1.1.2">ğ¿</ci><ci id="S3.F3.2.1.m1.1.1.3.cmml" xref="S3.F3.2.1.m1.1.1.3">ğ‘ˆ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.2.1.m1.1d">L_{U}</annotation></semantics></math>) aligns X embeddings with SAMâ€™s RGB image embeddings to ensure segmentation compatibility; 2) Weakly-supervised Multi-Modal Fusion (WMMF) incorporates Selective Fusion Gate (SFG) to generate multi-modal embeddings, trained with multi-modal pseudo-labeling for adaptive sensor fusion. The whole training is mask-free. During inference, MM-SAM supports segmentation for single or multiple modality data.</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>MM-SAM</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The main objective of MM-SAM design is to adapt SAMâ€™s image encoder to handle other modalities within SAMâ€™s segmentation pipeline. This requires the adapted image encoders to effectively encode modality-specific embeddings while maintaining segmentation compatibility, enabling seamless integration with SAMâ€™s prompt encoder and mask decoder for cross-modal segmentation. To this end, we directly align embeddings of non-RGB modalities with paired RGB embeddings, ensuring unified representations across sensor modalities within the latent space of SAMâ€™s image encoder.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">This strategy offers three key advantages: 1) It only adapts the image encoder, leaving the prompt encoder and mask decoder unchanged, minimizing the addition of parameters to SAMâ€™s architecture. 2) It fully utilizes SAMâ€™s powerful image encoder pre-trained on billion-scale RGB masks since such extensive training data is nearly impossible to obtain for other modalities. 3) The unified embedding space across sensor modalities simplifies multi-modal fusion, as detailed in SectionÂ <a href="#S3.SS2.SSS2" title="3.2.2 Multi-Modal Segmentation with WMMF â€£ 3.2 MM-SAM â€£ 3 Methodology â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">The overall pipeline of MM-SAM is depicted in Figure <a href="#S3.F3" title="Figure 3 â€£ 3.1 Preliminaries â€£ 3 Methodology â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
Built upon the frozen SAM architecture, MM-SAM inherits SAMâ€™s powerful zero-shot segmentation capabilities for RGB images. Additionally, it introduces two key modules for parameter-efficient and label-efficient adaptation: <span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Unsupervised Cross-Modal Transfer</span> (UCMT) for cross-modal segmentation and <span id="S3.SS2.p3.1.2" class="ltx_text ltx_font_bold">Weakly-supervised Multi-Modal Fusion</span> (WMMF) for multi-modal segmentation.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Cross-Modal Segmentation with UCMT</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.7" class="ltx_p">As depicted in FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3.1 Preliminaries â€£ 3 Methodology â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, MM-SAM operates on pairs of modalities <math id="S3.SS2.SSS1.p1.1.m1.2" class="ltx_Math" alttext="(I,X)" display="inline"><semantics id="S3.SS2.SSS1.p1.1.m1.2a"><mrow id="S3.SS2.SSS1.p1.1.m1.2.3.2" xref="S3.SS2.SSS1.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p1.1.m1.2.3.2.1" xref="S3.SS2.SSS1.p1.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml">I</mi><mo id="S3.SS2.SSS1.p1.1.m1.2.3.2.2" xref="S3.SS2.SSS1.p1.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS2.SSS1.p1.1.m1.2.2" xref="S3.SS2.SSS1.p1.1.m1.2.2.cmml">X</mi><mo stretchy="false" id="S3.SS2.SSS1.p1.1.m1.2.3.2.3" xref="S3.SS2.SSS1.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.2b"><interval closure="open" id="S3.SS2.SSS1.p1.1.m1.2.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.2.3.2"><ci id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">ğ¼</ci><ci id="S3.SS2.SSS1.p1.1.m1.2.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.2.2">ğ‘‹</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.2c">(I,X)</annotation></semantics></math> from sensor suites, where <math id="S3.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><mi id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><ci id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">I</annotation></semantics></math> represents RGB images and <math id="S3.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS2.SSS1.p1.3.m3.1a"><mi id="S3.SS2.SSS1.p1.3.m3.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.3.m3.1b"><ci id="S3.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.3.m3.1c">X</annotation></semantics></math> denotes its corresponding observation in another modality.
Similar to how SAM processes RGB images, <math id="S3.SS2.SSS1.p1.4.m4.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS2.SSS1.p1.4.m4.1a"><mi id="S3.SS2.SSS1.p1.4.m4.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.4.m4.1b"><ci id="S3.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.4.m4.1c">X</annotation></semantics></math> is divided into fixed-sized patches with matching spatial resolutions.
To process <math id="S3.SS2.SSS1.p1.5.m5.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS2.SSS1.p1.5.m5.1a"><mi id="S3.SS2.SSS1.p1.5.m5.1.1" xref="S3.SS2.SSS1.p1.5.m5.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.5.m5.1b"><ci id="S3.SS2.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.5.m5.1c">X</annotation></semantics></math> directly, we introduce a trainable patch embedding module at the beginning of the ViT architecture, adjusting input channel numbers to match <math id="S3.SS2.SSS1.p1.6.m6.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS2.SSS1.p1.6.m6.1a"><mi id="S3.SS2.SSS1.p1.6.m6.1.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.6.m6.1b"><ci id="S3.SS2.SSS1.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.6.m6.1c">X</annotation></semantics></math> while maintaining the output channel number consistent with SAMâ€™s original patch embedding module for RGB images.
In addition, we introduce parameter-efficient tuning structures in the backbone to adaptively encode modal-specific features from <math id="S3.SS2.SSS1.p1.7.m7.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS2.SSS1.p1.7.m7.1a"><mi id="S3.SS2.SSS1.p1.7.m7.1.1" xref="S3.SS2.SSS1.p1.7.m7.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.7.m7.1b"><ci id="S3.SS2.SSS1.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS1.p1.7.m7.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.7.m7.1c">X</annotation></semantics></math>.
Specifically, we use LoRAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite> in each transformer block of ViT for its efficiency and lightweight nature (see SectionÂ <a href="#S4.SS3" title="4.3 Discussions and Analysis â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>). More details are provided in Appendix <a href="#A1.SS1" title="A.1 Image encoder for non-RGB modalities â€£ Appendix A MM-SAM detailed structure â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.3" class="ltx_p">Once <math id="S3.SS2.SSS1.p2.1.m1.2" class="ltx_Math" alttext="(I,X)" display="inline"><semantics id="S3.SS2.SSS1.p2.1.m1.2a"><mrow id="S3.SS2.SSS1.p2.1.m1.2.3.2" xref="S3.SS2.SSS1.p2.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p2.1.m1.2.3.2.1" xref="S3.SS2.SSS1.p2.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS2.SSS1.p2.1.m1.1.1" xref="S3.SS2.SSS1.p2.1.m1.1.1.cmml">I</mi><mo id="S3.SS2.SSS1.p2.1.m1.2.3.2.2" xref="S3.SS2.SSS1.p2.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS2.SSS1.p2.1.m1.2.2" xref="S3.SS2.SSS1.p2.1.m1.2.2.cmml">X</mi><mo stretchy="false" id="S3.SS2.SSS1.p2.1.m1.2.3.2.3" xref="S3.SS2.SSS1.p2.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.1.m1.2b"><interval closure="open" id="S3.SS2.SSS1.p2.1.m1.2.3.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.2.3.2"><ci id="S3.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1">ğ¼</ci><ci id="S3.SS2.SSS1.p2.1.m1.2.2.cmml" xref="S3.SS2.SSS1.p2.1.m1.2.2">ğ‘‹</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.1.m1.2c">(I,X)</annotation></semantics></math> are encoded into image and X embeddings <math id="S3.SS2.SSS1.p2.2.m2.1" class="ltx_Math" alttext="e_{I}" display="inline"><semantics id="S3.SS2.SSS1.p2.2.m2.1a"><msub id="S3.SS2.SSS1.p2.2.m2.1.1" xref="S3.SS2.SSS1.p2.2.m2.1.1.cmml"><mi id="S3.SS2.SSS1.p2.2.m2.1.1.2" xref="S3.SS2.SSS1.p2.2.m2.1.1.2.cmml">e</mi><mi id="S3.SS2.SSS1.p2.2.m2.1.1.3" xref="S3.SS2.SSS1.p2.2.m2.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.2.m2.1b"><apply id="S3.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.2">ğ‘’</ci><ci id="S3.SS2.SSS1.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1.3">ğ¼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.2.m2.1c">e_{I}</annotation></semantics></math>, <math id="S3.SS2.SSS1.p2.3.m3.1" class="ltx_Math" alttext="e_{X}" display="inline"><semantics id="S3.SS2.SSS1.p2.3.m3.1a"><msub id="S3.SS2.SSS1.p2.3.m3.1.1" xref="S3.SS2.SSS1.p2.3.m3.1.1.cmml"><mi id="S3.SS2.SSS1.p2.3.m3.1.1.2" xref="S3.SS2.SSS1.p2.3.m3.1.1.2.cmml">e</mi><mi id="S3.SS2.SSS1.p2.3.m3.1.1.3" xref="S3.SS2.SSS1.p2.3.m3.1.1.3.cmml">X</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.3.m3.1b"><apply id="S3.SS2.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p2.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.1.2">ğ‘’</ci><ci id="S3.SS2.SSS1.p2.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.1.3">ğ‘‹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.3.m3.1c">e_{X}</annotation></semantics></math>, UCMT optimizes the trainable parameters through unsupervised embedding unification:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="L_{U}=||e_{I}-e_{X}||_{2}^{2}." display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml">L</mi><mi id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml">U</mi></msub><mo id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">=</mo><msubsup id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">e</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">I</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">e</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">X</mi></msub></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">2</mn><mn id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml">2</mn></msubsup></mrow><mo lspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></eq><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2">ğ¿</ci><ci id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3">ğ‘ˆ</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2">ğ‘’</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3">ğ¼</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.2">ğ‘’</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.3">ğ‘‹</ci></apply></apply></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">2</cn></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">L_{U}=||e_{I}-e_{X}||_{2}^{2}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS1.p2.4" class="ltx_p">Minimizing <math id="S3.SS2.SSS1.p2.4.m1.1" class="ltx_Math" alttext="L_{U}" display="inline"><semantics id="S3.SS2.SSS1.p2.4.m1.1a"><msub id="S3.SS2.SSS1.p2.4.m1.1.1" xref="S3.SS2.SSS1.p2.4.m1.1.1.cmml"><mi id="S3.SS2.SSS1.p2.4.m1.1.1.2" xref="S3.SS2.SSS1.p2.4.m1.1.1.2.cmml">L</mi><mi id="S3.SS2.SSS1.p2.4.m1.1.1.3" xref="S3.SS2.SSS1.p2.4.m1.1.1.3.cmml">U</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.4.m1.1b"><apply id="S3.SS2.SSS1.p2.4.m1.1.1.cmml" xref="S3.SS2.SSS1.p2.4.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.4.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p2.4.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p2.4.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p2.4.m1.1.1.2">ğ¿</ci><ci id="S3.SS2.SSS1.p2.4.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p2.4.m1.1.1.3">ğ‘ˆ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.4.m1.1c">L_{U}</annotation></semantics></math> ensures that X-modal embeddings closely align with the established RGB embedding space from SAMâ€™s image encoder. This alignment ensures compatibility with SAMâ€™s prompt encoder and mask decoder, enabling seamless integration into SAMâ€™s segmentation pipeline.
Despite its simplicity, this alignment approach achieves robust and superior adaptation across various non-RGB modalities, as detailed results in SectionÂ <a href="#S4" title="4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Multi-Modal Segmentation with WMMF</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">WMMF, similar to UCMT, operates in the output embedding space of the image encoder and fuses data of multiple sensor modalities to generate more comprehensive embeddings. The core idea is to generate patch-wise weights conditioned on all input sensor modalities, enabling a weighted fusion of paired embeddings. This ensures robust sensor fusion and multi-modal segmentation that adapts to varying conditions. As illustrated in Figure <a href="#S3.F3" title="Figure 3 â€£ 3.1 Preliminaries â€£ 3 Methodology â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, WMMF introduces two innovative designs to achieve multi-modality fusion, namely, Selective Fusion Gate (SFG) for multi-modal fusion and multi-modal pseudo-labeling for mask-free training.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.6" class="ltx_p"><span id="S3.SS2.SSS2.p2.6.1" class="ltx_text ltx_font_bold">Selective Fusion Gate (SFG).</span> We concatenate embeddings <math id="S3.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="e_{I}" display="inline"><semantics id="S3.SS2.SSS2.p2.1.m1.1a"><msub id="S3.SS2.SSS2.p2.1.m1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.SSS2.p2.1.m1.1.1.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.2.cmml">e</mi><mi id="S3.SS2.SSS2.p2.1.m1.1.1.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.1.m1.1b"><apply id="S3.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.2">ğ‘’</ci><ci id="S3.SS2.SSS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3">ğ¼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.1.m1.1c">e_{I}</annotation></semantics></math> and <math id="S3.SS2.SSS2.p2.2.m2.1" class="ltx_Math" alttext="e_{X}" display="inline"><semantics id="S3.SS2.SSS2.p2.2.m2.1a"><msub id="S3.SS2.SSS2.p2.2.m2.1.1" xref="S3.SS2.SSS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.SSS2.p2.2.m2.1.1.2" xref="S3.SS2.SSS2.p2.2.m2.1.1.2.cmml">e</mi><mi id="S3.SS2.SSS2.p2.2.m2.1.1.3" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.cmml">X</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.2.m2.1b"><apply id="S3.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.2">ğ‘’</ci><ci id="S3.SS2.SSS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.3">ğ‘‹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.2.m2.1c">e_{X}</annotation></semantics></math> to form embedding <math id="S3.SS2.SSS2.p2.3.m3.1" class="ltx_Math" alttext="e_{F}" display="inline"><semantics id="S3.SS2.SSS2.p2.3.m3.1a"><msub id="S3.SS2.SSS2.p2.3.m3.1.1" xref="S3.SS2.SSS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.SSS2.p2.3.m3.1.1.2" xref="S3.SS2.SSS2.p2.3.m3.1.1.2.cmml">e</mi><mi id="S3.SS2.SSS2.p2.3.m3.1.1.3" xref="S3.SS2.SSS2.p2.3.m3.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.3.m3.1b"><apply id="S3.SS2.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1.2">ğ‘’</ci><ci id="S3.SS2.SSS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1.3">ğ¹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.3.m3.1c">e_{F}</annotation></semantics></math> and forward it to a weight filter comprising a two-layer convolution sub-network followed by a softmax layer. The outcome of the weight filter, i.e., the weights <math id="S3.SS2.SSS2.p2.4.m4.1" class="ltx_Math" alttext="\omega" display="inline"><semantics id="S3.SS2.SSS2.p2.4.m4.1a"><mi id="S3.SS2.SSS2.p2.4.m4.1.1" xref="S3.SS2.SSS2.p2.4.m4.1.1.cmml">Ï‰</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.4.m4.1b"><ci id="S3.SS2.SSS2.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS2.p2.4.m4.1.1">ğœ”</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.4.m4.1c">\omega</annotation></semantics></math>, is applied to perform a patch-wise weighted average of the embedding <math id="S3.SS2.SSS2.p2.5.m5.1" class="ltx_Math" alttext="e_{F}" display="inline"><semantics id="S3.SS2.SSS2.p2.5.m5.1a"><msub id="S3.SS2.SSS2.p2.5.m5.1.1" xref="S3.SS2.SSS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.SSS2.p2.5.m5.1.1.2" xref="S3.SS2.SSS2.p2.5.m5.1.1.2.cmml">e</mi><mi id="S3.SS2.SSS2.p2.5.m5.1.1.3" xref="S3.SS2.SSS2.p2.5.m5.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.5.m5.1b"><apply id="S3.SS2.SSS2.p2.5.m5.1.1.cmml" xref="S3.SS2.SSS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.SSS2.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.SSS2.p2.5.m5.1.1.2">ğ‘’</ci><ci id="S3.SS2.SSS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.SSS2.p2.5.m5.1.1.3">ğ¹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.5.m5.1c">e_{F}</annotation></semantics></math>, producing the multi-modal embeddings <math id="S3.SS2.SSS2.p2.6.m6.1" class="ltx_Math" alttext="\hat{e}_{F}" display="inline"><semantics id="S3.SS2.SSS2.p2.6.m6.1a"><msub id="S3.SS2.SSS2.p2.6.m6.1.1" xref="S3.SS2.SSS2.p2.6.m6.1.1.cmml"><mover accent="true" id="S3.SS2.SSS2.p2.6.m6.1.1.2" xref="S3.SS2.SSS2.p2.6.m6.1.1.2.cmml"><mi id="S3.SS2.SSS2.p2.6.m6.1.1.2.2" xref="S3.SS2.SSS2.p2.6.m6.1.1.2.2.cmml">e</mi><mo id="S3.SS2.SSS2.p2.6.m6.1.1.2.1" xref="S3.SS2.SSS2.p2.6.m6.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.SSS2.p2.6.m6.1.1.3" xref="S3.SS2.SSS2.p2.6.m6.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.6.m6.1b"><apply id="S3.SS2.SSS2.p2.6.m6.1.1.cmml" xref="S3.SS2.SSS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.SSS2.p2.6.m6.1.1">subscript</csymbol><apply id="S3.SS2.SSS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.SSS2.p2.6.m6.1.1.2"><ci id="S3.SS2.SSS2.p2.6.m6.1.1.2.1.cmml" xref="S3.SS2.SSS2.p2.6.m6.1.1.2.1">^</ci><ci id="S3.SS2.SSS2.p2.6.m6.1.1.2.2.cmml" xref="S3.SS2.SSS2.p2.6.m6.1.1.2.2">ğ‘’</ci></apply><ci id="S3.SS2.SSS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.SSS2.p2.6.m6.1.1.3">ğ¹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.6.m6.1c">\hat{e}_{F}</annotation></semantics></math>, i.e.,</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\hat{e}_{F}=\omega e_{F}=\omega_{i}e_{I_{i}}+(1-\omega_{i})e_{X_{i}}," display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.1.3.2.2.cmml">e</mi><mo id="S3.E2.m1.1.1.1.1.3.2.1" xref="S3.E2.m1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml">F</mi></msub><mo id="S3.E2.m1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.4.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.5" xref="S3.E2.m1.1.1.1.1.5.cmml"><mi id="S3.E2.m1.1.1.1.1.5.2" xref="S3.E2.m1.1.1.1.1.5.2.cmml">Ï‰</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.5.1" xref="S3.E2.m1.1.1.1.1.5.1.cmml">â€‹</mo><msub id="S3.E2.m1.1.1.1.1.5.3" xref="S3.E2.m1.1.1.1.1.5.3.cmml"><mi id="S3.E2.m1.1.1.1.1.5.3.2" xref="S3.E2.m1.1.1.1.1.5.3.2.cmml">e</mi><mi id="S3.E2.m1.1.1.1.1.5.3.3" xref="S3.E2.m1.1.1.1.1.5.3.3.cmml">F</mi></msub></mrow><mo id="S3.E2.m1.1.1.1.1.6" xref="S3.E2.m1.1.1.1.1.6.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3.cmml"><msub id="S3.E2.m1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.3.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.1.1.3.2.2.cmml">Ï‰</mi><mi id="S3.E2.m1.1.1.1.1.1.3.2.3" xref="S3.E2.m1.1.1.1.1.1.3.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.3.1.cmml">â€‹</mo><msub id="S3.E2.m1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.1.3.3.2.cmml">e</mi><msub id="S3.E2.m1.1.1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.1.1.3.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.3.3.3.2" xref="S3.E2.m1.1.1.1.1.1.3.3.3.2.cmml">I</mi><mi id="S3.E2.m1.1.1.1.1.1.3.3.3.3" xref="S3.E2.m1.1.1.1.1.1.3.3.3.3.cmml">i</mi></msub></msub></mrow><mo id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml">+</mo><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mn id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">Ï‰</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">â€‹</mo><msub id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.3.2.cmml">e</mi><msub id="S3.E2.m1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.1.1.3.3.2.cmml">X</mi><mi id="S3.E2.m1.1.1.1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.1.1.1.3.3.3.cmml">i</mi></msub></msub></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><and id="S3.E2.m1.1.1.1.1a.cmml" xref="S3.E2.m1.1.1.1"></and><apply id="S3.E2.m1.1.1.1.1b.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.4"></eq><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2"><ci id="S3.E2.m1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2.1">^</ci><ci id="S3.E2.m1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2">ğ‘’</ci></apply><ci id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3">ğ¹</ci></apply><apply id="S3.E2.m1.1.1.1.1.5.cmml" xref="S3.E2.m1.1.1.1.1.5"><times id="S3.E2.m1.1.1.1.1.5.1.cmml" xref="S3.E2.m1.1.1.1.1.5.1"></times><ci id="S3.E2.m1.1.1.1.1.5.2.cmml" xref="S3.E2.m1.1.1.1.1.5.2">ğœ”</ci><apply id="S3.E2.m1.1.1.1.1.5.3.cmml" xref="S3.E2.m1.1.1.1.1.5.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.5.3.1.cmml" xref="S3.E2.m1.1.1.1.1.5.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.5.3.2.cmml" xref="S3.E2.m1.1.1.1.1.5.3.2">ğ‘’</ci><ci id="S3.E2.m1.1.1.1.1.5.3.3.cmml" xref="S3.E2.m1.1.1.1.1.5.3.3">ğ¹</ci></apply></apply></apply><apply id="S3.E2.m1.1.1.1.1c.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.6.cmml" xref="S3.E2.m1.1.1.1.1.6"></eq><share href="#S3.E2.m1.1.1.1.1.5.cmml" id="S3.E2.m1.1.1.1.1d.cmml" xref="S3.E2.m1.1.1.1"></share><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><plus id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"></plus><apply id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3"><times id="S3.E2.m1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.3.1"></times><apply id="S3.E2.m1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.3.2.2">ğœ”</ci><ci id="S3.E2.m1.1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3.2.3">ğ‘–</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3.2">ğ‘’</ci><apply id="S3.E2.m1.1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3.3.2">ğ¼</ci><ci id="S3.E2.m1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3.3.3">ğ‘–</ci></apply></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"><minus id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2">1</cn><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2">ğœ”</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.2">ğ‘’</ci><apply id="S3.E2.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.3.2">ğ‘‹</ci><ci id="S3.E2.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.3.3">ğ‘–</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\hat{e}_{F}=\omega e_{F}=\omega_{i}e_{I_{i}}+(1-\omega_{i})e_{X_{i}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS2.p2.11" class="ltx_p">where <math id="S3.SS2.SSS2.p2.7.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.SSS2.p2.7.m1.1a"><mi id="S3.SS2.SSS2.p2.7.m1.1.1" xref="S3.SS2.SSS2.p2.7.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.7.m1.1b"><ci id="S3.SS2.SSS2.p2.7.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.7.m1.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.7.m1.1c">i</annotation></semantics></math> denotes the patch index. Similar to <math id="S3.SS2.SSS2.p2.8.m2.1" class="ltx_Math" alttext="e_{I}" display="inline"><semantics id="S3.SS2.SSS2.p2.8.m2.1a"><msub id="S3.SS2.SSS2.p2.8.m2.1.1" xref="S3.SS2.SSS2.p2.8.m2.1.1.cmml"><mi id="S3.SS2.SSS2.p2.8.m2.1.1.2" xref="S3.SS2.SSS2.p2.8.m2.1.1.2.cmml">e</mi><mi id="S3.SS2.SSS2.p2.8.m2.1.1.3" xref="S3.SS2.SSS2.p2.8.m2.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.8.m2.1b"><apply id="S3.SS2.SSS2.p2.8.m2.1.1.cmml" xref="S3.SS2.SSS2.p2.8.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.8.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p2.8.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.8.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p2.8.m2.1.1.2">ğ‘’</ci><ci id="S3.SS2.SSS2.p2.8.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p2.8.m2.1.1.3">ğ¼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.8.m2.1c">e_{I}</annotation></semantics></math> and <math id="S3.SS2.SSS2.p2.9.m3.1" class="ltx_Math" alttext="e_{X}" display="inline"><semantics id="S3.SS2.SSS2.p2.9.m3.1a"><msub id="S3.SS2.SSS2.p2.9.m3.1.1" xref="S3.SS2.SSS2.p2.9.m3.1.1.cmml"><mi id="S3.SS2.SSS2.p2.9.m3.1.1.2" xref="S3.SS2.SSS2.p2.9.m3.1.1.2.cmml">e</mi><mi id="S3.SS2.SSS2.p2.9.m3.1.1.3" xref="S3.SS2.SSS2.p2.9.m3.1.1.3.cmml">X</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.9.m3.1b"><apply id="S3.SS2.SSS2.p2.9.m3.1.1.cmml" xref="S3.SS2.SSS2.p2.9.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.9.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p2.9.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.9.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p2.9.m3.1.1.2">ğ‘’</ci><ci id="S3.SS2.SSS2.p2.9.m3.1.1.3.cmml" xref="S3.SS2.SSS2.p2.9.m3.1.1.3">ğ‘‹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.9.m3.1c">e_{X}</annotation></semantics></math>, <math id="S3.SS2.SSS2.p2.10.m4.1" class="ltx_Math" alttext="\hat{e}_{F}" display="inline"><semantics id="S3.SS2.SSS2.p2.10.m4.1a"><msub id="S3.SS2.SSS2.p2.10.m4.1.1" xref="S3.SS2.SSS2.p2.10.m4.1.1.cmml"><mover accent="true" id="S3.SS2.SSS2.p2.10.m4.1.1.2" xref="S3.SS2.SSS2.p2.10.m4.1.1.2.cmml"><mi id="S3.SS2.SSS2.p2.10.m4.1.1.2.2" xref="S3.SS2.SSS2.p2.10.m4.1.1.2.2.cmml">e</mi><mo id="S3.SS2.SSS2.p2.10.m4.1.1.2.1" xref="S3.SS2.SSS2.p2.10.m4.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.SSS2.p2.10.m4.1.1.3" xref="S3.SS2.SSS2.p2.10.m4.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.10.m4.1b"><apply id="S3.SS2.SSS2.p2.10.m4.1.1.cmml" xref="S3.SS2.SSS2.p2.10.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.10.m4.1.1.1.cmml" xref="S3.SS2.SSS2.p2.10.m4.1.1">subscript</csymbol><apply id="S3.SS2.SSS2.p2.10.m4.1.1.2.cmml" xref="S3.SS2.SSS2.p2.10.m4.1.1.2"><ci id="S3.SS2.SSS2.p2.10.m4.1.1.2.1.cmml" xref="S3.SS2.SSS2.p2.10.m4.1.1.2.1">^</ci><ci id="S3.SS2.SSS2.p2.10.m4.1.1.2.2.cmml" xref="S3.SS2.SSS2.p2.10.m4.1.1.2.2">ğ‘’</ci></apply><ci id="S3.SS2.SSS2.p2.10.m4.1.1.3.cmml" xref="S3.SS2.SSS2.p2.10.m4.1.1.3">ğ¹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.10.m4.1c">\hat{e}_{F}</annotation></semantics></math> can be integrated with SAMâ€™s prompt embeddings and jointly fed into the mask decoder for refined mask prediction <math id="S3.SS2.SSS2.p2.11.m5.1" class="ltx_Math" alttext="\hat{M}_{F}" display="inline"><semantics id="S3.SS2.SSS2.p2.11.m5.1a"><msub id="S3.SS2.SSS2.p2.11.m5.1.1" xref="S3.SS2.SSS2.p2.11.m5.1.1.cmml"><mover accent="true" id="S3.SS2.SSS2.p2.11.m5.1.1.2" xref="S3.SS2.SSS2.p2.11.m5.1.1.2.cmml"><mi id="S3.SS2.SSS2.p2.11.m5.1.1.2.2" xref="S3.SS2.SSS2.p2.11.m5.1.1.2.2.cmml">M</mi><mo id="S3.SS2.SSS2.p2.11.m5.1.1.2.1" xref="S3.SS2.SSS2.p2.11.m5.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.SSS2.p2.11.m5.1.1.3" xref="S3.SS2.SSS2.p2.11.m5.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.11.m5.1b"><apply id="S3.SS2.SSS2.p2.11.m5.1.1.cmml" xref="S3.SS2.SSS2.p2.11.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.11.m5.1.1.1.cmml" xref="S3.SS2.SSS2.p2.11.m5.1.1">subscript</csymbol><apply id="S3.SS2.SSS2.p2.11.m5.1.1.2.cmml" xref="S3.SS2.SSS2.p2.11.m5.1.1.2"><ci id="S3.SS2.SSS2.p2.11.m5.1.1.2.1.cmml" xref="S3.SS2.SSS2.p2.11.m5.1.1.2.1">^</ci><ci id="S3.SS2.SSS2.p2.11.m5.1.1.2.2.cmml" xref="S3.SS2.SSS2.p2.11.m5.1.1.2.2">ğ‘€</ci></apply><ci id="S3.SS2.SSS2.p2.11.m5.1.1.3.cmml" xref="S3.SS2.SSS2.p2.11.m5.1.1.3">ğ¹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.11.m5.1c">\hat{M}_{F}</annotation></semantics></math>. More details about the SFG structure are provided in AppendixÂ <a href="#A1.SS2" title="A.2 Selective Fusion Gate â€£ Appendix A MM-SAM detailed structure â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a>.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.4" class="ltx_p"><span id="S3.SS2.SSS2.p3.4.1" class="ltx_text ltx_font_bold">Multi-Modal Pseudo Labeling.</span> While supervised learning with human mask annotations is straightforward, it is costly and labor-intensive while handling many downstream applications. We design multi-modal pseudo-labeling to mitigate this issue. Given geometric prompts, MM-SAM generates two single-modal mask predictions <math id="S3.SS2.SSS2.p3.1.m1.1" class="ltx_Math" alttext="M_{I}" display="inline"><semantics id="S3.SS2.SSS2.p3.1.m1.1a"><msub id="S3.SS2.SSS2.p3.1.m1.1.1" xref="S3.SS2.SSS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.SSS2.p3.1.m1.1.1.2" xref="S3.SS2.SSS2.p3.1.m1.1.1.2.cmml">M</mi><mi id="S3.SS2.SSS2.p3.1.m1.1.1.3" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.1.m1.1b"><apply id="S3.SS2.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.2">ğ‘€</ci><ci id="S3.SS2.SSS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3">ğ¼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.1.m1.1c">M_{I}</annotation></semantics></math> and <math id="S3.SS2.SSS2.p3.2.m2.1" class="ltx_Math" alttext="M_{X}" display="inline"><semantics id="S3.SS2.SSS2.p3.2.m2.1a"><msub id="S3.SS2.SSS2.p3.2.m2.1.1" xref="S3.SS2.SSS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.SSS2.p3.2.m2.1.1.2" xref="S3.SS2.SSS2.p3.2.m2.1.1.2.cmml">M</mi><mi id="S3.SS2.SSS2.p3.2.m2.1.1.3" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.cmml">X</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.2.m2.1b"><apply id="S3.SS2.SSS2.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.2">ğ‘€</ci><ci id="S3.SS2.SSS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3">ğ‘‹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.2.m2.1c">M_{X}</annotation></semantics></math> from data of RGB and X-modality, respectively. The predictions are then fused to produce a refined mask prediction <math id="S3.SS2.SSS2.p3.3.m3.1" class="ltx_Math" alttext="M_{F}" display="inline"><semantics id="S3.SS2.SSS2.p3.3.m3.1a"><msub id="S3.SS2.SSS2.p3.3.m3.1.1" xref="S3.SS2.SSS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.SSS2.p3.3.m3.1.1.2" xref="S3.SS2.SSS2.p3.3.m3.1.1.2.cmml">M</mi><mi id="S3.SS2.SSS2.p3.3.m3.1.1.3" xref="S3.SS2.SSS2.p3.3.m3.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.3.m3.1b"><apply id="S3.SS2.SSS2.p3.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.2">ğ‘€</ci><ci id="S3.SS2.SSS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.3">ğ¹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.3.m3.1c">M_{F}</annotation></semantics></math>. Specifically, we derive <math id="S3.SS2.SSS2.p3.4.m4.1" class="ltx_Math" alttext="M_{F}" display="inline"><semantics id="S3.SS2.SSS2.p3.4.m4.1a"><msub id="S3.SS2.SSS2.p3.4.m4.1.1" xref="S3.SS2.SSS2.p3.4.m4.1.1.cmml"><mi id="S3.SS2.SSS2.p3.4.m4.1.1.2" xref="S3.SS2.SSS2.p3.4.m4.1.1.2.cmml">M</mi><mi id="S3.SS2.SSS2.p3.4.m4.1.1.3" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.4.m4.1b"><apply id="S3.SS2.SSS2.p3.4.m4.1.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.2">ğ‘€</ci><ci id="S3.SS2.SSS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.3">ğ¹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.4.m4.1c">M_{F}</annotation></semantics></math> by selecting the most confident predictions from corresponding patches of the paired modalities, and employ it as pseudo ground truth for SFG training:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="L_{F}=L_{bce}(\hat{M}_{F},M_{F})+L_{dice}(\hat{M}_{F},M_{F})," display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.6" xref="S3.E3.m1.1.1.1.1.6.cmml"><mi id="S3.E3.m1.1.1.1.1.6.2" xref="S3.E3.m1.1.1.1.1.6.2.cmml">L</mi><mi id="S3.E3.m1.1.1.1.1.6.3" xref="S3.E3.m1.1.1.1.1.6.3.cmml">F</mi></msub><mo id="S3.E3.m1.1.1.1.1.5" xref="S3.E3.m1.1.1.1.1.5.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.4" xref="S3.E3.m1.1.1.1.1.4.cmml"><mrow id="S3.E3.m1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.2.2.cmml"><msub id="S3.E3.m1.1.1.1.1.2.2.4" xref="S3.E3.m1.1.1.1.1.2.2.4.cmml"><mi id="S3.E3.m1.1.1.1.1.2.2.4.2" xref="S3.E3.m1.1.1.1.1.2.2.4.2.cmml">L</mi><mrow id="S3.E3.m1.1.1.1.1.2.2.4.3" xref="S3.E3.m1.1.1.1.1.2.2.4.3.cmml"><mi id="S3.E3.m1.1.1.1.1.2.2.4.3.2" xref="S3.E3.m1.1.1.1.1.2.2.4.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.2.2.4.3.1" xref="S3.E3.m1.1.1.1.1.2.2.4.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.2.2.4.3.3" xref="S3.E3.m1.1.1.1.1.2.2.4.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.2.2.4.3.1a" xref="S3.E3.m1.1.1.1.1.2.2.4.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.2.2.4.3.4" xref="S3.E3.m1.1.1.1.1.2.2.4.3.4.cmml">e</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.2.2.3" xref="S3.E3.m1.1.1.1.1.2.2.3.cmml">â€‹</mo><mrow id="S3.E3.m1.1.1.1.1.2.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.2.2.2.2.3" xref="S3.E3.m1.1.1.1.1.2.2.2.3.cmml">(</mo><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">M</mi><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml">F</mi></msub><mo id="S3.E3.m1.1.1.1.1.2.2.2.2.4" xref="S3.E3.m1.1.1.1.1.2.2.2.3.cmml">,</mo><msub id="S3.E3.m1.1.1.1.1.2.2.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.2.2.2.cmml"><mi id="S3.E3.m1.1.1.1.1.2.2.2.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.2.2.2.2.cmml">M</mi><mi id="S3.E3.m1.1.1.1.1.2.2.2.2.2.3" xref="S3.E3.m1.1.1.1.1.2.2.2.2.2.3.cmml">F</mi></msub><mo stretchy="false" id="S3.E3.m1.1.1.1.1.2.2.2.2.5" xref="S3.E3.m1.1.1.1.1.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.1.1.1.1.4.5" xref="S3.E3.m1.1.1.1.1.4.5.cmml">+</mo><mrow id="S3.E3.m1.1.1.1.1.4.4" xref="S3.E3.m1.1.1.1.1.4.4.cmml"><msub id="S3.E3.m1.1.1.1.1.4.4.4" xref="S3.E3.m1.1.1.1.1.4.4.4.cmml"><mi id="S3.E3.m1.1.1.1.1.4.4.4.2" xref="S3.E3.m1.1.1.1.1.4.4.4.2.cmml">L</mi><mrow id="S3.E3.m1.1.1.1.1.4.4.4.3" xref="S3.E3.m1.1.1.1.1.4.4.4.3.cmml"><mi id="S3.E3.m1.1.1.1.1.4.4.4.3.2" xref="S3.E3.m1.1.1.1.1.4.4.4.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.4.4.4.3.1" xref="S3.E3.m1.1.1.1.1.4.4.4.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.4.4.4.3.3" xref="S3.E3.m1.1.1.1.1.4.4.4.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.4.4.4.3.1a" xref="S3.E3.m1.1.1.1.1.4.4.4.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.4.4.4.3.4" xref="S3.E3.m1.1.1.1.1.4.4.4.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.4.4.4.3.1b" xref="S3.E3.m1.1.1.1.1.4.4.4.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.4.4.4.3.5" xref="S3.E3.m1.1.1.1.1.4.4.4.3.5.cmml">e</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.4.4.3" xref="S3.E3.m1.1.1.1.1.4.4.3.cmml">â€‹</mo><mrow id="S3.E3.m1.1.1.1.1.4.4.2.2" xref="S3.E3.m1.1.1.1.1.4.4.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.4.4.2.2.3" xref="S3.E3.m1.1.1.1.1.4.4.2.3.cmml">(</mo><msub id="S3.E3.m1.1.1.1.1.3.3.1.1.1" xref="S3.E3.m1.1.1.1.1.3.3.1.1.1.cmml"><mover accent="true" id="S3.E3.m1.1.1.1.1.3.3.1.1.1.2" xref="S3.E3.m1.1.1.1.1.3.3.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.3.3.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.3.3.1.1.1.2.2.cmml">M</mi><mo id="S3.E3.m1.1.1.1.1.3.3.1.1.1.2.1" xref="S3.E3.m1.1.1.1.1.3.3.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.E3.m1.1.1.1.1.3.3.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.3.1.1.1.3.cmml">F</mi></msub><mo id="S3.E3.m1.1.1.1.1.4.4.2.2.4" xref="S3.E3.m1.1.1.1.1.4.4.2.3.cmml">,</mo><msub id="S3.E3.m1.1.1.1.1.4.4.2.2.2" xref="S3.E3.m1.1.1.1.1.4.4.2.2.2.cmml"><mi id="S3.E3.m1.1.1.1.1.4.4.2.2.2.2" xref="S3.E3.m1.1.1.1.1.4.4.2.2.2.2.cmml">M</mi><mi id="S3.E3.m1.1.1.1.1.4.4.2.2.2.3" xref="S3.E3.m1.1.1.1.1.4.4.2.2.2.3.cmml">F</mi></msub><mo stretchy="false" id="S3.E3.m1.1.1.1.1.4.4.2.2.5" xref="S3.E3.m1.1.1.1.1.4.4.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.5.cmml" xref="S3.E3.m1.1.1.1.1.5"></eq><apply id="S3.E3.m1.1.1.1.1.6.cmml" xref="S3.E3.m1.1.1.1.1.6"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.6.1.cmml" xref="S3.E3.m1.1.1.1.1.6">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.6.2.cmml" xref="S3.E3.m1.1.1.1.1.6.2">ğ¿</ci><ci id="S3.E3.m1.1.1.1.1.6.3.cmml" xref="S3.E3.m1.1.1.1.1.6.3">ğ¹</ci></apply><apply id="S3.E3.m1.1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.1.4"><plus id="S3.E3.m1.1.1.1.1.4.5.cmml" xref="S3.E3.m1.1.1.1.1.4.5"></plus><apply id="S3.E3.m1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2"><times id="S3.E3.m1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.3"></times><apply id="S3.E3.m1.1.1.1.1.2.2.4.cmml" xref="S3.E3.m1.1.1.1.1.2.2.4"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.2.4.1.cmml" xref="S3.E3.m1.1.1.1.1.2.2.4">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.2.2.4.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.4.2">ğ¿</ci><apply id="S3.E3.m1.1.1.1.1.2.2.4.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.4.3"><times id="S3.E3.m1.1.1.1.1.2.2.4.3.1.cmml" xref="S3.E3.m1.1.1.1.1.2.2.4.3.1"></times><ci id="S3.E3.m1.1.1.1.1.2.2.4.3.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.4.3.2">ğ‘</ci><ci id="S3.E3.m1.1.1.1.1.2.2.4.3.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.4.3.3">ğ‘</ci><ci id="S3.E3.m1.1.1.1.1.2.2.4.3.4.cmml" xref="S3.E3.m1.1.1.1.1.2.2.4.3.4">ğ‘’</ci></apply></apply><interval closure="open" id="S3.E3.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2"><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2"><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.2">ğ‘€</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.3">ğ¹</ci></apply><apply id="S3.E3.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2.2.2">ğ‘€</ci><ci id="S3.E3.m1.1.1.1.1.2.2.2.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2.2.3">ğ¹</ci></apply></interval></apply><apply id="S3.E3.m1.1.1.1.1.4.4.cmml" xref="S3.E3.m1.1.1.1.1.4.4"><times id="S3.E3.m1.1.1.1.1.4.4.3.cmml" xref="S3.E3.m1.1.1.1.1.4.4.3"></times><apply id="S3.E3.m1.1.1.1.1.4.4.4.cmml" xref="S3.E3.m1.1.1.1.1.4.4.4"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.4.4.4.1.cmml" xref="S3.E3.m1.1.1.1.1.4.4.4">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.4.4.4.2.cmml" xref="S3.E3.m1.1.1.1.1.4.4.4.2">ğ¿</ci><apply id="S3.E3.m1.1.1.1.1.4.4.4.3.cmml" xref="S3.E3.m1.1.1.1.1.4.4.4.3"><times id="S3.E3.m1.1.1.1.1.4.4.4.3.1.cmml" xref="S3.E3.m1.1.1.1.1.4.4.4.3.1"></times><ci id="S3.E3.m1.1.1.1.1.4.4.4.3.2.cmml" xref="S3.E3.m1.1.1.1.1.4.4.4.3.2">ğ‘‘</ci><ci id="S3.E3.m1.1.1.1.1.4.4.4.3.3.cmml" xref="S3.E3.m1.1.1.1.1.4.4.4.3.3">ğ‘–</ci><ci id="S3.E3.m1.1.1.1.1.4.4.4.3.4.cmml" xref="S3.E3.m1.1.1.1.1.4.4.4.3.4">ğ‘</ci><ci id="S3.E3.m1.1.1.1.1.4.4.4.3.5.cmml" xref="S3.E3.m1.1.1.1.1.4.4.4.3.5">ğ‘’</ci></apply></apply><interval closure="open" id="S3.E3.m1.1.1.1.1.4.4.2.3.cmml" xref="S3.E3.m1.1.1.1.1.4.4.2.2"><apply id="S3.E3.m1.1.1.1.1.3.3.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.3.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1.1.1">subscript</csymbol><apply id="S3.E3.m1.1.1.1.1.3.3.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1.1.1.2"><ci id="S3.E3.m1.1.1.1.1.3.3.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1.1.1.2.1">^</ci><ci id="S3.E3.m1.1.1.1.1.3.3.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1.1.1.2.2">ğ‘€</ci></apply><ci id="S3.E3.m1.1.1.1.1.3.3.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1.1.1.3">ğ¹</ci></apply><apply id="S3.E3.m1.1.1.1.1.4.4.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.4.4.2.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.4.4.2.2.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.4.4.2.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.4.4.2.2.2.2">ğ‘€</ci><ci id="S3.E3.m1.1.1.1.1.4.4.2.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.4.4.2.2.2.3">ğ¹</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">L_{F}=L_{bce}(\hat{M}_{F},M_{F})+L_{dice}(\hat{M}_{F},M_{F}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS2.p3.6" class="ltx_p">where <math id="S3.SS2.SSS2.p3.5.m1.1" class="ltx_Math" alttext="L_{bce}" display="inline"><semantics id="S3.SS2.SSS2.p3.5.m1.1a"><msub id="S3.SS2.SSS2.p3.5.m1.1.1" xref="S3.SS2.SSS2.p3.5.m1.1.1.cmml"><mi id="S3.SS2.SSS2.p3.5.m1.1.1.2" xref="S3.SS2.SSS2.p3.5.m1.1.1.2.cmml">L</mi><mrow id="S3.SS2.SSS2.p3.5.m1.1.1.3" xref="S3.SS2.SSS2.p3.5.m1.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.5.m1.1.1.3.2" xref="S3.SS2.SSS2.p3.5.m1.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.5.m1.1.1.3.1" xref="S3.SS2.SSS2.p3.5.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p3.5.m1.1.1.3.3" xref="S3.SS2.SSS2.p3.5.m1.1.1.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.5.m1.1.1.3.1a" xref="S3.SS2.SSS2.p3.5.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p3.5.m1.1.1.3.4" xref="S3.SS2.SSS2.p3.5.m1.1.1.3.4.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.5.m1.1b"><apply id="S3.SS2.SSS2.p3.5.m1.1.1.cmml" xref="S3.SS2.SSS2.p3.5.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.5.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p3.5.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.5.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p3.5.m1.1.1.2">ğ¿</ci><apply id="S3.SS2.SSS2.p3.5.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p3.5.m1.1.1.3"><times id="S3.SS2.SSS2.p3.5.m1.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.5.m1.1.1.3.1"></times><ci id="S3.SS2.SSS2.p3.5.m1.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.5.m1.1.1.3.2">ğ‘</ci><ci id="S3.SS2.SSS2.p3.5.m1.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.5.m1.1.1.3.3">ğ‘</ci><ci id="S3.SS2.SSS2.p3.5.m1.1.1.3.4.cmml" xref="S3.SS2.SSS2.p3.5.m1.1.1.3.4">ğ‘’</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.5.m1.1c">L_{bce}</annotation></semantics></math> denotes the binary cross-entropy loss and <math id="S3.SS2.SSS2.p3.6.m2.1" class="ltx_Math" alttext="L_{dice}" display="inline"><semantics id="S3.SS2.SSS2.p3.6.m2.1a"><msub id="S3.SS2.SSS2.p3.6.m2.1.1" xref="S3.SS2.SSS2.p3.6.m2.1.1.cmml"><mi id="S3.SS2.SSS2.p3.6.m2.1.1.2" xref="S3.SS2.SSS2.p3.6.m2.1.1.2.cmml">L</mi><mrow id="S3.SS2.SSS2.p3.6.m2.1.1.3" xref="S3.SS2.SSS2.p3.6.m2.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.6.m2.1.1.3.2" xref="S3.SS2.SSS2.p3.6.m2.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.6.m2.1.1.3.1" xref="S3.SS2.SSS2.p3.6.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p3.6.m2.1.1.3.3" xref="S3.SS2.SSS2.p3.6.m2.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.6.m2.1.1.3.1a" xref="S3.SS2.SSS2.p3.6.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p3.6.m2.1.1.3.4" xref="S3.SS2.SSS2.p3.6.m2.1.1.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p3.6.m2.1.1.3.1b" xref="S3.SS2.SSS2.p3.6.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p3.6.m2.1.1.3.5" xref="S3.SS2.SSS2.p3.6.m2.1.1.3.5.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.6.m2.1b"><apply id="S3.SS2.SSS2.p3.6.m2.1.1.cmml" xref="S3.SS2.SSS2.p3.6.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.6.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p3.6.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.6.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p3.6.m2.1.1.2">ğ¿</ci><apply id="S3.SS2.SSS2.p3.6.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p3.6.m2.1.1.3"><times id="S3.SS2.SSS2.p3.6.m2.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.6.m2.1.1.3.1"></times><ci id="S3.SS2.SSS2.p3.6.m2.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.6.m2.1.1.3.2">ğ‘‘</ci><ci id="S3.SS2.SSS2.p3.6.m2.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.6.m2.1.1.3.3">ğ‘–</ci><ci id="S3.SS2.SSS2.p3.6.m2.1.1.3.4.cmml" xref="S3.SS2.SSS2.p3.6.m2.1.1.3.4">ğ‘</ci><ci id="S3.SS2.SSS2.p3.6.m2.1.1.3.5.cmml" xref="S3.SS2.SSS2.p3.6.m2.1.1.3.5">ğ‘’</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.6.m2.1c">L_{dice}</annotation></semantics></math> represents the dice loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>.</p>
</div>
<div id="S3.SS2.SSS2.p4" class="ltx_para">
<p id="S3.SS2.SSS2.p4.1" class="ltx_p">The whole tuning objective of MM-SAM is summarized as follows:</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="L=L_{U}+L_{F}." display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.cmml">L</mi><mo id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml"><msub id="S3.E4.m1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.3.2.cmml"><mi id="S3.E4.m1.1.1.1.1.3.2.2" xref="S3.E4.m1.1.1.1.1.3.2.2.cmml">L</mi><mi id="S3.E4.m1.1.1.1.1.3.2.3" xref="S3.E4.m1.1.1.1.1.3.2.3.cmml">U</mi></msub><mo id="S3.E4.m1.1.1.1.1.3.1" xref="S3.E4.m1.1.1.1.1.3.1.cmml">+</mo><msub id="S3.E4.m1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.3.3.cmml"><mi id="S3.E4.m1.1.1.1.1.3.3.2" xref="S3.E4.m1.1.1.1.1.3.3.2.cmml">L</mi><mi id="S3.E4.m1.1.1.1.1.3.3.3" xref="S3.E4.m1.1.1.1.1.3.3.3.cmml">F</mi></msub></mrow></mrow><mo lspace="0em" id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><eq id="S3.E4.m1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"></eq><ci id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2">ğ¿</ci><apply id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3"><plus id="S3.E4.m1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.1"></plus><apply id="S3.E4.m1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2.2">ğ¿</ci><ci id="S3.E4.m1.1.1.1.1.3.2.3.cmml" xref="S3.E4.m1.1.1.1.1.3.2.3">ğ‘ˆ</ci></apply><apply id="S3.E4.m1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.3.2">ğ¿</ci><ci id="S3.E4.m1.1.1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3.3">ğ¹</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">L=L_{U}+L_{F}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.SSS2.p5" class="ltx_para">
<p id="S3.SS2.SSS2.p5.1" class="ltx_p"><span id="S3.SS2.SSS2.p5.1.1" class="ltx_text ltx_font_bold">Expanding MM-SAM to Include More Sensor Modalities.</span> While our discussion has focused on two modalities <math id="S3.SS2.SSS2.p5.1.m1.2" class="ltx_Math" alttext="(I,X)" display="inline"><semantics id="S3.SS2.SSS2.p5.1.m1.2a"><mrow id="S3.SS2.SSS2.p5.1.m1.2.3.2" xref="S3.SS2.SSS2.p5.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p5.1.m1.2.3.2.1" xref="S3.SS2.SSS2.p5.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS2.SSS2.p5.1.m1.1.1" xref="S3.SS2.SSS2.p5.1.m1.1.1.cmml">I</mi><mo id="S3.SS2.SSS2.p5.1.m1.2.3.2.2" xref="S3.SS2.SSS2.p5.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS2.SSS2.p5.1.m1.2.2" xref="S3.SS2.SSS2.p5.1.m1.2.2.cmml">X</mi><mo stretchy="false" id="S3.SS2.SSS2.p5.1.m1.2.3.2.3" xref="S3.SS2.SSS2.p5.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p5.1.m1.2b"><interval closure="open" id="S3.SS2.SSS2.p5.1.m1.2.3.1.cmml" xref="S3.SS2.SSS2.p5.1.m1.2.3.2"><ci id="S3.SS2.SSS2.p5.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p5.1.m1.1.1">ğ¼</ci><ci id="S3.SS2.SSS2.p5.1.m1.2.2.cmml" xref="S3.SS2.SSS2.p5.1.m1.2.2">ğ‘‹</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p5.1.m1.2c">(I,X)</annotation></semantics></math> for simplicity, the MM-SAM allows seamlessly integrating additional modalities by expanding SFG for generating fusion weights of more modalities. Incorporating more sensor types further enriches the segmentation system with a broader spectrum of information, leading to enhanced performance and versatility. Further experimental insights and discussions regarding the integration of additional modalities are provided in SectionÂ <a href="#S4.SS2.SSS2" title="4.2.2 Time-Asynchronous Sensor Suites â€£ 4.2 Segmentation Results â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Training and Inference</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">During training, we freeze the pre-trained SAM parameters and only update the newly-included trainable parameters in two phases. In the UCMT training phase, pairs of modalities are directly fed into the image encoder for optimization. In the WMMF training phase, parameters introduced in the previous stage remain frozen, while only the SFG is updated with provided geometric prompts.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">During inference, MM-SAM supports segmentation for both single-modal and multi-modal data. For cross-modal segmentation, the encoded embedding <math id="S3.SS2.SSS3.p2.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS2.SSS3.p2.1.m1.1a"><mi id="S3.SS2.SSS3.p2.1.m1.1.1" xref="S3.SS2.SSS3.p2.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.1.m1.1b"><ci id="S3.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.1.m1.1c">X</annotation></semantics></math> from the image encoder is directly forwarded to the mask decoder alongside a geometric prompt for mask prediction, following SAMâ€™s process for RGB images. In multi-modal segmentation, the Selective Fusion Gate (SFG) integrates different modality embeddings to generate the final embeddings of the image encoder.</p>
</div>
<div id="Thmremarkx1" class="ltx_theorem ltx_theorem_remark">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span id="Thmremarkx1.1.1.1" class="ltx_text ltx_font_bold">Remark</span></span><span id="Thmremarkx1.2.2" class="ltx_text ltx_font_bold"> </span>(<span id="Thmremarkx1.3.3" class="ltx_text ltx_font_bold">Efficiency</span>)<span id="Thmremarkx1.4.4" class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmremarkx1.p1" class="ltx_para">
<p id="Thmremarkx1.p1.1" class="ltx_p"><span id="Thmremarkx1.p1.1.1" class="ltx_text ltx_font_italic">The training of MM-SAM features two notable properties:</span></p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Parameter Efficiency</span><span id="S3.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">: TableÂ </span><a href="#S3.T1" title="Table 1 â€£ 3.2.3 Training and Inference â€£ 3.2 MM-SAM â€£ 3 Methodology â€£ Segment Anything with Multiple Modalities" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S3.I1.i1.p1.1.3" class="ltx_text ltx_font_italic"> compares trainable parameters between SAM and MM-SAM across different data modalities (implemented with ViT-BÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.I1.i1.p1.1.4.1" class="ltx_text ltx_font_italic">[</span><a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a><span id="S3.I1.i1.p1.1.5.2" class="ltx_text ltx_font_italic">]</span></cite><span id="S3.I1.i1.p1.1.6" class="ltx_text ltx_font_italic">), more details to be elaborated in SectionÂ </span><a href="#S4" title="4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S3.I1.i1.p1.1.7" class="ltx_text ltx_font_italic">. It is evident that MM-SAM introduces limited additional parameters yet enhances the performance significantly across diverse modalities.</span></p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Label Efficiency</span><span id="S3.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">: The entire tuning process of MM-SAM requires no mask annotations: UCMT operates in an unsupervised manner, using only unlabeled modality pairs, while WMMF is weakly-supervised with geometric prompts which are notably easier to collect than mask annotations.</span></p>
</div>
</li>
</ul>
</div>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.8.1.1" class="ltx_text" style="font-size:113%;">Table 1</span>: </span><span id="S3.T1.9.2" class="ltx_text" style="font-size:113%;">Comparison of trainable parameters between ViT-BÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>-based SAM and MM-SAM with different sensor modality pairs (RGB+X). Channel numbers of individual X are indicated in brackets.</span></figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;" colspan="2"><span id="S3.T1.1.2.1.1.1" class="ltx_text" style="font-size:80%;">Model</span></th>
<th id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;" colspan="7"><span id="S3.T1.1.2.1.2.1" class="ltx_text" style="font-size:80%;">Trainable parameters</span></th>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;" colspan="2">
<span id="S3.T1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">SAMÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T1.1.3.2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a><span id="S3.T1.1.3.2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<th id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;" colspan="7"><span id="S3.T1.1.3.2.2.1" class="ltx_text" style="font-size:80%;">91M</span></th>
</tr>
<tr id="S3.T1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;" rowspan="4"><span id="S3.T1.1.1.2.1" class="ltx_text" style="font-size:80%;">MM-SAM</span></th>
<th id="S3.T1.1.1.1" class="ltx_td ltx_nopad ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><svg version="1.1" height="15.38" width="85.48" overflow="visible"><g transform="translate(0,15.38) scale(1,-1)"><path d="M 0,15.38 85.48,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,7.69) scale(1, -1)"><foreignObject width="36.28" height="7.69" overflow="visible">
<span id="S3.T1.1.1.1.pic1.1.1" class="ltx_inline-block">
<span id="S3.T1.1.1.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S3.T1.1.1.1.pic1.1.1.1.1" class="ltx_p"><span id="S3.T1.1.1.1.pic1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Module</span></span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(42.74,7.69)"><g transform="translate(0,7.69) scale(1, -1)"><foreignObject width="42.74" height="7.69" overflow="visible">
<span id="S3.T1.1.1.1.pic1.2.1" class="ltx_inline-block">
<span id="S3.T1.1.1.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S3.T1.1.1.1.pic1.2.1.1.1" class="ltx_p"><span id="S3.T1.1.1.1.pic1.2.1.1.1.1" class="ltx_text" style="font-size:80%;">X-Modal</span></span>
</span>
</span></foreignObject></g></g></g></svg></th>
<th id="S3.T1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.1.3.1" class="ltx_text" style="font-size:80%;">Thermal(1)</span></th>
<th id="S3.T1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.1.4.1" class="ltx_text" style="font-size:80%;">Depth(1)</span></th>
<th id="S3.T1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.1.5.1" class="ltx_text" style="font-size:80%;">LiDAR(4)</span></th>
<th id="S3.T1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.1.6.1" class="ltx_text" style="font-size:80%;">HSI(48)</span></th>
<th id="S3.T1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.1.7.1" class="ltx_text" style="font-size:80%;">MS-LiDAR(6)</span></th>
<th id="S3.T1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.1.8.1" class="ltx_text" style="font-size:80%;">SAR(1)</span></th>
<th id="S3.T1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.1.9.1" class="ltx_text" style="font-size:80%;">DSM(1)</span></th>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.4.3.1.1" class="ltx_text" style="font-size:80%;">UCMT</span></th>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.4.3.2.1" class="ltx_text" style="font-size:80%;">344.8K</span></td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.4.3.3.1" class="ltx_text" style="font-size:80%;">344.8K</span></td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.4.3.4.1" class="ltx_text" style="font-size:80%;">934.7K</span></td>
<td id="S3.T1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.4.3.5.1" class="ltx_text" style="font-size:80%;">9.6M</span></td>
<td id="S3.T1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.4.3.6.1" class="ltx_text" style="font-size:80%;">1.3M</span></td>
<td id="S3.T1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.4.3.7.1" class="ltx_text" style="font-size:80%;">344.8K</span></td>
<td id="S3.T1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.4.3.8.1" class="ltx_text" style="font-size:80%;">344.8K</span></td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.5.4.1.1" class="ltx_text" style="font-size:80%;">WMMF</span></th>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_center" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.5.4.2.1" class="ltx_text" style="font-size:80%;">148.1K</span></td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_center" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.5.4.3.1" class="ltx_text" style="font-size:80%;">148.1K</span></td>
<td id="S3.T1.1.5.4.4" class="ltx_td ltx_align_center" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.5.4.4.1" class="ltx_text" style="font-size:80%;">148.1K</span></td>
<td id="S3.T1.1.5.4.5" class="ltx_td ltx_align_center" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.5.4.5.1" class="ltx_text" style="font-size:80%;">148.1K</span></td>
<td id="S3.T1.1.5.4.6" class="ltx_td ltx_align_center" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.5.4.6.1" class="ltx_text" style="font-size:80%;">148.1K</span></td>
<td id="S3.T1.1.5.4.7" class="ltx_td ltx_align_center" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.5.4.7.1" class="ltx_text" style="font-size:80%;">148.1K</span></td>
<td id="S3.T1.1.5.4.8" class="ltx_td ltx_align_center" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.5.4.8.1" class="ltx_text" style="font-size:80%;">148.1K</span></td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<th id="S3.T1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.6.5.1.1" class="ltx_text" style="font-size:80%;">Total</span></th>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.6.5.2.1" class="ltx_text" style="font-size:80%;">492.9K</span></td>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.6.5.3.1" class="ltx_text" style="font-size:80%;">492.9K</span></td>
<td id="S3.T1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.6.5.4.1" class="ltx_text" style="font-size:80%;">1.1M</span></td>
<td id="S3.T1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.6.5.5.1" class="ltx_text" style="font-size:80%;">9.7M</span></td>
<td id="S3.T1.1.6.5.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.6.5.6.1" class="ltx_text" style="font-size:80%;">1.5M</span></td>
<td id="S3.T1.1.6.5.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.6.5.7.1" class="ltx_text" style="font-size:80%;">492.9K</span></td>
<td id="S3.T1.1.6.5.8" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S3.T1.1.6.5.8.1" class="ltx_text" style="font-size:80%;">492.9K</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="Thmremarkx2" class="ltx_theorem ltx_theorem_remark">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span id="Thmremarkx2.1.1.1" class="ltx_text ltx_font_bold">Remark</span></span><span id="Thmremarkx2.2.2" class="ltx_text ltx_font_bold"> </span>(<span id="Thmremarkx2.3.3" class="ltx_text ltx_font_bold">Insights</span>)<span id="Thmremarkx2.4.4" class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmremarkx2.p1" class="ltx_para">
<p id="Thmremarkx2.p1.1" class="ltx_p"><span id="Thmremarkx2.p1.1.1" class="ltx_text ltx_font_italic">To the best of our knowledge, this is the first study that explores visual foundation models for sensor suites. Our analysis of MM-SAM yields several key insights:</span></p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_italic">MM-SAM proves the feasibility of sharing the output latent space of SAMâ€™s powerful image encoder across sensor modalities. This robust sharability allows capturing embeddings of different sensor data that are modality specific yet still compatible with other components in SAM (i.e., the prompt encoder and mask decoder), facilitating cross-modal segmentation.</span></p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_italic">The shared latent space enables sensor fusion, wherein MM-SAM adaptively weights embeddings of different sensor modalities and generates more informative embeddings for enhanced segmentation.</span></p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p"><span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_italic">MM-SAM is a general framework and easily applicable to various sensor types, suggesting promising avenues for further research in visual foundation models and sensor fusion.</span></p>
</div>
</li>
</ul>
</div>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S3.T2.3.2" class="ltx_text" style="font-size:90%;">We benchmark MM-SAM across seven datasets with eight different sensor modalities.</span></figcaption>
<table id="S3.T2.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.4.1.1" class="ltx_tr">
<td id="S3.T2.4.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.1.1.1.1" class="ltx_text" style="font-size:80%;">Sensor suite</span></td>
<td id="S3.T2.4.1.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.1.1.2.1" class="ltx_text" style="font-size:80%;">Dataset</span></td>
<td id="S3.T2.4.1.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.1.1.3.1" class="ltx_text" style="font-size:80%;">Modalities</span></td>
<td id="S3.T2.4.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.1.1.4.1" class="ltx_text" style="font-size:80%;">Task</span></td>
<td id="S3.T2.4.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.1.1.5.1" class="ltx_text" style="font-size:80%;">#Cls</span></td>
<td id="S3.T2.4.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.1.1.6.1" class="ltx_text" style="font-size:80%;">#Train</span></td>
<td id="S3.T2.4.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.1.1.7.1" class="ltx_text" style="font-size:80%;">#Test</span></td>
</tr>
<tr id="S3.T2.4.2.2" class="ltx_tr">
<td id="S3.T2.4.2.2.1" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td id="S3.T2.4.2.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S3.T2.4.2.2.2.1" class="ltx_text" style="font-size:80%;">MFNetÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.4.2.2.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a><span id="S3.T2.4.2.2.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.4.2.2.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.2.2.3.1" class="ltx_text" style="font-size:80%;">RGB, Thermal</span></td>
<td id="S3.T2.4.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.2.2.4.1" class="ltx_text" style="font-size:80%;">Road Scene Seg.</span></td>
<td id="S3.T2.4.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.2.2.5.1" class="ltx_text" style="font-size:80%;">8</span></td>
<td id="S3.T2.4.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.2.2.6.1" class="ltx_text" style="font-size:80%;">784</span></td>
<td id="S3.T2.4.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.2.2.7.1" class="ltx_text" style="font-size:80%;">393</span></td>
</tr>
<tr id="S3.T2.4.3.3" class="ltx_tr">
<td id="S3.T2.4.3.3.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.3.3.1.1" class="ltx_text" style="font-size:80%;">Time-</span></td>
<td id="S3.T2.4.3.3.2" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S3.T2.4.3.3.2.1" class="ltx_text" style="font-size:80%;">FreiburgThermalÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.4.3.3.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a><span id="S3.T2.4.3.3.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.4.3.3.3" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.3.3.3.1" class="ltx_text" style="font-size:80%;">RGB, Thermal</span></td>
<td id="S3.T2.4.3.3.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.3.3.4.1" class="ltx_text" style="font-size:80%;">Road Scene Seg.</span></td>
<td id="S3.T2.4.3.3.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.3.3.5.1" class="ltx_text" style="font-size:80%;">12</span></td>
<td id="S3.T2.4.3.3.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.3.3.6.1" class="ltx_text" style="font-size:80%;">20,853</span></td>
<td id="S3.T2.4.3.3.7" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.3.3.7.1" class="ltx_text" style="font-size:80%;">64</span></td>
</tr>
<tr id="S3.T2.4.4.4" class="ltx_tr">
<td id="S3.T2.4.4.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.4.4.1.1" class="ltx_text" style="font-size:80%;">synchronized</span></td>
<td id="S3.T2.4.4.4.2" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S3.T2.4.4.4.2.1" class="ltx_text" style="font-size:80%;">SUN RGB-DÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.4.4.4.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a><span id="S3.T2.4.4.4.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.4.4.4.3" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.4.4.3.1" class="ltx_text" style="font-size:80%;">RGB, Depth</span></td>
<td id="S3.T2.4.4.4.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.4.4.4.1" class="ltx_text" style="font-size:80%;">Indoor Scene Seg.</span></td>
<td id="S3.T2.4.4.4.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.4.4.5.1" class="ltx_text" style="font-size:80%;">37</span></td>
<td id="S3.T2.4.4.4.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.4.4.6.1" class="ltx_text" style="font-size:80%;">5,285</span></td>
<td id="S3.T2.4.4.4.7" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.4.4.7.1" class="ltx_text" style="font-size:80%;">5,050</span></td>
</tr>
<tr id="S3.T2.4.5.5" class="ltx_tr">
<td id="S3.T2.4.5.5.1" class="ltx_td ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td id="S3.T2.4.5.5.2" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S3.T2.4.5.5.2.1" class="ltx_text" style="font-size:80%;">SemanticKITTIÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.4.5.5.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a><span id="S3.T2.4.5.5.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.4.5.5.3" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.5.5.3.1" class="ltx_text" style="font-size:80%;">RGB, LiDAR</span></td>
<td id="S3.T2.4.5.5.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.5.5.4.1" class="ltx_text" style="font-size:80%;">Road Scene Seg.</span></td>
<td id="S3.T2.4.5.5.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.5.5.5.1" class="ltx_text" style="font-size:80%;">8</span></td>
<td id="S3.T2.4.5.5.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.5.5.6.1" class="ltx_text" style="font-size:80%;">19,130</span></td>
<td id="S3.T2.4.5.5.7" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.5.5.7.1" class="ltx_text" style="font-size:80%;">4,071</span></td>
</tr>
<tr id="S3.T2.4.6.6" class="ltx_tr">
<td id="S3.T2.4.6.6.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;" rowspan="3"><span id="S3.T2.4.6.6.1.1" class="ltx_text" style="font-size:80%;">Time-asynchronized</span></td>
<td id="S3.T2.4.6.6.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S3.T2.4.6.6.2.1" class="ltx_text" style="font-size:80%;">DFC2018Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.4.6.6.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a><span id="S3.T2.4.6.6.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.4.6.6.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.6.6.3.1" class="ltx_text" style="font-size:80%;">RGB, HSI, MS-LiDAR</span></td>
<td id="S3.T2.4.6.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.6.6.4.1" class="ltx_text" style="font-size:80%;">Building Seg.</span></td>
<td id="S3.T2.4.6.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.6.6.5.1" class="ltx_text" style="font-size:80%;">1</span></td>
<td id="S3.T2.4.6.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.6.6.6.1" class="ltx_text" style="font-size:80%;">12</span></td>
<td id="S3.T2.4.6.6.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.6.6.7.1" class="ltx_text" style="font-size:80%;">2</span></td>
</tr>
<tr id="S3.T2.4.7.7" class="ltx_tr">
<td id="S3.T2.4.7.7.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S3.T2.4.7.7.1.1" class="ltx_text" style="font-size:80%;">DFC2023Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.4.7.7.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a><span id="S3.T2.4.7.7.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.4.7.7.2" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.7.7.2.1" class="ltx_text" style="font-size:80%;">RGB, SAR</span></td>
<td id="S3.T2.4.7.7.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.7.7.3.1" class="ltx_text" style="font-size:80%;">Building Seg.</span></td>
<td id="S3.T2.4.7.7.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.7.7.4.1" class="ltx_text" style="font-size:80%;">1</span></td>
<td id="S3.T2.4.7.7.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.7.7.5.1" class="ltx_text" style="font-size:80%;">2,969</span></td>
<td id="S3.T2.4.7.7.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.7.7.6.1" class="ltx_text" style="font-size:80%;">751</span></td>
</tr>
<tr id="S3.T2.4.8.8" class="ltx_tr">
<td id="S3.T2.4.8.8.1" class="ltx_td ltx_align_left ltx_border_b" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S3.T2.4.8.8.1.1" class="ltx_text" style="font-size:80%;">ISPRS Potsdam</span><span id="footnotex5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span id="footnotex5.1" class="ltx_text" style="font-size:80%;">ISPRS 2D Semantic Labeling Contest Potsdam (2016). Available from: </span><a target="_blank" href="https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-potsdam.aspx" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-potsdam.aspx</a></span></span></span>
</td>
<td id="S3.T2.4.8.8.2" class="ltx_td ltx_align_left ltx_border_b" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.8.8.2.1" class="ltx_text" style="font-size:80%;">RGB, DSM</span></td>
<td id="S3.T2.4.8.8.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.8.8.3.1" class="ltx_text" style="font-size:80%;">Building Seg.</span></td>
<td id="S3.T2.4.8.8.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.8.8.4.1" class="ltx_text" style="font-size:80%;">1</span></td>
<td id="S3.T2.4.8.8.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.8.8.5.1" class="ltx_text" style="font-size:80%;">32</span></td>
<td id="S3.T2.4.8.8.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S3.T2.4.8.8.6.1" class="ltx_text" style="font-size:80%;">6</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiment Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We first describe the main experimental setups, with full details provided in the appendix.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Datasets.</span> We evaluated MM-SAM over two broad categories of sensor suites: <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_italic">time-synchronized suites</span> and <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_italic">time-asynchronous suites</span>, as described in Section <a href="#S3.SS1" title="3.1 Preliminaries â€£ 3 Methodology â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. Table <a href="#S3.T2" title="Table 2 â€£ 3.2.3 Training and Inference â€£ 3.2 MM-SAM â€£ 3 Methodology â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes the seven datasets, which cover a diverse range of non-RGB modalities including depth, thermal, LiDAR for autonomous driving, airborne multispectral LiDAR (MS-LiDAR), hyperspectral (HSI), Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). Detailed descriptions of the seven datasets and their processing details are available in Appendix <a href="#A2" title="Appendix B Datasets and Metrics â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Implementation Details.</span>
Experiments were conducted on four NVIDIA A100 GPUs. Detailed hyperparameters used to tune each of the models reported in Tables <a href="#S4.T3" title="Table 3 â€£ 4.2.1 Time-Synchronized Sensor Suites â€£ 4.2 Segmentation Results â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, <a href="#S4.T4" title="Table 4 â€£ 4.2.2 Time-Asynchronous Sensor Suites â€£ 4.2 Segmentation Results â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> are provided in AppendixÂ <a href="#A3.SS1" title="C.1 Training implementation details â€£ Appendix C Implementation Details â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C.1</span></a>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Segmentation Results</h3>

<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Time-Synchronized Sensor Suites</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.4" class="ltx_p">Table <a href="#S4.T3" title="Table 3 â€£ 4.2.1 Time-Synchronized Sensor Suites â€£ 4.2 Segmentation Results â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the segmentation performance of SAM and MM-SAM on time-synchronized sensor suites. SAMâ€™s performance is evaluated on RGB images. For reference, we also transform <math id="S4.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><mi id="S4.SS2.SSS1.p1.1.m1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.1b"><ci id="S4.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.1c">X</annotation></semantics></math> into false-color images (denoted as X*) and test it with SAM for comparisons. MM-SAM is evaluated on another paired modality data <math id="S4.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S4.SS2.SSS1.p1.2.m2.1a"><mi id="S4.SS2.SSS1.p1.2.m2.1.1" xref="S4.SS2.SSS1.p1.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.2.m2.1b"><ci id="S4.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.2.m2.1c">X</annotation></semantics></math> alone as well as RGB+<math id="S4.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S4.SS2.SSS1.p1.3.m3.1a"><mi id="S4.SS2.SSS1.p1.3.m3.1.1" xref="S4.SS2.SSS1.p1.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.3.m3.1b"><ci id="S4.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.3.m3.1c">X</annotation></semantics></math>. Here, <math id="S4.SS2.SSS1.p1.4.m4.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S4.SS2.SSS1.p1.4.m4.1a"><mi id="S4.SS2.SSS1.p1.4.m4.1.1" xref="S4.SS2.SSS1.p1.4.m4.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.4.m4.1b"><ci id="S4.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.4.m4.1c">X</annotation></semantics></math> represents thermal images in MFNet, depth images in SUN RGB-D, and LiDAR point clouds in SemanticKITTI.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">We can observe that SAM achieves much better segmentation on RGB images than on false-color images from other modalities due to distribution discrepancies. In contrast, MM-SAM improves segmentation consistently by large margins across three non-RGB modalities. Notably, in MFNet and SemanticKITTI, MM-SAM on thermal images and LiDAR point clouds even outperforms SAM on paired RGB images, highlighting potential limitations of RGB cameras and strengths of non-RGB sensors in different scenarios. In addition, MM-SAM demonstrates effective sensor fusion by consistently surpassing any individual modalities alone, underscoring its robustness and versatility across time-synchronized sensor suites. These results demonstrate the efficacy of MM-SAM in leveraging diverse sensor data with superior segmentation performance.</p>
</div>
<figure id="S4.T3" class="ltx_table">

<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Segmentation results on time-<span id="S4.T3.6.1" class="ltx_text ltx_font_italic">synchronized</span> sensor suites using bounding box prompts. For MFNet, mIoU is reported for total/day/night, following the official criteria. The symbol * indicates false-color images transformed from each non-RGB modality. </figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.T3.st1" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">(a) </span>MFNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite></figcaption>
<table id="S4.T3.st1.6" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.st1.6.1.1" class="ltx_tr">
<td id="S4.T3.st1.6.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st1.6.1.1.1.1" class="ltx_text" style="font-size:90%;">Model</span></td>
<td id="S4.T3.st1.6.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st1.6.1.1.2.1" class="ltx_text" style="font-size:90%;">Modal</span></td>
<td id="S4.T3.st1.6.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st1.6.1.1.3.1" class="ltx_text" style="font-size:90%;">mIoU</span></td>
</tr>
<tr id="S4.T3.st1.6.2.2" class="ltx_tr">
<td id="S4.T3.st1.6.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" style="padding-left:0.6pt;padding-right:0.6pt;" rowspan="2"><span id="S4.T3.st1.6.2.2.1.1" class="ltx_text" style="font-size:90%;">SAMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite></span></td>
<td id="S4.T3.st1.6.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st1.6.2.2.2.1" class="ltx_text" style="font-size:90%;">RGB</span></td>
<td id="S4.T3.st1.6.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st1.6.2.2.3.1" class="ltx_text" style="font-size:90%;">68.2/72.6/65.1</span></td>
</tr>
<tr id="S4.T3.st1.6.3.3" class="ltx_tr">
<td id="S4.T3.st1.6.3.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st1.6.3.3.1.1" class="ltx_text" style="font-size:90%;">Thermal*</span></td>
<td id="S4.T3.st1.6.3.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st1.6.3.3.2.1" class="ltx_text" style="font-size:90%;">64.5/61.4/65.0</span></td>
</tr>
<tr id="S4.T3.st1.6.4.4" class="ltx_tr">
<td id="S4.T3.st1.6.4.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:0.6pt;padding-right:0.6pt;" rowspan="2"><span id="S4.T3.st1.6.4.4.1.1" class="ltx_text" style="font-size:90%;">MM-SAM</span></td>
<td id="S4.T3.st1.6.4.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st1.6.4.4.2.1" class="ltx_text" style="font-size:90%;">Thermal</span></td>
<td id="S4.T3.st1.6.4.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st1.6.4.4.3.1" class="ltx_text" style="font-size:90%;">72.3/67.7/73.1</span></td>
</tr>
<tr id="S4.T3.st1.6.5.5" class="ltx_tr">
<td id="S4.T3.st1.6.5.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st1.6.5.5.1.1" class="ltx_text" style="font-size:90%;">RGB+Thermal</span></td>
<td id="S4.T3.st1.6.5.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.6pt;padding-right:0.6pt;">
<span id="S4.T3.st1.6.5.5.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">75.9</span><span id="S4.T3.st1.6.5.5.2.2" class="ltx_text" style="font-size:90%;">/</span><span id="S4.T3.st1.6.5.5.2.3" class="ltx_text ltx_font_bold" style="font-size:90%;">74.7</span><span id="S4.T3.st1.6.5.5.2.4" class="ltx_text" style="font-size:90%;">/</span><span id="S4.T3.st1.6.5.5.2.5" class="ltx_text ltx_font_bold" style="font-size:90%;">74.7</span>
</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.T3.st2" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">(b) </span>SUN RGB-D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite></figcaption>
<table id="S4.T3.st2.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.st2.6.1.1" class="ltx_tr">
<th id="S4.T3.st2.6.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st2.6.1.1.1.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<th id="S4.T3.st2.6.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st2.6.1.1.2.1" class="ltx_text" style="font-size:90%;">Modal</span></th>
<th id="S4.T3.st2.6.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st2.6.1.1.3.1" class="ltx_text" style="font-size:90%;">mIoU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.st2.6.2.1" class="ltx_tr">
<td id="S4.T3.st2.6.2.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" style="padding-left:0.6pt;padding-right:0.6pt;" rowspan="2"><span id="S4.T3.st2.6.2.1.1.1" class="ltx_text" style="font-size:90%;">SAMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite></span></td>
<td id="S4.T3.st2.6.2.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st2.6.2.1.2.1" class="ltx_text" style="font-size:90%;">RGB</span></td>
<td id="S4.T3.st2.6.2.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st2.6.2.1.3.1" class="ltx_text" style="font-size:90%;">78.7</span></td>
</tr>
<tr id="S4.T3.st2.6.3.2" class="ltx_tr">
<td id="S4.T3.st2.6.3.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st2.6.3.2.1.1" class="ltx_text" style="font-size:90%;">Depth*</span></td>
<td id="S4.T3.st2.6.3.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st2.6.3.2.2.1" class="ltx_text" style="font-size:90%;">68.1</span></td>
</tr>
<tr id="S4.T3.st2.6.4.3" class="ltx_tr">
<td id="S4.T3.st2.6.4.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:0.6pt;padding-right:0.6pt;" rowspan="2"><span id="S4.T3.st2.6.4.3.1.1" class="ltx_text" style="font-size:90%;">MM-SAM</span></td>
<td id="S4.T3.st2.6.4.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st2.6.4.3.2.1" class="ltx_text" style="font-size:90%;">Depth</span></td>
<td id="S4.T3.st2.6.4.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st2.6.4.3.3.1" class="ltx_text" style="font-size:90%;">77.2</span></td>
</tr>
<tr id="S4.T3.st2.6.5.4" class="ltx_tr">
<td id="S4.T3.st2.6.5.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st2.6.5.4.1.1" class="ltx_text" style="font-size:90%;">RGB+Depth</span></td>
<td id="S4.T3.st2.6.5.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st2.6.5.4.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">81.2</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.T3.st3" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">(c) </span>SemanticKITTIÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite></figcaption>
<table id="S4.T3.st3.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.st3.6.1.1" class="ltx_tr">
<th id="S4.T3.st3.6.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st3.6.1.1.1.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<th id="S4.T3.st3.6.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st3.6.1.1.2.1" class="ltx_text" style="font-size:90%;">Modal</span></th>
<th id="S4.T3.st3.6.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st3.6.1.1.3.1" class="ltx_text" style="font-size:90%;">mIoU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.st3.6.2.1" class="ltx_tr">
<td id="S4.T3.st3.6.2.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" style="padding-left:0.6pt;padding-right:0.6pt;" rowspan="2"><span id="S4.T3.st3.6.2.1.1.1" class="ltx_text" style="font-size:90%;">SAMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite></span></td>
<td id="S4.T3.st3.6.2.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st3.6.2.1.2.1" class="ltx_text" style="font-size:90%;">RGB</span></td>
<td id="S4.T3.st3.6.2.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st3.6.2.1.3.1" class="ltx_text" style="font-size:90%;">67.8</span></td>
</tr>
<tr id="S4.T3.st3.6.3.2" class="ltx_tr">
<td id="S4.T3.st3.6.3.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st3.6.3.2.1.1" class="ltx_text" style="font-size:90%;">LiDAR*</span></td>
<td id="S4.T3.st3.6.3.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st3.6.3.2.2.1" class="ltx_text" style="font-size:90%;">60.1</span></td>
</tr>
<tr id="S4.T3.st3.6.4.3" class="ltx_tr">
<td id="S4.T3.st3.6.4.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:0.6pt;padding-right:0.6pt;" rowspan="2"><span id="S4.T3.st3.6.4.3.1.1" class="ltx_text" style="font-size:90%;">MM-SAM</span></td>
<td id="S4.T3.st3.6.4.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st3.6.4.3.2.1" class="ltx_text" style="font-size:90%;">LiDAR</span></td>
<td id="S4.T3.st3.6.4.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st3.6.4.3.3.1" class="ltx_text" style="font-size:90%;">68.7</span></td>
</tr>
<tr id="S4.T3.st3.6.5.4" class="ltx_tr">
<td id="S4.T3.st3.6.5.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st3.6.5.4.1.1" class="ltx_text" style="font-size:90%;">RGB+LiDAR</span></td>
<td id="S4.T3.st3.6.5.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:0.6pt;padding-right:0.6pt;"><span id="S4.T3.st3.6.5.4.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">69.9</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Time-Asynchronous Sensor Suites</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">We further evaluate MM-SAM over challenging time-asynchronous sensor suites. We examine it on commonly used earth-observation datasets that often involve significant time gaps and variations in scanning angles and resolutions, introducing substantial domain discrepancies across modalities. Table <a href="#S4.T4" title="Table 4 â€£ 4.2.2 Time-Asynchronous Sensor Suites â€£ 4.2 Segmentation Results â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents experiments for RGB images paired with SAR in DFC2023, HSI and MS-LiDAR in DFC2018, and DSM in ISPRS Potsdam. Similar to the experiments in Table <a href="#S4.T3" title="Table 3 â€£ 4.2.1 Time-Synchronized Sensor Suites â€£ 4.2 Segmentation Results â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, MM-SAM demonstrates advanced cross-modal segmentation performance and harvests the benefits of multi-modal sensors effectively.</p>
</div>
<figure id="S4.T4" class="ltx_table">

<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.6.1.1" class="ltx_text" style="font-size:113%;">Table 4</span>: </span><span id="S4.T4.7.2" class="ltx_text" style="font-size:113%;">Segmentation results over time-<span id="S4.T4.7.2.1" class="ltx_text ltx_font_italic">asynchronous</span> sensor suites using bounding box prompts. The symbol * denotes false-color images transformed from each non-RGB modality.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T4.tab1" class="ltx_table ltx_figure_panel">
<table id="S4.T4.tab1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.tab1.1.1.1" class="ltx_tr">
<th id="S4.T4.tab1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:8.5pt;padding-right:8.5pt;" colspan="3"><span id="S4.T4.tab1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">(a) DFC2023</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.tab1.1.2.1" class="ltx_tr">
<td id="S4.T4.tab1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.2.1.1.1" class="ltx_text" style="font-size:80%;">Model</span></td>
<td id="S4.T4.tab1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.2.1.2.1" class="ltx_text" style="font-size:80%;">Modal</span></td>
<td id="S4.T4.tab1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.2.1.3.1" class="ltx_text" style="font-size:80%;">IoU</span></td>
</tr>
<tr id="S4.T4.tab1.1.3.2" class="ltx_tr">
<td id="S4.T4.tab1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;" rowspan="2"><span id="S4.T4.tab1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">SAMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite></span></td>
<td id="S4.T4.tab1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.3.2.2.1" class="ltx_text" style="font-size:80%;">RGB</span></td>
<td id="S4.T4.tab1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.3.2.3.1" class="ltx_text" style="font-size:80%;">75.3</span></td>
</tr>
<tr id="S4.T4.tab1.1.4.3" class="ltx_tr">
<td id="S4.T4.tab1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.4.3.1.1" class="ltx_text" style="font-size:80%;">SAR*</span></td>
<td id="S4.T4.tab1.1.4.3.2" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.4.3.2.1" class="ltx_text" style="font-size:80%;">53.0</span></td>
</tr>
<tr id="S4.T4.tab1.1.5.4" class="ltx_tr">
<td id="S4.T4.tab1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;" rowspan="2"><span id="S4.T4.tab1.1.5.4.1.1" class="ltx_text" style="font-size:80%;">MM-SAM</span></td>
<td id="S4.T4.tab1.1.5.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.5.4.2.1" class="ltx_text" style="font-size:80%;">SAR</span></td>
<td id="S4.T4.tab1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.5.4.3.1" class="ltx_text" style="font-size:80%;">67.5</span></td>
</tr>
<tr id="S4.T4.tab1.1.6.5" class="ltx_tr">
<td id="S4.T4.tab1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.6.5.1.1" class="ltx_text" style="font-size:80%;">RGB+SAR</span></td>
<td id="S4.T4.tab1.1.6.5.2" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.6.5.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">77.4</span></td>
</tr>
<tr id="S4.T4.tab1.1.7.6" class="ltx_tr">
<th id="S4.T4.tab1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;" colspan="3"><span id="S4.T4.tab1.1.7.6.1.1" class="ltx_text" style="font-size:80%;">(c) ISPRS Potsdam</span></th>
</tr>
<tr id="S4.T4.tab1.1.8.7" class="ltx_tr">
<td id="S4.T4.tab1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.8.7.1.1" class="ltx_text" style="font-size:80%;">Model</span></td>
<td id="S4.T4.tab1.1.8.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.8.7.2.1" class="ltx_text" style="font-size:80%;">Modal</span></td>
<td id="S4.T4.tab1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.8.7.3.1" class="ltx_text" style="font-size:80%;">IoU</span></td>
</tr>
<tr id="S4.T4.tab1.1.9.8" class="ltx_tr">
<td id="S4.T4.tab1.1.9.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;" rowspan="2"><span id="S4.T4.tab1.1.9.8.1.1" class="ltx_text" style="font-size:80%;">SAMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite></span></td>
<td id="S4.T4.tab1.1.9.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.9.8.2.1" class="ltx_text" style="font-size:80%;">RGB</span></td>
<td id="S4.T4.tab1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.9.8.3.1" class="ltx_text" style="font-size:80%;">75.0</span></td>
</tr>
<tr id="S4.T4.tab1.1.10.9" class="ltx_tr">
<td id="S4.T4.tab1.1.10.9.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.10.9.1.1" class="ltx_text" style="font-size:80%;">DSM*</span></td>
<td id="S4.T4.tab1.1.10.9.2" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.10.9.2.1" class="ltx_text" style="font-size:80%;">74.3</span></td>
</tr>
<tr id="S4.T4.tab1.1.11.10" class="ltx_tr">
<td id="S4.T4.tab1.1.11.10.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;" rowspan="2"><span id="S4.T4.tab1.1.11.10.1.1" class="ltx_text" style="font-size:80%;">MM-SAM</span></td>
<td id="S4.T4.tab1.1.11.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.11.10.2.1" class="ltx_text" style="font-size:80%;">DSM</span></td>
<td id="S4.T4.tab1.1.11.10.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.11.10.3.1" class="ltx_text" style="font-size:80%;">79.1</span></td>
</tr>
<tr id="S4.T4.tab1.1.12.11" class="ltx_tr">
<td id="S4.T4.tab1.1.12.11.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.12.11.1.1" class="ltx_text" style="font-size:80%;">RGB+DSM</span></td>
<td id="S4.T4.tab1.1.12.11.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S4.T4.tab1.1.12.11.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">83.6</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T4.tab2" class="ltx_table ltx_figure_panel">
<table id="S4.T4.tab2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.tab2.1.1.1" class="ltx_tr">
<th id="S4.T4.tab2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column" style="padding-top:0.6pt;padding-bottom:0.6pt;" colspan="3"><span id="S4.T4.tab2.1.1.1.1.1" class="ltx_text" style="font-size:80%;">(b) DFC2018</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.tab2.1.2.1" class="ltx_tr">
<td id="S4.T4.tab2.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.2.1.1.1" class="ltx_text" style="font-size:80%;">Model</span></td>
<td id="S4.T4.tab2.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.2.1.2.1" class="ltx_text" style="font-size:80%;">Modal</span></td>
<td id="S4.T4.tab2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.2.1.3.1" class="ltx_text" style="font-size:80%;">IoU</span></td>
</tr>
<tr id="S4.T4.tab2.1.3.2" class="ltx_tr">
<td id="S4.T4.tab2.1.3.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.6pt;padding-bottom:0.6pt;" rowspan="2"><span id="S4.T4.tab2.1.3.2.1.1" class="ltx_text" style="font-size:80%;">SAMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite></span></td>
<td id="S4.T4.tab2.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.3.2.2.1" class="ltx_text" style="font-size:80%;">RGB</span></td>
<td id="S4.T4.tab2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.3.2.3.1" class="ltx_text" style="font-size:80%;">78.1</span></td>
</tr>
<tr id="S4.T4.tab2.1.4.3" class="ltx_tr">
<td id="S4.T4.tab2.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.4.3.1.1" class="ltx_text" style="font-size:80%;">HSI*</span></td>
<td id="S4.T4.tab2.1.4.3.2" class="ltx_td ltx_align_center" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.4.3.2.1" class="ltx_text" style="font-size:80%;">69.5</span></td>
</tr>
<tr id="S4.T4.tab2.1.5.4" class="ltx_tr">
<td id="S4.T4.tab2.1.5.4.1" class="ltx_td ltx_border_r" style="padding-top:0.6pt;padding-bottom:0.6pt;"></td>
<td id="S4.T4.tab2.1.5.4.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.5.4.2.1" class="ltx_text" style="font-size:80%;">MS-LiDAR*</span></td>
<td id="S4.T4.tab2.1.5.4.3" class="ltx_td ltx_align_center" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.5.4.3.1" class="ltx_text" style="font-size:80%;">75.1</span></td>
</tr>
<tr id="S4.T4.tab2.1.6.5" class="ltx_tr">
<td id="S4.T4.tab2.1.6.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.6pt;padding-bottom:0.6pt;" rowspan="5"><span id="S4.T4.tab2.1.6.5.1.1" class="ltx_text" style="font-size:80%;">MM-SAM</span></td>
<td id="S4.T4.tab2.1.6.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.6.5.2.1" class="ltx_text" style="font-size:80%;">HSI</span></td>
<td id="S4.T4.tab2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.6.5.3.1" class="ltx_text" style="font-size:80%;">78.1</span></td>
</tr>
<tr id="S4.T4.tab2.1.7.6" class="ltx_tr">
<td id="S4.T4.tab2.1.7.6.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.7.6.1.1" class="ltx_text" style="font-size:80%;">MS-LiDAR</span></td>
<td id="S4.T4.tab2.1.7.6.2" class="ltx_td ltx_align_center" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.7.6.2.1" class="ltx_text" style="font-size:80%;">85.1</span></td>
</tr>
<tr id="S4.T4.tab2.1.8.7" class="ltx_tr">
<td id="S4.T4.tab2.1.8.7.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.8.7.1.1" class="ltx_text" style="font-size:80%;">RGB+HSI</span></td>
<td id="S4.T4.tab2.1.8.7.2" class="ltx_td ltx_align_center" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.8.7.2.1" class="ltx_text" style="font-size:80%;">88.5</span></td>
</tr>
<tr id="S4.T4.tab2.1.9.8" class="ltx_tr">
<td id="S4.T4.tab2.1.9.8.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.9.8.1.1" class="ltx_text" style="font-size:80%;">RGB+MS-LiDAR</span></td>
<td id="S4.T4.tab2.1.9.8.2" class="ltx_td ltx_align_center" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.9.8.2.1" class="ltx_text" style="font-size:80%;">87.9</span></td>
</tr>
<tr id="S4.T4.tab2.1.10.9" class="ltx_tr">
<td id="S4.T4.tab2.1.10.9.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.10.9.1.1" class="ltx_text" style="font-size:80%;">HSI+MS-LiDAR</span></td>
<td id="S4.T4.tab2.1.10.9.2" class="ltx_td ltx_align_center" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.10.9.2.1" class="ltx_text" style="font-size:80%;">86.5</span></td>
</tr>
<tr id="S4.T4.tab2.1.11.10" class="ltx_tr">
<td id="S4.T4.tab2.1.11.10.1" class="ltx_td ltx_border_b ltx_border_r" style="padding-top:0.6pt;padding-bottom:0.6pt;"></td>
<td id="S4.T4.tab2.1.11.10.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.11.10.2.1" class="ltx_text" style="font-size:80%;">RGB+HSI+MS-LiDAR</span></td>
<td id="S4.T4.tab2.1.11.10.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.6pt;padding-bottom:0.6pt;"><span id="S4.T4.tab2.1.11.10.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">89.3</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p"><span id="S4.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">MM-SAM for More Sensor Modalities.</span> Table <a href="#S4.T4" title="Table 4 â€£ 4.2.2 Time-Asynchronous Sensor Suites â€£ 4.2 Segmentation Results â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (b) examines MM-SAMâ€™s performance with suites containing three sensor modalities: RGB, HSI, and MS-LiDAR. MM-SAM achieves impressive cross-modal segmentation with both HSI and MS-LiDAR. Moreover, fusing RGB with either HSI or MS-LiDAR results in consistent segmentation improvements. Notably, combining all three modalities yields the best performance, surpassing both the fusion of any two modalities and the results from individual modalities. This highlights MM-SAMâ€™s scalability, showcasing its ability to accommodate additional sensors and develop more comprehensive perception systems for various applications.</p>
</div>
<div id="S4.SS2.SSS2.p3" class="ltx_para">
<p id="S4.SS2.SSS2.p3.2" class="ltx_p"><span id="S4.SS2.SSS2.p3.2.1" class="ltx_text ltx_font_bold">Fusion without RGB.</span> Another notable observation in Table <a href="#S4.T4" title="Table 4 â€£ 4.2.2 Time-Asynchronous Sensor Suites â€£ 4.2 Segmentation Results â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (b) is MM-SAMâ€™s ability to perform fusion of two non-RGB modalities (<math id="S4.SS2.SSS2.p3.1.m1.2" class="ltx_Math" alttext="X_{1},X_{2}" display="inline"><semantics id="S4.SS2.SSS2.p3.1.m1.2a"><mrow id="S4.SS2.SSS2.p3.1.m1.2.2.2" xref="S4.SS2.SSS2.p3.1.m1.2.2.3.cmml"><msub id="S4.SS2.SSS2.p3.1.m1.1.1.1.1" xref="S4.SS2.SSS2.p3.1.m1.1.1.1.1.cmml"><mi id="S4.SS2.SSS2.p3.1.m1.1.1.1.1.2" xref="S4.SS2.SSS2.p3.1.m1.1.1.1.1.2.cmml">X</mi><mn id="S4.SS2.SSS2.p3.1.m1.1.1.1.1.3" xref="S4.SS2.SSS2.p3.1.m1.1.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS2.SSS2.p3.1.m1.2.2.2.3" xref="S4.SS2.SSS2.p3.1.m1.2.2.3.cmml">,</mo><msub id="S4.SS2.SSS2.p3.1.m1.2.2.2.2" xref="S4.SS2.SSS2.p3.1.m1.2.2.2.2.cmml"><mi id="S4.SS2.SSS2.p3.1.m1.2.2.2.2.2" xref="S4.SS2.SSS2.p3.1.m1.2.2.2.2.2.cmml">X</mi><mn id="S4.SS2.SSS2.p3.1.m1.2.2.2.2.3" xref="S4.SS2.SSS2.p3.1.m1.2.2.2.2.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p3.1.m1.2b"><list id="S4.SS2.SSS2.p3.1.m1.2.2.3.cmml" xref="S4.SS2.SSS2.p3.1.m1.2.2.2"><apply id="S4.SS2.SSS2.p3.1.m1.1.1.1.1.cmml" xref="S4.SS2.SSS2.p3.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p3.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.p3.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.SSS2.p3.1.m1.1.1.1.1.2.cmml" xref="S4.SS2.SSS2.p3.1.m1.1.1.1.1.2">ğ‘‹</ci><cn type="integer" id="S4.SS2.SSS2.p3.1.m1.1.1.1.1.3.cmml" xref="S4.SS2.SSS2.p3.1.m1.1.1.1.1.3">1</cn></apply><apply id="S4.SS2.SSS2.p3.1.m1.2.2.2.2.cmml" xref="S4.SS2.SSS2.p3.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p3.1.m1.2.2.2.2.1.cmml" xref="S4.SS2.SSS2.p3.1.m1.2.2.2.2">subscript</csymbol><ci id="S4.SS2.SSS2.p3.1.m1.2.2.2.2.2.cmml" xref="S4.SS2.SSS2.p3.1.m1.2.2.2.2.2">ğ‘‹</ci><cn type="integer" id="S4.SS2.SSS2.p3.1.m1.2.2.2.2.3.cmml" xref="S4.SS2.SSS2.p3.1.m1.2.2.2.2.3">2</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.1.m1.2c">X_{1},X_{2}</annotation></semantics></math>), without relying on paired RGB images (i.e., RGB+<math id="S4.SS2.SSS2.p3.2.m2.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S4.SS2.SSS2.p3.2.m2.1a"><mi id="S4.SS2.SSS2.p3.2.m2.1.1" xref="S4.SS2.SSS2.p3.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p3.2.m2.1b"><ci id="S4.SS2.SSS2.p3.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p3.2.m2.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.2.m2.1c">X</annotation></semantics></math>). Specifically, by training on pairs of (RGB, HSI) and (RGB, MS-LiDAR), MM-SAM achieves effective fusion of HSI and MS-LiDAR ("HSI+MS-LiDAR" in the table). In the experiments, we first train two adapted image encoders for HSI and MS-LiDAR with Unsupervised Cross-Modal Transfer on (RGB, HSI) and (RGB, MS-LiDAR) pairs, respectively, without involving SFG. Then, we perform cross-modal segmentation on HSI and MS-LiDAR to generate multi-modal pseudo labels, which are used to train an SFG for (HSI, MS-LiDAR) fusion. The results show better segmentation than using HSI or MS-LiDAR alone, suggesting that MM-SAM can potentially be deployed in sensor suites without RGB cameras, revealing further opportunities for sensor fusion.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Discussions and Analysis</h3>

<figure id="S4.F4" class="ltx_figure"><img src="/html/2408.09085/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="182" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Visual illustration of adaptive fusion for enhanced segmentation with MM-SAM, using one sample of paired RGB and thermal images from the MFNet dataset. The second column shows fusion weights from the SFG, where brighter areas represent higher weights.</span></figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Visual Analysis of Selective Fusion Gate (SFG).</span> To understand how SFG dynamically adjusts the weighting of different sensors based on multi-modal inputs, we analyze an example from the MFNet dataset, as shown in Figure <a href="#S4.F4" title="Figure 4 â€£ 4.3 Discussions and Analysis â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. In this scenario, the carâ€™s high beam creates strong light interference, making it challenging to recognize and segment the car in the RGB image. In contrast, the thermal image remains unaffected by the visible light. Consequently, SFG assigns a significantly higher weight to the thermal image in this area and a much lower weight to the corresponding RGB image area. This adaptive weighting results in more accurate segmentation. This example demonstrates how SFG manages complex and dynamic situations within sensor suites, effectively leveraging the strengths of each modality to improve segmentation robustness and accuracy.</p>
</div>
<figure id="S4.T5" class="ltx_table">

<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.7.1.1" class="ltx_text" style="font-size:113%;">Table 5</span>: </span><span id="S4.T5.8.2" class="ltx_text" style="font-size:113%;">Zero-shot Segmentation results. For FreiburgThermalÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite>, mIoU is reported for total/day/night. The symbol * denotes false-color images transformed from each non-RGB modality. </span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T5.st1" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.st1.9.1.1" class="ltx_text" style="font-size:113%;">(a)</span> </span><span id="S4.T5.st1.10.2" class="ltx_text" style="font-size:113%;">MFNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>â†’FreiburgThermalÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite></span></figcaption>
<table id="S4.T5.st1.11" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T5.st1.11.1.1" class="ltx_tr">
<td id="S4.T5.st1.11.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st1.11.1.1.1.1" class="ltx_text" style="font-size:80%;">Model</span></td>
<td id="S4.T5.st1.11.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st1.11.1.1.2.1" class="ltx_text" style="font-size:80%;">Modal</span></td>
<td id="S4.T5.st1.11.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st1.11.1.1.3.1" class="ltx_text" style="font-size:80%;">FreiburgThermal</span></td>
</tr>
<tr id="S4.T5.st1.11.2.2" class="ltx_tr">
<td id="S4.T5.st1.11.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" rowspan="2"><span id="S4.T5.st1.11.2.2.1.1" class="ltx_text" style="font-size:80%;">SAMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite></span></td>
<td id="S4.T5.st1.11.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st1.11.2.2.2.1" class="ltx_text" style="font-size:80%;">RGB</span></td>
<td id="S4.T5.st1.11.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st1.11.2.2.3.1" class="ltx_text" style="font-size:80%;">65.3/71.8/60.7</span></td>
</tr>
<tr id="S4.T5.st1.11.3.3" class="ltx_tr">
<td id="S4.T5.st1.11.3.3.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st1.11.3.3.1.1" class="ltx_text" style="font-size:80%;">Thermal*</span></td>
<td id="S4.T5.st1.11.3.3.2" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st1.11.3.3.2.1" class="ltx_text" style="font-size:80%;">62.2/61.9/62.3</span></td>
</tr>
<tr id="S4.T5.st1.11.4.4" class="ltx_tr">
<td id="S4.T5.st1.11.4.4.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" rowspan="2"><span id="S4.T5.st1.11.4.4.1.1" class="ltx_text" style="font-size:80%;">MM-SAM</span></td>
<td id="S4.T5.st1.11.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st1.11.4.4.2.1" class="ltx_text" style="font-size:80%;">Thermal</span></td>
<td id="S4.T5.st1.11.4.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st1.11.4.4.3.1" class="ltx_text" style="font-size:80%;">66.5/66.2/66.4</span></td>
</tr>
<tr id="S4.T5.st1.11.5.5" class="ltx_tr">
<td id="S4.T5.st1.11.5.5.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st1.11.5.5.1.1" class="ltx_text" style="font-size:80%;">RGB+Thermal</span></td>
<td id="S4.T5.st1.11.5.5.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="S4.T5.st1.11.5.5.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">70.8</span><span id="S4.T5.st1.11.5.5.2.2" class="ltx_text" style="font-size:80%;">/</span><span id="S4.T5.st1.11.5.5.2.3" class="ltx_text ltx_font_bold" style="font-size:80%;">72.8</span><span id="S4.T5.st1.11.5.5.2.4" class="ltx_text" style="font-size:80%;">/</span><span id="S4.T5.st1.11.5.5.2.5" class="ltx_text ltx_font_bold" style="font-size:80%;">69.0</span>
</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T5.st2" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.st2.12.1.1" class="ltx_text" style="font-size:113%;">(b)</span> </span><span id="S4.T5.st2.13.2" class="ltx_text" style="font-size:113%;">SUN RGB-DÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>â†’NYUÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>&amp;B3DOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite></span></figcaption>
<table id="S4.T5.st2.14" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.st2.14.1.1" class="ltx_tr">
<th id="S4.T5.st2.14.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st2.14.1.1.1.1" class="ltx_text" style="font-size:80%;">Model</span></th>
<th id="S4.T5.st2.14.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st2.14.1.1.2.1" class="ltx_text" style="font-size:80%;">Modal</span></th>
<th id="S4.T5.st2.14.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st2.14.1.1.3.1" class="ltx_text" style="font-size:80%;">NYU</span></th>
<th id="S4.T5.st2.14.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st2.14.1.1.4.1" class="ltx_text" style="font-size:80%;">B3DO</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.st2.14.2.1" class="ltx_tr">
<th id="S4.T5.st2.14.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" rowspan="2"><span id="S4.T5.st2.14.2.1.1.1" class="ltx_text" style="font-size:80%;">SAMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite></span></th>
<th id="S4.T5.st2.14.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st2.14.2.1.2.1" class="ltx_text" style="font-size:80%;">RGB</span></th>
<td id="S4.T5.st2.14.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st2.14.2.1.3.1" class="ltx_text" style="font-size:80%;">79.5</span></td>
<td id="S4.T5.st2.14.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st2.14.2.1.4.1" class="ltx_text" style="font-size:80%;">77.4</span></td>
</tr>
<tr id="S4.T5.st2.14.3.2" class="ltx_tr">
<th id="S4.T5.st2.14.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st2.14.3.2.1.1" class="ltx_text" style="font-size:80%;">Depth*</span></th>
<td id="S4.T5.st2.14.3.2.2" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st2.14.3.2.2.1" class="ltx_text" style="font-size:80%;">69.0</span></td>
<td id="S4.T5.st2.14.3.2.3" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st2.14.3.2.3.1" class="ltx_text" style="font-size:80%;">67.1</span></td>
</tr>
<tr id="S4.T5.st2.14.4.3" class="ltx_tr">
<th id="S4.T5.st2.14.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;" rowspan="2"><span id="S4.T5.st2.14.4.3.1.1" class="ltx_text" style="font-size:80%;">MM-SAM</span></th>
<th id="S4.T5.st2.14.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st2.14.4.3.2.1" class="ltx_text" style="font-size:80%;">Depth</span></th>
<td id="S4.T5.st2.14.4.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st2.14.4.3.3.1" class="ltx_text" style="font-size:80%;">75.5</span></td>
<td id="S4.T5.st2.14.4.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st2.14.4.3.4.1" class="ltx_text" style="font-size:80%;">73.4</span></td>
</tr>
<tr id="S4.T5.st2.14.5.4" class="ltx_tr">
<th id="S4.T5.st2.14.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st2.14.5.4.1.1" class="ltx_text" style="font-size:80%;">RGB+Depth</span></th>
<td id="S4.T5.st2.14.5.4.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st2.14.5.4.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">81.4</span></td>
<td id="S4.T5.st2.14.5.4.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S4.T5.st2.14.5.4.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">80.1</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Zero-shot Segmentation.</span> We evaluated the generalization ability of MM-SAM on unseen domains for zero-shot segmentation tasks MFNetâ†’FreiburgThermalÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> datasets (both with RGB plus thermal) and SUN RGB-Dâ†’NYUÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>&amp;B3DOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> datasets (all with RGB plus depth).
As detailed in AppendixÂ <a href="#A3.SS2" title="C.2 Zero-shot experimental details â€£ Appendix C Implementation Details â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C.2</span></a>, for MFNetâ†’FreiburgThermal, we use the model trained on MFNet (in TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.2.1 Time-Synchronized Sensor Suites â€£ 4.2 Segmentation Results â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a)) and test it on FreiburgThermal; While for SUN RGB-Dâ†’NYU&amp;B3DO, we re-trained MM-SAM using the SUN RGB-D training set but excluding its subsets NYU&amp;B3DO for cross-sensor testingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>.
The results are presented in Table <a href="#S4.T5" title="Table 5 â€£ 4.3 Discussions and Analysis â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
MM-SAM demonstrates superior and consistent zero-shot segmentation performance for both cross-modal segmentation and multi-modal fusion. The trend mirrors the positive results observed in previous intra-domain evaluations. These findings underscore the zero-shot potential of MM-SAM, highlighting its generalizability and effectiveness in segmentation to unseen domains.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">MM-SAM with different tuning approaches.</span> We assessed the effectiveness of various parameter-efficient tuning (PEFT) methods within MM-SAM for extracting modality-specific features. Specifically, we integrated three state-of-the-art PEFT methods: LoRAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>, AdapterFormerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>, and VPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> in the image encoder. Figure <a href="#S4.F5" title="Figure 5 â€£ 4.3 Discussions and Analysis â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (a) compares the number of introduced trainable parameters and their performance on the MFNet dataset. The results show that LoRA introduces the fewest trainable parameters while achieving the best performance. We thus empirically select LoRA for the final implementation of MM-SAM. Nevertheless, all three PEFT methods demonstrate improved cross-modal segmentation and exceptional multi-modal segmentation capabilities, indicating MM-SAMâ€™s versatility and compatibility with various PEFT methods.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2408.09085/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="103" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Segmentation performance of MM-SAM on the MFNet ("Total" split) using different parameter-efficient tuning (PEFT) methods in (a) and various ViT backbones in (b).</span></figcaption>
</figure>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">MM-SAM with different backbones.</span> We evaluate MM-SAMâ€™s performance using various backbones for SAMâ€™s image encoder.
Figure <a href="#S4.F5" title="Figure 5 â€£ 4.3 Discussions and Analysis â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (b) presents results of MM-SAM variants with ViT-B, ViT-L, and ViT-HÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> based image encoders on the MFNet dataset, following the same setup as in Table <a href="#S4.T3" title="Table 3 â€£ 4.2.1 Time-Synchronized Sensor Suites â€£ 4.2 Segmentation Results â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a).
The results show that MM-SAM is robust to backbone variations and achieves consistently advanced cross-modal and multi-modal segmentation across different encoder architectures.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para ltx_noindent">
<p id="S4.SS3.p5.1" class="ltx_p"><span id="S4.SS3.p5.1.1" class="ltx_text ltx_font_bold">Limitations.</span>
Though MM-SAM just introduces a small number of extra parameters, it remains computationally intensive and cannot operate at real-time speeds because of its dependence on SAM. This reliance demands substantial GPU resources, restricting its use in applications like video processing. In addition, similar to SAM, it is limited to binary mask segmentation and does not perform semantic or panoptic segmentation. Training MM-SAM requires paired modalities with RGB images, meaning an RGB camera must be included in sensor suites to collect training data. However, this constraint does not apply during inference.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this study, we extended and expanded the Segment Anything Model (SAM) to accommodate various sensor suites. We proposed MM-SAM, a parameter-efficient and label-efficient adaptation method that enhances SAMâ€™s capabilities for cross-modal and multi-modal segmentation. By utilizing mask-free training, our approach significantly improves adaptation efficiency. Extensive evaluations across seven datasets and eight different sensor modalities demonstrated that our method significantly enhances SAMâ€™s robustness and performance in complex and dynamic scenarios.
We hope that MM-SAM could lay a strong foundation and encourage future research to provide deeper insights into visual foundation models for sensor suites.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.2.2.1" class="ltx_text" style="font-size:90%;">[1]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.4.1" class="ltx_text" style="font-size:90%;">
Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.5.1" class="ltx_text" style="font-size:90%;">Semantickitti: A dataset for semantic scene understanding of lidar sequences.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</span><span id="bib.bib1.8.3" class="ltx_text" style="font-size:90%;">, pages 9297â€“9307, 2019.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.2.2.1" class="ltx_text" style="font-size:90%;">[2]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.4.1" class="ltx_text" style="font-size:90%;">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.5.1" class="ltx_text" style="font-size:90%;">Language models are few-shot learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib2.7.2" class="ltx_text" style="font-size:90%;">, 33:1877â€“1901, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.2.2.1" class="ltx_text" style="font-size:90%;">[3]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.4.1" class="ltx_text" style="font-size:90%;">
Keyan Chen, Chenyang Liu, Hao Chen, Haotian Zhang, Wenyuan Li, Zhengxia Zou, and Zhenwei Shi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.5.1" class="ltx_text" style="font-size:90%;">Rsprompter: Learning to prompt for remote sensing instance segmentation based on visual foundation model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Geoscience and Remote Sensing</span><span id="bib.bib3.7.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.2.2.1" class="ltx_text" style="font-size:90%;">[4]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.4.1" class="ltx_text" style="font-size:90%;">
Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.5.1" class="ltx_text" style="font-size:90%;">Adaptformer: Adapting vision transformers for scalable visual recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib4.7.2" class="ltx_text" style="font-size:90%;">, 35:16664â€“16678, 2022.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.2.2.1" class="ltx_text" style="font-size:90%;">[5]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.4.1" class="ltx_text" style="font-size:90%;">
Tianrun Chen, Lanyun Zhu, Chaotao Deng, Runlong Cao, Yan Wang, Shangzhan Zhang, Zejian Li, Lingyun Sun, Ying Zang, and Papa Mao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.5.1" class="ltx_text" style="font-size:90%;">Sam-adapter: Adapting segment anything in underperformed scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span id="bib.bib5.8.3" class="ltx_text" style="font-size:90%;">, pages 3367â€“3375, 2023.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.2.2.1" class="ltx_text" style="font-size:90%;">[6]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.4.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.5.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2010.11929</span><span id="bib.bib6.7.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.2.2.1" class="ltx_text" style="font-size:90%;">[7]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.4.1" class="ltx_text" style="font-size:90%;">
Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and YuÂ Qiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.5.1" class="ltx_text" style="font-size:90%;">Clip-adapter: Better vision-language models with feature adapters.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib7.7.2" class="ltx_text" style="font-size:90%;">, pages 1â€“15, 2023.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.2.2.1" class="ltx_text" style="font-size:90%;">[8]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.4.1" class="ltx_text" style="font-size:90%;">
Shizhan Gong, Yuan Zhong, Wenao Ma, Jinpeng Li, Zhao Wang, Jingyang Zhang, Pheng-Ann Heng, and QiÂ Dou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.5.1" class="ltx_text" style="font-size:90%;">3dsam-adapter: Holistic adaptation of sam from 2d to 3d for promptable medical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.13465</span><span id="bib.bib8.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.2.2.1" class="ltx_text" style="font-size:90%;">[9]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.4.1" class="ltx_text" style="font-size:90%;">
Saurabh Gupta, Judy Hoffman, and Jitendra Malik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.5.1" class="ltx_text" style="font-size:90%;">Cross modal distillation for supervision transfer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span id="bib.bib9.8.3" class="ltx_text" style="font-size:90%;">, pages 2827â€“2836, 2016.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.2.2.1" class="ltx_text" style="font-size:90%;">[10]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.4.1" class="ltx_text" style="font-size:90%;">
Qishen Ha, Kohei Watanabe, Takumi Karasawa, Yoshitaka Ushiku, and Tatsuya Harada.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.5.1" class="ltx_text" style="font-size:90%;">Mfnet: Towards real-time semantic segmentation for autonomous vehicles with multi-spectral scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</span><span id="bib.bib10.8.3" class="ltx_text" style="font-size:90%;">, pages 5108â€“5115. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.2.2.1" class="ltx_text" style="font-size:90%;">[11]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.4.1" class="ltx_text" style="font-size:90%;">
Caner Hazirbas, Lingni Ma, Csaba Domokos, and Daniel Cremers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.5.1" class="ltx_text" style="font-size:90%;">Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Visionâ€“ACCV 2016: 13th Asian Conference on Computer Vision, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part I 13</span><span id="bib.bib11.8.3" class="ltx_text" style="font-size:90%;">, pages 213â€“228. Springer, 2017.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.2.2.1" class="ltx_text" style="font-size:90%;">[12]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.4.1" class="ltx_text" style="font-size:90%;">
Danfeng Hong, Lianru Gao, Naoto Yokoya, Jing Yao, Jocelyn Chanussot, Qian Du, and Bing Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.5.1" class="ltx_text" style="font-size:90%;">More diverse means better: Multimodal deep learning meets remote-sensing imagery classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Geoscience and Remote Sensing</span><span id="bib.bib12.7.2" class="ltx_text" style="font-size:90%;">, 59(5):4340â€“4354, 2020.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.2.2.1" class="ltx_text" style="font-size:90%;">[13]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.4.1" class="ltx_text" style="font-size:90%;">
EdwardÂ J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, LuÂ Wang, and Weizhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.5.1" class="ltx_text" style="font-size:90%;">Lora: Low-rank adaptation of large language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2106.09685</span><span id="bib.bib13.7.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.2.2.1" class="ltx_text" style="font-size:90%;">[14]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.4.1" class="ltx_text" style="font-size:90%;">
Allison Janoch, Sergey Karayev, Yangqing Jia, JonathanÂ T Barron, Mario Fritz, Kate Saenko, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.5.1" class="ltx_text" style="font-size:90%;">A category-level 3d object dataset: Putting the kinect to work.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Consumer Depth Cameras for Computer Vision: Research Topics and Applications</span><span id="bib.bib14.7.2" class="ltx_text" style="font-size:90%;">, pages 141â€“165, 2013.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.2.2.1" class="ltx_text" style="font-size:90%;">[15]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.4.1" class="ltx_text" style="font-size:90%;">
Maximilian Jaritz, Tuan-Hung Vu, RaoulÂ de Charette, Emilie Wirbel, and Patrick PÃ©rez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.5.1" class="ltx_text" style="font-size:90%;">xmuda: Cross-modal unsupervised domain adaptation for 3d semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span id="bib.bib15.8.3" class="ltx_text" style="font-size:90%;">, pages 12605â€“12614, 2020.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.2.2.1" class="ltx_text" style="font-size:90%;">[16]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.4.1" class="ltx_text" style="font-size:90%;">
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.5.1" class="ltx_text" style="font-size:90%;">Visual prompt tuning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib16.8.3" class="ltx_text" style="font-size:90%;">, pages 709â€“727. Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.2.2.1" class="ltx_text" style="font-size:90%;">[17]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.4.1" class="ltx_text" style="font-size:90%;">
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.5.1" class="ltx_text" style="font-size:90%;">Visual prompt tuning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib17.8.3" class="ltx_text" style="font-size:90%;">, pages 709â€“727. Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.2.2.1" class="ltx_text" style="font-size:90%;">[18]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.4.1" class="ltx_text" style="font-size:90%;">
Lei Ke, Mingqiao Ye, Martin Danelljan, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.5.1" class="ltx_text" style="font-size:90%;">Segment anything in high quality.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib18.7.2" class="ltx_text" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.2.2.1" class="ltx_text" style="font-size:90%;">[19]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.4.1" class="ltx_text" style="font-size:90%;">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, AlexanderÂ C Berg, Wan-Yen Lo, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.5.1" class="ltx_text" style="font-size:90%;">Segment anything.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span id="bib.bib19.8.3" class="ltx_text" style="font-size:90%;">, pages 4015â€“4026, 2023.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.2.2.1" class="ltx_text" style="font-size:90%;">[20]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.4.1" class="ltx_text" style="font-size:90%;">
BoÂ Li, Haoke Xiao, and LvÂ Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.5.1" class="ltx_text" style="font-size:90%;">Asam: Boosting segment anything model with adversarial tuning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2405.00256</span><span id="bib.bib20.7.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.2.2.1" class="ltx_text" style="font-size:90%;">[21]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.4.1" class="ltx_text" style="font-size:90%;">
Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.5.1" class="ltx_text" style="font-size:90%;">Semantic-sam: Segment and recognize anything at any granularity.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2307.04767</span><span id="bib.bib21.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.2.2.1" class="ltx_text" style="font-size:90%;">[22]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.4.1" class="ltx_text" style="font-size:90%;">
Yingwei Li, AdamsÂ Wei Yu, Tianjian Meng, Ben Caine, Jiquan Ngiam, Daiyi Peng, Junyang Shen, Yifeng Lu, Denny Zhou, QuocÂ V Le, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.5.1" class="ltx_text" style="font-size:90%;">Deepfusion: Lidar-camera deep fusion for multi-modal 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib22.8.3" class="ltx_text" style="font-size:90%;">, pages 17182â€“17191, 2022.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.2.2.1" class="ltx_text" style="font-size:90%;">[23]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.4.1" class="ltx_text" style="font-size:90%;">
PaulÂ Pu Liang, Amir Zadeh, and Louis-Philippe Morency.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.5.1" class="ltx_text" style="font-size:90%;">Foundations &amp; trends in multimodal machine learning: Principles, challenges, and open questions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM Computing Surveys</span><span id="bib.bib23.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.2.2.1" class="ltx_text" style="font-size:90%;">[24]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.4.1" class="ltx_text" style="font-size:90%;">
Jiehong Lin, Lihua Liu, Dekun Lu, and Kui Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.5.1" class="ltx_text" style="font-size:90%;">Sam-6d: Segment anything model meets zero-shot 6d object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2311.15707</span><span id="bib.bib24.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.2.2.1" class="ltx_text" style="font-size:90%;">[25]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.4.1" class="ltx_text" style="font-size:90%;">
Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and BoÂ Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.5.1" class="ltx_text" style="font-size:90%;">Segment anything in medical images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nature Communications</span><span id="bib.bib25.7.2" class="ltx_text" style="font-size:90%;">, 15(1):654, 2024.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.2.2.1" class="ltx_text" style="font-size:90%;">[26]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.4.1" class="ltx_text" style="font-size:90%;">
Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.5.1" class="ltx_text" style="font-size:90%;">V-net: Fully convolutional neural networks for volumetric medical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2016 fourth international conference on 3D vision (3DV)</span><span id="bib.bib26.8.3" class="ltx_text" style="font-size:90%;">, pages 565â€“571. Ieee, 2016.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.2.2.1" class="ltx_text" style="font-size:90%;">[27]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.4.1" class="ltx_text" style="font-size:90%;">
PushmeetÂ Kohli NathanÂ Silberman, DerekÂ Hoiem and Rob Fergus.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.5.1" class="ltx_text" style="font-size:90%;">Indoor segmentation and support inference from rgbd images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span id="bib.bib27.8.3" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.2.2.1" class="ltx_text" style="font-size:90%;">[28]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.4.1" class="ltx_text" style="font-size:90%;">
Maxime Oquab, TimothÃ©e Darcet, ThÃ©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.5.1" class="ltx_text" style="font-size:90%;">Dinov2: Learning robust visual features without supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2304.07193</span><span id="bib.bib28.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.2.2.1" class="ltx_text" style="font-size:90%;">[29]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.4.1" class="ltx_text" style="font-size:90%;">
Zelin Peng, Zhengqin Xu, Zhilin Zeng, Lingxi Xie, QiÂ Tian, and Wei Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.5.1" class="ltx_text" style="font-size:90%;">Parameter efficient fine-tuning via cross block orchestration for segment anything model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2311.17112</span><span id="bib.bib29.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.2.2.1" class="ltx_text" style="font-size:90%;">[30]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.4.1" class="ltx_text" style="font-size:90%;">
Saurabh Prasad, Bertrand LeÂ Saux, Naoto Yokoya, and Ronny Hansch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.5.1" class="ltx_text" style="font-size:90%;">2018 ieee grss data fusion challenge â€“ fusion of multispectral lidar and hyperspectral data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Dataport</span><span id="bib.bib30.7.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.2.2.1" class="ltx_text" style="font-size:90%;">[31]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.4.1" class="ltx_text" style="font-size:90%;">
CharlesÂ R Qi, Wei Liu, Chenxia Wu, Hao Su, and LeonidasÂ J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.5.1" class="ltx_text" style="font-size:90%;">Frustum pointnets for 3d object detection from rgb-d data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span id="bib.bib31.8.3" class="ltx_text" style="font-size:90%;">, pages 918â€“927, 2018.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.2.2.1" class="ltx_text" style="font-size:90%;">[32]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.4.1" class="ltx_text" style="font-size:90%;">
Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.5.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib32.8.3" class="ltx_text" style="font-size:90%;">, pages 8748â€“8763. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.2.2.1" class="ltx_text" style="font-size:90%;">[33]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.4.1" class="ltx_text" style="font-size:90%;">
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.5.1" class="ltx_text" style="font-size:90%;">Learning multiple visual domains with residual adapters.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib33.7.2" class="ltx_text" style="font-size:90%;">, 30, 2017.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.2.2.1" class="ltx_text" style="font-size:90%;">[34]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.4.1" class="ltx_text" style="font-size:90%;">
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.5.1" class="ltx_text" style="font-size:90%;">Efficient parametrization of multi-domain deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib34.8.3" class="ltx_text" style="font-size:90%;">, pages 8119â€“8127, 2018.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.2.2.1" class="ltx_text" style="font-size:90%;">[35]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.4.1" class="ltx_text" style="font-size:90%;">
Shuran Song, SamuelÂ P Lichtenberg, and Jianxiong Xiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.5.1" class="ltx_text" style="font-size:90%;">Sun rgb-d: A rgb-d scene understanding benchmark suite.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span id="bib.bib35.8.3" class="ltx_text" style="font-size:90%;">, pages 567â€“576, 2015.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.2.2.1" class="ltx_text" style="font-size:90%;">[36]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.4.1" class="ltx_text" style="font-size:90%;">
Yiran Song, Qianyu Zhou, Xiangtai Li, Deng-Ping Fan, Xuequan Lu, and Lizhuang Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.5.1" class="ltx_text" style="font-size:90%;">Ba-sam: Scalable bias-mode attention mask for segment anything model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2401.02317</span><span id="bib.bib36.7.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.2.2.1" class="ltx_text" style="font-size:90%;">[37]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.4.1" class="ltx_text" style="font-size:90%;">
Claudio Persello; Ronny HÃ¤nsch; Gemine Vivone; Kaiqiang Chen; Zhiyuan Yan; Deke Tang; Hai Huang; Michael Schmitt;Â Xian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.5.1" class="ltx_text" style="font-size:90%;">2023 ieee grss data fusion contest: Large-scale fine-grained building classification for semantic urban reconstruction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Dataport</span><span id="bib.bib37.7.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.2.2.1" class="ltx_text" style="font-size:90%;">[38]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.4.1" class="ltx_text" style="font-size:90%;">
Johan Vertens, Jannik ZÃ¼rn, and Wolfram Burgard.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.5.1" class="ltx_text" style="font-size:90%;">Heatnet: Bridging the day-night domain gap in semantic segmentation with thermal images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</span><span id="bib.bib38.8.3" class="ltx_text" style="font-size:90%;">, pages 8461â€“8468, 2020.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.2.2.1" class="ltx_text" style="font-size:90%;">[39]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.4.1" class="ltx_text" style="font-size:90%;">
Haoxiang Wang, Pavan KumarÂ Anasosalu Vasu, Fartash Faghri, Raviteja Vemulapalli, Mehrdad Farajtabar, Sachin Mehta, Mohammad Rastegari, Oncel Tuzel, and Hadi Pouransari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.5.1" class="ltx_text" style="font-size:90%;">Sam-clip: Merging vision foundation models towards semantic and spatial understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2310.15308</span><span id="bib.bib39.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.2.2.1" class="ltx_text" style="font-size:90%;">[40]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.4.1" class="ltx_text" style="font-size:90%;">
Bichen Wu, Alvin Wan, Xiangyu Yue, and Kurt Keutzer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.5.1" class="ltx_text" style="font-size:90%;">Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 IEEE international conference on robotics and automation (ICRA)</span><span id="bib.bib40.8.3" class="ltx_text" style="font-size:90%;">, pages 1887â€“1893. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.2.2.1" class="ltx_text" style="font-size:90%;">[41]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.4.1" class="ltx_text" style="font-size:90%;">
Aoran Xiao, Weihao Xuan, Heli Qi, Yun Xing, Ruijie Ren, Xiaoqin Zhang, and Shijian Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.5.1" class="ltx_text" style="font-size:90%;">Cat-sam: Conditional tuning for few-shot adaptation of segmentation anything model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span id="bib.bib41.8.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.2.2.1" class="ltx_text" style="font-size:90%;">[42]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.4.1" class="ltx_text" style="font-size:90%;">
Aoran Xiao, Xiaofei Yang, Shijian Lu, Dayan Guan, and Jiaxing Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.5.1" class="ltx_text" style="font-size:90%;">Fps-net: A convolutional fusion network for large-scale lidar point cloud segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ISPRS Journal of Photogrammetry and Remote Sensing</span><span id="bib.bib42.7.2" class="ltx_text" style="font-size:90%;">, 176:237â€“249, 2021.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.2.2.1" class="ltx_text" style="font-size:90%;">[43]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.4.1" class="ltx_text" style="font-size:90%;">
Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang, Fei Sun, Forrest Iandola, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.5.1" class="ltx_text" style="font-size:90%;">Efficientsam: Leveraged masked image pretraining for efficient segment anything.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2312.00863</span><span id="bib.bib43.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.2.2.1" class="ltx_text" style="font-size:90%;">[44]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.4.1" class="ltx_text" style="font-size:90%;">
Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, and Xiang Bai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.5.1" class="ltx_text" style="font-size:90%;">Side adapter network for open-vocabulary semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib44.8.3" class="ltx_text" style="font-size:90%;">, pages 2945â€“2954, 2023.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.2.2.1" class="ltx_text" style="font-size:90%;">[45]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.4.1" class="ltx_text" style="font-size:90%;">
Zihui Xue, Sucheng Ren, Zhengqi Gao, and Hang Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.5.1" class="ltx_text" style="font-size:90%;">Multimodal knowledge expansion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span id="bib.bib45.8.3" class="ltx_text" style="font-size:90%;">, pages 854â€“863, 2021.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.2.2.1" class="ltx_text" style="font-size:90%;">[46]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.4.1" class="ltx_text" style="font-size:90%;">
Chaoning Zhang, Dongshen Han, YuÂ Qiao, JungÂ Uk Kim, Sung-Ho Bae, Seungkyu Lee, and ChoongÂ Seon Hong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.5.1" class="ltx_text" style="font-size:90%;">Faster segment anything: Towards lightweight sam for mobile applications.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.14289</span><span id="bib.bib46.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.2.2.1" class="ltx_text" style="font-size:90%;">[47]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.4.1" class="ltx_text" style="font-size:90%;">
Haojie Zhang, Yongyi Su, Xun Xu, and Kui Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.5.1" class="ltx_text" style="font-size:90%;">Improving the generalization of segmentation foundation model under distribution shift via weakly supervised adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2312.03502</span><span id="bib.bib47.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.2.2.1" class="ltx_text" style="font-size:90%;">[48]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.4.1" class="ltx_text" style="font-size:90%;">
Haojie Zhang, Yongyi Su, Xun Xu, and Kui Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.5.1" class="ltx_text" style="font-size:90%;">Improving the generalization of segmentation foundation model under distribution shift via weakly supervised adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2312.03502</span><span id="bib.bib48.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.2.2.1" class="ltx_text" style="font-size:90%;">[49]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.4.1" class="ltx_text" style="font-size:90%;">
Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Hao Dong, Peng Gao, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.5.1" class="ltx_text" style="font-size:90%;">Personalize segment anything model with one shot.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.03048</span><span id="bib.bib49.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.2.2.1" class="ltx_text" style="font-size:90%;">[50]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.4.1" class="ltx_text" style="font-size:90%;">
Weiming Zhang, Yexin Liu, XuÂ Zheng, and Lin Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.5.1" class="ltx_text" style="font-size:90%;">Goodsam: Bridging domain and capacity gaps via segment anything model for distortion-aware panoramic semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2403.16370</span><span id="bib.bib50.7.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.2.2.1" class="ltx_text" style="font-size:90%;">[51]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.4.1" class="ltx_text" style="font-size:90%;">
Xin Zhang, YuÂ Liu, Yuming Lin, Qingmin Liao, and Yong Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.5.1" class="ltx_text" style="font-size:90%;">Uv-sam: Adapting segment anything model for urban village identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial Intelligence</span><span id="bib.bib51.8.3" class="ltx_text" style="font-size:90%;">, volumeÂ 38, pages 22520â€“22528, 2024.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.2.2.1" class="ltx_text" style="font-size:90%;">[52]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.4.1" class="ltx_text" style="font-size:90%;">
XuÂ Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.5.1" class="ltx_text" style="font-size:90%;">Fast segment anything.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.12156</span><span id="bib.bib52.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.2.2.1" class="ltx_text" style="font-size:90%;">[53]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.4.1" class="ltx_text" style="font-size:90%;">
Kaiyang Zhou, Jingkang Yang, ChenÂ Change Loy, and Ziwei Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.5.1" class="ltx_text" style="font-size:90%;">Learning to prompt for vision-language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib53.7.2" class="ltx_text" style="font-size:90%;">, 130(9):2337â€“2348, 2022.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.2.2.1" class="ltx_text" style="font-size:90%;">[54]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.4.1" class="ltx_text" style="font-size:90%;">
Zhuangwei Zhuang, Rong Li, Kui Jia, Qicheng Wang, Yuanqing Li, and Mingkui Tan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.5.1" class="ltx_text" style="font-size:90%;">Perception-aware multi-sensor fusion for 3d lidar semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span id="bib.bib54.8.3" class="ltx_text" style="font-size:90%;">, pages 16280â€“16290, 2021.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold" style="font-size:120%;">Appendix</span><span id="p1.1.2" class="ltx_text" style="font-size:90%;"></span></p>
</div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:90%;">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>MM-SAM detailed structure</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Image encoder for non-RGB modalities</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.4" class="ltx_p"><span id="A1.SS1.p1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Modality-specific projection.</span><span id="A1.SS1.p1.4.2" class="ltx_text" style="font-size:90%;"> To process non-RGB modality data, we introduce a new patch embedding module at the beginning of SAMâ€™s image encoder backbone (i.e., ViTÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.SS1.p1.4.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a><span id="A1.SS1.p1.4.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A1.SS1.p1.4.5" class="ltx_text" style="font-size:90%;">). This module generates modality-specific patches. The input to the new patch embedding is </span><math id="A1.SS1.p1.1.m1.4" class="ltx_Math" alttext="(B,D,1024,1024)" display="inline"><semantics id="A1.SS1.p1.1.m1.4a"><mrow id="A1.SS1.p1.1.m1.4.5.2" xref="A1.SS1.p1.1.m1.4.5.1.cmml"><mo maxsize="90%" minsize="90%" id="A1.SS1.p1.1.m1.4.5.2.1" xref="A1.SS1.p1.1.m1.4.5.1.cmml">(</mo><mi mathsize="90%" id="A1.SS1.p1.1.m1.1.1" xref="A1.SS1.p1.1.m1.1.1.cmml">B</mi><mo mathsize="90%" id="A1.SS1.p1.1.m1.4.5.2.2" xref="A1.SS1.p1.1.m1.4.5.1.cmml">,</mo><mi mathsize="90%" id="A1.SS1.p1.1.m1.2.2" xref="A1.SS1.p1.1.m1.2.2.cmml">D</mi><mo mathsize="90%" id="A1.SS1.p1.1.m1.4.5.2.3" xref="A1.SS1.p1.1.m1.4.5.1.cmml">,</mo><mn mathsize="90%" id="A1.SS1.p1.1.m1.3.3" xref="A1.SS1.p1.1.m1.3.3.cmml">1024</mn><mo mathsize="90%" id="A1.SS1.p1.1.m1.4.5.2.4" xref="A1.SS1.p1.1.m1.4.5.1.cmml">,</mo><mn mathsize="90%" id="A1.SS1.p1.1.m1.4.4" xref="A1.SS1.p1.1.m1.4.4.cmml">1024</mn><mo maxsize="90%" minsize="90%" id="A1.SS1.p1.1.m1.4.5.2.5" xref="A1.SS1.p1.1.m1.4.5.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.1.m1.4b"><vector id="A1.SS1.p1.1.m1.4.5.1.cmml" xref="A1.SS1.p1.1.m1.4.5.2"><ci id="A1.SS1.p1.1.m1.1.1.cmml" xref="A1.SS1.p1.1.m1.1.1">ğµ</ci><ci id="A1.SS1.p1.1.m1.2.2.cmml" xref="A1.SS1.p1.1.m1.2.2">ğ·</ci><cn type="integer" id="A1.SS1.p1.1.m1.3.3.cmml" xref="A1.SS1.p1.1.m1.3.3">1024</cn><cn type="integer" id="A1.SS1.p1.1.m1.4.4.cmml" xref="A1.SS1.p1.1.m1.4.4">1024</cn></vector></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.1.m1.4c">(B,D,1024,1024)</annotation></semantics></math><span id="A1.SS1.p1.4.6" class="ltx_text" style="font-size:90%;"> compared to the original </span><math id="A1.SS1.p1.2.m2.4" class="ltx_Math" alttext="(B,3,1024,1024)" display="inline"><semantics id="A1.SS1.p1.2.m2.4a"><mrow id="A1.SS1.p1.2.m2.4.5.2" xref="A1.SS1.p1.2.m2.4.5.1.cmml"><mo maxsize="90%" minsize="90%" id="A1.SS1.p1.2.m2.4.5.2.1" xref="A1.SS1.p1.2.m2.4.5.1.cmml">(</mo><mi mathsize="90%" id="A1.SS1.p1.2.m2.1.1" xref="A1.SS1.p1.2.m2.1.1.cmml">B</mi><mo mathsize="90%" id="A1.SS1.p1.2.m2.4.5.2.2" xref="A1.SS1.p1.2.m2.4.5.1.cmml">,</mo><mn mathsize="90%" id="A1.SS1.p1.2.m2.2.2" xref="A1.SS1.p1.2.m2.2.2.cmml">3</mn><mo mathsize="90%" id="A1.SS1.p1.2.m2.4.5.2.3" xref="A1.SS1.p1.2.m2.4.5.1.cmml">,</mo><mn mathsize="90%" id="A1.SS1.p1.2.m2.3.3" xref="A1.SS1.p1.2.m2.3.3.cmml">1024</mn><mo mathsize="90%" id="A1.SS1.p1.2.m2.4.5.2.4" xref="A1.SS1.p1.2.m2.4.5.1.cmml">,</mo><mn mathsize="90%" id="A1.SS1.p1.2.m2.4.4" xref="A1.SS1.p1.2.m2.4.4.cmml">1024</mn><mo maxsize="90%" minsize="90%" id="A1.SS1.p1.2.m2.4.5.2.5" xref="A1.SS1.p1.2.m2.4.5.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.2.m2.4b"><vector id="A1.SS1.p1.2.m2.4.5.1.cmml" xref="A1.SS1.p1.2.m2.4.5.2"><ci id="A1.SS1.p1.2.m2.1.1.cmml" xref="A1.SS1.p1.2.m2.1.1">ğµ</ci><cn type="integer" id="A1.SS1.p1.2.m2.2.2.cmml" xref="A1.SS1.p1.2.m2.2.2">3</cn><cn type="integer" id="A1.SS1.p1.2.m2.3.3.cmml" xref="A1.SS1.p1.2.m2.3.3">1024</cn><cn type="integer" id="A1.SS1.p1.2.m2.4.4.cmml" xref="A1.SS1.p1.2.m2.4.4">1024</cn></vector></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.2.m2.4c">(B,3,1024,1024)</annotation></semantics></math><span id="A1.SS1.p1.4.7" class="ltx_text" style="font-size:90%;">, both producing the same output sizes. Here, </span><math id="A1.SS1.p1.3.m3.1" class="ltx_Math" alttext="B" display="inline"><semantics id="A1.SS1.p1.3.m3.1a"><mi mathsize="90%" id="A1.SS1.p1.3.m3.1.1" xref="A1.SS1.p1.3.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.3.m3.1b"><ci id="A1.SS1.p1.3.m3.1.1.cmml" xref="A1.SS1.p1.3.m3.1.1">ğµ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.3.m3.1c">B</annotation></semantics></math><span id="A1.SS1.p1.4.8" class="ltx_text" style="font-size:90%;"> is the batch size, and </span><math id="A1.SS1.p1.4.m4.1" class="ltx_Math" alttext="D" display="inline"><semantics id="A1.SS1.p1.4.m4.1a"><mi mathsize="90%" id="A1.SS1.p1.4.m4.1.1" xref="A1.SS1.p1.4.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.4.m4.1b"><ci id="A1.SS1.p1.4.m4.1.1.cmml" xref="A1.SS1.p1.4.m4.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.4.m4.1c">D</annotation></semantics></math><span id="A1.SS1.p1.4.9" class="ltx_text" style="font-size:90%;"> represents the dimension of the specific modality, such as 1 for depth images, as detailed in Appendix </span><a href="#A2.SS1" title="B.1 Data Representations â€£ Appendix B Datasets and Metrics â€£ Segment Anything with Multiple Modalities" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">B.1</span></a><span id="A1.SS1.p1.4.10" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="A1.SS1.p2" class="ltx_para">
<p id="A1.SS1.p2.1" class="ltx_p"><span id="A1.SS1.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">LoRAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite> for parameter-efficient tuning.</span><span id="A1.SS1.p2.1.2" class="ltx_text" style="font-size:90%;"> To learn modality-specific features, we integrate LoRA structures into each transformer block of SAMâ€™s pre-trained encoder.
Each LoRA block uses a rank of 4, balancing the learning of modality-specific features with the number of tuning parameters. More descriptions can be found inÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.SS1.p2.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a><span id="A1.SS1.p2.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A1.SS1.p2.1.5" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Selective Fusion Gate</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.14" class="ltx_p"><span id="A1.SS2.p1.14.1" class="ltx_text" style="font-size:90%;">As shown in Figure </span><a href="#S3.F3" title="Figure 3 â€£ 3.1 Preliminaries â€£ 3 Methodology â€£ Segment Anything with Multiple Modalities" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="A1.SS2.p1.14.2" class="ltx_text" style="font-size:90%;">, when fusing </span><math id="A1.SS2.p1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="A1.SS2.p1.1.m1.1a"><mi mathsize="90%" id="A1.SS2.p1.1.m1.1.1" xref="A1.SS2.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.1.m1.1b"><ci id="A1.SS2.p1.1.m1.1.1.cmml" xref="A1.SS2.p1.1.m1.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.1.m1.1c">K</annotation></semantics></math><span id="A1.SS2.p1.14.3" class="ltx_text" style="font-size:90%;"> modalities, we first generate embeddings from the image encoder, denoted as </span><math id="A1.SS2.p1.2.m2.1" class="ltx_Math" alttext="e_{X_{k}}" display="inline"><semantics id="A1.SS2.p1.2.m2.1a"><msub id="A1.SS2.p1.2.m2.1.1" xref="A1.SS2.p1.2.m2.1.1.cmml"><mi mathsize="90%" id="A1.SS2.p1.2.m2.1.1.2" xref="A1.SS2.p1.2.m2.1.1.2.cmml">e</mi><msub id="A1.SS2.p1.2.m2.1.1.3" xref="A1.SS2.p1.2.m2.1.1.3.cmml"><mi mathsize="90%" id="A1.SS2.p1.2.m2.1.1.3.2" xref="A1.SS2.p1.2.m2.1.1.3.2.cmml">X</mi><mi mathsize="90%" id="A1.SS2.p1.2.m2.1.1.3.3" xref="A1.SS2.p1.2.m2.1.1.3.3.cmml">k</mi></msub></msub><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.2.m2.1b"><apply id="A1.SS2.p1.2.m2.1.1.cmml" xref="A1.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A1.SS2.p1.2.m2.1.1.1.cmml" xref="A1.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="A1.SS2.p1.2.m2.1.1.2.cmml" xref="A1.SS2.p1.2.m2.1.1.2">ğ‘’</ci><apply id="A1.SS2.p1.2.m2.1.1.3.cmml" xref="A1.SS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="A1.SS2.p1.2.m2.1.1.3.1.cmml" xref="A1.SS2.p1.2.m2.1.1.3">subscript</csymbol><ci id="A1.SS2.p1.2.m2.1.1.3.2.cmml" xref="A1.SS2.p1.2.m2.1.1.3.2">ğ‘‹</ci><ci id="A1.SS2.p1.2.m2.1.1.3.3.cmml" xref="A1.SS2.p1.2.m2.1.1.3.3">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.2.m2.1c">e_{X_{k}}</annotation></semantics></math><span id="A1.SS2.p1.14.4" class="ltx_text" style="font-size:90%;"> for </span><math id="A1.SS2.p1.3.m3.3" class="ltx_Math" alttext="k={1,â€¦,K}" display="inline"><semantics id="A1.SS2.p1.3.m3.3a"><mrow id="A1.SS2.p1.3.m3.3.4" xref="A1.SS2.p1.3.m3.3.4.cmml"><mi mathsize="90%" id="A1.SS2.p1.3.m3.3.4.2" xref="A1.SS2.p1.3.m3.3.4.2.cmml">k</mi><mo mathsize="90%" id="A1.SS2.p1.3.m3.3.4.1" xref="A1.SS2.p1.3.m3.3.4.1.cmml">=</mo><mrow id="A1.SS2.p1.3.m3.3.4.3.2" xref="A1.SS2.p1.3.m3.3.4.3.1.cmml"><mn mathsize="90%" id="A1.SS2.p1.3.m3.1.1" xref="A1.SS2.p1.3.m3.1.1.cmml">1</mn><mo mathsize="90%" id="A1.SS2.p1.3.m3.3.4.3.2.1" xref="A1.SS2.p1.3.m3.3.4.3.1.cmml">,</mo><mi mathsize="90%" mathvariant="normal" id="A1.SS2.p1.3.m3.2.2" xref="A1.SS2.p1.3.m3.2.2.cmml">â€¦</mi><mo mathsize="90%" id="A1.SS2.p1.3.m3.3.4.3.2.2" xref="A1.SS2.p1.3.m3.3.4.3.1.cmml">,</mo><mi mathsize="90%" id="A1.SS2.p1.3.m3.3.3" xref="A1.SS2.p1.3.m3.3.3.cmml">K</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.3.m3.3b"><apply id="A1.SS2.p1.3.m3.3.4.cmml" xref="A1.SS2.p1.3.m3.3.4"><eq id="A1.SS2.p1.3.m3.3.4.1.cmml" xref="A1.SS2.p1.3.m3.3.4.1"></eq><ci id="A1.SS2.p1.3.m3.3.4.2.cmml" xref="A1.SS2.p1.3.m3.3.4.2">ğ‘˜</ci><list id="A1.SS2.p1.3.m3.3.4.3.1.cmml" xref="A1.SS2.p1.3.m3.3.4.3.2"><cn type="integer" id="A1.SS2.p1.3.m3.1.1.cmml" xref="A1.SS2.p1.3.m3.1.1">1</cn><ci id="A1.SS2.p1.3.m3.2.2.cmml" xref="A1.SS2.p1.3.m3.2.2">â€¦</ci><ci id="A1.SS2.p1.3.m3.3.3.cmml" xref="A1.SS2.p1.3.m3.3.3">ğ¾</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.3.m3.3c">k={1,â€¦,K}</annotation></semantics></math><span id="A1.SS2.p1.14.5" class="ltx_text" style="font-size:90%;">. The RGB embedding can be one of them, denoted as </span><math id="A1.SS2.p1.4.m4.1" class="ltx_Math" alttext="e_{X_{j}}" display="inline"><semantics id="A1.SS2.p1.4.m4.1a"><msub id="A1.SS2.p1.4.m4.1.1" xref="A1.SS2.p1.4.m4.1.1.cmml"><mi mathsize="90%" id="A1.SS2.p1.4.m4.1.1.2" xref="A1.SS2.p1.4.m4.1.1.2.cmml">e</mi><msub id="A1.SS2.p1.4.m4.1.1.3" xref="A1.SS2.p1.4.m4.1.1.3.cmml"><mi mathsize="90%" id="A1.SS2.p1.4.m4.1.1.3.2" xref="A1.SS2.p1.4.m4.1.1.3.2.cmml">X</mi><mi mathsize="90%" id="A1.SS2.p1.4.m4.1.1.3.3" xref="A1.SS2.p1.4.m4.1.1.3.3.cmml">j</mi></msub></msub><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.4.m4.1b"><apply id="A1.SS2.p1.4.m4.1.1.cmml" xref="A1.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A1.SS2.p1.4.m4.1.1.1.cmml" xref="A1.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="A1.SS2.p1.4.m4.1.1.2.cmml" xref="A1.SS2.p1.4.m4.1.1.2">ğ‘’</ci><apply id="A1.SS2.p1.4.m4.1.1.3.cmml" xref="A1.SS2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="A1.SS2.p1.4.m4.1.1.3.1.cmml" xref="A1.SS2.p1.4.m4.1.1.3">subscript</csymbol><ci id="A1.SS2.p1.4.m4.1.1.3.2.cmml" xref="A1.SS2.p1.4.m4.1.1.3.2">ğ‘‹</ci><ci id="A1.SS2.p1.4.m4.1.1.3.3.cmml" xref="A1.SS2.p1.4.m4.1.1.3.3">ğ‘—</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.4.m4.1c">e_{X_{j}}</annotation></semantics></math><span id="A1.SS2.p1.14.6" class="ltx_text" style="font-size:90%;"> (i.e., </span><math id="A1.SS2.p1.5.m5.1" class="ltx_Math" alttext="e_{X_{j}}=e_{I}" display="inline"><semantics id="A1.SS2.p1.5.m5.1a"><mrow id="A1.SS2.p1.5.m5.1.1" xref="A1.SS2.p1.5.m5.1.1.cmml"><msub id="A1.SS2.p1.5.m5.1.1.2" xref="A1.SS2.p1.5.m5.1.1.2.cmml"><mi mathsize="90%" id="A1.SS2.p1.5.m5.1.1.2.2" xref="A1.SS2.p1.5.m5.1.1.2.2.cmml">e</mi><msub id="A1.SS2.p1.5.m5.1.1.2.3" xref="A1.SS2.p1.5.m5.1.1.2.3.cmml"><mi mathsize="90%" id="A1.SS2.p1.5.m5.1.1.2.3.2" xref="A1.SS2.p1.5.m5.1.1.2.3.2.cmml">X</mi><mi mathsize="90%" id="A1.SS2.p1.5.m5.1.1.2.3.3" xref="A1.SS2.p1.5.m5.1.1.2.3.3.cmml">j</mi></msub></msub><mo mathsize="90%" id="A1.SS2.p1.5.m5.1.1.1" xref="A1.SS2.p1.5.m5.1.1.1.cmml">=</mo><msub id="A1.SS2.p1.5.m5.1.1.3" xref="A1.SS2.p1.5.m5.1.1.3.cmml"><mi mathsize="90%" id="A1.SS2.p1.5.m5.1.1.3.2" xref="A1.SS2.p1.5.m5.1.1.3.2.cmml">e</mi><mi mathsize="90%" id="A1.SS2.p1.5.m5.1.1.3.3" xref="A1.SS2.p1.5.m5.1.1.3.3.cmml">I</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.5.m5.1b"><apply id="A1.SS2.p1.5.m5.1.1.cmml" xref="A1.SS2.p1.5.m5.1.1"><eq id="A1.SS2.p1.5.m5.1.1.1.cmml" xref="A1.SS2.p1.5.m5.1.1.1"></eq><apply id="A1.SS2.p1.5.m5.1.1.2.cmml" xref="A1.SS2.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="A1.SS2.p1.5.m5.1.1.2.1.cmml" xref="A1.SS2.p1.5.m5.1.1.2">subscript</csymbol><ci id="A1.SS2.p1.5.m5.1.1.2.2.cmml" xref="A1.SS2.p1.5.m5.1.1.2.2">ğ‘’</ci><apply id="A1.SS2.p1.5.m5.1.1.2.3.cmml" xref="A1.SS2.p1.5.m5.1.1.2.3"><csymbol cd="ambiguous" id="A1.SS2.p1.5.m5.1.1.2.3.1.cmml" xref="A1.SS2.p1.5.m5.1.1.2.3">subscript</csymbol><ci id="A1.SS2.p1.5.m5.1.1.2.3.2.cmml" xref="A1.SS2.p1.5.m5.1.1.2.3.2">ğ‘‹</ci><ci id="A1.SS2.p1.5.m5.1.1.2.3.3.cmml" xref="A1.SS2.p1.5.m5.1.1.2.3.3">ğ‘—</ci></apply></apply><apply id="A1.SS2.p1.5.m5.1.1.3.cmml" xref="A1.SS2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="A1.SS2.p1.5.m5.1.1.3.1.cmml" xref="A1.SS2.p1.5.m5.1.1.3">subscript</csymbol><ci id="A1.SS2.p1.5.m5.1.1.3.2.cmml" xref="A1.SS2.p1.5.m5.1.1.3.2">ğ‘’</ci><ci id="A1.SS2.p1.5.m5.1.1.3.3.cmml" xref="A1.SS2.p1.5.m5.1.1.3.3">ğ¼</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.5.m5.1c">e_{X_{j}}=e_{I}</annotation></semantics></math><span id="A1.SS2.p1.14.7" class="ltx_text" style="font-size:90%;">). All embeddings have the same size </span><math id="A1.SS2.p1.6.m6.4" class="ltx_Math" alttext="(B,256,64,64)" display="inline"><semantics id="A1.SS2.p1.6.m6.4a"><mrow id="A1.SS2.p1.6.m6.4.5.2" xref="A1.SS2.p1.6.m6.4.5.1.cmml"><mo maxsize="90%" minsize="90%" id="A1.SS2.p1.6.m6.4.5.2.1" xref="A1.SS2.p1.6.m6.4.5.1.cmml">(</mo><mi mathsize="90%" id="A1.SS2.p1.6.m6.1.1" xref="A1.SS2.p1.6.m6.1.1.cmml">B</mi><mo mathsize="90%" id="A1.SS2.p1.6.m6.4.5.2.2" xref="A1.SS2.p1.6.m6.4.5.1.cmml">,</mo><mn mathsize="90%" id="A1.SS2.p1.6.m6.2.2" xref="A1.SS2.p1.6.m6.2.2.cmml">256</mn><mo mathsize="90%" id="A1.SS2.p1.6.m6.4.5.2.3" xref="A1.SS2.p1.6.m6.4.5.1.cmml">,</mo><mn mathsize="90%" id="A1.SS2.p1.6.m6.3.3" xref="A1.SS2.p1.6.m6.3.3.cmml">64</mn><mo mathsize="90%" id="A1.SS2.p1.6.m6.4.5.2.4" xref="A1.SS2.p1.6.m6.4.5.1.cmml">,</mo><mn mathsize="90%" id="A1.SS2.p1.6.m6.4.4" xref="A1.SS2.p1.6.m6.4.4.cmml">64</mn><mo maxsize="90%" minsize="90%" id="A1.SS2.p1.6.m6.4.5.2.5" xref="A1.SS2.p1.6.m6.4.5.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.6.m6.4b"><vector id="A1.SS2.p1.6.m6.4.5.1.cmml" xref="A1.SS2.p1.6.m6.4.5.2"><ci id="A1.SS2.p1.6.m6.1.1.cmml" xref="A1.SS2.p1.6.m6.1.1">ğµ</ci><cn type="integer" id="A1.SS2.p1.6.m6.2.2.cmml" xref="A1.SS2.p1.6.m6.2.2">256</cn><cn type="integer" id="A1.SS2.p1.6.m6.3.3.cmml" xref="A1.SS2.p1.6.m6.3.3">64</cn><cn type="integer" id="A1.SS2.p1.6.m6.4.4.cmml" xref="A1.SS2.p1.6.m6.4.4">64</cn></vector></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.6.m6.4c">(B,256,64,64)</annotation></semantics></math><span id="A1.SS2.p1.14.8" class="ltx_text" style="font-size:90%;">. We concatenate these embeddings into </span><math id="A1.SS2.p1.7.m7.1" class="ltx_Math" alttext="e_{F}" display="inline"><semantics id="A1.SS2.p1.7.m7.1a"><msub id="A1.SS2.p1.7.m7.1.1" xref="A1.SS2.p1.7.m7.1.1.cmml"><mi mathsize="90%" id="A1.SS2.p1.7.m7.1.1.2" xref="A1.SS2.p1.7.m7.1.1.2.cmml">e</mi><mi mathsize="90%" id="A1.SS2.p1.7.m7.1.1.3" xref="A1.SS2.p1.7.m7.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.7.m7.1b"><apply id="A1.SS2.p1.7.m7.1.1.cmml" xref="A1.SS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="A1.SS2.p1.7.m7.1.1.1.cmml" xref="A1.SS2.p1.7.m7.1.1">subscript</csymbol><ci id="A1.SS2.p1.7.m7.1.1.2.cmml" xref="A1.SS2.p1.7.m7.1.1.2">ğ‘’</ci><ci id="A1.SS2.p1.7.m7.1.1.3.cmml" xref="A1.SS2.p1.7.m7.1.1.3">ğ¹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.7.m7.1c">e_{F}</annotation></semantics></math><span id="A1.SS2.p1.14.9" class="ltx_text" style="font-size:90%;"> with a size of </span><math id="A1.SS2.p1.8.m8.4" class="ltx_Math" alttext="(B,256\times K,64,64)" display="inline"><semantics id="A1.SS2.p1.8.m8.4a"><mrow id="A1.SS2.p1.8.m8.4.4.1" xref="A1.SS2.p1.8.m8.4.4.2.cmml"><mo maxsize="90%" minsize="90%" id="A1.SS2.p1.8.m8.4.4.1.2" xref="A1.SS2.p1.8.m8.4.4.2.cmml">(</mo><mi mathsize="90%" id="A1.SS2.p1.8.m8.1.1" xref="A1.SS2.p1.8.m8.1.1.cmml">B</mi><mo mathsize="90%" id="A1.SS2.p1.8.m8.4.4.1.3" xref="A1.SS2.p1.8.m8.4.4.2.cmml">,</mo><mrow id="A1.SS2.p1.8.m8.4.4.1.1" xref="A1.SS2.p1.8.m8.4.4.1.1.cmml"><mn mathsize="90%" id="A1.SS2.p1.8.m8.4.4.1.1.2" xref="A1.SS2.p1.8.m8.4.4.1.1.2.cmml">256</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="A1.SS2.p1.8.m8.4.4.1.1.1" xref="A1.SS2.p1.8.m8.4.4.1.1.1.cmml">Ã—</mo><mi mathsize="90%" id="A1.SS2.p1.8.m8.4.4.1.1.3" xref="A1.SS2.p1.8.m8.4.4.1.1.3.cmml">K</mi></mrow><mo mathsize="90%" id="A1.SS2.p1.8.m8.4.4.1.4" xref="A1.SS2.p1.8.m8.4.4.2.cmml">,</mo><mn mathsize="90%" id="A1.SS2.p1.8.m8.2.2" xref="A1.SS2.p1.8.m8.2.2.cmml">64</mn><mo mathsize="90%" id="A1.SS2.p1.8.m8.4.4.1.5" xref="A1.SS2.p1.8.m8.4.4.2.cmml">,</mo><mn mathsize="90%" id="A1.SS2.p1.8.m8.3.3" xref="A1.SS2.p1.8.m8.3.3.cmml">64</mn><mo maxsize="90%" minsize="90%" id="A1.SS2.p1.8.m8.4.4.1.6" xref="A1.SS2.p1.8.m8.4.4.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.8.m8.4b"><vector id="A1.SS2.p1.8.m8.4.4.2.cmml" xref="A1.SS2.p1.8.m8.4.4.1"><ci id="A1.SS2.p1.8.m8.1.1.cmml" xref="A1.SS2.p1.8.m8.1.1">ğµ</ci><apply id="A1.SS2.p1.8.m8.4.4.1.1.cmml" xref="A1.SS2.p1.8.m8.4.4.1.1"><times id="A1.SS2.p1.8.m8.4.4.1.1.1.cmml" xref="A1.SS2.p1.8.m8.4.4.1.1.1"></times><cn type="integer" id="A1.SS2.p1.8.m8.4.4.1.1.2.cmml" xref="A1.SS2.p1.8.m8.4.4.1.1.2">256</cn><ci id="A1.SS2.p1.8.m8.4.4.1.1.3.cmml" xref="A1.SS2.p1.8.m8.4.4.1.1.3">ğ¾</ci></apply><cn type="integer" id="A1.SS2.p1.8.m8.2.2.cmml" xref="A1.SS2.p1.8.m8.2.2">64</cn><cn type="integer" id="A1.SS2.p1.8.m8.3.3.cmml" xref="A1.SS2.p1.8.m8.3.3">64</cn></vector></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.8.m8.4c">(B,256\times K,64,64)</annotation></semantics></math><span id="A1.SS2.p1.14.10" class="ltx_text" style="font-size:90%;">, which is then input to the weight module. This module consists of two convolutional layers with </span><math id="A1.SS2.p1.9.m9.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="A1.SS2.p1.9.m9.1a"><mrow id="A1.SS2.p1.9.m9.1.1" xref="A1.SS2.p1.9.m9.1.1.cmml"><mn mathsize="90%" id="A1.SS2.p1.9.m9.1.1.2" xref="A1.SS2.p1.9.m9.1.1.2.cmml">3</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="A1.SS2.p1.9.m9.1.1.1" xref="A1.SS2.p1.9.m9.1.1.1.cmml">Ã—</mo><mn mathsize="90%" id="A1.SS2.p1.9.m9.1.1.3" xref="A1.SS2.p1.9.m9.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.9.m9.1b"><apply id="A1.SS2.p1.9.m9.1.1.cmml" xref="A1.SS2.p1.9.m9.1.1"><times id="A1.SS2.p1.9.m9.1.1.1.cmml" xref="A1.SS2.p1.9.m9.1.1.1"></times><cn type="integer" id="A1.SS2.p1.9.m9.1.1.2.cmml" xref="A1.SS2.p1.9.m9.1.1.2">3</cn><cn type="integer" id="A1.SS2.p1.9.m9.1.1.3.cmml" xref="A1.SS2.p1.9.m9.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.9.m9.1c">3\times 3</annotation></semantics></math><span id="A1.SS2.p1.14.11" class="ltx_text" style="font-size:90%;"> kernels, a GELU activation function, and a softmax layer. The convolutional layers have output channels of </span><math id="A1.SS2.p1.10.m10.1" class="ltx_Math" alttext="16\times K" display="inline"><semantics id="A1.SS2.p1.10.m10.1a"><mrow id="A1.SS2.p1.10.m10.1.1" xref="A1.SS2.p1.10.m10.1.1.cmml"><mn mathsize="90%" id="A1.SS2.p1.10.m10.1.1.2" xref="A1.SS2.p1.10.m10.1.1.2.cmml">16</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="A1.SS2.p1.10.m10.1.1.1" xref="A1.SS2.p1.10.m10.1.1.1.cmml">Ã—</mo><mi mathsize="90%" id="A1.SS2.p1.10.m10.1.1.3" xref="A1.SS2.p1.10.m10.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.10.m10.1b"><apply id="A1.SS2.p1.10.m10.1.1.cmml" xref="A1.SS2.p1.10.m10.1.1"><times id="A1.SS2.p1.10.m10.1.1.1.cmml" xref="A1.SS2.p1.10.m10.1.1.1"></times><cn type="integer" id="A1.SS2.p1.10.m10.1.1.2.cmml" xref="A1.SS2.p1.10.m10.1.1.2">16</cn><ci id="A1.SS2.p1.10.m10.1.1.3.cmml" xref="A1.SS2.p1.10.m10.1.1.3">ğ¾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.10.m10.1c">16\times K</annotation></semantics></math><span id="A1.SS2.p1.14.12" class="ltx_text" style="font-size:90%;"> and </span><math id="A1.SS2.p1.11.m11.1" class="ltx_Math" alttext="1\times K" display="inline"><semantics id="A1.SS2.p1.11.m11.1a"><mrow id="A1.SS2.p1.11.m11.1.1" xref="A1.SS2.p1.11.m11.1.1.cmml"><mn mathsize="90%" id="A1.SS2.p1.11.m11.1.1.2" xref="A1.SS2.p1.11.m11.1.1.2.cmml">1</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="A1.SS2.p1.11.m11.1.1.1" xref="A1.SS2.p1.11.m11.1.1.1.cmml">Ã—</mo><mi mathsize="90%" id="A1.SS2.p1.11.m11.1.1.3" xref="A1.SS2.p1.11.m11.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.11.m11.1b"><apply id="A1.SS2.p1.11.m11.1.1.cmml" xref="A1.SS2.p1.11.m11.1.1"><times id="A1.SS2.p1.11.m11.1.1.1.cmml" xref="A1.SS2.p1.11.m11.1.1.1"></times><cn type="integer" id="A1.SS2.p1.11.m11.1.1.2.cmml" xref="A1.SS2.p1.11.m11.1.1.2">1</cn><ci id="A1.SS2.p1.11.m11.1.1.3.cmml" xref="A1.SS2.p1.11.m11.1.1.3">ğ¾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.11.m11.1c">1\times K</annotation></semantics></math><span id="A1.SS2.p1.14.13" class="ltx_text" style="font-size:90%;">, respectively. The final softmax output, denoted as </span><math id="A1.SS2.p1.12.m12.1" class="ltx_Math" alttext="\omega" display="inline"><semantics id="A1.SS2.p1.12.m12.1a"><mi mathsize="90%" id="A1.SS2.p1.12.m12.1.1" xref="A1.SS2.p1.12.m12.1.1.cmml">Ï‰</mi><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.12.m12.1b"><ci id="A1.SS2.p1.12.m12.1.1.cmml" xref="A1.SS2.p1.12.m12.1.1">ğœ”</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.12.m12.1c">\omega</annotation></semantics></math><span id="A1.SS2.p1.14.14" class="ltx_text" style="font-size:90%;"> with a size of </span><math id="A1.SS2.p1.13.m13.4" class="ltx_Math" alttext="(B,K,64,64)" display="inline"><semantics id="A1.SS2.p1.13.m13.4a"><mrow id="A1.SS2.p1.13.m13.4.5.2" xref="A1.SS2.p1.13.m13.4.5.1.cmml"><mo maxsize="90%" minsize="90%" id="A1.SS2.p1.13.m13.4.5.2.1" xref="A1.SS2.p1.13.m13.4.5.1.cmml">(</mo><mi mathsize="90%" id="A1.SS2.p1.13.m13.1.1" xref="A1.SS2.p1.13.m13.1.1.cmml">B</mi><mo mathsize="90%" id="A1.SS2.p1.13.m13.4.5.2.2" xref="A1.SS2.p1.13.m13.4.5.1.cmml">,</mo><mi mathsize="90%" id="A1.SS2.p1.13.m13.2.2" xref="A1.SS2.p1.13.m13.2.2.cmml">K</mi><mo mathsize="90%" id="A1.SS2.p1.13.m13.4.5.2.3" xref="A1.SS2.p1.13.m13.4.5.1.cmml">,</mo><mn mathsize="90%" id="A1.SS2.p1.13.m13.3.3" xref="A1.SS2.p1.13.m13.3.3.cmml">64</mn><mo mathsize="90%" id="A1.SS2.p1.13.m13.4.5.2.4" xref="A1.SS2.p1.13.m13.4.5.1.cmml">,</mo><mn mathsize="90%" id="A1.SS2.p1.13.m13.4.4" xref="A1.SS2.p1.13.m13.4.4.cmml">64</mn><mo maxsize="90%" minsize="90%" id="A1.SS2.p1.13.m13.4.5.2.5" xref="A1.SS2.p1.13.m13.4.5.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.13.m13.4b"><vector id="A1.SS2.p1.13.m13.4.5.1.cmml" xref="A1.SS2.p1.13.m13.4.5.2"><ci id="A1.SS2.p1.13.m13.1.1.cmml" xref="A1.SS2.p1.13.m13.1.1">ğµ</ci><ci id="A1.SS2.p1.13.m13.2.2.cmml" xref="A1.SS2.p1.13.m13.2.2">ğ¾</ci><cn type="integer" id="A1.SS2.p1.13.m13.3.3.cmml" xref="A1.SS2.p1.13.m13.3.3">64</cn><cn type="integer" id="A1.SS2.p1.13.m13.4.4.cmml" xref="A1.SS2.p1.13.m13.4.4">64</cn></vector></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.13.m13.4c">(B,K,64,64)</annotation></semantics></math><span id="A1.SS2.p1.14.15" class="ltx_text" style="font-size:90%;">, performs a Hadamard product with </span><math id="A1.SS2.p1.14.m14.1" class="ltx_Math" alttext="e_{F}" display="inline"><semantics id="A1.SS2.p1.14.m14.1a"><msub id="A1.SS2.p1.14.m14.1.1" xref="A1.SS2.p1.14.m14.1.1.cmml"><mi mathsize="90%" id="A1.SS2.p1.14.m14.1.1.2" xref="A1.SS2.p1.14.m14.1.1.2.cmml">e</mi><mi mathsize="90%" id="A1.SS2.p1.14.m14.1.1.3" xref="A1.SS2.p1.14.m14.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.14.m14.1b"><apply id="A1.SS2.p1.14.m14.1.1.cmml" xref="A1.SS2.p1.14.m14.1.1"><csymbol cd="ambiguous" id="A1.SS2.p1.14.m14.1.1.1.cmml" xref="A1.SS2.p1.14.m14.1.1">subscript</csymbol><ci id="A1.SS2.p1.14.m14.1.1.2.cmml" xref="A1.SS2.p1.14.m14.1.1.2">ğ‘’</ci><ci id="A1.SS2.p1.14.m14.1.1.3.cmml" xref="A1.SS2.p1.14.m14.1.1.3">ğ¹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.14.m14.1c">e_{F}</annotation></semantics></math><span id="A1.SS2.p1.14.16" class="ltx_text" style="font-size:90%;"> as described in Equation </span><a href="#S3.E2" title="In 3.2.2 Multi-Modal Segmentation with WMMF â€£ 3.2 MM-SAM â€£ 3 Methodology â€£ Segment Anything with Multiple Modalities" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="A1.SS2.p1.14.17" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:90%;">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Datasets and Metrics</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p"><span id="A2.p1.1.1" class="ltx_text" style="font-size:90%;">We conduct the experiments on time-synchronized and time-asynchronized sensor suites: 1) </span><span id="A2.p1.1.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Time-synchronized</span><span id="A2.p1.1.3" class="ltx_text" style="font-size:90%;">, including MFNetÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.p1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a><span id="A2.p1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.p1.1.6" class="ltx_text" style="font-size:90%;"> (RGB-Thermal), SUN RGB-DÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.p1.1.7.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a><span id="A2.p1.1.8.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.p1.1.9" class="ltx_text" style="font-size:90%;"> (RGB-Depth) and SemanticKITTIÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.p1.1.10.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a><span id="A2.p1.1.11.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.p1.1.12" class="ltx_text" style="font-size:90%;"> (RGB-LiDAR); 2) </span><span id="A2.p1.1.13" class="ltx_text ltx_font_italic" style="font-size:90%;">Time-asynchronized</span><span id="A2.p1.1.14" class="ltx_text" style="font-size:90%;">, including Data Fusion Contest 2018 Dataset (DFC2018)Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.p1.1.15.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a><span id="A2.p1.1.16.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.p1.1.17" class="ltx_text" style="font-size:90%;"> (RGB-Hyperspectral-Multispectral LiDAR), Data Fusion Contest 2023 Dataset (DFC2023)Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.p1.1.18.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a><span id="A2.p1.1.19.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.p1.1.20" class="ltx_text" style="font-size:90%;"> (RGB-SAR) and ISPRS Potsdam Dataset</span><span id="footnotex7" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">2</span></span></span></span><span id="A2.p1.1.21" class="ltx_text" style="font-size:90%;"> (RGB-DSM).</span></p>
</div>
<div id="A2.p2" class="ltx_para ltx_noindent">
<p id="A2.p2.1" class="ltx_p"><span id="A2.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">MFNet</span><span id="A2.p2.1.2" class="ltx_text" style="font-size:90%;">Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.p2.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a><span id="A2.p2.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.p2.1.5" class="ltx_text" style="font-size:90%;"> is a multi-sepctral RGB-thermal image dataset for autonomous driving research. Collected with an InfRec R500 camera, the dataset offers 1,569 densely annotated and synchronized RGB and thermal images across eight common driving obstacles captured in both day and night conditions. It provides eight classes of pixel-wise annotations for semantic segmentation.
We use the original data split as described in the original paper </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.p2.1.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a><span id="A2.p2.1.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.p2.1.8" class="ltx_text" style="font-size:90%;">.
In this work, we use all eight classes, including car, person, bike, curve, car stop, guardrail, color cone and bump for per-class Intersection-over-Union (IoU) and report their mean Intersection-over-Union (mIoU) of all classes. Since no samples containing â€™guardrailâ€™ category in daytime data, we report mIoU of daytime ("day" in Table </span><a href="#S4.T3" title="Table 3 â€£ 4.2.1 Time-Synchronized Sensor Suites â€£ 4.2 Segmentation Results â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="A2.p2.1.9" class="ltx_text" style="font-size:90%;"> (a)) using only the rest seven classes. For "total" and "night" in the table, mIoU is calculated on all eight classes.
More details of this dataset can be found at </span><a target="_blank" href="https://www.mi.t.u-tokyo.ac.jp/static/projects/mil_multispectral/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.mi.t.u-tokyo.ac.jp/static/projects/mil_multispectral/</a><span id="A2.p2.1.10" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="A2.p3" class="ltx_para ltx_noindent">
<p id="A2.p3.1" class="ltx_p"><span id="A2.p3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">FreiburgThermal</span><span id="A2.p3.1.2" class="ltx_text" style="font-size:90%;">Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.p3.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a><span id="A2.p3.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.p3.1.5" class="ltx_text" style="font-size:90%;"> is a dataset consisting of over 20,000 synchronized RGB and thermal images collected across different urban and rural environments during both day and night. It features pixel-wise semantic annotations of 12 classes. The dataset is designed to enhance research in thermal image segmentation. FreiburgThermal is valuable for multi-modal semantic segmentation tasks, especially in varying lighting conditions. We adopt the original dataset split.
In this work, we use 12 classes for evaluation, including road, sidewalk, building, curb, fence, pole, vegetation, terrain, sky, person, car and bicycle, and report their mIoU.
More details of this dataset can be found at </span><a target="_blank" href="http://thermal.cs.uni-freiburg.de" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://thermal.cs.uni-freiburg.de</a><span id="A2.p3.1.6" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="A2.p4" class="ltx_para ltx_noindent">
<p id="A2.p4.1" class="ltx_p"><span id="A2.p4.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">SUN RGB-D</span><span id="A2.p4.1.2" class="ltx_text" style="font-size:90%;">Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.p4.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a><span id="A2.p4.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.p4.1.5" class="ltx_text" style="font-size:90%;"> is an RGB-Depth dataset for visual scene understanding. It includes 10,335 RGB and depth images of indoor environments captured by different types of RGB-D cameras, with the RGB and depth images precisely aligned at the pixel level to enable accurate data fusion and analysis. Each image is annotated with detailed semantic segmentation labels of 37 categories.
We follow the official data split for experiments.
In this work, we use all 37 classes for evaluation and report their mIoU.
More details of this dataset can be found at </span><a target="_blank" href="https://rgbd.cs.princeton.edu" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://rgbd.cs.princeton.edu</a><span id="A2.p4.1.6" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="A2.p5" class="ltx_para ltx_noindent">
<p id="A2.p5.1" class="ltx_p"><span id="A2.p5.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">SemanticKITTI</span><span id="A2.p5.1.2" class="ltx_text" style="font-size:90%;">Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.p5.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a><span id="A2.p5.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.p5.1.5" class="ltx_text" style="font-size:90%;"> is a outdoor point cloud dataset designed for 3D semantic segmentation within the context of autonomous driving. Every scene in this dataset is captured using a Velodyne-HDLE64 LiDAR sensor. It includes 22 sequences, which are split into different subsets: a training set comprising 10 sequences with 19,130 frames, a validation set that includes 1 sequence with 4,071 frames, and a testing set containing 11 sequences with 20,351 frames. Point-wise annotations of 32 classes are provided.
We follow the original data split used in the SemanticKITTI dataset. In this work, we use 8 foreground classes with mask annotations for evaluation and report their mIoU.
More details of this dataset can be found at </span><a target="_blank" href="http://semantic-kitti.org" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://semantic-kitti.org</a><span id="A2.p5.1.6" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="A2.p6" class="ltx_para ltx_noindent">
<p id="A2.p6.4" class="ltx_p"><span id="A2.p6.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">DFC2018</span><span id="A2.p6.4.2" class="ltx_text" style="font-size:90%;">Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.p6.4.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a><span id="A2.p6.4.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.p6.4.5" class="ltx_text" style="font-size:90%;"> contains 14 tiles of multi-source optical imagery from Houston, Texas. It features co-registered Very High Resolution (VHR) color images, hyperspectral images, and multispectral LiDAR point clouds. Hyperspectral data covers 380-1050 nm spectral range with 48 bands while multispectral LiDAR provides point cloud data at 1550 nm, 1064 nm, and 532 nm with intensity rasters from first return per channel.
The dataset covers </span><math id="A2.p6.1.m1.1" class="ltx_Math" alttext="4172\times 1202m^{2}" display="inline"><semantics id="A2.p6.1.m1.1a"><mrow id="A2.p6.1.m1.1.1" xref="A2.p6.1.m1.1.1.cmml"><mrow id="A2.p6.1.m1.1.1.2" xref="A2.p6.1.m1.1.1.2.cmml"><mn mathsize="90%" id="A2.p6.1.m1.1.1.2.2" xref="A2.p6.1.m1.1.1.2.2.cmml">4172</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="A2.p6.1.m1.1.1.2.1" xref="A2.p6.1.m1.1.1.2.1.cmml">Ã—</mo><mn mathsize="90%" id="A2.p6.1.m1.1.1.2.3" xref="A2.p6.1.m1.1.1.2.3.cmml">1202</mn></mrow><mo lspace="0em" rspace="0em" id="A2.p6.1.m1.1.1.1" xref="A2.p6.1.m1.1.1.1.cmml">â€‹</mo><msup id="A2.p6.1.m1.1.1.3" xref="A2.p6.1.m1.1.1.3.cmml"><mi mathsize="90%" id="A2.p6.1.m1.1.1.3.2" xref="A2.p6.1.m1.1.1.3.2.cmml">m</mi><mn mathsize="90%" id="A2.p6.1.m1.1.1.3.3" xref="A2.p6.1.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.p6.1.m1.1b"><apply id="A2.p6.1.m1.1.1.cmml" xref="A2.p6.1.m1.1.1"><times id="A2.p6.1.m1.1.1.1.cmml" xref="A2.p6.1.m1.1.1.1"></times><apply id="A2.p6.1.m1.1.1.2.cmml" xref="A2.p6.1.m1.1.1.2"><times id="A2.p6.1.m1.1.1.2.1.cmml" xref="A2.p6.1.m1.1.1.2.1"></times><cn type="integer" id="A2.p6.1.m1.1.1.2.2.cmml" xref="A2.p6.1.m1.1.1.2.2">4172</cn><cn type="integer" id="A2.p6.1.m1.1.1.2.3.cmml" xref="A2.p6.1.m1.1.1.2.3">1202</cn></apply><apply id="A2.p6.1.m1.1.1.3.cmml" xref="A2.p6.1.m1.1.1.3"><csymbol cd="ambiguous" id="A2.p6.1.m1.1.1.3.1.cmml" xref="A2.p6.1.m1.1.1.3">superscript</csymbol><ci id="A2.p6.1.m1.1.1.3.2.cmml" xref="A2.p6.1.m1.1.1.3.2">ğ‘š</ci><cn type="integer" id="A2.p6.1.m1.1.1.3.3.cmml" xref="A2.p6.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p6.1.m1.1c">4172\times 1202m^{2}</annotation></semantics></math><span id="A2.p6.4.6" class="ltx_text" style="font-size:90%;"> square meters with spatial resolution </span><math id="A2.p6.2.m2.1" class="ltx_Math" alttext="5cm/pixel" display="inline"><semantics id="A2.p6.2.m2.1a"><mrow id="A2.p6.2.m2.1.1" xref="A2.p6.2.m2.1.1.cmml"><mrow id="A2.p6.2.m2.1.1.2" xref="A2.p6.2.m2.1.1.2.cmml"><mrow id="A2.p6.2.m2.1.1.2.2" xref="A2.p6.2.m2.1.1.2.2.cmml"><mn mathsize="90%" id="A2.p6.2.m2.1.1.2.2.2" xref="A2.p6.2.m2.1.1.2.2.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="A2.p6.2.m2.1.1.2.2.1" xref="A2.p6.2.m2.1.1.2.2.1.cmml">â€‹</mo><mi mathsize="90%" id="A2.p6.2.m2.1.1.2.2.3" xref="A2.p6.2.m2.1.1.2.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="A2.p6.2.m2.1.1.2.2.1a" xref="A2.p6.2.m2.1.1.2.2.1.cmml">â€‹</mo><mi mathsize="90%" id="A2.p6.2.m2.1.1.2.2.4" xref="A2.p6.2.m2.1.1.2.2.4.cmml">m</mi></mrow><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true" id="A2.p6.2.m2.1.1.2.1" xref="A2.p6.2.m2.1.1.2.1.cmml">/</mo><mi mathsize="90%" id="A2.p6.2.m2.1.1.2.3" xref="A2.p6.2.m2.1.1.2.3.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="A2.p6.2.m2.1.1.1" xref="A2.p6.2.m2.1.1.1.cmml">â€‹</mo><mi mathsize="90%" id="A2.p6.2.m2.1.1.3" xref="A2.p6.2.m2.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="A2.p6.2.m2.1.1.1a" xref="A2.p6.2.m2.1.1.1.cmml">â€‹</mo><mi mathsize="90%" id="A2.p6.2.m2.1.1.4" xref="A2.p6.2.m2.1.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="A2.p6.2.m2.1.1.1b" xref="A2.p6.2.m2.1.1.1.cmml">â€‹</mo><mi mathsize="90%" id="A2.p6.2.m2.1.1.5" xref="A2.p6.2.m2.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="A2.p6.2.m2.1.1.1c" xref="A2.p6.2.m2.1.1.1.cmml">â€‹</mo><mi mathsize="90%" id="A2.p6.2.m2.1.1.6" xref="A2.p6.2.m2.1.1.6.cmml">l</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.p6.2.m2.1b"><apply id="A2.p6.2.m2.1.1.cmml" xref="A2.p6.2.m2.1.1"><times id="A2.p6.2.m2.1.1.1.cmml" xref="A2.p6.2.m2.1.1.1"></times><apply id="A2.p6.2.m2.1.1.2.cmml" xref="A2.p6.2.m2.1.1.2"><divide id="A2.p6.2.m2.1.1.2.1.cmml" xref="A2.p6.2.m2.1.1.2.1"></divide><apply id="A2.p6.2.m2.1.1.2.2.cmml" xref="A2.p6.2.m2.1.1.2.2"><times id="A2.p6.2.m2.1.1.2.2.1.cmml" xref="A2.p6.2.m2.1.1.2.2.1"></times><cn type="integer" id="A2.p6.2.m2.1.1.2.2.2.cmml" xref="A2.p6.2.m2.1.1.2.2.2">5</cn><ci id="A2.p6.2.m2.1.1.2.2.3.cmml" xref="A2.p6.2.m2.1.1.2.2.3">ğ‘</ci><ci id="A2.p6.2.m2.1.1.2.2.4.cmml" xref="A2.p6.2.m2.1.1.2.2.4">ğ‘š</ci></apply><ci id="A2.p6.2.m2.1.1.2.3.cmml" xref="A2.p6.2.m2.1.1.2.3">ğ‘</ci></apply><ci id="A2.p6.2.m2.1.1.3.cmml" xref="A2.p6.2.m2.1.1.3">ğ‘–</ci><ci id="A2.p6.2.m2.1.1.4.cmml" xref="A2.p6.2.m2.1.1.4">ğ‘¥</ci><ci id="A2.p6.2.m2.1.1.5.cmml" xref="A2.p6.2.m2.1.1.5">ğ‘’</ci><ci id="A2.p6.2.m2.1.1.6.cmml" xref="A2.p6.2.m2.1.1.6">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p6.2.m2.1c">5cm/pixel</annotation></semantics></math><span id="A2.p6.4.7" class="ltx_text" style="font-size:90%;"> (0.05m GSD) for RGB images, </span><math id="A2.p6.3.m3.1" class="ltx_Math" alttext="100cm/pixel" display="inline"><semantics id="A2.p6.3.m3.1a"><mrow id="A2.p6.3.m3.1.1" xref="A2.p6.3.m3.1.1.cmml"><mrow id="A2.p6.3.m3.1.1.2" xref="A2.p6.3.m3.1.1.2.cmml"><mrow id="A2.p6.3.m3.1.1.2.2" xref="A2.p6.3.m3.1.1.2.2.cmml"><mn mathsize="90%" id="A2.p6.3.m3.1.1.2.2.2" xref="A2.p6.3.m3.1.1.2.2.2.cmml">100</mn><mo lspace="0em" rspace="0em" id="A2.p6.3.m3.1.1.2.2.1" xref="A2.p6.3.m3.1.1.2.2.1.cmml">â€‹</mo><mi mathsize="90%" id="A2.p6.3.m3.1.1.2.2.3" xref="A2.p6.3.m3.1.1.2.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="A2.p6.3.m3.1.1.2.2.1a" xref="A2.p6.3.m3.1.1.2.2.1.cmml">â€‹</mo><mi mathsize="90%" id="A2.p6.3.m3.1.1.2.2.4" xref="A2.p6.3.m3.1.1.2.2.4.cmml">m</mi></mrow><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true" id="A2.p6.3.m3.1.1.2.1" xref="A2.p6.3.m3.1.1.2.1.cmml">/</mo><mi mathsize="90%" id="A2.p6.3.m3.1.1.2.3" xref="A2.p6.3.m3.1.1.2.3.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="A2.p6.3.m3.1.1.1" xref="A2.p6.3.m3.1.1.1.cmml">â€‹</mo><mi mathsize="90%" id="A2.p6.3.m3.1.1.3" xref="A2.p6.3.m3.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="A2.p6.3.m3.1.1.1a" xref="A2.p6.3.m3.1.1.1.cmml">â€‹</mo><mi mathsize="90%" id="A2.p6.3.m3.1.1.4" xref="A2.p6.3.m3.1.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="A2.p6.3.m3.1.1.1b" xref="A2.p6.3.m3.1.1.1.cmml">â€‹</mo><mi mathsize="90%" id="A2.p6.3.m3.1.1.5" xref="A2.p6.3.m3.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="A2.p6.3.m3.1.1.1c" xref="A2.p6.3.m3.1.1.1.cmml">â€‹</mo><mi mathsize="90%" id="A2.p6.3.m3.1.1.6" xref="A2.p6.3.m3.1.1.6.cmml">l</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.p6.3.m3.1b"><apply id="A2.p6.3.m3.1.1.cmml" xref="A2.p6.3.m3.1.1"><times id="A2.p6.3.m3.1.1.1.cmml" xref="A2.p6.3.m3.1.1.1"></times><apply id="A2.p6.3.m3.1.1.2.cmml" xref="A2.p6.3.m3.1.1.2"><divide id="A2.p6.3.m3.1.1.2.1.cmml" xref="A2.p6.3.m3.1.1.2.1"></divide><apply id="A2.p6.3.m3.1.1.2.2.cmml" xref="A2.p6.3.m3.1.1.2.2"><times id="A2.p6.3.m3.1.1.2.2.1.cmml" xref="A2.p6.3.m3.1.1.2.2.1"></times><cn type="integer" id="A2.p6.3.m3.1.1.2.2.2.cmml" xref="A2.p6.3.m3.1.1.2.2.2">100</cn><ci id="A2.p6.3.m3.1.1.2.2.3.cmml" xref="A2.p6.3.m3.1.1.2.2.3">ğ‘</ci><ci id="A2.p6.3.m3.1.1.2.2.4.cmml" xref="A2.p6.3.m3.1.1.2.2.4">ğ‘š</ci></apply><ci id="A2.p6.3.m3.1.1.2.3.cmml" xref="A2.p6.3.m3.1.1.2.3">ğ‘</ci></apply><ci id="A2.p6.3.m3.1.1.3.cmml" xref="A2.p6.3.m3.1.1.3">ğ‘–</ci><ci id="A2.p6.3.m3.1.1.4.cmml" xref="A2.p6.3.m3.1.1.4">ğ‘¥</ci><ci id="A2.p6.3.m3.1.1.5.cmml" xref="A2.p6.3.m3.1.1.5">ğ‘’</ci><ci id="A2.p6.3.m3.1.1.6.cmml" xref="A2.p6.3.m3.1.1.6">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p6.3.m3.1c">100cm/pixel</annotation></semantics></math><span id="A2.p6.4.8" class="ltx_text" style="font-size:90%;"> (1m GSD) for HSI images, and </span><math id="A2.p6.4.m4.1" class="ltx_Math" alttext="50cm/pixel" display="inline"><semantics id="A2.p6.4.m4.1a"><mrow id="A2.p6.4.m4.1.1" xref="A2.p6.4.m4.1.1.cmml"><mrow id="A2.p6.4.m4.1.1.2" xref="A2.p6.4.m4.1.1.2.cmml"><mrow id="A2.p6.4.m4.1.1.2.2" xref="A2.p6.4.m4.1.1.2.2.cmml"><mn mathsize="90%" id="A2.p6.4.m4.1.1.2.2.2" xref="A2.p6.4.m4.1.1.2.2.2.cmml">50</mn><mo lspace="0em" rspace="0em" id="A2.p6.4.m4.1.1.2.2.1" xref="A2.p6.4.m4.1.1.2.2.1.cmml">â€‹</mo><mi mathsize="90%" id="A2.p6.4.m4.1.1.2.2.3" xref="A2.p6.4.m4.1.1.2.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="A2.p6.4.m4.1.1.2.2.1a" xref="A2.p6.4.m4.1.1.2.2.1.cmml">â€‹</mo><mi mathsize="90%" id="A2.p6.4.m4.1.1.2.2.4" xref="A2.p6.4.m4.1.1.2.2.4.cmml">m</mi></mrow><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true" id="A2.p6.4.m4.1.1.2.1" xref="A2.p6.4.m4.1.1.2.1.cmml">/</mo><mi mathsize="90%" id="A2.p6.4.m4.1.1.2.3" xref="A2.p6.4.m4.1.1.2.3.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="A2.p6.4.m4.1.1.1" xref="A2.p6.4.m4.1.1.1.cmml">â€‹</mo><mi mathsize="90%" id="A2.p6.4.m4.1.1.3" xref="A2.p6.4.m4.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="A2.p6.4.m4.1.1.1a" xref="A2.p6.4.m4.1.1.1.cmml">â€‹</mo><mi mathsize="90%" id="A2.p6.4.m4.1.1.4" xref="A2.p6.4.m4.1.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="A2.p6.4.m4.1.1.1b" xref="A2.p6.4.m4.1.1.1.cmml">â€‹</mo><mi mathsize="90%" id="A2.p6.4.m4.1.1.5" xref="A2.p6.4.m4.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="A2.p6.4.m4.1.1.1c" xref="A2.p6.4.m4.1.1.1.cmml">â€‹</mo><mi mathsize="90%" id="A2.p6.4.m4.1.1.6" xref="A2.p6.4.m4.1.1.6.cmml">l</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.p6.4.m4.1b"><apply id="A2.p6.4.m4.1.1.cmml" xref="A2.p6.4.m4.1.1"><times id="A2.p6.4.m4.1.1.1.cmml" xref="A2.p6.4.m4.1.1.1"></times><apply id="A2.p6.4.m4.1.1.2.cmml" xref="A2.p6.4.m4.1.1.2"><divide id="A2.p6.4.m4.1.1.2.1.cmml" xref="A2.p6.4.m4.1.1.2.1"></divide><apply id="A2.p6.4.m4.1.1.2.2.cmml" xref="A2.p6.4.m4.1.1.2.2"><times id="A2.p6.4.m4.1.1.2.2.1.cmml" xref="A2.p6.4.m4.1.1.2.2.1"></times><cn type="integer" id="A2.p6.4.m4.1.1.2.2.2.cmml" xref="A2.p6.4.m4.1.1.2.2.2">50</cn><ci id="A2.p6.4.m4.1.1.2.2.3.cmml" xref="A2.p6.4.m4.1.1.2.2.3">ğ‘</ci><ci id="A2.p6.4.m4.1.1.2.2.4.cmml" xref="A2.p6.4.m4.1.1.2.2.4">ğ‘š</ci></apply><ci id="A2.p6.4.m4.1.1.2.3.cmml" xref="A2.p6.4.m4.1.1.2.3">ğ‘</ci></apply><ci id="A2.p6.4.m4.1.1.3.cmml" xref="A2.p6.4.m4.1.1.3">ğ‘–</ci><ci id="A2.p6.4.m4.1.1.4.cmml" xref="A2.p6.4.m4.1.1.4">ğ‘¥</ci><ci id="A2.p6.4.m4.1.1.5.cmml" xref="A2.p6.4.m4.1.1.5">ğ‘’</ci><ci id="A2.p6.4.m4.1.1.6.cmml" xref="A2.p6.4.m4.1.1.6">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p6.4.m4.1c">50cm/pixel</annotation></semantics></math><span id="A2.p6.4.9" class="ltx_text" style="font-size:90%;"> (0.5m GSD) for MS-LiDAR. To test our fine-grained segmentation ability, we relabeled two tiles (272652_3289689, 273248_3289689) from the test set with super high quality building masks serving as evaluation ground-truth, which will be released together with code.
The dataset is used for </span><span id="A2.p6.4.10" class="ltx_text ltx_font_italic" style="font-size:90%;">building</span><span id="A2.p6.4.11" class="ltx_text" style="font-size:90%;"> segmentation in this paper.
More details of this dataset can be found at </span><a target="_blank" href="https://hyperspectral.ee.uh.edu/?page_id=1075" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://hyperspectral.ee.uh.edu/?page_id=1075</a><span id="A2.p6.4.12" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="A2.p7" class="ltx_para ltx_noindent">
<p id="A2.p7.1" class="ltx_p"><span id="A2.p7.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">DFC2023</span><span id="A2.p7.1.2" class="ltx_text" style="font-size:90%;">Â </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.p7.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a><span id="A2.p7.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.p7.1.5" class="ltx_text" style="font-size:90%;"> focuses on building detection using high-resolution optical satellite imagery and Synthetic Aperture Radar (SAR) images. The dataset encompasses buildings from 17 cities across 6 continents.
We use this dataset to segment </span><span id="A2.p7.1.6" class="ltx_text ltx_font_italic" style="font-size:90%;">buildings</span><span id="A2.p7.1.7" class="ltx_text" style="font-size:90%;">. Specifically, data from Soochow and Copenhagen are used as the test set, while data from the remaining cities constitutes the training set.
More details of this dataset can be found at </span><a target="_blank" href="https://ieee-dataport.org/competitions/2023-ieee-grss-data-fusion-contest-large-scale-fine-grained-building-classification" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://ieee-dataport.org/competitions/2023-ieee-grss-data-fusion-contest-large-scale-fine-grained-building-classification</a><span id="A2.p7.1.8" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="A2.p8" class="ltx_para ltx_noindent">
<p id="A2.p8.1" class="ltx_p"><span id="A2.p8.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">ISPRS Potsdam</span><span id="A2.p8.1.2" class="ltx_text" style="font-size:90%;"> contains 38 high-resolution images of Potsdam City, Germany, with a ground sampling distance of 5 cm. This dataset includes two modalities: true orthophoto (TOP) and digital surface model (DSM). The TOP modality corresponds to RGB images, while the DSM modality includes the near-infrared band. In this study, we utilize both TOP and DSM data to construct a cross-modal and multi-modal learning paradigm. We designate images 6_07, 6_08, 6_09, 7_07, 7_08, and 7_09 as the test set, using the remaining images for training. In this paper, we ulilize this dataset for </span><span id="A2.p8.1.3" class="ltx_text ltx_font_italic" style="font-size:90%;">building</span><span id="A2.p8.1.4" class="ltx_text" style="font-size:90%;"> segmentation and report IoU performance.
More details of this dataset can be found at </span><a target="_blank" href="https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-potsdam.aspx" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-potsdam.aspx</a><span id="A2.p8.1.5" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Data Representations</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p id="A2.SS1.p1.2" class="ltx_p"><span id="A2.SS1.p1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">MM-SAM.</span><span id="A2.SS1.p1.2.2" class="ltx_text" style="font-size:90%;"> We use the standard RGB representations for </span><span id="A2.SS1.p1.2.3" class="ltx_text ltx_font_bold" style="font-size:90%;">visual</span><span id="A2.SS1.p1.2.4" class="ltx_text" style="font-size:90%;"> images. For </span><span id="A2.SS1.p1.2.5" class="ltx_text ltx_font_bold" style="font-size:90%;">LiDAR</span><span id="A2.SS1.p1.2.6" class="ltx_text" style="font-size:90%;"> point clouds from SemanticKITTI, we follow common practices in projection-based methodsÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.SS1.p1.2.7.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>, <a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a><span id="A2.SS1.p1.2.8.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.SS1.p1.2.9" class="ltx_text" style="font-size:90%;"> and project them into 4-channel images with coordinates </span><math id="A2.SS1.p1.1.m1.3" class="ltx_Math" alttext="(x,y,z)" display="inline"><semantics id="A2.SS1.p1.1.m1.3a"><mrow id="A2.SS1.p1.1.m1.3.4.2" xref="A2.SS1.p1.1.m1.3.4.1.cmml"><mo maxsize="90%" minsize="90%" id="A2.SS1.p1.1.m1.3.4.2.1" xref="A2.SS1.p1.1.m1.3.4.1.cmml">(</mo><mi mathsize="90%" id="A2.SS1.p1.1.m1.1.1" xref="A2.SS1.p1.1.m1.1.1.cmml">x</mi><mo mathsize="90%" id="A2.SS1.p1.1.m1.3.4.2.2" xref="A2.SS1.p1.1.m1.3.4.1.cmml">,</mo><mi mathsize="90%" id="A2.SS1.p1.1.m1.2.2" xref="A2.SS1.p1.1.m1.2.2.cmml">y</mi><mo mathsize="90%" id="A2.SS1.p1.1.m1.3.4.2.3" xref="A2.SS1.p1.1.m1.3.4.1.cmml">,</mo><mi mathsize="90%" id="A2.SS1.p1.1.m1.3.3" xref="A2.SS1.p1.1.m1.3.3.cmml">z</mi><mo maxsize="90%" minsize="90%" id="A2.SS1.p1.1.m1.3.4.2.4" xref="A2.SS1.p1.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.1.m1.3b"><vector id="A2.SS1.p1.1.m1.3.4.1.cmml" xref="A2.SS1.p1.1.m1.3.4.2"><ci id="A2.SS1.p1.1.m1.1.1.cmml" xref="A2.SS1.p1.1.m1.1.1">ğ‘¥</ci><ci id="A2.SS1.p1.1.m1.2.2.cmml" xref="A2.SS1.p1.1.m1.2.2">ğ‘¦</ci><ci id="A2.SS1.p1.1.m1.3.3.cmml" xref="A2.SS1.p1.1.m1.3.3">ğ‘§</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.1.m1.3c">(x,y,z)</annotation></semantics></math><span id="A2.SS1.p1.2.10" class="ltx_text" style="font-size:90%;"> and laser reflectance intensity. We use a single-channel image for the </span><span id="A2.SS1.p1.2.11" class="ltx_text ltx_font_bold" style="font-size:90%;">thermal</span><span id="A2.SS1.p1.2.12" class="ltx_text" style="font-size:90%;"> data from MFNet and FreiburgThermal datasets due to its natural form in which current infrared thermal sensors return data.
For </span><span id="A2.SS1.p1.2.13" class="ltx_text ltx_font_bold" style="font-size:90%;">depth</span><span id="A2.SS1.p1.2.14" class="ltx_text" style="font-size:90%;"> images from SUN RGB-D, we use the single channel of depth.
In DFC2018, for </span><span id="A2.SS1.p1.2.15" class="ltx_text ltx_font_bold" style="font-size:90%;">hyperspectral</span><span id="A2.SS1.p1.2.16" class="ltx_text" style="font-size:90%;"> imaging, we directly use the original 48 channels with full information; While for </span><span id="A2.SS1.p1.2.17" class="ltx_text ltx_font_bold" style="font-size:90%;">multispectral-LiDAR</span><span id="A2.SS1.p1.2.18" class="ltx_text" style="font-size:90%;"> data, we use officially provided LiDAR point cloud tiles to project </span><math id="A2.SS1.p1.2.m2.3" class="ltx_Math" alttext="x,y,z" display="inline"><semantics id="A2.SS1.p1.2.m2.3a"><mrow id="A2.SS1.p1.2.m2.3.4.2" xref="A2.SS1.p1.2.m2.3.4.1.cmml"><mi mathsize="90%" id="A2.SS1.p1.2.m2.1.1" xref="A2.SS1.p1.2.m2.1.1.cmml">x</mi><mo mathsize="90%" id="A2.SS1.p1.2.m2.3.4.2.1" xref="A2.SS1.p1.2.m2.3.4.1.cmml">,</mo><mi mathsize="90%" id="A2.SS1.p1.2.m2.2.2" xref="A2.SS1.p1.2.m2.2.2.cmml">y</mi><mo mathsize="90%" id="A2.SS1.p1.2.m2.3.4.2.2" xref="A2.SS1.p1.2.m2.3.4.1.cmml">,</mo><mi mathsize="90%" id="A2.SS1.p1.2.m2.3.3" xref="A2.SS1.p1.2.m2.3.3.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.2.m2.3b"><list id="A2.SS1.p1.2.m2.3.4.1.cmml" xref="A2.SS1.p1.2.m2.3.4.2"><ci id="A2.SS1.p1.2.m2.1.1.cmml" xref="A2.SS1.p1.2.m2.1.1">ğ‘¥</ci><ci id="A2.SS1.p1.2.m2.2.2.cmml" xref="A2.SS1.p1.2.m2.2.2">ğ‘¦</ci><ci id="A2.SS1.p1.2.m2.3.3.cmml" xref="A2.SS1.p1.2.m2.3.3">ğ‘§</ci></list></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.2.m2.3c">x,y,z</annotation></semantics></math><span id="A2.SS1.p1.2.19" class="ltx_text" style="font-size:90%;"> onto rasters and generate 6 channel data, including geo-coordinates and intensity rasters at wavelengths of C1 (1550 nm), C2 (1064 nm), and C3 (532 nm).
For the </span><span id="A2.SS1.p1.2.20" class="ltx_text ltx_font_bold" style="font-size:90%;">SAR</span><span id="A2.SS1.p1.2.21" class="ltx_text" style="font-size:90%;"> images from DFC2023, we follow the official data format and use single-channel images.
For the </span><span id="A2.SS1.p1.2.22" class="ltx_text ltx_font_bold" style="font-size:90%;">digital surface model</span><span id="A2.SS1.p1.2.23" class="ltx_text" style="font-size:90%;"> data from Potsdam, we also use the single-channel (height value) images as input.</span></p>
</div>
<div id="A2.SS1.p2" class="ltx_para">
<p id="A2.SS1.p2.1" class="ltx_p"><span id="A2.SS1.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">SAM for false-color images from non-RGB modalities.</span><span id="A2.SS1.p2.1.2" class="ltx_text" style="font-size:90%;">
To meet the input requirements for SAMâ€™s processing, we convert all non-RGB modalities into three-channel false-color images. We use typical false colorization on single-channel images, i.e., normalizing them before stacking them into three channels. We apply this false colorization on thermal images and SAR images. For depth data, we follow common practice and map depth values to disparity before false colorization. Point clouds from SemanticKITTI are converted to depth data and conducted with false colorization. For hyperspectral imaging, we use the default bands of RGB channels. For multispectral-LiDAR data, we directly stack C1, C2, and C3 bands. For the DSM model from the Potsdam dataset, we perform a log normalization process to standardize the elevation values, followed by generating false colorization similar to depth.</span></p>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:90%;">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Implementation Details</h2>

<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Training implementation details</h3>

<div id="A3.SS1.p1" class="ltx_para">
<p id="A3.SS1.p1.1" class="ltx_p"><span id="A3.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">Table </span><a href="#A3.T6" title="Table 6 â€£ C.1 Training implementation details â€£ Appendix C Implementation Details â€£ Segment Anything with Multiple Modalities" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">6</span></a><span id="A3.SS1.p1.1.2" class="ltx_text" style="font-size:90%;"> provides the hyperparameters used to train each model reported in Tables </span><a href="#S4.T3" title="Table 3 â€£ 4.2.1 Time-Synchronized Sensor Suites â€£ 4.2 Segmentation Results â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="A3.SS1.p1.1.3" class="ltx_text" style="font-size:90%;"> and </span><a href="#S4.T4" title="Table 4 â€£ 4.2.2 Time-Asynchronous Sensor Suites â€£ 4.2 Segmentation Results â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="A3.SS1.p1.1.4" class="ltx_text" style="font-size:90%;">. Table </span><a href="#A3.T7" title="Table 7 â€£ C.1 Training implementation details â€£ Appendix C Implementation Details â€£ Segment Anything with Multiple Modalities" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">7</span></a><span id="A3.SS1.p1.1.5" class="ltx_text" style="font-size:90%;"> lists the data augmentations applied for UCMT on each dataset, while no augmentations are used for WMMF. MM-SAM for all tested datasets could be trained with 4 NVIDIA A100 GPUs within 20 hours except for SemanticKITTI which took 35 hours. Note that we adopted simple training configurations for MM-SAM across different benchmarks. While more sophisticated tuning could potentially improve performance, it is not the main objective of our study.</span></p>
</div>
<figure id="A3.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="A3.T6.4.1.1" class="ltx_text" style="font-size:113%;">Table 6</span>: </span><span id="A3.T6.5.2" class="ltx_text" style="font-size:113%;">Training hyperparameters of MM-SAM including UCMF and WMMF.</span></figcaption>
<table id="A3.T6.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A3.T6.6.1.1" class="ltx_tr">
<th id="A3.T6.6.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:14.5pt;padding-right:14.5pt;"></th>
<td id="A3.T6.6.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.1.1.2.1" class="ltx_text" style="font-size:80%;">UCMT</span></td>
<td id="A3.T6.6.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.1.1.3.1" class="ltx_text" style="font-size:80%;">WMMF</span></td>
</tr>
<tr id="A3.T6.6.2.2" class="ltx_tr">
<th id="A3.T6.6.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.2.2.1.1" class="ltx_text" style="font-size:80%;">Total epochs</span></th>
<td id="A3.T6.6.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.2.2.2.1" class="ltx_text" style="font-size:80%;">50</span></td>
<td id="A3.T6.6.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.2.2.3.1" class="ltx_text" style="font-size:80%;">30</span></td>
</tr>
<tr id="A3.T6.6.3.3" class="ltx_tr">
<th id="A3.T6.6.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.3.3.1.1" class="ltx_text" style="font-size:80%;">Batch size</span></th>
<td id="A3.T6.6.3.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.3.3.2.1" class="ltx_text" style="font-size:80%;">16</span></td>
<td id="A3.T6.6.3.3.3" class="ltx_td ltx_align_center" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.3.3.3.1" class="ltx_text" style="font-size:80%;">16</span></td>
</tr>
<tr id="A3.T6.6.4.4" class="ltx_tr">
<th id="A3.T6.6.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.4.4.1.1" class="ltx_text" style="font-size:80%;">Optimizer</span></th>
<td id="A3.T6.6.4.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.4.4.2.1" class="ltx_text" style="font-size:80%;">AdamW</span></td>
<td id="A3.T6.6.4.4.3" class="ltx_td ltx_align_center" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.4.4.3.1" class="ltx_text" style="font-size:80%;">AdamW</span></td>
</tr>
<tr id="A3.T6.6.5.5" class="ltx_tr">
<th id="A3.T6.6.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.5.5.1.1" class="ltx_text" style="font-size:80%;">Peak learning rate</span></th>
<td id="A3.T6.6.5.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.5.5.2.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">1.6e-3</span></td>
<td id="A3.T6.6.5.5.3" class="ltx_td ltx_align_center" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.5.5.3.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">4e-4</span></td>
</tr>
<tr id="A3.T6.6.6.6" class="ltx_tr">
<th id="A3.T6.6.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.6.6.1.1" class="ltx_text" style="font-size:80%;">Scheduler</span></th>
<td id="A3.T6.6.6.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.6.6.2.1" class="ltx_text" style="font-size:80%;">CosineAnnealingLR</span></td>
<td id="A3.T6.6.6.6.3" class="ltx_td ltx_align_center" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.6.6.3.1" class="ltx_text" style="font-size:80%;">CosineAnnealingLR</span></td>
</tr>
<tr id="A3.T6.6.7.7" class="ltx_tr">
<th id="A3.T6.6.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.7.7.1.1" class="ltx_text" style="font-size:80%;">Minimum learning rate</span></th>
<td id="A3.T6.6.7.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.7.7.2.1" class="ltx_text" style="font-size:80%;">eta_min=1e-5</span></td>
<td id="A3.T6.6.7.7.3" class="ltx_td ltx_align_center" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.7.7.3.1" class="ltx_text" style="font-size:80%;">eta_min=1e-5</span></td>
</tr>
<tr id="A3.T6.6.8.8" class="ltx_tr">
<th id="A3.T6.6.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.8.8.1.1" class="ltx_text" style="font-size:80%;">Input prompts</span></th>
<td id="A3.T6.6.8.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.8.8.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="A3.T6.6.8.8.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:14.5pt;padding-right:14.5pt;"><span id="A3.T6.6.8.8.3.1" class="ltx_text" style="font-size:80%;">Boxes</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="A3.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="A3.T7.4.1.1" class="ltx_text" style="font-size:113%;">Table 7</span>: </span><span id="A3.T7.5.2" class="ltx_text" style="font-size:113%;">Data augmentation strategies over different datasets.</span></figcaption>
<table id="A3.T7.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A3.T7.6.1.1" class="ltx_tr">
<th id="A3.T7.6.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;"><span id="A3.T7.6.1.1.1.1" class="ltx_text" style="font-size:80%;">SUN RGB-D</span></th>
<td id="A3.T7.6.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;"><span id="A3.T7.6.1.1.2.1" class="ltx_text" style="font-size:80%;">z-score, RandomCrop with a scale factor of [0.8, 1.0]</span></td>
</tr>
<tr id="A3.T7.6.2.2" class="ltx_tr">
<th id="A3.T7.6.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:12.8pt;padding-right:12.8pt;"><span id="A3.T7.6.2.2.1.1" class="ltx_text" style="font-size:80%;">MFNet</span></th>
<td id="A3.T7.6.2.2.2" class="ltx_td ltx_align_center" style="padding-left:12.8pt;padding-right:12.8pt;"><span id="A3.T7.6.2.2.2.1" class="ltx_text" style="font-size:80%;">z-score, RandomCrop with a scale factor of [0.8, 1.0]</span></td>
</tr>
<tr id="A3.T7.6.3.3" class="ltx_tr">
<th id="A3.T7.6.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:12.8pt;padding-right:12.8pt;"><span id="A3.T7.6.3.3.1.1" class="ltx_text" style="font-size:80%;">DFC2023</span></th>
<td id="A3.T7.6.3.3.2" class="ltx_td ltx_align_center" style="padding-left:12.8pt;padding-right:12.8pt;"><span id="A3.T7.6.3.3.2.1" class="ltx_text" style="font-size:80%;">z-score, RandomCrop with a scale factor of [0.8, 1.0]</span></td>
</tr>
<tr id="A3.T7.6.4.4" class="ltx_tr">
<th id="A3.T7.6.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:12.8pt;padding-right:12.8pt;"><span id="A3.T7.6.4.4.1.1" class="ltx_text" style="font-size:80%;">Postdam</span></th>
<td id="A3.T7.6.4.4.2" class="ltx_td ltx_align_center" style="padding-left:12.8pt;padding-right:12.8pt;"><span id="A3.T7.6.4.4.2.1" class="ltx_text" style="font-size:80%;">log+min-max, RandomCrop with a scale factor of [0.8, 1.0]</span></td>
</tr>
<tr id="A3.T7.6.5.5" class="ltx_tr">
<th id="A3.T7.6.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:12.8pt;padding-right:12.8pt;"><span id="A3.T7.6.5.5.1.1" class="ltx_text" style="font-size:80%;">DFC2018</span></th>
<td id="A3.T7.6.5.5.2" class="ltx_td ltx_align_center" style="padding-left:12.8pt;padding-right:12.8pt;"><span id="A3.T7.6.5.5.2.1" class="ltx_text" style="font-size:80%;">z-score</span></td>
</tr>
<tr id="A3.T7.6.6.6" class="ltx_tr">
<th id="A3.T7.6.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:12.8pt;padding-right:12.8pt;"><span id="A3.T7.6.6.6.1.1" class="ltx_text" style="font-size:80%;">SemanticKITT</span></th>
<td id="A3.T7.6.6.6.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:12.8pt;padding-right:12.8pt;"><span id="A3.T7.6.6.6.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="A3.T8" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="A3.T8.6.1.1" class="ltx_text" style="font-size:113%;">Table 8</span>: </span><span id="A3.T8.7.2" class="ltx_text" style="font-size:113%;">Segmentation performance on MFNet by MM-SAM with different embedding unification losses. mIoU is reported for total/day/night.</span></figcaption>
<table id="A3.T8.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A3.T8.2.3.1" class="ltx_tr">
<th id="A3.T8.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:22.8pt;padding-right:22.8pt;"><span id="A3.T8.2.3.1.1.1" class="ltx_text" style="font-size:80%;">Â Â Â Â Â Â Â Loss Type</span></th>
<th id="A3.T8.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:22.8pt;padding-right:22.8pt;"><span id="A3.T8.2.3.1.2.1" class="ltx_text" style="font-size:80%;">Â Â Â Â Â Â Â Thermal</span></th>
<th id="A3.T8.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:22.8pt;padding-right:22.8pt;"><span id="A3.T8.2.3.1.3.1" class="ltx_text" style="font-size:80%;">Â Â Â Â Â Â Â RGB+Thermal</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A3.T8.1.1" class="ltx_tr">
<th id="A3.T8.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:22.8pt;padding-right:22.8pt;">
<span id="A3.T8.1.1.1.1" class="ltx_text" style="font-size:80%;">Â Â Â Â Â Â Â </span><math id="A3.T8.1.1.1.m1.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="A3.T8.1.1.1.m1.1a"><msub id="A3.T8.1.1.1.m1.1.1" xref="A3.T8.1.1.1.m1.1.1.cmml"><mi mathsize="80%" id="A3.T8.1.1.1.m1.1.1.2" xref="A3.T8.1.1.1.m1.1.1.2.cmml">L</mi><mn mathsize="80%" id="A3.T8.1.1.1.m1.1.1.3" xref="A3.T8.1.1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A3.T8.1.1.1.m1.1b"><apply id="A3.T8.1.1.1.m1.1.1.cmml" xref="A3.T8.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="A3.T8.1.1.1.m1.1.1.1.cmml" xref="A3.T8.1.1.1.m1.1.1">subscript</csymbol><ci id="A3.T8.1.1.1.m1.1.1.2.cmml" xref="A3.T8.1.1.1.m1.1.1.2">ğ¿</ci><cn type="integer" id="A3.T8.1.1.1.m1.1.1.3.cmml" xref="A3.T8.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.1.1.1.m1.1c">L_{1}</annotation></semantics></math><span id="A3.T8.1.1.1.2" class="ltx_text" style="font-size:80%;"> loss</span>
</th>
<td id="A3.T8.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:22.8pt;padding-right:22.8pt;"><span id="A3.T8.1.1.2.1" class="ltx_text" style="font-size:80%;">Â Â Â Â Â Â Â 71.7/66.7/72.4</span></td>
<td id="A3.T8.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:22.8pt;padding-right:22.8pt;"><span id="A3.T8.1.1.3.1" class="ltx_text" style="font-size:80%;">Â Â Â Â Â Â Â 75.5/74.8/74.1</span></td>
</tr>
<tr id="A3.T8.2.2" class="ltx_tr">
<th id="A3.T8.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:22.8pt;padding-right:22.8pt;">
<span id="A3.T8.2.2.1.1" class="ltx_text" style="font-size:80%;">Â Â Â Â Â Â Â </span><math id="A3.T8.2.2.1.m1.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="A3.T8.2.2.1.m1.1a"><msub id="A3.T8.2.2.1.m1.1.1" xref="A3.T8.2.2.1.m1.1.1.cmml"><mi mathsize="80%" id="A3.T8.2.2.1.m1.1.1.2" xref="A3.T8.2.2.1.m1.1.1.2.cmml">L</mi><mn mathsize="80%" id="A3.T8.2.2.1.m1.1.1.3" xref="A3.T8.2.2.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A3.T8.2.2.1.m1.1b"><apply id="A3.T8.2.2.1.m1.1.1.cmml" xref="A3.T8.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="A3.T8.2.2.1.m1.1.1.1.cmml" xref="A3.T8.2.2.1.m1.1.1">subscript</csymbol><ci id="A3.T8.2.2.1.m1.1.1.2.cmml" xref="A3.T8.2.2.1.m1.1.1.2">ğ¿</ci><cn type="integer" id="A3.T8.2.2.1.m1.1.1.3.cmml" xref="A3.T8.2.2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.2.2.1.m1.1c">L_{2}</annotation></semantics></math><span id="A3.T8.2.2.1.2" class="ltx_text" style="font-size:80%;"> loss</span>
</th>
<td id="A3.T8.2.2.2" class="ltx_td ltx_align_center" style="padding-left:22.8pt;padding-right:22.8pt;"><span id="A3.T8.2.2.2.1" class="ltx_text" style="font-size:80%;">Â Â Â Â Â Â Â 72.3/67.7/73.1</span></td>
<td id="A3.T8.2.2.3" class="ltx_td ltx_align_center" style="padding-left:22.8pt;padding-right:22.8pt;"><span id="A3.T8.2.2.3.1" class="ltx_text" style="font-size:80%;">Â Â Â Â Â Â Â 75.9/74.7/74.7</span></td>
</tr>
<tr id="A3.T8.2.4.1" class="ltx_tr">
<th id="A3.T8.2.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:22.8pt;padding-right:22.8pt;"><span id="A3.T8.2.4.1.1.1" class="ltx_text" style="font-size:80%;">Â Â Â Â Â Â Â Cosine similarity loss</span></th>
<td id="A3.T8.2.4.1.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:22.8pt;padding-right:22.8pt;"><span id="A3.T8.2.4.1.2.1" class="ltx_text" style="font-size:80%;">Â Â Â Â Â Â Â 72.6/67.6/73.2</span></td>
<td id="A3.T8.2.4.1.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:22.8pt;padding-right:22.8pt;"><span id="A3.T8.2.4.1.3.1" class="ltx_text" style="font-size:80%;">Â Â Â Â Â Â Â 75.5/74.8/74.2</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="A3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Zero-shot experimental details</h3>

<div id="A3.SS2.p1" class="ltx_para">
<p id="A3.SS2.p1.1" class="ltx_p"><span id="A3.SS2.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">MFNetâ†’FreiburgThermal.</span><span id="A3.SS2.p1.1.2" class="ltx_text" style="font-size:90%;"> For the zero-shot results in Table </span><a href="#S4.T5" title="Table 5 â€£ 4.3 Discussions and Analysis â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="A3.SS2.p1.1.3" class="ltx_text" style="font-size:90%;"> (a), we follow the same strategy as for intra-domain evaluation, using the official training set of MFNet and the testing set of FreiburgThermal.</span></p>
</div>
<div id="A3.SS2.p2" class="ltx_para">
<p id="A3.SS2.p2.1" class="ltx_p"><span id="A3.SS2.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">SUN RGB-Dâ†’NYU&amp;B3DO.</span><span id="A3.SS2.p2.1.2" class="ltx_text" style="font-size:90%;"> SUN RGB-D consists of data collected from four types of sensors: Kinect v1, Kinect v2, Xtion, and Realsense. For testing, we use data from Kinect v1 (specifically its NYU and B3DO subsets), while the remaining sensors are used for training, creating a robust cross-sensor evaluation of zero-shot segmentation as in Table </span><a href="#S4.T5" title="Table 5 â€£ 4.3 Discussions and Analysis â€£ 4 Experiment â€£ Segment Anything with Multiple Modalities" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="A3.SS2.p2.1.3" class="ltx_text" style="font-size:90%;"> (b).</span></p>
</div>
</section>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:90%;">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Additional Results</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.4" class="ltx_p"><span id="A4.p1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Design choices in losses.</span><span id="A4.p1.4.2" class="ltx_text" style="font-size:90%;"> We tested different losses for embedding unification in UCMT, including </span><math id="A4.p1.1.m1.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="A4.p1.1.m1.1a"><msub id="A4.p1.1.m1.1.1" xref="A4.p1.1.m1.1.1.cmml"><mi mathsize="90%" id="A4.p1.1.m1.1.1.2" xref="A4.p1.1.m1.1.1.2.cmml">L</mi><mn mathsize="90%" id="A4.p1.1.m1.1.1.3" xref="A4.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A4.p1.1.m1.1b"><apply id="A4.p1.1.m1.1.1.cmml" xref="A4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A4.p1.1.m1.1.1.1.cmml" xref="A4.p1.1.m1.1.1">subscript</csymbol><ci id="A4.p1.1.m1.1.1.2.cmml" xref="A4.p1.1.m1.1.1.2">ğ¿</ci><cn type="integer" id="A4.p1.1.m1.1.1.3.cmml" xref="A4.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.1.m1.1c">L_{1}</annotation></semantics></math><span id="A4.p1.4.3" class="ltx_text" style="font-size:90%;"> loss, </span><math id="A4.p1.2.m2.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="A4.p1.2.m2.1a"><msub id="A4.p1.2.m2.1.1" xref="A4.p1.2.m2.1.1.cmml"><mi mathsize="90%" id="A4.p1.2.m2.1.1.2" xref="A4.p1.2.m2.1.1.2.cmml">L</mi><mn mathsize="90%" id="A4.p1.2.m2.1.1.3" xref="A4.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A4.p1.2.m2.1b"><apply id="A4.p1.2.m2.1.1.cmml" xref="A4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A4.p1.2.m2.1.1.1.cmml" xref="A4.p1.2.m2.1.1">subscript</csymbol><ci id="A4.p1.2.m2.1.1.2.cmml" xref="A4.p1.2.m2.1.1.2">ğ¿</ci><cn type="integer" id="A4.p1.2.m2.1.1.3.cmml" xref="A4.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.2.m2.1c">L_{2}</annotation></semantics></math><span id="A4.p1.4.4" class="ltx_text" style="font-size:90%;"> loss, and Cosine Similarity loss. Table </span><a href="#A3.T8" title="Table 8 â€£ C.1 Training implementation details â€£ Appendix C Implementation Details â€£ Segment Anything with Multiple Modalities" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">8</span></a><span id="A4.p1.4.5" class="ltx_text" style="font-size:90%;"> shows segmentation results of MM-SAM trained with these different losses on MFNet, including cross-modal segmentation on thermal and multi-modal segmentation on RGB+thermal. All three losses achieved superior results, with the </span><math id="A4.p1.3.m3.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="A4.p1.3.m3.1a"><msub id="A4.p1.3.m3.1.1" xref="A4.p1.3.m3.1.1.cmml"><mi mathsize="90%" id="A4.p1.3.m3.1.1.2" xref="A4.p1.3.m3.1.1.2.cmml">L</mi><mn mathsize="90%" id="A4.p1.3.m3.1.1.3" xref="A4.p1.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A4.p1.3.m3.1b"><apply id="A4.p1.3.m3.1.1.cmml" xref="A4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A4.p1.3.m3.1.1.1.cmml" xref="A4.p1.3.m3.1.1">subscript</csymbol><ci id="A4.p1.3.m3.1.1.2.cmml" xref="A4.p1.3.m3.1.1.2">ğ¿</ci><cn type="integer" id="A4.p1.3.m3.1.1.3.cmml" xref="A4.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.3.m3.1c">L_{2}</annotation></semantics></math><span id="A4.p1.4.6" class="ltx_text" style="font-size:90%;"> loss showing the best multi-modal segmentation result, though with only a marginal gap from the other two. We thus empirically select </span><math id="A4.p1.4.m4.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="A4.p1.4.m4.1a"><msub id="A4.p1.4.m4.1.1" xref="A4.p1.4.m4.1.1.cmml"><mi mathsize="90%" id="A4.p1.4.m4.1.1.2" xref="A4.p1.4.m4.1.1.2.cmml">L</mi><mn mathsize="90%" id="A4.p1.4.m4.1.1.3" xref="A4.p1.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A4.p1.4.m4.1b"><apply id="A4.p1.4.m4.1.1.cmml" xref="A4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A4.p1.4.m4.1.1.1.cmml" xref="A4.p1.4.m4.1.1">subscript</csymbol><ci id="A4.p1.4.m4.1.1.2.cmml" xref="A4.p1.4.m4.1.1.2">ğ¿</ci><cn type="integer" id="A4.p1.4.m4.1.1.3.cmml" xref="A4.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.4.m4.1c">L_{2}</annotation></semantics></math><span id="A4.p1.4.7" class="ltx_text" style="font-size:90%;"> in our implementation. The results demonstrate that MM-SAM is robust to different losses.</span></p>
</div>
<figure id="A4.F6" class="ltx_figure"><img src="/html/2408.09085/assets/x6.png" id="A4.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="506" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Visual comparisons of SAMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> and MM-SAM. Red boxes denote geometric prompts, colored regions are mask predictions. The symbol * denotes false-color images transformed from each non-RGB modality.</figcaption>
</figure>
<div id="A4.p2" class="ltx_para">
<p id="A4.p2.1" class="ltx_p"><span id="A4.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Visual Illustrations.</span><span id="A4.p2.1.2" class="ltx_text" style="font-size:90%;"> Figure </span><a href="#A4.F6" title="Figure 6 â€£ Appendix D Additional Results â€£ Segment Anything with Multiple Modalities" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">6</span></a><span id="A4.p2.1.3" class="ltx_text" style="font-size:90%;"> shows qualitative comparisons between SAM and MM-SAM across multiple segmentation tasks. These illustrations demonstrate how our proposed MM-SAM achieves superior cross-modal and multi-modal segmentation.</span></p>
</div>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:90%;">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Broad Impact</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p"><span id="A5.p1.1.1" class="ltx_text" style="font-size:90%;">MM-SAM is environmentally friendly due to its resource-efficient design, including both parameter and label efficiency. It enhances the robustness of perception systems, particularly in challenging and dynamic conditions, by integrating various sensors. Additionally, MM-SAM improves AI-assisted labeling in areas where SAM underperforms. However, like SAM, it carries potential risks, such as surveillance overreach, which can raise ethical and privacy concerns.</span></p>
</div>
<div id="A5.p2" class="ltx_para">
<p id="A5.p2.1" class="ltx_p"><span id="A5.p2.1.1" class="ltx_text" style="font-size:90%;">MM-SAM creates a unified embedding for multiple modalities, which may lead to unintentional associations. Therefore, it is crucial to study joint embedding models, including MM-SAM, carefully to understand these associations and their implications. MM-SAM leverages image embeddings learned by a pretrained model on large web-based data, which can introduce biases, as documented in various studiesÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A5.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a><span id="A5.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A5.p2.1.4" class="ltx_text" style="font-size:90%;">. For creating unified embeddings for other modalities like thermal, HSI, and LiDAR, we use datasets mentioned in Appendix </span><a href="#A2" title="Appendix B Datasets and Metrics â€£ Segment Anything with Multiple Modalities" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">B</span></a><span id="A5.p2.1.5" class="ltx_text" style="font-size:90%;">. These joint embeddings are limited to the concepts present in these datasets. For instance, the thermal datasets used are limited to outdoor street scenes, while the HSI datasets are confined to remote sensing.</span></p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.09084" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.09085" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.09085">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.09085" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.09086" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 17:17:39 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
