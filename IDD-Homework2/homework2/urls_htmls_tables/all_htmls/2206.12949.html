<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2206.12949] Cross-Silo Federated Learning: Challenges and Opportunities</title><meta property="og:description" content="Federated learning (FL) is an emerging technology that enables the training of machine learning models from multiple clients while keeping the data distributed and private. Based on the participating clients and the mo…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Cross-Silo Federated Learning: Challenges and Opportunities">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Cross-Silo Federated Learning: Challenges and Opportunities">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2206.12949">

<!--Generated on Mon Mar 11 16:24:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
federated learning,  cross-silo federated learning,  machine learning,  artificial intelligence
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Cross-Silo Federated Learning:
<br class="ltx_break">Challenges and Opportunities</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chao Huang, Jianwei Huang,, and Xin Liu
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Federated learning (FL) is an emerging technology that enables the training of machine learning models from multiple clients while keeping the data distributed and private. Based on the participating clients and the model training scale, federated learning can be classified into two types: cross-device FL where clients are typically mobile devices and the client number can reach up to a scale of millions; cross-silo FL where clients are organizations or companies and the client number is usually small (e.g., within a hundred). While existing studies mainly focus on cross-device FL, this paper aims to provide an overview of the cross-silo FL. More specifically, we first discuss applications of cross-silo FL and outline its major challenges. We then provide a systematic overview of the existing approaches to the challenges in cross-silo FL by focusing on their connections and differences to cross-device FL. Finally, we discuss future directions and open issues that merit research efforts from the community.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
federated learning, cross-silo federated learning, machine learning, artificial intelligence

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated learning (FL) is a distributed machine learning
scheme where a bunch of clients collaboratively train
a global model under the coordination of a central server. Clients train models using their private local data and they
only need to upload model updates (e.g., represented by parameters
or gradients) to the server. Based on the participating clients and training scale, FL can be divided into two types: cross-device FL and cross-silo FL. In
cross-device FL, clients are small distributed entities
(e.g., smartphones, wearables, and edge devices), and each client is likely to have a relatively small amount of local data. Hence, for
cross-device FL to succeed, it usually requires a large number (e.g., up to millions) of edge devices to
participate in the training process. In cross-silo FL, however, clients are typically companies or organizations (e.g., hospitals and
banks). The number of participants is small (e.g., from two to a hundred), and each client is expected to participate in the entire training process.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Previous studies focus on cross-device FL. Interested readers can refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> for excellent surveys.
The focus of this paper, however, is cross-silo FL. Practical examples of cross-silo FL abound <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. In the health care domain, different medical institutes collaborate to train disease prediction models. For example, Owkin collaborates with pharmaceutical companies to train models for drug discovery based
on sensitive screening datasets. In the finance realm, financial organizations cooperate to train prediction models and provide customized services. For instance, WeBank and Swiss Re collectively perform data analysis and provide
financial and insurance services. In the transportation sector, companies who own distributed traffic data train a global model to predict future traffic flow.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">While there has an emerging stream of research on cross-silo FL, a systematic overview is missing in the current literature. We aim to fill this gap by providing a first overview of cross-silo FL. In particular, we overview the key challenges and solutions in cross-silo FL by explaining its connections to and focusing on its differences from cross-device FL.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The remainder of this paper is as follows. In Section <a href="#S2" title="II Problem Definition and Taxonomy ‣ Cross-Silo Federated Learning: Challenges and Opportunities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we introduce the problem formulation and taxonomy. From Section <a href="#S3" title="III Effectiveness and Efficiency ‣ Cross-Silo Federated Learning: Challenges and Opportunities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> to Section <a href="#S5" title="V Cooperation and Incentives ‣ Cross-Silo Federated Learning: Challenges and Opportunities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. we discuss the key challenges and existing approaches in cross-silo FL while elaborating on the key differences to cross-device FL. We discuss open issues and future directions in Section <a href="#S6" title="VI Future Directions and Open Issues ‣ Cross-Silo Federated Learning: Challenges and Opportunities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2206.12949/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="268" height="125" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A typical cross-silo FL process.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Problem Definition and Taxonomy</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we first give a problem definition for cross-silo FL in subsection <a href="#S2.SS1" title="II-A Problem Definition ‣ II Problem Definition and Taxonomy ‣ Cross-Silo Federated Learning: Challenges and Opportunities" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a>, and then provide a taxonomy for the key challenges and solutions in subsection <a href="#S2.SS2" title="II-B Challenge Overview ‣ II Problem Definition and Taxonomy ‣ Cross-Silo Federated Learning: Challenges and Opportunities" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a>.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Problem Definition</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">A typical cross-silo FL process consists of multiple rounds. In each round, there are four steps (see also Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Cross-Silo Federated Learning: Challenges and Opportunities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>):</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Step 1</span>: The clients download the global model generated from the previous round from the central server (in round one, the downloaded model is randomly initialized).</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Step 2</span>: The clients train the downloaded models using their private local data sets and derive updated local models.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Step 3</span>: The clients upload their local model updates to the central server.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p"><span id="S2.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Step 4</span>: The server aggregates the uploaded model updates and generates a new global model to be sent to the clients in the next round.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The cross-silo FL terminates when the global model converges or the number of training rounds exceeds a predefined threshold.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Challenge Overview</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">There are three aspects of challenges regarding the practical design of cross-silo FL as follows:</p>
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p"><span id="S2.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Effectiveness and Efficiency</span>: One major challenge is how to execute cross-silo FL effectively and efficiently. Effectiveness refers to obtaining satisfactory global (and local) models accounting for various client heterogeneity, and efficiency refers to obtaining an effective model fast with a low cost.</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p"><span id="S2.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Privacy and Security</span>: Another important challenge is privacy and security. Privacy is concerned with protecting clients’ local data from being leaked, while security pertains to detecting and refraining adversaries from jeopardizing the model training process and the model performance.</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p id="S2.I2.i3.p1.1" class="ltx_p"><span id="S2.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Cooperation and Incentives</span>: Unlike edge devices in cross-device FL, organizations or companies in cross-silo FL usually have clear long-term strategic focuses and development goals. This makes the long-term cooperation relationship more possible, as long as we can design a proper incentive mechanism.</p>
</div>
</li>
</ul>
<p id="S2.SS2.p1.2" class="ltx_p">Fig. <a href="#S2.F2" title="Figure 2 ‣ II-B Challenge Overview ‣ II Problem Definition and Taxonomy ‣ Cross-Silo Federated Learning: Challenges and Opportunities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes the taxonomy of challenges in cross-silo FL, where we further outline the corresponding key solutions (to be detailed in the following sections). In particular, the challenges in red color are the ones that are more critical in cross-silo settings and hence deserve more research attention.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2206.12949/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="206" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A taxonomy of challenges and solutions in cross-silo FL.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Effectiveness and Efficiency</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The success of cross-silo FL depends on two critical factors: <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">effectiveness</span> and <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">efficiency</span>. Effectiveness is concerned with training a global model with a desirable performance considering the high heterogeneity of participating clients. Efficiency pertains to training the model with few computation and communication overheads. In this section, we discuss methods to achieve better effectiveness and efficiency.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Optimization of Effectiveness</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Two components crucially affect the effectiveness of FL, i.e., statistical heterogeneity and system heterogeneity.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.4.1.1" class="ltx_text">III-A</span>1 </span>Statistical heterogeneity</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">This often refers to the non-i.i.d. data of clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. For example, different hospitals may hold non-i.i.d. disease data due to having geographically and demographically varying patients. Different banks can have heterogeneous data since customers from different regions/countries can have diverse economic backgrounds and investment habits.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">To tackle the statistical heterogeneity, existing studies have developed three main approaches discussed as follows <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Data moderation approaches</span>: Intuitively, the FL performance degradation with non-i.i.d. data stems from the heterogeneous data distributions. Data moderation aims to tackle this issue by modifying the distributions. One possible method is <span id="S3.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">data sharing</span>, where clients’ local models are trained by not only the local data but also the shared global data at the server. Another method is <span id="S3.I1.i1.p1.1.3" class="ltx_text ltx_font_italic">data augmentation</span>, which replenishes the local data with augmentations based on information from other clients’ data distributions.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Personalization approaches</span>: The global model computed by the server may negatively impact the clients’ local model performances in terms of performance and generalization, due to the drift of heterogeneous data. Personalization approaches aim to address this issue by adjusting the local models based on the local tasks. There are several types of personalization approaches. One is <span id="S3.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">local fine-tuning</span>, which fine-tunes local models using local data after receiving the global model. Another is <span id="S3.I1.i2.p1.1.3" class="ltx_text ltx_font_italic">personalized layer</span>, in which clients’ local models have base layers and personalized layers, and only the base layers need to be uploaded for aggregation. The third one is <span id="S3.I1.i2.p1.1.4" class="ltx_text ltx_font_italic">knowledge distillation</span>, which aims to transfer knowledge from the server (or other clients) to a specified client to enhance its local model performance on heterogeneous data.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Clustering approaches</span>: While most FL frameworks assume that there is only one global model, it can be insufficient to learn clients’ information when data are highly heterogeneous. Clustering approaches try to mitigate this issue by grouping the clients into different clusters, in which each cluster maintains a server-client framework. In particular, similar clients are allocated to the same cluster, where similarity is measured by the loss values or their uploaded model weights <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.4.1.1" class="ltx_text">III-A</span>2 </span>System Heterogeneity</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">Different clients often vary in system characteristics such as hardware, network connectivity, and energy constraints. This can lead to the straggler effect, mostly in cross-device FL. Interested readers can refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> for a more detailed discussion on tackling system heterogeneity.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p"><span id="S3.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_italic">Remark</span>: Statistical heterogeneity and system heterogeneity exist in both cross-device and cross-silo FL. However, we posit that system heterogeneity is not a major issue in a cross-silo setting, as the organizations are likely to have ample computation/energy resources and stable network connections.
Instead, cross-silo FL may put more focus on tackling statistical heterogeneity, e.g., training a satisfactory model with highly non-i.i.d. data across different organizations. This is because cross-silo FL applications can have a more stringent requirement in terms of model performance than cross-device scenarios. For example, diagnosis models trained by hospitals are expected to have a sufficiently high accuracy. In cross-device setting, the next-word prediction model can have a relatively lower accuracy requirement since the wrong predictions would cause less harm than improper medical treatment.
Furthermore, among the three types of approaches to tackling statistical heterogeneity, we argue that personalization can be more appropriate than data moderation and clustering for cross-silo FL, due to a lower risk of data leakage.
</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Optimization of Efficiency</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">As a distributed machine learning paradigm, federated learning faces the efficiency challenge and it aims to minimize the <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">computation</span> and <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">communication</span> overheads.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS1.4.1.1" class="ltx_text">III-B</span>1 </span>Computation Overheads</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">The local model training process consumes computational resources, e.g., the CPU cycles and energy consumption. To reduce the computation overheads, a plausible method is to select proper and possibly simpler local model structures (e.g., switch from deep neural networks to random forests). This can be tricky, as there is no universal criterion for model selection, and the proper/optimal choice of the model structure is an open problem in machine learning.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS2.4.1.1" class="ltx_text">III-B</span>2 </span>Communication Overheads</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Clients upload their local model updates to the server for aggregation, and then download the updated global model for the next training rounds. This can require a substantial amount of communication resources, e.g., network bandwidths. To reduce the communication overheads, existing studies focus on the following two methods:</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Model compression</span>: It compresses the model data by techniques such as quantization, random rotation, and secondary sampling. This can reduce the communication overheads between the server and the FL clients.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Client selection</span>: It only allows the communication between the server and a selected set of clients. The selection criterion can be based on factors such as historical dropout rates, communication history, and model updates.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p"><span id="S3.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_italic">Remark</span>: The optimization of efficiency is a less major issue in cross-silo FL than in cross-device scenario. First, the cross-silo clients (e.g., companies or organizations) usually have ample computational resources.
Second, the cross-silo FL clients are expected to use reliable transmission networks (e.g., high-speed wired networks) instead of the mobile cellular networks used by most edge devices in cross-device FL.
</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Privacy and Security</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">For cross-silo FL to gain trust and be widely adopted in practice, its privacy and security implications must be well understood and taken care of. In this section, we overview the key existing approaches to privacy protection and security enhancement.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Privacy Protection</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Even if federated learning does not directly expose raw data, privacy concerns can arise due to various inference attacks. One type is <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">membership inference attack</span>, which aims to check whether certain data points belong to the training set. Canonically, the attacker infers the information via guesswork and training the predictive model to predict the original training data. Another type is <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">reconstruction attack</span>, which aims to reconstruct the training data using inference techniques. In particular, reconstruction attack uses GANs to synthesize fake samples that have the same distributions as the training set without accessing the raw data.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">To defend against these attacks, many researchers proposed privacy protection strategies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, which can be summarized into three categories below:</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Differential privacy</span>: This is a widely adopted privacy-preserving technique in both academia and industry. The basic idea is to add noise to personal sensitive attributes to protect privacy. In the context of federated learning, the clients add noise to their uploaded model updates to mitigate inference attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Note that using differential privacy improves clients’ privacy protection at the cost of global model performance (e.g., slower convergence and/or accuracy loss). Such a performance degradation may be a major concern in certain cross-silo settings where there is a strict requirement for model performance.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Homomorphic encryption</span>: Such an approach enables certain operations (e.g., addition) to be performed directly on cipher texts without decryption requirements. In cross-silo FL, each client uploads the encrypted local model updates to the server for aggregation, and the result is then sent back to each client for local decryption. Note that even if homomorphic encryption enhances privacy without hurting the global model performance, it incurs a large amount of computation and communication costs due to the calculation and transmission of the encrypted results. However, as we mentioned, organizations are likely to have sufficient computation and communication resources. Hence, cross-silo FL applications may prefer homomorphic encryption to differential privacy.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Multi-party computation</span>: This provides a generic
approach that enables cross-silo clients to jointly compute
an arbitrary functionality without revealing their raw data.
The aggregated model is calculated by exchanging these secret shares among clients, following well-designed protocols. Similar to homomorphic encryption, multi-party computation involves even more expensive cryptographic operations and hence causes larger communication and computation costs.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_italic">Remark</span>: Privacy protection is an even more critical issue in the cross-silo scenario than in the cross-device setting. On the one hand, a wide range of applications of cross-silo FL require proper treatments of human personal data. For example, hospitals can be highly sensitive to their patients’ medical records and banks must prevent their customers’ financial data from being leaked. On the other hand, existing cross-silo FL practices usually impose very strict requirements among organizations for data protection. This calls for the further enhancement of privacy in cross-silo FL and deserves sufficient attention. Furthermore, existing approaches cannot achieve a desirable tradeoff between model performance degradation (e.g., differential privacy) and computation/communication overheads (e.g., homomorphic encryption and multi-party computation). It would be important for researchers to develop more sophisticated approaches that can potentially achieve a better tradeoff.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Security Enhancement</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Security is another important challenge since federated learning can be vulnerable to adversarial attacks such as poisoning. To be more specific, a proportion of FL clients can be compromised, i.e., either controlled or even owned by some adversary, who acts maliciously to corrupt the training of global model. There are two major types of poisoning attacks. One is <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">data poisoning</span>, which aims to fabricate data so that local model updates are wrongly calculated. Another is <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">model poisoning</span>, which generates poisoned local updates (e.g., by direct manipulation of model updates) so that the collective model updates deviate from a benign direction.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">To enhance security in FL, numerous studies have proposed various <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">robust aggregation</span> schemes. The key idea is that when doing model aggregation, the central server removes or attenuates the model updates that are judged to be maliciously poisoned based on certain criterion (e.g., the updates that are highly dissimilar to others). See <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> for an excellent survey on robust aggregation.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_italic">Remark</span>: While the security issue exists in both cross-device and cross-silo settings, this can be a less major issue in cross-silo FL. This is because compared to edge devices, organizations are less likely to be compromised by adversaries. In fact, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> conducted comprehensive experiments and found that poisoning attacks have a minor impact on cross-silo FL.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Cooperation and Incentives</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In cross-silo FL, clients are organizations or companies who are likely to have (long-term) development goals and are strategic in maximizing their own benefits.
It is important to analyze the game theoretical interactions
among cross-silo clients and provide insights on how to promote cooperation.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_italic">Remark</span>: While many of the game-theoretical methods to be mentioned below apply to the cross-device setting, we believe that they are more suitable in cross-silo settings. On one hand, cross-device clients may not have many incentive concerns in certain applications. For example, the federated learning process for next word prediction is embedded in the mobile applications and automatically executed by Google. Hence, there is a small flexibility where mobile users can interact with the server on the incentive issues. On the other hand, cross-silo clients (e.g., organizations) usually have stronger computational resources and reasoning capabilities (than mobile devices in cross-device FL who tend to be short-sighted and boundedly rational), so that it is practical to analyze their interactions based on the full rationality assumption typically assumed in game theoretical analysis.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Data Valuation</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Data valuation (i.e., contribution evaluation) pertains to assessing each client’s contribution to cross-silo FL.
Data valuation was originally intended
to explain black-box predictions through the lens of data values. In the context of federated learning, data valuation is used to evaluate the contribution of each participating client, based on which one can design a good incentive mechanism to enhance inter-client cooperation.
</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The data valuation methods can be classified into two categories, i.e., quality-agnostic and quality-aware methods.</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Quality-agnostic methods</span>: Quality-agnostic methods measure each client’s contribution using metrics such as data sizes and computational resources. For example, <span id="S5.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">linearly proportional</span> is a quality-agnostic method that calculates a client’s contribution linearly proportional to the data size that the client uses for model training. It requires that the central server knows the clients’ used data size (e.g., FedAvg). In case such information is unknown, the server can design auction-based mechanisms to induce clients to truthfully report the information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Note that quality-agnostic methods are only suitable for the case where clients have i.i.d. data. For the non-i.i.d. case, it is inappropriate to quantify the value of data solely based on data sizes or computational resources. This limits the applicability of quality-agnostic methods.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Quality-aware methods</span>: Quality-aware methods take into account the data quality by retraining models using different combinations of clients’ data. Hence, the quality-aware methods are applicable to both the i.i.d. and non-i.i.d. scenarios. There are two types of quality-aware methods. One is <span id="S5.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">Leave-one-out</span>, which is a data-counterfactual method that measures a counterfactual of each client’ data and examines how much the global model performance changes due to the absence of that data. We will consider the resulting model performance change as the client contribution.
Another is <span id="S5.I1.i2.p1.1.3" class="ltx_text ltx_font_italic">Shapley-value based</span> methods.
It assigns a unique data valuation profile via
the average marginal model improvement made by each client. More specifically, it compares all the possible training data combinations that include data from one client versus combinations that exclude the data from that particular client. While the Shapley value method has a better empirical performance than leave-one-out, it suffers from exponentially expensive (in the number of participating clients) computation costs. Many empirical methods have been proposed based on Shapley value to reduce the computation costs, such as Monte-Carlo Shapley value and gradient-based Shapley value <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Profit Allocation</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Based on the data valuation results, we need to further determine a proper profit/benefit allocation mechanism to incentivize clients to contribute to cross-silo FL. Intuitively, clients who have higher contributions should be allocated more profits to ensure their willingness to cooperate. In the cross-device scenario, the profit allocation mechanism is usually decided by the central server. In cross-silo FL, however, the mechanism can be negotiated by the organizations themselves and enforced by binding agreements and contracts. There are two types of profit allocation mechanisms as follows:</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<ul id="S5.I2" class="ltx_itemize">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i1.p1" class="ltx_para">
<p id="S5.I2.i1.p1.1" class="ltx_p"><span id="S5.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Posted-price mechanisms</span>: A posted-price mechanism specifies how the price (reward) depends on the contribution of each client, and such a mechanism can be announced prior to model training. For example, a posted-price mechanism can offer rewards to a client based on the volume of its data points to be used for model training.</p>
</div>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i2.p1" class="ltx_para">
<p id="S5.I2.i2.p1.1" class="ltx_p"><span id="S5.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Profit-sharing mechanisms</span>: A profit-sharing mechanism allocates the total profit/benefit to all the participating clients when they finish the FL task. More specifically, after the cross-silo FL terminates, the clients can utilize a trusted third party to obtain some benefits from the global model (e.g., via selling it in a model trading market). Then, the clients share the profits among themselves using the predefined mechanisms.</p>
</div>
</li>
</ul>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">While existing profit allocation mechanisms mainly focus on enhancing inter-client cooperation (e.g., improving global model performance), they can cause <span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">fairness issues</span>, which deserve special attention in cross-silo FL.
There are two facets of fairness.</p>
<ul id="S5.I3" class="ltx_itemize">
<li id="S5.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I3.i1.p1" class="ltx_para">
<p id="S5.I3.i1.p1.1" class="ltx_p"><span id="S5.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">Inter-client fairness</span>: One is the fairness among cross-silo clients. For example, different organizations or companies can differ substantially in training-related resources (e.g., data volume and computation capabilities) and market powers. In this case, the “stronger” organizations may reap most of the benefits (e.g., due to a larger data volume), discouraging clients to contribute in FL. Some research efforts have been devoted to enhancing fairness.
Examples include collaborative fairness methods, which balance the shared profit based on the contribution to the model accuracy, and egalitarian equity methods that shift more focus to client with the worst performance.</p>
</div>
</li>
<li id="S5.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I3.i2.p1" class="ltx_para">
<p id="S5.I3.i2.p1.1" class="ltx_p"><span id="S5.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">Model fairness</span>: Another facet is on training a fair global model. For instance, hospitals strive to train fair models with medical data collected from geographically varying populations, so that the global model can have a minimum bias toward patients. We believe that it is important to understand how the cross-silo clients’ training behaviors affect the fairness of the global model and more research efforts should be devoted along this line.</p>
</div>
</li>
</ul>
<p id="S5.SS2.p3.2" class="ltx_p">Another important challenge is that even though fairness in FL has been extensively studied, the definitions of fairness substantially vary in literature. Earlier work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> established a unified framework on fair resource allocation, and it is important to borrow their techniques on the unified definition of fairness and derive potential solutions in cross-silo settings.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Coalition</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">While prior work focuses on the design and optimization of data valuation and profit allocation, it is also important to explore the motivation and incentives regarding how clients form coalitions. In practical cross-silo FL, the organizations may be interested in forming coalitions due to various reasons as follows.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">First, the cross-silo clients usually have heterogeneous and different amounts of data drawn from their own distributions. Consider the case where a group of hospitals collaboratively train a model. Different hospitals are likely to hold heterogeneous data due to, for example, the varying patient populations or variants of the procedure implementation. While the use of federated learning decreases the global model’s error due to model variance, the data heterogeneity issue would give rise to a larger model bias. Therefore, some cross-silo clients may have an incentive to form coalitions to jointly construct models (e.g., via sharing local model parameters) such that the eventual global model would be desirable for clients within the coalition. The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> proposed the first game-theoretical framework to analyze how heterogeneous clients optimally form coalitions. While coalition in cross-silo FL receives little attention so far, we believe that more research efforts should be devoted, e.g., to characterize the stability of the coalitions using the concept of core.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">Second, cross-silo clients may have incentives to form coalitions when the market is relatively competitive. Consider that there are numerous banks in the market aiming to provide finance-related services to the potential customers. The banks with limited data may form a coalition to collaboratively train a model. This can help the banks obtain a better model and improve their service quality. As a result, the small banks become more competitive (compared to other bigger banks) and are likely to gain more market power. To the best of our knowledge, there is no prior work along this line and more research attention should be given.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Future Directions and Open Issues</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The research on cross-silo federated learning is still at its early stage, and there are many interesting open problems to address. We list some of them here.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS1.4.1.1" class="ltx_text">VI-A</span> </span><span id="S6.SS1.5.2" class="ltx_text ltx_font_italic">Multi-Objective Optimization</span>
</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">Current research studies mainly proceed with the perspective of single-objective optimization, e.g., optimizing efficiency/effectiveness, improving privacy/security, or enhancing cooperation, without taking a unified and multi-objective angle. In practice, however, cross-silo FL would encounter multiple challenges at the same time and hence need a more comprehensive approach. For example, differential privacy is a powerful framework to improve privacy, but it jeopardizes the model performance (i.e., effectiveness) due to noise injection. It would be important to propose a multi-objective optimization framework to understand the interplay among various objectives and derive solutions that give the best tradeoffs.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS2.4.1.1" class="ltx_text">VI-B</span> </span><span id="S6.SS2.5.2" class="ltx_text ltx_font_italic">Long-Term Cooperation</span>
</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">In cross-device FL, clients are mobile edge devices and hence unlikely to form long-term cooperation relations. There are a few reasons. First, edge devices are relatively unstable and they could easily drop out of the training process if the batteries die. Second, the central server which is also the initiator of FL (e.g., Google) usually would dynamically sample part of clients for model training. Third, it is difficult for mobile devices to even identify their cooperating partners due to a vast number of participating clients.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">Different from cross-device FL, cross-silo FL provides a hotbed for long-term cooperation. On the one hand, the organizations are relatively stable since they have strong computation and communication resources. On the other hand, the clients themselves initiate the training process, know who their partners are, and hence are likely to stay with the whole training process due to binding agreements/contracts.
A practice example is that WeBank and Swiss Re established a long-term cooperation project in the field of reinsurance. For future work, it would be intriguing to study the strategic behaviors among the cross-silo clients and propose (game-theoretical) mechanisms to promote their long-term cooperation.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS3.4.1.1" class="ltx_text">VI-C</span> </span><span id="S6.SS3.5.2" class="ltx_text ltx_font_italic">Business Competition</span>
</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">Besides cooperation, the cross-silo clients may become business competitors.
In the domain of finance and insurance, different banks or companies may also strive to induce customers to use their services rather than their competitors. It would be interesting to study how the business competition problem affects the clients’ willingness to cooperate and devote resources for model training. If competition is found to have a hindering impact on the cooperation, then proper mechanisms are needed to mitigate competition and promote competition, which ensures the success of cross-silo FL.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learning: Concept
and applications,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and
Technology (TIST)</em>, vol. 10, no. 2, pp. 1–19, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
P. Kairouz <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Advances and open problems in federated learning,”
<em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">Foundations and Trends® in Machine Learning</em>, vol. 14,
no. 1–2, pp. 1–210, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
L. Li, Y. Fan, M. Tse, and K.-Y. Lin, “A review of applications in federated
learning,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Computers &amp; Industrial Engineering</em>, vol. 149, p. 106854,
2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
K. Wei, J. Li, C. Ma, M. Ding, S. Wei, F. Wu, G. Chen, and T. Ranbaduge,
“Vertical federated learning: Challenges, methodologies and experiments,”
<em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.04309</em>, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
H. Zhu, J. Xu, S. Liu, and Y. Jin, “Federated learning on non-iid data: A
survey,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, vol. 465, pp. 371–390, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
X. Ouyang, Z. Xie, J. Zhou, J. Huang, and G. Xing, “Clusterfl: a
similarity-aware federated learning system for human activity recognition,”
in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 19th Annual International Conference on Mobile
Systems, Applications, and Services</em>, 2021, pp. 54–66.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning:
Challenges, methods, and future directions,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing
Magazine</em>, vol. 37, no. 3, pp. 50–60, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
V. Mothukuri, R. M. Parizi, S. Pouriyeh, Y. Huang, A. Dehghantanha, and
G. Srivastava, “A survey on security and privacy of federated learning,”
<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Future Generation Computer Systems</em>, vol. 115, pp. 619–640, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
P. Sun, X. Chen, G. Liao, and J. Huang, “A profit-maximizing model marketplace
with differentially private federated learning,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proc. of IEEE
INFOCOM</em>, 2022, pp. 1439–1448.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
K. Pillutla, S. M. Kakade, and Z. Harchaoui, “Robust aggregation for federated
learning,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.13445</em>, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
V. Shejwalkar, A. Houmansadr, P. Kairouz, and D. Ramage, “Back to the drawing
board: A critical evaluation of poisoning attacks on production federated
learning,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE Symposium on Security and Privacy</em>, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y. Sarikaya and O. Ercetin, “Motivating workers in federated learning: A
stackelberg game perspective,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Networking Letters</em>, vol. 2,
no. 1, pp. 23–27, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Ghorbani and J. Zou, “Data shapley: Equitable valuation of data for machine
learning,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2019, pp. 2242–2251.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
C. Joe-Wong, S. Sen, T. Lan, and M. Chiang, “Multiresource allocation:
Fairness–efficiency tradeoffs in a unifying framework,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM
Transactions on Networking</em>, vol. 21, no. 6, pp. 1785–1798, 2013.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
K. Donahue and J. Kleinberg, “Model-sharing games: Analyzing federated
learning under voluntary participation,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI
Conference on Artificial Intelligence</em>, vol. 35, no. 6, pp. 5303–5311, May
2021.

</span>
</li>
</ul>
</section>
<figure id="id1" class="ltx_float biography">
<table id="id1.1" class="ltx_tabular">
<tr id="id1.1.1" class="ltx_tr">
<td id="id1.1.1.1" class="ltx_td"><img src="/html/2206.12949/assets/figure/Chao-photo.jpg" id="id1.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="84" height="125" alt="[Uncaptioned image]"></td>
<td id="id1.1.1.2" class="ltx_td">
<span id="id1.1.1.2.1" class="ltx_inline-block">
<span id="id1.1.1.2.1.1" class="ltx_p"><span id="id1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Chao Huang</span>  received the Ph.D. degree from the Chinese University of Hong Kong in 2021. During July-November 2021, he was a Post-Doctoral Research Fellow with the Department of Management Sciences, City University of Hong Kong. He is now working as a Post-Doctoral Researcher with the Department of Computer Science, University of California, Davis. His recent research interests span the spectrum of distributed machine learning, network economics, and low-carbon systems.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id2" class="ltx_float biography">
<table id="id2.1" class="ltx_tabular">
<tr id="id2.1.1" class="ltx_tr">
<td id="id2.1.1.1" class="ltx_td"><img src="/html/2206.12949/assets/figure/Huang-bio.jpg" id="id2.1.1.1.g1" class="ltx_graphics ltx_img_square" width="105" height="105" alt="[Uncaptioned image]"></td>
<td id="id2.1.1.2" class="ltx_td">
<span id="id2.1.1.2.1" class="ltx_inline-block">
<span id="id2.1.1.2.1.1" class="ltx_p"><span id="id2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Jianwei Huang (F’16)</span>  is a Presidential Chair Professor and Associate Dean of School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen. His research interests are in the area of network optimization, network economics, and network games. He has published more than 300 papers in leading venues, with a Google Scholar citation of 14600+ and an H-index of 61. He has co-authored 10 Best Paper Awards, including the 2011 IEEE Marconi Prize Paper Award in Wireless Communications. He has co-authored seven books, including the textbook on ”Wireless Network Pricing.” He is an IEEE Fellow, and was an IEEE ComSoc Distinguished Lecturer and a Clarivate Web of Science Highly Cited Researcher. He is the Editor-in-Chief of IEEE Transactions on Network Science and Engineering, and was the Associate Editor-in-Chief of IEEE Open Journal of the Communications Society.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id3" class="ltx_float biography">
<table id="id3.1" class="ltx_tabular">
<tr id="id3.1.1" class="ltx_tr">
<td id="id3.1.1.1" class="ltx_td"><img src="/html/2206.12949/assets/figure/Liu-photo.png" id="id3.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="112" alt="[Uncaptioned image]"></td>
<td id="id3.1.1.2" class="ltx_td">
<span id="id3.1.1.2.1" class="ltx_inline-block">
<span id="id3.1.1.2.1.1" class="ltx_p"><span id="id3.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Xin Liu</span>  received her Ph.D. degree in electrical engineering from Purdue University in 2002. She is currently a Professor in Computer Science at the University of California, Davis. Her current research interests fall in the general areas of machine learning algorithm development and machine learning applications in human and animal healthcare, food systems, and communication networks. Her research on networking includes cellular networks, cognitive radio networks, wireless sensor networks, network information theory, network security, and IoT systems.</span>
</span>
</td>
</tr>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2206.12948" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2206.12949" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2206.12949">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2206.12949" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2206.12950" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 16:24:55 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
