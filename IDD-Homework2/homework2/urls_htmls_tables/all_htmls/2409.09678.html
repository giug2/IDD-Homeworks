<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.09678] A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities</title><meta property="og:description" content="Human Activity Recognition (HAR) systems aim to understand human behaviour and assign a label to each action, attracting significant attention in computer vision due to their wide range of applications. HAR can leverag…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.09678">

<!--Generated on Sun Oct  6 00:48:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">[1]<span id="p1.1.1" class="ltx_ERROR undefined">\fnm</span>Jungpil <span id="p1.1.2" class="ltx_ERROR undefined">\sur</span>Shin
<span id="p1.1.3" class="ltx_ERROR undefined">\equalcont</span>These authors contributed equally to this work.

<span id="p1.1.4" class="ltx_ERROR undefined">\equalcont</span>These authors contributed equally to this work.

<span id="p1.1.5" class="ltx_ERROR undefined">\equalcont</span>These authors contributed equally to this work.

<span id="p1.1.6" class="ltx_ERROR undefined">\equalcont</span>These authors contributed equally to this work.</p>
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p">[1]<span id="p2.1.1" class="ltx_ERROR undefined">\orgdiv</span>School of Computer Science and Engineering, <span id="p2.1.2" class="ltx_ERROR undefined">\orgname</span>The University of Aizu, <span id="p2.1.3" class="ltx_ERROR undefined">\orgaddress</span><span id="p2.1.4" class="ltx_ERROR undefined">\city</span>Aizuwakamatsu, <span id="p2.1.5" class="ltx_ERROR undefined">\state</span>Fukushima, <span id="p2.1.6" class="ltx_ERROR undefined">\country</span>Japan</p>
</div>
<h1 class="ltx_title ltx_title_document">A Comprehensive Methodological Survey of Human
Activity Recognition Across Divers Data Modalities</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:jpshin@u-aizu.ac.jp">jpshin@u-aizu.ac.jp</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id1.1.id1" class="ltx_ERROR undefined">\fnm</span>Najmul <span id="id2.2.id2" class="ltx_ERROR undefined">\sur</span>Hassan
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id3.1.id1" class="ltx_ERROR undefined">\fnm</span>Abu Saleh Musa <span id="id4.2.id2" class="ltx_ERROR undefined">\sur</span>Miah
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id5.1.id1" class="ltx_ERROR undefined">\fnm</span>Satoshi  <span id="id6.2.id2" class="ltx_ERROR undefined">\sur</span>Nishimura
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">*
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">Human Activity Recognition (HAR) systems aim to understand human behaviour and assign a label to each action, attracting significant attention in computer vision due to their wide range of applications. HAR can leverage various data modalities, such as RGB images and video, skeleton, depth, infrared, point cloud, event stream, audio, acceleration, and radar signals. Each modality provides unique and complementary information suited to different application scenarios. Consequently, numerous studies have investigated diverse approaches for HAR using these modalities. This paper presents a comprehensive survey of the latest advancements in HAR from 2014 to 2024, focusing on machine learning (ML) and deep learning (DL) approaches categorized by input data modalities. We review both single-modality and multi-modality techniques, highlighting fusion-based and co-learning frameworks. Additionally, we cover advancements in hand-crafted action features, methods for recognizing human-object interactions, and activity detection. Our survey includes a detailed dataset description for each modality and a summary of the latest HAR systems, offering comparative results on benchmark datasets. Finally, we provide insightful observations and propose effective future research directions in HAR.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Human activity recognition (HAR), Diverse modality, Deep learning (DL), Machine learning (ML), Vision and Sensor Based HAR, Classification.
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Human action recognition (HAR) has been a very active research topic for the past two decades in the field of computer vision and artificial intelligence (AI). That focuses on the automated analysis and understanding of human actions and recognition based on the movements and poses of the entire body. HAR plays an important role in various applications such as surveillance; healthcare <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, remote monitoring, intelligent human-machine interfaces, entertainment, storage video, and retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> human-computer interaction<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
<br class="ltx_break">However, monitoring in 24 hours for security purposes makes it difficult to detect HAR. HAR is very important in computer vision and covers many research topics, including HAR in video, human tracking, and analysis and understanding in videos captured with a moving camera, where motion patterns exist due to video objects and moving camera as well <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. In such a scenario, it becomes ambiguous to recognize objects. The HAR methods were categorized into three distinct tiers: human action detection, human action tracking, and behaviour understanding methods.
In recent years, the investigation of interaction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and human action detection<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> has emerged as a prominent area of research.
Many state-of-the-art techniques deal with action recognition using action frames as images and are only able to detect the presence of an object in it. They cannot properly recognize the object in an image or video. By properly recognizing an action in a video, it is possible to recognize the class of action more accurately. To perform action recognition, there has been an increased interest in this field in recent years due to the increased availability of computing resources as well as new advances in ML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and DL.
Robust human action modelling and feature representation are essential components for achieving effective HAR. The main issue of representing and selecting features is a well-established problem within the fields of computer vision and ML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
Unlike the representation of features in an image domain, the representation of features of human actions in a video not only depicts the visual attributes of the human being(s) within the image domain but also must the extraction of alterations in visual attributes and pose. The problem representation of features has been expanded from a 2D space to a 3D spatio-temporal context. In the past few years, many types of action representation techniques have been proposed. These techniques include various approaches, such as local and global features that rely on temporal and spatial alterations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, trajectory features that are based on key point tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, motion changes that are derived from depth information<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and action features that are derived from human pose changes<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
With the performance and successful application of DL to activity recognition and classification, many researchers have used DL for HAR. This facilitates the automatically learned features from the video data set<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. However, the aforementioned review articles have only examined certain specific facets, such as the spatial, temporal interesting point (STIP) and HOF-found techniques for HAR, as well as the approaches for analyzing human walking and DL-based techniques. Numerous novel approaches have been recently developed, primarily about the utilization of depth learning techniques for feature learning. Hence, a comprehensive examination of these fresh approaches for recognizing human actions is of significant interest.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Article Search and Survey Methodology</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">The first step in conducting a comprehensive literature review involves gathering all relevant documents from 2014 to 2024 for Human Activity Recognition (HAR). This entails a meticulous screening process, including downloading and scrutinizing materials related to science, technology, or computer science. Publications are broadly categorized into journals, proceedings, book chapters, and lecture notes, focusing on articles presenting in-depth analysis and commentary.
Initially, articles were collected using relevant keywords such as:</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Human Action Recognition, Human Activity Recognition (HAR)</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Action features including RGB, Skeleton, Sensor, Multimodality datasets</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">ML and DL-based HAR</p>
</div>
</li>
</ul>
<p id="S1.SS1.p2.1" class="ltx_p">Subsequently, additional pertinent studies were incorporated after the initial selection of literature. Finally, supplementary investigations derived from the action recognition multimodal dataset were included to finalize this study.
In our investigation, most of the literature was collected from scholarly periodicals, journals, and conferences on computer vision. We prioritized articles published in prestigious journals and conferences such as:</p>
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p">IEEE Transactions on Image Processing (TIP)</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p">International Conference on Computer Vision and Pattern Recognition (CVPR)</p>
</div>
</li>
<li id="S1.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i4.p1" class="ltx_para">
<p id="S1.I2.i4.p1.1" class="ltx_p">IEEE International Conference on Computer Vision (ICCV)</p>
</div>
</li>
<li id="S1.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i5.p1" class="ltx_para">
<p id="S1.I2.i5.p1.1" class="ltx_p">Springer, ELSEVIER, MDPI, Frontier, etc.</p>
</div>
</li>
</ul>
<p id="S1.SS1.p2.2" class="ltx_p">Simultaneously, to ensure our paper includes comprehensive methodologies, we selectively adopted a fundamental or exemplar approach when discussing similar methods in detail.</p>
</div>
<section id="S1.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.1.1 </span>Inclusion and Exclusion Criteria</h4>

<div id="S1.SS1.SSS1.p1" class="ltx_para">
<p id="S1.SS1.SSS1.p1.1" class="ltx_p">To refine and ensure relevance in our initial search results, we applied the following criteria:
<br class="ltx_break"><span id="S1.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_bold">Inclusion Criteria:</span></p>
<ul id="S1.I3" class="ltx_itemize">
<li id="S1.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I3.i1.p1" class="ltx_para">
<p id="S1.I3.i1.p1.1" class="ltx_p">Publication date between 2014 and 2024;</p>
</div>
</li>
<li id="S1.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I3.i2.p1" class="ltx_para">
<p id="S1.I3.i2.p1.1" class="ltx_p">Inclusion of journals, proceedings, book chapters, and lecture notes;</p>
</div>
</li>
<li id="S1.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I3.i3.p1" class="ltx_para">
<p id="S1.I3.i3.p1.1" class="ltx_p">Focus on RGB-based, skeleton-based, sensor-based, and fusion HAR methods;</p>
</div>
</li>
<li id="S1.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I3.i4.p1" class="ltx_para">
<p id="S1.I3.i4.p1.1" class="ltx_p">Emphasis on the evolution of data acquisition, environments, and human activity portrayals.</p>
</div>
</li>
</ul>
<p id="S1.SS1.SSS1.p1.2" class="ltx_p"><span id="S1.SS1.SSS1.p1.2.1" class="ltx_text ltx_font_bold">Exclusion Criteria:</span></p>
<ul id="S1.I4" class="ltx_itemize">
<li id="S1.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I4.i1.p1" class="ltx_para">
<p id="S1.I4.i1.p1.1" class="ltx_p">Exclusion of studies lacking in-depth information about their experimental procedures;</p>
</div>
</li>
<li id="S1.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I4.i2.p1" class="ltx_para">
<p id="S1.I4.i2.p1.1" class="ltx_p">Exclusion of research articles where the complete text isn’t accessible, both in physical and digital formats;</p>
</div>
</li>
<li id="S1.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I4.i3.p1" class="ltx_para">
<p id="S1.I4.i3.p1.1" class="ltx_p">Exclusion of research articles that include opinions, keynote speeches, discussions, editorials, tutorials, remarks, introductions, viewpoints, and slide presentations.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S1.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.1.2 </span>Article Selection</h4>

<div id="S1.SS1.SSS2.p1" class="ltx_para">
<p id="S1.SS1.SSS2.p1.1" class="ltx_p">We conducted a thorough survey of HAR methods, focusing on the evolution of data acquisition, environments, and human activity portrayals from 2014 to 2024. The preference is given to articles published in prestigious journals and conferences.
Figure <a href="#S1.F1" title="Figure 1 ‣ 1.1.3 Keywords and Search Strategy ‣ 1.1 Article Search and Survey Methodology ‣ 1 Introduction ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> depicts the article selection process, illustrating the systematic approach adopted. Figure <a href="#S1.F2" title="Figure 2 ‣ 1.1.3 Keywords and Search Strategy ‣ 1.1 Article Search and Survey Methodology ‣ 1 Introduction ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> demonstrates the percentage of the journal, conference, and other ratios.
Figure <a href="#S1.F3" title="Figure 3 ‣ 1.1.3 Keywords and Search Strategy ‣ 1.1 Article Search and Survey Methodology ‣ 1 Introduction ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the year-wise number of references.</p>
</div>
</section>
<section id="S1.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.1.3 </span>Keywords and Search Strategy</h4>

<div id="S1.SS1.SSS3.p1" class="ltx_para">
<p id="S1.SS1.SSS3.p1.1" class="ltx_p">Two primary keywords, “HAR” and “computer vision,” determine the study’s focal point. These keywords, supplemented by additional relevant terms, form the backbone of our search strategy across various databases and resources. Various materials, including original articles, review articles, book chapters, conference papers, and lecture notes, were gathered to review the subject matter comprehensively.
We reviewed each article through a structured process involving:</p>
</div>
<div id="S1.SS1.SSS3.p2" class="ltx_para">
<ul id="S1.I5" class="ltx_itemize">
<li id="S1.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I5.i1.p1" class="ltx_para">
<p id="S1.I5.i1.p1.1" class="ltx_p">Abstract review</p>
</div>
</li>
<li id="S1.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I5.i2.p1" class="ltx_para">
<p id="S1.I5.i2.p1.1" class="ltx_p">Methodology analysis</p>
</div>
</li>
<li id="S1.I5.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I5.i3.p1" class="ltx_para">
<p id="S1.I5.i3.p1.1" class="ltx_p">Discussion</p>
</div>
</li>
<li id="S1.I5.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I5.i4.p1" class="ltx_para">
<p id="S1.I5.i4.p1.1" class="ltx_p">Result evaluations</p>
</div>
</li>
</ul>
<p id="S1.SS1.SSS3.p2.1" class="ltx_p">Different modalities used in HAR have unique features, each with advantages and disadvantages in various tables. This approach ensures a thorough and systematic review of the HAR literature, providing a solid foundation for understanding the advancements and trends in this field.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.09678/assets/Article_Selection_Procedure.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1033" height="289" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Article selection process block diagram.</figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2409.09678/assets/Precent_vise_papers.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1008" height="543" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Article types journal conferences and others.</figcaption>
</figure>
<figure id="S1.F3" class="ltx_figure"><img src="/html/2409.09678/assets/Years_vise_paper.png" id="S1.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="976" height="600" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Yearwise papers gathering in this study.</figcaption>
</figure>
</section>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Motivation</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">Many researchers have been working to develop a HAR system using various technologies, including ML and DL techniques with diverse feature extraction techniques. Herath et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> reported the classification techniques specific to HAR, disregarding an inquiry into the methods of interaction recognition and detecting actions. It is worth noting that in a recent study, Yu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> performed a comprehensive analysis of the existing literature on the topic of action recognition and action prediction. In addition, the literature was summarized by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> within the framework of three key areas: sensor modality, deep models, and application.
Guo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> analyzed methods employed in human HAR with still images, exploring various ML and DL-based approaches for extracting low-level features and representing actions at higher levels. Vrigkas et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> also reviewed HAR using RGB static images, covering both single-mode and multi-mode approaches. Vishwakarma et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> summarized classical HAR methods, categorizing them into hierarchical and non-hierarchical methods based on feature representation. The survey by Ke et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> provided a comprehensive overview of handcrafted methods in HAR. Additionally, surveys <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> extensively discuss the strengths and weaknesses of handcrafted versus DL methods, emphasizing the advantages of DL-based approaches. Xing et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> focused on HAR development using 3D skeleton data, reviewing various DL-based techniques and comparing their performance across different dimensions.
Presti et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> presented HAR techniques based on 3D skeleton data. Methods for HAR using depth and skeleton data have been thoroughly reviewed by Ye et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>; they also present HAR techniques using depth data. 
<br class="ltx_break">Although certain review articles discuss data fusion methods, they offer a limited overview of HAR approaches to particular data types. Similarly, Subetha et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> presented the same strategy to review action recognition methods. However, in distinction to those studies, we categorize HAR into four distinct categories: action recognition RGB and handcrafted features, action recognition RGB and DL, action recognition skeleton and handcrafted features, action recognition skeleton-based and DL, and action recognition using multimodal dataset. The crucial element of the analysis regarding the literature on HAR is that most surveys have focused on the representations of human action features. The data of the image sequences that have been processed are typically well-segmented and consist solely of a single action event.
More recently, many researchers have been working to make an HAR survey study with some specific point of view. Such as some researchers surveyed graph convolutional network (GCN) structures
and data modalities for HAR and the application of GCNs in HAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>,
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.
Gupta et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> explored current and future directions in skeleton-based HAR and introduced the skeleton-152 dataset, marking a significant advancement in the field. Meanwhile, Song et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> reviewed advancements in human pose estimation and its applications in HAR, emphasizing its importance. Additionally, Shaikh et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> focused on data integration and recognition approaches within a visual framework, specifically from an RGB-D perspective. Majumder et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> provided reviews of popular methods using vision and inertial sensors for HAR. More recently, want et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> survey HAR by performing two modalities of RGB-based and skeleton-based HAR techniques. Similarly, Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> survey HAR with various multi-modality methods.</p>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Research Gaps and New Research Challenges</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.1" class="ltx_p">Also, each survey paper can give us an overall summary of the existing work in this domain. Still, it lacks comparative studies of the RGB, Skeleton, sensor, and fusion-based diverse modality-based HAR system of the recent technologies.
From a data perspective, most reviews on HAR are limited to methodologies based on specific data, such as RGB, depth, and fusion data modalities. Moreover, we did not find a HAR survey paper that included diverse modality-based HAR, including their benchmark dataset and latest performance accuracy for 2014-2024. The study inspires us <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> to complete a survey study with current research trends for HAR.</p>
</div>
</section>
<section id="S1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.4 </span>Our Contribution</h3>

<div id="S1.SS4.p1" class="ltx_para">
<p id="S1.SS4.p1.1" class="ltx_p">Figure <a href="#S1.F4" title="Figure 4 ‣ 1.5 Research Questions ‣ 1 Introduction ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> demonstrates the proposed methodology flowchart. In this study, we survey state-of-the-art methods for HAR, addressing their challenges and future directions across vision, sensor, and fusion-based data modalities. We also summarize the current 2 dimensions and 3 dimensions pose estimation algorithms before discussing skeleton-based feature representation methods. Additionally, we categorize action recognition techniques into handcrafted feature-based ML and end-to-end DL-based methods. Our main contributions are as follows:</p>
</div>
<div id="S1.SS4.p2" class="ltx_para">
<ul id="S1.I6" class="ltx_itemize">
<li id="S1.I6.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I6.i1.p1" class="ltx_para">
<p id="S1.I6.i1.p1.1" class="ltx_p"><span id="S1.I6.i1.p1.1.1" class="ltx_text ltx_font_bold">Comprehensive Review with Diverse Modality</span>: We conduct a thorough survey of RGB-based, skeleton-based, sensor-based, and fusion HAR-based methods, focusing on the evolution of data acquisition, environments, and human activity portrayals from 2014 to 2024.</p>
</div>
</li>
<li id="S1.I6.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I6.i2.p1" class="ltx_para">
<p id="S1.I6.i2.p1.1" class="ltx_p"><span id="S1.I6.i2.p1.1.1" class="ltx_text ltx_font_bold">Dataset Description</span>: We provide a detailed overview of benchmark public datasets for RGB, skeleton, sensor, and fusion data, highlighting their latest performance accuracy with reference. </p>
</div>
</li>
<li id="S1.I6.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I6.i3.p1" class="ltx_para">
<p id="S1.I6.i3.p1.1" class="ltx_p"><span id="S1.I6.i3.p1.1.1" class="ltx_text ltx_font_bold">Unique Process</span>: Our study covers feature representation methods, common datasets, challenges, and future directions, emphasizing the extraction of distinguishable action features from video data despite environmental and hardware limitations.</p>
</div>
</li>
<li id="S1.I6.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I6.i4.p1" class="ltx_para">
<p id="S1.I6.i4.p1.1" class="ltx_p"><span id="S1.I6.i4.p1.1.1" class="ltx_text ltx_font_bold">Identification of Gaps and Future Directions</span>: We identify significant gaps in current research and propose future research directions supported by the latest performance data for each modality.</p>
</div>
</li>
<li id="S1.I6.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I6.i5.p1" class="ltx_para">
<p id="S1.I6.i5.p1.1" class="ltx_p"><span id="S1.I6.i5.p1.1.1" class="ltx_text ltx_font_bold">Evaluation of System Efficacy</span>: We assess existing HAR systems by analyzing their recognition accuracy and providing benchmark datasets for future development.</p>
</div>
</li>
<li id="S1.I6.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I6.i6.p1" class="ltx_para">
<p id="S1.I6.i6.p1.1" class="ltx_p"><span id="S1.I6.i6.p1.1.1" class="ltx_text ltx_font_bold">Guidance for Practitioners</span>: Our review offers practical guidance for developing robust and accurate HAR systems, providing insights into current techniques, highlighting challenges, and suggesting future research directions to advance HAR system development.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S1.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.5 </span>Research Questions</h3>

<div id="S1.SS5.p1" class="ltx_para">
<p id="S1.SS5.p1.1" class="ltx_p">This research addresses the following major questions:
1. What are the main difficulties faced in Human activity recognition? 
<br class="ltx_break">2. What are some challenges faced with Human activity recognition? 
<br class="ltx_break">3. What are the major algorithms involved in Human activity recognition? 
<br class="ltx_break"></p>
</div>
<figure id="S1.F4" class="ltx_figure"><img src="/html/2409.09678/assets/x1.jpg" id="S1.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="488" height="286" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The structure of this paper.</figcaption>
</figure>
</section>
<section id="S1.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.6 </span>Organization of the Work</h3>

<div id="S1.SS6.p1" class="ltx_para">
<p id="S1.SS6.p1.1" class="ltx_p">The paper is categorized as follows. The benchmark datasets are provided in section <a href="#S2.SS1" title="2.1 RGB-Based Datasets of HAR ‣ 2 RGB-Data Modality Based Action Recognition Methods ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>. The action recognition RGB-data modality methods and skeleton data modality-based are discussed in sections <a href="#S2" title="2 RGB-Data Modality Based Action Recognition Methods ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S3" title="3 Skeleton Data Modality Based Action Recognition Method ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, respectively. In sections <a href="#S4" title="4 Sensor Based HAR ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, <a href="#S5" title="5 Multimodal Fusion Modality Based Action Recognition ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, and <a href="#S6" title="6 Current Challenges ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we introduce sensor modality-based human activity recognition, multimodal fusion modality-based, and current challenges, including four data modalities, respectively. We discuss future research trends and direction in sections <a href="#S7" title="7 Discussion ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Finally, in the last section <a href="#S8" title="8 Conclusion ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we summarized the conclusions. The detailed structure of this paper is shown in Figure <a href="#S1.F4" title="Figure 4 ‣ 1.5 Research Questions ‣ 1 Introduction ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>RGB-Data Modality Based Action Recognition Methods</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Figure <a href="#S2.F5" title="Figure 5 ‣ 2 RGB-Data Modality Based Action Recognition Methods ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> demonstrated a common workflow diagram of the RGB-based action recognition methods. The early stages of research about the HAR were conducted based on the RGB data, and initially, feature extraction mostly depended on manual annotation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. These annotations often relied on existing knowledge and prior assumptions. After this, DL-based architectures were developed to extract the most effective features and the best performances. The following sections describe the dataset, the methodological review of RGB-based handcrafted features with ML, and various ideas for DL-based approaches. Moreover, Table <a href="#S2.T3" title="Table 3 ‣ 2.3.2 Multi Stream Based Network ‣ 2.3 End-to-End Deep Learning Approach ‣ 2 RGB-Data Modality Based Action Recognition Methods ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> lists detailed information about the RGB data modality, including the datasets, features extraction methods, classifier, years, and performance accuracy.</p>
</div>
<figure id="S2.F5" class="ltx_figure"><img src="/html/2409.09678/assets/RGB_BASED_HCF.png" id="S2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="677" height="64" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Action recognition RGB data and handcrafted features.</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>RGB-Based Datasets of HAR</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">We provided the most popular benchmark HAR datasets, which come from the RGB skeleton, which is demonstrated in Table <a href="#S2.T1" title="Table 1 ‣ 2.1 RGB-Based Datasets of HAR ‣ 2 RGB-Data Modality Based Action Recognition Methods ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The dataset table demonstrated the details of the datasets, including modalities, creation year, number of classes, number of subjects who participated in recording the dataset, number of samples, and latest performance accuracy of the dataset with citation.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The RGB dataset encompasses several prominent benchmarks for Human Activity Recognition (HAR). Notably, the Activity Net dataset, introduced in 2015, comprises 203 activity classes and an extensive 27,208 samples, achieving an impressive accuracy of 94.7% in recent evaluations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. The Kinetics-400 and Kinetics-700 datasets, from 2017 and 2019 respectively, include 400 and 700 classes with approximately 306,245 and 650,317 samples. These datasets are notable for their high accuracy rates of 92.1% and 85.9% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. The AVA dataset, also from 2017, contains 80 classes and 437 samples, with a recorded accuracy of 83.0% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. The EPIC Kitchen 55 dataset from 2018 offers a comprehensive view with 149 classes and 39,596 samples. The Moments in Time dataset, released in 2019, is one of the largest with 339 classes and around 1,000,000 samples, although it has a relatively lower accuracy of 51.2% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. Each dataset is instrumental for training and evaluating HAR models, providing diverse scenarios and activities.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Benchmark datasets for HAR RGB and Skeleton.</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Data set modalities</span></td>
<td id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Year</span></td>
<td id="S2.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Class</span></td>
<td id="S2.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">Subject</span></td>
<td id="S2.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.6.1" class="ltx_text ltx_font_bold">Sample</span></td>
<td id="S2.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.7.1" class="ltx_text ltx_font_bold">Latest Accuracy</span></td>
</tr>
<tr id="S2.T1.1.2.2" class="ltx_tr">
<td id="S2.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.2.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.2.2.1.1.1" class="ltx_tr">
<td id="S2.T1.1.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">UPCV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Skeleton</td>
<td id="S2.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2014</td>
<td id="S2.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10</td>
<td id="S2.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20</td>
<td id="S2.T1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">400</td>
<td id="S2.T1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.20% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.3.3" class="ltx_tr">
<td id="S2.T1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.3.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.3.3.1.1.1" class="ltx_tr">
<td id="S2.T1.1.3.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Activity Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB</td>
<td id="S2.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2015</td>
<td id="S2.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">203</td>
<td id="S2.T1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27208</td>
<td id="S2.T1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">94.7% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.4.4" class="ltx_tr">
<td id="S2.T1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.4.4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.4.4.1.1.1" class="ltx_tr">
<td id="S2.T1.1.4.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Kinetics-400 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB</td>
<td id="S2.T1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2017</td>
<td id="S2.T1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">400</td>
<td id="S2.T1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3̃06245</td>
<td id="S2.T1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">92.1% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.5.5" class="ltx_tr">
<td id="S2.T1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.5.5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.5.5.1.1.1" class="ltx_tr">
<td id="S2.T1.1.5.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">AVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB</td>
<td id="S2.T1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2017</td>
<td id="S2.T1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">80</td>
<td id="S2.T1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">437</td>
<td id="S2.T1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">83.0% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.6.6" class="ltx_tr">
<td id="S2.T1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.6.6.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.6.6.1.1.1" class="ltx_tr">
<td id="S2.T1.1.6.6.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">EPIC Kitchen 55 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB</td>
<td id="S2.T1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2018</td>
<td id="S2.T1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">149</td>
<td id="S2.T1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32</td>
<td id="S2.T1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">39596</td>
<td id="S2.T1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.1.7.7" class="ltx_tr">
<td id="S2.T1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.7.7.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.7.7.1.1.1" class="ltx_tr">
<td id="S2.T1.1.7.7.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">AVE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB</td>
<td id="S2.T1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2018</td>
<td id="S2.T1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">28</td>
<td id="S2.T1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4143</td>
<td id="S2.T1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.1.8.8" class="ltx_tr">
<td id="S2.T1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.8.8.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.8.8.1.1.1" class="ltx_tr">
<td id="S2.T1.1.8.8.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Moments in Times <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB</td>
<td id="S2.T1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2019</td>
<td id="S2.T1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">339</td>
<td id="S2.T1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1̃000000</td>
<td id="S2.T1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">51.2% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.9.9" class="ltx_tr">
<td id="S2.T1.1.9.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.9.9.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.9.9.1.1.1" class="ltx_tr">
<td id="S2.T1.1.9.9.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Kinetics-700 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB</td>
<td id="S2.T1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2019</td>
<td id="S2.T1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">700</td>
<td id="S2.T1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6̃50317</td>
<td id="S2.T1.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">85.9% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.10.10" class="ltx_tr">
<td id="S2.T1.1.10.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.10.10.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.10.10.1.1.1" class="ltx_tr">
<td id="S2.T1.1.10.10.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">RareAct <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB</td>
<td id="S2.T1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2020</td>
<td id="S2.T1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">122</td>
<td id="S2.T1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">905</td>
<td id="S2.T1.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2024</td>
<td id="S2.T1.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.80%</td>
</tr>
<tr id="S2.T1.1.11.11" class="ltx_tr">
<td id="S2.T1.1.11.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.11.11.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.11.11.1.1.1" class="ltx_tr">
<td id="S2.T1.1.11.11.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">HiEve <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB, Skeleton</td>
<td id="S2.T1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2020</td>
<td id="S2.T1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.1.11.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.1.11.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">95.5% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.12.12" class="ltx_tr">
<td id="S2.T1.1.12.12.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.12.12.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.12.12.1.1.1" class="ltx_tr">
<td id="S2.T1.1.12.12.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">UPCV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Skeleton</td>
<td id="S2.T1.1.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2014</td>
<td id="S2.T1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10</td>
<td id="S2.T1.1.12.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20</td>
<td id="S2.T1.1.12.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">400</td>
<td id="S2.T1.1.12.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.20% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.13.13" class="ltx_tr">
<td id="S2.T1.1.13.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.13.13.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.13.13.1.1.1" class="ltx_tr">
<td id="S2.T1.1.13.13.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">MSRDaily</td>
</tr>
<tr id="S2.T1.1.13.13.1.1.2" class="ltx_tr">
<td id="S2.T1.1.13.13.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Activity3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.13.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB, Skeleton</td>
<td id="S2.T1.1.13.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2012</td>
<td id="S2.T1.1.13.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16</td>
<td id="S2.T1.1.13.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10</td>
<td id="S2.T1.1.13.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">320</td>
<td id="S2.T1.1.13.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">97.50% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.14.14" class="ltx_tr">
<td id="S2.T1.1.14.14.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.14.14.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.14.14.1.1.1" class="ltx_tr">
<td id="S2.T1.1.14.14.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">N-UCLA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.14.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB, Skeleton</td>
<td id="S2.T1.1.14.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2014</td>
<td id="S2.T1.1.14.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10</td>
<td id="S2.T1.1.14.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10</td>
<td id="S2.T1.1.14.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1475</td>
<td id="S2.T1.1.14.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.10% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.15.15" class="ltx_tr">
<td id="S2.T1.1.15.15.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.15.15.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.15.15.1.1.1" class="ltx_tr">
<td id="S2.T1.1.15.15.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Multi-View TJU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.15.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB, Skeleton</td>
<td id="S2.T1.1.15.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2014</td>
<td id="S2.T1.1.15.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20</td>
<td id="S2.T1.1.15.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22</td>
<td id="S2.T1.1.15.15.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7040</td>
<td id="S2.T1.1.15.15.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.1.16.16" class="ltx_tr">
<td id="S2.T1.1.16.16.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.16.16.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.16.16.1.1.1" class="ltx_tr">
<td id="S2.T1.1.16.16.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">UTD-MHAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.16.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB, Skeleton</td>
<td id="S2.T1.1.16.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2015</td>
<td id="S2.T1.1.16.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27</td>
<td id="S2.T1.1.16.16.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8</td>
<td id="S2.T1.1.16.16.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">861</td>
<td id="S2.T1.1.16.16.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">95.0% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.17.17" class="ltx_tr">
<td id="S2.T1.1.17.17.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.17.17.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.17.17.1.1.1" class="ltx_tr">
<td id="S2.T1.1.17.17.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">UWA3D</td>
</tr>
<tr id="S2.T1.1.17.17.1.1.2" class="ltx_tr">
<td id="S2.T1.1.17.17.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Multiview II <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.17.17.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB, Skeleton</td>
<td id="S2.T1.1.17.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2015</td>
<td id="S2.T1.1.17.17.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30</td>
<td id="S2.T1.1.17.17.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10</td>
<td id="S2.T1.1.17.17.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1075</td>
<td id="S2.T1.1.17.17.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.1.18.18" class="ltx_tr">
<td id="S2.T1.1.18.18.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.18.18.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.18.18.1.1.1" class="ltx_tr">
<td id="S2.T1.1.18.18.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">NTU RGB+D 60 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.18.18.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB, Skeleton</td>
<td id="S2.T1.1.18.18.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2016</td>
<td id="S2.T1.1.18.18.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">60</td>
<td id="S2.T1.1.18.18.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40</td>
<td id="S2.T1.1.18.18.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">56880</td>
<td id="S2.T1.1.18.18.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">97.40% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.19.19" class="ltx_tr">
<td id="S2.T1.1.19.19.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.19.19.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.19.19.1.1.1" class="ltx_tr">
<td id="S2.T1.1.19.19.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">PKU-MMD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.19.19.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB, Skeleton</td>
<td id="S2.T1.1.19.19.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2017</td>
<td id="S2.T1.1.19.19.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">51</td>
<td id="S2.T1.1.19.19.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66</td>
<td id="S2.T1.1.19.19.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10076</td>
<td id="S2.T1.1.19.19.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">94.40% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.20.20" class="ltx_tr">
<td id="S2.T1.1.20.20.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.20.20.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.20.20.1.1.1" class="ltx_tr">
<td id="S2.T1.1.20.20.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">NEU-UB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.20.20.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB</td>
<td id="S2.T1.1.20.20.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2017</td>
<td id="S2.T1.1.20.20.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6</td>
<td id="S2.T1.1.20.20.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20</td>
<td id="S2.T1.1.20.20.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">600</td>
<td id="S2.T1.1.20.20.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.1.21.21" class="ltx_tr">
<td id="S2.T1.1.21.21.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.21.21.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.21.21.1.1.1" class="ltx_tr">
<td id="S2.T1.1.21.21.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Kinetics-600 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.21.21.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB, Skeleton</td>
<td id="S2.T1.1.21.21.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2018</td>
<td id="S2.T1.1.21.21.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">600</td>
<td id="S2.T1.1.21.21.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.1.21.21.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">595445</td>
<td id="S2.T1.1.21.21.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">91.90% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.22.22" class="ltx_tr">
<td id="S2.T1.1.22.22.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.22.22.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.22.22.1.1.1" class="ltx_tr">
<td id="S2.T1.1.22.22.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">RGB-D</td>
</tr>
<tr id="S2.T1.1.22.22.1.1.2" class="ltx_tr">
<td id="S2.T1.1.22.22.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Varing-View <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.22.22.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB, Skeleton</td>
<td id="S2.T1.1.22.22.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2018</td>
<td id="S2.T1.1.22.22.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40</td>
<td id="S2.T1.1.22.22.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">118</td>
<td id="S2.T1.1.22.22.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">25600</td>
<td id="S2.T1.1.22.22.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.1.23.23" class="ltx_tr">
<td id="S2.T1.1.23.23.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.23.23.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.23.23.1.1.1" class="ltx_tr">
<td id="S2.T1.1.23.23.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">NTU RGB+D 120 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.23.23.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB, Skeleton</td>
<td id="S2.T1.1.23.23.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2019</td>
<td id="S2.T1.1.23.23.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">120</td>
<td id="S2.T1.1.23.23.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">106</td>
<td id="S2.T1.1.23.23.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">114480</td>
<td id="S2.T1.1.23.23.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">95.60% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.24.24" class="ltx_tr">
<td id="S2.T1.1.24.24.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.24.24.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.24.24.1.1.1" class="ltx_tr">
<td id="S2.T1.1.24.24.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Drive&amp;Act <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.24.24.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB, Skeleton</td>
<td id="S2.T1.1.24.24.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2019</td>
<td id="S2.T1.1.24.24.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">83</td>
<td id="S2.T1.1.24.24.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15</td>
<td id="S2.T1.1.24.24.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.1.24.24.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.61% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.25.25" class="ltx_tr">
<td id="S2.T1.1.25.25.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.25.25.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.25.25.1.1.1" class="ltx_tr">
<td id="S2.T1.1.25.25.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">MMAct <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.25.25.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB, Skeleton</td>
<td id="S2.T1.1.25.25.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2019</td>
<td id="S2.T1.1.25.25.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">37</td>
<td id="S2.T1.1.25.25.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20</td>
<td id="S2.T1.1.25.25.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36764</td>
<td id="S2.T1.1.25.25.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">98.60% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.26.26" class="ltx_tr">
<td id="S2.T1.1.26.26.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.26.26.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.26.26.1.1.1" class="ltx_tr">
<td id="S2.T1.1.26.26.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Toyota-SH <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.26.26.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB, Skeleton</td>
<td id="S2.T1.1.26.26.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2019</td>
<td id="S2.T1.1.26.26.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31</td>
<td id="S2.T1.1.26.26.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">18</td>
<td id="S2.T1.1.26.26.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16115</td>
<td id="S2.T1.1.26.26.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.1.27.27" class="ltx_tr">
<td id="S2.T1.1.27.27.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.27.27.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.27.27.1.1.1" class="ltx_tr">
<td id="S2.T1.1.27.27.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">IKEA ASM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.27.27.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB, Skeleton</td>
<td id="S2.T1.1.27.27.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2020</td>
<td id="S2.T1.1.27.27.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">33</td>
<td id="S2.T1.1.27.27.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48</td>
<td id="S2.T1.1.27.27.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16764</td>
<td id="S2.T1.1.27.27.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.1.28.28" class="ltx_tr">
<td id="S2.T1.1.28.28.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.28.28.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.28.28.1.1.1" class="ltx_tr">
<td id="S2.T1.1.28.28.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">ETRI-Activity3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.28.28.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB, Skeleton</td>
<td id="S2.T1.1.28.28.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2020</td>
<td id="S2.T1.1.28.28.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">55</td>
<td id="S2.T1.1.28.28.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">100</td>
<td id="S2.T1.1.28.28.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">112620</td>
<td id="S2.T1.1.28.28.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">95.09% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.29.29" class="ltx_tr">
<td id="S2.T1.1.29.29.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<table id="S2.T1.1.29.29.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.29.29.1.1.1" class="ltx_tr">
<td id="S2.T1.1.29.29.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">UAV-Human <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S2.T1.1.29.29.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">RGB, Skeleton</td>
<td id="S2.T1.1.29.29.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2021</td>
<td id="S2.T1.1.29.29.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">155</td>
<td id="S2.T1.1.29.29.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">119</td>
<td id="S2.T1.1.29.29.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">27428</td>
<td id="S2.T1.1.29.29.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">55.00% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Handcrafted Features with ML-Based Approach</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Researchers employed handcrafted feature extraction with ML-based systems at early ages to develop HAR systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>. In the action representation step, the RGB data is utilized to transform into the feature vector, and these feature vectors are fed into the classifier <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>, <a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite> to get the desired results of the action classification step. Table <a href="#S2.T2" title="Table 2 ‣ 2.2.3 Classification Approach ‣ 2.2 Handcrafted Features with ML-Based Approach ‣ 2 RGB-Data Modality Based Action Recognition Methods ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the analysis of the handcrafted-based approach, including the datasets, methods of feature extraction, classifier, years, and performance accuracy.
Handcrafted features are designed to capture the physical motions performed by humans and the spatial and temporal variations depicted in videos that portray actions. These variations include methods that utilize the spatiotemporal volume-based representation of actions, methods based on the Space-Time Interest Point (STIP), methods that rely on the trajectory of skeleton joints for action representation, and methods that utilize human image sequences for action representation.
Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> demonstrate this by employing DMM-based gestures for motion information extraction, while Local Binary Pattern (LBP) feature encoding enhances discriminative power for action recognition. Meanwhile, Patel et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> fuse various features, including HOG and LBP, to improve network performance in recognizing human activities. The handcrafted feature can be categorized as below:</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Holistic Feature Extraction</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">Many researchers have been working to develop Human Activity Recognition (HAR) systems based on holistic features and machine learning algorithms. Holistic representation aims to capture motion information of the entire human subject. Spatiotemporal action recognition often uses template-matching techniques, with key methods focusing on creating effective action templates.
Bobick et al. introduced two approaches, Motion Energy Image (MEI) and Motion History Image (MHI), to perform action representation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>. Meanwhile, Zhang et al. utilized polar coordinates in MHI and developed a Motion Context Descriptor (MCD) based on the Scale-Invariant Feature Transform (SIFT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>. Somasundaram et al. applied sparse representation and dictionary learning to calculate video self-similarity in both time and space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite>. In scenarios with a stationary camera, these approaches effectively capture shape-related information like human silhouettes and contours through background subtraction.
However, accurately capturing silhouettes and contours in complex scenes or with camera movements remains challenging, especially when the human body is partially obscured. Many methods employ a sliding window approach to detect multiple actions within the same scene, which can be computationally expensive. These approaches transform dynamic human motion into a holistic representation in a single image. While they capture relevant foreground information, they are sensitive to background noise, including irrelevant information.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Local and Global Representation</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">Holistic feature extraction techniques for HAR face several limitations, including sensitivity to background noise, reliance on stationary cameras, difficulty in complex scenes, occlusion issues, high computational cost, limited robustness to variations, and neglect of contextual information, making them less effective in dynamic, real-world scenarios.</p>
</div>
<figure id="S2.F6" class="ltx_figure"><img src="/html/2409.09678/assets/Year_wise_rgb_skeleton_task.png" id="S2.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1254" height="446" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Milestone approaches for HAR: RGB-based milestone methods are in pink font, while skeleton-based milestone methods are in green font</figcaption>
</figure>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">Combining local and global representations can effectively address HAR’s holistic feature extraction limitations. Local features reduce background noise sensitivity and handle occlusions, while global features ensure comprehensive activity recognition. This combination enhances robustness to variations, manages complex scenes, and optimizes computational efficiency, improving HAR accuracy and reliability.
The local presentation means identifying a specific region, while the global representation means identifying the whole region with significant motion information. These methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> contain local and global features based on spatial-temporal changes trajectory attributes that are founded on key point tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, motion changes that are derived from depth information<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite> and action-based features that are predicated on human pose changes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
The HoG is one of the feature-based techniques that calculate features on the base orientation of gradients in an image or video sequence. The HoG features are then used to encode local and global texture information, aiming to recognize different actions. Some of the presented approaches exploit the HoG in action recognition, including <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>, <a href="#bib.bib100" title="" class="ltx_ref">100</a>, <a href="#bib.bib101" title="" class="ltx_ref">101</a>, <a href="#bib.bib102" title="" class="ltx_ref">102</a>, <a href="#bib.bib103" title="" class="ltx_ref">103</a>, <a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> in various ways. Histogram of optical flow (HOF) is a used feature extraction method in action recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>, <a href="#bib.bib106" title="" class="ltx_ref">106</a>, <a href="#bib.bib107" title="" class="ltx_ref">107</a>, <a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>. It involves building histograms to present different actions over the spatio-temporal domain in a video. However, in this method, the number of bins needs to be set in advance. The challenge addresses cluttered backgrounds and camera movement by performing a physical feature-driven approach HOF.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Classification Approach</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">Once we have the feature representation, we feed it into classifiers such as support vector machine (SVM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>, <a href="#bib.bib110" title="" class="ltx_ref">110</a>, <a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite>, random forest, and KNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>, <a href="#bib.bib113" title="" class="ltx_ref">113</a>, <a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite> to predict the activity label. While some classification methods based on sequential such as Hidden Markov Models (HMM), Condition Random Fields (CRF) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>, <a href="#bib.bib116" title="" class="ltx_ref">116</a>, <a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite>, Structured Support Vector Machine (SSVM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>, <a href="#bib.bib118" title="" class="ltx_ref">118</a>, <a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite>, and Global Gaussian Mixture Models (GGMM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> these approaches perform sequential based for classification tasks. Additionally, luo et al. utilized features fusion-based methods, Maximum Margin Distance Learning (MMDL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite> and Multi-task Spare Learning Model (MTSLM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite>. These methods perform the classification task based on combining various characteristics to enhance the classification task.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Handcrafted features based on existing techniques for action recognition.</figcaption>
<table id="S2.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T2.1.1.1" class="ltx_tr">
<td id="S2.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S2.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Author</span></td>
<td id="S2.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S2.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Year</span></td>
<td id="S2.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S2.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Dataset Name</span></td>
<td id="S2.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S2.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">Modality</span></td>
<td id="S2.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S2.T2.1.1.1.5.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S2.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S2.T2.1.1.1.6.1" class="ltx_text ltx_font_bold">Classifier</span></td>
<td id="S2.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S2.T2.1.1.1.7.1" class="ltx_text ltx_font_bold">Accuracy [%]</span></td>
</tr>
<tr id="S2.T2.1.2.2" class="ltx_tr">
<td id="S2.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Chakraborty et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite>
</td>
<td id="S2.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2011</td>
<td id="S2.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.2.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.2.2.3.1.1" class="ltx_tr">
<td id="S2.T2.1.2.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Weizmann</td>
</tr>
<tr id="S2.T2.1.2.2.3.1.2" class="ltx_tr">
<td id="S2.T2.1.2.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">KTH</td>
</tr>
<tr id="S2.T2.1.2.2.3.1.3" class="ltx_tr">
<td id="S2.T2.1.2.2.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">You Tube</td>
</tr>
</table>
</td>
<td id="S2.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">STIP</td>
<td id="S2.T2.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SVM</td>
<td id="S2.T2.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.2.2.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.2.2.7.1.1" class="ltx_tr">
<td id="S2.T2.1.2.2.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">100.00</td>
</tr>
<tr id="S2.T2.1.2.2.7.1.2" class="ltx_tr">
<td id="S2.T2.1.2.2.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">96.35</td>
</tr>
<tr id="S2.T2.1.2.2.7.1.3" class="ltx_tr">
<td id="S2.T2.1.2.2.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">86.98</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T2.1.3.3" class="ltx_tr">
<td id="S2.T2.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Gan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite>
</td>
<td id="S2.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2013</td>
<td id="S2.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">UTKinect-Action</td>
<td id="S2.T2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RF</td>
<td id="S2.T2.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">APJ3D</td>
<td id="S2.T2.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">92.00</td>
</tr>
<tr id="S2.T2.1.4.4" class="ltx_tr">
<td id="S2.T2.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Everts et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite>
</td>
<td id="S2.T2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2014</td>
<td id="S2.T2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.4.4.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.4.4.3.1.1" class="ltx_tr">
<td id="S2.T2.1.4.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF11</td>
</tr>
<tr id="S2.T2.1.4.4.3.1.2" class="ltx_tr">
<td id="S2.T2.1.4.4.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF50</td>
</tr>
</table>
</td>
<td id="S2.T2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T2.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">multi-channel STIP</td>
<td id="S2.T2.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SVM</td>
<td id="S2.T2.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.4.4.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.4.4.7.1.1" class="ltx_tr">
<td id="S2.T2.1.4.4.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">78.6</td>
</tr>
<tr id="S2.T2.1.4.4.7.1.2" class="ltx_tr">
<td id="S2.T2.1.4.4.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">72.9</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T2.1.5.5" class="ltx_tr">
<td id="S2.T2.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Zhu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite>
</td>
<td id="S2.T2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2014</td>
<td id="S2.T2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.5.5.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.5.5.3.1.1" class="ltx_tr">
<td id="S2.T2.1.5.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">MSRAction3D</td>
</tr>
<tr id="S2.T2.1.5.5.3.1.2" class="ltx_tr">
<td id="S2.T2.1.5.5.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UTKinectAction</td>
</tr>
<tr id="S2.T2.1.5.5.3.1.3" class="ltx_tr">
<td id="S2.T2.1.5.5.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">CAD-60</td>
</tr>
<tr id="S2.T2.1.5.5.3.1.4" class="ltx_tr">
<td id="S2.T2.1.5.5.3.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">MSRDailyActivity3D</td>
</tr>
<tr id="S2.T2.1.5.5.3.1.5" class="ltx_tr">
<td id="S2.T2.1.5.5.3.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HMDB51</td>
</tr>
</table>
</td>
<td id="S2.T2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T2.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">STIP (HOG/HOF)</td>
<td id="S2.T2.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SVM</td>
<td id="S2.T2.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.5.5.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.5.5.7.1.1" class="ltx_tr">
<td id="S2.T2.1.5.5.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">94.3</td>
</tr>
<tr id="S2.T2.1.5.5.7.1.2" class="ltx_tr">
<td id="S2.T2.1.5.5.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">91.9</td>
</tr>
<tr id="S2.T2.1.5.5.7.1.3" class="ltx_tr">
<td id="S2.T2.1.5.5.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">87.5</td>
</tr>
<tr id="S2.T2.1.5.5.7.1.4" class="ltx_tr">
<td id="S2.T2.1.5.5.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">80.0</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T2.1.6.6" class="ltx_tr">
<td id="S2.T2.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
<td id="S2.T2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2014</td>
<td id="S2.T2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">MSR Action3D</td>
<td id="S2.T2.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T2.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">EigenJoints-based</td>
<td id="S2.T2.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">NBNN</td>
<td id="S2.T2.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">97.8</td>
</tr>
<tr id="S2.T2.1.7.7" class="ltx_tr">
<td id="S2.T2.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite>
</td>
<td id="S2.T2.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2015</td>
<td id="S2.T2.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.7.7.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.7.7.3.1.1" class="ltx_tr">
<td id="S2.T2.1.7.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">KTH</td>
</tr>
<tr id="S2.T2.1.7.7.3.1.2" class="ltx_tr">
<td id="S2.T2.1.7.7.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HMDB51</td>
</tr>
<tr id="S2.T2.1.7.7.3.1.3" class="ltx_tr">
<td id="S2.T2.1.7.7.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF YouTube</td>
</tr>
<tr id="S2.T2.1.7.7.3.1.4" class="ltx_tr">
<td id="S2.T2.1.7.7.3.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Hollywood2</td>
</tr>
</table>
</td>
<td id="S2.T2.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T2.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">GP-learned descriptors</td>
<td id="S2.T2.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SVM</td>
<td id="S2.T2.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.7.7.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.7.7.7.1.1" class="ltx_tr">
<td id="S2.T2.1.7.7.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">95.0</td>
</tr>
<tr id="S2.T2.1.7.7.7.1.2" class="ltx_tr">
<td id="S2.T2.1.7.7.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">48.4</td>
</tr>
<tr id="S2.T2.1.7.7.7.1.3" class="ltx_tr">
<td id="S2.T2.1.7.7.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">82.3</td>
</tr>
<tr id="S2.T2.1.7.7.7.1.4" class="ltx_tr">
<td id="S2.T2.1.7.7.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">46.8</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T2.1.8.8" class="ltx_tr">
<td id="S2.T2.1.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Xu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite>
</td>
<td id="S2.T2.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2016</td>
<td id="S2.T2.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.8.8.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.8.8.3.1.1" class="ltx_tr">
<td id="S2.T2.1.8.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">MSRAction3D</td>
</tr>
<tr id="S2.T2.1.8.8.3.1.2" class="ltx_tr">
<td id="S2.T2.1.8.8.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UTKinectAction</td>
</tr>
<tr id="S2.T2.1.8.8.3.1.3" class="ltx_tr">
<td id="S2.T2.1.8.8.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Florence 3D-Action</td>
</tr>
</table>
</td>
<td id="S2.T2.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T2.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">PSO-SVM</td>
<td id="S2.T2.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S2.T2.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.8.8.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.8.8.7.1.1" class="ltx_tr">
<td id="S2.T2.1.8.8.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">93.75</td>
</tr>
<tr id="S2.T2.1.8.8.7.1.2" class="ltx_tr">
<td id="S2.T2.1.8.8.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.45</td>
</tr>
<tr id="S2.T2.1.8.8.7.1.3" class="ltx_tr">
<td id="S2.T2.1.8.8.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">91.20</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T2.1.9.9" class="ltx_tr">
<td id="S2.T2.1.9.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Vishwakarma et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite>
</td>
<td id="S2.T2.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2016</td>
<td id="S2.T2.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.9.9.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.9.9.3.1.1" class="ltx_tr">
<td id="S2.T2.1.9.9.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">KTH</td>
</tr>
<tr id="S2.T2.1.9.9.3.1.2" class="ltx_tr">
<td id="S2.T2.1.9.9.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Weizmann</td>
</tr>
<tr id="S2.T2.1.9.9.3.1.3" class="ltx_tr">
<td id="S2.T2.1.9.9.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">i3Dpost</td>
</tr>
<tr id="S2.T2.1.9.9.3.1.4" class="ltx_tr">
<td id="S2.T2.1.9.9.3.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Ballet</td>
</tr>
<tr id="S2.T2.1.9.9.3.1.5" class="ltx_tr">
<td id="S2.T2.1.9.9.3.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">IXMAS</td>
</tr>
</table>
</td>
<td id="S2.T2.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T2.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SDEG</td>
<td id="S2.T2.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SVM</td>
<td id="S2.T2.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.9.9.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.9.9.7.1.1" class="ltx_tr">
<td id="S2.T2.1.9.9.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">95.5</td>
</tr>
<tr id="S2.T2.1.9.9.7.1.2" class="ltx_tr">
<td id="S2.T2.1.9.9.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">100</td>
</tr>
<tr id="S2.T2.1.9.9.7.1.3" class="ltx_tr">
<td id="S2.T2.1.9.9.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">92.92</td>
</tr>
<tr id="S2.T2.1.9.9.7.1.4" class="ltx_tr">
<td id="S2.T2.1.9.9.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">93.25</td>
</tr>
<tr id="S2.T2.1.9.9.7.1.5" class="ltx_tr">
<td id="S2.T2.1.9.9.7.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">85.8</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T2.1.10.10" class="ltx_tr">
<td id="S2.T2.1.10.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Singh et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite>
</td>
<td id="S2.T2.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2017</td>
<td id="S2.T2.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.10.10.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.10.10.3.1.1" class="ltx_tr">
<td id="S2.T2.1.10.10.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCSDped-1</td>
</tr>
<tr id="S2.T2.1.10.10.3.1.2" class="ltx_tr">
<td id="S2.T2.1.10.10.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCSDped-2</td>
</tr>
<tr id="S2.T2.1.10.10.3.1.3" class="ltx_tr">
<td id="S2.T2.1.10.10.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UMN</td>
</tr>
</table>
</td>
<td id="S2.T2.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T2.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Graph formulation</td>
<td id="S2.T2.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SVM</td>
<td id="S2.T2.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.10.10.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.10.10.7.1.1" class="ltx_tr">
<td id="S2.T2.1.10.10.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.14</td>
</tr>
<tr id="S2.T2.1.10.10.7.1.2" class="ltx_tr">
<td id="S2.T2.1.10.10.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">90.13</td>
</tr>
<tr id="S2.T2.1.10.10.7.1.3" class="ltx_tr">
<td id="S2.T2.1.10.10.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">95.24</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T2.1.11.11" class="ltx_tr">
<td id="S2.T2.1.11.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Jalal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib130" title="" class="ltx_ref">130</a>]</cite>
</td>
<td id="S2.T2.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2017</td>
<td id="S2.T2.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.11.11.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.11.11.3.1.1" class="ltx_tr">
<td id="S2.T2.1.11.11.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">IM-DailyDepthActivity</td>
</tr>
<tr id="S2.T2.1.11.11.3.1.2" class="ltx_tr">
<td id="S2.T2.1.11.11.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">MSRAction3D</td>
</tr>
<tr id="S2.T2.1.11.11.3.1.3" class="ltx_tr">
<td id="S2.T2.1.11.11.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">MSRDailyActivity3D</td>
</tr>
</table>
</td>
<td id="S2.T2.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T2.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">HOG-DDS</td>
<td id="S2.T2.1.11.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">HMM</td>
<td id="S2.T2.1.11.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.11.11.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.11.11.7.1.1" class="ltx_tr">
<td id="S2.T2.1.11.11.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">72.86</td>
</tr>
<tr id="S2.T2.1.11.11.7.1.2" class="ltx_tr">
<td id="S2.T2.1.11.11.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">93.3</td>
</tr>
<tr id="S2.T2.1.11.11.7.1.3" class="ltx_tr">
<td id="S2.T2.1.11.11.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.9</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T2.1.12.12" class="ltx_tr">
<td id="S2.T2.1.12.12.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Nazir et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite>
</td>
<td id="S2.T2.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2018</td>
<td id="S2.T2.1.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.12.12.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.12.12.3.1.1" class="ltx_tr">
<td id="S2.T2.1.12.12.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">KTH</td>
</tr>
<tr id="S2.T2.1.12.12.3.1.2" class="ltx_tr">
<td id="S2.T2.1.12.12.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF Sports</td>
</tr>
<tr id="S2.T2.1.12.12.3.1.3" class="ltx_tr">
<td id="S2.T2.1.12.12.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF11</td>
</tr>
<tr id="S2.T2.1.12.12.3.1.4" class="ltx_tr">
<td id="S2.T2.1.12.12.3.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Hollywood</td>
</tr>
</table>
</td>
<td id="S2.T2.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T2.1.12.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">D-STBoE</td>
<td id="S2.T2.1.12.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SVM</td>
<td id="S2.T2.1.12.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.12.12.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.12.12.7.1.1" class="ltx_tr">
<td id="S2.T2.1.12.12.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">91.82</td>
</tr>
<tr id="S2.T2.1.12.12.7.1.2" class="ltx_tr">
<td id="S2.T2.1.12.12.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">94.00</td>
</tr>
<tr id="S2.T2.1.12.12.7.1.3" class="ltx_tr">
<td id="S2.T2.1.12.12.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">94.00</td>
</tr>
<tr id="S2.T2.1.12.12.7.1.4" class="ltx_tr">
<td id="S2.T2.1.12.12.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">68.10</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T2.1.13.13" class="ltx_tr">
<td id="S2.T2.1.13.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Ullah et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite>
</td>
<td id="S2.T2.1.13.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2021</td>
<td id="S2.T2.1.13.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.13.13.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.13.13.3.1.1" class="ltx_tr">
<td id="S2.T2.1.13.13.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF Sports</td>
</tr>
<tr id="S2.T2.1.13.13.3.1.2" class="ltx_tr">
<td id="S2.T2.1.13.13.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF101</td>
</tr>
</table>
</td>
<td id="S2.T2.1.13.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T2.1.13.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Weekly supervised based</td>
<td id="S2.T2.1.13.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SVM</td>
<td id="S2.T2.1.13.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.13.13.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.13.13.7.1.1" class="ltx_tr">
<td id="S2.T2.1.13.13.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98.27</td>
</tr>
<tr id="S2.T2.1.13.13.7.1.2" class="ltx_tr">
<td id="S2.T2.1.13.13.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">84.72</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T2.1.14.14" class="ltx_tr">
<td id="S2.T2.1.14.14.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Al et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>
</td>
<td id="S2.T2.1.14.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2021</td>
<td id="S2.T2.1.14.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.14.14.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.14.14.3.1.1" class="ltx_tr">
<td id="S2.T2.1.14.14.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">E-KTH</td>
</tr>
<tr id="S2.T2.1.14.14.3.1.2" class="ltx_tr">
<td id="S2.T2.1.14.14.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">E-UCF11</td>
</tr>
<tr id="S2.T2.1.14.14.3.1.3" class="ltx_tr">
<td id="S2.T2.1.14.14.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">E-HMDB51</td>
</tr>
<tr id="S2.T2.1.14.14.3.1.4" class="ltx_tr">
<td id="S2.T2.1.14.14.3.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">E-UCF50</td>
</tr>
<tr id="S2.T2.1.14.14.3.1.5" class="ltx_tr">
<td id="S2.T2.1.14.14.3.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">R-UCF11</td>
</tr>
<tr id="S2.T2.1.14.14.3.1.6" class="ltx_tr">
<td id="S2.T2.1.14.14.3.1.6.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">R-UCF50</td>
</tr>
<tr id="S2.T2.1.14.14.3.1.7" class="ltx_tr">
<td id="S2.T2.1.14.14.3.1.7.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">N-Actions</td>
</tr>
</table>
</td>
<td id="S2.T2.1.14.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T2.1.14.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.14.14.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.14.14.5.1.1" class="ltx_tr">
<td id="S2.T2.1.14.14.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Local and global</td>
</tr>
<tr id="S2.T2.1.14.14.5.1.2" class="ltx_tr">
<td id="S2.T2.1.14.14.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">feature extraction</td>
</tr>
</table>
</td>
<td id="S2.T2.1.14.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">QSVM</td>
<td id="S2.T2.1.14.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.14.14.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.14.14.7.1.1" class="ltx_tr">
<td id="S2.T2.1.14.14.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">93.14</td>
</tr>
<tr id="S2.T2.1.14.14.7.1.2" class="ltx_tr">
<td id="S2.T2.1.14.14.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">94.43</td>
</tr>
<tr id="S2.T2.1.14.14.7.1.3" class="ltx_tr">
<td id="S2.T2.1.14.14.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">87.61</td>
</tr>
<tr id="S2.T2.1.14.14.7.1.4" class="ltx_tr">
<td id="S2.T2.1.14.14.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">69.45</td>
</tr>
<tr id="S2.T2.1.14.14.7.1.5" class="ltx_tr">
<td id="S2.T2.1.14.14.7.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">82.61</td>
</tr>
<tr id="S2.T2.1.14.14.7.1.6" class="ltx_tr">
<td id="S2.T2.1.14.14.7.1.6.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">68.96</td>
</tr>
<tr id="S2.T2.1.14.14.7.1.7" class="ltx_tr">
<td id="S2.T2.1.14.14.7.1.7.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">61.94</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T2.1.15.15" class="ltx_tr">
<td id="S2.T2.1.15.15.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Hejazi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite>
</td>
<td id="S2.T2.1.15.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2022</td>
<td id="S2.T2.1.15.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.15.15.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.15.15.3.1.1" class="ltx_tr">
<td id="S2.T2.1.15.15.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF101</td>
</tr>
<tr id="S2.T2.1.15.15.3.1.2" class="ltx_tr">
<td id="S2.T2.1.15.15.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Kinetics-400</td>
</tr>
<tr id="S2.T2.1.15.15.3.1.3" class="ltx_tr">
<td id="S2.T2.1.15.15.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Kinetics-700</td>
</tr>
</table>
</td>
<td id="S2.T2.1.15.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T2.1.15.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Optical flow based</td>
<td id="S2.T2.1.15.15.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">KNN</td>
<td id="S2.T2.1.15.15.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.15.15.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.15.15.7.1.1" class="ltx_tr">
<td id="S2.T2.1.15.15.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">99.21</td>
</tr>
<tr id="S2.T2.1.15.15.7.1.2" class="ltx_tr">
<td id="S2.T2.1.15.15.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98.24</td>
</tr>
<tr id="S2.T2.1.15.15.7.1.3" class="ltx_tr">
<td id="S2.T2.1.15.15.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">96.35</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T2.1.16.16" class="ltx_tr">
<td id="S2.T2.1.16.16.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite>
</td>
<td id="S2.T2.1.16.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2022</td>
<td id="S2.T2.1.16.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.16.16.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.16.16.3.1.1" class="ltx_tr">
<td id="S2.T2.1.16.16.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF 11</td>
</tr>
<tr id="S2.T2.1.16.16.3.1.2" class="ltx_tr">
<td id="S2.T2.1.16.16.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF 50</td>
</tr>
<tr id="S2.T2.1.16.16.3.1.3" class="ltx_tr">
<td id="S2.T2.1.16.16.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF 101</td>
</tr>
<tr id="S2.T2.1.16.16.3.1.4" class="ltx_tr">
<td id="S2.T2.1.16.16.3.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">JHMDB51</td>
</tr>
<tr id="S2.T2.1.16.16.3.1.5" class="ltx_tr">
<td id="S2.T2.1.16.16.3.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UT-Interaction</td>
</tr>
</table>
</td>
<td id="S2.T2.1.16.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T2.1.16.16.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">FV+BoTF</td>
<td id="S2.T2.1.16.16.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SVM</td>
<td id="S2.T2.1.16.16.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T2.1.16.16.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.16.16.7.1.1" class="ltx_tr">
<td id="S2.T2.1.16.16.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">99.21</td>
</tr>
<tr id="S2.T2.1.16.16.7.1.2" class="ltx_tr">
<td id="S2.T2.1.16.16.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">92.5</td>
</tr>
<tr id="S2.T2.1.16.16.7.1.3" class="ltx_tr">
<td id="S2.T2.1.16.16.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">95.1</td>
</tr>
<tr id="S2.T2.1.16.16.7.1.4" class="ltx_tr">
<td id="S2.T2.1.16.16.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">70.8</td>
</tr>
<tr id="S2.T2.1.16.16.7.1.5" class="ltx_tr">
<td id="S2.T2.1.16.16.7.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">91.50</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T2.1.17.17" class="ltx_tr">
<td id="S2.T2.1.17.17.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Fatima et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib136" title="" class="ltx_ref">136</a>]</cite>
</td>
<td id="S2.T2.1.17.17.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2023</td>
<td id="S2.T2.1.17.17.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">UT-Interaction</td>
<td id="S2.T2.1.17.17.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T2.1.17.17.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SIFT and ORB</td>
<td id="S2.T2.1.17.17.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Decision Tree</td>
<td id="S2.T2.1.17.17.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">94.6</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S2.F7" class="ltx_figure"><img src="/html/2409.09678/assets/Year_wise_multimodal_task.png" id="S2.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1255" height="434" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Milestone approaches for HAR. The pink font is the multimodality-based method and the black font is the sensor-based method. </figcaption>
</figure>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>End-to-End Deep Learning Approach</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The holistic, local, and global features reported promising results in the HAR task, but these handcrafted features need much specific knowledge to define relevant parameters. Additionally, they do not generalize the sizeable data set well. In recent years, significant focus has been on utilizing DL in computer vision. Numerous approaches have been used deep neural network-based to recognize human activity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>, <a href="#bib.bib138" title="" class="ltx_ref">138</a>, <a href="#bib.bib139" title="" class="ltx_ref">139</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib140" title="" class="ltx_ref">140</a>, <a href="#bib.bib141" title="" class="ltx_ref">141</a>, <a href="#bib.bib142" title="" class="ltx_ref">142</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib143" title="" class="ltx_ref">143</a>, <a href="#bib.bib144" title="" class="ltx_ref">144</a>, <a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite>.
Figure <a href="#S2.F6" title="Figure 6 ‣ 2.2.2 Local and Global Representation ‣ 2.2 Handcrafted Features with ML-Based Approach ‣ 2 RGB-Data Modality Based Action Recognition Methods ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> demonstrates the year-wise end-to-end deep learning method developed by various researchers for the RGB-based HAR systems.
Recently, researchers have utilized different ideas for spatiotemporal feature extraction, divided into three categories: two-stream networks, multi-stream networks, 3D CNN, and Hybrid Networks.</p>
</div>
<figure id="S2.F8" class="ltx_figure"><img src="/html/2409.09678/assets/x2.jpg" id="S2.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="527" height="153" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>RGB Based Two-stream architecture HAR.</figcaption>
</figure>
<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Two Stream Based Network</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para">
<p id="S2.SS3.SSS1.p1.1" class="ltx_p">The motion of an object can be represented based on the optical flow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref">146</a>]</cite>.
Simonyan et al. proposed a two-stream convolutional network to recognize human activity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> as depicted in Figure <a href="#S2.F8" title="Figure 8 ‣ 2.3 End-to-End Deep Learning Approach ‣ 2 RGB-Data Modality Based Action Recognition Methods ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.
In a convolutional network with two streams, the optical flow information is computed from the sequence of images. Two separate CNNs process image and optical flow sequences as inputs during model training. Fusion of these inputs occurs at the final classification layer. The two-stream network handles a single-frame image and a stack of optical flow frames using 2D convolution. In contrast, a 3D convolutional network treats the video as a space-time structure and employs 3D convolution to capture human action features.
<br class="ltx_break">Numerous research endeavors have been conducted to enhance the efficacy of these two network architectures. Noteworthy advancements in the two-stream CNNs have been made by Zhang et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite>, who substituted the optical flow sequence with the motion vector in the video stream. This substitution resulted in improved calculation speed and facilitated real-time implementation of the aforementioned HAR technique. The process of merging spatial and temporal information has been modified by Feichtenhofer et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib147" title="" class="ltx_ref">147</a>]</cite>, shifting it from the initial final classification layer to an intermediate position within the network. As a result, the accuracy of action recognition has been further enhanced. The input structure of the convolutional network, as well as the strategy for training, have been extensively examined by Wang et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite>. Moreover, an additional enhancement to the performance of the two-stream convolutional network was introduced through the proposal of a temporal segment network (TSN). Moreover, the recognition results of TSN were further improved by the contributions of both Lan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib149" title="" class="ltx_ref">149</a>]</cite> and Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib150" title="" class="ltx_ref">150</a>]</cite>.
Depending on the architecture of the deep learning network, notable works typically focus on methods using two-stream CNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Transfer learning with RGB data enhances action recognition by leveraging pre-trained models’ knowledge. Pham et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>]</cite> present a DL-based framework where poses extracted from RGB video sequences are converted into image-based representations and inputted into a deep CNN, utilizing attention mechanisms to highlight critical features.</p>
</div>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Multi Stream Based Network</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para">
<p id="S2.SS3.SSS2.p1.1" class="ltx_p">RGB data paired with CNNs offers powerful action recognition capabilities. Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib152" title="" class="ltx_ref">152</a>]</cite> leverage a multi-stream convolutional network to enhance recognition performance by incorporating manually crafted skeleton joint information with CNN-derived features. Shi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib153" title="" class="ltx_ref">153</a>]</cite> employ transfer learning techniques in a three-stream network, incorporating dense trajectories to characterize long-term motion effectively.
Attention mechanisms with RGB data focus on relevant regions for better action recognition.</p>
</div>
<figure id="S2.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>RGB and deep learning-based existing techniques for action recognition.</figcaption>
<table id="S2.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T3.1.1.1" class="ltx_tr">
<td id="S2.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S2.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Author</span></td>
<td id="S2.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S2.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Year</span></td>
<td id="S2.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S2.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">Dataset Name</span></td>
<td id="S2.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S2.T3.1.1.1.4.1" class="ltx_text ltx_font_bold">Modality</span></td>
<td id="S2.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S2.T3.1.1.1.5.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S2.T3.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S2.T3.1.1.1.6.1" class="ltx_text ltx_font_bold">Classifier</span></td>
<td id="S2.T3.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S2.T3.1.1.1.7.1" class="ltx_text ltx_font_bold">Accuracy [%]</span></td>
</tr>
<tr id="S2.T3.1.2.2" class="ltx_tr">
<td id="S2.T3.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Ji et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite>
</td>
<td id="S2.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2012</td>
<td id="S2.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">KTH</td>
<td id="S2.T3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">3DCNN</td>
<td id="S2.T3.1.2.2.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S2.T3.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">90.2</td>
</tr>
<tr id="S2.T3.1.3.3" class="ltx_tr">
<td id="S2.T3.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite>
</td>
<td id="S2.T3.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2015</td>
<td id="S2.T3.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.3.3.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.3.3.3.1.1" class="ltx_tr">
<td id="S2.T3.1.3.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF101</td>
</tr>
<tr id="S2.T3.1.3.3.3.1.2" class="ltx_tr">
<td id="S2.T3.1.3.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HMDB51</td>
</tr>
</table>
</td>
<td id="S2.T3.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.3.3.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.3.3.5.1.1" class="ltx_tr">
<td id="S2.T3.1.3.3.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">2-stream</td>
</tr>
<tr id="S2.T3.1.3.3.5.1.2" class="ltx_tr">
<td id="S2.T3.1.3.3.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Convolution Network</td>
</tr>
</table>
</td>
<td id="S2.T3.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S2.T3.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.3.3.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.3.3.7.1.1" class="ltx_tr">
<td id="S2.T3.1.3.3.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">91.5</td>
</tr>
<tr id="S2.T3.1.3.3.7.1.2" class="ltx_tr">
<td id="S2.T3.1.3.3.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">65.9</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.4.4" class="ltx_tr">
<td id="S2.T3.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Sharma et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite>
</td>
<td id="S2.T3.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2015</td>
<td id="S2.T3.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.4.4.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.4.4.3.1.1" class="ltx_tr">
<td id="S2.T3.1.4.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF11</td>
</tr>
<tr id="S2.T3.1.4.4.3.1.2" class="ltx_tr">
<td id="S2.T3.1.4.4.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HMDB51</td>
</tr>
<tr id="S2.T3.1.4.4.3.1.3" class="ltx_tr">
<td id="S2.T3.1.4.4.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Hollywood2</td>
</tr>
</table>
</td>
<td id="S2.T3.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Stacked LSTM</td>
<td id="S2.T3.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S2.T3.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.4.4.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.4.4.7.1.1" class="ltx_tr">
<td id="S2.T3.1.4.4.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">84.96</td>
</tr>
<tr id="S2.T3.1.4.4.7.1.2" class="ltx_tr">
<td id="S2.T3.1.4.4.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">41.31</td>
</tr>
<tr id="S2.T3.1.4.4.7.1.3" class="ltx_tr">
<td id="S2.T3.1.4.4.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">43.91</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.5.5" class="ltx_tr">
<td id="S2.T3.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Ijjina et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib156" title="" class="ltx_ref">156</a>]</cite>
</td>
<td id="S2.T3.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2016</td>
<td id="S2.T3.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">UCF50</td>
<td id="S2.T3.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">CNN-Genetic Algorithm</td>
<td id="S2.T3.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">CNN</td>
<td id="S2.T3.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">99.98</td>
</tr>
<tr id="S2.T3.1.6.6" class="ltx_tr">
<td id="S2.T3.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Feichtenhofer et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib147" title="" class="ltx_ref">147</a>]</cite>
</td>
<td id="S2.T3.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2016</td>
<td id="S2.T3.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.6.6.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.6.6.3.1.1" class="ltx_tr">
<td id="S2.T3.1.6.6.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF101</td>
</tr>
<tr id="S2.T3.1.6.6.3.1.2" class="ltx_tr">
<td id="S2.T3.1.6.6.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HMDB51</td>
</tr>
</table>
</td>
<td id="S2.T3.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">CNN Two-Stream</td>
<td id="S2.T3.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S2.T3.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.6.6.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.6.6.7.1.1" class="ltx_tr">
<td id="S2.T3.1.6.6.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">92.5</td>
</tr>
<tr id="S2.T3.1.6.6.7.1.2" class="ltx_tr">
<td id="S2.T3.1.6.6.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">65.4</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.7.7" class="ltx_tr">
<td id="S2.T3.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite>
</td>
<td id="S2.T3.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2016</td>
<td id="S2.T3.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.7.7.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.7.7.3.1.1" class="ltx_tr">
<td id="S2.T3.1.7.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HMDB51</td>
</tr>
<tr id="S2.T3.1.7.7.3.1.2" class="ltx_tr">
<td id="S2.T3.1.7.7.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF101</td>
</tr>
</table>
</td>
<td id="S2.T3.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">TSN</td>
<td id="S2.T3.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S2.T3.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.7.7.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.7.7.7.1.1" class="ltx_tr">
<td id="S2.T3.1.7.7.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">69.4</td>
</tr>
<tr id="S2.T3.1.7.7.7.1.2" class="ltx_tr">
<td id="S2.T3.1.7.7.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">94.2</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.8.8" class="ltx_tr">
<td id="S2.T3.1.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Akilan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib157" title="" class="ltx_ref">157</a>]</cite>
</td>
<td id="S2.T3.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2017</td>
<td id="S2.T3.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.8.8.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.8.8.3.1.1" class="ltx_tr">
<td id="S2.T3.1.8.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">CIFAR100</td>
</tr>
<tr id="S2.T3.1.8.8.3.1.2" class="ltx_tr">
<td id="S2.T3.1.8.8.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Caltech101</td>
</tr>
<tr id="S2.T3.1.8.8.3.1.3" class="ltx_tr">
<td id="S2.T3.1.8.8.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">CIFAR10</td>
</tr>
</table>
</td>
<td id="S2.T3.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">ConvNets</td>
<td id="S2.T3.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S2.T3.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.8.8.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.8.8.7.1.1" class="ltx_tr">
<td id="S2.T3.1.8.8.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">75.87</td>
</tr>
<tr id="S2.T3.1.8.8.7.1.2" class="ltx_tr">
<td id="S2.T3.1.8.8.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">95.54</td>
</tr>
<tr id="S2.T3.1.8.8.7.1.3" class="ltx_tr">
<td id="S2.T3.1.8.8.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">91.83</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.9.9" class="ltx_tr">
<td id="S2.T3.1.9.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Shi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib153" title="" class="ltx_ref">153</a>]</cite>
</td>
<td id="S2.T3.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2017</td>
<td id="S2.T3.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.9.9.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.9.9.3.1.1" class="ltx_tr">
<td id="S2.T3.1.9.9.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">KTH</td>
</tr>
<tr id="S2.T3.1.9.9.3.1.2" class="ltx_tr">
<td id="S2.T3.1.9.9.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF101</td>
</tr>
<tr id="S2.T3.1.9.9.3.1.3" class="ltx_tr">
<td id="S2.T3.1.9.9.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HMDB51</td>
</tr>
</table>
</td>
<td id="S2.T3.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">3-stream CNN</td>
<td id="S2.T3.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S2.T3.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.9.9.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.9.9.7.1.1" class="ltx_tr">
<td id="S2.T3.1.9.9.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">96.8</td>
</tr>
<tr id="S2.T3.1.9.9.7.1.2" class="ltx_tr">
<td id="S2.T3.1.9.9.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">94.33</td>
</tr>
<tr id="S2.T3.1.9.9.7.1.3" class="ltx_tr">
<td id="S2.T3.1.9.9.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">92.2</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.10.10" class="ltx_tr">
<td id="S2.T3.1.10.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Ahsan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib158" title="" class="ltx_ref">158</a>]</cite>
</td>
<td id="S2.T3.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2018</td>
<td id="S2.T3.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.10.10.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.10.10.3.1.1" class="ltx_tr">
<td id="S2.T3.1.10.10.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF101</td>
</tr>
<tr id="S2.T3.1.10.10.3.1.2" class="ltx_tr">
<td id="S2.T3.1.10.10.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HMDB51</td>
</tr>
</table>
</td>
<td id="S2.T3.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">GAN</td>
<td id="S2.T3.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S2.T3.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.10.10.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.10.10.7.1.1" class="ltx_tr">
<td id="S2.T3.1.10.10.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">47.2</td>
</tr>
<tr id="S2.T3.1.10.10.7.1.2" class="ltx_tr">
<td id="S2.T3.1.10.10.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">41.40</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.11.11" class="ltx_tr">
<td id="S2.T3.1.11.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Tu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib159" title="" class="ltx_ref">159</a>]</cite>
</td>
<td id="S2.T3.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2018</td>
<td id="S2.T3.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.11.11.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.11.11.3.1.1" class="ltx_tr">
<td id="S2.T3.1.11.11.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">JHMDB</td>
</tr>
<tr id="S2.T3.1.11.11.3.1.2" class="ltx_tr">
<td id="S2.T3.1.11.11.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HMDB51</td>
</tr>
<tr id="S2.T3.1.11.11.3.1.3" class="ltx_tr">
<td id="S2.T3.1.11.11.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF Sports</td>
</tr>
<tr id="S2.T3.1.11.11.3.1.4" class="ltx_tr">
<td id="S2.T3.1.11.11.3.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF101</td>
</tr>
</table>
</td>
<td id="S2.T3.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Multi-Stream CNN</td>
<td id="S2.T3.1.11.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S2.T3.1.11.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.11.11.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.11.11.7.1.1" class="ltx_tr">
<td id="S2.T3.1.11.11.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">71.17</td>
</tr>
<tr id="S2.T3.1.11.11.7.1.2" class="ltx_tr">
<td id="S2.T3.1.11.11.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">69.8</td>
</tr>
<tr id="S2.T3.1.11.11.7.1.3" class="ltx_tr">
<td id="S2.T3.1.11.11.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">58.12</td>
</tr>
<tr id="S2.T3.1.11.11.7.1.4" class="ltx_tr">
<td id="S2.T3.1.11.11.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">94.5</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.12.12" class="ltx_tr">
<td id="S2.T3.1.12.12.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib160" title="" class="ltx_ref">160</a>]</cite>
</td>
<td id="S2.T3.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2018</td>
<td id="S2.T3.1.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.12.12.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.12.12.3.1.1" class="ltx_tr">
<td id="S2.T3.1.12.12.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HMDB51</td>
</tr>
<tr id="S2.T3.1.12.12.3.1.2" class="ltx_tr">
<td id="S2.T3.1.12.12.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF101</td>
</tr>
</table>
</td>
<td id="S2.T3.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.12.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">TMiCT-Net</td>
<td id="S2.T3.1.12.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">CNN</td>
<td id="S2.T3.1.12.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.12.12.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.12.12.7.1.1" class="ltx_tr">
<td id="S2.T3.1.12.12.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">70.5</td>
</tr>
<tr id="S2.T3.1.12.12.7.1.2" class="ltx_tr">
<td id="S2.T3.1.12.12.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">94.7</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.13.13" class="ltx_tr">
<td id="S2.T3.1.13.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Jian et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite>
</td>
<td id="S2.T3.1.13.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2019</td>
<td id="S2.T3.1.13.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Sport video</td>
<td id="S2.T3.1.13.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.13.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">FCN</td>
<td id="S2.T3.1.13.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S2.T3.1.13.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">97.40</td>
</tr>
<tr id="S2.T3.1.14.14" class="ltx_tr">
<td id="S2.T3.1.14.14.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Ullah et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
<td id="S2.T3.1.14.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2019</td>
<td id="S2.T3.1.14.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.14.14.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.14.14.3.1.1" class="ltx_tr">
<td id="S2.T3.1.14.14.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF50</td>
</tr>
<tr id="S2.T3.1.14.14.3.1.2" class="ltx_tr">
<td id="S2.T3.1.14.14.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF101</td>
</tr>
<tr id="S2.T3.1.14.14.3.1.3" class="ltx_tr">
<td id="S2.T3.1.14.14.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">YouTube action</td>
</tr>
<tr id="S2.T3.1.14.14.3.1.4" class="ltx_tr">
<td id="S2.T3.1.14.14.3.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HMDB51</td>
</tr>
</table>
</td>
<td id="S2.T3.1.14.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.14.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Deep autoencoder</td>
<td id="S2.T3.1.14.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SVM</td>
<td id="S2.T3.1.14.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.14.14.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.14.14.7.1.1" class="ltx_tr">
<td id="S2.T3.1.14.14.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">96.4</td>
</tr>
<tr id="S2.T3.1.14.14.7.1.2" class="ltx_tr">
<td id="S2.T3.1.14.14.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">94.33</td>
</tr>
<tr id="S2.T3.1.14.14.7.1.3" class="ltx_tr">
<td id="S2.T3.1.14.14.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">96.21</td>
</tr>
<tr id="S2.T3.1.14.14.7.1.4" class="ltx_tr">
<td id="S2.T3.1.14.14.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">70.33</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.15.15" class="ltx_tr">
<td id="S2.T3.1.15.15.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Gowda et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib162" title="" class="ltx_ref">162</a>]</cite>
</td>
<td id="S2.T3.1.15.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2020</td>
<td id="S2.T3.1.15.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.15.15.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.15.15.3.1.1" class="ltx_tr">
<td id="S2.T3.1.15.15.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF101</td>
</tr>
<tr id="S2.T3.1.15.15.3.1.2" class="ltx_tr">
<td id="S2.T3.1.15.15.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HMDB51</td>
</tr>
<tr id="S2.T3.1.15.15.3.1.3" class="ltx_tr">
<td id="S2.T3.1.15.15.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">FCVID</td>
</tr>
<tr id="S2.T3.1.15.15.3.1.4" class="ltx_tr">
<td id="S2.T3.1.15.15.3.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">ActivityNet</td>
</tr>
</table>
</td>
<td id="S2.T3.1.15.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.15.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SMART</td>
<td id="S2.T3.1.15.15.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S2.T3.1.15.15.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.15.15.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.15.15.7.1.1" class="ltx_tr">
<td id="S2.T3.1.15.15.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98.6</td>
</tr>
<tr id="S2.T3.1.15.15.7.1.2" class="ltx_tr">
<td id="S2.T3.1.15.15.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">84.3</td>
</tr>
<tr id="S2.T3.1.15.15.7.1.3" class="ltx_tr">
<td id="S2.T3.1.15.15.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">82.1</td>
</tr>
<tr id="S2.T3.1.15.15.7.1.4" class="ltx_tr">
<td id="S2.T3.1.15.15.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">84.4</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.16.16" class="ltx_tr">
<td id="S2.T3.1.16.16.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Khan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>
</td>
<td id="S2.T3.1.16.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2020</td>
<td id="S2.T3.1.16.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.16.16.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.16.16.3.1.1" class="ltx_tr">
<td id="S2.T3.1.16.16.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HMDB51</td>
</tr>
<tr id="S2.T3.1.16.16.3.1.2" class="ltx_tr">
<td id="S2.T3.1.16.16.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF Sports</td>
</tr>
<tr id="S2.T3.1.16.16.3.1.3" class="ltx_tr">
<td id="S2.T3.1.16.16.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">YouTube</td>
</tr>
<tr id="S2.T3.1.16.16.3.1.4" class="ltx_tr">
<td id="S2.T3.1.16.16.3.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">IXMAS</td>
</tr>
<tr id="S2.T3.1.16.16.3.1.5" class="ltx_tr">
<td id="S2.T3.1.16.16.3.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">KTH</td>
</tr>
</table>
</td>
<td id="S2.T3.1.16.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.16.16.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">VGG19 CNN</td>
<td id="S2.T3.1.16.16.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Naive Bayes</td>
<td id="S2.T3.1.16.16.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.16.16.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.16.16.7.1.1" class="ltx_tr">
<td id="S2.T3.1.16.16.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">93.7</td>
</tr>
<tr id="S2.T3.1.16.16.7.1.2" class="ltx_tr">
<td id="S2.T3.1.16.16.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98.0</td>
</tr>
<tr id="S2.T3.1.16.16.7.1.3" class="ltx_tr">
<td id="S2.T3.1.16.16.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">94.4</td>
</tr>
<tr id="S2.T3.1.16.16.7.1.4" class="ltx_tr">
<td id="S2.T3.1.16.16.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">99.4</td>
</tr>
<tr id="S2.T3.1.16.16.7.1.5" class="ltx_tr">
<td id="S2.T3.1.16.16.7.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">95.2</td>
</tr>
<tr id="S2.T3.1.16.16.7.1.6" class="ltx_tr">
<td id="S2.T3.1.16.16.7.1.6.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.0</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.17.17" class="ltx_tr">
<td id="S2.T3.1.17.17.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Ullah et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite>
</td>
<td id="S2.T3.1.17.17.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2021</td>
<td id="S2.T3.1.17.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.17.17.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.17.17.3.1.1" class="ltx_tr">
<td id="S2.T3.1.17.17.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HMDB51</td>
</tr>
<tr id="S2.T3.1.17.17.3.1.2" class="ltx_tr">
<td id="S2.T3.1.17.17.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF101</td>
</tr>
<tr id="S2.T3.1.17.17.3.1.3" class="ltx_tr">
<td id="S2.T3.1.17.17.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF50</td>
</tr>
<tr id="S2.T3.1.17.17.3.1.4" class="ltx_tr">
<td id="S2.T3.1.17.17.3.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Hollywood2</td>
</tr>
<tr id="S2.T3.1.17.17.3.1.5" class="ltx_tr">
<td id="S2.T3.1.17.17.3.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">YouTube Actions</td>
</tr>
</table>
</td>
<td id="S2.T3.1.17.17.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.17.17.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">DS-GRU</td>
<td id="S2.T3.1.17.17.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S2.T3.1.17.17.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.17.17.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.17.17.7.1.1" class="ltx_tr">
<td id="S2.T3.1.17.17.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">72.3</td>
</tr>
<tr id="S2.T3.1.17.17.7.1.2" class="ltx_tr">
<td id="S2.T3.1.17.17.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">95.5</td>
</tr>
<tr id="S2.T3.1.17.17.7.1.3" class="ltx_tr">
<td id="S2.T3.1.17.17.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">95.2</td>
</tr>
<tr id="S2.T3.1.17.17.7.1.4" class="ltx_tr">
<td id="S2.T3.1.17.17.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">71.3</td>
</tr>
<tr id="S2.T3.1.17.17.7.1.5" class="ltx_tr">
<td id="S2.T3.1.17.17.7.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.17</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.18.18" class="ltx_tr">
<td id="S2.T3.1.18.18.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib165" title="" class="ltx_ref">165</a>]</cite>
</td>
<td id="S2.T3.1.18.18.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2021</td>
<td id="S2.T3.1.18.18.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.18.18.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.18.18.3.1.1" class="ltx_tr">
<td id="S2.T3.1.18.18.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">SomethingV1</td>
</tr>
<tr id="S2.T3.1.18.18.3.1.2" class="ltx_tr">
<td id="S2.T3.1.18.18.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">SomethingV2</td>
</tr>
<tr id="S2.T3.1.18.18.3.1.3" class="ltx_tr">
<td id="S2.T3.1.18.18.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Kinetics-400</td>
</tr>
</table>
</td>
<td id="S2.T3.1.18.18.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.18.18.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Temporal Difference Networks</td>
<td id="S2.T3.1.18.18.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">TDN</td>
<td id="S2.T3.1.18.18.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.18.18.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.18.18.7.1.1" class="ltx_tr">
<td id="S2.T3.1.18.18.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">84.1</td>
</tr>
<tr id="S2.T3.1.18.18.7.1.2" class="ltx_tr">
<td id="S2.T3.1.18.18.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">91.6</td>
</tr>
<tr id="S2.T3.1.18.18.7.1.3" class="ltx_tr">
<td id="S2.T3.1.18.18.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">94.4</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.19.19" class="ltx_tr">
<td id="S2.T3.1.19.19.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>]</cite>
</td>
<td id="S2.T3.1.19.19.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2022</td>
<td id="S2.T3.1.19.19.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">UCF101</td>
<td id="S2.T3.1.19.19.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.19.19.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">HyRSM</td>
<td id="S2.T3.1.19.19.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S2.T3.1.19.19.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">93.0</td>
</tr>
<tr id="S2.T3.1.20.20" class="ltx_tr">
<td id="S2.T3.1.20.20.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Wensel et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite>
</td>
<td id="S2.T3.1.20.20.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2023</td>
<td id="S2.T3.1.20.20.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.20.20.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.20.20.3.1.1" class="ltx_tr">
<td id="S2.T3.1.20.20.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">YouTube Action</td>
</tr>
<tr id="S2.T3.1.20.20.3.1.2" class="ltx_tr">
<td id="S2.T3.1.20.20.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HMDB51</td>
</tr>
<tr id="S2.T3.1.20.20.3.1.3" class="ltx_tr">
<td id="S2.T3.1.20.20.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF50</td>
</tr>
<tr id="S2.T3.1.20.20.3.1.4" class="ltx_tr">
<td id="S2.T3.1.20.20.3.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF101</td>
</tr>
</table>
</td>
<td id="S2.T3.1.20.20.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.20.20.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">ViT-ReT</td>
<td id="S2.T3.1.20.20.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S2.T3.1.20.20.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.20.20.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.20.20.7.1.1" class="ltx_tr">
<td id="S2.T3.1.20.20.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">92.4</td>
</tr>
<tr id="S2.T3.1.20.20.7.1.2" class="ltx_tr">
<td id="S2.T3.1.20.20.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">78.4</td>
</tr>
<tr id="S2.T3.1.20.20.7.1.3" class="ltx_tr">
<td id="S2.T3.1.20.20.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.1</td>
</tr>
<tr id="S2.T3.1.20.20.7.1.4" class="ltx_tr">
<td id="S2.T3.1.20.20.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">94.7</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.21.21" class="ltx_tr">
<td id="S2.T3.1.21.21.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Hassan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite>
</td>
<td id="S2.T3.1.21.21.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2024</td>
<td id="S2.T3.1.21.21.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.21.21.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.21.21.3.1.1" class="ltx_tr">
<td id="S2.T3.1.21.21.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF11</td>
</tr>
<tr id="S2.T3.1.21.21.3.1.2" class="ltx_tr">
<td id="S2.T3.1.21.21.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF Sports</td>
</tr>
<tr id="S2.T3.1.21.21.3.1.3" class="ltx_tr">
<td id="S2.T3.1.21.21.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">JHMDB</td>
</tr>
</table>
</td>
<td id="S2.T3.1.21.21.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S2.T3.1.21.21.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Deep Bi-LSTM</td>
<td id="S2.T3.1.21.21.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S2.T3.1.21.21.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S2.T3.1.21.21.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.21.21.7.1.1" class="ltx_tr">
<td id="S2.T3.1.21.21.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">99.2</td>
</tr>
<tr id="S2.T3.1.21.21.7.1.2" class="ltx_tr">
<td id="S2.T3.1.21.21.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">93.3</td>
</tr>
<tr id="S2.T3.1.21.21.7.1.3" class="ltx_tr">
<td id="S2.T3.1.21.21.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">76.3</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S2.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.3 </span>3D CNN and Hybrid Networks</h4>

<div id="S2.SS3.SSS3.p1" class="ltx_para">
<p id="S2.SS3.SSS3.p1.1" class="ltx_p">Traditional two-stream techniques often separate spatial and temporal information, which can render them less suitable for real-time deployment. However, subsequent research introduced 3D convolutional approaches that directly extract information across all three dimensions. These 3D approaches aim to address the limitations of the earlier two-stream networks.
Ji et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite> utilized the 3D CNN model for the action recognition task. This model extracts features from both the spatial and the temporal dimensions.
Tran et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> used C3D to extract spatiotemporal features for a large dataset to train the model, which is the extension of the 3DCNN model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite>.
Carreira et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite> developed I3D, extending the network to extract spatiotemporal features along with temporal dimension. They proposed image classification models to create 3D CNNs by transferring weights from 2D models pre-trained on ImageNet to align with the weights in the 3D model. P3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite> and R(2+1)D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib171" title="" class="ltx_ref">171</a>]</cite> streamlined 3D network training using factorization, combining 2D spatial convolutions (1×3) with 1D temporal convolutions (3×1×1) instead of traditional 3D convolutions (3×3). For improved motion analysis, trajectory convolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib172" title="" class="ltx_ref">172</a>]</cite> employed deformable convolutions in the temporal domain. Other approaches simplify 3D CNNs by integrating 2D and 3D convolutions within single networks to enhance feature maps, exemplified by models like MiCTNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib160" title="" class="ltx_ref">160</a>]</cite>, ARTNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib173" title="" class="ltx_ref">173</a>]</cite>, and S3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib174" title="" class="ltx_ref">174</a>]</cite>.
To enhance the performances of 3DCNN, CSN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite> has demonstrated the effectiveness of decomposing 3D convolution by separating channel interactions from spatiotemporal interactions, leading to state-of-the-art performance improvements. This technique can achieve speeds 2 to 3 times faster than previous methods. Feichtenhofer et al. developed the X3D methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib176" title="" class="ltx_ref">176</a>]</cite> as shown in Figure <a href="#S2.F9" title="Figure 9 ‣ 2.3.3 3D CNN and Hybrid Networks ‣ 2.3 End-to-End Deep Learning Approach ‣ 2 RGB-Data Modality Based Action Recognition Methods ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. The X3D network included both spatial and temporal dimensions with enhanced spatial, input resolution, and channel dimensions.
Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib177" title="" class="ltx_ref">177</a>]</cite> proposed that morphologically similar actions like walking, jogging, and running require discrimination assisted by visual speed. They proposed a Temporal Pyramid Network (TPN) similar to X3D. This approach enables the extraction of effective features at various temporal rates, reducing computational complexity while enhancing efficiency performances.
Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib178" title="" class="ltx_ref">178</a>]</cite> proposed a 4D CNN with 4D convolution to capture the evolution of distant spatiotemporal representations.
<br class="ltx_break">Similarly, numerous researchers have made efforts to expand various 2D CNNs to 3D spatiotemporal structures to acquire knowledge about and identify human action features, drawing inspiration from the concept of 3D (Three-dimensional) ConvNet. Carreira et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite> expanded the network architecture of inception-V1 to incorporate 3D and introduced the two-stream inflated 3D ConvNet for HAR.
Qin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite> propose a fusion scheme combining classical descriptors with 3D CNN-learned features, achieving robustness against geometric and optical deformations. Diba et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite> extended DenseNet and introduced a temporal 3D ConvNet for HAR. Zhu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite> expanded pooling operations across spatial and temporal dimensions, transforming the two-stream convolution network into a three-dimensional structure. Carreira et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite> conducted a comparison of five architectures: LSTM with CNN, 3D ConvNet, two-stream network, two-stream inflated 3D ConvNet, and 3D-fused two-stream network. In essence, 3D CNNs establish relationships between temporal and spatial features in various ways, complementing rather than replacing two-stream networks.
Hassan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite> a deep bidirectional LSTM model, which effectively integrates the advantages of temporal effective features extraction through bi-LSTM and spatial feature extraction via CNN. The LSTM architecture is not feasible to support parallel computing, which can limit its efficiency. To overcome this problem, the transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib182" title="" class="ltx_ref">182</a>]</cite> has become popular in DL to address this limitation. Girdhar et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib183" title="" class="ltx_ref">183</a>]</cite> used the transformer-based architecture to add context features and developed an attention mechanism to improve performance.</p>
</div>
<figure id="S2.F9" class="ltx_figure"><img src="/html/2409.09678/assets/X3D_framework.png" id="S2.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="574" height="297" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>The X3D model framework.</figcaption>
</figure>
</section>
<section id="S2.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.4 </span>Recurrent Neural Network Based Approach</h4>

<div id="S2.SS3.SSS4.p1" class="ltx_para">
<p id="S2.SS3.SSS4.p1.1" class="ltx_p">Unlike two-stream and 3D ConvNet, which use various convolutional temporal feature pooling architectures to model action, LSTM-based methods well perform view a video as a sequential arrangement of frames. The representation of HAR can subsequently be depicted through the alterations in features observed in each frame. Donahue et al. explored LSTM and developed LRCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib184" title="" class="ltx_ref">184</a>]</cite> to model CNN-generated spatial features across temporal sequences.
Another significant HAR technique involves the use of LSTM with CNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>, <a href="#bib.bib185" title="" class="ltx_ref">185</a>]</cite>. Ng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib185" title="" class="ltx_ref">185</a>]</cite> introduced a recurrent neural network (RNN) model to identify and classify the action, which performs a connection between the LSTM cell and the output of the underlying CNN. Furthermore, Qiu et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite> proposed a novel architectural design termed Pseudo-3D ResNet (P3D ResNet), wherein each block is assembled in a distinct ResNet configuration. Donahue et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib184" title="" class="ltx_ref">184</a>]</cite> proposed a method of using long-term RNNs to map video frames of varying lengths to outputs of varying lengths, such as action descriptive text, rather than simply assigning them to a specific action category. Song et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib186" title="" class="ltx_ref">186</a>]</cite> introduced a model using RNNs with LSTM that employed multiple attention levels to discern key joints in the skeleton across each input frame.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Skeleton Data Modality Based Action Recognition Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The main challenges of the RGB-based data modality-based HAR system are redundant background and computational complexity issues, and the Skeleton-based data modality helps us overcome these challenges. In addition, coupled with joint coordinate estimation algorithms such as OpenPose and SDK <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib187" title="" class="ltx_ref">187</a>]</cite> has improved the performance of accuracy and reliability of the skeleton data. Skeleton data obtained from the joint position offers several benefits over the RGB data, such as illumination variations, viewing angles, and background occlusions, making it less susceptible to noise interference. The research prefers to perform HAR by using the skeleton data because it provides more focused information and reduces redundancy. Based on the feature extraction methods for HAR, the skeleton data can be divided into DL-based methods, relying on learned features, and ML-based methods, which use handcrafted features. In addition, the skeleton data depends on the precise joint position and pose estimation techniques.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Figure <a href="#S3.F10" title="Figure 10 ‣ 3.1 Skeleton Based HAR Dataset ‣ 3 Skeleton Data Modality Based Action Recognition Method ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> shows the framework of skeleton-based approaches. Table <a href="#S3.T4" title="Table 4 ‣ 3.2.2 3D Human Pose Estimation Based Methods ‣ 3.2 Pose Estimation ‣ 3 Skeleton Data Modality Based Action Recognition Method ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> describes the key information about the skeleton-based data modality on the existing model, including datasets, classification methods, years, and performance accuracy. We describe the well-known pose estimation algorithms in the following section.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Skeleton Based HAR Dataset</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We provided the most popular benchmark HAR datasets, which come from the skeleton, which is demonstrated in Table <a href="#S2.T1" title="Table 1 ‣ 2.1 RGB-Based Datasets of HAR ‣ 2 RGB-Data Modality Based Action Recognition Methods ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The dataset table demonstrated the details of the datasets, including modalities, creation year, number of classes, number of subjects who participated in recording the dataset, number of samples, and latest performance accuracy of the dataset with citation.
The Skeleton dataset includes a variety of notable benchmarks essential for Human Activity Recognition (HAR). The UPCV dataset from 2014 features 10 classes, 20 subjects, and 400 samples, achieving an outstanding accuracy of 99.2% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. The NTU RGB+D dataset, introduced in 2016 and expanded in 2019, is one of the most comprehensive, with 60 and 120 classes, 40 and 106 subjects, and 56,880 and 114,480 samples, respectively, both versions recording an accuracy of 97.4% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>. The MSRDailyActivity3D dataset from 2012 includes 16 classes, 10 subjects, and 320 samples, with an accuracy of 97.5% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. The PKU-MMD dataset from 2017 contains 51 classes, 66 subjects, and 10,076 samples, with a notable accuracy of 94.4% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>. The Multi-View TJU dataset from 2014 offers 20 classes, 22 subjects, and 7,040 samples. These datasets are crucial for training and testing HAR models, offering diverse activities and scenarios to enhance model robustness and accuracy.</p>
</div>
<figure id="S3.F10" class="ltx_figure"><img src="/html/2409.09678/assets/Seklton_Based_HAR.png" id="S3.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="981" height="93" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Skeleton-based action recognition.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Pose Estimation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We can extract human joint skeleton points from the RGB video using media pipe, openpose, AlphaPose  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib188" title="" class="ltx_ref">188</a>, <a href="#bib.bib189" title="" class="ltx_ref">189</a>]</cite>, MMPose, etc. Using a media pipe, figure <a href="#S3.F11" title="Figure 11 ‣ 3.2 Pose Estimation ‣ 3 Skeleton Data Modality Based Action Recognition Method ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> demonstrated the 33 joint skeleton points from the whole body. Human limb trunk reconstruction included estimating human pose by detecting joint positions in the skeleton and establishing their connections. Traditional methods, relying on manual feature labeling and regression for joint coordinate retrieval, suffer from low accuracy. DL-based methods, including 2D and 3D pose estimation, have become pivotal in this research domain.</p>
</div>
<figure id="S3.F11" class="ltx_figure"><img src="/html/2409.09678/assets/x3.jpg" id="S3.F11.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="137" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Landmarks position.</figcaption>
</figure>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>2D Human Pose Estimation Based Methods</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">The objective of 2D human pose estimation is to identify significant body parts in an image and connect them sequentially to form a human skeleton graph. Research commonly addresses the classification of single and multiple human subjects. In single-person pose estimation, the goal is to detect a solitary individual in an image. This involves first recognizing all joints of the person’s body and subsequently generating a bounding box around them. Two main categories of models exist for single-person pose estimation. The first utilizes a direct regression approach, where key points are directly predicted from extracted features. In 2D pose estimation, one can employ deformable part models to recognize the object by matching a set of templates. Nevertheless, these deformable part models exhibit limited expressiveness and fail to consider the global context. Yan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib190" title="" class="ltx_ref">190</a>]</cite> proposed a pose-based and performed two main methods: detection-based and regression-based approaches. Detection-based methods utilize powerful part detectors based on CNNs, which can be integrated using graphical models as described by Yuille et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib191" title="" class="ltx_ref">191</a>]</cite>. For solving the detection problem, pose estimation can be represented as a heat map where each pixel indicates the detection confidence of a joint, as outlined by Bulat et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib192" title="" class="ltx_ref">192</a>]</cite>. However, detection approaches do not directly provide joint coordinates. A post-processing step is applied to recover poses where (x, y) coordinates are obtained by utilizing the max function. Toshev et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib193" title="" class="ltx_ref">193</a>]</cite> proposed a cascade of regressor methods to estimate poses, they employ the regression-based approach with a nonlinear function that maps the joint coordinates and refines pose estimates. Carreira et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib194" title="" class="ltx_ref">194</a>]</cite> propose the Iterative Error Feedback (IEF) approach, where iterative prediction is performed to correct the current estimates. Instead of predicting outputs in a single step, a self-correcting model is employed, which modifies an initial solution by incorporating error predictions, also called IEF. However, the sub-optimal nature of the regression function leads to lower performance than detection-based techniques.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>3D Human Pose Estimation Based Methods</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Conversely, when presented with an image containing an individual, the objective of 3D pose estimation is to generate a 3D pose that accurately aligns with the spatial location of the person depicted. The accurate reconstruction of 3D poses from real-life images holds significant potential in various fields of HAR such as entertainment and human-computer interaction, particularly indoors and outdoors. Earlier approaches relied on feature engineering techniques, whereas the most advanced techniques are based on deep neural networks, as proposed by Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib195" title="" class="ltx_ref">195</a>]</cite> 3D pose estimation is acknowledged to be more complex than its 2D handle due to its management of a larger 3D pose space and an increased number of ambiguities. Nunes et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib196" title="" class="ltx_ref">196</a>]</cite> presented skeleton extraction through depth images, wherein skeleton joints are inferred frame by frame. A manually selected set of 15 skeleton joints, as determined by Gan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite>, they used to form an APJ3D representation, which is based on relative positions and local spherical angles. These 15 joints, which have been deliberately selected, play a crucial role in the development of a concise representation of human posture. Spatial features are encoded using diverse metrics, including joint distances, orientations, vectors, distances between joints and lines, and angles between lines. These measures collectively contribute to a comprehensive texture feature set, as suggested by Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib197" title="" class="ltx_ref">197</a>]</cite>. Additionally, a CNN-based network is trained to recognize corresponding actions.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Skeleton and deep learning based on existing approach for action recognition
</figcaption>
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.1.1.1" class="ltx_tr">
<th id="S3.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Author</span></th>
<th id="S3.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Year</span></th>
<th id="S3.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">Dataset Name</span></th>
<th id="S3.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T4.1.1.1.4.1" class="ltx_text ltx_font_bold">Modality</span></th>
<th id="S3.T4.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T4.1.1.1.5.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S3.T4.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T4.1.1.1.6.1" class="ltx_text ltx_font_bold">Classifier</span></th>
<th id="S3.T4.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T4.1.1.1.7.1" class="ltx_text ltx_font_bold">Accuracy [%]</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.1.2.1" class="ltx_tr">
<td id="S3.T4.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Veeriah et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib198" title="" class="ltx_ref">198</a>]</cite>
</td>
<td id="S3.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2015</td>
<td id="S3.T4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.2.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.2.1.3.1.1" class="ltx_tr">
<td id="S3.T4.1.2.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">MSRAction3D (CV)</td>
</tr>
<tr id="S3.T4.1.2.1.3.1.2" class="ltx_tr">
<td id="S3.T4.1.2.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">KTH-1 (CV)</td>
</tr>
<tr id="S3.T4.1.2.1.3.1.3" class="ltx_tr">
<td id="S3.T4.1.2.1.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">KTH-2 (CV)</td>
</tr>
</table>
</td>
<td id="S3.T4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Differential RNN</td>
<td id="S3.T4.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S3.T4.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.2.1.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.2.1.7.1.1" class="ltx_tr">
<td id="S3.T4.1.2.1.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">92.03</td>
</tr>
<tr id="S3.T4.1.2.1.7.1.2" class="ltx_tr">
<td id="S3.T4.1.2.1.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">93.96, 92.12</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.3.2" class="ltx_tr">
<td id="S3.T4.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Xu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite>
</td>
<td id="S3.T4.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2016</td>
<td id="S3.T4.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.3.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.3.2.3.1.1" class="ltx_tr">
<td id="S3.T4.1.3.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">MSRAction3D</td>
</tr>
<tr id="S3.T4.1.3.2.3.1.2" class="ltx_tr">
<td id="S3.T4.1.3.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UTKinect</td>
</tr>
<tr id="S3.T4.1.3.2.3.1.3" class="ltx_tr">
<td id="S3.T4.1.3.2.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Florence3D action</td>
</tr>
</table>
</td>
<td id="S3.T4.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SVM with PSO</td>
<td id="S3.T4.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SVM</td>
<td id="S3.T4.1.3.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.3.2.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.3.2.7.1.1" class="ltx_tr">
<td id="S3.T4.1.3.2.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">93.75</td>
</tr>
<tr id="S3.T4.1.3.2.7.1.2" class="ltx_tr">
<td id="S3.T4.1.3.2.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.45, 91.20</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.4.3" class="ltx_tr">
<td id="S3.T4.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Zhu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib199" title="" class="ltx_ref">199</a>]</cite>
</td>
<td id="S3.T4.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2016</td>
<td id="S3.T4.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.4.3.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.4.3.3.1.1" class="ltx_tr">
<td id="S3.T4.1.4.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">SBU Kinect</td>
</tr>
<tr id="S3.T4.1.4.3.3.1.2" class="ltx_tr">
<td id="S3.T4.1.4.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HDM05, CMU</td>
</tr>
</table>
</td>
<td id="S3.T4.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Stacked LSTM</td>
<td id="S3.T4.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMAx</td>
<td id="S3.T4.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.4.3.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.4.3.7.1.1" class="ltx_tr">
<td id="S3.T4.1.4.3.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">90.41</td>
</tr>
<tr id="S3.T4.1.4.3.7.1.2" class="ltx_tr">
<td id="S3.T4.1.4.3.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.25, 81.04</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.5.4" class="ltx_tr">
<td id="S3.T4.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib200" title="" class="ltx_ref">200</a>]</cite>
</td>
<td id="S3.T4.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2017</td>
<td id="S3.T4.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.5.4.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.5.4.3.1.1" class="ltx_tr">
<td id="S3.T4.1.5.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UTD-MHAD</td>
</tr>
<tr id="S3.T4.1.5.4.3.1.2" class="ltx_tr">
<td id="S3.T4.1.5.4.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD (CV)</td>
</tr>
<tr id="S3.T4.1.5.4.3.1.3" class="ltx_tr">
<td id="S3.T4.1.5.4.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD (CS)</td>
</tr>
</table>
</td>
<td id="S3.T4.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">CNN</td>
<td id="S3.T4.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Maximum Score</td>
<td id="S3.T4.1.5.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.5.4.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.5.4.7.1.1" class="ltx_tr">
<td id="S3.T4.1.5.4.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">88.10</td>
</tr>
<tr id="S3.T4.1.5.4.7.1.2" class="ltx_tr">
<td id="S3.T4.1.5.4.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">82.3</td>
</tr>
<tr id="S3.T4.1.5.4.7.1.3" class="ltx_tr">
<td id="S3.T4.1.5.4.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">76.2</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.6.5" class="ltx_tr">
<td id="S3.T4.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Soo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib201" title="" class="ltx_ref">201</a>]</cite>
</td>
<td id="S3.T4.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2017</td>
<td id="S3.T4.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.6.5.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.6.5.3.1.1" class="ltx_tr">
<td id="S3.T4.1.6.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD (CV)</td>
</tr>
<tr id="S3.T4.1.6.5.3.1.2" class="ltx_tr">
<td id="S3.T4.1.6.5.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD (CS)</td>
</tr>
</table>
</td>
<td id="S3.T4.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Temporal CNN</td>
<td id="S3.T4.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S3.T4.1.6.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.6.5.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.6.5.7.1.1" class="ltx_tr">
<td id="S3.T4.1.6.5.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">83.1</td>
</tr>
<tr id="S3.T4.1.6.5.7.1.2" class="ltx_tr">
<td id="S3.T4.1.6.5.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">74.3</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.7.6" class="ltx_tr">
<td id="S3.T4.1.7.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib152" title="" class="ltx_ref">152</a>]</cite>
</td>
<td id="S3.T4.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2017</td>
<td id="S3.T4.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.7.6.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.7.6.3.1.1" class="ltx_tr">
<td id="S3.T4.1.7.6.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD (CS)</td>
</tr>
<tr id="S3.T4.1.7.6.3.1.2" class="ltx_tr">
<td id="S3.T4.1.7.6.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD (CV)</td>
</tr>
<tr id="S3.T4.1.7.6.3.1.3" class="ltx_tr">
<td id="S3.T4.1.7.6.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">MSRC-12 (CS)</td>
</tr>
<tr id="S3.T4.1.7.6.3.1.4" class="ltx_tr">
<td id="S3.T4.1.7.6.3.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Northwestern-UCLA</td>
</tr>
</table>
</td>
<td id="S3.T4.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Multi-stream CNN</td>
<td id="S3.T4.1.7.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S3.T4.1.7.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.7.6.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.7.6.7.1.1" class="ltx_tr">
<td id="S3.T4.1.7.6.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">80.03, 87.21</td>
</tr>
<tr id="S3.T4.1.7.6.7.1.2" class="ltx_tr">
<td id="S3.T4.1.7.6.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">96.62, 92.61</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.8.7" class="ltx_tr">
<td id="S3.T4.1.8.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Das et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib202" title="" class="ltx_ref">202</a>]</cite>
</td>
<td id="S3.T4.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2018</td>
<td id="S3.T4.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.8.7.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.8.7.3.1.1" class="ltx_tr">
<td id="S3.T4.1.8.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">MSRDailyActivity3D</td>
</tr>
<tr id="S3.T4.1.8.7.3.1.2" class="ltx_tr">
<td id="S3.T4.1.8.7.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD (CS)</td>
</tr>
<tr id="S3.T4.1.8.7.3.1.3" class="ltx_tr">
<td id="S3.T4.1.8.7.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">CAD-60</td>
</tr>
</table>
</td>
<td id="S3.T4.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.8.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Stacked LSTM</td>
<td id="S3.T4.1.8.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S3.T4.1.8.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.8.7.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.8.7.7.1.1" class="ltx_tr">
<td id="S3.T4.1.8.7.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">91.56</td>
</tr>
<tr id="S3.T4.1.8.7.7.1.2" class="ltx_tr">
<td id="S3.T4.1.8.7.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">64.49, 67.64</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.9.8" class="ltx_tr">
<td id="S3.T4.1.9.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Si et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib203" title="" class="ltx_ref">203</a>]</cite>
</td>
<td id="S3.T4.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2019</td>
<td id="S3.T4.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.9.8.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.9.8.3.1.1" class="ltx_tr">
<td id="S3.T4.1.9.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD (CS)</td>
</tr>
<tr id="S3.T4.1.9.8.3.1.2" class="ltx_tr">
<td id="S3.T4.1.9.8.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD (CV)</td>
</tr>
<tr id="S3.T4.1.9.8.3.1.3" class="ltx_tr">
<td id="S3.T4.1.9.8.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCLA</td>
</tr>
</table>
</td>
<td id="S3.T4.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.9.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">AGCN-LSTM</td>
<td id="S3.T4.1.9.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Sigmoid</td>
<td id="S3.T4.1.9.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.9.8.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.9.8.7.1.1" class="ltx_tr">
<td id="S3.T4.1.9.8.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">89.2, 95.0</td>
</tr>
<tr id="S3.T4.1.9.8.7.1.2" class="ltx_tr">
<td id="S3.T4.1.9.8.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">93.3</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.10.9" class="ltx_tr">
<td id="S3.T4.1.10.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Shi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib204" title="" class="ltx_ref">204</a>]</cite>
</td>
<td id="S3.T4.1.10.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2019</td>
<td id="S3.T4.1.10.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.10.9.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.10.9.3.1.1" class="ltx_tr">
<td id="S3.T4.1.10.9.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD (CS)</td>
</tr>
<tr id="S3.T4.1.10.9.3.1.2" class="ltx_tr">
<td id="S3.T4.1.10.9.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD (CV)</td>
</tr>
<tr id="S3.T4.1.10.9.3.1.3" class="ltx_tr">
<td id="S3.T4.1.10.9.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Kinetics</td>
</tr>
</table>
</td>
<td id="S3.T4.1.10.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.10.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">AGCN</td>
<td id="S3.T4.1.10.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S3.T4.1.10.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.10.9.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.10.9.7.1.1" class="ltx_tr">
<td id="S3.T4.1.10.9.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">88.5</td>
</tr>
<tr id="S3.T4.1.10.9.7.1.2" class="ltx_tr">
<td id="S3.T4.1.10.9.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">95.1</td>
</tr>
<tr id="S3.T4.1.10.9.7.1.3" class="ltx_tr">
<td id="S3.T4.1.10.9.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">58.7</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.11.10" class="ltx_tr">
<td id="S3.T4.1.11.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Trelinski et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib205" title="" class="ltx_ref">205</a>]</cite>
</td>
<td id="S3.T4.1.11.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2019</td>
<td id="S3.T4.1.11.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.11.10.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.11.10.3.1.1" class="ltx_tr">
<td id="S3.T4.1.11.10.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UTD-MHAD</td>
</tr>
<tr id="S3.T4.1.11.10.3.1.2" class="ltx_tr">
<td id="S3.T4.1.11.10.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">MSR-Action3D</td>
</tr>
</table>
</td>
<td id="S3.T4.1.11.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.11.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">CNN-based</td>
<td id="S3.T4.1.11.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S3.T4.1.11.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.11.10.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.11.10.7.1.1" class="ltx_tr">
<td id="S3.T4.1.11.10.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">95.8, 77.44</td>
</tr>
<tr id="S3.T4.1.11.10.7.1.2" class="ltx_tr">
<td id="S3.T4.1.11.10.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">80.36</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.12.11" class="ltx_tr">
<td id="S3.T4.1.12.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib206" title="" class="ltx_ref">206</a>]</cite>
</td>
<td id="S3.T4.1.12.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2019</td>
<td id="S3.T4.1.12.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.12.11.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.12.11.3.1.1" class="ltx_tr">
<td id="S3.T4.1.12.11.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD (CS)</td>
</tr>
<tr id="S3.T4.1.12.11.3.1.2" class="ltx_tr">
<td id="S3.T4.1.12.11.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Kinetics (CV)</td>
</tr>
</table>
</td>
<td id="S3.T4.1.12.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.12.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.12.11.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.12.11.5.1.1" class="ltx_tr">
<td id="S3.T4.1.12.11.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Actional graph</td>
</tr>
<tr id="S3.T4.1.12.11.5.1.2" class="ltx_tr">
<td id="S3.T4.1.12.11.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">based CNN</td>
</tr>
</table>
</td>
<td id="S3.T4.1.12.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S3.T4.1.12.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.12.11.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.12.11.7.1.1" class="ltx_tr">
<td id="S3.T4.1.12.11.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">86.8</td>
</tr>
<tr id="S3.T4.1.12.11.7.1.2" class="ltx_tr">
<td id="S3.T4.1.12.11.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">56.5</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.13.12" class="ltx_tr">
<td id="S3.T4.1.13.12.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Huynh et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib207" title="" class="ltx_ref">207</a>]</cite>
</td>
<td id="S3.T4.1.13.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2019</td>
<td id="S3.T4.1.13.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.13.12.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.13.12.3.1.1" class="ltx_tr">
<td id="S3.T4.1.13.12.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">MSRAction3D</td>
</tr>
<tr id="S3.T4.1.13.12.3.1.2" class="ltx_tr">
<td id="S3.T4.1.13.12.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UTKinect-3D</td>
</tr>
<tr id="S3.T4.1.13.12.3.1.3" class="ltx_tr">
<td id="S3.T4.1.13.12.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">SBU-Kinect Interaction</td>
</tr>
</table>
</td>
<td id="S3.T4.1.13.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.13.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">ConvNets</td>
<td id="S3.T4.1.13.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S3.T4.1.13.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.13.12.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.13.12.7.1.1" class="ltx_tr">
<td id="S3.T4.1.13.12.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.9</td>
</tr>
<tr id="S3.T4.1.13.12.7.1.2" class="ltx_tr">
<td id="S3.T4.1.13.12.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98.5, 96.2</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.14.13" class="ltx_tr">
<td id="S3.T4.1.14.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Huynh et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib208" title="" class="ltx_ref">208</a>]</cite>
</td>
<td id="S3.T4.1.14.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2020</td>
<td id="S3.T4.1.14.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.14.13.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.14.13.3.1.1" class="ltx_tr">
<td id="S3.T4.1.14.13.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGB+D</td>
</tr>
<tr id="S3.T4.1.14.13.3.1.2" class="ltx_tr">
<td id="S3.T4.1.14.13.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UTKinect-Action3D</td>
</tr>
</table>
</td>
<td id="S3.T4.1.14.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.14.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">PoT2I with CNN</td>
<td id="S3.T4.1.14.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S3.T4.1.14.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.14.13.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.14.13.7.1.1" class="ltx_tr">
<td id="S3.T4.1.14.13.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">83.85,98.5</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.15.14" class="ltx_tr">
<td id="S3.T4.1.15.14.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Naveenkumar et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib209" title="" class="ltx_ref">209</a>]</cite>
</td>
<td id="S3.T4.1.15.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2020</td>
<td id="S3.T4.1.15.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.15.14.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.15.14.3.1.1" class="ltx_tr">
<td id="S3.T4.1.15.14.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UTKinect-Action3D</td>
</tr>
<tr id="S3.T4.1.15.14.3.1.2" class="ltx_tr">
<td id="S3.T4.1.15.14.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGB+D</td>
</tr>
</table>
</td>
<td id="S3.T4.1.15.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.15.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Deep ensemble</td>
<td id="S3.T4.1.15.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S3.T4.1.15.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.15.14.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.15.14.7.1.1" class="ltx_tr">
<td id="S3.T4.1.15.14.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98.9, 84.2</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.16.15" class="ltx_tr">
<td id="S3.T4.1.16.15.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Plizzari et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib210" title="" class="ltx_ref">210</a>]</cite>
</td>
<td id="S3.T4.1.16.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2021</td>
<td id="S3.T4.1.16.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.16.15.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.16.15.3.1.1" class="ltx_tr">
<td id="S3.T4.1.16.15.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD 60</td>
</tr>
<tr id="S3.T4.1.16.15.3.1.2" class="ltx_tr">
<td id="S3.T4.1.16.15.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD 120</td>
</tr>
<tr id="S3.T4.1.16.15.3.1.3" class="ltx_tr">
<td id="S3.T4.1.16.15.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Kinetics Skeleton-400</td>
</tr>
</table>
</td>
<td id="S3.T4.1.16.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.16.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">ST-GCN</td>
<td id="S3.T4.1.16.15.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S3.T4.1.16.15.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.16.15.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.16.15.7.1.1" class="ltx_tr">
<td id="S3.T4.1.16.15.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">96.3, 87.1</td>
</tr>
<tr id="S3.T4.1.16.15.7.1.2" class="ltx_tr">
<td id="S3.T4.1.16.15.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">60.5</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.17.16" class="ltx_tr">
<td id="S3.T4.1.17.16.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Snoun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib211" title="" class="ltx_ref">211</a>]</cite>
</td>
<td id="S3.T4.1.17.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2021</td>
<td id="S3.T4.1.17.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.17.16.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.17.16.3.1.1" class="ltx_tr">
<td id="S3.T4.1.17.16.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">RGBD-HuDact, KTH</td>
</tr>
</table>
</td>
<td id="S3.T4.1.17.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.17.16.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">VGG16</td>
<td id="S3.T4.1.17.16.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S3.T4.1.17.16.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.17.16.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.17.16.7.1.1" class="ltx_tr">
<td id="S3.T4.1.17.16.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">95.7, 93.5</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.18.17" class="ltx_tr">
<td id="S3.T4.1.18.17.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Duan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib212" title="" class="ltx_ref">212</a>]</cite>
</td>
<td id="S3.T4.1.18.17.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2022</td>
<td id="S3.T4.1.18.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.18.17.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.18.17.3.1.1" class="ltx_tr">
<td id="S3.T4.1.18.17.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD</td>
</tr>
<tr id="S3.T4.1.18.17.3.1.2" class="ltx_tr">
<td id="S3.T4.1.18.17.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF101</td>
</tr>
</table>
</td>
<td id="S3.T4.1.18.17.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.18.17.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">PYSKL</td>
<td id="S3.T4.1.18.17.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T4.1.18.17.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.18.17.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.18.17.7.1.1" class="ltx_tr">
<td id="S3.T4.1.18.17.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.4, 86.9</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.19.18" class="ltx_tr">
<td id="S3.T4.1.19.18.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Song et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib213" title="" class="ltx_ref">213</a>]</cite>
</td>
<td id="S3.T4.1.19.18.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2022</td>
<td id="S3.T4.1.19.18.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.19.18.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.19.18.3.1.1" class="ltx_tr">
<td id="S3.T4.1.19.18.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD</td>
</tr>
</table>
</td>
<td id="S3.T4.1.19.18.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.19.18.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">GCN</td>
<td id="S3.T4.1.19.18.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S3.T4.1.19.18.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.19.18.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.19.18.7.1.1" class="ltx_tr">
<td id="S3.T4.1.19.18.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">96.1</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.20.19" class="ltx_tr">
<td id="S3.T4.1.20.19.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Zhu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib214" title="" class="ltx_ref">214</a>]</cite>
</td>
<td id="S3.T4.1.20.19.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2023</td>
<td id="S3.T4.1.20.19.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.20.19.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.20.19.3.1.1" class="ltx_tr">
<td id="S3.T4.1.20.19.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UESTC</td>
</tr>
<tr id="S3.T4.1.20.19.3.1.2" class="ltx_tr">
<td id="S3.T4.1.20.19.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-60 (CS)</td>
</tr>
</table>
</td>
<td id="S3.T4.1.20.19.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.20.19.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RSA-Net</td>
<td id="S3.T4.1.20.19.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S3.T4.1.20.19.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.20.19.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.20.19.7.1.1" class="ltx_tr">
<td id="S3.T4.1.20.19.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">93.9, 91.8</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.21.20" class="ltx_tr">
<td id="S3.T4.1.21.20.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib215" title="" class="ltx_ref">215</a>]</cite>
</td>
<td id="S3.T4.1.21.20.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2023</td>
<td id="S3.T4.1.21.20.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.21.20.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.21.20.3.1.1" class="ltx_tr">
<td id="S3.T4.1.21.20.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD</td>
</tr>
<tr id="S3.T4.1.21.20.3.1.2" class="ltx_tr">
<td id="S3.T4.1.21.20.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Kinetics-Skeleton</td>
</tr>
</table>
</td>
<td id="S3.T4.1.21.20.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.21.20.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Multilayer LSTM</td>
<td id="S3.T4.1.21.20.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S3.T4.1.21.20.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.21.20.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.21.20.7.1.1" class="ltx_tr">
<td id="S3.T4.1.21.20.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">83.3</td>
</tr>
<tr id="S3.T4.1.21.20.7.1.2" class="ltx_tr">
<td id="S3.T4.1.21.20.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">27.8(Top-1)</td>
</tr>
<tr id="S3.T4.1.21.20.7.1.3" class="ltx_tr">
<td id="S3.T4.1.21.20.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">50.2( Top-5)</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.22.21" class="ltx_tr">
<td id="S3.T4.1.22.21.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib216" title="" class="ltx_ref">216</a>]</cite>
</td>
<td id="S3.T4.1.22.21.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2023</td>
<td id="S3.T4.1.22.21.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.22.21.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.22.21.3.1.1" class="ltx_tr">
<td id="S3.T4.1.22.21.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD 60</td>
</tr>
<tr id="S3.T4.1.22.21.3.1.2" class="ltx_tr">
<td id="S3.T4.1.22.21.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">(CV)NTU-RGBD 120 (CS)</td>
</tr>
</table>
</td>
<td id="S3.T4.1.22.21.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.22.21.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">LKJ-GSN</td>
<td id="S3.T4.1.22.21.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S3.T4.1.22.21.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.22.21.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.22.21.7.1.1" class="ltx_tr">
<td id="S3.T4.1.22.21.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">96.1</td>
</tr>
<tr id="S3.T4.1.22.21.7.1.2" class="ltx_tr">
<td id="S3.T4.1.22.21.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">86.3</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.1.23.22" class="ltx_tr">
<td id="S3.T4.1.23.22.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Liang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib217" title="" class="ltx_ref">217</a>]</cite>
</td>
<td id="S3.T4.1.23.22.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2024</td>
<td id="S3.T4.1.23.22.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.23.22.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.23.22.3.1.1" class="ltx_tr">
<td id="S3.T4.1.23.22.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD (CV)</td>
</tr>
<tr id="S3.T4.1.23.22.3.1.2" class="ltx_tr">
<td id="S3.T4.1.23.22.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU-RGBD 120 (CS)</td>
</tr>
<tr id="S3.T4.1.23.22.3.1.3" class="ltx_tr">
<td id="S3.T4.1.23.22.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">FineGYM</td>
</tr>
</table>
</td>
<td id="S3.T4.1.23.22.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skeleton</td>
<td id="S3.T4.1.23.22.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">MTCF</td>
<td id="S3.T4.1.23.22.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S3.T4.1.23.22.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T4.1.23.22.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.23.22.7.1.1" class="ltx_tr">
<td id="S3.T4.1.23.22.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">96.9, 86.6</td>
</tr>
<tr id="S3.T4.1.23.22.7.1.2" class="ltx_tr">
<td id="S3.T4.1.23.22.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">94.1</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Handcrafted Feature and ML Based Classification Approach</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Researchers determine handcrafted features using statistical features extracted from action data. These features describe the dynamics or statistical properties of the action analyzed. Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> proposed a method to extract the super vector features to determine the action based on the depth information. Shao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib218" title="" class="ltx_ref">218</a>]</cite> combine shape and motion information for HAR through temporal segmentation, utilizing MHI and Predicted Gradients (PCOG) as feature descriptors. Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib219" title="" class="ltx_ref">219</a>]</cite> introduced the depth motion map (DMM) technique, which allows for the projection and compression of the spatiotemporal depth structure from different viewpoints, including the side, front, and upper views. This process results in the formation of three distinct motion history maps. To represent these motion history maps, the authors employed the HOG feature. Instead of using HOG, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> employed local binary pattern features to describe human activities based on Dynamic Motion Models (DMMs). Additionally, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib220" title="" class="ltx_ref">220</a>]</cite> introduced a spatiotemporal depth layout across frontal, lateral, and upper orientations. Departing from depth compression methods, they extracted motion trajectory shapes and boundary histogram features from spatiotemporal interest points, leveraging dense sampling and joint points in each perspective to depict actions. Moreover, Miao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib221" title="" class="ltx_ref">221</a>]</cite> applied the discrete cosine variation technique for effective compression of depth maps. Simultaneously, they generated action features by utilizing transform coefficients. From the available depth data, it is possible to estimate the structure of the human skeleton promptly and precisely. Shotton et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib222" title="" class="ltx_ref">222</a>]</cite> proposed a method for real-time estimation of body postures from depth images, thereby facilitating rapid segmentation of humans based on depth. Within this context, the problem of detecting joints has been simplified to a per-pixel classification task. Additionally, there is ongoing research in the field of HAR that employs depth data and focuses on methods utilizing the human skeleton. These approaches analyze changes in the joint points of the human body across consecutive video frames to characterize actions, encompassing alterations in both the position and appearance of the joint points. Xia et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib223" title="" class="ltx_ref">223</a>]</cite> proposed a three-dimensional joint point histogram as a means to depict the human pose and subsequently formulated the action using a discrete hidden Markov model. Keceli et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib224" title="" class="ltx_ref">224</a>]</cite>, captured depth and human skeleton information via the employment of the Kinect sensor, and subsequently derived human action features by assessing the angle and displacement information about the skeleton joint points. Similarly, Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> developed a method based on the EigenJoints, which leverages an accumulative motion energy (AME) function to identify video frames and joint points that offer richer information for action modeling. Pazhoumand et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib225" title="" class="ltx_ref">225</a>]</cite> utilized the longest common subsequent method to select distinctive features with high discriminatory power from the skeleton’s relative motion trajectories, thereby providing a comprehensive description of the corresponding action.
<br class="ltx_break">Handcrafted features offer high interpretability, simplicity, and straight-forward. However, the handcrafted features-based method requires prior knowledge, which is difficult to generalize.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>End to End Deep Learning Based Approach</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Recently, there has been a growing HAR of the advantages of integrating skeleton data with DL-based techniques. The handcrafted features have reduced discriminative capability for HAR; conversely, to extract features efficiently, the utilization of methods based on DL necessitates a substantial quantity of training data. Figure <a href="#S2.F6" title="Figure 6 ‣ 2.2.2 Local and Global Representation ‣ 2.2 Handcrafted Features with ML-Based Approach ‣ 2 RGB-Data Modality Based Action Recognition Methods ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> also demonstrates the year-wise end-to-end deep learning method developed by various researchers for the skeleton-based HAR systems. As shown, several notable models leveraging recurrent neural networks (RNN), CNN, and graph convolutional networks (GCN) have developed.</p>
</div>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>CNN-Based Methods</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">Skeleton data combined with ML methods provides efficient action recognition capabilities. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib226" title="" class="ltx_ref">226</a>]</cite> utilize the Kinect sensor to capture skeletal representations, enabling the recognition of actions based on body part movements. Skeleton data paired with CNNs offers robust action recognition.
As a result, in the work of Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite>, an advantage is found in combining handcrafted and DL-based features through the use of an enhanced trajectory. Additionally, the Trajectory-pooled Deep-Convolutional Descriptor (TpDD), also referred to as Two-stream ConvNets is employed. The construction of an effective descriptor is achieved through the learning of multi-scale convolutional feature maps within a deep architecture. Ding et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib227" title="" class="ltx_ref">227</a>]</cite> developed a CNNs-based model to extract high-level effective semantic features from RGB textured images obtained from using skeletal data. However, these methodologies have a lot of preprocessing steps and a chance to miss some effective information. Caetano et al. suggested SkeleMotion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib228" title="" class="ltx_ref">228</a>]</cite>, which offers a novel skeleton image representation as an alternative input for neural networks to address these issues. Researchers have explored solutions to the challenge of long-time dependence, especially considering that CNN did not extract long-distance motion information. To overcome this issue Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib229" title="" class="ltx_ref">229</a>]</cite> suggested a Subsequence Attention Network (SSAN) to improve the capture of long-term features. This network, combined with 3DCNN, uses skeleton data to record long-term features more effectively.</p>
</div>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>RNN-LSTM Based Methods</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p">Approaches relying on Recurrent Neural Networks with LSTM units (RNN-LSTM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib230" title="" class="ltx_ref">230</a>, <a href="#bib.bib231" title="" class="ltx_ref">231</a>]</cite> have garnered considerable popularity as a predominant DL methodology for skeleton-based action recognition. Moreover, these approaches have demonstrated exceptional proficiency in accomplishing video-based action recognition tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib232" title="" class="ltx_ref">232</a>, <a href="#bib.bib198" title="" class="ltx_ref">198</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib140" title="" class="ltx_ref">140</a>, <a href="#bib.bib199" title="" class="ltx_ref">199</a>, <a href="#bib.bib233" title="" class="ltx_ref">233</a>]</cite>. The spatio-temporal patterns of skeletons exhibit temporal evolutions. Consequently, these patterns can be effectively represented by memory cells within the structure of RNN-LSTM models, as proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib230" title="" class="ltx_ref">230</a>]</cite>. In a similar vein, Du et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib232" title="" class="ltx_ref">232</a>]</cite> introduced a hierarchical RNN approach to capture the long-term contextual information of skeletal data. This involved dividing the human skeleton into five distinct parts based on its physical structure. Subsequently, each lower-level part was represented using an RNN, and these representations were then integrated to form the final representation of higher-level parts, which facilitated action classification.
The problem related to gradient explosion and vanishing gradients occurs if the sequences are too long for actual training. To overcome this issue li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib234" title="" class="ltx_ref">234</a>]</cite> suggested an independent recurrent neural network (IndRNN) to regulate gradient backpropagation over time, allowing the network to capture long-term dependencies.
Shahroudy et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> introduced a model for human action learning using a part-aware LSTM. This model involves splitting the long-term memory of the entire motion into part-based cells and independently learning the long-term context of each body part. The network’s output is then formed by combining the independent body part context information. Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib140" title="" class="ltx_ref">140</a>]</cite> presented a spatio-temporal LSTM network named ST-LSTM, which aims at 3D action recognition from skeletal data. They proposed a technique called skeleton-based tree traversal to feed the structure of the skeletal data into a sequential LSTM network and improved the performance of ST-LSTM by incorporating additional trust gates.
In their recent work, Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib233" title="" class="ltx_ref">233</a>]</cite> directed their attention towards the selection of the most informative joints in the skeleton by employing a novel type of LSTM network called Global Context-Aware Attention (GCA-LSTM) to recognize actions based on 3D skeleton data. Two layers of LSTM were utilized in his study. The initial layer encoded the input sequences and produced a global context memory for these sequences. Simultaneously, the second layer carried out attention mechanisms over the input sequences with the support of the acquired global context memory. The resulting attention representation was subsequently employed to refine the global context. Numerous iterations of attention mechanisms were conducted, and the final global contextual information was employed in the task of action classification. Compared to the methodologies based on hand-crafted designed local features, the RNN-LSTM methodologies and their variations have demonstrated superior performance in recognition of actions.
Nevertheless, these methodologies tend to excessively emphasize the temporal information while neglecting the spatial information of skeletons <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib232" title="" class="ltx_ref">232</a>, <a href="#bib.bib198" title="" class="ltx_ref">198</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib140" title="" class="ltx_ref">140</a>, <a href="#bib.bib199" title="" class="ltx_ref">199</a>, <a href="#bib.bib233" title="" class="ltx_ref">233</a>]</cite>. RNN-LSTM methodologies continue to face difficulties in dealing with the intricate spatio-temporal variations of skeletal movements due to multiple issues, such as jitters and variability in movement speed. Another drawback of the RNN-LSTM networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib230" title="" class="ltx_ref">230</a>, <a href="#bib.bib231" title="" class="ltx_ref">231</a>]</cite> is their sole focus on modelling the overall temporal dynamics of actions, disregarding the detailed temporal dynamics. To address these limitations, in this investigation, a CNN-based methodology can extract discriminative characteristics of actions and model various temporal dynamics of skeleton sequences via the suggested Enhanced-SPMF representation, encompassing short-term, medium-term, and long-term actions.</p>
</div>
</section>
<section id="S3.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>GNN or GCN-Based Methods</h4>

<div id="S3.SS4.SSS3.p1" class="ltx_para">
<p id="S3.SS4.SSS3.p1.1" class="ltx_p">Graph convolutional neural networks (GCNNs) are powerful DL-based methods designed to perform non-Euclidean data. Unlike traditional CNNs and RNNs, which perform well with Euclidean data (such as images, text, and speech), they are unable to perform with non-Euclidean data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib235" title="" class="ltx_ref">235</a>, <a href="#bib.bib145" title="" class="ltx_ref">145</a>, <a href="#bib.bib236" title="" class="ltx_ref">236</a>, <a href="#bib.bib237" title="" class="ltx_ref">237</a>, <a href="#bib.bib238" title="" class="ltx_ref">238</a>, <a href="#bib.bib239" title="" class="ltx_ref">239</a>, <a href="#bib.bib240" title="" class="ltx_ref">240</a>, <a href="#bib.bib241" title="" class="ltx_ref">241</a>]</cite>. The GCN was first introduced by Gori et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib242" title="" class="ltx_ref">242</a>]</cite> in 2005 to handle graph data. GCNNs with skeleton data enable spatial dependencies to be captured for accurate action recognition. The human skeleton data, consisting of joint points and skeletal lines, can be viewed as non-Euclidean graph data. Therefore, GCNs are particularly suited for learning from such data. There are two main branches of GCNs: Spectral GCN and Spatial GCN.</p>
</div>
<div id="S3.SS4.SSS3.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Spectral GCNs based methods:</span>
Using and leveraging both eigenvalues and eigenvectors of the graph Laplacian matrix (GLM) to convert graph data from the temporal to the spatial domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib243" title="" class="ltx_ref">243</a>]</cite>, but this model is not computationally efficient. To address this issue, kipf et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib244" title="" class="ltx_ref">244</a>]</cite> enhanced the spectral GCN approach by allowing the filter operation of only one neighbour node to reduce the computational cost. While spectral GCNs have shown effectiveness in HAR tasks, their computational cost poses challenges when dealing with graphs.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Spatial GCN-based methods:</span> They are more efficient in terms of computational than spectral GCNs. Therefore, spatial GCNs have become the main focus in many GCN-based HAR approaches due to efficiency. Yan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite> developed the concept of ST-GCN, a model specifically designed for spatiotemporal data.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS4.SSS3.p3" class="ltx_para">
<p id="S3.SS4.SSS3.p3.1" class="ltx_p">As depicted in Figure <a href="#S3.F12" title="Figure 12 ‣ 3.4.3 GNN or GCN-Based Methods ‣ 3.4 End to End Deep Learning Based Approach ‣ 3 Skeleton Data Modality Based Action Recognition Method ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> the ST-GCN, bodily joints (such as joints in a human skeleton) serve as the vertices in the graph while the edges denote the connection between the bodily bones within the same frame.
Shi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib204" title="" class="ltx_ref">204</a>]</cite> developed two-stream adaptive GCN models to improve the flexibility of graph networks. This model allows for the use of the end-to-end approach to learning the graph’s topology within the model. By adopting a data-driven methodology, the 2sAGCN model becomes more adaptable to diverse data samples, increasing flexibility. Additionally, an attention mechanism is included to improve the robustness of the 2sAGCN model. For a further improvement to explore the enhancement of HAR methods, Shiraki et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib245" title="" class="ltx_ref">245</a>]</cite> proposed the spatiotemporal attentional graph (STA)- GCN to determine the challenge varying importance of joints across different human actions. Unlike traditional GCNs, STA-GCN takes into account both the significance and interrelationship of joints within the graph. Researchers have drawn inspiration from STA-GCN to further enhance GCN models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib246" title="" class="ltx_ref">246</a>, <a href="#bib.bib247" title="" class="ltx_ref">247</a>]</cite>. For instance, the shift-GCN model introduces the innovative shift-graph method to enhance the flexibility of the spatio-temporal graph’s (STG) receptive domain.
Additionally, the lightweight dot convolution technique is utilized to reduce the number of feature channels and make the model more efficient. Song et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib248" title="" class="ltx_ref">248</a>]</cite> present the residual-based GCN model to improve the performance of the model in terms of accuracy and computational efficiency for HAR. Similarly, Thakkar et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib249" title="" class="ltx_ref">249</a>]</cite> and Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib250" title="" class="ltx_ref">250</a>]</cite> presented methods to divide the human skeleton into separate body parts and they developed the partial-based graph convolutional network (PB-GCN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib249" title="" class="ltx_ref">249</a>]</cite>, which learns four subgraphs of the skeleton data. Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib250" title="" class="ltx_ref">250</a>]</cite> developed the spatio-temporal graph routing (STGR) scheme to better determine the connections between joints. These methods help improve the segmentation of body parts for HAR.</p>
</div>
<figure id="S3.F12" class="ltx_figure"><img src="/html/2409.09678/assets/x4.jpg" id="S3.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="261" height="65" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Skeleton-based HAR using ST-GCN.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Sensor Based HAR</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Sensor-based HAR has gained significant attention due to wearable technology and its applications in various domains. These include health monitoring, industrial safety, sports training, and more <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib251" title="" class="ltx_ref">251</a>]</cite>. Unlike computer vision-based or WIFI-based HAR, wearable sensors offer advantages such as privacy, user acceptance, and independence from environmental factors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib252" title="" class="ltx_ref">252</a>]</cite>. Challenges in sensor-based HAR include diverse data collection, handling missing values, and complex activity recognition. Wearable devices use sensors like accelerometers and gyroscopes to identify human activities, but feature extraction and model training remain challenging. The challenges with machine learning approaches rely on manual feature extraction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib253" title="" class="ltx_ref">253</a>]</cite> while the DL approaches now enable automatic feature extraction from raw sensor data, leading to superior results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib252" title="" class="ltx_ref">252</a>]</cite>. Overall, sensor-based HAR holds promise for improving healthcare and safety <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib254" title="" class="ltx_ref">254</a>, <a href="#bib.bib255" title="" class="ltx_ref">255</a>, <a href="#bib.bib256" title="" class="ltx_ref">256</a>, <a href="#bib.bib257" title="" class="ltx_ref">257</a>]</cite>.
<br class="ltx_break">Table <a href="#S4.T6" title="Table 6 ‣ 4 Sensor Based HAR ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> summarizes various existing works based on sensor modality for HAR using traditional ML and DL techniques, including the author name, year, datasets, modality sensor names, methods, classifier, and performance accuracy. As mentioned in Table, researchers have enhanced HAR classification performance by improving ML feature engineering, and some researchers have developed advanced DL models like CNN and LSTM for automatic feature extraction. Most studies utilized datasets from multiple sensor types placed at different body positions. Additionally, we summarize several publically available datasets in Table <a href="#S4.T5" title="Table 5 ‣ 4 Sensor Based HAR ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, including year, sensor modalities, number of sensors, number of participants, number of activities, activity categories, and latest performance accuracy.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Databases for Sensor Modality</figcaption>
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.1.1.1.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Dataset</td>
</tr>
<tr id="S4.T5.1.1.1.1.1.2" class="ltx_tr">
<td id="S4.T5.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Names</td>
</tr>
</table>
</th>
<th id="S4.T5.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.1.1.2.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Year</td>
</tr>
</table>
</th>
<th id="S4.T5.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.1.1.3.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Sensor</td>
</tr>
<tr id="S4.T5.1.1.1.3.1.2" class="ltx_tr">
<td id="S4.T5.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">modalities</td>
</tr>
</table>
</th>
<th id="S4.T5.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.1.1.4.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">No. of</td>
</tr>
<tr id="S4.T5.1.1.1.4.1.2" class="ltx_tr">
<td id="S4.T5.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">sensors</td>
</tr>
</table>
</th>
<th id="S4.T5.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.1.1.5.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">No. of</td>
</tr>
<tr id="S4.T5.1.1.1.5.1.2" class="ltx_tr">
<td id="S4.T5.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">people</td>
</tr>
</table>
</th>
<th id="S4.T5.1.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.1.1.6.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">No. of</td>
</tr>
<tr id="S4.T5.1.1.1.6.1.2" class="ltx_tr">
<td id="S4.T5.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Activities</td>
</tr>
</table>
</th>
<th id="S4.T5.1.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.1.1.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.1.1.7.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Activity</td>
</tr>
<tr id="S4.T5.1.1.1.7.1.2" class="ltx_tr">
<td id="S4.T5.1.1.1.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Categories</td>
</tr>
</table>
</th>
<th id="S4.T5.1.1.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.1.1.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.1.1.8.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Latest</td>
</tr>
<tr id="S4.T5.1.1.1.8.1.2" class="ltx_tr">
<td id="S4.T5.1.1.1.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Performances</td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.2.1" class="ltx_tr">
<td id="S4.T5.1.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">HHAR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib258" title="" class="ltx_ref">258</a>]</cite>
</td>
<td id="S4.T5.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2015</td>
<td id="S4.T5.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer, Gyroscope</td>
<td id="S4.T5.1.2.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">36</td>
<td id="S4.T5.1.2.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">9</td>
<td id="S4.T5.1.2.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">6</td>
<td id="S4.T5.1.2.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.2.1.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.2.1.7.1.1" class="ltx_tr">
<td id="S4.T5.1.2.1.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Daily living activity,</td>
</tr>
<tr id="S4.T5.1.2.1.7.1.2" class="ltx_tr">
<td id="S4.T5.1.2.1.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Sports fitness activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.2.1.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.2.1.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.2.1.8.1.1" class="ltx_tr">
<td id="S4.T5.1.2.1.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">99.99%
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib259" title="" class="ltx_ref">259</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.3.2" class="ltx_tr">
<td id="S4.T5.1.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">MHEALTH<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib260" title="" class="ltx_ref">260</a>]</cite>
</td>
<td id="S4.T5.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2014</td>
<td id="S4.T5.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.3.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.3.2.3.1.1" class="ltx_tr">
<td id="S4.T5.1.3.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer, Gyroscope,</td>
</tr>
<tr id="S4.T5.1.3.2.3.1.2" class="ltx_tr">
<td id="S4.T5.1.3.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Magnetometer,</td>
</tr>
<tr id="S4.T5.1.3.2.3.1.3" class="ltx_tr">
<td id="S4.T5.1.3.2.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Electrocardiogram</td>
</tr>
</table>
</td>
<td id="S4.T5.1.3.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">3</td>
<td id="S4.T5.1.3.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">10</td>
<td id="S4.T5.1.3.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">12</td>
<td id="S4.T5.1.3.2.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.3.2.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.3.2.7.1.1" class="ltx_tr">
<td id="S4.T5.1.3.2.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Atomic activity,</td>
</tr>
<tr id="S4.T5.1.3.2.7.1.2" class="ltx_tr">
<td id="S4.T5.1.3.2.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Daily living activity,</td>
</tr>
<tr id="S4.T5.1.3.2.7.1.3" class="ltx_tr">
<td id="S4.T5.1.3.2.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Sports fitness activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.3.2.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.3.2.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.3.2.8.1.1" class="ltx_tr">
<td id="S4.T5.1.3.2.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">97.83% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib261" title="" class="ltx_ref">261</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.4.3" class="ltx_tr">
<td id="S4.T5.1.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">OPPT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib262" title="" class="ltx_ref">262</a>]</cite>
</td>
<td id="S4.T5.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2013</td>
<td id="S4.T5.1.4.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.4.3.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.4.3.3.1.1" class="ltx_tr">
<td id="S4.T5.1.4.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Acceleration, Rate of Turn</td>
</tr>
<tr id="S4.T5.1.4.3.3.1.2" class="ltx_tr">
<td id="S4.T5.1.4.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Magnetic field, Reed switches</td>
</tr>
</table>
</td>
<td id="S4.T5.1.4.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
40</td>
<td id="S4.T5.1.4.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">4</td>
<td id="S4.T5.1.4.3.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">17</td>
<td id="S4.T5.1.4.3.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.4.3.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.4.3.7.1.1" class="ltx_tr">
<td id="S4.T5.1.4.3.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Daily living activity,</td>
</tr>
<tr id="S4.T5.1.4.3.7.1.2" class="ltx_tr">
<td id="S4.T5.1.4.3.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Composite activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.4.3.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.4.3.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.4.3.8.1.1" class="ltx_tr">
<td id="S4.T5.1.4.3.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">100% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib263" title="" class="ltx_ref">263</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.5.4" class="ltx_tr">
<td id="S4.T5.1.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">WISDM<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib264" title="" class="ltx_ref">264</a>]</cite>
</td>
<td id="S4.T5.1.5.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2011</td>
<td id="S4.T5.1.5.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer, Gyroscopes</td>
<td id="S4.T5.1.5.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1</td>
<td id="S4.T5.1.5.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">33</td>
<td id="S4.T5.1.5.4.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">6</td>
<td id="S4.T5.1.5.4.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.5.4.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.5.4.7.1.1" class="ltx_tr">
<td id="S4.T5.1.5.4.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Daily living activity,</td>
</tr>
<tr id="S4.T5.1.5.4.7.1.2" class="ltx_tr">
<td id="S4.T5.1.5.4.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Sports fitness activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.5.4.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.5.4.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.5.4.8.1.1" class="ltx_tr">
<td id="S4.T5.1.5.4.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">97.8% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib265" title="" class="ltx_ref">265</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.6.5" class="ltx_tr">
<td id="S4.T5.1.6.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">UCIHAR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib266" title="" class="ltx_ref">266</a>]</cite>
</td>
<td id="S4.T5.1.6.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2013</td>
<td id="S4.T5.1.6.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer, Gyroscope</td>
<td id="S4.T5.1.6.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1</td>
<td id="S4.T5.1.6.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">30</td>
<td id="S4.T5.1.6.5.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">6</td>
<td id="S4.T5.1.6.5.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Daily living activity</td>
<td id="S4.T5.1.6.5.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
</tr>
<tr id="S4.T5.1.7.6" class="ltx_tr">
<td id="S4.T5.1.7.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">PAMAP2<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib267" title="" class="ltx_ref">267</a>]</cite>
</td>
<td id="S4.T5.1.7.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2012</td>
<td id="S4.T5.1.7.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.7.6.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.7.6.3.1.1" class="ltx_tr">
<td id="S4.T5.1.7.6.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer, Gyroscope,</td>
</tr>
<tr id="S4.T5.1.7.6.3.1.2" class="ltx_tr">
<td id="S4.T5.1.7.6.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Magnetometer,</td>
</tr>
<tr id="S4.T5.1.7.6.3.1.3" class="ltx_tr">
<td id="S4.T5.1.7.6.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Temperature</td>
</tr>
</table>
</td>
<td id="S4.T5.1.7.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">4</td>
<td id="S4.T5.1.7.6.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">9</td>
<td id="S4.T5.1.7.6.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">18</td>
<td id="S4.T5.1.7.6.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.7.6.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.7.6.7.1.1" class="ltx_tr">
<td id="S4.T5.1.7.6.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Daily living activity,</td>
</tr>
<tr id="S4.T5.1.7.6.7.1.2" class="ltx_tr">
<td id="S4.T5.1.7.6.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Sports fitness activity,</td>
</tr>
<tr id="S4.T5.1.7.6.7.1.3" class="ltx_tr">
<td id="S4.T5.1.7.6.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Composite activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.7.6.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.7.6.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.7.6.8.1.1" class="ltx_tr">
<td id="S4.T5.1.7.6.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">94.72% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib268" title="" class="ltx_ref">268</a>]</cite>
</td>
</tr>
<tr id="S4.T5.1.7.6.8.1.2" class="ltx_tr">
<td id="S4.T5.1.7.6.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">82.12% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib263" title="" class="ltx_ref">263</a>]</cite>
</td>
</tr>
<tr id="S4.T5.1.7.6.8.1.3" class="ltx_tr">
<td id="S4.T5.1.7.6.8.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">90.27% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib265" title="" class="ltx_ref">265</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.8.7" class="ltx_tr">
<td id="S4.T5.1.8.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">DSADS<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib269" title="" class="ltx_ref">269</a>]</cite>
</td>
<td id="S4.T5.1.8.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2010</td>
<td id="S4.T5.1.8.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.8.7.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.8.7.3.1.1" class="ltx_tr">
<td id="S4.T5.1.8.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer Gyroscope</td>
</tr>
<tr id="S4.T5.1.8.7.3.1.2" class="ltx_tr">
<td id="S4.T5.1.8.7.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Magnetometer</td>
</tr>
</table>
</td>
<td id="S4.T5.1.8.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">45</td>
<td id="S4.T5.1.8.7.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">8</td>
<td id="S4.T5.1.8.7.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">19</td>
<td id="S4.T5.1.8.7.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.8.7.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.8.7.7.1.1" class="ltx_tr">
<td id="S4.T5.1.8.7.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Daily living activity,</td>
</tr>
<tr id="S4.T5.1.8.7.7.1.2" class="ltx_tr">
<td id="S4.T5.1.8.7.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Sports fitness activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.8.7.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.8.7.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.8.7.8.1.1" class="ltx_tr">
<td id="S4.T5.1.8.7.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">99.48%<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib270" title="" class="ltx_ref">270</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.9.8" class="ltx_tr">
<td id="S4.T5.1.9.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RealWorld<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib271" title="" class="ltx_ref">271</a>]</cite>
</td>
<td id="S4.T5.1.9.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2016</td>
<td id="S4.T5.1.9.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Acceleration</td>
<td id="S4.T5.1.9.8.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">7</td>
<td id="S4.T5.1.9.8.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">15</td>
<td id="S4.T5.1.9.8.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">8</td>
<td id="S4.T5.1.9.8.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.9.8.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.9.8.7.1.1" class="ltx_tr">
<td id="S4.T5.1.9.8.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Daily living activity,</td>
</tr>
<tr id="S4.T5.1.9.8.7.1.2" class="ltx_tr">
<td id="S4.T5.1.9.8.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Sports fitness activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.9.8.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.9.8.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.9.8.8.1.1" class="ltx_tr">
<td id="S4.T5.1.9.8.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">95% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib272" title="" class="ltx_ref">272</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.10.9" class="ltx_tr">
<td id="S4.T5.1.10.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Exer. Activity<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib273" title="" class="ltx_ref">273</a>]</cite>
</td>
<td id="S4.T5.1.10.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2013</td>
<td id="S4.T5.1.10.9.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.10.9.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.10.9.3.1.1" class="ltx_tr">
<td id="S4.T5.1.10.9.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer, Gyroscope</td>
</tr>
</table>
</td>
<td id="S4.T5.1.10.9.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">3</td>
<td id="S4.T5.1.10.9.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">20</td>
<td id="S4.T5.1.10.9.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">10</td>
<td id="S4.T5.1.10.9.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.10.9.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.10.9.7.1.1" class="ltx_tr">
<td id="S4.T5.1.10.9.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Sports fitness activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.10.9.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.10.9.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.10.9.8.1.1" class="ltx_tr">
<td id="S4.T5.1.10.9.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.11.10" class="ltx_tr">
<td id="S4.T5.1.11.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">UTD-MHAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>
</td>
<td id="S4.T5.1.11.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2015</td>
<td id="S4.T5.1.11.10.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.11.10.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.11.10.3.1.1" class="ltx_tr">
<td id="S4.T5.1.11.10.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer Gyroscope</td>
</tr>
<tr id="S4.T5.1.11.10.3.1.2" class="ltx_tr">
<td id="S4.T5.1.11.10.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">RGB camera, depth camera</td>
</tr>
</table>
</td>
<td id="S4.T5.1.11.10.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">3</td>
<td id="S4.T5.1.11.10.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">8</td>
<td id="S4.T5.1.11.10.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">27</td>
<td id="S4.T5.1.11.10.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.11.10.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.11.10.7.1.1" class="ltx_tr">
<td id="S4.T5.1.11.10.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Daily living activity,</td>
</tr>
<tr id="S4.T5.1.11.10.7.1.2" class="ltx_tr">
<td id="S4.T5.1.11.10.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Sports fitness activity</td>
</tr>
<tr id="S4.T5.1.11.10.7.1.3" class="ltx_tr">
<td id="S4.T5.1.11.10.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Composite activity</td>
</tr>
<tr id="S4.T5.1.11.10.7.1.4" class="ltx_tr">
<td id="S4.T5.1.11.10.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Atomic activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.11.10.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.11.10.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.11.10.8.1.1" class="ltx_tr">
<td id="S4.T5.1.11.10.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">76.35% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib274" title="" class="ltx_ref">274</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.12.11" class="ltx_tr">
<td id="S4.T5.1.12.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Shoaib <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib275" title="" class="ltx_ref">275</a>]</cite>
</td>
<td id="S4.T5.1.12.11.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2014</td>
<td id="S4.T5.1.12.11.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.12.11.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.12.11.3.1.1" class="ltx_tr">
<td id="S4.T5.1.12.11.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer Gyroscope</td>
</tr>
</table>
</td>
<td id="S4.T5.1.12.11.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">5</td>
<td id="S4.T5.1.12.11.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">10</td>
<td id="S4.T5.1.12.11.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">7</td>
<td id="S4.T5.1.12.11.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.12.11.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.12.11.7.1.1" class="ltx_tr">
<td id="S4.T5.1.12.11.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Daily living activity,</td>
</tr>
<tr id="S4.T5.1.12.11.7.1.2" class="ltx_tr">
<td id="S4.T5.1.12.11.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Sports fitness activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.12.11.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.12.11.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.12.11.8.1.1" class="ltx_tr">
<td id="S4.T5.1.12.11.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">99.86% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib276" title="" class="ltx_ref">276</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.13.12" class="ltx_tr">
<td id="S4.T5.1.13.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">TUD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib277" title="" class="ltx_ref">277</a>]</cite>
</td>
<td id="S4.T5.1.13.12.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2008</td>
<td id="S4.T5.1.13.12.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.13.12.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.13.12.3.1.1" class="ltx_tr">
<td id="S4.T5.1.13.12.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer</td>
</tr>
</table>
</td>
<td id="S4.T5.1.13.12.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2</td>
<td id="S4.T5.1.13.12.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1</td>
<td id="S4.T5.1.13.12.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">34</td>
<td id="S4.T5.1.13.12.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.13.12.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.13.12.7.1.1" class="ltx_tr">
<td id="S4.T5.1.13.12.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Daily living activity,</td>
</tr>
<tr id="S4.T5.1.13.12.7.1.2" class="ltx_tr">
<td id="S4.T5.1.13.12.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Sports fitness</td>
</tr>
<tr id="S4.T5.1.13.12.7.1.3" class="ltx_tr">
<td id="S4.T5.1.13.12.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Composite activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.13.12.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.13.12.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.13.12.8.1.1" class="ltx_tr">
<td id="S4.T5.1.13.12.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.14.13" class="ltx_tr">
<td id="S4.T5.1.14.13.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SHAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib278" title="" class="ltx_ref">278</a>]</cite>
</td>
<td id="S4.T5.1.14.13.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2017</td>
<td id="S4.T5.1.14.13.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.14.13.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.14.13.3.1.1" class="ltx_tr">
<td id="S4.T5.1.14.13.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer</td>
</tr>
</table>
</td>
<td id="S4.T5.1.14.13.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2</td>
<td id="S4.T5.1.14.13.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">30</td>
<td id="S4.T5.1.14.13.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">17</td>
<td id="S4.T5.1.14.13.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.14.13.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.14.13.7.1.1" class="ltx_tr">
<td id="S4.T5.1.14.13.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Daily living activity,</td>
</tr>
<tr id="S4.T5.1.14.13.7.1.2" class="ltx_tr">
<td id="S4.T5.1.14.13.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Sports fitness activity</td>
</tr>
<tr id="S4.T5.1.14.13.7.1.3" class="ltx_tr">
<td id="S4.T5.1.14.13.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Atomic activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.14.13.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.14.13.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.14.13.8.1.1" class="ltx_tr">
<td id="S4.T5.1.14.13.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">82.79%<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib279" title="" class="ltx_ref">279</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.15.14" class="ltx_tr">
<td id="S4.T5.1.15.14.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">USC-HAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib280" title="" class="ltx_ref">280</a>]</cite>
</td>
<td id="S4.T5.1.15.14.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2012</td>
<td id="S4.T5.1.15.14.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.15.14.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.15.14.3.1.1" class="ltx_tr">
<td id="S4.T5.1.15.14.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer, Gyroscope</td>
</tr>
</table>
</td>
<td id="S4.T5.1.15.14.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1</td>
<td id="S4.T5.1.15.14.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">14</td>
<td id="S4.T5.1.15.14.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">12</td>
<td id="S4.T5.1.15.14.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.15.14.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.15.14.7.1.1" class="ltx_tr">
<td id="S4.T5.1.15.14.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Daily living activity,</td>
</tr>
<tr id="S4.T5.1.15.14.7.1.2" class="ltx_tr">
<td id="S4.T5.1.15.14.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Sports fitness</td>
</tr>
<tr id="S4.T5.1.15.14.7.1.3" class="ltx_tr">
<td id="S4.T5.1.15.14.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">activity activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.15.14.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.15.14.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.15.14.8.1.1" class="ltx_tr">
<td id="S4.T5.1.15.14.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">97.25% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib279" title="" class="ltx_ref">279</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.16.15" class="ltx_tr">
<td id="S4.T5.1.16.15.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Mobi-Act <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib281" title="" class="ltx_ref">281</a>]</cite>
</td>
<td id="S4.T5.1.16.15.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2016</td>
<td id="S4.T5.1.16.15.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.16.15.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.16.15.3.1.1" class="ltx_tr">
<td id="S4.T5.1.16.15.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer, Gyroscope</td>
</tr>
<tr id="S4.T5.1.16.15.3.1.2" class="ltx_tr">
<td id="S4.T5.1.16.15.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">orientation sensors</td>
</tr>
</table>
</td>
<td id="S4.T5.1.16.15.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1</td>
<td id="S4.T5.1.16.15.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">50</td>
<td id="S4.T5.1.16.15.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">13</td>
<td id="S4.T5.1.16.15.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.16.15.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.16.15.7.1.1" class="ltx_tr">
<td id="S4.T5.1.16.15.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Daily living activity,</td>
</tr>
<tr id="S4.T5.1.16.15.7.1.2" class="ltx_tr">
<td id="S4.T5.1.16.15.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Atomic activity activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.16.15.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.16.15.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.16.15.8.1.1" class="ltx_tr">
<td id="S4.T5.1.16.15.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">75.87% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib282" title="" class="ltx_ref">282</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.17.16" class="ltx_tr">
<td id="S4.T5.1.17.16.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Motion Sense <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib283" title="" class="ltx_ref">283</a>]</cite>
</td>
<td id="S4.T5.1.17.16.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2018</td>
<td id="S4.T5.1.17.16.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.17.16.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.17.16.3.1.1" class="ltx_tr">
<td id="S4.T5.1.17.16.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer, Gyroscope</td>
</tr>
</table>
</td>
<td id="S4.T5.1.17.16.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1</td>
<td id="S4.T5.1.17.16.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">24</td>
<td id="S4.T5.1.17.16.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">6</td>
<td id="S4.T5.1.17.16.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.17.16.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.17.16.7.1.1" class="ltx_tr">
<td id="S4.T5.1.17.16.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Daily living activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.17.16.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.17.16.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.17.16.8.1.1" class="ltx_tr">
<td id="S4.T5.1.17.16.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">95.35%<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib284" title="" class="ltx_ref">284</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.18.17" class="ltx_tr">
<td id="S4.T5.1.18.17.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">van Kasteren <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib285" title="" class="ltx_ref">285</a>]</cite>
</td>
<td id="S4.T5.1.18.17.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2011</td>
<td id="S4.T5.1.18.17.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.18.17.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.18.17.3.1.1" class="ltx_tr">
<td id="S4.T5.1.18.17.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">switches, contacts</td>
</tr>
<tr id="S4.T5.1.18.17.3.1.2" class="ltx_tr">
<td id="S4.T5.1.18.17.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">passive infrared (PIR)</td>
</tr>
</table>
</td>
<td id="S4.T5.1.18.17.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">14</td>
<td id="S4.T5.1.18.17.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1</td>
<td id="S4.T5.1.18.17.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">10</td>
<td id="S4.T5.1.18.17.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.18.17.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.18.17.7.1.1" class="ltx_tr">
<td id="S4.T5.1.18.17.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Daily living activity</td>
</tr>
<tr id="S4.T5.1.18.17.7.1.2" class="ltx_tr">
<td id="S4.T5.1.18.17.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Composite activity</td>
</tr>
<tr id="S4.T5.1.18.17.7.1.3" class="ltx_tr">
<td id="S4.T5.1.18.17.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.18.17.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.18.17.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.18.17.8.1.1" class="ltx_tr">
<td id="S4.T5.1.18.17.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.19.18" class="ltx_tr">
<td id="S4.T5.1.19.18.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">CASAS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib286" title="" class="ltx_ref">286</a>]</cite>
</td>
<td id="S4.T5.1.19.18.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2012</td>
<td id="S4.T5.1.19.18.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.19.18.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.19.18.3.1.1" class="ltx_tr">
<td id="S4.T5.1.19.18.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Temperature</td>
</tr>
<tr id="S4.T5.1.19.18.3.1.2" class="ltx_tr">
<td id="S4.T5.1.19.18.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Infrared motion/light sensor</td>
</tr>
</table>
</td>
<td id="S4.T5.1.19.18.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">52</td>
<td id="S4.T5.1.19.18.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1</td>
<td id="S4.T5.1.19.18.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">7</td>
<td id="S4.T5.1.19.18.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.19.18.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.19.18.7.1.1" class="ltx_tr">
<td id="S4.T5.1.19.18.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Daily living activity</td>
</tr>
<tr id="S4.T5.1.19.18.7.1.2" class="ltx_tr">
<td id="S4.T5.1.19.18.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Composite activity</td>
</tr>
<tr id="S4.T5.1.19.18.7.1.3" class="ltx_tr">
<td id="S4.T5.1.19.18.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.19.18.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.19.18.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.19.18.8.1.1" class="ltx_tr">
<td id="S4.T5.1.19.18.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">88.4% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib287" title="" class="ltx_ref">287</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.20.19" class="ltx_tr">
<td id="S4.T5.1.20.19.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Skoda <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib288" title="" class="ltx_ref">288</a>]</cite>
</td>
<td id="S4.T5.1.20.19.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2008</td>
<td id="S4.T5.1.20.19.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.20.19.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.20.19.3.1.1" class="ltx_tr">
<td id="S4.T5.1.20.19.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer</td>
</tr>
</table>
</td>
<td id="S4.T5.1.20.19.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">19</td>
<td id="S4.T5.1.20.19.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1</td>
<td id="S4.T5.1.20.19.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">10</td>
<td id="S4.T5.1.20.19.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.20.19.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.20.19.7.1.1" class="ltx_tr">
<td id="S4.T5.1.20.19.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Daily living activity</td>
</tr>
<tr id="S4.T5.1.20.19.7.1.2" class="ltx_tr">
<td id="S4.T5.1.20.19.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Composite activity</td>
</tr>
<tr id="S4.T5.1.20.19.7.1.3" class="ltx_tr">
<td id="S4.T5.1.20.19.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.20.19.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.20.19.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.20.19.8.1.1" class="ltx_tr">
<td id="S4.T5.1.20.19.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">97%<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib289" title="" class="ltx_ref">289</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.21.20" class="ltx_tr">
<td id="S4.T5.1.21.20.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Widar3.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib290" title="" class="ltx_ref">290</a>]</cite>
</td>
<td id="S4.T5.1.21.20.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2019</td>
<td id="S4.T5.1.21.20.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.21.20.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.21.20.3.1.1" class="ltx_tr">
<td id="S4.T5.1.21.20.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Wi-Fi</td>
</tr>
</table>
</td>
<td id="S4.T5.1.21.20.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">7</td>
<td id="S4.T5.1.21.20.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1</td>
<td id="S4.T5.1.21.20.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">6</td>
<td id="S4.T5.1.21.20.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.21.20.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.21.20.7.1.1" class="ltx_tr">
<td id="S4.T5.1.21.20.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Atomic activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.21.20.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.21.20.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.21.20.8.1.1" class="ltx_tr">
<td id="S4.T5.1.21.20.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">82.18%<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib291" title="" class="ltx_ref">291</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.22.21" class="ltx_tr">
<td id="S4.T5.1.22.21.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">UCI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib266" title="" class="ltx_ref">266</a>]</cite>
</td>
<td id="S4.T5.1.22.21.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2013</td>
<td id="S4.T5.1.22.21.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.22.21.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.22.21.3.1.1" class="ltx_tr">
<td id="S4.T5.1.22.21.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer, Gyroscope</td>
</tr>
</table>
</td>
<td id="S4.T5.1.22.21.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2</td>
<td id="S4.T5.1.22.21.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">30</td>
<td id="S4.T5.1.22.21.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">6</td>
<td id="S4.T5.1.22.21.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.22.21.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.22.21.7.1.1" class="ltx_tr">
<td id="S4.T5.1.22.21.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Human activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.22.21.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.22.21.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.22.21.8.1.1" class="ltx_tr">
<td id="S4.T5.1.22.21.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">95.90% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib268" title="" class="ltx_ref">268</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.23.22" class="ltx_tr">
<td id="S4.T5.1.23.22.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">HAPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib292" title="" class="ltx_ref">292</a>]</cite>
</td>
<td id="S4.T5.1.23.22.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2016</td>
<td id="S4.T5.1.23.22.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.23.22.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.23.22.3.1.1" class="ltx_tr">
<td id="S4.T5.1.23.22.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer, Gyroscope</td>
</tr>
</table>
</td>
<td id="S4.T5.1.23.22.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1</td>
<td id="S4.T5.1.23.22.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">30</td>
<td id="S4.T5.1.23.22.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">12</td>
<td id="S4.T5.1.23.22.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.23.22.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.23.22.7.1.1" class="ltx_tr">
<td id="S4.T5.1.23.22.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Human activity</td>
</tr>
</table>
</td>
<td id="S4.T5.1.23.22.8" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T5.1.23.22.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.23.22.8.1.1" class="ltx_tr">
<td id="S4.T5.1.23.22.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">92.14% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib268" title="" class="ltx_ref">268</a>]</cite>
</td>
</tr>
<tr id="S4.T5.1.23.22.8.1.2" class="ltx_tr">
<td id="S4.T5.1.23.22.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">98.73%<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib276" title="" class="ltx_ref">276</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Sensor data modality-based HAR models and performance. </figcaption>
<table id="S4.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.1.1.1" class="ltx_tr">
<th id="S4.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.1.1.1.1.1" class="ltx_text ltx_font_bold">Author</span></th>
<th id="S4.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.1.1.1.2.1" class="ltx_text ltx_font_bold">Year</span></th>
<th id="S4.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.1.1.1.3.1" class="ltx_text ltx_font_bold">Dataset Name</span></th>
<th id="S4.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.1.1.4.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.1.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold">Modality</span></td>
</tr>
<tr id="S4.T6.1.1.1.4.1.2" class="ltx_tr">
<td id="S4.T6.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.1.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold">Sensor</span></td>
</tr>
<tr id="S4.T6.1.1.1.4.1.3" class="ltx_tr">
<td id="S4.T6.1.1.1.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.1.1.1.4.1.3.1.1" class="ltx_text ltx_font_bold">Name</span></td>
</tr>
</table>
</th>
<th id="S4.T6.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.1.1.1.5.1" class="ltx_text ltx_font_bold">Methods</span></th>
<th id="S4.T6.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.1.1.1.6.1" class="ltx_text ltx_font_bold">Classifier</span></th>
<th id="S4.T6.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.1.1.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.1.1.7.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T6.1.1.1.7.1.1.1.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
</tr>
<tr id="S4.T6.1.1.1.7.1.2" class="ltx_tr">
<td id="S4.T6.1.1.1.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">%</td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.1.2.1" class="ltx_tr">
<td id="S4.T6.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Ignatov et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib293" title="" class="ltx_ref">293</a>]</cite>
</td>
<td id="S4.T6.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2018</td>
<td id="S4.T6.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.2.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.2.1.3.1.1" class="ltx_tr">
<td id="S4.T6.1.2.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">WISDM</td>
</tr>
<tr id="S4.T6.1.2.1.3.1.2" class="ltx_tr">
<td id="S4.T6.1.2.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCI HAR</td>
</tr>
</table>
</td>
<td id="S4.T6.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">IMU Sensor</td>
<td id="S4.T6.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">CNN</td>
<td id="S4.T6.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S4.T6.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.2.1.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.2.1.7.1.1" class="ltx_tr">
<td id="S4.T6.1.2.1.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">93.32</td>
</tr>
<tr id="S4.T6.1.2.1.7.1.2" class="ltx_tr">
<td id="S4.T6.1.2.1.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.63</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T6.1.3.2" class="ltx_tr">
<td id="S4.T6.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Jain et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib294" title="" class="ltx_ref">294</a>]</cite>
</td>
<td id="S4.T6.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2018</td>
<td id="S4.T6.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.3.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.3.2.3.1.1" class="ltx_tr">
<td id="S4.T6.1.3.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCI HAR</td>
</tr>
</table>
</td>
<td id="S4.T6.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">IMU Sensor</td>
<td id="S4.T6.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Fusion based</td>
<td id="S4.T6.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SVM,KNN</td>
<td id="S4.T6.1.3.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.3.2.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.3.2.7.1.1" class="ltx_tr">
<td id="S4.T6.1.3.2.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.12</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T6.1.4.3" class="ltx_tr">
<td id="S4.T6.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib295" title="" class="ltx_ref">295</a>]</cite>
</td>
<td id="S4.T6.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2019</td>
<td id="S4.T6.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.4.3.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.4.3.3.1.1" class="ltx_tr">
<td id="S4.T6.1.4.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">MHEALTH</td>
</tr>
<tr id="S4.T6.1.4.3.3.1.2" class="ltx_tr">
<td id="S4.T6.1.4.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">PAMAP2</td>
</tr>
<tr id="S4.T6.1.4.3.3.1.3" class="ltx_tr">
<td id="S4.T6.1.4.3.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCI HAR</td>
</tr>
</table>
</td>
<td id="S4.T6.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">IMU</td>
<td id="S4.T6.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">CNN</td>
<td id="S4.T6.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S4.T6.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.4.3.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.4.3.7.1.1" class="ltx_tr">
<td id="S4.T6.1.4.3.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">94.05, 83.42</td>
</tr>
<tr id="S4.T6.1.4.3.7.1.2" class="ltx_tr">
<td id="S4.T6.1.4.3.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">81.32</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T6.1.5.4" class="ltx_tr">
<td id="S4.T6.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Alawneh et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib296" title="" class="ltx_ref">296</a>]</cite>
</td>
<td id="S4.T6.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2020</td>
<td id="S4.T6.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.5.4.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.5.4.3.1.1" class="ltx_tr">
<td id="S4.T6.1.5.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UniMib Shar</td>
</tr>
<tr id="S4.T6.1.5.4.3.1.2" class="ltx_tr">
<td id="S4.T6.1.5.4.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">WISDM</td>
</tr>
</table>
</td>
<td id="S4.T6.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.5.4.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.5.4.4.1.1" class="ltx_tr">
<td id="S4.T6.1.5.4.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer</td>
</tr>
<tr id="S4.T6.1.5.4.4.1.2" class="ltx_tr">
<td id="S4.T6.1.5.4.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">IMU Senso</td>
</tr>
</table> r</td>
<td id="S4.T6.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Bi-LSTM</td>
<td id="S4.T6.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S4.T6.1.5.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.5.4.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.5.4.7.1.1" class="ltx_tr">
<td id="S4.T6.1.5.4.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">99.25</td>
</tr>
<tr id="S4.T6.1.5.4.7.1.2" class="ltx_tr">
<td id="S4.T6.1.5.4.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98.11</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T6.1.6.5" class="ltx_tr">
<td id="S4.T6.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Lin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib297" title="" class="ltx_ref">297</a>]</cite>
</td>
<td id="S4.T6.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2020</td>
<td id="S4.T6.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.6.5.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.6.5.3.1.1" class="ltx_tr">
<td id="S4.T6.1.6.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Smartwach</td>
</tr>
</table>
</td>
<td id="S4.T6.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.6.5.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.6.5.4.1.1" class="ltx_tr">
<td id="S4.T6.1.6.5.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer</td>
</tr>
<tr id="S4.T6.1.6.5.4.1.2" class="ltx_tr">
<td id="S4.T6.1.6.5.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">gyroscope</td>
</tr>
</table>
</td>
<td id="S4.T6.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Dilated CNN</td>
<td id="S4.T6.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S4.T6.1.6.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.6.5.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.6.5.7.1.1" class="ltx_tr">
<td id="S4.T6.1.6.5.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">95.49</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T6.1.7.6" class="ltx_tr">
<td id="S4.T6.1.7.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib298" title="" class="ltx_ref">298</a>]</cite>
</td>
<td id="S4.T6.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2020</td>
<td id="S4.T6.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.7.6.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.7.6.3.1.1" class="ltx_tr">
<td id="S4.T6.1.7.6.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">WaFi CSI</td>
</tr>
</table>
</td>
<td id="S4.T6.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Wafi signal</td>
<td id="S4.T6.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Dense-LSTM</td>
<td id="S4.T6.1.7.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S4.T6.1.7.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.7.6.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.7.6.7.1.1" class="ltx_tr">
<td id="S4.T6.1.7.6.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">90.0</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T6.1.8.7" class="ltx_tr">
<td id="S4.T6.1.8.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Nadeem et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib299" title="" class="ltx_ref">299</a>]</cite>
</td>
<td id="S4.T6.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2021</td>
<td id="S4.T6.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.8.7.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.8.7.3.1.1" class="ltx_tr">
<td id="S4.T6.1.8.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">WISDM</td>
</tr>
<tr id="S4.T6.1.8.7.3.1.2" class="ltx_tr">
<td id="S4.T6.1.8.7.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">PAMAP2</td>
</tr>
<tr id="S4.T6.1.8.7.3.1.3" class="ltx_tr">
<td id="S4.T6.1.8.7.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">USC-HAD</td>
</tr>
</table>
</td>
<td id="S4.T6.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.8.7.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.8.7.4.1.1" class="ltx_tr">
<td id="S4.T6.1.8.7.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">IMU</td>
</tr>
</table>
</td>
<td id="S4.T6.1.8.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">HMM</td>
<td id="S4.T6.1.8.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S4.T6.1.8.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.8.7.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.8.7.7.1.1" class="ltx_tr">
<td id="S4.T6.1.8.7.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">91.28</td>
</tr>
<tr id="S4.T6.1.8.7.7.1.2" class="ltx_tr">
<td id="S4.T6.1.8.7.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">91.73</td>
</tr>
<tr id="S4.T6.1.8.7.7.1.3" class="ltx_tr">
<td id="S4.T6.1.8.7.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">90.19</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T6.1.9.8" class="ltx_tr">
<td id="S4.T6.1.9.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">kavuncuoug et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib300" title="" class="ltx_ref">300</a>]</cite>
</td>
<td id="S4.T6.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2021</td>
<td id="S4.T6.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.9.8.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.9.8.3.1.1" class="ltx_tr">
<td id="S4.T6.1.9.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Fall and ADLs</td>
</tr>
</table>
</td>
<td id="S4.T6.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.9.8.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.9.8.4.1.1" class="ltx_tr">
<td id="S4.T6.1.9.8.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometer</td>
</tr>
<tr id="S4.T6.1.9.8.4.1.2" class="ltx_tr">
<td id="S4.T6.1.9.8.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Gyroscope</td>
</tr>
<tr id="S4.T6.1.9.8.4.1.3" class="ltx_tr">
<td id="S4.T6.1.9.8.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Magnetometer</td>
</tr>
</table>
</td>
<td id="S4.T6.1.9.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">ML</td>
<td id="S4.T6.1.9.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.9.8.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.9.8.6.1.1" class="ltx_tr">
<td id="S4.T6.1.9.8.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">SVM, K-NN</td>
</tr>
</table>
</td>
<td id="S4.T6.1.9.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.9.8.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.9.8.7.1.1" class="ltx_tr">
<td id="S4.T6.1.9.8.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">99.96</td>
</tr>
<tr id="S4.T6.1.9.8.7.1.2" class="ltx_tr">
<td id="S4.T6.1.9.8.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">95.27</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T6.1.10.9" class="ltx_tr">
<td id="S4.T6.1.10.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Lu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib301" title="" class="ltx_ref">301</a>]</cite>
</td>
<td id="S4.T6.1.10.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2022</td>
<td id="S4.T6.1.10.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.10.9.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.10.9.3.1.1" class="ltx_tr">
<td id="S4.T6.1.10.9.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">WISDM, PAMAP2</td>
</tr>
<tr id="S4.T6.1.10.9.3.1.2" class="ltx_tr">
<td id="S4.T6.1.10.9.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCI-HAR</td>
</tr>
</table>
</td>
<td id="S4.T6.1.10.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.10.9.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.10.9.4.1.1" class="ltx_tr">
<td id="S4.T6.1.10.9.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">IMUs</td>
</tr>
<tr id="S4.T6.1.10.9.4.1.2" class="ltx_tr">
<td id="S4.T6.1.10.9.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometers</td>
</tr>
<tr id="S4.T6.1.10.9.4.1.3" class="ltx_tr">
<td id="S4.T6.1.10.9.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometers</td>
</tr>
</table>
</td>
<td id="S4.T6.1.10.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">CNN-GRU</td>
<td id="S4.T6.1.10.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S4.T6.1.10.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.10.9.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.10.9.7.1.1" class="ltx_tr">
<td id="S4.T6.1.10.9.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">96.41</td>
</tr>
<tr id="S4.T6.1.10.9.7.1.2" class="ltx_tr">
<td id="S4.T6.1.10.9.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">96.25</td>
</tr>
<tr id="S4.T6.1.10.9.7.1.3" class="ltx_tr">
<td id="S4.T6.1.10.9.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">96.67</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T6.1.11.10" class="ltx_tr">
<td id="S4.T6.1.11.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Kim et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib302" title="" class="ltx_ref">302</a>]</cite>
</td>
<td id="S4.T6.1.11.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2022</td>
<td id="S4.T6.1.11.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.11.10.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.11.10.3.1.1" class="ltx_tr">
<td id="S4.T6.1.11.10.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">WISDM</td>
</tr>
<tr id="S4.T6.1.11.10.3.1.2" class="ltx_tr">
<td id="S4.T6.1.11.10.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">USC-HAR</td>
</tr>
</table>
</td>
<td id="S4.T6.1.11.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.11.10.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.11.10.4.1.1" class="ltx_tr">
<td id="S4.T6.1.11.10.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">IMUs</td>
</tr>
</table>
</td>
<td id="S4.T6.1.11.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">CNN-BiGRU</td>
<td id="S4.T6.1.11.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S4.T6.1.11.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.11.10.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.11.10.7.1.1" class="ltx_tr">
<td id="S4.T6.1.11.10.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">99.49</td>
</tr>
<tr id="S4.T6.1.11.10.7.1.2" class="ltx_tr">
<td id="S4.T6.1.11.10.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">88.31</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T6.1.12.11" class="ltx_tr">
<td id="S4.T6.1.12.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Sarkar et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib303" title="" class="ltx_ref">303</a>]</cite>
</td>
<td id="S4.T6.1.12.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2023</td>
<td id="S4.T6.1.12.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.12.11.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.12.11.3.1.1" class="ltx_tr">
<td id="S4.T6.1.12.11.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCI-HAR</td>
</tr>
<tr id="S4.T6.1.12.11.3.1.2" class="ltx_tr">
<td id="S4.T6.1.12.11.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">WISDM, MHEALTH</td>
</tr>
<tr id="S4.T6.1.12.11.3.1.3" class="ltx_tr">
<td id="S4.T6.1.12.11.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">PAMAP2</td>
</tr>
<tr id="S4.T6.1.12.11.3.1.4" class="ltx_tr">
<td id="S4.T6.1.12.11.3.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HHAR</td>
</tr>
</table>
</td>
<td id="S4.T6.1.12.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.12.11.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.12.11.4.1.1" class="ltx_tr">
<td id="S4.T6.1.12.11.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">IMUs</td>
</tr>
<tr id="S4.T6.1.12.11.4.1.2" class="ltx_tr">
<td id="S4.T6.1.12.11.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometers</td>
</tr>
<tr id="S4.T6.1.12.11.4.1.3" class="ltx_tr">
<td id="S4.T6.1.12.11.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometers</td>
</tr>
</table>
</td>
<td id="S4.T6.1.12.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">CNN with GA</td>
<td id="S4.T6.1.12.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SVM</td>
<td id="S4.T6.1.12.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.12.11.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.12.11.7.1.1" class="ltx_tr">
<td id="S4.T6.1.12.11.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98.74</td>
</tr>
<tr id="S4.T6.1.12.11.7.1.2" class="ltx_tr">
<td id="S4.T6.1.12.11.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98.34</td>
</tr>
<tr id="S4.T6.1.12.11.7.1.3" class="ltx_tr">
<td id="S4.T6.1.12.11.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">99.72</td>
</tr>
<tr id="S4.T6.1.12.11.7.1.4" class="ltx_tr">
<td id="S4.T6.1.12.11.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.55</td>
</tr>
<tr id="S4.T6.1.12.11.7.1.5" class="ltx_tr">
<td id="S4.T6.1.12.11.7.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">96.87</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T6.1.13.12" class="ltx_tr">
<td id="S4.T6.1.13.12.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Semwal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib304" title="" class="ltx_ref">304</a>]</cite>
</td>
<td id="S4.T6.1.13.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2023</td>
<td id="S4.T6.1.13.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.13.12.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.13.12.3.1.1" class="ltx_tr">
<td id="S4.T6.1.13.12.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">WISDM</td>
</tr>
<tr id="S4.T6.1.13.12.3.1.2" class="ltx_tr">
<td id="S4.T6.1.13.12.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">PAMAP2</td>
</tr>
<tr id="S4.T6.1.13.12.3.1.3" class="ltx_tr">
<td id="S4.T6.1.13.12.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">USC-HAD</td>
</tr>
</table>
</td>
<td id="S4.T6.1.13.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.13.12.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.13.12.4.1.1" class="ltx_tr">
<td id="S4.T6.1.13.12.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">IMUs</td>
</tr>
</table>
</td>
<td id="S4.T6.1.13.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">CNN and LSTM</td>
<td id="S4.T6.1.13.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S4.T6.1.13.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.13.12.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.13.12.7.1.1" class="ltx_tr">
<td id="S4.T6.1.13.12.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">95.76</td>
</tr>
<tr id="S4.T6.1.13.12.7.1.2" class="ltx_tr">
<td id="S4.T6.1.13.12.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">94.64</td>
</tr>
<tr id="S4.T6.1.13.12.7.1.3" class="ltx_tr">
<td id="S4.T6.1.13.12.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">89.83</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T6.1.14.13" class="ltx_tr">
<td id="S4.T6.1.14.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Yao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib279" title="" class="ltx_ref">279</a>]</cite>
</td>
<td id="S4.T6.1.14.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2024</td>
<td id="S4.T6.1.14.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.14.13.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.14.13.3.1.1" class="ltx_tr">
<td id="S4.T6.1.14.13.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">PAMAP2</td>
</tr>
<tr id="S4.T6.1.14.13.3.1.2" class="ltx_tr">
<td id="S4.T6.1.14.13.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">USC-HAD, UniMiB-SHAR</td>
</tr>
<tr id="S4.T6.1.14.13.3.1.3" class="ltx_tr">
<td id="S4.T6.1.14.13.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">OPPORTUNITY</td>
</tr>
</table>
</td>
<td id="S4.T6.1.14.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.14.13.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.14.13.4.1.1" class="ltx_tr">
<td id="S4.T6.1.14.13.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">IMUs</td>
</tr>
<tr id="S4.T6.1.14.13.4.1.2" class="ltx_tr">
<td id="S4.T6.1.14.13.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Accelerometers</td>
</tr>
</table>
</td>
<td id="S4.T6.1.14.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">ELK ResNet</td>
<td id="S4.T6.1.14.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S4.T6.1.14.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.14.13.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.14.13.7.1.1" class="ltx_tr">
<td id="S4.T6.1.14.13.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">95.53</td>
</tr>
<tr id="S4.T6.1.14.13.7.1.2" class="ltx_tr">
<td id="S4.T6.1.14.13.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.25</td>
</tr>
<tr id="S4.T6.1.14.13.7.1.3" class="ltx_tr">
<td id="S4.T6.1.14.13.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">82.79</td>
</tr>
<tr id="S4.T6.1.14.13.7.1.4" class="ltx_tr">
<td id="S4.T6.1.14.13.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">87.96</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T6.1.15.14" class="ltx_tr">
<td id="S4.T6.1.15.14.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Wei et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib305" title="" class="ltx_ref">305</a>]</cite>
</td>
<td id="S4.T6.1.15.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2024</td>
<td id="S4.T6.1.15.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.15.14.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.15.14.3.1.1" class="ltx_tr">
<td id="S4.T6.1.15.14.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">WISDM</td>
</tr>
<tr id="S4.T6.1.15.14.3.1.2" class="ltx_tr">
<td id="S4.T6.1.15.14.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">PAMAP2</td>
</tr>
<tr id="S4.T6.1.15.14.3.1.3" class="ltx_tr">
<td id="S4.T6.1.15.14.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">USC-HAD</td>
</tr>
</table>
</td>
<td id="S4.T6.1.15.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.15.14.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.15.14.4.1.1" class="ltx_tr">
<td id="S4.T6.1.15.14.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">IMU</td>
</tr>
</table>
</td>
<td id="S4.T6.1.15.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">TCN-Attention</td>
<td id="S4.T6.1.15.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S4.T6.1.15.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.15.14.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.15.14.7.1.1" class="ltx_tr">
<td id="S4.T6.1.15.14.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">99.03</td>
</tr>
<tr id="S4.T6.1.15.14.7.1.2" class="ltx_tr">
<td id="S4.T6.1.15.14.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98.35</td>
</tr>
<tr id="S4.T6.1.15.14.7.1.3" class="ltx_tr">
<td id="S4.T6.1.15.14.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">96.32</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T6.1.16.15" class="ltx_tr">
<td id="S4.T6.1.16.15.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">El<span id="S4.T6.1.16.15.1.1" class="ltx_text">-</span>Adawi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib261" title="" class="ltx_ref">261</a>]</cite>
</td>
<td id="S4.T6.1.16.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2024</td>
<td id="S4.T6.1.16.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.16.15.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.16.15.3.1.1" class="ltx_tr">
<td id="S4.T6.1.16.15.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">MHEALTH</td>
</tr>
</table>
</td>
<td id="S4.T6.1.16.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.16.15.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.16.15.4.1.1" class="ltx_tr">
<td id="S4.T6.1.16.15.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">IMU</td>
</tr>
</table>
</td>
<td id="S4.T6.1.16.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">GAF+DenseNet169</td>
<td id="S4.T6.1.16.15.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S4.T6.1.16.15.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.16.15.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.16.15.7.1.1" class="ltx_tr">
<td id="S4.T6.1.16.15.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.83</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T6.1.17.16" class="ltx_tr">
<td id="S4.T6.1.17.16.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Ye et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib263" title="" class="ltx_ref">263</a>]</cite>
</td>
<td id="S4.T6.1.17.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2024</td>
<td id="S4.T6.1.17.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.17.16.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.17.16.3.1.1" class="ltx_tr">
<td id="S4.T6.1.17.16.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">OPPT, PAMAP2</td>
</tr>
</table>
</td>
<td id="S4.T6.1.17.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.17.16.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.17.16.4.1.1" class="ltx_tr">
<td id="S4.T6.1.17.16.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">IMU</td>
</tr>
</table>
</td>
<td id="S4.T6.1.17.16.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">CVAE-USM</td>
<td id="S4.T6.1.17.16.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">GMM</td>
<td id="S4.T6.1.17.16.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.17.16.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.17.16.7.1.1" class="ltx_tr">
<td id="S4.T6.1.17.16.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">100</td>
</tr>
<tr id="S4.T6.1.17.16.7.1.2" class="ltx_tr">
<td id="S4.T6.1.17.16.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">82.12</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T6.1.18.17" class="ltx_tr">
<td id="S4.T6.1.18.17.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Kaya et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib265" title="" class="ltx_ref">265</a>]</cite>
</td>
<td id="S4.T6.1.18.17.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2024</td>
<td id="S4.T6.1.18.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.18.17.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.18.17.3.1.1" class="ltx_tr">
<td id="S4.T6.1.18.17.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCI-HAPT</td>
</tr>
<tr id="S4.T6.1.18.17.3.1.2" class="ltx_tr">
<td id="S4.T6.1.18.17.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">WISDM,PAMAP2</td>
</tr>
</table>
</td>
<td id="S4.T6.1.18.17.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.18.17.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.18.17.4.1.1" class="ltx_tr">
<td id="S4.T6.1.18.17.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">IMU</td>
</tr>
</table>
</td>
<td id="S4.T6.1.18.17.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Deep CNN</td>
<td id="S4.T6.1.18.17.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S4.T6.1.18.17.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.18.17.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.18.17.7.1.1" class="ltx_tr">
<td id="S4.T6.1.18.17.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98</td>
</tr>
<tr id="S4.T6.1.18.17.7.1.2" class="ltx_tr">
<td id="S4.T6.1.18.17.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.8</td>
</tr>
<tr id="S4.T6.1.18.17.7.1.3" class="ltx_tr">
<td id="S4.T6.1.18.17.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">90.27</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T6.1.19.18" class="ltx_tr">
<td id="S4.T6.1.19.18.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib270" title="" class="ltx_ref">270</a>]</cite>
</td>
<td id="S4.T6.1.19.18.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2024</td>
<td id="S4.T6.1.19.18.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.19.18.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.19.18.3.1.1" class="ltx_tr">
<td id="S4.T6.1.19.18.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Shoaib, SisFall</td>
</tr>
<tr id="S4.T6.1.19.18.3.1.2" class="ltx_tr">
<td id="S4.T6.1.19.18.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HCIHAR, KU-HAR</td>
</tr>
</table>
</td>
<td id="S4.T6.1.19.18.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.19.18.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.19.18.4.1.1" class="ltx_tr">
<td id="S4.T6.1.19.18.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">IMU</td>
</tr>
</table>
</td>
<td id="S4.T6.1.19.18.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.19.18.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.19.18.5.1.1" class="ltx_tr">
<td id="S4.T6.1.19.18.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">1DCNN-Att</td>
</tr>
<tr id="S4.T6.1.19.18.5.1.2" class="ltx_tr">
<td id="S4.T6.1.19.18.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-BiLSTM</td>
</tr>
</table>
</td>
<td id="S4.T6.1.19.18.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SVM</td>
<td id="S4.T6.1.19.18.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.19.18.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.19.18.7.1.1" class="ltx_tr">
<td id="S4.T6.1.19.18.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">99.48</td>
</tr>
<tr id="S4.T6.1.19.18.7.1.2" class="ltx_tr">
<td id="S4.T6.1.19.18.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">91.85</td>
</tr>
<tr id="S4.T6.1.19.18.7.1.3" class="ltx_tr">
<td id="S4.T6.1.19.18.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">96.67</td>
</tr>
<tr id="S4.T6.1.19.18.7.1.4" class="ltx_tr">
<td id="S4.T6.1.19.18.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.99</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T6.1.20.19" class="ltx_tr">
<td id="S4.T6.1.20.19.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib276" title="" class="ltx_ref">276</a>]</cite>
</td>
<td id="S4.T6.1.20.19.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2024</td>
<td id="S4.T6.1.20.19.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.20.19.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.20.19.3.1.1" class="ltx_tr">
<td id="S4.T6.1.20.19.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">DSADS</td>
</tr>
<tr id="S4.T6.1.20.19.3.1.2" class="ltx_tr">
<td id="S4.T6.1.20.19.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HAPT</td>
</tr>
</table>
</td>
<td id="S4.T6.1.20.19.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.20.19.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.20.19.4.1.1" class="ltx_tr">
<td id="S4.T6.1.20.19.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">IMU</td>
</tr>
</table>
</td>
<td id="S4.T6.1.20.19.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Multi-STMT</td>
<td id="S4.T6.1.20.19.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S4.T6.1.20.19.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.20.19.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.20.19.7.1.1" class="ltx_tr">
<td id="S4.T6.1.20.19.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">99.86</td>
</tr>
<tr id="S4.T6.1.20.19.7.1.2" class="ltx_tr">
<td id="S4.T6.1.20.19.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98.73</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T6.1.21.20" class="ltx_tr">
<td id="S4.T6.1.21.20.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Saha et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib284" title="" class="ltx_ref">284</a>]</cite>
</td>
<td id="S4.T6.1.21.20.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2024</td>
<td id="S4.T6.1.21.20.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.21.20.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.21.20.3.1.1" class="ltx_tr">
<td id="S4.T6.1.21.20.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCI HAR</td>
</tr>
<tr id="S4.T6.1.21.20.3.1.2" class="ltx_tr">
<td id="S4.T6.1.21.20.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Motion-Sense</td>
</tr>
</table>
</td>
<td id="S4.T6.1.21.20.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.21.20.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.21.20.4.1.1" class="ltx_tr">
<td id="S4.T6.1.21.20.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">IMU</td>
</tr>
</table>
</td>
<td id="S4.T6.1.21.20.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">FusionActNet</td>
<td id="S4.T6.1.21.20.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S4.T6.1.21.20.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S4.T6.1.21.20.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.21.20.7.1.1" class="ltx_tr">
<td id="S4.T6.1.21.20.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.35</td>
</tr>
<tr id="S4.T6.1.21.20.7.1.2" class="ltx_tr">
<td id="S4.T6.1.21.20.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">95.35</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Preprocessing of the Sensor Dataset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Preprocessing sensor data is very crucial for reliable analysis and effective maintenance. Consequently, data collected from sensing devices must be preprocessed before being utilized for any analysis. Poor data quality, including missing values, outliers, and spikes, can impact the performance results. Preprocessing steps like imputing missing data, noise reduction, and normalization are significant. A fast, scalable module is needed for real-time data preprocessing, especially in predictive maintenance systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib306" title="" class="ltx_ref">306</a>]</cite>. After preprocessing the sensor data, the second step is feature engineering, which involves creating new characteristics from existing data. Its main goals are to improve connections between input and output variables in forecasting models and to select the most useful features, enhancing model quality and efficiency. Finally, a proper model must be designed and implemented.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Sensor Data Modality Based HAR System Using Feature Extraction with Machine Learning</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Previous studies on sensor-based HAR have involved manually extracting features from raw sensor data and using conventional ML techniques like SVM, RF, KNN, DT, and NB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib307" title="" class="ltx_ref">307</a>, <a href="#bib.bib308" title="" class="ltx_ref">308</a>, <a href="#bib.bib309" title="" class="ltx_ref">309</a>, <a href="#bib.bib310" title="" class="ltx_ref">310</a>, <a href="#bib.bib311" title="" class="ltx_ref">311</a>]</cite>. Kavuncuoglu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib300" title="" class="ltx_ref">300</a>]</cite> combining accelerometer and magnetometer data with SVM improves fall and activity classification. Feature-level fusion has outperformed fraction-level fusion with multiclass SVM and KNN classifiers on UCI HAR and physical activity sensor datasets. Using EEG data, models like RF and GB demonstrated excellent performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib294" title="" class="ltx_ref">294</a>]</cite>, with LIME providing insights into significant EEG features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib312" title="" class="ltx_ref">312</a>]</cite>. Introducing new activity classifications and novel feature engineering with models like GBDT, RF, KNN, and SVM has enhanced activity recognition accuracy. However, these traditional methods depend heavily on the quality of feature engineering, requiring domain-specific expertise to extract and select relevant features, which may not generalize across all activities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib313" title="" class="ltx_ref">313</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Sensor Data Modality Based HAR System Using Deep Learning Approach</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Recently many researchers have developed DL-based methods for HAR using sensor-based datasets, such as CNNs and RNNs, which automatically learn complex features from raw sensor data without manual feature extraction. Figure <a href="#S2.F7" title="Figure 7 ‣ 2.2.3 Classification Approach ‣ 2.2 Handcrafted Features with ML-Based Approach ‣ 2 RGB-Data Modality Based Action Recognition Methods ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> demonstrates the year-wise end-to-end deep learning method developed by various researchers for sensor-based HAR systems. These models achieve state-of-the-art results HAR. However, CNNs may not capture time-domain characteristics effectively.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Background of the Deep Learning Based Temporal Modeling TCN</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">Recently, the study revolves around advancements in Human Activity Recognition (HAR) using ambient sensors. It highlights the integration of various types of sensors—user-driven, environment-driven, and object-driven—into HAR systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib287" title="" class="ltx_ref">287</a>]</cite>. Recent progress in HAR involves leveraging DL-based techniques, including Transformer models with multi-head attention mechanisms, to effectively capture temporal dependencies in activity data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Additionally, the importance of sensor frequency information and the analysis of time and frequency domains in understanding sensor-driven time series data are emphasized <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib314" title="" class="ltx_ref">314</a>]</cite>. The previous approach performs to addresses challenges such as adapting HAR systems to new activities in dynamic environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib252" title="" class="ltx_ref">252</a>]</cite>. Kim et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib287" title="" class="ltx_ref">287</a>]</cite> developed a contrastive learning-based novelty detection (CLAN) method for HAR from sensor data. They perform to address challenges like temporal and frequency features, complex activity dynamics, and sensor modality variations by leveraging diverse negative pairs through data augmentation. The two-tower model extracts invariant representations of known activities, enhancing recognition of new activities, even with shared features. Wei et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib305" title="" class="ltx_ref">305</a>]</cite> presented a Time Convolution Network with Attention Mechanism (TCN-Attention-HAR) model designed to enhance HAR using wearable sensor data. Addressing challenges such as effective temporal feature extraction and gradient issues in deep networks, the model optimizes feature extraction with appropriate temporal convolution sizes and prioritizes important information using attention mechanisms.
Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib270" title="" class="ltx_ref">270</a>]</cite> presents Multi-STMT, a multilevel model for HAR using wearable sensors that integrate spatiotemporal attention and multiscale temporal embedding; the model combines CNN and BiGRU modules with attention mechanisms to capture nuanced differences in activities.
The Conditional Variational Autoencoder with Universal Sequence Mapping (CVAE-USM) for HAR. This method addresses the challenge of non-i.i.d. data distributions in cross-user scenarios by leveraging temporal relationships in time-series data. They combining VAE and USM techniques, CVAE-USM effectively aligns user data distributions, capturing common temporal patterns to enhance activity recognition accuracy.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>CNN based Various Stream for HAR</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">Ignatov et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib293" title="" class="ltx_ref">293</a>]</cite> utilized a DL-based approach for real-time HAR with mobile sensor data. They employ CNN for local feature extraction and integrate simple statistical features to capture global time series patterns. The experimental evaluations of the WISDM and UCI datasets demonstrate high accuracy across various users and datasets, highlighting their effectiveness in the DL-based method without needing complex computational resources or manual feature engineering. Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib295" title="" class="ltx_ref">295</a>]</cite> developed a semi-supervised DL-based model for imbalanced HAR utilized multimodal wearable sensory data. Addressing challenges such as limited labelled data and class imbalance, the model employs a pattern-balanced framework to extract diverse activity patterns. They used recurrent convolutional attention networks to identify salient features across modalities. Kaya et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib265" title="" class="ltx_ref">265</a>]</cite> presented a 1D-CNN-based approach to accurately HAR from sensor data. They evaluated their model using raw accelerometer and gyroscope sensor data from three public datasets: UCI-HAPT, WISDM, and PAMAP2. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib289" title="" class="ltx_ref">289</a>]</cite> presented a method HAR using sensor data modality called ConvTransformer. They combine CNN, Transformer, and attention mechanisms to handle the challenge of extracting both detailed and overall features from sensor data.</p>
</div>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>RNN, LSTM, Bi-LSTM for HAR</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">In most of the recent work, including RNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib315" title="" class="ltx_ref">315</a>]</cite> play a crucial role in handling temporal dependencies in sensor data for HAR. To address challenges like gradient issues, LSTM networks were developed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib316" title="" class="ltx_ref">316</a>]</cite>. Researchers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib296" title="" class="ltx_ref">296</a>, <a href="#bib.bib317" title="" class="ltx_ref">317</a>, <a href="#bib.bib270" title="" class="ltx_ref">270</a>, <a href="#bib.bib298" title="" class="ltx_ref">298</a>]</cite> have also explored attention-based BiLSTM models, achieving the best performance compared to other DL-based methods. The experimental evaluations on various datasets shown in Table <a href="#S4.T6" title="Table 6 ‣ 4 Sensor Based HAR ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> demonstrate high accuracy across various users and datasets, highlighting their effectiveness in the DL-based method without needing complex computational resources or manual feature engineering. Saha et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib284" title="" class="ltx_ref">284</a>]</cite> presented Fusion ActNet, an advanced method for HAR using sensor data. It features dedicated residual networks to capture static and dynamic actions separately, alongside a guidance module for decision-making, through a two-stage training process and evaluations on benchmark datasets. Murad et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib316" title="" class="ltx_ref">316</a>]</cite> used deep recurrent neural networks (DRNNs) in HAR, highlighting their ability to capture long-range dependencies in variable-length input sequences from body-worn sensors. Unlike traditional approaches that overlook temporal correlations, DRNNs, including unidirectional, bidirectional, and cascaded LSTM frameworks, perform well on diverse benchmark datasets. They perform the comparison of conventional machine learning approaches like SVM and KNN, as well as other deep learning techniques such as DBNs and CNNs, demonstrating their effectiveness in activity recognition tasks.</p>
</div>
</section>
<section id="S4.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.4 </span>Integratation CNN and LSTM Based Technique</h4>

<div id="S4.SS3.SSS4.p1" class="ltx_para">
<p id="S4.SS3.SSS4.p1.1" class="ltx_p">Several studies have developed that utilize hybrid models, combining different DL architectures can report high-performance accuracy in HAR. For instance, a hybrid CNN-LSTM model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib318" title="" class="ltx_ref">318</a>, <a href="#bib.bib304" title="" class="ltx_ref">304</a>]</cite> improved sleep-wake detection using heterogeneous sensors. Additionally, designs like TCCSNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib319" title="" class="ltx_ref">319</a>]</cite> and CSNet leverage temporal and channel dependencies to enhance human behaviour detection. Ordonez et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib315" title="" class="ltx_ref">315</a>]</cite> developed a model for HAR using CNN and LSTM recurrent units. They extract features from raw sensor data, support multimodal sensor fusion, and model complex temporal dynamics without manual feature design. Evaluation of benchmark datasets, such as Opportunity and Skoda, shows significant performance improvements over traditional methods, highlighting their effectiveness in HAR applications. Zhang et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib320" title="" class="ltx_ref">320</a>]</cite> developed a multi-channel DL-based network called a hybrid model (1DCNN-Att-BiLSTM) for improved recognition performance, evaluation using publicly accessible datasets, and comparison with ML and DL models. El-adawi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib261" title="" class="ltx_ref">261</a>]</cite> developed a HAR model within a Wireless Body Area Network (WBAN). The model leverages the Gramian Angular Field (GAF) and DenseNet. By converting time series data into 2D images using GAF and integrating them with DenseNet they achieve good performance accuracy.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Multimodal Fusion Modality Based Action Recognition</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Actions recognition through the utilization of a dataset that consists of multiple modalities necessitates the act of discerning and categorizing human actions or activities. This dataset encompasses various forms of information, including visual, audio, and sensor data. Integrating diverse sources of information within multi-modal datasets affords a better comprehension of actions. From the perspective of the input data’s modality, DL techniques can acquire human action characteristics through a diverse range of modal data. Similarly, the ML-based algorithm aims to process the information from multiple modalities. By using the strengths of various data types, multi-modal ML can often perform more accurate HAR tasks. There are several types of multimodality learning methods, including fusion-based methods such as RGB with skeleton and dept-based modalities. Generally, fusion refers to combining the information of two or more modalities to train the model and provide accurate results of HAR. There are two main approaches widely utilized in multi-modality fusion schemes, namely score fusion and feature fusion. The fusion-based approach combines scores obtained from various sources, including weight averaging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib321" title="" class="ltx_ref">321</a>]</cite> or learning a score fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib322" title="" class="ltx_ref">322</a>]</cite> model, while the feature fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib323" title="" class="ltx_ref">323</a>]</cite> focuses on integrating features extracted from different modalities. Ramani et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib324" title="" class="ltx_ref">324</a>]</cite> developed an algorithm that combines depth image and 3D joint position data using local spatiotemporal features and dominant skeleton movements.
Researchers have increasingly explored DL techniques to extract action-effective features utilizing the RGB, depth, and skeleton data. These methods facilitate multimodal feature learning from deep networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib140" title="" class="ltx_ref">140</a>, <a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite>, encompassing appearance image information such as optical flow sequences, depth sequences, and skeleton sequences. DL networks are proficient at learning human action effective features by performing single-modal data or multimodal fusion data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib325" title="" class="ltx_ref">325</a>, <a href="#bib.bib188" title="" class="ltx_ref">188</a>, <a href="#bib.bib326" title="" class="ltx_ref">326</a>]</cite>. Note that score fusion and feature fusion are important in advancing HAR technology to provide accurate results.
Table <a href="#S5.T8" title="Table 8 ‣ 5.3 Fusion of Signal and Visual Modalities ‣ 5 Multimodal Fusion Modality Based Action Recognition ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> lists the basic information of the existing model, including datasets, multi-modality, features extraction methods, classifier, years, and performance accuracy.
<br class="ltx_break"></p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Multimodal Fusion Based HAR Dataset</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We provided the most popular benchmark HAR datasets, which come from the multi-modal fusion dataset, which is demonstrated in Table <a href="#S5.T8" title="Table 8 ‣ 5.3 Fusion of Signal and Visual Modalities ‣ 5 Multimodal Fusion Modality Based Action Recognition ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. The dataset table demonstrated the details of the datasets, including modalities, creation year, number of classes, number of subjects who participated in recording the dataset, number of samples, and latest performance accuracy of the dataset with citation. Figure <a href="#S2.F7" title="Figure 7 ‣ 2.2.3 Classification Approach ‣ 2.2 Handcrafted Features with ML-Based Approach ‣ 2 RGB-Data Modality Based Action Recognition Methods ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> also demonstrates the year-wise end-to-end deep learning method developed by various researchers for multimodal fusion-based HAR systems.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Table <a href="#S5.T7" title="Table 7 ‣ 5.1 Multimodal Fusion Based HAR Dataset ‣ 5 Multimodal Fusion Modality Based Action Recognition ‣ A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> presents a comprehensive overview of benchmark datasets for Human Activity Recognition (HAR) using various modalities. The datasets include combinations of RGB, Skeleton, Depth, Infrared, Acceleration, and Gyroscope data, providing rich and diverse sources for model training and evaluation. For instance, the MSRDailyActivity3D dataset, introduced in 2012, includes RGB, Skeleton, and Depth data, featuring 16 classes, 10 subjects, and 320 samples with a notable accuracy of 97.50% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. The N-UCLA dataset from 2014 also incorporates RGB, Skeleton, and Depth data, spanning 10 classes, 10 subjects, and 1475 samples, achieving an impressive 99.10% accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>. Another significant dataset, NTU RGB+D, initially released in 2016 and updated in 2019, includes RGB, Skeleton, Depth, and Infrared modalities, with 60 and 120 classes, 40 and 106 subjects, and 56880 and 114480 samples respectively, both recording a high accuracy of 97.40% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>. The Kinetics-600 dataset, published in 2018, is one of the largest, containing RGB, Skeleton, Depth, and Infrared data across 600 classes and 595445 samples, with an accuracy of 91.90% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. These datasets are crucial for advancing HAR research, offering extensive and varied data for developing robust and accurate models.</p>
</div>
<figure id="S5.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Multimodality fusion based HAR benchmark datasets.</figcaption>
<table id="S5.T7.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T7.1.1.1" class="ltx_tr">
<th id="S5.T7.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T7.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S5.T7.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T7.1.1.1.2.1" class="ltx_text ltx_font_bold">Data set modalities</span></th>
<th id="S5.T7.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T7.1.1.1.3.1" class="ltx_text ltx_font_bold">Year</span></th>
<th id="S5.T7.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T7.1.1.1.4.1" class="ltx_text ltx_font_bold">Class</span></th>
<th id="S5.T7.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T7.1.1.1.5.1" class="ltx_text ltx_font_bold">Subject</span></th>
<th id="S5.T7.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T7.1.1.1.6.1" class="ltx_text ltx_font_bold">Sample</span></th>
<th id="S5.T7.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T7.1.1.1.7.1" class="ltx_text ltx_font_bold">Latest Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T7.1.2.1" class="ltx_tr">
<td id="S5.T7.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T7.1.2.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.2.1.1.1.1" class="ltx_tr">
<td id="S5.T7.1.2.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">MSRDaily</td>
</tr>
<tr id="S5.T7.1.2.1.1.1.2" class="ltx_tr">
<td id="S5.T7.1.2.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Activity3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T7.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Skeleton, Depth</td>
<td id="S5.T7.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2012</td>
<td id="S5.T7.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">16</td>
<td id="S5.T7.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">10</td>
<td id="S5.T7.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">320</td>
<td id="S5.T7.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">97.50% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>
</td>
</tr>
<tr id="S5.T7.1.3.2" class="ltx_tr">
<td id="S5.T7.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T7.1.3.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.3.2.1.1.1" class="ltx_tr">
<td id="S5.T7.1.3.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">N-UCLA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T7.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Skeleton, Depth</td>
<td id="S5.T7.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2014</td>
<td id="S5.T7.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">10</td>
<td id="S5.T7.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">10</td>
<td id="S5.T7.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1475</td>
<td id="S5.T7.1.3.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">99.10% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
</tr>
<tr id="S5.T7.1.4.3" class="ltx_tr">
<td id="S5.T7.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T7.1.4.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.4.3.1.1.1" class="ltx_tr">
<td id="S5.T7.1.4.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Multi-View TJU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T7.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Skeleton, Depth</td>
<td id="S5.T7.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2014</td>
<td id="S5.T7.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">20</td>
<td id="S5.T7.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">22</td>
<td id="S5.T7.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">7040</td>
<td id="S5.T7.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S5.T7.1.5.4" class="ltx_tr">
<td id="S5.T7.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T7.1.5.4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.5.4.1.1.1" class="ltx_tr">
<td id="S5.T7.1.5.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UTD-MHAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T7.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Skeleton, Depth, Acceleration, Gyroscope</td>
<td id="S5.T7.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2015</td>
<td id="S5.T7.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">27</td>
<td id="S5.T7.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">8</td>
<td id="S5.T7.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">861</td>
<td id="S5.T7.1.5.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">95.0% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>
</td>
</tr>
<tr id="S5.T7.1.6.5" class="ltx_tr">
<td id="S5.T7.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T7.1.6.5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.6.5.1.1.1" class="ltx_tr">
<td id="S5.T7.1.6.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UWA3D</td>
</tr>
<tr id="S5.T7.1.6.5.1.1.2" class="ltx_tr">
<td id="S5.T7.1.6.5.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Multiview II <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T7.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Skeleton, Depth</td>
<td id="S5.T7.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2015</td>
<td id="S5.T7.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">30</td>
<td id="S5.T7.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">10</td>
<td id="S5.T7.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1075</td>
<td id="S5.T7.1.6.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S5.T7.1.7.6" class="ltx_tr">
<td id="S5.T7.1.7.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T7.1.7.6.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.7.6.1.1.1" class="ltx_tr">
<td id="S5.T7.1.7.6.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU RGB+D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T7.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Skeleton, Depth, Infrared</td>
<td id="S5.T7.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2016</td>
<td id="S5.T7.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">60</td>
<td id="S5.T7.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">40</td>
<td id="S5.T7.1.7.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">56880</td>
<td id="S5.T7.1.7.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">97.40% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
</tr>
<tr id="S5.T7.1.8.7" class="ltx_tr">
<td id="S5.T7.1.8.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T7.1.8.7.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.8.7.1.1.1" class="ltx_tr">
<td id="S5.T7.1.8.7.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">PKU-MMD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T7.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Skeleton, Depth, Infrared</td>
<td id="S5.T7.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2017</td>
<td id="S5.T7.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">51</td>
<td id="S5.T7.1.8.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">66</td>
<td id="S5.T7.1.8.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">10076</td>
<td id="S5.T7.1.8.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">94.40% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>
</td>
</tr>
<tr id="S5.T7.1.9.8" class="ltx_tr">
<td id="S5.T7.1.9.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T7.1.9.8.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.9.8.1.1.1" class="ltx_tr">
<td id="S5.T7.1.9.8.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NEU-UB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T7.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Depth</td>
<td id="S5.T7.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2017</td>
<td id="S5.T7.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">6</td>
<td id="S5.T7.1.9.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">20</td>
<td id="S5.T7.1.9.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">600</td>
<td id="S5.T7.1.9.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S5.T7.1.10.9" class="ltx_tr">
<td id="S5.T7.1.10.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T7.1.10.9.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.10.9.1.1.1" class="ltx_tr">
<td id="S5.T7.1.10.9.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Kinetics-600 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T7.1.10.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Skeleton, Depth, Infrared</td>
<td id="S5.T7.1.10.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2018</td>
<td id="S5.T7.1.10.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">600</td>
<td id="S5.T7.1.10.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S5.T7.1.10.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">595445</td>
<td id="S5.T7.1.10.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">91.90% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</td>
</tr>
<tr id="S5.T7.1.11.10" class="ltx_tr">
<td id="S5.T7.1.11.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T7.1.11.10.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.11.10.1.1.1" class="ltx_tr">
<td id="S5.T7.1.11.10.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">RGB-D</td>
</tr>
<tr id="S5.T7.1.11.10.1.1.2" class="ltx_tr">
<td id="S5.T7.1.11.10.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Varing-View <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T7.1.11.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Skeleton, Depth</td>
<td id="S5.T7.1.11.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2018</td>
<td id="S5.T7.1.11.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">40</td>
<td id="S5.T7.1.11.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">118</td>
<td id="S5.T7.1.11.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">25600</td>
<td id="S5.T7.1.11.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S5.T7.1.12.11" class="ltx_tr">
<td id="S5.T7.1.12.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T7.1.12.11.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.12.11.1.1.1" class="ltx_tr">
<td id="S5.T7.1.12.11.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Drive&amp;Act <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T7.1.12.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Skeleton, Depth</td>
<td id="S5.T7.1.12.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2019</td>
<td id="S5.T7.1.12.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">83</td>
<td id="S5.T7.1.12.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">15</td>
<td id="S5.T7.1.12.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S5.T7.1.12.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">77.61% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>
</td>
</tr>
<tr id="S5.T7.1.13.12" class="ltx_tr">
<td id="S5.T7.1.13.12.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T7.1.13.12.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.13.12.1.1.1" class="ltx_tr">
<td id="S5.T7.1.13.12.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">MMAct <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T7.1.13.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Skeleton, Acceleration, Gyroscope</td>
<td id="S5.T7.1.13.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2019</td>
<td id="S5.T7.1.13.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">37</td>
<td id="S5.T7.1.13.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">20</td>
<td id="S5.T7.1.13.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">36764</td>
<td id="S5.T7.1.13.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">98.60% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>
</td>
</tr>
<tr id="S5.T7.1.14.13" class="ltx_tr">
<td id="S5.T7.1.14.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T7.1.14.13.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.14.13.1.1.1" class="ltx_tr">
<td id="S5.T7.1.14.13.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Toyota-SH <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T7.1.14.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Skeleton, Depth</td>
<td id="S5.T7.1.14.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2019</td>
<td id="S5.T7.1.14.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">31</td>
<td id="S5.T7.1.14.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">18</td>
<td id="S5.T7.1.14.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">16115</td>
<td id="S5.T7.1.14.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S5.T7.1.15.14" class="ltx_tr">
<td id="S5.T7.1.15.14.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T7.1.15.14.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.15.14.1.1.1" class="ltx_tr">
<td id="S5.T7.1.15.14.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">IKEA ASM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T7.1.15.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Skeleton, Depth</td>
<td id="S5.T7.1.15.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2020</td>
<td id="S5.T7.1.15.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">33</td>
<td id="S5.T7.1.15.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">48</td>
<td id="S5.T7.1.15.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">16764</td>
<td id="S5.T7.1.15.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S5.T7.1.16.15" class="ltx_tr">
<td id="S5.T7.1.16.15.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T7.1.16.15.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.16.15.1.1.1" class="ltx_tr">
<td id="S5.T7.1.16.15.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">ETRI-Activity3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T7.1.16.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Skeleton, Depth</td>
<td id="S5.T7.1.16.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2020</td>
<td id="S5.T7.1.16.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">55</td>
<td id="S5.T7.1.16.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">100</td>
<td id="S5.T7.1.16.15.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">112620</td>
<td id="S5.T7.1.16.15.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">95.09% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>
</td>
</tr>
<tr id="S5.T7.1.17.16" class="ltx_tr">
<td id="S5.T7.1.17.16.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T7.1.17.16.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.17.16.1.1.1" class="ltx_tr">
<td id="S5.T7.1.17.16.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UAV-Human <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T7.1.17.16.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Skeleton, Depth</td>
<td id="S5.T7.1.17.16.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2021</td>
<td id="S5.T7.1.17.16.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">155</td>
<td id="S5.T7.1.17.16.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">119</td>
<td id="S5.T7.1.17.16.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">27428</td>
<td id="S5.T7.1.17.16.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">55.00% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Fusion of RGB, Skeleton, and Depth Modalities</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Recently, several hand-crafted feature-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib327" title="" class="ltx_ref">327</a>]</cite> have been developed to explore multi-modalities such as RGB, skeleton, and depth to improve the performance of the action recognition tasks. While the DL-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib328" title="" class="ltx_ref">328</a>, <a href="#bib.bib329" title="" class="ltx_ref">329</a>, <a href="#bib.bib330" title="" class="ltx_ref">330</a>]</cite> have been proposed due to providing good performance. Shahoudy et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> study and explore the concept of correlation analysis between the different modalities and factorize them into desired independent components. They used a structured spared classifier for the HAR task. Hu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib328" title="" class="ltx_ref">328</a>]</cite> analysis between the time-varying information across the fusion of multimodality such as RGB, Skelton, and depth-based. They extracted temporal features from each modality and then concatenated them along the desired modality dimension. These multi-modal temporal features were then input into the model. Khaire et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib329" title="" class="ltx_ref">329</a>]</cite> developed a CNN network with five streams. These streams take inputs from MHI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>, DMM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib219" title="" class="ltx_ref">219</a>]</cite>, and skeleton images generated from RGB, depth, and skeleton sequences. Each CNN stream was trained separately, and the final classification scores were obtained by combining the output scores of all five CNN streams utilizing a weighted product model. Similarly, Khair et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib331" title="" class="ltx_ref">331</a>]</cite>, a fusion of three methods to merge skeletal, RGB, and depth modalities. Cardens et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib330" title="" class="ltx_ref">330</a>]</cite> utilized three distinct optical spectra channels from skeleton data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib332" title="" class="ltx_ref">332</a>]</cite> and dynamic images from RGB and depth videos. These features were fed into a pre-trained CNN to extract multi-modal features. Finally, they used a feature aggregation module for classification tasks.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Fusion of Signal and Visual Modalities</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Signal data complements visual data by providing additional information. Various DL-based approaches have been proposed to merge these modalities for HAR. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib333" title="" class="ltx_ref">333</a>]</cite> proposed three-stream CNN models to extract features from multimodalities. They evaluated the performance of both feature fusion and score fusion, with feature fusion showing superior performance. Owens et al. proposed a model of a two-stream CNN in a self-supervised manner to detect misalignments between audio and visual sequences.
Subsequently, they refined the model using HAR datasets for audio-visual recognition. TSN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite> showed improved performance by Kazakos et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib334" title="" class="ltx_ref">334</a>]</cite> introduced the Temporal Binding Network (TBN) for egocentric HAR, integrating audio, RGB, and optical flow inputs. TBN utilized a three-stream CNN to merge these inputs within each Temporal Binding Window, enhancing classification through temporal aggregation. Their findings demonstrated TBN’s superiority over TSN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite> in audio-visual HAR tasks. Additionally, Gao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib335" title="" class="ltx_ref">335</a>]</cite> utilized audio data to minimize temporal redundancies in videos, employing knowledge distillation from a teacher network trained on video clips to a student network trained on image-audio pairs for efficient HAR. Xiao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib336" title="" class="ltx_ref">336</a>]</cite> developed a novel framework combining audio and visual information, incorporating slow and fast visual pathways alongside a faster audio pathway across multiple layers. They employed two training strategies: randomly dropping the audio pathway and hierarchical audio-visual synchronization, facilitating the training of audio-video integration.
In addition, the multimodal HAR-based approaches such as
Bruce et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib337" title="" class="ltx_ref">337</a>]</cite> multimodal network (MMNet) fuses skeleton and RGB data using a spatiotemporal GNN to transfer attention weights, significantly improving HAR accuracy while Venkatachalam et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib338" title="" class="ltx_ref">338</a>]</cite> proposes a hybrid 1D CNN with LSTM classifier for HAR.
Overall, the objective of data fusion methods is to capitalize on the benefits of integrating various datasets to achieve a more robust and comprehensive feature representation. Consequently, the central issue that arises in the development of most data-fusion-based techniques revolves around determining the most efficient manner in which to combine disparate data types. This is typically addressed by employing the conventional early and late fusion strategies. The initial fusion occurs at the feature level, involving feature concatenation as the input to the recognition model. In contrast, the latter scenario performs fusion at the score level, integrating the output scores of the recognition model with diverse data types. The multimodal data fusion methods generally yield better recognition results than single-data approaches. However, the multimodal data fusion methods approach requires processing larger datasets and dealing with higher feature dimensions, thereby increasing the computational complexity of action recognition algorithms.
</p>
</div>
<figure id="S5.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Multi modality data fusion based HAR system models and performance.</figcaption>
<table id="S5.T8.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T8.1.1.1" class="ltx_tr">
<td id="S5.T8.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T8.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S5.T8.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T8.1.1.1.2.1" class="ltx_text ltx_font_bold">Classifier</span></td>
<td id="S5.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T8.1.1.1.3.1" class="ltx_text ltx_font_bold">Methods</span></td>
<td id="S5.T8.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T8.1.1.1.4.1" class="ltx_text ltx_font_bold">Data set type</span></td>
<td id="S5.T8.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T8.1.1.1.5.1" class="ltx_text ltx_font_bold">Year</span></td>
<td id="S5.T8.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T8.1.1.1.6.1" class="ltx_text ltx_font_bold">Reference</span></td>
<td id="S5.T8.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T8.1.1.1.7.1" class="ltx_text ltx_font_bold">Accuracy[%]</span></td>
</tr>
<tr id="S5.T8.1.2.2" class="ltx_tr">
<td id="S5.T8.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.2.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.2.2.1.1.1" class="ltx_tr">
<td id="S5.T8.1.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU RGB+D (CS)</td>
</tr>
<tr id="S5.T8.1.2.2.1.1.2" class="ltx_tr">
<td id="S5.T8.1.2.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU RGB+D (CV)</td>
</tr>
</table>
</td>
<td id="S5.T8.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SVM</td>
<td id="S5.T8.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">P-LSTM</td>
<td id="S5.T8.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Depth</td>
<td id="S5.T8.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2016</td>
<td id="S5.T8.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite></td>
<td id="S5.T8.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.2.2.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.2.2.7.1.1" class="ltx_tr">
<td id="S5.T8.1.2.2.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">62.93</td>
</tr>
<tr id="S5.T8.1.2.2.7.1.2" class="ltx_tr">
<td id="S5.T8.1.2.2.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">70.27</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.1.3.3" class="ltx_tr">
<td id="S5.T8.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.3.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.3.3.1.1.1" class="ltx_tr">
<td id="S5.T8.1.3.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCI-HAD</td>
</tr>
<tr id="S5.T8.1.3.3.1.1.2" class="ltx_tr">
<td id="S5.T8.1.3.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">USC-HAD</td>
</tr>
<tr id="S5.T8.1.3.3.1.1.3" class="ltx_tr">
<td id="S5.T8.1.3.3.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Opportunity</td>
</tr>
<tr id="S5.T8.1.3.3.1.1.4" class="ltx_tr">
<td id="S5.T8.1.3.3.1.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Daphnet FOG</td>
</tr>
<tr id="S5.T8.1.3.3.1.1.5" class="ltx_tr">
<td id="S5.T8.1.3.3.1.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Skoda</td>
</tr>
</table>
</td>
<td id="S5.T8.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SVM KNN</td>
<td id="S5.T8.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">DRNN</td>
<td id="S5.T8.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Sensors</td>
<td id="S5.T8.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2017</td>
<td id="S5.T8.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib316" title="" class="ltx_ref">316</a>]</cite></td>
<td id="S5.T8.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.3.3.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.3.3.7.1.1" class="ltx_tr">
<td id="S5.T8.1.3.3.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">96.7</td>
</tr>
<tr id="S5.T8.1.3.3.7.1.2" class="ltx_tr">
<td id="S5.T8.1.3.3.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.8</td>
</tr>
<tr id="S5.T8.1.3.3.7.1.3" class="ltx_tr">
<td id="S5.T8.1.3.3.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">92.5</td>
</tr>
<tr id="S5.T8.1.3.3.7.1.4" class="ltx_tr">
<td id="S5.T8.1.3.3.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">94.1</td>
</tr>
<tr id="S5.T8.1.3.3.7.1.5" class="ltx_tr">
<td id="S5.T8.1.3.3.7.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">92.6</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.1.4.4" class="ltx_tr">
<td id="S5.T8.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.4.4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.4.4.1.1.1" class="ltx_tr">
<td id="S5.T8.1.4.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Smartwach</td>
</tr>
</table>
</td>
<td id="S5.T8.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S5.T8.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Dilated CNN</td>
<td id="S5.T8.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">sensor</td>
<td id="S5.T8.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2020</td>
<td id="S5.T8.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib297" title="" class="ltx_ref">297</a>]</cite></td>
<td id="S5.T8.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.4.4.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.4.4.7.1.1" class="ltx_tr">
<td id="S5.T8.1.4.4.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">95.49</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.1.5.5" class="ltx_tr">
<td id="S5.T8.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.5.5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.5.5.1.1.1" class="ltx_tr">
<td id="S5.T8.1.5.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UTD-MHAD</td>
</tr>
<tr id="S5.T8.1.5.5.1.1.2" class="ltx_tr">
<td id="S5.T8.1.5.5.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU RGB+D</td>
</tr>
</table>
</td>
<td id="S5.T8.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S5.T8.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Vission based</td>
<td id="S5.T8.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB,Depth,Skeleton</td>
<td id="S5.T8.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2021</td>
<td id="S5.T8.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib339" title="" class="ltx_ref">339</a>]</cite></td>
<td id="S5.T8.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.5.5.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.5.5.7.1.1" class="ltx_tr">
<td id="S5.T8.1.5.5.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98.88</td>
</tr>
<tr id="S5.T8.1.5.5.7.1.2" class="ltx_tr">
<td id="S5.T8.1.5.5.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">75.50</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.1.6.6" class="ltx_tr">
<td id="S5.T8.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.6.6.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.6.6.1.1.1" class="ltx_tr">
<td id="S5.T8.1.6.6.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU RGB+D (CS)</td>
</tr>
<tr id="S5.T8.1.6.6.1.1.2" class="ltx_tr">
<td id="S5.T8.1.6.6.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU RGB+D (CV)</td>
</tr>
<tr id="S5.T8.1.6.6.1.1.3" class="ltx_tr">
<td id="S5.T8.1.6.6.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">SYSU 3D HOI</td>
</tr>
<tr id="S5.T8.1.6.6.1.1.4" class="ltx_tr">
<td id="S5.T8.1.6.6.1.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UWA3D II</td>
</tr>
</table>
</td>
<td id="S5.T8.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.6.6.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.6.6.2.1.1" class="ltx_tr">
<td id="S5.T8.1.6.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">hierarchical-</td>
</tr>
<tr id="S5.T8.1.6.6.2.1.2" class="ltx_tr">
<td id="S5.T8.1.6.6.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">score fusion</td>
</tr>
</table>
</td>
<td id="S5.T8.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Multi Model</td>
<td id="S5.T8.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB Depth</td>
<td id="S5.T8.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2021</td>
<td id="S5.T8.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib340" title="" class="ltx_ref">340</a>]</cite></td>
<td id="S5.T8.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.6.6.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.6.6.7.1.1" class="ltx_tr">
<td id="S5.T8.1.6.6.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">89.70</td>
</tr>
<tr id="S5.T8.1.6.6.7.1.2" class="ltx_tr">
<td id="S5.T8.1.6.6.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">92.97</td>
</tr>
<tr id="S5.T8.1.6.6.7.1.3" class="ltx_tr">
<td id="S5.T8.1.6.6.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">87.08</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.1.7.7" class="ltx_tr">
<td id="S5.T8.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.7.7.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.7.7.1.1.1" class="ltx_tr">
<td id="S5.T8.1.7.7.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF-101</td>
</tr>
<tr id="S5.T8.1.7.7.1.1.2" class="ltx_tr">
<td id="S5.T8.1.7.7.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Something-Something-v2</td>
</tr>
<tr id="S5.T8.1.7.7.1.1.3" class="ltx_tr">
<td id="S5.T8.1.7.7.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Kinetics-600</td>
</tr>
</table>
</td>
<td id="S5.T8.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S5.T8.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">MM-ViT</td>
<td id="S5.T8.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB</td>
<td id="S5.T8.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2022</td>
<td id="S5.T8.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib341" title="" class="ltx_ref">341</a>]</cite></td>
<td id="S5.T8.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.7.7.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.7.7.7.1.1" class="ltx_tr">
<td id="S5.T8.1.7.7.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98.9</td>
</tr>
<tr id="S5.T8.1.7.7.7.1.2" class="ltx_tr">
<td id="S5.T8.1.7.7.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">90.8</td>
</tr>
<tr id="S5.T8.1.7.7.7.1.3" class="ltx_tr">
<td id="S5.T8.1.7.7.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">96.8</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.1.8.8" class="ltx_tr">
<td id="S5.T8.1.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.8.8.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.8.8.1.1.1" class="ltx_tr">
<td id="S5.T8.1.8.8.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">MHEALTH</td>
</tr>
<tr id="S5.T8.1.8.8.1.1.2" class="ltx_tr">
<td id="S5.T8.1.8.8.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCI-HAR</td>
</tr>
</table>
</td>
<td id="S5.T8.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S5.T8.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">CNN-LSTM</td>
<td id="S5.T8.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Sensor</td>
<td id="S5.T8.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2022</td>
<td id="S5.T8.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib342" title="" class="ltx_ref">342</a>]</cite></td>
<td id="S5.T8.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.8.8.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.8.8.7.1.1" class="ltx_tr">
<td id="S5.T8.1.8.8.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98.76</td>
</tr>
<tr id="S5.T8.1.8.8.7.1.2" class="ltx_tr">
<td id="S5.T8.1.8.8.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">93.11</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.1.9.9" class="ltx_tr">
<td id="S5.T8.1.9.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.9.9.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.9.9.1.1.1" class="ltx_tr">
<td id="S5.T8.1.9.9.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCI-HAR</td>
</tr>
<tr id="S5.T8.1.9.9.1.1.2" class="ltx_tr">
<td id="S5.T8.1.9.9.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">WISDM</td>
</tr>
<tr id="S5.T8.1.9.9.1.1.3" class="ltx_tr">
<td id="S5.T8.1.9.9.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">MHEALTH</td>
</tr>
<tr id="S5.T8.1.9.9.1.1.4" class="ltx_tr">
<td id="S5.T8.1.9.9.1.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">PAMAP2</td>
</tr>
<tr id="S5.T8.1.9.9.1.1.5" class="ltx_tr">
<td id="S5.T8.1.9.9.1.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">HHAR</td>
</tr>
</table>
</td>
<td id="S5.T8.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SVM</td>
<td id="S5.T8.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">CNN with GA</td>
<td id="S5.T8.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Sensors</td>
<td id="S5.T8.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2023</td>
<td id="S5.T8.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib303" title="" class="ltx_ref">303</a>]</cite></td>
<td id="S5.T8.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.9.9.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.9.9.7.1.1" class="ltx_tr">
<td id="S5.T8.1.9.9.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98.74</td>
</tr>
<tr id="S5.T8.1.9.9.7.1.2" class="ltx_tr">
<td id="S5.T8.1.9.9.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98.34</td>
</tr>
<tr id="S5.T8.1.9.9.7.1.3" class="ltx_tr">
<td id="S5.T8.1.9.9.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">99.72</td>
</tr>
<tr id="S5.T8.1.9.9.7.1.4" class="ltx_tr">
<td id="S5.T8.1.9.9.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.55</td>
</tr>
<tr id="S5.T8.1.9.9.7.1.5" class="ltx_tr">
<td id="S5.T8.1.9.9.7.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">96.87</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.1.10.10" class="ltx_tr">
<td id="S5.T8.1.10.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.10.10.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.10.10.1.1.1" class="ltx_tr">
<td id="S5.T8.1.10.10.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU RGB+D 60</td>
</tr>
<tr id="S5.T8.1.10.10.1.1.2" class="ltx_tr">
<td id="S5.T8.1.10.10.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU RGB+D120</td>
</tr>
<tr id="S5.T8.1.10.10.1.1.3" class="ltx_tr">
<td id="S5.T8.1.10.10.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">PKU-MMD</td>
</tr>
<tr id="S5.T8.1.10.10.1.1.4" class="ltx_tr">
<td id="S5.T8.1.10.10.1.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Northwestern</td>
</tr>
<tr id="S5.T8.1.10.10.1.1.5" class="ltx_tr">
<td id="S5.T8.1.10.10.1.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCLAMultiview</td>
</tr>
<tr id="S5.T8.1.10.10.1.1.6" class="ltx_tr">
<td id="S5.T8.1.10.10.1.1.6.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Toyota Smarthome</td>
</tr>
</table>
</td>
<td id="S5.T8.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S5.T8.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">MMNet</td>
<td id="S5.T8.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Depth</td>
<td id="S5.T8.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2023</td>
<td id="S5.T8.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib337" title="" class="ltx_ref">337</a>]</cite></td>
<td id="S5.T8.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.10.10.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.10.10.7.1.1" class="ltx_tr">
<td id="S5.T8.1.10.10.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98.0</td>
</tr>
<tr id="S5.T8.1.10.10.7.1.2" class="ltx_tr">
<td id="S5.T8.1.10.10.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">90.5</td>
</tr>
<tr id="S5.T8.1.10.10.7.1.3" class="ltx_tr">
<td id="S5.T8.1.10.10.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98.0</td>
</tr>
<tr id="S5.T8.1.10.10.7.1.4" class="ltx_tr">
<td id="S5.T8.1.10.10.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">93.3</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.1.11.11" class="ltx_tr">
<td id="S5.T8.1.11.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.11.11.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.11.11.1.1.1" class="ltx_tr">
<td id="S5.T8.1.11.11.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU RGB+D 60</td>
</tr>
<tr id="S5.T8.1.11.11.1.1.2" class="ltx_tr">
<td id="S5.T8.1.11.11.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU RGB+D120</td>
</tr>
<tr id="S5.T8.1.11.11.1.1.3" class="ltx_tr">
<td id="S5.T8.1.11.11.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NW-UCLA</td>
</tr>
</table>
</td>
<td id="S5.T8.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S5.T8.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">InfoGCN</td>
<td id="S5.T8.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Depth</td>
<td id="S5.T8.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2023</td>
<td id="S5.T8.1.11.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib343" title="" class="ltx_ref">343</a>]</cite></td>
<td id="S5.T8.1.11.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.11.11.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.11.11.7.1.1" class="ltx_tr">
<td id="S5.T8.1.11.11.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">93.0</td>
</tr>
<tr id="S5.T8.1.11.11.7.1.2" class="ltx_tr">
<td id="S5.T8.1.11.11.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">89.8</td>
</tr>
<tr id="S5.T8.1.11.11.7.1.3" class="ltx_tr">
<td id="S5.T8.1.11.11.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.0</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.1.12.12" class="ltx_tr">
<td id="S5.T8.1.12.12.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.12.12.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.12.12.1.1.1" class="ltx_tr">
<td id="S5.T8.1.12.12.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU RGB+D</td>
</tr>
<tr id="S5.T8.1.12.12.1.1.2" class="ltx_tr">
<td id="S5.T8.1.12.12.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU RGB+D120</td>
</tr>
</table>
</td>
<td id="S5.T8.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Softmax</td>
<td id="S5.T8.1.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Two-stream Transformer</td>
<td id="S5.T8.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Depth</td>
<td id="S5.T8.1.12.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2023</td>
<td id="S5.T8.1.12.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib344" title="" class="ltx_ref">344</a>]</cite></td>
<td id="S5.T8.1.12.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.12.12.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.12.12.7.1.1" class="ltx_tr">
<td id="S5.T8.1.12.12.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">94.8</td>
</tr>
<tr id="S5.T8.1.12.12.7.1.2" class="ltx_tr">
<td id="S5.T8.1.12.12.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">93.8</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.1.13.13" class="ltx_tr">
<td id="S5.T8.1.13.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.13.13.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.13.13.1.1.1" class="ltx_tr">
<td id="S5.T8.1.13.13.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU RGB+D</td>
</tr>
<tr id="S5.T8.1.13.13.1.1.2" class="ltx_tr">
<td id="S5.T8.1.13.13.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTU RGB+D120</td>
</tr>
<tr id="S5.T8.1.13.13.1.1.3" class="ltx_tr">
<td id="S5.T8.1.13.13.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NW-UCLA</td>
</tr>
</table>
</td>
<td id="S5.T8.1.13.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Softmax</td>
<td id="S5.T8.1.13.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.13.13.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.13.13.3.1.1" class="ltx_tr">
<td id="S5.T8.1.13.13.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Language</td>
</tr>
<tr id="S5.T8.1.13.13.3.1.2" class="ltx_tr">
<td id="S5.T8.1.13.13.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">knowledge-assisted</td>
</tr>
</table>
</td>
<td id="S5.T8.1.13.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Depth</td>
<td id="S5.T8.1.13.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2023</td>
<td id="S5.T8.1.13.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib345" title="" class="ltx_ref">345</a>]</cite></td>
<td id="S5.T8.1.13.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.13.13.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.13.13.7.1.1" class="ltx_tr">
<td id="S5.T8.1.13.13.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.2</td>
</tr>
<tr id="S5.T8.1.13.13.7.1.2" class="ltx_tr">
<td id="S5.T8.1.13.13.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">91.8</td>
</tr>
<tr id="S5.T8.1.13.13.7.1.3" class="ltx_tr">
<td id="S5.T8.1.13.13.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.6</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.1.14.14" class="ltx_tr">
<td id="S5.T8.1.14.14.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.14.14.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.14.14.1.1.1" class="ltx_tr">
<td id="S5.T8.1.14.14.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UCF51</td>
</tr>
<tr id="S5.T8.1.14.14.1.1.2" class="ltx_tr">
<td id="S5.T8.1.14.14.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Kinetics Sound</td>
</tr>
</table>
</td>
<td id="S5.T8.1.14.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S5.T8.1.14.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">MAIVAR-CH</td>
<td id="S5.T8.1.14.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, audio</td>
<td id="S5.T8.1.14.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2024</td>
<td id="S5.T8.1.14.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib346" title="" class="ltx_ref">346</a>]</cite></td>
<td id="S5.T8.1.14.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.14.14.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.14.14.7.1.1" class="ltx_tr">
<td id="S5.T8.1.14.14.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">87.9</td>
</tr>
<tr id="S5.T8.1.14.14.7.1.2" class="ltx_tr">
<td id="S5.T8.1.14.14.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">79.0</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.1.15.15" class="ltx_tr">
<td id="S5.T8.1.15.15.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.15.15.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.15.15.1.1.1" class="ltx_tr">
<td id="S5.T8.1.15.15.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Drive Act</td>
</tr>
</table>
</td>
<td id="S5.T8.1.15.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S5.T8.1.15.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Dual Feature Shift</td>
<td id="S5.T8.1.15.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Depth, Infrared</td>
<td id="S5.T8.1.15.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2024</td>
<td id="S5.T8.1.15.15.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite></td>
<td id="S5.T8.1.15.15.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.15.15.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.15.15.7.1.1" class="ltx_tr">
<td id="S5.T8.1.15.15.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">77.61</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.1.16.16" class="ltx_tr">
<td id="S5.T8.1.16.16.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.16.16.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.16.16.1.1.1" class="ltx_tr">
<td id="S5.T8.1.16.16.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Florence3DAction</td>
</tr>
<tr id="S5.T8.1.16.16.1.1.2" class="ltx_tr">
<td id="S5.T8.1.16.16.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">UTKinect-Action3D</td>
</tr>
<tr id="S5.T8.1.16.16.1.1.3" class="ltx_tr">
<td id="S5.T8.1.16.16.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">3DActionPairs</td>
</tr>
<tr id="S5.T8.1.16.16.1.1.4" class="ltx_tr">
<td id="S5.T8.1.16.16.1.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">NTURGB+D</td>
</tr>
</table>
</td>
<td id="S5.T8.1.16.16.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SoftMax</td>
<td id="S5.T8.1.16.16.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.16.16.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.16.16.3.1.1" class="ltx_tr">
<td id="S5.T8.1.16.16.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">two-stream</td>
</tr>
<tr id="S5.T8.1.16.16.3.1.2" class="ltx_tr">
<td id="S5.T8.1.16.16.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">spatial-temporal</td>
</tr>
<tr id="S5.T8.1.16.16.3.1.3" class="ltx_tr">
<td id="S5.T8.1.16.16.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">architecture</td>
</tr>
</table>
</td>
<td id="S5.T8.1.16.16.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">RGB, Depth, Infrared</td>
<td id="S5.T8.1.16.16.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2024</td>
<td id="S5.T8.1.16.16.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib347" title="" class="ltx_ref">347</a>]</cite></td>
<td id="S5.T8.1.16.16.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T8.1.16.16.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.16.16.7.1.1" class="ltx_tr">
<td id="S5.T8.1.16.16.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">93.8</td>
</tr>
<tr id="S5.T8.1.16.16.7.1.2" class="ltx_tr">
<td id="S5.T8.1.16.16.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">98.7</td>
</tr>
<tr id="S5.T8.1.16.16.7.1.3" class="ltx_tr">
<td id="S5.T8.1.16.16.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">97.3</td>
</tr>
<tr id="S5.T8.1.16.16.7.1.4" class="ltx_tr">
<td id="S5.T8.1.16.16.7.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">90.2</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Current Challenges</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Although notable progress in HAR utilizing four data modalities, several challenges persist due to the intricate nature of the various aspects of this task.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>RGB Data Modality Based Current Challenges</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">The researcher explores the challenges specific to RGB-based methods in HAR. RGB data, which represents color information from regular images or videos, is widely used for determining human actions. In the following section, we described key challenges associated with RGB-based HAR:</p>
</div>
<section id="S6.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1 </span>Efficient Action Recognition Analysis</h4>

<div id="S6.SS1.SSS1.p1" class="ltx_para">
<p id="S6.SS1.SSS1.p1.1" class="ltx_p">The good performance of numerous HAR approaches often comes with the cost of high computational complexity. However, an efficient HAR system is vital for many real-world applications. Therefore, it is essential to explore ways to minimize computational costs (such as CPU, GPU, and energy usage) to perform efficient and fast HAR. These limitations led to a notable impact on the computation efficiency of the network. Additionally, the process of accurately and efficiently labeling video data incurs substantial labor and time expenses due to the diversity and scale of the data.</p>
</div>
</section>
<section id="S6.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2 </span>Complexity within the Environment</h4>

<div id="S6.SS1.SSS2.p1" class="ltx_para">
<p id="S6.SS1.SSS2.p1.1" class="ltx_p">Certain HAR techniques perform strongly in controlled environments but tend to underperform in uncontrolled outdoor settings. This is mostly caused by motion vector noise, which can drastically degrade resolution. Extracting effective features from complex images is an extremely tough task. For example, the rapid movement of the camera complicates the extraction of effective action features. Accurate feature extraction will also affect environmental issues such as (poor lighting, dynamic background, etc.)</p>
</div>
</section>
<section id="S6.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.3 </span>Large Memory of the Dataset and Limitations </h4>

<div id="S6.SS1.SSS3.p1" class="ltx_para">
<p id="S6.SS1.SSS3.p1.1" class="ltx_p">The dataset exhibits both intra-class variation and inter-class similarity. Many people perform the same action in diverse manners, and even a single person may execute it in multiple ways. Additionally, different actions might have similar presentations. Furthermore, many existing datasets include unfiltered sequences, potentially compromising the timeliness and reducing the HAR accuracy of the model. 
<br class="ltx_break">The dataset’s large memory requirements pose significant limitations, particularly in terms of storage and processing capabilities. Handling massive amounts of data necessitates robust computational resources, including high-capacity storage solutions and powerful processing units. Additionally, working with large datasets may lead to challenges related to data transfer speeds, memory management, and computational efficiency. These limitations can impact the scalability, accessibility, and usability of the dataset, potentially hindering its widespread adoption and utilization in research and applications. Therefore, addressing the constraints posed by the dataset’s large memory footprint is crucial for maximizing its utility and effectiveness in various domains.</p>
</div>
</section>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Skeleton Data Modality Based Challenges</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">The challenges are specific to skeleton-based approaches in HAR. Skeleton data, which obtained joint positions and movements, is a valuable modality for understanding human actions. In the following section, some key challenges are described.</p>
</div>
<section id="S6.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1 </span>Pose Preparation and Analysis</h4>

<div id="S6.SS2.SSS1.p1" class="ltx_para">
<p id="S6.SS2.SSS1.p1.1" class="ltx_p">Depending on depth cameras and sensors, Skeleton data acquisition is affected by environmental complexity, capture duration, and equipment exposure conditions. Another common challenge in daily life scenarios is Occlusion, caused by surrounding objects or human interaction, which further contributes to detection errors in skeletons.</p>
</div>
</section>
<section id="S6.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2 </span>Viewpoint Variation</h4>

<div id="S6.SS2.SSS2.p1" class="ltx_para">
<p id="S6.SS2.SSS2.p1.1" class="ltx_p">Accurately distinguishing skeleton features from different perspectives poses a significant challenge, as certain features may be lost during changes in viewpoint. While modern RGBD cameras <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib348" title="" class="ltx_ref">348</a>, <a href="#bib.bib349" title="" class="ltx_ref">349</a>, <a href="#bib.bib350" title="" class="ltx_ref">350</a>, <a href="#bib.bib351" title="" class="ltx_ref">351</a>]</cite> can normalize 3D human skeletons <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib225" title="" class="ltx_ref">225</a>]</cite> from various angles to a single pose with viewpoint invariance utilizing pose estimation transformation matrices. However, in this process, there is a risk of losing some of the relative motion between the original skeletons. This loss of relative motion can impact the accuracy and completeness of the skeleton data, highlighting the need for careful consideration and validation of viewpoint normalization techniques in skeleton feature extraction.</p>
</div>
</section>
<section id="S6.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.3 </span>Single Scale Data Analysis</h4>

<div id="S6.SS2.SSS3.p1" class="ltx_para">
<p id="S6.SS2.SSS3.p1.1" class="ltx_p">As several skeleton-based datasets mostly provide information based on the scale of body joints, numerous techniques focus solely on extracting features related to the human joint scale. However, this technique often leads to the loss of fine joint features. Moreover, certain actions, such as shaving, tooth brushing, and applying lipstick, exhibit similar joint interactions. Therefore, there is a critical need to enhance local feature extraction while maintaining the effectiveness of holistic feature extraction techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib352" title="" class="ltx_ref">352</a>, <a href="#bib.bib353" title="" class="ltx_ref">353</a>, <a href="#bib.bib354" title="" class="ltx_ref">354</a>, <a href="#bib.bib355" title="" class="ltx_ref">355</a>]</cite>. This improvement is crucial for achieving more accurate action recognition and understanding subtle variations in human movements.
Even though DL methods yield superior recognition performance compared to handcrafted action features, certain challenges persist in recognizing human actions based on DL, particularly in the fusion of multimodal data in DL methods. Most of the aforementioned DL-based approaches concentrate on learning action features from diverse modality data; however, only a few studies address the fusion of multimodal data. The effective fusion based on multimodal data: (RGB, optical flow, depth, and skeleton data) remains a significant unresolved challenge in HAR and DL. This area also represents a prominent research focus within HAR.</p>
</div>
</section>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Sensor Based HAR Current Challenges and Possible Solution</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">In sensor-based HAR, different activities with similar characteristics (like walking and running) pose a challenge for feature extraction. Creating unique features to represent each activity becomes difficult due to this inter-activity similarity.
<br class="ltx_break">Another challenge is annotation scarcity due to expensive data collection and class imbalance, particularly for rare or unexpected activities.
In sensor-based HAR, three critical factors—users, time, and sensors—contribute to distribution discrepancies between training and test data. These factors include person-dependent activity patterns, evolving activity concepts over time, and diverse sensor configurations.
When designing a HAR system, two key considerations are resource efficiency for portable devices and addressing privacy risks associated with continuous life recording. 
<br class="ltx_break">When dealing with sensory data, accurate recognition solutions must address interpretability and understand which parts of the data contribute to recognition and which parts introduce noise.</p>
</div>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Multimodal-Based Challenges</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">In the field of HAR, researchers have explored many multi-modality approaches. These approaches include multi-modality fusion-based and cross-modality transfer learning. The fusion of data from different modalities, which can often complement each other, leads to enhancing HAR accuracy. However, it’s important to note that several existing multi-modality approaches are not as effective due to some challenges, such as overfitting, missing modalities, heterogeneous data modalities, and temporal synchronization. These suggestions that there are still possibilities to develop more effective fusion systems for multi-modality HAR.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We describe several potential directions for future research by amalgamating the current state of affairs and addressing the methodological and application-related challenges in RGB-based,skeleton-based, sensor modality-based, and multimodal-based HAR.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Development of the New Large Scale Datasets</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">Data is as very essential to DL as model construction. However, existing datasets pose challenges when it comes to generalizing to realistic scenes. Factors like realistic surroundings and dataset size play an important role in this complexity. Additionally, most of the datasets are mainly focused on spatial representation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib356" title="" class="ltx_ref">356</a>]</cite>. Unfortunately, there’s a scarcity of long-term modeling datasets. A notable issue arises due to regional constraints and privacy concerns. YouTube dataset managers commonly provide only video IDs or links for download rather than the actual video content. Consequently, some videos become inaccessible over time, resulting in an annual loss of approximately 5% of videos <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. To address these difficulties, researchers are actively working on gathering fresh datasets. These new datasets will contribute to advancing DL research and improving model performance.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Data Augmentation Techniques</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">Deep neural networks exhibit exceptional performance when trained on diverse datasets. However, limited data availability remains still a challenge. To overcome this issue, data augmentation plays an important role. In the domain of image recognition, various augmentation techniques have been proposed, spanning both DL-based techniques and simple image-processing approaches.</p>
</div>
<div id="S7.SS2.p2" class="ltx_para">
<p id="S7.SS2.p2.1" class="ltx_p">These approaches include random erasing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib357" title="" class="ltx_ref">357</a>]</cite> , generative adversarial networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib358" title="" class="ltx_ref">358</a>]</cite>, kernel filters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib359" title="" class="ltx_ref">359</a>]</cite>, feature space augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib360" title="" class="ltx_ref">360</a>]</cite>, adversarial training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib361" title="" class="ltx_ref">361</a>]</cite>, generative adversarial networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib358" title="" class="ltx_ref">358</a>]</cite>, and meta-learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib362" title="" class="ltx_ref">362</a>]</cite>.
For HAR, typical data augmentation techniques involve horizontal flipping, subclip extraction, and video merging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib363" title="" class="ltx_ref">363</a>]</cite>. However, these generated videos often lack realism. To overcome this limitation, Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib364" title="" class="ltx_ref">364</a>]</cite> used GANs to generate new data samples and implemented a ’self-paced selection’ strategy during training. Meanwhile, Gowda et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib365" title="" class="ltx_ref">365</a>]</cite> introduced Learn2Augment, which synthesizes videos from foreground and background sources as a method for data augmentation, resulting in diverse and realistic samples.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Advancements in Models Performances</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">HAR research predominantly revolves around DL-based models, much like other advancements in computer vision. Presently, ongoing progress in deep architectures is important for HAR including the RGB-based, skeleton-based, and multimodal-based approaches to perform the action recognition task. These advancements typically focus on the following key areas of model improvement.</p>
</div>
<div id="S7.SS3.p2" class="ltx_para">
<ul id="S7.I1" class="ltx_itemize">
<li id="S7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i1.p1" class="ltx_para">
<p id="S7.I1.i1.p1.1" class="ltx_p">Long-term Dependency Analysis:
Long-term correlations refer to the unfolding sequence of actions that occur over extended periods, akin to how memories are stored in our brains. When we reminisce about an event, one pattern naturally triggers the next. In the context of action recognition, it is important to focus not only on spatial modeling but also on the temporal component. This emphasis arises from the remarkably strong correlations observed between adjacent temporal features.</p>
</div>
</li>
<li id="S7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i2.p1" class="ltx_para">
<p id="S7.I1.i2.p1.1" class="ltx_p">Multimodal Modeling:
This involves integrating data from various devices, such as audiovisual sensors. There are two primary approaches to multi-modality video understanding.</p>
</div>
</li>
<li id="S7.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i3.p1" class="ltx_para">
<p id="S7.I1.i3.p1.1" class="ltx_p">Enhancing Video Representations:
The multi-modality data (such as depth, skeleton, and RGB information) is used to improve video representations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib366" title="" class="ltx_ref">366</a>, <a href="#bib.bib367" title="" class="ltx_ref">367</a>]</cite>. These representations can include scene understanding, object recognition, action detection, and audio analysis, using multimodality data like RGB, skeleton, and depth.</p>
</div>
</li>
</ul>
</div>
<div id="S7.SS3.p3" class="ltx_para">
<ul id="S7.I2" class="ltx_itemize">
<li id="S7.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I2.i1.p1" class="ltx_para">
<p id="S7.I2.i1.p1.1" class="ltx_p">Efficient Modeling Analysis:
Creating an efficient network architecture is crucial due to the challenges posed by existing models, including model complexity, excessive parameters, and real-time performance limitations. To address these issues, techniques like distributed training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib368" title="" class="ltx_ref">368</a>]</cite>, mobile networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib369" title="" class="ltx_ref">369</a>]</cite>, hybrid precision training, model compression, quantization, and pruning can be explored. These approaches can enhance both efficiency and effectiveness in image classification tasks.</p>
</div>
</li>
<li id="S7.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I2.i2.p1" class="ltx_para">
<p id="S7.I2.i2.p1.1" class="ltx_p">Semi-supervised and Unsupervised Learning Approaches:
Supervised learning approaches, especially those based on deep learning, typically require large, expensive labeled datasets for model training. In contrast, unsupervised and semi-supervised learning techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib370" title="" class="ltx_ref">370</a>]</cite> can utilize unlabeled data to train models, thereby reducing the need for extensive labeled datasets. Given that unlabeled action samples are often easier to collect, unsupervised and semi-supervised approaches to Human Activity Recognition (HAR) represent a crucial research direction deserving further exploration.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S7.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4 </span>Video Lengths in Human Action Recognition</h3>

<div id="S7.SS4.p1" class="ltx_para">
<p id="S7.SS4.p1.4" class="ltx_p">The action prediction tasks can be broadly categorized into short-term and long-term predictions. Short-term prediction involves predicting action labels from partially observed actions, typically seen in short videos lasting a few seconds. In contrast, long-term prediction assumes that current actions influence future actions and focuses on longer videos spanning several minutes, simulating changes in actions over time. Formally, given an action video <math id="S7.SS4.p1.1.m1.1" class="ltx_Math" alttext="x_{a}" display="inline"><semantics id="S7.SS4.p1.1.m1.1a"><msub id="S7.SS4.p1.1.m1.1.1" xref="S7.SS4.p1.1.m1.1.1.cmml"><mi id="S7.SS4.p1.1.m1.1.1.2" xref="S7.SS4.p1.1.m1.1.1.2.cmml">x</mi><mi id="S7.SS4.p1.1.m1.1.1.3" xref="S7.SS4.p1.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.1.m1.1b"><apply id="S7.SS4.p1.1.m1.1.1.cmml" xref="S7.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S7.SS4.p1.1.m1.1.1.1.cmml" xref="S7.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S7.SS4.p1.1.m1.1.1.2.cmml" xref="S7.SS4.p1.1.m1.1.1.2">𝑥</ci><ci id="S7.SS4.p1.1.m1.1.1.3.cmml" xref="S7.SS4.p1.1.m1.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.1.m1.1c">x_{a}</annotation></semantics></math>, which may depict either a complete or incomplete action sequence, the objective is to predict the subsequent action <math id="S7.SS4.p1.2.m2.1" class="ltx_Math" alttext="x_{b}" display="inline"><semantics id="S7.SS4.p1.2.m2.1a"><msub id="S7.SS4.p1.2.m2.1.1" xref="S7.SS4.p1.2.m2.1.1.cmml"><mi id="S7.SS4.p1.2.m2.1.1.2" xref="S7.SS4.p1.2.m2.1.1.2.cmml">x</mi><mi id="S7.SS4.p1.2.m2.1.1.3" xref="S7.SS4.p1.2.m2.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.2.m2.1b"><apply id="S7.SS4.p1.2.m2.1.1.cmml" xref="S7.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S7.SS4.p1.2.m2.1.1.1.cmml" xref="S7.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S7.SS4.p1.2.m2.1.1.2.cmml" xref="S7.SS4.p1.2.m2.1.1.2">𝑥</ci><ci id="S7.SS4.p1.2.m2.1.1.3.cmml" xref="S7.SS4.p1.2.m2.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.2.m2.1c">x_{b}</annotation></semantics></math>. These actions, <math id="S7.SS4.p1.3.m3.1" class="ltx_Math" alttext="x_{a}" display="inline"><semantics id="S7.SS4.p1.3.m3.1a"><msub id="S7.SS4.p1.3.m3.1.1" xref="S7.SS4.p1.3.m3.1.1.cmml"><mi id="S7.SS4.p1.3.m3.1.1.2" xref="S7.SS4.p1.3.m3.1.1.2.cmml">x</mi><mi id="S7.SS4.p1.3.m3.1.1.3" xref="S7.SS4.p1.3.m3.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.3.m3.1b"><apply id="S7.SS4.p1.3.m3.1.1.cmml" xref="S7.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S7.SS4.p1.3.m3.1.1.1.cmml" xref="S7.SS4.p1.3.m3.1.1">subscript</csymbol><ci id="S7.SS4.p1.3.m3.1.1.2.cmml" xref="S7.SS4.p1.3.m3.1.1.2">𝑥</ci><ci id="S7.SS4.p1.3.m3.1.1.3.cmml" xref="S7.SS4.p1.3.m3.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.3.m3.1c">x_{a}</annotation></semantics></math> and <math id="S7.SS4.p1.4.m4.1" class="ltx_Math" alttext="x_{b}" display="inline"><semantics id="S7.SS4.p1.4.m4.1a"><msub id="S7.SS4.p1.4.m4.1.1" xref="S7.SS4.p1.4.m4.1.1.cmml"><mi id="S7.SS4.p1.4.m4.1.1.2" xref="S7.SS4.p1.4.m4.1.1.2.cmml">x</mi><mi id="S7.SS4.p1.4.m4.1.1.3" xref="S7.SS4.p1.4.m4.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.4.m4.1b"><apply id="S7.SS4.p1.4.m4.1.1.cmml" xref="S7.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S7.SS4.p1.4.m4.1.1.1.cmml" xref="S7.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S7.SS4.p1.4.m4.1.1.2.cmml" xref="S7.SS4.p1.4.m4.1.1.2">𝑥</ci><ci id="S7.SS4.p1.4.m4.1.1.3.cmml" xref="S7.SS4.p1.4.m4.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.4.m4.1c">x_{b}</annotation></semantics></math>, are independent yet semantically significant, with a temporal relationship <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.
To advance action prediction research, it is essential to discover and model temporal correlations within vast datasets. Unexplored directions include understanding interpretability across different time scales, devising effective methods for modeling long-term dependencies, and leveraging multimodal data to enhance predictive models.</p>
</div>
</section>
<section id="S7.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5 </span>Limitations</h3>

<div id="S7.SS5.p1" class="ltx_para">
<p id="S7.SS5.p1.1" class="ltx_p">This study was focused on research papers published between 2014 and 2024, exclusively in English, excluding relevant studies in other languages. We exclusively considered studies that utilized visual data, including HAR feature ML-based and DL-based methods involving different data types, including RGB handcrafted features and DL-based action recognition RGB and skeleton-based methods for multimodal datasets such as RGB, depth, and skeleton, excluding on EMG based data. Furthermore, the diverse input methods and dataset variations across reviewed studies hindered direct result comparisons. Notably, some articles lacked statistical confidence intervals, making it challenging to compare their findings.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">HAR is an important task among multiple domains within the field of computer vision, including human-computer interaction, robotics, surveillance, and security. In the past decades, it has necessitated the proficient comprehension and interpretation of human actions with various data modalities. Researchers still find the HAR task challenging in real scenes due to various complicating factors in different data modalities, including various body positions, motions, and complex background occlusion. In the study, we presented a comprehensive survey of HAR methods, including advancements across various data modalities. We briefly reviewed human action recognition techniques, including hand-crafted features in RGB, skeleton, sensor, and multi-modality fusion with conventional and end-to-end DL-based action representation techniques. Moreover, we have also reviewed the most popular benchmark datasets of the RGB, skeleton, sensor, and fusion-based modalities with the latest performance accuracy. After providing an overview of the literature about each research direction in human activity recognition, the primary effective techniques were presented to familiarize researchers with the relevant research domains. The fundamental findings of this investigation on the study of human action recognition are summarized to help researchers, especially in the field of HAR.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Author contributions</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">All authors contributed equally to this work.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Funding</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">This work was supported by the Competitive Research Fund of The University of Aizu, Japan.</p>
</div>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Data Availability</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">The data used to support the findings of this study are included in the article</p>
</div>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Conflict of interest</h2>

<div id="Sx4.p1" class="ltx_para">
<p id="Sx4.p1.1" class="ltx_p">The authors declare no conflict of interest.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.1.1" class="ltx_bibitem">
<span class="ltx_bibblock"><span id="bib.1.1.1.1" class="ltx_ERROR undefined">\bibcommenthead</span>
</span>
</li>
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papadopoulos et al. [2014]</span>
<span class="ltx_bibblock">
Papadopoulos, G.T.,
Axenopoulos, A.,
Daras, P.:
Real-time skeleton-tracking-based human action recognition using kinect data.
In: MultiMedia Modeling: 20th Anniversary International Conference, MMM 2014, Dublin, Ireland, January 6-10, 2014, Proceedings, Part I 20,
pp. 473–483
(2014).
Springer


</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Presti and La Cascia [2016]</span>
<span class="ltx_bibblock">
Presti, L.L.,
La Cascia, M.:
3d skeleton-based human action classification: A survey.
Pattern Recognition
<span id="bib.bib2.1.1" class="ltx_text ltx_font_bold">53</span>,
130–147
(2016)


</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Islam et al. [2024]</span>
<span class="ltx_bibblock">
Islam, M.N.,
Jahangir, R.,
Mohim, N.S.,
Wasif-Ul-Islam, M.,
Ashraf, A.,
Khan, N.I.,
Mahjabin, M.R.,
Miah, A.S.M.,
Shin, J.:
A multilingual handwriting learning system for visually impaired people.
IEEE Access
(2024)


</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rahim et al. [2020]</span>
<span class="ltx_bibblock">
Rahim, M.A.,
Miah, A.S.M.,
Sayeed, A.,
Shin, J.:
Hand gesture recognition based on optimal segmentation in human-computer interaction.
In: 2020 3rd IEEE International Conference on Knowledge Innovation and Invention (ICKII),
pp. 163–166
(2020).
IEEE


</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Gemert et al. [2015]</span>
<span class="ltx_bibblock">
Van Gemert, J.C.,
Jain, M.,
Gati, E.,
Snoek, C.G., <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">et al.</span>:
Apt: Action localization proposals from dense trajectories.
In: BMVC,
vol. 2,
p. 4
(2015)


</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2017]</span>
<span class="ltx_bibblock">
Zhu, H.,
Vial, R.,
Lu, S.:
Tornado: A spatio-temporal convolutional regression network for video action proposal.
In: Proceedings of the IEEE International Conference on Computer Vision,
pp. 5813–5821
(2017)


</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lara and Labrador [2012]</span>
<span class="ltx_bibblock">
Lara, O.D.,
Labrador, M.A.:
A survey on human activity recognition using wearable sensors.
IEEE communications surveys &amp; tutorials
<span id="bib.bib7.1.1" class="ltx_text ltx_font_bold">15</span>(3),
1192–1209
(2012)


</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ziaeefard and Bergevin [2015]</span>
<span class="ltx_bibblock">
Ziaeefard, M.,
Bergevin, R.:
Semantic human activity recognition: A literature review.
Pattern Recognition
<span id="bib.bib8.1.1" class="ltx_text ltx_font_bold">48</span>(8),
2329–2345
(2015)


</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kibria et al. [2020]</span>
<span class="ltx_bibblock">
Kibria, K.A.,
Noman, A.S.,
Hossain, M.A.,
Bulbul, M.S.I.,
Rashid, M.M.,
Miah, A.S.M.:
Creation of a cost-efficient and effective personal assistant robot using arduino &amp; machine learning algorithm.
In: 2020 IEEE Region 10 Symposium (TENSYMP),
pp. 477–482
(2020).
IEEE


</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2011]</span>
<span class="ltx_bibblock">
Wu, S.,
Oreifej, O.,
Shah, M.:
Action recognition in videos acquired by a moving camera using motion decomposition of lagrangian particle trajectories.
In: 2011 International Conference on Computer Vision,
pp. 1419–1426
(2011).
IEEE


</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Herath et al. [2017]</span>
<span class="ltx_bibblock">
Herath, S.,
Harandi, M.,
Porikli, F.:
Going deeper into action recognition: A survey.
Image and vision computing
<span id="bib.bib11.1.1" class="ltx_text ltx_font_bold">60</span>,
4–21
(2017)


</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chao et al. [2015]</span>
<span class="ltx_bibblock">
Chao, Y.-W.,
Wang, Z.,
He, Y.,
Wang, J.,
Deng, J.:
Hico: A benchmark for recognizing human-object interactions in images.
In: Proceedings of the IEEE International Conference on Computer Vision,
pp. 1017–1025
(2015)


</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le et al. [2014]</span>
<span class="ltx_bibblock">
Le, D.-T.,
Uijlings, J.,
Bernardi, R.:
Tuhoi: Trento universal human object interaction dataset.
In: Proceedings of the Third Workshop on Vision and Language,
pp. 17–24
(2014)


</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng and Schmid [2016]</span>
<span class="ltx_bibblock">
Peng, X.,
Schmid, C.:
Multi-region two-stream r-cnn for action detection.
In: Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14,
pp. 744–759
(2016).
Springer


</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2018]</span>
<span class="ltx_bibblock">
Liu, J.,
Li, Y.,
Song, S.,
Xing, J.,
Lan, C.,
Zeng, W.:
Multi-modality multi-task recurrent neural network for online action detection.
IEEE Transactions on Circuits and Systems for Video Technology
<span id="bib.bib15.1.1" class="ltx_text ltx_font_bold">29</span>(9),
2667–2682
(2018)


</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patrona et al. [2018]</span>
<span class="ltx_bibblock">
Patrona, F.,
Chatzitofis, A.,
Zarpalas, D.,
Daras, P.:
Motion analysis: Action detection, recognition and evaluation based on motion capture data.
Pattern Recognition
<span id="bib.bib16.1.1" class="ltx_text ltx_font_bold">76</span>,
612–622
(2018)


</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bengio et al. [2013]</span>
<span class="ltx_bibblock">
Bengio, Y.,
Courville, A.,
Vincent, P.:
Representation learning: A review and new perspectives.
IEEE Transactions on pattern analysis and machine intelligence
<span id="bib.bib17.1.1" class="ltx_text ltx_font_bold">35</span>(8),
1798–1828
(2013)


</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das Dawn and Shaikh [2016]</span>
<span class="ltx_bibblock">
Das Dawn, D.,
Shaikh, S.H.:
A comprehensive survey of human action recognition with spatio-temporal interest point (stip) detector.
The Visual Computer
<span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">32</span>,
289–306
(2016)


</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. [2014]</span>
<span class="ltx_bibblock">
Nguyen, T.V.,
Song, Z.,
Yan, S.:
Stap: Spatial-temporal attention-aware pooling for action recognition.
IEEE Transactions on Circuits and Systems for Video Technology
<span id="bib.bib19.1.1" class="ltx_text ltx_font_bold">25</span>(1),
77–86
(2014)


</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. [2013]</span>
<span class="ltx_bibblock">
Shao, L.,
Zhen, X.,
Tao, D.,
Li, X.:
Spatio-temporal laplacian pyramid coding for action recognition.
IEEE Transactions on Cybernetics
<span id="bib.bib20.1.1" class="ltx_text ltx_font_bold">44</span>(6),
817–827
(2013)


</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burghouts et al. [2014]</span>
<span class="ltx_bibblock">
Burghouts, G.,
Schutte, K.,
Hove, R.J.-M.,
Broek, S.,
Baan, J.,
Rajadell, O.,
Huis, J.,
Rest, J.,
Hanckmann, P.,
Bouma, H., <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">et al.</span>:
Instantaneous threat detection based on a semantic representation of activities, zones and trajectories.
Signal, Image and Video Processing
<span id="bib.bib21.2.2" class="ltx_text ltx_font_bold">8</span>,
191–200
(2014)


</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang and Tian [2014]</span>
<span class="ltx_bibblock">
Yang, X.,
Tian, Y.:
Super normal vector for activity recognition using depth sequences.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 804–811
(2014)


</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. [2013]</span>
<span class="ltx_bibblock">
Ye, M.,
Zhang, Q.,
Wang, L.,
Zhu, J.,
Yang, R.,
Gall, J.:
A survey on human motion analysis from depth data.
In: Time-of-flight and Depth Imaging. Sensors, Algorithms, and Applications: Dagstuhl 2012 Seminar on Time-of-flight Imaging and GCPR 2013 Workshop on Imaging New Modalities,
pp. 149–187
(2013).
Springer


</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2016]</span>
<span class="ltx_bibblock">
Li, M.,
Leung, H.,
Shum, H.P.:
Human action recognition via skeletal and depth based feature fusion.
In: Proceedings of the 9th International Conference on Motion in Games,
pp. 123–132
(2016)


</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang and Tian [2014]</span>
<span class="ltx_bibblock">
Yang, X.,
Tian, Y.:
Effective 3d action recognition using eigenjoints.
Journal of Visual Communication and Image Representation
<span id="bib.bib25.1.1" class="ltx_text ltx_font_bold">25</span>(1),
2–11
(2014)


</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and Zisserman [2014]</span>
<span class="ltx_bibblock">
Simonyan, K.,
Zisserman, A.:
Two-stream convolutional networks for action recognition in videos.
Advances in neural information processing systems
<span id="bib.bib26.1.1" class="ltx_text ltx_font_bold">27</span>
(2014)


</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran et al. [2015]</span>
<span class="ltx_bibblock">
Tran, D.,
Bourdev, L.,
Fergus, R.,
Torresani, L.,
Paluri, M.:
Learning spatiotemporal features with 3d convolutional networks.
In: Proceedings of the IEEE International Conference on Computer Vision,
pp. 4489–4497
(2015)


</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Yu, K.,
Yun, F.:
Human action recognition and prediction: A survey. arxiv 2018.
arXiv preprint arXiv:1806.11230


</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2019]</span>
<span class="ltx_bibblock">
Wang, J.,
Chen, Y.,
Hao, S.,
Peng, X.,
Hu, L.:
Deep learning for sensor-based activity recognition: A survey.
Pattern recognition letters
<span id="bib.bib29.1.1" class="ltx_text ltx_font_bold">119</span>,
3–11
(2019)


</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vrigkas et al. [2015]</span>
<span class="ltx_bibblock">
Vrigkas, M.,
Nikou, C.,
Kakadiaris, I.A.:
A review of human activity recognition methods.
Frontiers in Robotics and AI
<span id="bib.bib30.1.1" class="ltx_text ltx_font_bold">2</span>,
28
(2015)


</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vishwakarma and Agrawal [2013]</span>
<span class="ltx_bibblock">
Vishwakarma, S.,
Agrawal, A.:
A survey on activity recognition and behavior understanding in video surveillance.
The Visual Computer
<span id="bib.bib31.1.1" class="ltx_text ltx_font_bold">29</span>,
983–1009
(2013)


</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke et al. [2013]</span>
<span class="ltx_bibblock">
Ke, S.-R.,
Thuc, H.L.U.,
Lee, Y.-J.,
Hwang, J.-N.,
Yoo, J.-H.,
Choi, K.-H.:
A review on video-based human activity recognition.
Computers
<span id="bib.bib32.1.1" class="ltx_text ltx_font_bold">2</span>(2),
88–131
(2013)


</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2020]</span>
<span class="ltx_bibblock">
Zhu, Y.,
Li, X.,
Liu, C.,
Zolfaghari, M.,
Xiong, Y.,
Wu, C.,
Zhang, Z.,
Tighe, J.,
Manmatha, R.,
Li, M.:
A comprehensive study of deep video action recognition.
arXiv preprint arXiv:2012.06567
(2020)


</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2019]</span>
<span class="ltx_bibblock">
Zhang, H.-B.,
Zhang, Y.-X.,
Zhong, B.,
Lei, Q.,
Yang, L.,
Du, J.-X.,
Chen, D.-S.:
A comprehensive survey of vision-based human action recognition methods.
Sensors
<span id="bib.bib34.1.1" class="ltx_text ltx_font_bold">19</span>(5),
1005
(2019)


</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kong and Fu [2022]</span>
<span class="ltx_bibblock">
Kong, Y.,
Fu, Y.:
Human action recognition and prediction: A survey.
International Journal of Computer Vision
<span id="bib.bib35.1.1" class="ltx_text ltx_font_bold">130</span>(5),
1366–1401
(2022)


</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. [2022]</span>
<span class="ltx_bibblock">
Ma, N.,
Wu, Z.,
Cheung, Y.-m.,
Guo, Y.,
Gao, Y.,
Li, J.,
Jiang, B.:
A survey of human action recognition and posture prediction.
Tsinghua Science and Technology
<span id="bib.bib36.1.1" class="ltx_text ltx_font_bold">27</span>(6),
973–1001
(2022)


</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xing and Zhu [2021]</span>
<span class="ltx_bibblock">
Xing, Y.,
Zhu, J.:
Deep learning-based action recognition with 3D skeleton: A survey.
Wiley Online Library
(2021)


</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Subetha and Chitrakala [2016]</span>
<span class="ltx_bibblock">
Subetha, T.,
Chitrakala, S.:
A survey on human activity recognition from videos.
In: 2016 International Conference on Information Communication and Embedded Systems (ICICES),
pp. 1–7
(2016).
IEEE


</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng and Meunier [2022]</span>
<span class="ltx_bibblock">
Feng, M.,
Meunier, J.:
Skeleton graph-neural-network-based human action recognition: A survey.
Sensors
<span id="bib.bib39.1.1" class="ltx_text ltx_font_bold">22</span>(6),
2091
(2022)


</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. [2022]</span>
<span class="ltx_bibblock">
Feng, L.,
Zhao, Y.,
Zhao, W.,
Tang, J.:
A comparative review of graph convolutional networks for human skeleton-based action recognition.
Artificial Intelligence Review,
1–31
(2022)


</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al. [2021]</span>
<span class="ltx_bibblock">
Gupta, P.,
Thatipelli, A.,
Aggarwal, A.,
Maheshwari, S.,
Trivedi, N.,
Das, S.,
Sarvadevabhatla, R.K.:
Quo vadis, skeleton action recognition?
International Journal of Computer Vision
<span id="bib.bib41.1.1" class="ltx_text ltx_font_bold">129</span>(7),
2097–2112
(2021)


</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. [2021]</span>
<span class="ltx_bibblock">
Song, L.,
Yu, G.,
Yuan, J.,
Liu, Z.:
Human pose estimation and its application to action recognition: A survey.
Journal of Visual Communication and Image Representation
<span id="bib.bib42.1.1" class="ltx_text ltx_font_bold">76</span>,
103055
(2021)


</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shaikh and Chai [2021]</span>
<span class="ltx_bibblock">
Shaikh, M.B.,
Chai, D.:
Rgb-d data-based action recognition: a review.
Sensors
<span id="bib.bib43.1.1" class="ltx_text ltx_font_bold">21</span>(12),
4246
(2021)


</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Majumder and Kehtarnavaz [2020]</span>
<span class="ltx_bibblock">
Majumder, S.,
Kehtarnavaz, N.:
Vision and inertial sensing fusion for human action recognition: A review.
IEEE Sensors Journal
<span id="bib.bib44.1.1" class="ltx_text ltx_font_bold">21</span>(3),
2454–2467
(2020)


</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2019]</span>
<span class="ltx_bibblock">
Wang, L.,
Huynh, D.Q.,
Koniusz, P.:
A comparative review of recent kinect-based action recognition algorithms.
IEEE Transactions on Image Processing
<span id="bib.bib45.1.1" class="ltx_text ltx_font_bold">29</span>,
15–28
(2019)


</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Yan [2023]</span>
<span class="ltx_bibblock">
Wang, C.,
Yan, J.:
A comprehensive survey of rgb-based and skeleton-based human action recognition.
IEEE Access
(2023)


</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2022]</span>
<span class="ltx_bibblock">
Sun, Z.,
Ke, Q.,
Rahmani, H.,
Bennamoun, M.,
Wang, G.,
Liu, J.:
Human action recognition from various data modalities: A review.
IEEE transactions on pattern analysis and machine intelligence
<span id="bib.bib47.1.1" class="ltx_text ltx_font_bold">45</span>(3),
3200–3225
(2022)


</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ullah et al. [2019]</span>
<span class="ltx_bibblock">
Ullah, A.,
Muhammad, K.,
Haq, I.U.,
Baik, S.W.:
Action recognition using optimized deep autoencoder and cnn for surveillance data streams of non-stationary environments.
Future Generation Computer Systems
<span id="bib.bib48.1.1" class="ltx_text ltx_font_bold">96</span>,
386–397
(2019)


</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lan et al. [2015]</span>
<span class="ltx_bibblock">
Lan, Z.,
Lin, M.,
Li, X.,
Hauptmann, A.G.,
Raj, B.:
Beyond gaussian pyramid: Multi-skip feature stacking for action recognition.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 204–212
(2015)


</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caba Heilbron et al. [2015]</span>
<span class="ltx_bibblock">
Caba Heilbron, F.,
Escorcia, V.,
Ghanem, B.,
Carlos Niebles, J.:
Activitynet: A large-scale video benchmark for human activity understanding.
In: Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition,
pp. 961–970
(2015)


</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2022]</span>
<span class="ltx_bibblock">
Li, K.,
Wang, Y.,
He, Y.,
Li, Y.,
Wang, Y.,
Wang, L.,
Qiao, Y.:
Uniformerv2: Spatiotemporal learning by arming image vits with video uniformer.
arXiv preprint arXiv:2211.09552
(2022)


</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kay et al. [2017]</span>
<span class="ltx_bibblock">
Kay, W.,
Carreira, J.,
Simonyan, K.,
Zhang, B.,
Hillier, C.,
Vijayanarasimhan, S.,
Viola, F.,
Green, T.,
Back, T.,
Natsev, P., et al.:
The kinetics human action video dataset.
arXiv preprint arXiv:1705.06950
(2017)


</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carreira et al. [2019]</span>
<span class="ltx_bibblock">
Carreira, J.,
Noland, E.,
Hillier, C.,
Zisserman, A.:
A short note on the kinetics-700 human action dataset.
arXiv preprint arXiv:1907.06987
(2019)


</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2024]</span>
<span class="ltx_bibblock">
Wang, Y.,
Li, K.,
Li, X.,
Yu, J.,
He, Y.,
Chen, G.,
Pei, B.,
Zheng, R.,
Xu, J.,
Wang, Z., et al.:
Internvideo2: Scaling video foundation models for multimodal video understanding.
arXiv preprint arXiv:2403.15377
(2024)


</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. [2018]</span>
<span class="ltx_bibblock">
Gu, C.,
Sun, C.,
Ross, D.A.,
Vondrick, C.,
Pantofaru, C.,
Li, Y.,
Vijayanarasimhan, S.,
Toderici, G.,
Ricco, S.,
Sukthankar, R., <span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">et al.</span>:
Ava: A video dataset of spatio-temporally localized atomic visual actions.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 6047–6056
(2018)


</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheng et al. [2018]</span>
<span class="ltx_bibblock">
Sheng, K.,
Dong, W.,
Ma, C.,
Mei, X.,
Huang, F.,
Hu, B.-G.:
Attention-based multi-patch aggregation for image aesthetic assessment.
In: Proceedings of the 26th ACM International Conference on Multimedia,
pp. 879–886
(2018)


</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Monfort et al. [2019]</span>
<span class="ltx_bibblock">
Monfort, M.,
Andonian, A.,
Zhou, B.,
Ramakrishnan, K.,
Bargal, S.A.,
Yan, T.,
Brown, L.,
Fan, Q.,
Gutfreund, D.,
Vondrick, C., <span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">et al.</span>:
Moments in time dataset: one million videos for event understanding.
IEEE transactions on pattern analysis and machine intelligence
<span id="bib.bib57.2.2" class="ltx_text ltx_font_bold">42</span>(2),
502–508
(2019)


</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Theodorakopoulos et al. [2014]</span>
<span class="ltx_bibblock">
Theodorakopoulos, I.,
Kastaniotis, D.,
Economou, G.,
Fotopoulos, S.:
Pose-based human action recognition via sparse representation in dissimilarity space.
Journal of Visual Communication and Image Representation
<span id="bib.bib58.1.1" class="ltx_text ltx_font_bold">25</span>(1),
12–23
(2014)


</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2022]</span>
<span class="ltx_bibblock">
Zhou, Q.,
Rasol, J.,
Xu, Y.,
Zhang, Z.,
Hu, L.:
A high-performance gait recognition method based on n-fold bernoulli theory.
IEEE Access
<span id="bib.bib59.1.1" class="ltx_text ltx_font_bold">10</span>,
115744–115757
(2022)


</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Damen et al. [2018]</span>
<span class="ltx_bibblock">
Damen, D.,
Doughty, H.,
Farinella, G.M.,
Fidler, S.,
Furnari, A.,
Kazakos, E.,
Moltisanti, D.,
Munro, J.,
Perrett, T.,
Price, W., <span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">et al.</span>:
Scaling egocentric vision: The epic-kitchens dataset.
In: Proceedings of the European Conference on Computer Vision (ECCV),
pp. 720–736
(2018)


</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al. [2018]</span>
<span class="ltx_bibblock">
Tian, Y.,
Shi, J.,
Li, B.,
Duan, Z.,
Xu, C.:
Audio-visual event localization in unconstrained videos.
In: Proceedings of the European Conference on Computer Vision (ECCV),
pp. 247–263
(2018)


</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miech et al. [2020]</span>
<span class="ltx_bibblock">
Miech, A.,
Alayrac, J.-B.,
Laptev, I.,
Sivic, J.,
Zisserman, A.:
Rareact: A video dataset of unusual interactions.
arXiv preprint arXiv:2008.01018
(2020)


</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2020]</span>
<span class="ltx_bibblock">
Lin, W.,
Liu, H.,
Liu, S.,
Li, Y.,
Qian, R.,
Wang, T.,
Xu, N.,
Xiong, H.,
Qi, G.-J.,
Sebe, N.:
Human in events: A large-scale benchmark for human-centric video analysis in complex events.
arXiv preprint arXiv:2005.04490
(2020)


</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duan [2024]</span>
<span class="ltx_bibblock">
Duan, X.:
Abnormal behavior recognition for human motion based on improved deep reinforcement learning.
International Journal of Image and Graphics
<span id="bib.bib64.1.1" class="ltx_text ltx_font_bold">24</span>(01),
2550029
(2024)


</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2012]</span>
<span class="ltx_bibblock">
Wang, J.,
Liu, Z.,
Wu, Y.,
Yuan, J.:
Mining actionlet ensemble for action recognition with depth cameras.
In: 2012 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1290–1297
(2012).
IEEE


</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shahroudy et al. [2017]</span>
<span class="ltx_bibblock">
Shahroudy, A.,
Ng, T.-T.,
Gong, Y.,
Wang, G.:
Deep multimodal feature analysis for action recognition in rgb+ d videos.
IEEE transactions on pattern analysis and machine intelligence
<span id="bib.bib66.1.1" class="ltx_text ltx_font_bold">40</span>(5),
1045–1058
(2017)


</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2014]</span>
<span class="ltx_bibblock">
Wang, J.,
Nie, X.,
Xia, Y.,
Wu, Y.,
Zhu, S.-C.:
Cross-view action modeling, learning and recognition.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 2649–2656
(2014)


</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. [2024]</span>
<span class="ltx_bibblock">
Cheng, Q.,
Cheng, J.,
Liu, Z.,
Ren, Z.,
Liu, J.:
A dense-sparse complementary network for human action recognition based on rgb and skeleton modalities.
Expert Systems with Applications
<span id="bib.bib68.1.1" class="ltx_text ltx_font_bold">244</span>,
123061
(2024)


</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2014]</span>
<span class="ltx_bibblock">
Liu, A.-A.,
Su, Y.-T.,
Jia, P.-P.,
Gao, Z.,
Hao, T.,
Yang, Z.-X.:
Multiple/single-view human action recognition via part-induced multitask structural learning.
IEEE transactions on cybernetics
<span id="bib.bib69.1.1" class="ltx_text ltx_font_bold">45</span>(6),
1194–1208
(2014)


</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2015]</span>
<span class="ltx_bibblock">
Chen, C.,
Jafari, R.,
Kehtarnavaz, N.:
Utd-mhad: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor.
In: 2015 IEEE International Conference on Image Processing (ICIP),
pp. 168–172
(2015).
IEEE


</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Yuan [2018]</span>
<span class="ltx_bibblock">
Liu, M.,
Yuan, J.:
Recognizing human actions as the evolution of pose estimation maps.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1159–1168
(2018)


</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rahmani et al. [2016]</span>
<span class="ltx_bibblock">
Rahmani, H.,
Mahmood, A.,
Huynh, D.,
Mian, A.:
Histogram of oriented principal components for cross-view action recognition.
IEEE transactions on pattern analysis and machine intelligence
<span id="bib.bib72.1.1" class="ltx_text ltx_font_bold">38</span>(12),
2430–2443
(2016)


</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shahroudy et al. [2016]</span>
<span class="ltx_bibblock">
Shahroudy, A.,
Liu, J.,
Ng, T.-T.,
Wang, G.:
Ntu rgb+ d: A large scale dataset for 3d human activity analysis.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1010–1019
(2016)


</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2017]</span>
<span class="ltx_bibblock">
Liu, C.,
Hu, Y.,
Li, Y.,
Song, S.,
Liu, J.:
Pku-mmd: A large scale benchmark for continuous multi-modal human action understanding.
arXiv preprint arXiv:1703.07475
(2017)


</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019]</span>
<span class="ltx_bibblock">
Li, T.,
Fan, L.,
Zhao, M.,
Liu, Y.,
Katabi, D.:
Making the invisible visible: Action recognition through walls and occlusions.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision,
pp. 872–881
(2019)


</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kong and Fu [2017]</span>
<span class="ltx_bibblock">
Kong, Y.,
Fu, Y.:
Max-margin heterogeneous information machine for rgb-d action recognition.
International Journal of Computer Vision
<span id="bib.bib76.1.1" class="ltx_text ltx_font_bold">123</span>,
350–371
(2017)


</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carreira et al. [2018]</span>
<span class="ltx_bibblock">
Carreira, J.,
Noland, E.,
Banki-Horvath, A.,
Hillier, C.,
Zisserman, A.:
A short note about kinetics-600.
arXiv preprint arXiv:1808.01340
(2018)


</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. [2018]</span>
<span class="ltx_bibblock">
Ji, Y.,
Xu, F.,
Yang, Y.,
Shen, F.,
Shen, H.T.,
Zheng, W.-S.:
A large-scale rgb-d database for arbitrary-view human action recognition.
In: Proceedings of the 26th ACM International Conference on Multimedia,
pp. 1510–1518
(2018)


</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2019]</span>
<span class="ltx_bibblock">
Liu, J.,
Shahroudy, A.,
Perez, M.,
Wang, G.,
Duan, L.-Y.,
Kot, A.C.:
Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding.
IEEE transactions on pattern analysis and machine intelligence
<span id="bib.bib79.1.1" class="ltx_text ltx_font_bold">42</span>(10),
2684–2701
(2019)


</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martin et al. [2019]</span>
<span class="ltx_bibblock">
Martin, M.,
Roitberg, A.,
Haurilet, M.,
Horne, M.,
Reiß, S.,
Voit, M.,
Stiefelhagen, R.:
Drive&amp;act: A multi-modal dataset for fine-grained driver behavior recognition in autonomous vehicles.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision,
pp. 2801–2810
(2019)


</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2024]</span>
<span class="ltx_bibblock">
Lin, D.,
Lee, P.H.Y.,
Li, Y.,
Wang, R.,
Yap, K.-H.,
Li, B.,
Ngim, Y.S.:
Multi-modality action recognition based on dual feature shift in vehicle cabin monitoring.
arXiv preprint arXiv:2401.14838
(2024)


</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kong et al. [2019]</span>
<span class="ltx_bibblock">
Kong, Q.,
Wu, Z.,
Deng, Z.,
Klinkigt, M.,
Tong, B.,
Murakami, T.:
Mmact: A large-scale dataset for cross modal human action understanding.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision,
pp. 8658–8667
(2019)


</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2021]</span>
<span class="ltx_bibblock">
Liu, Y.,
Wang, K.,
Li, G.,
Lin, L.:
Semantics-aware adaptive knowledge distillation for sensor-to-vision action recognition.
IEEE Transactions on Image Processing
<span id="bib.bib83.1.1" class="ltx_text ltx_font_bold">30</span>,
5573–5588
(2021)


</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das et al. [2019]</span>
<span class="ltx_bibblock">
Das, S.,
Dai, R.,
Koperski, M.,
Minciullo, L.,
Garattoni, L.,
Bremond, F.,
Francesca, G.:
Toyota smarthome: Real-world activities of daily living.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision,
pp. 833–842
(2019)


</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ben-Shabat et al. [2021]</span>
<span class="ltx_bibblock">
Ben-Shabat, Y.,
Yu, X.,
Saleh, F.,
Campbell, D.,
Rodriguez-Opazo, C.,
Li, H.,
Gould, S.:
The ikea asm dataset: Understanding people assembling furniture through actions, objects and pose.
In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,
pp. 847–859
(2021)


</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jang et al. [2020]</span>
<span class="ltx_bibblock">
Jang, J.,
Kim, D.,
Park, C.,
Jang, M.,
Lee, J.,
Kim, J.:
Etri-activity3d: A large-scale rgb-d dataset for robots to recognize daily activities of the elderly.
In: 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),
pp. 10990–10997
(2020).
IEEE


</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dokkar et al. [2023]</span>
<span class="ltx_bibblock">
Dokkar, R.R.,
Chaieb, F.,
Drira, H.,
Aberkane, A.:
Convivit–a deep neural network combining convolutions and factorized self-attention for human activity recognition.
arXiv preprint arXiv:2310.14416
(2023)


</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2021]</span>
<span class="ltx_bibblock">
Li, T.,
Liu, J.,
Zhang, W.,
Ni, Y.,
Wang, W.,
Li, Z.:
Uav-human: A large benchmark for human behavior understanding with unmanned aerial vehicles.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 16266–16275
(2021)


</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xian et al. [2024]</span>
<span class="ltx_bibblock">
Xian, R.,
Wang, X.,
Kothandaraman, D.,
Manocha, D.:
Pmi sampler: Patch similarity guided frame selection for aerial action recognition.
In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,
pp. 6982–6991
(2024)


</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel et al. [2018]</span>
<span class="ltx_bibblock">
Patel, C.I.,
Garg, S.,
Zaveri, T.,
Banerjee, A.,
Patel, R.:
Human action recognition using fusion of features for unconstrained video sequences.
Computers &amp; Electrical Engineering
<span id="bib.bib90.1.1" class="ltx_text ltx_font_bold">70</span>,
284–301
(2018)


</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2011]</span>
<span class="ltx_bibblock">
Liu, J.,
Kuipers, B.,
Savarese, S.:
Recognizing human actions by attributes.
In: CVPR 2011,
pp. 3337–3344
(2011).
IEEE


</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. [2011]</span>
<span class="ltx_bibblock">
Shi, Q.,
Cheng, L.,
Wang, L.,
Smola, A.:
Human action segmentation and recognition using discriminative semi-markov models.
International journal of computer vision
<span id="bib.bib92.1.1" class="ltx_text ltx_font_bold">93</span>,
22–32
(2011)


</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2015]</span>
<span class="ltx_bibblock">
Chen, C.,
Jafari, R.,
Kehtarnavaz, N.:
Action recognition from depth sequences using depth motion maps-based local binary patterns.
In: 2015 IEEE Winter Conference on Applications of Computer Vision,
pp. 1092–1099
(2015).
IEEE


</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bobick and Davis [2001]</span>
<span class="ltx_bibblock">
Bobick, A.F.,
Davis, J.W.:
The recognition of human movement using temporal templates.
IEEE Transactions on pattern analysis and machine intelligence
<span id="bib.bib94.1.1" class="ltx_text ltx_font_bold">23</span>(3),
257–267
(2001)


</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2008]</span>
<span class="ltx_bibblock">
Zhang, Z.,
Hu, Y.,
Chan, S.,
Chia, L.-T.:
Motion context: A new representation for human action recognition.
In: Computer Vision–ECCV 2008: 10th European Conference on Computer Vision, Marseille, France, October 12-18, 2008, Proceedings, Part IV 10,
pp. 817–829
(2008).
Springer


</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Somasundaram et al. [2014]</span>
<span class="ltx_bibblock">
Somasundaram, G.,
Cherian, A.,
Morellas, V.,
Papanikolopoulos, N.:
Action recognition using global spatio-temporal features derived from sparse representations.
Computer Vision and Image Understanding
<span id="bib.bib96.1.1" class="ltx_text ltx_font_bold">123</span>,
1–13
(2014)


</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Schmid [2013]</span>
<span class="ltx_bibblock">
Wang, H.,
Schmid, C.:
Action recognition with improved trajectories.
In: Proceedings of the IEEE International Conference on Computer Vision,
pp. 3551–3558
(2013)


</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oreifej and Liu [2013]</span>
<span class="ltx_bibblock">
Oreifej, O.,
Liu, Z.:
Hon4d: Histogram of oriented 4d normals for activity recognition from depth sequences.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 716–723
(2013)


</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel et al. [2020]</span>
<span class="ltx_bibblock">
Patel, C.I.,
Labana, D.,
Pandya, S.,
Modi, K.,
Ghayvat, H.,
Awais, M.:
Histogram of oriented gradient-based fusion of features for human action recognition in action video sequences.
Sensors
<span id="bib.bib99.1.1" class="ltx_text ltx_font_bold">20</span>(24),
7299
(2020)


</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al. [2020]</span>
<span class="ltx_bibblock">
Tan, P.S.,
Lim, K.M.,
Lee, C.P.:
Human action recognition with sparse autoencoder and histogram of oriented gradients.
In: 2020 IEEE 2nd International Conference on Artificial Intelligence in Engineering and Technology (IICAIET),
pp. 1–5
(2020).
IEEE


</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wattanapanich et al. [2021]</span>
<span class="ltx_bibblock">
Wattanapanich, C.,
Wei, H.,
Xu, W.:
Analysis of histogram of oriented gradients on gait recognition.
In: Pattern Recognition and Artificial Intelligence: 4th Mediterranean Conference, MedPRAI 2020, Hammamet, Tunisia, December 20–22, 2020, Proceedings 4,
pp. 86–97
(2021).
Springer


</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zuo et al. [2019]</span>
<span class="ltx_bibblock">
Zuo, Z.,
Yang, L.,
Liu, Y.,
Chao, F.,
Song, R.,
Qu, Y.:
Histogram of fuzzy local spatio-temporal descriptors for video action recognition.
IEEE Transactions on Industrial Informatics
<span id="bib.bib102.1.1" class="ltx_text ltx_font_bold">16</span>(6),
4059–4067
(2019)


</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sahoo et al. [2019]</span>
<span class="ltx_bibblock">
Sahoo, S.P.,
Silambarasi, R.,
Ari, S.:
Fusion of histogram based features for human action recognition.
In: 2019 5th International Conference on Advanced Computing &amp; Communication Systems (ICACCS),
pp. 1012–1016
(2019).
IEEE


</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. [2013]</span>
<span class="ltx_bibblock">
Ma, S.,
Zhang, J.,
Ikizler-Cinbis, N.,
Sclaroff, S.:
Action recognition and localization by hierarchical space-time segments.
In: Proceedings of the IEEE International Conference on Computer Vision,
pp. 2744–2751
(2013)


</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia and Ma [2021]</span>
<span class="ltx_bibblock">
Xia, L.,
Ma, W.:
Human action recognition using high-order feature of optical flows.
The Journal of Supercomputing
<span id="bib.bib105.1.1" class="ltx_text ltx_font_bold">77</span>(12),
14230–14251
(2021)


</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nebisoy and Malekzadeh [2021]</span>
<span class="ltx_bibblock">
Nebisoy, A.,
Malekzadeh, S.:
Video action recognition using spatio-temporal optical flow video frames.
arXiv preprint arXiv:2103.05101
(2021)


</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang [2020]</span>
<span class="ltx_bibblock">
Wang, H.:
Enhanced forest microexpression recognition based on optical flow direction histogram and deep multiview network.
Mathematical Problems in Engineering
<span id="bib.bib107.1.1" class="ltx_text ltx_font_bold">2020</span>,
1–11
(2020)


</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassan et al. [2018]</span>
<span class="ltx_bibblock">
Hassan, N.,
Bhatti, N., <span id="bib.bib108.1.1" class="ltx_text ltx_font_italic">et al.</span>:
Temporal superpixels based human action localization.
In: 2018 14th International Conference on Emerging Technologies (ICET),
pp. 1–6
(2018).
IEEE


</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marszalek et al. [2009]</span>
<span class="ltx_bibblock">
Marszalek, M.,
Laptev, I.,
Schmid, C.:
Actions in context.
In: 2009 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 2929–2936
(2009).
IEEE


</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laptev et al. [2008]</span>
<span class="ltx_bibblock">
Laptev, I.,
Marszalek, M.,
Schmid, C.,
Rozenfeld, B.:
Learning realistic human actions from movies.
In: 2008 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1–8
(2008).
IEEE


</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuldt et al. [2004]</span>
<span class="ltx_bibblock">
Schuldt, C.,
Laptev, I.,
Caputo, B.:
Recognizing human actions: a local svm approach.
In: Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.,
vol. 3,
pp. 32–36
(2004).
IEEE


</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blank et al. [2005]</span>
<span class="ltx_bibblock">
Blank, M.,
Gorelick, L.,
Shechtman, E.,
Irani, M.,
Basri, R.:
Actions as space-time shapes.
In: Tenth IEEE International Conference on Computer Vision (ICCV’05) Volume 1,
vol. 2,
pp. 1395–1402
(2005).
IEEE


</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laptev and Pérez [2007]</span>
<span class="ltx_bibblock">
Laptev, I.,
Pérez, P.:
Retrieving actions in movies.
In: 2007 IEEE 11th International Conference on Computer Vision,
pp. 1–8
(2007).
IEEE


</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran and Sorokin [2008]</span>
<span class="ltx_bibblock">
Tran, D.,
Sorokin, A.:
Human activity recognition with metric learning.
In: Computer Vision–ECCV 2008: 10th European Conference on Computer Vision, Marseille, France, October 12-18, 2008, Proceedings, Part I 10,
pp. 548–561
(2008).
Springer


</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morency et al. [2007]</span>
<span class="ltx_bibblock">
Morency, L.-P.,
Quattoni, A.,
Darrell, T.:
Latent-dynamic discriminative models for continuous gesture recognition.
In: 2007 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1–8
(2007).
IEEE


</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2006]</span>
<span class="ltx_bibblock">
Wang, S.B.,
Quattoni, A.,
Morency, L.-P.,
Demirdjian, D.,
Darrell, T.:
Hidden conditional random fields for gesture recognition.
In: 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06),
vol. 2,
pp. 1521–1527
(2006).
IEEE


</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Suter [2007]</span>
<span class="ltx_bibblock">
Wang, L.,
Suter, D.:
Recognizing human activities from silhouettes: Motion subspace and factorial discriminative graphical model.
In: 2007 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1–8
(2007).
IEEE


</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2012]</span>
<span class="ltx_bibblock">
Tang, K.,
Fei-Fei, L.,
Koller, D.:
Learning latent temporal structure for complex event detection.
In: 2012 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1250–1257
(2012).
IEEE


</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2012]</span>
<span class="ltx_bibblock">
Wang, Z.,
Wang, J.,
Xiao, J.,
Lin, K.-H.,
Huang, T.:
Substructure and boundary modeling for continuous action recognition.
In: 2012 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1330–1337
(2012).
IEEE


</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. [2014]</span>
<span class="ltx_bibblock">
Luo, G.,
Yang, S.,
Tian, G.,
Yuan, C.,
Hu, W.,
Maybank, S.J.:
Learning depth from monocular videos using deep neural networks.
Journal of Computer Vision
<span id="bib.bib120.1.1" class="ltx_text ltx_font_bold">10</span>(1),
1–10
(2014)


</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. [2013]</span>
<span class="ltx_bibblock">
Yuan, C.,
Hu, W.,
Tian, G.,
Yang, S.,
Wang, H.:
Multi-task sparse learning with beta process prior for action recognition.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 423–429
(2013)


</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chakraborty et al. [2011]</span>
<span class="ltx_bibblock">
Chakraborty, B.,
Holte, M.B.,
Moeslund, T.B.,
Gonzalez, J.,
Roca, F.X.:
A selective spatio-temporal interest point detector for human action recognition in complex scenes.
In: 2011 International Conference on Computer Vision,
pp. 1776–1783
(2011).
IEEE


</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan and Chen [2013]</span>
<span class="ltx_bibblock">
Gan, L.,
Chen, F.:
Human action recognition using apj3d and random forests.
J. Softw.
<span id="bib.bib123.1.1" class="ltx_text ltx_font_bold">8</span>(9),
2238–2245
(2013)


</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Everts et al. [2014]</span>
<span class="ltx_bibblock">
Everts, I.,
Van Gemert, J.C.,
Gevers, T.:
Evaluation of color spatio-temporal interest points for human action recognition.
IEEE Transactions on Image Processing
<span id="bib.bib124.1.1" class="ltx_text ltx_font_bold">23</span>(4),
1569–1580
(2014)


</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2014]</span>
<span class="ltx_bibblock">
Zhu, Y.,
Chen, W.,
Guo, G.:
Evaluating spatiotemporal interest point features for depth-based action recognition.
Image and Vision computing
<span id="bib.bib125.1.1" class="ltx_text ltx_font_bold">32</span>(8),
453–464
(2014)


</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2015]</span>
<span class="ltx_bibblock">
Liu, L.,
Shao, L.,
Li, X.,
Lu, K.:
Learning spatio-temporal representations for action recognition: A genetic programming approach.
IEEE transactions on cybernetics
<span id="bib.bib126.1.1" class="ltx_text ltx_font_bold">46</span>(1),
158–170
(2015)


</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2016]</span>
<span class="ltx_bibblock">
Xu, D.,
Xiao, X.,
Wang, X.,
Wang, J.:
Human action recognition based on kinect and pso-svm by representing 3d skeletons as points in lie group.
In: 2016 International Conference on Audio, Language and Image Processing (ICALIP),
pp. 568–573
(2016).
IEEE


</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vishwakarma et al. [2016]</span>
<span class="ltx_bibblock">
Vishwakarma, D.K.,
Kapoor, R.,
Dhiman, A.:
A proposed unified framework for the recognition of human activity by exploiting the characteristics of action dynamics.
Robotics and Autonomous Systems
<span id="bib.bib128.1.1" class="ltx_text ltx_font_bold">77</span>,
25–38
(2016)


</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh and Mohan [2017]</span>
<span class="ltx_bibblock">
Singh, D.,
Mohan, C.K.:
Graph formulation of video activities for abnormal activity recognition.
Pattern Recognition
<span id="bib.bib129.1.1" class="ltx_text ltx_font_bold">65</span>,
265–272
(2017)


</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jalal et al. [2017]</span>
<span class="ltx_bibblock">
Jalal, A.,
Kim, Y.-H.,
Kim, Y.-J.,
Kamal, S.,
Kim, D.:
Robust human activity recognition from depth video using spatiotemporal multi-fused features.
Pattern recognition
<span id="bib.bib130.1.1" class="ltx_text ltx_font_bold">61</span>,
295–308
(2017)


</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nazir et al. [2018]</span>
<span class="ltx_bibblock">
Nazir, S.,
Yousaf, M.H.,
Velastin, S.A.:
Evaluating a bag-of-visual features approach using spatio-temporal features for action recognition.
Computers &amp; Electrical Engineering
<span id="bib.bib131.1.1" class="ltx_text ltx_font_bold">72</span>,
660–669
(2018)


</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ullah et al. [2021]</span>
<span class="ltx_bibblock">
Ullah, S.,
Bhatti, N.,
Qasim, T.,
Hassan, N.,
Zia, M.:
Weakly-supervised action localization based on seed superpixels.
Multimedia Tools and Applications
<span id="bib.bib132.1.1" class="ltx_text ltx_font_bold">80</span>,
6203–6220
(2021)


</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Al-Obaidi et al. [2021]</span>
<span class="ltx_bibblock">
Al-Obaidi, S.,
Al-Khafaji, H.,
Abhayaratne, C.:
Making sense of neuromorphic event data for human action recognition.
IEEE Access
<span id="bib.bib133.1.1" class="ltx_text ltx_font_bold">9</span>,
82686–82700
(2021)


</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hejazi and Abhayaratne [2022]</span>
<span class="ltx_bibblock">
Hejazi, S.M.,
Abhayaratne, C.:
Handcrafted localized phase features for human action recognition.
Image and Vision Computing
<span id="bib.bib134.1.1" class="ltx_text ltx_font_bold">123</span>,
104465
(2022)


</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2022]</span>
<span class="ltx_bibblock">
Zhang, C.,
Xu, Y.,
Xu, Z.,
Huang, J.,
Lu, J.:
Hybrid handcrafted and learned feature framework for human action recognition.
Applied Intelligence
<span id="bib.bib135.1.1" class="ltx_text ltx_font_bold">52</span>(11),
12771–12787
(2022)


</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fatima et al. [2023]</span>
<span class="ltx_bibblock">
Fatima, T.,
Rahman, H.,
Jalal, A.:
A novel framework for human action recognition based on features fusion and decision tree.
IEEE ICACS
<span id="bib.bib136.1.1" class="ltx_text ltx_font_bold">53</span>
(2023)


</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kar et al. [2017]</span>
<span class="ltx_bibblock">
Kar, A.,
Rai, N.,
Sikka, K.,
Sharma, G.:
Adascan: Adaptive scan pooling in deep convolutional neural networks for human action recognition in videos.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 3376–3385
(2017)


</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Varol et al. [2017]</span>
<span class="ltx_bibblock">
Varol, G.,
Laptev, I.,
Schmid, C.:
Long-term temporal convolutions for action recognition.
IEEE transactions on pattern analysis and machine intelligence
<span id="bib.bib138.1.1" class="ltx_text ltx_font_bold">40</span>(6),
1510–1517
(2017)


</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feichtenhofer et al. [2017]</span>
<span class="ltx_bibblock">
Feichtenhofer, C.,
Pinz, A.,
Wildes, R.P.:
Spatiotemporal multiplier networks for video action recognition.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 4768–4777
(2017)


</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2016]</span>
<span class="ltx_bibblock">
Liu, J.,
Shahroudy, A.,
Xu, D.,
Wang, G.:
Spatio-temporal lstm with trust gates for 3d human action recognition.
In: Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14,
pp. 816–833
(2016).
Springer


</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2015]</span>
<span class="ltx_bibblock">
Wang, L.,
Qiao, Y.,
Tang, X.:
Action recognition with trajectory-pooled deep-convolutional descriptors.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 4305–4314
(2015)


</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. [2018]</span>
<span class="ltx_bibblock">
Yan, S.,
Xiong, Y.,
Lin, D.:
Spatial temporal graph convolutional networks for skeleton-based action recognition.
In: Proceedings of the AAAI Conference on Artificial Intelligence,
vol. 32
(2018)


</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2018]</span>
<span class="ltx_bibblock">
Zhang, B.,
Wang, L.,
Wang, Z.,
Qiao, Y.,
Wang, H.:
Real-time action recognition with deeply transferred motion vector cnns.
IEEE Transactions on Image Processing
<span id="bib.bib143.1.1" class="ltx_text ltx_font_bold">27</span>(5),
2326–2339
(2018)


</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miah et al. [2022]</span>
<span class="ltx_bibblock">
Miah, A.S.M.,
Shin, J.,
Hasan, M.A.M.,
Rahim, M.A.:
Bensignnet: Bengali sign language alphabet recognition using concatenated segmentation and convolutional neural network.
Applied Sciences
<span id="bib.bib144.1.1" class="ltx_text ltx_font_bold">12</span>(8),
3933
(2022)


</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miah et al. [2024]</span>
<span class="ltx_bibblock">
Miah, A.S.M.,
Hasan, M.A.M.,
Tomioka, Y.,
Shin, J.:
Hand gesture recognition for multi-culture sign language using graph and general deep learning network.
IEEE Open Journal of the Computer Society
(2024)


</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Horn and Schunck [1981]</span>
<span class="ltx_bibblock">
Horn, B.K.,
Schunck, B.G.:
Determining optical flow.
Artificial intelligence
<span id="bib.bib146.1.1" class="ltx_text ltx_font_bold">17</span>(1-3),
185–203
(1981)


</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feichtenhofer et al. [2016]</span>
<span class="ltx_bibblock">
Feichtenhofer, C.,
Pinz, A.,
Zisserman, A.:
Convolutional two-stream network fusion for video action recognition.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1933–1941
(2016)


</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2016]</span>
<span class="ltx_bibblock">
Wang, L.,
Xiong, Y.,
Wang, Z.,
Qiao, Y.,
Lin, D.,
Tang, X.,
Van Gool, L.:
Temporal segment networks: Towards good practices for deep action recognition.
In: European Conference on Computer Vision,
pp. 20–36
(2016).
Springer


</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lan et al. [2017]</span>
<span class="ltx_bibblock">
Lan, Z.,
Zhu, Y.,
Hauptmann, A.G.,
Newsam, S.:
Deep local video feature for action recognition.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,
pp. 1–7
(2017)


</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2018]</span>
<span class="ltx_bibblock">
Zhou, B.,
Andonian, A.,
Oliva, A.,
Torralba, A.:
Temporal relational reasoning in videos.
In: Proceedings of the European Conference on Computer Vision (ECCV),
pp. 803–818
(2018)


</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pham et al. [2020]</span>
<span class="ltx_bibblock">
Pham, H.H.,
Salmane, H.,
Khoudour, L.,
Crouzil, A.,
Velastin, S.A.,
Zegers, P.:
A unified deep framework for joint 3d pose estimation and action recognition from a single rgb camera.
Sensors
<span id="bib.bib151.1.1" class="ltx_text ltx_font_bold">20</span>(7),
1825
(2020)


</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2017]</span>
<span class="ltx_bibblock">
Liu, M.,
Liu, H.,
Chen, C.:
Enhanced skeleton visualization for view invariant human action recognition.
Pattern Recognition
<span id="bib.bib152.1.1" class="ltx_text ltx_font_bold">68</span>,
346–362
(2017)


</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. [2017]</span>
<span class="ltx_bibblock">
Shi, Y.,
Tian, Y.,
Wang, Y.,
Huang, T.:
Sequential deep trajectory descriptor for action recognition with three-stream cnn.
IEEE Transactions on Multimedia
<span id="bib.bib153.1.1" class="ltx_text ltx_font_bold">19</span>(7),
1510–1520
(2017)


</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. [2012]</span>
<span class="ltx_bibblock">
Ji, S.,
Xu, W.,
Yang, M.,
Yu, K.:
3d convolutional neural networks for human action recognition.
IEEE transactions on pattern analysis and machine intelligence
<span id="bib.bib154.1.1" class="ltx_text ltx_font_bold">35</span>(1),
221–231
(2012)


</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al. [2015]</span>
<span class="ltx_bibblock">
Sharma, S.,
Kiros, R.,
Salakhutdinov, R.:
Action recognition using visual attention.
arXiv preprint arXiv:1511.04119
(2015)


</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ijjina and Chalavadi [2016]</span>
<span class="ltx_bibblock">
Ijjina, E.P.,
Chalavadi, K.M.:
Human action recognition using genetic algorithms and convolutional neural networks.
Pattern recognition
<span id="bib.bib156.1.1" class="ltx_text ltx_font_bold">59</span>,
199–212
(2016)


</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akilan et al. [2017]</span>
<span class="ltx_bibblock">
Akilan, T.,
Wu, Q.J.,
Safaei, A.,
Jiang, W.:
A late fusion approach for harnessing multi-cnn model high-level features.
In: 2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC),
pp. 566–571
(2017).
IEEE


</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahsan et al. [2018]</span>
<span class="ltx_bibblock">
Ahsan, U.,
Sun, C.,
Essa, I.:
Discrimnet: Semi-supervised action recognition from videos using generative adversarial networks.
arXiv preprint arXiv:1801.07230
(2018)


</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tu et al. [2018]</span>
<span class="ltx_bibblock">
Tu, Z.,
Xie, W.,
Qin, Q.,
Poppe, R.,
Veltkamp, R.C.,
Li, B.,
Yuan, J.:
Multi-stream cnn: Learning representations based on human-related regions for action recognition.
Pattern Recognition
<span id="bib.bib159.1.1" class="ltx_text ltx_font_bold">79</span>,
32–43
(2018)


</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2018]</span>
<span class="ltx_bibblock">
Zhou, Y.,
Sun, X.,
Zha, Z.-J.,
Zeng, W.:
Mict: Mixed 3d/2d convolutional tube for human action recognition.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 449–458
(2018)


</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jian et al. [2019]</span>
<span class="ltx_bibblock">
Jian, M.,
Zhang, S.,
Wu, L.,
Zhang, S.,
Wang, X.,
He, Y.:
Deep key frame extraction for sport training.
Neurocomputing
<span id="bib.bib161.1.1" class="ltx_text ltx_font_bold">328</span>,
147–156
(2019)


</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
Gowda, S.,
Rohrbach, M.,
Sevilla-Lara, L.:
Smart frame selection for action recognition. arxiv 2020.
arXiv preprint arXiv:2012.10671


</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan et al. [2020]</span>
<span class="ltx_bibblock">
Khan, M.A.,
Javed, K.,
Khan, S.A.,
Saba, T.,
Habib, U.,
Khan, J.A.,
Abbasi, A.A.:
Human action recognition using fusion of multiview and deep features: an application to video surveillance.
Multimedia Tools and Applications
<span id="bib.bib163.1.1" class="ltx_text ltx_font_bold">79</span>(37-38),
27973–27995
(2020)


</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ullah et al. [2021]</span>
<span class="ltx_bibblock">
Ullah, A.,
Muhammad, K.,
Ding, W.,
Palade, V.,
Haq, I.U.,
Baik, S.W.:
Efficient activity recognition using lightweight cnn and ds-gru network for surveillance applications.
Applied Soft Computing
<span id="bib.bib164.1.1" class="ltx_text ltx_font_bold">103</span>,
107102
(2021)


</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2021]</span>
<span class="ltx_bibblock">
Wang, L.,
Tong, Z.,
Ji, B.,
Wu, G.:
Tdn: Temporal difference networks for efficient action recognition.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 1895–1904
(2021)


</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2022]</span>
<span class="ltx_bibblock">
Wang, X.,
Zhang, S.,
Qing, Z.,
Tang, M.,
Zuo, Z.,
Gao, C.,
Jin, R.,
Sang, N.:
Hybrid relation guided set matching for few-shot action recognition.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 19948–19957
(2022)


</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wensel et al. [2023]</span>
<span class="ltx_bibblock">
Wensel, J.,
Ullah, H.,
Munir, A.:
Vit-ret: Vision and recurrent transformer neural networks for human activity recognition in videos.
IEEE Access
(2023)


</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassan et al. [2024]</span>
<span class="ltx_bibblock">
Hassan, N.,
Miah, A.S.M.,
Shin, J.:
A deep bidirectional lstm model enhanced by transfer-learning-based feature extraction for dynamic human activity recognition.
Applied Sciences
<span id="bib.bib168.1.1" class="ltx_text ltx_font_bold">14</span>(2),
603
(2024)


</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carreira and Zisserman [2017]</span>
<span class="ltx_bibblock">
Carreira, J.,
Zisserman, A.:
Quo vadis, action recognition? a new model and the kinetics dataset. corr abs/1705.07750 (2017).
arXiv preprint arXiv:1705.07750
(2017)


</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu et al. [2017]</span>
<span class="ltx_bibblock">
Qiu, Z.,
Yao, T.,
Mei, T.:
Learning spatio-temporal representation with pseudo-3d residual networks. corr abs/1711.10305 (2017).
arXiv preprint arXiv:1711.10305
(2017)


</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran et al. [2018]</span>
<span class="ltx_bibblock">
Tran, D.,
Wang, H.,
Torresani, L.,
Ray, J.,
LeCun, Y.,
Paluri, M.:
A closer look at spatiotemporal convolutions for action recognition.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 6450–6459
(2018)


</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2018]</span>
<span class="ltx_bibblock">
Zhao, Y.,
Xiong, Y.,
Lin, D.:
Trajectory convolution for action recognition.
Advances in neural information processing systems
<span id="bib.bib172.1.1" class="ltx_text ltx_font_bold">31</span>
(2018)


</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2018]</span>
<span class="ltx_bibblock">
Wang, L.,
Li, W.,
Li, W.,
Van Gool, L.:
Appearance-and-relation networks for video classification.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1430–1439
(2018)


</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. [2018]</span>
<span class="ltx_bibblock">
Xie, S.,
Sun, C.,
Huang, J.,
Tu, Z.,
Murphy, K.:
Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification.
In: Proceedings of the European Conference on Computer Vision (ECCV),
pp. 305–321
(2018)


</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran et al. [2019]</span>
<span class="ltx_bibblock">
Tran, D.,
Wang, H.,
Torresani, L.,
Feiszli, M.:
Video classification with channel-separated convolutional networks.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision,
pp. 5552–5561
(2019)


</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feichtenhofer [2020]</span>
<span class="ltx_bibblock">
Feichtenhofer, C.:
X3d: Expanding architectures for efficient video recognition.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 203–213
(2020)


</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2020]</span>
<span class="ltx_bibblock">
Yang, C.,
Xu, Y.,
Shi, J.,
Dai, B.,
Zhou, B.:
Temporal pyramid network for action recognition.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 591–600
(2020)


</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2020]</span>
<span class="ltx_bibblock">
Zhang, S.,
Guo, S.,
Huang, W.,
Scott, M.R.,
Wang, L.:
V4d: 4d convolutional neural networks for video-level representation learning.
arXiv preprint arXiv:2002.07442
(2020)


</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. [2017]</span>
<span class="ltx_bibblock">
Qin, Y.,
Mo, L.,
Xie, B.:
Feature fusion for human action recognition based on classical descriptors and 3d convolutional networks.
In: 2017 Eleventh International Conference on Sensing Technology (ICST),
pp. 1–5
(2017).
IEEE


</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diba et al. [2017]</span>
<span class="ltx_bibblock">
Diba, A.,
Fayyaz, M.,
Sharma, V.,
Karami, A.H.,
Arzani, M.M.,
Yousefzadeh, R.,
Van Gool, L.:
Temporal 3d convnets: New architecture and transfer learning for video classification.
arXiv preprint arXiv:1711.08200
(2017)


</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2018]</span>
<span class="ltx_bibblock">
Zhu, J.,
Zhu, Z.,
Zou, W.:
End-to-end video-level representation learning for action recognition.
In: 2018 24th International Conference on Pattern Recognition (ICPR),
pp. 645–650
(2018).
IEEE


</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. [2017]</span>
<span class="ltx_bibblock">
Vaswani, A.,
Shazeer, N.,
Parmar, N.,
Uszkoreit, J.,
Jones, L.,
Gomez, A.N.,
Kaiser, Ł.,
Polosukhin, I.:
Attention is all you need.
Advances in neural information processing systems
<span id="bib.bib182.1.1" class="ltx_text ltx_font_bold">30</span>
(2017)


</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Girdhar et al. [2019]</span>
<span class="ltx_bibblock">
Girdhar, R.,
Carreira, J.,
Doersch, C.,
Zisserman, A.:
Video action transformer network.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 244–253
(2019)


</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Donahue et al. [2015]</span>
<span class="ltx_bibblock">
Donahue, J.,
Anne Hendricks, L.,
Guadarrama, S.,
Rohrbach, M.,
Venugopalan, S.,
Saenko, K.,
Darrell, T.:
Long-term recurrent convolutional networks for visual recognition and description.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 2625–2634
(2015)


</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ng et al. [2015]</span>
<span class="ltx_bibblock">
Ng, J.Y.-H.,
Hausknecht, M.J.,
Vijayanarasimhan, S.,
Vinyals, O.,
Monga, R.,
Toderici, G.:
Beyond short snippets: Deep networks for video classification. corr abs/1503.08909 (2015).
arXiv preprint arXiv:1503.08909
(2015)


</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. [2018]</span>
<span class="ltx_bibblock">
Song, S.,
Lan, C.,
Xing, J.,
Zeng, W.,
Liu, J.:
Spatio-temporal attention-based lstm networks for 3d action recognition and detection.
IEEE Transactions on image processing
<span id="bib.bib186.1.1" class="ltx_text ltx_font_bold">27</span>(7),
3459–3471
(2018)


</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. [2013]</span>
<span class="ltx_bibblock">
Han, J.,
Shao, L.,
Xu, D.,
Shotton, J.:
Enhanced computer vision with microsoft kinect sensor: A review.
IEEE transactions on cybernetics
<span id="bib.bib187.1.1" class="ltx_text ltx_font_bold">43</span>(5),
1318–1334
(2013)


</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. [2017]</span>
<span class="ltx_bibblock">
Fang, H.-S.,
Xie, S.,
Tai, Y.-W.,
Lu, C.:
Rmpe: Regional multi-person pose estimation.
In: Proceedings of the IEEE International Conference on Computer Vision,
pp. 2334–2343
(2017)


</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiu et al. [2018]</span>
<span class="ltx_bibblock">
Xiu, Y.,
Li, J.,
Wang, H.,
Fang, Y.,
Lu, C.:
Pose flow: Efficient online pose tracking.
arXiv preprint arXiv:1802.00977
(2018)


</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang and Ramanan [2012]</span>
<span class="ltx_bibblock">
Yang, Y.,
Ramanan, D.:
Articulated human detection with flexible mixtures of parts.
IEEE transactions on pattern analysis and machine intelligence
<span id="bib.bib190.1.1" class="ltx_text ltx_font_bold">35</span>(12),
2878–2890
(2012)


</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Yuille [2014]</span>
<span class="ltx_bibblock">
Chen, X.,
Yuille, A.L.:
Articulated pose estimation by a graphical model with image dependent pairwise relations.
Advances in neural information processing systems
<span id="bib.bib191.1.1" class="ltx_text ltx_font_bold">27</span>
(2014)


</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bulat and Tzimiropoulos [2016]</span>
<span class="ltx_bibblock">
Bulat, A.,
Tzimiropoulos, G.:
Human pose estimation via convolutional part heatmap regression.
In: Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part VII 14,
pp. 717–732
(2016).
Springer


</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toshev and Szegedy [2014]</span>
<span class="ltx_bibblock">
Toshev, A.,
Szegedy, C.:
Deeppose: Human pose estimation via deep neural networks.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1653–1660
(2014)


</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carreira et al. [2016]</span>
<span class="ltx_bibblock">
Carreira, J.,
Agrawal, P.,
Fragkiadaki, K.,
Malik, J.:
Human pose estimation with iterative error feedback.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 4733–4742
(2016)


</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2018]</span>
<span class="ltx_bibblock">
Zhou, X.,
Zhu, M.,
Pavlakos, G.,
Leonardos, S.,
Derpanis, K.G.,
Daniilidis, K.:
Monocap: Monocular human motion capture using a cnn coupled with a geometric prior.
IEEE transactions on pattern analysis and machine intelligence
<span id="bib.bib195.1.1" class="ltx_text ltx_font_bold">41</span>(4),
901–914
(2018)


</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nunes et al. [2017]</span>
<span class="ltx_bibblock">
Nunes, U.M.,
Faria, D.R.,
Peixoto, P.:
A human activity recognition framework using max-min features and key poses with differential evolution random forests classifier.
Pattern Recognition Letters
<span id="bib.bib196.1.1" class="ltx_text ltx_font_bold">99</span>,
21–31
(2017)


</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen [2015]</span>
<span class="ltx_bibblock">
Chen, Y.:
Reduced basis decomposition: a certified and fast lossy data compression algorithm.
Computers &amp; Mathematics with Applications
<span id="bib.bib197.1.1" class="ltx_text ltx_font_bold">70</span>(10),
2566–2574
(2015)


</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Veeriah et al. [2015]</span>
<span class="ltx_bibblock">
Veeriah, V.,
Zhuang, N.,
Qi, G.-J.:
Differential recurrent neural networks for action recognition.
In: Proceedings of the IEEE International Conference on Computer Vision,
pp. 4041–4049
(2015)


</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2016]</span>
<span class="ltx_bibblock">
Zhu, W.,
Lan, C.,
Xing, J.,
Zeng, W.,
Li, Y.,
Shen, L.,
Xie, X.:
Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks.
In: Proceedings of the AAAI Conference on Artificial Intelligence,
vol. 30
(2016)


</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2017]</span>
<span class="ltx_bibblock">
Li, C.,
Hou, Y.,
Wang, P.,
Li, W.:
Joint distance maps based action recognition with convolutional neural networks.
IEEE Signal Processing Letters
<span id="bib.bib200.1.1" class="ltx_text ltx_font_bold">24</span>(5),
624–628
(2017)


</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soo Kim and Reiter [2017]</span>
<span class="ltx_bibblock">
Soo Kim, T.,
Reiter, A.:
Interpretable 3d human action analysis with temporal convolutional networks.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,
pp. 20–28
(2017)


</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das et al. [2018]</span>
<span class="ltx_bibblock">
Das, S.,
Koperski, M.,
Bremond, F.,
Francesca, G.:
Deep-temporal lstm for daily living action recognition.
In: 2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),
pp. 1–6
(2018).
IEEE


</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Si et al. [2019]</span>
<span class="ltx_bibblock">
Si, C.,
Chen, W.,
Wang, W.,
Wang, L.,
Tan, T.:
An attention enhanced graph convolutional lstm network for skeleton-based action recognition.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 1227–1236
(2019)


</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. [2019]</span>
<span class="ltx_bibblock">
Shi, L.,
Zhang, Y.,
Cheng, J.,
Lu, H.:
Two-stream adaptive graph convolutional networks for skeleton-based action recognition.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 12026–12035
(2019)


</span>
</li>
<li id="bib.bib205" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trelinski and Kwolek [2019]</span>
<span class="ltx_bibblock">
Trelinski, J.,
Kwolek, B.:
Ensemble of classifiers using cnn and hand-crafted features for depth-based action recognition.
In: Artificial Intelligence and Soft Computing: 18th International Conference, ICAISC 2019, Zakopane, Poland, June 16–20, 2019, Proceedings, Part II 18,
pp. 91–103
(2019).
Springer


</span>
</li>
<li id="bib.bib206" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019]</span>
<span class="ltx_bibblock">
Li, M.,
Chen, S.,
Chen, X.,
Zhang, Y.,
Wang, Y.,
Tian, Q.:
Actional-structural graph convolutional networks for skeleton-based action recognition.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 3595–3603
(2019)


</span>
</li>
<li id="bib.bib207" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huynh-The et al. [2019]</span>
<span class="ltx_bibblock">
Huynh-The, T.,
Hua, C.-H.,
Kim, D.-S.:
Encoding pose features to images with data augmentation for 3-d action recognition.
IEEE Transactions on Industrial Informatics
<span id="bib.bib207.1.1" class="ltx_text ltx_font_bold">16</span>(5),
3100–3111
(2019)


</span>
</li>
<li id="bib.bib208" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huynh-The et al. [2020]</span>
<span class="ltx_bibblock">
Huynh-The, T.,
Hua, C.-H.,
Ngo, T.-T.,
Kim, D.-S.:
Image representation of pose-transition feature for 3d skeleton-based action recognition.
Information Sciences
<span id="bib.bib208.1.1" class="ltx_text ltx_font_bold">513</span>,
112–126
(2020)


</span>
</li>
<li id="bib.bib209" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Naveenkumar and Domnic [2020]</span>
<span class="ltx_bibblock">
Naveenkumar, M.,
Domnic, S.:
Deep ensemble network using distance maps and body part features for skeleton based action recognition.
Pattern Recognition
<span id="bib.bib209.1.1" class="ltx_text ltx_font_bold">100</span>,
107125
(2020)


</span>
</li>
<li id="bib.bib210" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plizzari et al. [2021]</span>
<span class="ltx_bibblock">
Plizzari, C.,
Cannici, M.,
Matteucci, M.:
Skeleton-based action recognition via spatial and temporal transformer networks.
Computer Vision and Image Understanding
<span id="bib.bib210.1.1" class="ltx_text ltx_font_bold">208</span>,
103219
(2021)


</span>
</li>
<li id="bib.bib211" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snoun et al. [2021]</span>
<span class="ltx_bibblock">
Snoun, A.,
Jlidi, N.,
Bouchrika, T.,
Jemai, O.,
Zaied, M.:
Towards a deep human activity recognition approach based on video to image transformation with skeleton data.
Multimedia Tools and Applications
(2021)
<a target="_blank" href="https://doi.org/10.1007/s11042-021-11188-1" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s11042-021-11188-1</a>


</span>
</li>
<li id="bib.bib212" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duan et al. [2022]</span>
<span class="ltx_bibblock">
Duan, H.,
Wang, J.,
Chen, K.,
Lin, D.:
Pyskl: Towards good practices for skeleton action recognition.
In: Proceedings of the 30th ACM International Conference on Multimedia,
pp. 7351–7354
(2022)


</span>
</li>
<li id="bib.bib213" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. [2022]</span>
<span class="ltx_bibblock">
Song, Y.-F.,
Zhang, Z.,
Shan, C.,
Wang, L.:
Constructing stronger and faster baselines for skeleton-based action recognition.
IEEE transactions on pattern analysis and machine intelligence
<span id="bib.bib213.1.1" class="ltx_text ltx_font_bold">45</span>(2),
1474–1488
(2022)


</span>
</li>
<li id="bib.bib214" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2023]</span>
<span class="ltx_bibblock">
Zhu, G.,
Wan, C.,
Cao, L.,
Wang, X.:
Relation-mining self-attention network for skeleton-based human action recognition.
Pattern Recognition
<span id="bib.bib214.1.1" class="ltx_text ltx_font_bold">135</span>,
109098
(2023)
<a target="_blank" href="https://doi.org/%****%20HAR_Survey.bbl%20Line%203175%20****10.1016/j.patcog.2023.109098" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/%****␣HAR_Survey.bbl␣Line␣3175␣****10.1016/j.patcog.2023.109098</a>


</span>
</li>
<li id="bib.bib215" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023]</span>
<span class="ltx_bibblock">
Zhang, G.,
Wen, S.,
Li, J.,
Che, H.:
Fast 3d-graph convolutional networks for skeleton-based action recognition.
Applied Soft Computing
<span id="bib.bib215.1.1" class="ltx_text ltx_font_bold">145</span>,
110575
(2023)


</span>
</li>
<li id="bib.bib216" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023]</span>
<span class="ltx_bibblock">
Liu, Y.,
Zhang, H.,
Li, Y.,
He, K.,
Xu, D.:
Skeleton-based human action recognition via large-kernel attention graph convolutional network.
IEEE Transactions on Visualization and Computer Graphics
<span id="bib.bib216.1.1" class="ltx_text ltx_font_bold">29</span>(5),
2575–2585
(2023)


</span>
</li>
<li id="bib.bib217" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. [2024]</span>
<span class="ltx_bibblock">
Liang, C.,
Yang, J.,
Du, R.,
Hu, W.,
Hou, N.:
Temporal-channel attention and convolution fusion for skeleton-based human action recognition.
IEEE Access
<span id="bib.bib217.1.1" class="ltx_text ltx_font_bold">12</span>,
64937–64949
(2024)
<a target="_blank" href="https://doi.org/10.1109/ACCESS.2024.3389499" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ACCESS.2024.3389499</a>


</span>
</li>
<li id="bib.bib218" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. [2012]</span>
<span class="ltx_bibblock">
Shao, L.,
Ji, L.,
Liu, Y.,
Zhang, J.:
Human action segmentation and recognition via motion and shape analysis.
Pattern Recognition Letters
<span id="bib.bib218.1.1" class="ltx_text ltx_font_bold">33</span>(4),
438–445
(2012)


</span>
</li>
<li id="bib.bib219" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2012]</span>
<span class="ltx_bibblock">
Yang, X.,
Zhang, C.,
Tian, Y.:
Recognizing actions using depth motion maps-based histograms of oriented gradients.
In: Proceedings of the 20th ACM International Conference on Multimedia,
pp. 1057–1060
(2012)


</span>
</li>
<li id="bib.bib220" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Guo [2015]</span>
<span class="ltx_bibblock">
Chen, W.,
Guo, G.:
Triviews: A general framework to use 3d depth data effectively for action recognition.
Journal of Visual Communication and Image Representation
<span id="bib.bib220.1.1" class="ltx_text ltx_font_bold">26</span>,
182–191
(2015)


</span>
</li>
<li id="bib.bib221" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miao et al. [2016]</span>
<span class="ltx_bibblock">
Miao, J.,
Jia, X.,
Mathew, R.,
Xu, X.,
Taubman, D.,
Qing, C.:
Efficient action recognition from compressed depth maps.
In: 2016 IEEE International Conference on Image Processing (ICIP),
pp. 16–20
(2016).
IEEE


</span>
</li>
<li id="bib.bib222" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shotton et al. [2013]</span>
<span class="ltx_bibblock">
Shotton, J.,
Sharp, T.,
Kipman, A.,
Fitzgibbon, A.,
Finocchio, M.,
Blake, A.,
Cook, M.,
Moore, R.:
Real-time human pose recognition in parts from single depth images.
Communications of the ACM
<span id="bib.bib222.1.1" class="ltx_text ltx_font_bold">56</span>(1),
116–124
(2013)


</span>
</li>
<li id="bib.bib223" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al. [2012]</span>
<span class="ltx_bibblock">
Xia, L.,
Chen, C.-C.,
Aggarwal, J.K.:
View invariant human action recognition using histograms of 3d joints.
In: 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops,
pp. 20–27
(2012).
IEEE


</span>
</li>
<li id="bib.bib224" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Keceli and Can [2014]</span>
<span class="ltx_bibblock">
Keceli, A.S.,
Can, A.B.:
Recognition of basic human actions using depth information.
International Journal of Pattern Recognition and Artificial Intelligence
<span id="bib.bib224.1.1" class="ltx_text ltx_font_bold">28</span>(02),
1450004
(2014)


</span>
</li>
<li id="bib.bib225" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pazhoumand-Dar et al. [2015]</span>
<span class="ltx_bibblock">
Pazhoumand-Dar, H.,
Lam, C.-P.,
Masek, M.:
Joint movement similarities for robust 3d action recognition using skeletal data.
Journal of Visual Communication and Image Representation
<span id="bib.bib225.1.1" class="ltx_text ltx_font_bold">30</span>,
10–21
(2015)


</span>
</li>
<li id="bib.bib226" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang [2012]</span>
<span class="ltx_bibblock">
Zhang, Z.:
Microsoft kinect sensor and its effect.
IEEE multimedia
<span id="bib.bib226.1.1" class="ltx_text ltx_font_bold">19</span>(2),
4–10
(2012)


</span>
</li>
<li id="bib.bib227" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. [2017]</span>
<span class="ltx_bibblock">
Ding, Z.,
Wang, P.,
Ogunbona, P.O.,
Li, W.:
Investigation of different skeleton features for cnn-based 3d action recognition.
In: 2017 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW),
pp. 617–622
(2017).
IEEE


</span>
</li>
<li id="bib.bib228" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caetano et al. [2019]</span>
<span class="ltx_bibblock">
Caetano, C.,
Sena, J.,
Brémond, F.,
Dos Santos, J.A.,
Schwartz, W.R.:
Skelemotion: A new representation of skeleton joint sequences based on motion information for 3d action recognition.
In: 2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),
pp. 1–8
(2019).
IEEE


</span>
</li>
<li id="bib.bib229" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2017]</span>
<span class="ltx_bibblock">
Liu, H.,
Tu, J.,
Liu, M.:
Two-stream 3d convolutional neural network for skeleton-based action recognition.
arXiv preprint arXiv:1705.08106
(2017)


</span>
</li>
<li id="bib.bib230" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hochreiter and Schmidhuber [1997]</span>
<span class="ltx_bibblock">
Hochreiter, S.,
Schmidhuber, J.:
Long short-term memory.
Neural computation
<span id="bib.bib230.1.1" class="ltx_text ltx_font_bold">9</span>(8),
1735–1780
(1997)


</span>
</li>
<li id="bib.bib231" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ogiela and Jain [2012]</span>
<span class="ltx_bibblock">
Ogiela, M.R.,
Jain, L.C.:
Computational Intelligence Paradigms in Advanced Pattern Classification
vol. 386.
Springer, ???
(2012)


</span>
</li>
<li id="bib.bib232" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. [2015]</span>
<span class="ltx_bibblock">
Du, Y.,
Wang, W.,
Wang, L.:
Hierarchical recurrent neural network for skeleton based action recognition.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1110–1118
(2015)


</span>
</li>
<li id="bib.bib233" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2017]</span>
<span class="ltx_bibblock">
Liu, J.,
Wang, G.,
Hu, P.,
Duan, L.-Y.,
Kot, A.C.:
Global context-aware attention lstm networks for 3d action recognition.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1647–1656
(2017)


</span>
</li>
<li id="bib.bib234" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2018]</span>
<span class="ltx_bibblock">
Li, S.,
Li, W.,
Cook, C.,
Zhu, C.,
Gao, Y.:
Independently recurrent neural network (indrnn): Building a longer and deeper rnn.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 5457–5466
(2018)


</span>
</li>
<li id="bib.bib235" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miah et al. [2023]</span>
<span class="ltx_bibblock">
Miah, A.S.M.,
Hasan, M.A.M.,
Shin, J.:
Dynamic hand gesture recognition using multi-branch attention based graph and general deep learning model.
IEEE Access
(2023)


</span>
</li>
<li id="bib.bib236" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miah et al. [2024]</span>
<span class="ltx_bibblock">
Miah, A.S.M.,
Hasan, M.A.M.,
Okuyama, Y.,
Tomioka, Y.,
Shin, J.:
Spatial–temporal attention with graph and general neural network-based sign language recognition.
Pattern Analysis and Applications
<span id="bib.bib236.1.1" class="ltx_text ltx_font_bold">27</span>(2),
37
(2024)


</span>
</li>
<li id="bib.bib237" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin et al. [2023]</span>
<span class="ltx_bibblock">
Shin, J.,
Miah, A.S.M.,
Suzuki, K.,
Hirooka, K.,
Hasan, M.A.M.:
Dynamic korean sign language recognition using pose estimation based and attention-based neural network.
IEEE Access
<span id="bib.bib237.1.1" class="ltx_text ltx_font_bold">11</span>,
143501–143513
(2023)
<a target="_blank" href="https://doi.org/10.1109/ACCESS.2023.3343404" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ACCESS.2023.3343404</a>


</span>
</li>
<li id="bib.bib238" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin et al. [2024]</span>
<span class="ltx_bibblock">
Shin, J.,
Kaneko, Y.,
Miah, A.S.M.,
Hassan, N.,
Nishimura, S.:
Anomaly detection in weakly supervised videos using multistage graphs and general deep learning based spatial-temporal feature enhancement.
IEEE Access
<span id="bib.bib238.1.1" class="ltx_text ltx_font_bold">12</span>,
65213–65227
(2024)


</span>
</li>
<li id="bib.bib239" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Egawa et al. [2023]</span>
<span class="ltx_bibblock">
Egawa, R.,
Miah, A.S.M.,
Hirooka, K.,
Tomioka, Y.,
Shin, J.:
Dynamic fall detection using graph-based spatial temporal convolution and attention network.
Electronics
<span id="bib.bib239.1.1" class="ltx_text ltx_font_bold">12</span>(15),
3234
(2023)


</span>
</li>
<li id="bib.bib240" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miah et al. [2024]</span>
<span class="ltx_bibblock">
Miah, A.S.M.,
Hasan, M.A.M.,
Nishimura, S.,
Shin, J.:
Sign language recognition using graph and general deep neural network based on large scale dataset.
IEEE Access
(2024)


</span>
</li>
<li id="bib.bib241" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miah et al. [2023]</span>
<span class="ltx_bibblock">
Miah, A.S.M.,
Hasan, M.A.M.,
Jang, S.-W.,
Lee, H.-S.,
Shin, J.:
Multi-stream general and graph-based deep neural networks for skeleton-based sign language recognition.
Electronics
<span id="bib.bib241.1.1" class="ltx_text ltx_font_bold">12</span>(13)
(2023)


</span>
</li>
<li id="bib.bib242" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gori et al. [2005]</span>
<span class="ltx_bibblock">
Gori, M.,
Monfardini, G.,
Scarselli, F.:
A new model for learning in graph domains.
In: Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.,
vol. 2,
pp. 729–734
(2005).
IEEE


</span>
</li>
<li id="bib.bib243" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2017]</span>
<span class="ltx_bibblock">
Li, R.,
Tapaswi, M.,
Liao, R.,
Jia, J.,
Urtasun, R.,
Fidler, S.:
Situation recognition with graph neural networks.
In: Proceedings of the IEEE International Conference on Computer Vision,
pp. 4173–4182
(2017)


</span>
</li>
<li id="bib.bib244" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kipf and Welling [2016]</span>
<span class="ltx_bibblock">
Kipf, T.N.,
Welling, M.:
Semi-supervised classification with graph convolutional networks.
arXiv preprint arXiv:1609.02907
(2016)


</span>
</li>
<li id="bib.bib245" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shiraki et al. [2020]</span>
<span class="ltx_bibblock">
Shiraki, K.,
Hirakawa, T.,
Yamashita, T.,
Fujiyoshi, H.:
Spatial temporal attention graph convolutional networks with mechanics-stream for skeleton-based action recognition.
In: Proceedings of the Asian Conference on Computer Vision
(2020)


</span>
</li>
<li id="bib.bib246" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. [2020]</span>
<span class="ltx_bibblock">
Shi, L.,
Zhang, Y.,
Cheng, J.,
Lu, H.:
Skeleton-based action recognition with multi-stream adaptive graph convolutional networks.
IEEE Transactions on Image Processing
<span id="bib.bib246.1.1" class="ltx_text ltx_font_bold">29</span>,
9532–9545
(2020)


</span>
</li>
<li id="bib.bib247" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2020]</span>
<span class="ltx_bibblock">
Huang, J.,
Xiang, X.,
Gong, X.,
Zhang, B., <span id="bib.bib247.1.1" class="ltx_text ltx_font_italic">et al.</span>:
Long-short graph memory network for skeleton-based action recognition.
In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,
pp. 645–652
(2020)


</span>
</li>
<li id="bib.bib248" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. [2020]</span>
<span class="ltx_bibblock">
Song, Y.-F.,
Zhang, Z.,
Shan, C.,
Wang, L.:
Stronger, faster and more explainable: A graph convolutional baseline for skeleton-based action recognition.
In: Proceedings of the 28th ACM International Conference on Multimedia,
pp. 1625–1633
(2020)


</span>
</li>
<li id="bib.bib249" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thakkar and Narayanan [2018]</span>
<span class="ltx_bibblock">
Thakkar, K.,
Narayanan, P.:
Part-based graph convolutional network for action recognition.
arXiv preprint arXiv:1809.04983
(2018)


</span>
</li>
<li id="bib.bib250" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019]</span>
<span class="ltx_bibblock">
Li, B.,
Li, X.,
Zhang, Z.,
Wu, F.:
Spatio-temporal graph routing for skeleton-based action recognition.
In: Proceedings of the AAAI Conference on Artificial Intelligence,
vol. 33,
pp. 8561–8568
(2019)


</span>
</li>
<li id="bib.bib251" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanhudo et al. [2021]</span>
<span class="ltx_bibblock">
Sanhudo, L.,
Calvetti, D.,
Martins, J.P.,
Ramos, N.M.,
Meda, P.,
Goncalves, M.C.,
Sousa, H.:
Activity classification using accelerometers and machine learning for complex construction worker activities.
Journal of Building Engineering
<span id="bib.bib251.1.1" class="ltx_text ltx_font_bold">35</span>,
102001
(2021)


</span>
</li>
<li id="bib.bib252" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2021]</span>
<span class="ltx_bibblock">
Chen, K.,
Zhang, D.,
Yao, L.,
Guo, B.,
Yu, Z.,
Liu, Y.:
Deep learning for sensor-based human activity recognition: Overview, challenges, and opportunities.
ACM Computing Surveys (CSUR)
<span id="bib.bib252.1.1" class="ltx_text ltx_font_bold">54</span>(4),
1–40
(2021)


</span>
</li>
<li id="bib.bib253" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huan et al. [2021]</span>
<span class="ltx_bibblock">
Huan, R.,
Jiang, C.,
Ge, L.,
Shu, J.,
Zhan, Z.,
Chen, P.,
Chi, K.,
Liang, R.:
Human complex activity recognition with sensor data using multiple features.
IEEE Sensors Journal
<span id="bib.bib253.1.1" class="ltx_text ltx_font_bold">22</span>(1),
757–775
(2021)


</span>
</li>
<li id="bib.bib254" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nafea et al. [2021]</span>
<span class="ltx_bibblock">
Nafea, O.,
Abdul, W.,
Muhammad, G.,
Alsulaiman, M.:
Sensor-based human activity recognition with spatio-temporal deep learning.
Sensors
<span id="bib.bib254.1.1" class="ltx_text ltx_font_bold">21</span>(6),
2141
(2021)


</span>
</li>
<li id="bib.bib255" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miah et al. [2017]</span>
<span class="ltx_bibblock">
Miah, A.S.M.,
Islam, M.R.,
Molla, M.K.I.:
Motor imagery classification using subband tangent space mapping.
In: 2017 20th International Conference of Computer and Information Technology (ICCIT),
pp. 1–5
(2017).
IEEE


</span>
</li>
<li id="bib.bib256" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miah et al. [2022]</span>
<span class="ltx_bibblock">
Miah, A.S.M.,
Shin, J.,
Hasan, M.A.M.,
Molla, M.K.I.,
Okuyama, Y.,
Tomioka, Y.:
Movie oriented positive negative emotion classification from eeg signal using wavelet transformation and machine learning approaches.
In: 2022 IEEE 15th International Symposium on Embedded Multicore/many-core Systems-on-chip (MCSoC),
pp. 26–31
(2022).
IEEE


</span>
</li>
<li id="bib.bib257" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kabir et al. [2023]</span>
<span class="ltx_bibblock">
Kabir, M.H.,
Mahmood, S.,
Al Shiam, A.,
Musa Miah, A.S.,
Shin, J.,
Molla, M.K.I.:
Investigating feature selection techniques to enhance the performance of eeg-based motor imagery tasks classification.
Mathematics
<span id="bib.bib257.1.1" class="ltx_text ltx_font_bold">11</span>(8),
1921
(2023)


</span>
</li>
<li id="bib.bib258" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stisen et al. [2015]</span>
<span class="ltx_bibblock">
Stisen, A.,
Blunck, H.,
Bhattacharya, S.,
Prentow, T.S.,
Kjærgaard, M.B.,
Dey, A.,
Sonne, T.,
Jensen, M.M.:
Smart devices are different: Assessing and mitigatingmobile sensing heterogeneities for activity recognition.
In: Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems,
pp. 127–140
(2015)


</span>
</li>
<li id="bib.bib259" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abbas et al. [2024]</span>
<span class="ltx_bibblock">
Abbas, S.,
Alsubai, S.,
Sampedro, G.A.,
Haque, M.I.,
Almadhor, A.,
Al Hejaili, A.,
Ivanochko, I.:
Active machine learning for heterogeneity activity recognition through smartwatch sensors.
IEEE Access
(2024)


</span>
</li>
<li id="bib.bib260" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banos et al. [2014]</span>
<span class="ltx_bibblock">
Banos, O.,
Garcia, R.,
Holgado-Terriza, J.A.,
Damas, M.,
Pomares, H.,
Rojas, I.,
Saez, A.:
Villalonga, c. mhealthdroid: A novel framework for agile development of mobile health applications.
In: Proceedings of the International Workshop on Ambient Assisted Living, Belfast, UK,
pp. 2–5
(2014)


</span>
</li>
<li id="bib.bib261" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">El-Adawi et al. [2024]</span>
<span class="ltx_bibblock">
El-Adawi, E.,
Essa, E.,
Handosa, M.,
Elmougy, S.:
Wireless body area sensor networks based human activity recognition using deep learning.
Scientific Reports
<span id="bib.bib261.1.1" class="ltx_text ltx_font_bold">14</span>(1),
2702
(2024)


</span>
</li>
<li id="bib.bib262" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chavarriaga et al. [2013]</span>
<span class="ltx_bibblock">
Chavarriaga, R.,
Sagha, H.,
Calatroni, A.,
Digumarti, S.T.,
Tröster, G.,
Millán, J.d.R.,
Roggen, D.:
The opportunity challenge: A benchmark database for on-body sensor-based activity recognition.
Pattern Recognition Letters
<span id="bib.bib262.1.1" class="ltx_text ltx_font_bold">34</span>(15),
2033–2042
(2013)


</span>
</li>
<li id="bib.bib263" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye and Wang [2024]</span>
<span class="ltx_bibblock">
Ye, X.,
Wang, K.I.-K.:
Deep generative domain adaptation with temporal relation knowledge for cross-user activity recognition.
arXiv preprint arXiv:2403.14682
(2024)


</span>
</li>
<li id="bib.bib264" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwapisz et al. [2011]</span>
<span class="ltx_bibblock">
Kwapisz, J.R.,
Weiss, G.M.,
Moore, S.A.:
Activity recognition using cell phone accelerometers.
ACM SigKDD Explorations Newsletter
<span id="bib.bib264.1.1" class="ltx_text ltx_font_bold">12</span>(2),
74–82
(2011)


</span>
</li>
<li id="bib.bib265" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaya and Topuz [2024]</span>
<span class="ltx_bibblock">
Kaya, Y.,
Topuz, E.K.:
Human activity recognition from multiple sensors data using deep cnns.
Multimedia Tools and Applications
<span id="bib.bib265.1.1" class="ltx_text ltx_font_bold">83</span>(4),
10815–10838
(2024)


</span>
</li>
<li id="bib.bib266" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anguita et al. [2013]</span>
<span class="ltx_bibblock">
Anguita, D.,
Ghio, A.,
Oneto, L.,
Parra, X.,
Reyes-Ortiz, J.L., <span id="bib.bib266.1.1" class="ltx_text ltx_font_italic">et al.</span>:
A public domain dataset for human activity recognition using smartphones.
In: Esann,
vol. 3,
p. 3
(2013)


</span>
</li>
<li id="bib.bib267" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reiss and Stricker [2012]</span>
<span class="ltx_bibblock">
Reiss, A.,
Stricker, D.:
Introducing a new benchmarked dataset for activity monitoring.
In: 2012 16th International Symposium on Wearable Computers,
pp. 108–109
(2012).
IEEE


</span>
</li>
<li id="bib.bib268" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2023]</span>
<span class="ltx_bibblock">
Zhu, Y.,
Luo, H.,
Chen, R.,
Zhao, F.:
Diamondnet: A neural-network-based heterogeneous sensor attentive fusion for human activity recognition.
IEEE Transactions on Neural Networks and Learning Systems
(2023)


</span>
</li>
<li id="bib.bib269" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Altun et al. [2010]</span>
<span class="ltx_bibblock">
Altun, K.,
Barshan, B.,
Tunçel, O.:
Comparative study on classifying human activities with miniature inertial and magnetic sensors.
Pattern Recognition
<span id="bib.bib269.1.1" class="ltx_text ltx_font_bold">43</span>(10),
3605–3620
(2010)


</span>
</li>
<li id="bib.bib270" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Xu [2024]</span>
<span class="ltx_bibblock">
Zhang, H.,
Xu, L.:
Multi-stmt: multi-level network for human activity recognition based on wearable sensors.
IEEE Transactions on Instrumentation and Measurement
(2024)


</span>
</li>
<li id="bib.bib271" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sztyler and Stuckenschmidt [2016]</span>
<span class="ltx_bibblock">
Sztyler, T.,
Stuckenschmidt, H.:
On-body localization of wearable devices: An investigation of position-aware activity recognition.
In: 2016 IEEE International Conference on Pervasive Computing and Communications (PerCom),
pp. 1–9
(2016).
IEEE


</span>
</li>
<li id="bib.bib272" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan et al. [2024]</span>
<span class="ltx_bibblock">
Khan, D.,
Al Mudawi, N.,
Abdelhaq, M.,
Alazeb, A.,
Alotaibi, S.S.,
Algarni, A.,
Jalal, A.:
A wearable inertial sensor approach for locomotion and localization recognition on physical activity.
Sensors
<span id="bib.bib272.1.1" class="ltx_text ltx_font_bold">24</span>(3),
735
(2024)


</span>
</li>
<li id="bib.bib273" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. [2013]</span>
<span class="ltx_bibblock">
Cheng, H.-T.,
Sun, F.-T.,
Griss, M.,
Davis, P.,
Li, J.,
You, D.:
Nuactiv: Recognizing unseen new activities using semantic attribute-based learning.
In: Proceeding of the 11th Annual International Conference on Mobile Systems, Applications, and Services,
pp. 361–374
(2013)


</span>
</li>
<li id="bib.bib274" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zolfaghari et al. [2024]</span>
<span class="ltx_bibblock">
Zolfaghari, P.,
Rey, V.F.,
Ray, L.,
Kim, H.,
Suh, S.,
Lukowicz, P.:
Sensor data augmentation from skeleton pose sequences for improving human activity recognition.
arXiv preprint arXiv:2406.16886
(2024)


</span>
</li>
<li id="bib.bib275" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shoaib et al. [2014]</span>
<span class="ltx_bibblock">
Shoaib, M.,
Bosch, S.,
Incel, O.D.,
Scholten, H.,
Havinga, P.J.:
Fusion of smartphone motion sensors for physical activity recognition.
Sensors
<span id="bib.bib275.1.1" class="ltx_text ltx_font_bold">14</span>(6),
10146–10176
(2014)


</span>
</li>
<li id="bib.bib276" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2024]</span>
<span class="ltx_bibblock">
Zhang, L.,
Yu, J.,
Gao, Z.,
Ni, Q.:
A multi-channel hybrid deep learning framework for multi-sensor fusion enabled human activity recognition.
Alexandria Engineering Journal
<span id="bib.bib276.1.1" class="ltx_text ltx_font_bold">91</span>,
472–485
(2024)


</span>
</li>
<li id="bib.bib277" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huynh et al. [2008]</span>
<span class="ltx_bibblock">
Huynh, T.,
Fritz, M.,
Schiele, B.:
Discovery of activity patterns using topic models.
In: Proceedings of the 10th International Conference on Ubiquitous Computing,
pp. 10–19
(2008)


</span>
</li>
<li id="bib.bib278" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Micucci et al. [2017]</span>
<span class="ltx_bibblock">
Micucci, D.,
Mobilio, M.,
Napoletano, P.:
Unimib shar: A dataset for human activity recognition using acceleration data from smartphones.
Applied Sciences
<span id="bib.bib278.1.1" class="ltx_text ltx_font_bold">7</span>(10),
1101
(2017)


</span>
</li>
<li id="bib.bib279" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. [2024]</span>
<span class="ltx_bibblock">
Yao, M.,
Zhang, L.,
Cheng, D.,
Qin, L.,
Liu, X.,
Fu, Z.,
Wu, H.,
Song, A.:
Revisiting large-kernel cnn design via structural re-parameterization for sensor-based human activity recognition.
IEEE Sensors Journal
(2024)


</span>
</li>
<li id="bib.bib280" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Sawchuk [2012]</span>
<span class="ltx_bibblock">
Zhang, M.,
Sawchuk, A.A.:
Usc-had: A daily activity dataset for ubiquitous activity recognition using wearable sensors.
In: Proceedings of the 2012 ACM Conference on Ubiquitous Computing,
pp. 1036–1043
(2012)


</span>
</li>
<li id="bib.bib281" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vavoulas et al. [2016]</span>
<span class="ltx_bibblock">
Vavoulas, G.,
Chatzaki, C.,
Malliotakis, T.,
Pediaditis, M.,
Tsiknakis, M.:
The mobiact dataset: Recognition of activities of daily living using smartphones.
In: International Conference on Information and Communication Technologies for Ageing Well and E-health,
vol. 2,
pp. 143–151
(2016).
SciTePress


</span>
</li>
<li id="bib.bib282" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khaertdinov and Asteriadis [2023]</span>
<span class="ltx_bibblock">
Khaertdinov, B.,
Asteriadis, S.:
Explaining, analyzing, and probing representations of self-supervised learning models for sensor-based human activity recognition.
In: 2023 IEEE International Joint Conference on Biometrics (IJCB),
pp. 1–10
(2023).
IEEE


</span>
</li>
<li id="bib.bib283" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malekzadeh et al. [2018]</span>
<span class="ltx_bibblock">
Malekzadeh, M.,
Clegg, R.G.,
Cavallaro, A.,
Haddadi, H.:
Protecting sensory data against sensitive inferences.
In: Proceedings of the 1st Workshop on Privacy by Design in Distributed Systems,
pp. 1–6
(2018)


</span>
</li>
<li id="bib.bib284" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saha et al. [2024]</span>
<span class="ltx_bibblock">
Saha, U.,
Saha, S.,
Kabir, M.T.,
Fattah, S.A.,
Saquib, M.:
Decoding human activities: Analyzing wearable accelerometer and gyroscope data for activity recognition.
IEEE Sensors Letters
(2024)


</span>
</li>
<li id="bib.bib285" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van Kasteren et al. [2011]</span>
<span class="ltx_bibblock">
Kasteren, T.L.,
Englebienne, G.,
Kröse, B.J.:
Human activity recognition from wireless sensor network data: Benchmark and software.
In: Activity Recognition in Pervasive Intelligent Environments,
pp. 165–186.
Springer, ???
(2011)


</span>
</li>
<li id="bib.bib286" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cook et al. [2012]</span>
<span class="ltx_bibblock">
Cook, D.J.,
Crandall, A.S.,
Thomas, B.L.,
Krishnan, N.C.:
Casas: A smart home in a box.
Computer
<span id="bib.bib286.1.1" class="ltx_text ltx_font_bold">46</span>(7),
62–69
(2012)


</span>
</li>
<li id="bib.bib287" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim and Lee [2024]</span>
<span class="ltx_bibblock">
Kim, H.,
Lee, D.:
Clan: A contrastive learning based novelty detection framework for human activity recognition.
arXiv preprint arXiv:2401.10288
(2024)


</span>
</li>
<li id="bib.bib288" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zappi et al. [2008]</span>
<span class="ltx_bibblock">
Zappi, P.,
Lombriser, C.,
Stiefmeier, T.,
Farella, E.,
Roggen, D.,
Benini, L.,
Tröster, G.:
Activity recognition from on-body sensors: accuracy-power trade-off by dynamic sensor selection.
In: Wireless Sensor Networks: 5th European Conference, EWSN 2008, Bologna, Italy, January 30-February 1, 2008. Proceedings,
pp. 17–33
(2008).
Springer


</span>
</li>
<li id="bib.bib289" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023]</span>
<span class="ltx_bibblock">
Zhang, Z.,
Wang, W.,
An, A.,
Qin, Y.,
Yang, F.:
A human activity recognition method using wearable sensors based on convtransformer model.
Evolving Systems
<span id="bib.bib289.1.1" class="ltx_text ltx_font_bold">14</span>(6),
939–955
(2023)


</span>
</li>
<li id="bib.bib290" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2019]</span>
<span class="ltx_bibblock">
Zheng, Y.,
Zhang, Y.,
Qian, K.,
Zhang, G.,
Liu, Y.,
Wu, C.,
Yang, Z.:
Zero-effort cross-domain gesture recognition with wi-fi.
In: Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services,
pp. 313–325
(2019)


</span>
</li>
<li id="bib.bib291" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2024]</span>
<span class="ltx_bibblock">
Chen, J.,
Xu, X.,
Wang, T.,
Jeon, G.,
Camacho, D.:
An aiot framework with multi-modal frequency fusion for wifi-based coarse and fine activity recognition.
IEEE Internet of Things Journal
(2024)


</span>
</li>
<li id="bib.bib292" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reyes-Ortiz et al. [2016]</span>
<span class="ltx_bibblock">
Reyes-Ortiz, J.-L.,
Oneto, L.,
Samà, A.,
Parra, X.,
Anguita, D.:
Transition-aware human activity recognition using smartphones.
Neurocomputing
<span id="bib.bib292.1.1" class="ltx_text ltx_font_bold">171</span>,
754–767
(2016)


</span>
</li>
<li id="bib.bib293" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ignatov [2018]</span>
<span class="ltx_bibblock">
Ignatov, A.:
Real-time human activity recognition from accelerometer data using convolutional neural networks.
Applied Soft Computing
<span id="bib.bib293.1.1" class="ltx_text ltx_font_bold">62</span>,
915–922
(2018)


</span>
</li>
<li id="bib.bib294" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain and Kanhangad [2017]</span>
<span class="ltx_bibblock">
Jain, A.,
Kanhangad, V.:
Human activity classification in smartphones using accelerometer and gyroscope sensors.
IEEE Sensors Journal
<span id="bib.bib294.1.1" class="ltx_text ltx_font_bold">18</span>(3),
1169–1177
(2017)


</span>
</li>
<li id="bib.bib295" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2019]</span>
<span class="ltx_bibblock">
Chen, K.,
Yao, L.,
Zhang, D.,
Wang, X.,
Chang, X.,
Nie, F.:
A semisupervised recurrent convolutional attention model for human activity recognition.
IEEE transactions on neural networks and learning systems
<span id="bib.bib295.1.1" class="ltx_text ltx_font_bold">31</span>(5),
1747–1756
(2019)


</span>
</li>
<li id="bib.bib296" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alawneh et al. [2020]</span>
<span class="ltx_bibblock">
Alawneh, L.,
Mohsen, B.,
Al-Zinati, M.,
Shatnawi, A.,
Al-Ayyoub, M.:
A comparison of unidirectional and bidirectional lstm networks for human activity recognition.
In: 2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops),
pp. 1–6
(2020).
IEEE


</span>
</li>
<li id="bib.bib297" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2020]</span>
<span class="ltx_bibblock">
Lin, Y.,
Wu, J., et al.:
A novel multichannel dilated convolution neural network for human activity recognition.
Mathematical Problems in Engineering
<span id="bib.bib297.1.1" class="ltx_text ltx_font_bold">2020</span>
(2020)


</span>
</li>
<li id="bib.bib298" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2020]</span>
<span class="ltx_bibblock">
Zhang, J.,
Wu, F.,
Wei, B.,
Zhang, Q.,
Huang, H.,
Shah, S.W.,
Cheng, J.:
Data augmentation and dense-lstm for human activity recognition using wifi signal.
IEEE Internet of Things Journal
<span id="bib.bib298.1.1" class="ltx_text ltx_font_bold">8</span>(6),
4628–4641
(2020)


</span>
</li>
<li id="bib.bib299" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nadeem et al. [2021]</span>
<span class="ltx_bibblock">
Nadeem, A.,
Jalal, A.,
Kim, K.:
Automatic human posture estimation for sport activity recognition with robust body parts detection and entropy markov model.
Multimedia Tools and Applications
<span id="bib.bib299.1.1" class="ltx_text ltx_font_bold">80</span>,
21465–21498
(2021)


</span>
</li>
<li id="bib.bib300" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kavuncuoğlu et al. [2022]</span>
<span class="ltx_bibblock">
Kavuncuoğlu, E.,
Uzunhisarcıklı, E.,
Barshan, B.,
Özdemir, A.T.:
Investigating the performance of wearable motion sensors on recognizing falls and daily activities via machine learning.
Digital Signal Processing
<span id="bib.bib300.1.1" class="ltx_text ltx_font_bold">126</span>,
103365
(2022)


</span>
</li>
<li id="bib.bib301" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2022]</span>
<span class="ltx_bibblock">
Lu, L.,
Zhang, C.,
Cao, K.,
Deng, T.,
Yang, Q.:
A multichannel cnn-gru model for human activity recognition.
IEEE Access
<span id="bib.bib301.1.1" class="ltx_text ltx_font_bold">10</span>,
66797–66810
(2022)


</span>
</li>
<li id="bib.bib302" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. [2022]</span>
<span class="ltx_bibblock">
Kim, Y.W.,
Cho, W.H.,
Kim, K.S.,
Lee, S.:
Oversampling technique-based data augmentation and 1d-cnn and bidirectional gru ensemble model for human activity recognition.
Journal of Mechanics in Medicine and Biology
<span id="bib.bib302.1.1" class="ltx_text ltx_font_bold">22</span>(09),
2240048
(2022)


</span>
</li>
<li id="bib.bib303" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarkar et al. [2023]</span>
<span class="ltx_bibblock">
Sarkar, A.,
Hossain, S.S.,
Sarkar, R.:
Human activity recognition from sensor data using spatial attention-aided cnn with genetic algorithm.
Neural Computing and Applications
<span id="bib.bib303.1.1" class="ltx_text ltx_font_bold">35</span>(7),
5165–5191
(2023)


</span>
</li>
<li id="bib.bib304" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Semwal et al. [2023]</span>
<span class="ltx_bibblock">
Semwal, V.B.,
Jain, R.,
Maheshwari, P.,
Khatwani, S.:
Gait reference trajectory generation at different walking speeds using lstm and cnn.
Multimedia Tools and Applications
<span id="bib.bib304.1.1" class="ltx_text ltx_font_bold">82</span>(21),
33401–33419
(2023)


</span>
</li>
<li id="bib.bib305" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei and Wang [2024]</span>
<span class="ltx_bibblock">
Wei, X.,
Wang, Z.:
Tcn-attention-har: human activity recognition based on attention mechanism time convolutional network.
Scientific Reports
<span id="bib.bib305.1.1" class="ltx_text ltx_font_bold">14</span>(1),
7414
(2024)


</span>
</li>
<li id="bib.bib306" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Latyshev [2018]</span>
<span class="ltx_bibblock">
Latyshev, E.:
Sensor data preprocessing, feature engineering and equipment remaining lifetime forecasting for predictive maintenance.
In: DAMDID/RCDL,
pp. 226–231
(2018)


</span>
</li>
<li id="bib.bib307" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miah et al. [2019]</span>
<span class="ltx_bibblock">
Miah, A.S.M.,
Islam, M.R.,
Molla, M.K.I.:
Eeg classification for mi-bci using csp with averaging covariance matrices: An experimental study.
In: 2019 International Conference on Computer, Communication, Chemical, Materials and Electronic Engineering (IC4ME2),
pp. 1–5
(2019).
IEEE


</span>
</li>
<li id="bib.bib308" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joy et al. [2020]</span>
<span class="ltx_bibblock">
Joy, M.M.H.,
Hasan, M.,
Miah, A.S.M.,
Ahmed, A.,
Tohfa, S.A.,
Bhuaiyan, M.F.I.,
Zannat, A.,
Rashid, M.M.:
Multiclass mi-task classification using logistic regression and filter bank common spatial patterns.
In: International Conference on Computing Science, Communication and Security,
pp. 160–170
(2020).
Springer


</span>
</li>
<li id="bib.bib309" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miah et al. [2020]</span>
<span class="ltx_bibblock">
Miah, A.S.M.,
Rahim, M.A.,
Shin, J.:
Motor-imagery classification using riemannian geometry with median absolute deviation.
Electronics
<span id="bib.bib309.1.1" class="ltx_text ltx_font_bold">9</span>(10),
1584
(2020)


</span>
</li>
<li id="bib.bib310" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zobaed et al. [2020]</span>
<span class="ltx_bibblock">
Zobaed, T.,
Ahmed, S.R.A.,
Miah, A.S.M.,
Binta, S.M.,
Ahmed, M.R.A.,
Rashid, M.:
Real time sleep onset detection from single channel eeg signal using block sample entropy.
In: IOP Conference Series: Materials Science and Engineering,
vol. 928,
p. 032021
(2020).
IOP Publishing


</span>
</li>
<li id="bib.bib311" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miah et al. [2021]</span>
<span class="ltx_bibblock">
Miah, A.S.M.,
Mouly, M.A.,
Debnath, C.,
Shin, J.,
Sadakatul Bari, S.:
Event-related potential classification based on eeg data using xdwan with mdm and knn.
In: International Conference on Computing Science, Communication and Security,
pp. 112–126
(2021).
Springer


</span>
</li>
<li id="bib.bib312" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hussain et al. [2023]</span>
<span class="ltx_bibblock">
Hussain, I.,
Jany, R.,
Boyer, R.,
Azad, A.,
Alyami, S.A.,
Park, S.J.,
Hasan, M.M.,
Hossain, M.A.:
An explainable eeg-based human activity recognition model using machine-learning approach and lime.
Sensors
<span id="bib.bib312.1.1" class="ltx_text ltx_font_bold">23</span>(17),
7452
(2023)


</span>
</li>
<li id="bib.bib313" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thakur et al. [2022]</span>
<span class="ltx_bibblock">
Thakur, D.,
Biswas, S.,
Ho, E.S.,
Chattopadhyay, S.:
Convae-lstm: Convolutional autoencoder long short-term memory network for smartphone-based human activity recognition.
IEEE Access
<span id="bib.bib313.1.1" class="ltx_text ltx_font_bold">10</span>,
4137–4156
(2022)


</span>
</li>
<li id="bib.bib314" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madsen [2007]</span>
<span class="ltx_bibblock">
Madsen, H.:
Time Series Analysis.
Chapman and Hall/CRC, ???
(2007)


</span>
</li>
<li id="bib.bib315" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ordóñez and Roggen [2016]</span>
<span class="ltx_bibblock">
Ordóñez, F.J.,
Roggen, D.:
Deep convolutional and lstm recurrent neural networks for multimodal wearable activity recognition.
Sensors
<span id="bib.bib315.1.1" class="ltx_text ltx_font_bold">16</span>(1),
115
(2016)


</span>
</li>
<li id="bib.bib316" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Murad and Pyun [2017]</span>
<span class="ltx_bibblock">
Murad, A.,
Pyun, J.-Y.:
Deep recurrent neural networks for human activity recognition.
Sensors
<span id="bib.bib316.1.1" class="ltx_text ltx_font_bold">17</span>(11),
2556
(2017)


</span>
</li>
<li id="bib.bib317" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta [2021]</span>
<span class="ltx_bibblock">
Gupta, S.:
Deep learning based human activity recognition (har) using wearable sensor data.
International Journal of Information Management Data Insights
<span id="bib.bib317.1.1" class="ltx_text ltx_font_bold">1</span>(2),
100046
(2021)


</span>
</li>
<li id="bib.bib318" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2020]</span>
<span class="ltx_bibblock">
Chen, Z.,
Wu, M.,
Cui, W.,
Liu, C.,
Li, X.:
An attention based cnn-lstm approach for sleep-wake detection with heterogeneous sensors.
IEEE Journal of Biomedical and Health Informatics
<span id="bib.bib318.1.1" class="ltx_text ltx_font_bold">25</span>(9),
3270–3277
(2020)


</span>
</li>
<li id="bib.bib319" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Essa and Abdelmaksoud [2023]</span>
<span class="ltx_bibblock">
Essa, E.,
Abdelmaksoud, I.R.:
Temporal-channel convolution with self-attention network for human activity recognition using wearable sensors.
Knowledge-Based Systems
<span id="bib.bib319.1.1" class="ltx_text ltx_font_bold">278</span>,
110867
(2023)


</span>
</li>
<li id="bib.bib320" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2020]</span>
<span class="ltx_bibblock">
Zhang, X.-Y.,
Shi, H.,
Li, C.,
Li, P.:
Multi-instance multi-label action recognition and localization based on spatio-temporal pre-trimming for untrimmed videos.
In: Proceedings of the AAAI Conference on Artificial Intelligence,
vol. 34,
pp. 12886–12893
(2020)


</span>
</li>
<li id="bib.bib321" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rani et al. [2021]</span>
<span class="ltx_bibblock">
Rani, S.S.,
Naidu, G.A.,
Shree, V.U.:
Kinematic joint descriptor and depth motion descriptor with convolutional neural networks for human action recognition.
Materials Today: Proceedings
<span id="bib.bib321.1.1" class="ltx_text ltx_font_bold">37</span>,
3164–3173
(2021)


</span>
</li>
<li id="bib.bib322" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhiman and Vishwakarma [2020]</span>
<span class="ltx_bibblock">
Dhiman, C.,
Vishwakarma, D.K.:
View-invariant deep architecture for human action recognition using two-stream motion and shape temporal dynamics.
IEEE Transactions on Image Processing
<span id="bib.bib322.1.1" class="ltx_text ltx_font_bold">29</span>,
3835–3844
(2020)


</span>
</li>
<li id="bib.bib323" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2019]</span>
<span class="ltx_bibblock">
Wang, L.,
Ding, Z.,
Tao, Z.,
Liu, Y.,
Fu, Y.:
Generative multi-view human action recognition.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision,
pp. 6212–6221
(2019)


</span>
</li>
<li id="bib.bib324" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rahmani et al. [2014]</span>
<span class="ltx_bibblock">
Rahmani, H.,
Mahmood, A.,
Huynh, D.Q.,
Mian, A.:
Real time action recognition using histograms of depth gradients and random decision forests.
In: IEEE Winter Conference on Applications of Computer Vision,
pp. 626–633
(2014).
IEEE


</span>
</li>
<li id="bib.bib325" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Güler et al. [2018]</span>
<span class="ltx_bibblock">
Güler, R.A.,
Neverova, N.,
Kokkinos, I.:
Densepose: Dense human pose estimation in the wild.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 7297–7306
(2018)


</span>
</li>
<li id="bib.bib326" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. [2017]</span>
<span class="ltx_bibblock">
Cao, Z.,
Simon, T.,
Wei, S.,
Sheikh, Y.:
Realtime multi-person 2d pose estimation using part affinity fields proceedings of the ieee conference on computer vision and pattern recognitionjuly 2017honolulu.
HI, USA7291–7299
(2017)


</span>
</li>
<li id="bib.bib327" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2015]</span>
<span class="ltx_bibblock">
Hu, J.-F.,
Zheng, W.-S.,
Lai, J.,
Zhang, J.:
Jointly learning heterogeneous features for rgb-d activity recognition.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 5344–5352
(2015)


</span>
</li>
<li id="bib.bib328" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2018]</span>
<span class="ltx_bibblock">
Hu, J.-F.,
Zheng, W.-S.,
Pan, J.,
Lai, J.,
Zhang, J.:
Deep bilinear learning for rgb-d action recognition.
In: Proceedings of the European Conference on Computer Vision (ECCV),
pp. 335–351
(2018)


</span>
</li>
<li id="bib.bib329" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khaire et al. [2018]</span>
<span class="ltx_bibblock">
Khaire, P.,
Kumar, P.,
Imran, J.:
Combining cnn streams of rgb-d and skeletal data for human activity recognition.
Pattern Recognition Letters
<span id="bib.bib329.1.1" class="ltx_text ltx_font_bold">115</span>,
107–116
(2018)


</span>
</li>
<li id="bib.bib330" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cardenas and Chavez [2018]</span>
<span class="ltx_bibblock">
Cardenas, E.E.,
Chavez, G.C.:
Multimodal human action recognition based on a fusion of dynamic images using cnn descriptors.
In: 2018 31st SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI),
pp. 95–102
(2018).
IEEE


</span>
</li>
<li id="bib.bib331" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khaire et al. [2018]</span>
<span class="ltx_bibblock">
Khaire, P.,
Imran, J.,
Kumar, P.:
Human activity recognition by fusion of rgb, depth, and skeletal data.
In: Proceedings of 2nd International Conference on Computer Vision &amp; Image Processing: CVIP 2017, Volume 1,
pp. 409–421
(2018).
Springer


</span>
</li>
<li id="bib.bib332" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al. [2016]</span>
<span class="ltx_bibblock">
Hou, Y.,
Li, Z.,
Wang, P.,
Li, W.:
Skeleton optical spectra-based action recognition using convolutional neural networks.
IEEE Transactions on Circuits and Systems for Video Technology
<span id="bib.bib332.1.1" class="ltx_text ltx_font_bold">28</span>(3),
807–811
(2016)


</span>
</li>
<li id="bib.bib333" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2016]</span>
<span class="ltx_bibblock">
Wang, C.,
Yang, H.,
Meinel, C.:
Exploring multimodal video representation for action recognition.
In: 2016 International Joint Conference on Neural Networks (IJCNN),
pp. 1924–1931
(2016).
IEEE


</span>
</li>
<li id="bib.bib334" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kazakos et al. [2019]</span>
<span class="ltx_bibblock">
Kazakos, E.,
Nagrani, A.,
Zisserman, A.,
Damen, D.:
Epic-fusion: Audio-visual temporal binding for egocentric action recognition.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision,
pp. 5492–5501
(2019)


</span>
</li>
<li id="bib.bib335" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2020]</span>
<span class="ltx_bibblock">
Gao, R.,
Oh, T.-H.,
Grauman, K.,
Torresani, L.:
Listen to look: Action recognition by previewing audio.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 10457–10467
(2020)


</span>
</li>
<li id="bib.bib336" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. [2020]</span>
<span class="ltx_bibblock">
Xiao, F.,
Lee, Y.J.,
Grauman, K.,
Malik, J.,
Feichtenhofer, C.:
Audiovisual slowfast networks for video recognition.
arXiv preprint arXiv:2001.08740
(2020)


</span>
</li>
<li id="bib.bib337" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bruce et al. [2022]</span>
<span class="ltx_bibblock">
Bruce, X.,
Liu, Y.,
Zhang, X.,
Zhong, S.-h.,
Chan, K.C.:
Mmnet: A model-based multimodal network for human action recognition in rgb-d videos.
IEEE Transactions on Pattern Analysis and Machine Intelligence
<span id="bib.bib337.1.1" class="ltx_text ltx_font_bold">45</span>(3),
3522–3538
(2022)


</span>
</li>
<li id="bib.bib338" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Venkatachalam et al. [2023]</span>
<span class="ltx_bibblock">
Venkatachalam, K.,
Yang, Z.,
Trojovskỳ, P.,
Bacanin, N.,
Deveci, M.,
Ding, W.:
Bimodal har-an efficient approach to human activity analysis and recognition using bimodal hybrid classifiers.
Information Sciences
<span id="bib.bib338.1.1" class="ltx_text ltx_font_bold">628</span>,
542–557
(2023)


</span>
</li>
<li id="bib.bib339" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Romaissa et al. [2021]</span>
<span class="ltx_bibblock">
Romaissa, B.D.,
Mourad, O.,
Brahim, N.:
Vision-based multi-modal framework for action recognition.
In: 2020 25th International Conference on Pattern Recognition (ICPR),
pp. 5859–5866
(2021).
IEEE


</span>
</li>
<li id="bib.bib340" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. [2021]</span>
<span class="ltx_bibblock">
Ren, Z.,
Zhang, Q.,
Gao, X.,
Hao, P.,
Cheng, J.:
Multi-modality learning for human action recognition.
Multimedia Tools and Applications
<span id="bib.bib340.1.1" class="ltx_text ltx_font_bold">80</span>(11),
16185–16203
(2021)


</span>
</li>
<li id="bib.bib341" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Ho [2022]</span>
<span class="ltx_bibblock">
Chen, J.,
Ho, C.M.:
Mm-vit: Multi-modal video transformer for compressed video action recognition.
In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,
pp. 1910–1921
(2022)


</span>
</li>
<li id="bib.bib342" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khatun et al. [2022]</span>
<span class="ltx_bibblock">
Khatun, M.A.,
Yousuf, M.A.,
Ahmed, S.,
Uddin, M.Z.,
Alyami, S.A.,
Al-Ashhab, S.,
Akhdar, H.F.,
Khan, A.,
Azad, A.,
Moni, M.A.:
Deep cnn-lstm with self-attention model for human activity recognition using wearable sensor.
IEEE Journal of Translational Engineering in Health and Medicine
<span id="bib.bib342.1.1" class="ltx_text ltx_font_bold">10</span>,
1–16
(2022)


</span>
</li>
<li id="bib.bib343" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chi et al. [2022]</span>
<span class="ltx_bibblock">
Chi, H.-g.,
Ha, M.H.,
Chi, S.,
Lee, S.W.,
Huang, Q.,
Ramani, K.:
Infogcn: Representation learning for human skeleton-based action recognition.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 20186–20196
(2022)


</span>
</li>
<li id="bib.bib344" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Koniusz [2023]</span>
<span class="ltx_bibblock">
Wang, L.,
Koniusz, P.:
3mformer: Multi-order multi-mode transformer for skeletal action recognition.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 5620–5631
(2023)


</span>
</li>
<li id="bib.bib345" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2023]</span>
<span class="ltx_bibblock">
Xu, H.,
Gao, Y.,
Hui, Z.,
Li, J.,
Gao, X.:
Language knowledge-assisted representation learning for skeleton-based action recognition.
arXiv preprint arXiv:2305.12398
(2023)


</span>
</li>
<li id="bib.bib346" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shaikh et al. [2024]</span>
<span class="ltx_bibblock">
Shaikh, M.B.,
Chai, D.,
Islam, S.M.S.,
Akhtar, N.:
Multimodal fusion for audio-image and video action recognition.
Neural Computing and Applications,
1–15
(2024)


</span>
</li>
<li id="bib.bib347" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. [2024]</span>
<span class="ltx_bibblock">
Dai, C.,
Lu, S.,
Liu, C.,
Guo, B.:
A light-weight skeleton human action recognition model with knowledge distillation for edge intelligent surveillance applications.
Applied Soft Computing
<span id="bib.bib347.1.1" class="ltx_text ltx_font_bold">151</span>,
111166
(2024)


</span>
</li>
<li id="bib.bib348" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Keselman et al. [2017]</span>
<span class="ltx_bibblock">
Keselman, L.,
Iselin Woodfill, J.,
Grunnet-Jepsen, A.,
Bhowmik, A.:
Intel realsense stereoscopic depth cameras.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,
pp. 1–10
(2017)


</span>
</li>
<li id="bib.bib349" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Drouin and Seoud [2020]</span>
<span class="ltx_bibblock">
Drouin, M.-A.,
Seoud, L.:
Consumer-grade rgb-d cameras.
3D Imaging, Analysis and Applications,
215–264
(2020)


</span>
</li>
<li id="bib.bib350" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grunnet-Jepsen et al. [2018]</span>
<span class="ltx_bibblock">
Grunnet-Jepsen, A.,
Sweetser, J.N.,
Woodfill, J.:
Best-known-methods for tuning intel® realsense™ d400 depth cameras for best performance.
Intel Corporation: Satan Clara, CA, USA
<span id="bib.bib350.1.1" class="ltx_text ltx_font_bold">1</span>
(2018)


</span>
</li>
<li id="bib.bib351" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zabatani et al. [2019]</span>
<span class="ltx_bibblock">
Zabatani, A.,
Surazhsky, V.,
Sperling, E.,
Moshe, S.B.,
Menashe, O.,
Silver, D.H.,
Karni, Z.,
Bronstein, A.M.,
Bronstein, M.M.,
Kimmel, R.:
Intel® realsense™ sr300 coded light depth camera.
IEEE transactions on pattern analysis and machine intelligence
<span id="bib.bib351.1.1" class="ltx_text ltx_font_bold">42</span>(10),
2333–2345
(2019)


</span>
</li>
<li id="bib.bib352" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2020]</span>
<span class="ltx_bibblock">
Li, T.,
Zhang, R.,
Li, Q.:
Multi scale temporal graph networks for skeleton-based action recognition.
arXiv preprint arXiv:2012.02970
(2020)


</span>
</li>
<li id="bib.bib353" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parsa et al. [2020]</span>
<span class="ltx_bibblock">
Parsa, B.,
Dariush, B., <span id="bib.bib353.1.1" class="ltx_text ltx_font_italic">et al.</span>:
Spatio-temporal pyramid graph convolutions for human action recognition and postural assessment.
In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,
pp. 1080–1090
(2020)


</span>
</li>
<li id="bib.bib354" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2020]</span>
<span class="ltx_bibblock">
Zhu, G.,
Zhang, L.,
Li, H.,
Shen, P.,
Shah, S.A.A.,
Bennamoun, M.:
Topology-learnable graph convolution for skeleton-based action recognition.
Pattern Recognition Letters
<span id="bib.bib354.1.1" class="ltx_text ltx_font_bold">135</span>,
286–292
(2020)


</span>
</li>
<li id="bib.bib355" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2021]</span>
<span class="ltx_bibblock">
Li, M.,
Chen, S.,
Chen, X.,
Zhang, Y.,
Wang, Y.,
Tian, Q.:
Symbiotic graph neural networks for 3d skeleton-based human action recognition and motion prediction.
IEEE transactions on pattern analysis and machine intelligence
<span id="bib.bib355.1.1" class="ltx_text ltx_font_bold">44</span>(6),
3316–3333
(2021)


</span>
</li>
<li id="bib.bib356" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2018]</span>
<span class="ltx_bibblock">
Li, Y.,
Li, Y.,
Vasconcelos, N.:
Resound: Towards action recognition without representation bias.
In: Proceedings of the European Conference on Computer Vision (ECCV),
pp. 513–528
(2018)


</span>
</li>
<li id="bib.bib357" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al. [2020]</span>
<span class="ltx_bibblock">
Zhong, Z.,
Zheng, L.,
Kang, G.,
Li, S.,
Yang, Y.:
Random erasing data augmentation.
In: Proceedings of the AAAI Conference on Artificial Intelligence,
vol. 34,
pp. 13001–13008
(2020)


</span>
</li>
<li id="bib.bib358" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bowles et al. [2018]</span>
<span class="ltx_bibblock">
Bowles, C.,
Chen, L.,
Guerrero, R.,
Bentley, P.,
Gunn, R.,
Hammers, A.,
Dickie, D.A.,
Hernández, M.V.,
Wardlaw, J.,
Rueckert, D.:
Gan augmentation: Augmenting training data using generative adversarial networks.
arXiv preprint arXiv:1810.10863
(2018)


</span>
</li>
<li id="bib.bib359" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al. [2017]</span>
<span class="ltx_bibblock">
Kang, G.,
Dong, X.,
Zheng, L.,
Yang, Y.:
Patchshuffle regularization.
arXiv preprint arXiv:1707.07103
(2017)


</span>
</li>
<li id="bib.bib360" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeVries and Taylor [2017]</span>
<span class="ltx_bibblock">
DeVries, T.,
Taylor, G.W.:
Dataset augmentation in feature space.
arXiv preprint arXiv:1702.05538
(2017)


</span>
</li>
<li id="bib.bib361" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2018]</span>
<span class="ltx_bibblock">
Li, S.,
Chen, Y.,
Peng, Y.,
Bai, L.:
Learning more robust features with adversarial training.
arXiv preprint arXiv:1804.07757
(2018)


</span>
</li>
<li id="bib.bib362" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Real et al. [2017]</span>
<span class="ltx_bibblock">
Real, E.,
Moore, S.,
Selle, A.,
Saxena, S.,
Suematsu, Y.L.,
Tan, J.,
Le, Q.V.,
Kurakin, A.:
Large-scale evolution of image classifiers.
In: International Conference on Machine Learning,
pp. 2902–2911
(2017).
PMLR


</span>
</li>
<li id="bib.bib363" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou et al. [2023]</span>
<span class="ltx_bibblock">
Zou, Y.,
Choi, J.,
Wang, Q.,
Huang, J.-B.:
Learning representational invariances for data-efficient action recognition.
Computer Vision and Image Understanding
<span id="bib.bib363.1.1" class="ltx_text ltx_font_bold">227</span>,
103597
(2023)


</span>
</li>
<li id="bib.bib364" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2020]</span>
<span class="ltx_bibblock">
Zhang, Y.,
Jia, G.,
Chen, L.,
Zhang, M.,
Yong, J.:
Self-paced video data augmentation by generative adversarial networks with insufficient samples.
In: Proceedings of the 28th ACM International Conference on Multimedia,
pp. 1652–1660
(2020)


</span>
</li>
<li id="bib.bib365" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gowda et al. [2022]</span>
<span class="ltx_bibblock">
Gowda, S.N.,
Rohrbach, M.,
Keller, F.,
Sevilla-Lara, L.:
Learn2augment: Learning to composite videos for data augmentation in action recognition.
In: European Conference on Computer Vision,
pp. 242–259
(2022).
Springer


</span>
</li>
<li id="bib.bib366" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gabeur et al. [2020]</span>
<span class="ltx_bibblock">
Gabeur, V.,
Sun, C.,
Alahari, K.,
Schmid, C.:
Multi-modal transformer for video retrieval.
In: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IV 16,
pp. 214–229
(2020).
Springer


</span>
</li>
<li id="bib.bib367" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Piergiovanni and Ryoo [2020]</span>
<span class="ltx_bibblock">
Piergiovanni, A.,
Ryoo, M.:
Learning multimodal representations for unseen activities.
In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,
pp. 517–526
(2020)


</span>
</li>
<li id="bib.bib368" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2019]</span>
<span class="ltx_bibblock">
Lin, J.,
Gan, C.,
Han, S.:
Training kinetics in 15 minutes: Large-scale distributed training on videos.
arXiv preprint arXiv:1910.00932
(2019)


</span>
</li>
<li id="bib.bib369" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Howard et al. [2019]</span>
<span class="ltx_bibblock">
Howard, A.,
Sandler, M.,
Chu, G.,
Chen, L.-C.,
Chen, B.,
Tan, M.,
Wang, W.,
Zhu, Y.,
Pang, R.,
Vasudevan, V., <span id="bib.bib369.1.1" class="ltx_text ltx_font_italic">et al.</span>:
Searching for mobilenetv3.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision,
pp. 1314–1324
(2019)


</span>
</li>
<li id="bib.bib370" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. [2021]</span>
<span class="ltx_bibblock">
Singh, A.,
Chakraborty, O.,
Varshney, A.,
Panda, R.,
Feris, R.,
Saenko, K.,
Das, A.:
Semi-supervised action recognition with temporal contrastive learning.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 10389–10399
(2021)


</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.09677" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.09678" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.09678">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.09678" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.09679" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 00:48:45 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
