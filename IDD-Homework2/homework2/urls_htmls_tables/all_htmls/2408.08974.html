<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.08974] Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE</title><meta property="og:description" content="Federated Learning (FL) has garnered significant attention in manufacturing for its robust model development and privacy-preserving capabilities. This paper contributes to research focused on the robustness of FL model…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.08974">

<!--Generated on Thu Sep  5 13:39:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated learning,  deep learning,  small-object detection,  hybrid dataset,  synthetic dataset.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="id1" class="ltx_logical-block">
<div id="id1.p1" class="ltx_para">
<p id="id1.p1.1" class="ltx_p ltx_align_center"><span id="id1.p1.1.1" class="ltx_text ltx_font_bold">© 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses,
<br class="ltx_break">in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.
<br class="ltx_break"></span> <span id="id1.p1.1.2" class="ltx_text ltx_font_bold">Submitted and Presented at the IEEE International Conference on Innovative Engineering Sciences and Technological Research (ICIESTR-2024)</span></p>
</div>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<h1 class="ltx_title ltx_title_document">Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques
<br class="ltx_break"><span id="id2.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 
<br class="ltx_break">
<br class="ltx_break">979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vinit Hegiste
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_font_italic">Chair of Machine Tools and Control Systems</span>
<br class="ltx_break"><span id="id4.2.id2" class="ltx_text ltx_font_italic">RPTU Kaierslautern-Landau
<br class="ltx_break"></span>Kaiserslautern, Germany 
<br class="ltx_break">vinit.hegiste@rptu.de
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Snehal Walunj
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_font_italic">Innovative Factory Systems (IFS)</span>
<br class="ltx_break"><span id="id6.2.id2" class="ltx_text ltx_font_italic">German Research Center for Artificial Intelligence (DFKI)
<br class="ltx_break"></span>Kaiserslautern, Germany 
<br class="ltx_break">snehal.walunj@dfki.de
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jibinraj Antony
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_font_italic">Innovative Factory Systems (IFS)</span>
<br class="ltx_break"><span id="id8.2.id2" class="ltx_text ltx_font_italic">German Research Center for Artificial Intelligence (DFKI)
<br class="ltx_break"></span>Kaiserslautern, Germany 
<br class="ltx_break">jibinraj.antony@dfki.de
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tatjana Legler
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id9.1.id1" class="ltx_text ltx_font_italic">Chair of Machine Tools and Control Systems</span>
<br class="ltx_break"><span id="id10.2.id2" class="ltx_text ltx_font_italic">RPTU Kaierslautern-Landau
<br class="ltx_break"></span>Kaiserslautern, Germany 
<br class="ltx_break">tatjana.legler@rptu.de
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Martin Ruskowski
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id11.1.id1" class="ltx_text ltx_align_center ltx_font_italic">Innovative Factory Systems (IFS)</span>
<br class="ltx_break">
<br class="ltx_break"><span id="id12.2.id2" class="ltx_text ltx_font_italic">German Research Center for Artificial Intelligence (DFKI)
<br class="ltx_break"></span>Kaiserslautern, Germany 
<br class="ltx_break">martin.ruskowski@dfki.de
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id13.id1" class="ltx_p">Federated Learning (FL) has garnered significant attention in manufacturing for its robust model development and privacy-preserving capabilities. This paper contributes to research focused on the robustness of FL models in object detection, hereby presenting a comparative study with conventional techniques using a hybrid dataset for small object detection. Our findings demonstrate the superior performance of FL over centralized training models and different deep learning techniques when tested on test data recorded in a different environment with a variety of object viewpoints, lighting conditions, cluttered backgrounds, etc. These results highlight the potential of FL in achieving robust global models that perform efficiently even in unseen environments. The study provides valuable insights for deploying resilient object detection models in manufacturing environments.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated learning, deep learning, small-object detection, hybrid dataset, synthetic dataset.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated learning (FL) has gained significance in the manufacturing domain owing to its ability to produce robust models while preserving data privacy, which is critical for companies to avoid data leaks. Recent research efforts, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, have focused on the privacy-preserving aspects, aiming to train models that match the performance of those trained using centralized datasets.
FL not only contributes to data-privacy protection but also yields robust models by addressing bias and handling repetitive data samples, as highlighted in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Additionally, federated learning techniques demonstrate the ability to generate robust models that outperform centrally trained models during testing in unseen environments, as discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Despite the evident advantages, there is limited coverage of research papers or scientific work focusing on this robust aspect of federated learning models.
Therefore, this paper aims to compare federated learning for object detection with conventional object detection techniques and other deep learning methods using a hybrid dataset specifically for small object detection.
In the realm of computer vision, particularly object detection, small objects are defined as those whose size is relatively minuscule in comparison to the complete image frame, typically less than 32*32 pixels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Detection of small objects poses challenges due to limited features, lack of spatial information, lower context in terms of background, noise clutter, occlusion, and intra-class variability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The motivation extends beyond detecting small objects to include a comprehensive comparison of the efficiency of federated learning models. The hybrid dataset strategy involves using a synthetic dataset as one client and a real dataset as another client. The hybrid dataset is also subjected to training using the conventional YOLOv5 algorithm, incorporating techniques such as transfer learning and fine-tune learning with the synthetic dataset, followed by training with the real dataset. This comprehensive training approach enables further comparison with the federated learning model.
Another algorithm explored in this study is the YOLO ensemble technique, where models trained with synthetic and real datasets are ensembled to produce the final output.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Literature Review</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The task of detecting small objects within the field of computer vision poses a significant research challenge, due to the variability in the effectiveness of detection methods depending on specific applications, the nature of the data used, and the available computational resources. Object detection is crucial for a wide range of applications, such as autonomous driving technologies, industrial automation systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, extended reality experiences, surveillance systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, and analysis of aerial imagery.
There has been a notable surge in the development and utilization of synthetic datasets within computer vision research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, driven by the cost-effectiveness of synthetic data generation methods compared to traditional data collection techniques. These synthetic datasets are increasingly being adopted across various sectors, although some applications still rely on real datasets to minimize the domain gap between training and real-world data.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Small objects, defined as those with dimensions smaller than 32*32 pixels, present unique challenges for object detection models due to their limited feature sets, lower resolution, and consequent reduction in contextual information available for learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Factors such as varying illumination conditions and the poses of objects disproportionately impact the detection of small objects compared to larger ones. Moreover, an imbalance in class distribution can significantly hinder the performance of small-object detection models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
With the evolution of deep learning methodologies, transfer learning and fine-tuning have become increasingly prevalent. Transfer learning involves updating only the final layers of a pre-trained model to adapt to new features, thereby preserving the generality of the learned features from the initial training while streamlining the adaptation process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. In contrast, fine-tuning adjusts all parameters of a pre-trained model, facilitating more extensive optimization for a new dataset by leveraging prior learned knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Both strategies offer distinct advantages in various deep learning scenarios.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Federated learning, a paradigm designed to enhance data privacy by sharing only model weights instead of raw data among clients, is emerging in the context of object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Although federated learning has been explored in general object detection scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, its application to small object detection remains under-investigated. This gap in research is what this study seeks to address, inspired by findings from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> that demonstrate federated learning’s potential to function as an ensemble technique, thereby improving model performance on distributed datasets.
In machine learning, regularization techniques are crucial for preventing overfitting, a common issue where a model learns the training data too well and fails to generalize to new data. Federated learning naturally incorporates regularization through the inherent heterogeneity and distribution of the training data across multiple devices, introducing variability and noise that can help regularize the model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Additionally, the communication constraints inherent in federated learning can further regularize the model by limiting the amount of data shared between devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. This study aims to evaluate the performance of object detection models trained using various deep learning and federated learning techniques against an unseen test dataset, leveraging the regularization benefits of federated learning.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Dataset</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The hybrid dataset includes real and synthetic images captured for both small and normal-sized objects from an assembly product, featuring: micro-Arduino, tactile buttons, resistors, LED lights, and buzzers mounted on a breadboard. The synthetic dataset is generated using CAD models in Unity3D scene, incorporating variable viewpoints, backgrounds, illuminations, camera to object distances, and object states (assembled and disassembled).
Fig. <a href="#S3.F2" title="Figure 2 ‣ III-A Dataset ‣ III Methodology ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and Fig. <a href="#S3.F2" title="Figure 2 ‣ III-A Dataset ‣ III Methodology ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Figures are cropped and enlarged for visual purpose</span></span></span> show samples from the real and synthetic datasets, respectively. The CAD models used are open source and simple, detailed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
The real dataset had a total of 3001 bounding boxes and synthetic with 2700 bounding boxes.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">For testing the robustness of the model and facilitate better comparison, a new test dataset was created. This dataset includes 116 images with varied and cluttered backgrounds, lighting conditions, and motion blur. Sixteen images contain only backgrounds to assess false positives. The sample image is shown in Figure <a href="#S3.F3" title="Figure 3 ‣ III-A Dataset ‣ III Methodology ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The remaining 100 images consist of a total of 1101 bounding boxes distributed across 5 classes.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2408.08974/assets/images/RealImage.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="228" height="125" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Image sample from the Real images dataset</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2408.08974/assets/images/SyntheticImage.png" id="S3.F2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="228" height="192" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Image sample from the Synthetic images dataset</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2408.08974/assets/images/img_114.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="228" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Test image having a different background, lighting and blur</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Algorithms</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">This paper aims to compare the performance on a hybrid dataset using various algorithmic techniques, including:</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Transfer Learning:</span> The model is initially trained with synthetic data and then applied to a real dataset.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Fine-tuning:</span> The model, initially trained with synthetic data, is further fine-tuned on a real dataset.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">YOLOv5 Ensemble Technique:</span> An ensemble model is created by combining the output of two YOLOv5 models trained with a real and synthetic dataset.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Federated Learning:</span> The dataset is divided, with one client having a synthetic dataset and the other having a real dataset, employing federated learning to achieve a global model.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p"><span id="S3.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">FedEnsemble Technique:</span> The centralized dataset is divided into three clients, and a global federated model is obtained through federated learning.</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">These techniques are compared with centralized learning YOLOv5l model using a hybrid dataset, where an equivalent number of real and synthetic images are utilized, as detailed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Implementation</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In line with the findings by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, the introduction of a synthetic dataset, even a simple one, during training significantly improves the mean Average Precision (mAP) of the model. For a small dataset of 300 real images, the optimal approach involved supplementing it with an equivalent number of synthetic data, ensuring class balancing, particularly for LEDs and Arduinos, which were present once per image.
While maintaining this dataset for training, a new test dataset was created to assess model performance in a different environment. This test dataset featured variations in backgrounds, lighting conditions, blur, and distance from the HoloLens. Annotations were applied to this new dataset, and the results obtained from all algorithms are detailed in Section <a href="#S5" title="V Results and Discussion ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>.
All algorithms utilized the YOLOv5l model architecture and pre-trained YOLOv5l model weights as the starting point. The centralized training approaches incorporated early stopping to determine the epochs for optimal model weights. Specifically, the hybrid dataset was trained on using the YOLOv5l architecture, and the best weights were achieved after approximately 200 epochs.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2408.08974/assets/images/finetunee.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="240" height="186" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Transfer learning (left) and Fine-tune model (right) are first trained on synthetic and then the knowledge is transferred or fine-tuned to real image dataset</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2408.08974/assets/images/Ensemble.png" id="S4.F5.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="240" height="189" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Model ensembling using individual Yolov5 as base models trained separately on synthetic and real data</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Hyperparameters for Training Techniques</span>
</h3>

<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS1.4.1.1" class="ltx_text">IV-A</span>1 </span>Fine-tuning</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">The model was initially trained on a synthetic dataset of 300 images, achieving optimal performance on an NVIDIA A40 GPU cluster. In a second fine-tuning stage, the model was further trained on a real image dataset of 300 images. With early stopping, the optimal weights for the synthetic dataset were reached around 200 epochs. Subsequently, an additional 200 epochs were applied for fine-tuning these weights on the real dataset, as shown in Figure <a href="#S4.F5" title="Figure 5 ‣ IV Implementation ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS2.4.1.1" class="ltx_text">IV-A</span>2 </span>Transfer Learning</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">The YOLOv5l model, fine-tuned on the synthetic dataset, underwent transfer learning. The core 10 layers of the trained model were frozen, and final layers were updated through training on the NVIDIA A40 GPU cluster using real-world scenario image data. With early stopping, the optimal weights for the synthetic dataset were reached around 200 epochs (as mentioned above). And the early stopping for transfer learning was reached around 150 epochs, as shown in Figure <a href="#S4.F5" title="Figure 5 ‣ IV Implementation ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS3.4.1.1" class="ltx_text">IV-A</span>3 </span>Model Ensembling</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">The YOLOv5l model ensembling technique involved training two models separately on synthetic and real datasets. Inference on test images was performed by ensembling the results, showcasing improvements in ensemble inference, as shown in Figure <a href="#S4.F5" title="Figure 5 ‣ IV Implementation ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
These 2 models individually are also used for comparison, as shown in Table <a href="#S5.T2" title="TABLE II ‣ V Results and Discussion ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> and <a href="#S5.T2" title="TABLE II ‣ V Results and Discussion ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<figure id="S4.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2408.08974/assets/images/fedl.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="228" height="168" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Federated learning with 2 clients with one having a real dataset and another synthetic dataset</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2408.08974/assets/images/Fed_ensemble.png" id="S4.F7.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="228" height="207" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Federated ensemble (FedEnsemble) learning with 3 clients with each having a subset of the shuffled centralized hybrid dataset</figcaption>
</figure>
</section>
<section id="S4.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS4.4.1.1" class="ltx_text">IV-A</span>4 </span>Federated Learning and FedEnsemble</h4>

<div id="S4.SS1.SSS4.p1" class="ltx_para">
<p id="S4.SS1.SSS4.p1.1" class="ltx_p">The hybrid learning dataset was partitioned into two clients for federated learning. Figure <a href="#S4.F7" title="Figure 7 ‣ IV-A3 Model Ensembling ‣ IV-A Hyperparameters for Training Techniques ‣ IV Implementation ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates this process, where the first client received real images, and the second client received the synthetic dataset. The goal is to compare the global model obtained through federated learning with a model trained on a centralized hybrid dataset.
The FedAvg algorithm,  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, was used for model aggregation. Various optimizers, including ADAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, ADAMW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, SGD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, and SGD with momentum <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, were explored.
The object detection model used was YOLOv5l for consistency across experiments. Optimal global model weights were attained using SGD with momentum, 15 local epochs, and 10 communication rounds.</p>
</div>
<div id="S4.SS1.SSS4.p2" class="ltx_para">
<p id="S4.SS1.SSS4.p2.1" class="ltx_p">For the FedEnsemble method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, the centralized hybrid dataset was divided into three client datasets, as shown in Figure <a href="#S4.F7" title="Figure 7 ‣ IV-A3 Model Ensembling ‣ IV-A Hyperparameters for Training Techniques ‣ IV Implementation ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Using YOLOv5l and a similar setup as federated learning with the FedAvg algorithm, the global model was achieved with 15 local epochs and 10 communication rounds. FedEnsemble demonstrated increased robustness compared to centrally trained models on objects in an unseen environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S4.SS1.SSS4.p3" class="ltx_para">
<p id="S4.SS1.SSS4.p3.1" class="ltx_p">The best models were selected after extensive epochs and communication rounds combinations, and were tested on 100 images from the same dataset distribution as the training dataset and an additional 116 test images, as mentioned in Subsection <a href="#S3.SS1" title="III-A Dataset ‣ III Methodology ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>. Detailed results are presented in Section <a href="#S5" title="V Results and Discussion ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. All architectures had the same optimizer, SGD with momentum=0.937, batch size=8, IoU training threshold=0.20 image size=1080, weight decay=0.0005, lr=0.01 etc. as mentioned Yolov5 default training setting.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Results and Discussion</span>
</h2>

<figure id="S5.T2" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Algorithm</th>
<th id="S5.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">AP</th>
<th id="S5.T2.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">AP50</th>
<th id="S5.T2.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">AP75</th>
<th id="S5.T2.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">APsmall</th>
<th id="S5.T2.1.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">APmedium</th>
<th id="S5.T2.1.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">APlarge</th>
<th id="S5.T2.1.1.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">mAP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.2.1" class="ltx_tr">
<td id="S5.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt">FedEnsemble model</td>
<td id="S5.T2.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">0.6530</td>
<td id="S5.T2.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">0.9718</td>
<td id="S5.T2.1.2.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">0.7643</td>
<td id="S5.T2.1.2.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">0.6212</td>
<td id="S5.T2.1.2.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">0.6349</td>
<td id="S5.T2.1.2.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">0.4101</td>
<td id="S5.T2.1.2.1.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">0.9749</td>
</tr>
<tr id="S5.T2.1.3.2" class="ltx_tr">
<td id="S5.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Federated learning</td>
<td id="S5.T2.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.5723</td>
<td id="S5.T2.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.9586</td>
<td id="S5.T2.1.3.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.5825</td>
<td id="S5.T2.1.3.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.5011</td>
<td id="S5.T2.1.3.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.5702</td>
<td id="S5.T2.1.3.2.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3945</td>
<td id="S5.T2.1.3.2.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.9660</td>
</tr>
<tr id="S5.T2.1.4.3" class="ltx_tr">
<td id="S5.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Transferlearning</td>
<td id="S5.T2.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.6635</td>
<td id="S5.T2.1.4.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.9725</td>
<td id="S5.T2.1.4.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.7763</td>
<td id="S5.T2.1.4.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.6092</td>
<td id="S5.T2.1.4.3.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.6453</td>
<td id="S5.T2.1.4.3.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.4142</td>
<td id="S5.T2.1.4.3.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.9755</td>
</tr>
<tr id="S5.T2.1.5.4" class="ltx_tr">
<td id="S5.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Fine-tune model</td>
<td id="S5.T2.1.5.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.1.5.4.2.1" class="ltx_text ltx_font_bold">0.6680</span></td>
<td id="S5.T2.1.5.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.1.5.4.3.1" class="ltx_text ltx_font_bold">0.9743</span></td>
<td id="S5.T2.1.5.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.7124</td>
<td id="S5.T2.1.5.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.1.5.4.5.1" class="ltx_text ltx_font_bold">0.6671</span></td>
<td id="S5.T2.1.5.4.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.1.5.4.6.1" class="ltx_text ltx_font_bold">0.6561</span></td>
<td id="S5.T2.1.5.4.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.1.5.4.7.1" class="ltx_text ltx_font_bold">0.4247</span></td>
<td id="S5.T2.1.5.4.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.9774</td>
</tr>
<tr id="S5.T2.1.6.5" class="ltx_tr">
<td id="S5.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Hybrid Centralized learning</td>
<td id="S5.T2.1.6.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.6643</td>
<td id="S5.T2.1.6.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.9702</td>
<td id="S5.T2.1.6.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.7455</td>
<td id="S5.T2.1.6.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.6318</td>
<td id="S5.T2.1.6.5.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.6399</td>
<td id="S5.T2.1.6.5.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.4240</td>
<td id="S5.T2.1.6.5.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.1.6.5.8.1" class="ltx_text ltx_font_bold">0.9783</span></td>
</tr>
<tr id="S5.T2.1.7.6" class="ltx_tr">
<td id="S5.T2.1.7.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Real Centralized learning</td>
<td id="S5.T2.1.7.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.6676</td>
<td id="S5.T2.1.7.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.9663</td>
<td id="S5.T2.1.7.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.1.7.6.4.1" class="ltx_text ltx_font_bold">0.7820</span></td>
<td id="S5.T2.1.7.6.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.6598</td>
<td id="S5.T2.1.7.6.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.6472</td>
<td id="S5.T2.1.7.6.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.4107</td>
<td id="S5.T2.1.7.6.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.9743</td>
</tr>
<tr id="S5.T2.1.8.7" class="ltx_tr">
<td id="S5.T2.1.8.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Synthetic Centralized learning</td>
<td id="S5.T2.1.8.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.1509</td>
<td id="S5.T2.1.8.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3595</td>
<td id="S5.T2.1.8.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.1025</td>
<td id="S5.T2.1.8.7.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.0902</td>
<td id="S5.T2.1.8.7.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.1890</td>
<td id="S5.T2.1.8.7.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.0946</td>
<td id="S5.T2.1.8.7.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3663</td>
</tr>
<tr id="S5.T2.1.9.8" class="ltx_tr">
<td id="S5.T2.1.9.8.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">YOLO Ensemble</td>
<td id="S5.T2.1.9.8.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.5314</td>
<td id="S5.T2.1.9.8.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.9171</td>
<td id="S5.T2.1.9.8.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.5470</td>
<td id="S5.T2.1.9.8.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.5731</td>
<td id="S5.T2.1.9.8.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.5232</td>
<td id="S5.T2.1.9.8.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.3036</td>
<td id="S5.T2.1.9.8.8" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.9188</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison of all models based on COCO metrics and mAP on test dataset which was captured in the same environment as the training dataset</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S5.T2.2" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.2.1.1" class="ltx_tr">
<th id="S5.T2.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Algorithm</th>
<th id="S5.T2.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">AP</th>
<th id="S5.T2.2.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">AP50</th>
<th id="S5.T2.2.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">AP75</th>
<th id="S5.T2.2.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">APsmall</th>
<th id="S5.T2.2.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">APmedium</th>
<th id="S5.T2.2.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">APlarge</th>
<th id="S5.T2.2.1.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">mAP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.2.2.1" class="ltx_tr">
<td id="S5.T2.2.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt">FedEnsemble model</td>
<td id="S5.T2.2.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">0.4375</td>
<td id="S5.T2.2.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">0.8193</td>
<td id="S5.T2.2.2.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">0.4108</td>
<td id="S5.T2.2.2.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">0.2882</td>
<td id="S5.T2.2.2.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">0.4188</td>
<td id="S5.T2.2.2.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">0.7480</td>
<td id="S5.T2.2.2.1.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">0.8212</td>
</tr>
<tr id="S5.T2.2.3.2" class="ltx_tr">
<td id="S5.T2.2.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Federated learning</td>
<td id="S5.T2.2.3.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.2.3.2.2.1" class="ltx_text ltx_font_bold">0.4807</span></td>
<td id="S5.T2.2.3.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.2.3.2.3.1" class="ltx_text ltx_font_bold">0.8600</span></td>
<td id="S5.T2.2.3.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.2.3.2.4.1" class="ltx_text ltx_font_bold">0.4729</span></td>
<td id="S5.T2.2.3.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.2.3.2.5.1" class="ltx_text ltx_font_bold">0.2969</span></td>
<td id="S5.T2.2.3.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.2.3.2.6.1" class="ltx_text ltx_font_bold">0.4555</span></td>
<td id="S5.T2.2.3.2.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.7580</td>
<td id="S5.T2.2.3.2.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.2.3.2.8.1" class="ltx_text ltx_font_bold">0.8638</span></td>
</tr>
<tr id="S5.T2.2.4.3" class="ltx_tr">
<td id="S5.T2.2.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Transferlearning</td>
<td id="S5.T2.2.4.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3812</td>
<td id="S5.T2.2.4.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.7544</td>
<td id="S5.T2.2.4.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3417</td>
<td id="S5.T2.2.4.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.2373</td>
<td id="S5.T2.2.4.3.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3272</td>
<td id="S5.T2.2.4.3.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.7245</td>
<td id="S5.T2.2.4.3.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.7681</td>
</tr>
<tr id="S5.T2.2.5.4" class="ltx_tr">
<td id="S5.T2.2.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Fine-tune model</td>
<td id="S5.T2.2.5.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.4239</td>
<td id="S5.T2.2.5.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.7726</td>
<td id="S5.T2.2.5.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.4142</td>
<td id="S5.T2.2.5.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.2912</td>
<td id="S5.T2.2.5.4.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3606</td>
<td id="S5.T2.2.5.4.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.2.5.4.7.1" class="ltx_text ltx_font_bold">0.7837</span></td>
<td id="S5.T2.2.5.4.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.7856</td>
</tr>
<tr id="S5.T2.2.6.5" class="ltx_tr">
<td id="S5.T2.2.6.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Hybrid Centralized learning</td>
<td id="S5.T2.2.6.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.4432</td>
<td id="S5.T2.2.6.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.7776</td>
<td id="S5.T2.2.6.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.4726</td>
<td id="S5.T2.2.6.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.2843</td>
<td id="S5.T2.2.6.5.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3801</td>
<td id="S5.T2.2.6.5.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.7531</td>
<td id="S5.T2.2.6.5.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.7865</td>
</tr>
<tr id="S5.T2.2.7.6" class="ltx_tr">
<td id="S5.T2.2.7.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Real Centralized learning</td>
<td id="S5.T2.2.7.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3955</td>
<td id="S5.T2.2.7.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.7481</td>
<td id="S5.T2.2.7.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.4041</td>
<td id="S5.T2.2.7.6.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.2556</td>
<td id="S5.T2.2.7.6.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3559</td>
<td id="S5.T2.2.7.6.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.7322</td>
<td id="S5.T2.2.7.6.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.7638</td>
</tr>
<tr id="S5.T2.2.8.7" class="ltx_tr">
<td id="S5.T2.2.8.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Synthetic Centralized learning</td>
<td id="S5.T2.2.8.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.2096</td>
<td id="S5.T2.2.8.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.4080</td>
<td id="S5.T2.2.8.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.1841</td>
<td id="S5.T2.2.8.7.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.0875</td>
<td id="S5.T2.2.8.7.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.2445</td>
<td id="S5.T2.2.8.7.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3828</td>
<td id="S5.T2.2.8.7.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.4066</td>
</tr>
<tr id="S5.T2.2.9.8" class="ltx_tr">
<td id="S5.T2.2.9.8.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">YOLO Ensemble</td>
<td id="S5.T2.2.9.8.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.3693</td>
<td id="S5.T2.2.9.8.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.7566</td>
<td id="S5.T2.2.9.8.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.3230</td>
<td id="S5.T2.2.9.8.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.2709</td>
<td id="S5.T2.2.9.8.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.3451</td>
<td id="S5.T2.2.9.8.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.6422</td>
<td id="S5.T2.2.9.8.8" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.7664</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Comparison of all models based on COCO metrics and mAP on test dataset which was captured in an entirely different environment than the training dataset</figcaption>
</figure>
<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section showcases a comparison of results from all the algorithms on two test datasets. The first dataset (referred to as Testset1) is similar to the background and setup shown in image <a href="#S3.F2" title="Figure 2 ‣ III-A Dataset ‣ III Methodology ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, and the second dataset (referred to as Testset2) features unseen environment parameters, as shown in figure <a href="#S3.F3" title="Figure 3 ‣ III-A Dataset ‣ III Methodology ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
Table <a href="#S5.T2" title="TABLE II ‣ V Results and Discussion ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> presents results based on Testset1, while Table <a href="#S5.T2" title="TABLE II ‣ V Results and Discussion ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> presents results based on Testset2 in COCO metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, which includes IoU-Aware (Intersection over Union) and object size-relevant metrics. The last column denotes mAP (mean Average Precision) at an IoU threshold of 0.5 in terms of PASCAL metrics, which is a single IoU threshold metric. Tables <a href="#S5.T2" title="TABLE II ‣ V Results and Discussion ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> and <a href="#S5.T2" title="TABLE II ‣ V Results and Discussion ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> include two additional tests beyond those mentioned in section <a href="#S4" title="IV Implementation ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.
The ’Real Centralized learning’ test uses only the 300 real images dataset to train the YOLOv5 model for 200 epochs, and the resulting best model weights are used to test both Testset1 and Testset2. Similarly, the ’Synthetic Centralized learning’ model is trained using only the 300 synthetic images from the hybrid dataset and is tested on both datasets.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Starting with Table <a href="#S5.T2" title="TABLE II ‣ V Results and Discussion ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we observe that most models achieve an mAP of more than 95%, except for the model trained using only the synthetic dataset and the YOLOv5 ensemble model (which uses this synthetic dataset model as one of its two models). These two models exhibit a lower mAP, and the average precision for small objects is also lower. The model trained solely on the real dataset of 300 images performs well in all AP metrics.
Federated learning and FedEnsemble learning techniques perform on par with the centrally trained model but do not achieve better results. Transfer learning, fine-tune learning, and the model trained directly on the hybrid dataset also perform well on the test dataset, achieving APsmall greater than 60% and mAP above 97%. The fine-tune model stands out among all the algorithms, performing well in terms of all AP metrics. However, it’s important to note that the test images are from the same sample set distribution as the training dataset (500 images were captured, of which 300 were used for training, 100 for validation, and 100 for testing, as shown in figure <a href="#S3.F2" title="Figure 2 ‣ III-A Dataset ‣ III Methodology ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Hence, it is yet to be confirmed if the models really perform well or are overfitted to the training dataset samples.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">To test the real accuracy of the models, all the models were tested on a newly created test dataset (as mentioned in section <a href="#S3.SS1" title="III-A Dataset ‣ III Methodology ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>, figure <a href="#S3.F3" title="Figure 3 ‣ III-A Dataset ‣ III Methodology ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
Table <a href="#S5.T2" title="TABLE II ‣ V Results and Discussion ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> reveals that the ’Real Centralized learning’ model and the transfer learning model achieve an mAP of around 76%, with APsmall at 25% and 23%, respectively. The fine-tuned model and hybrid dataset model only slightly outperform them, achieving around 75% mAP and 28% APsmall metrics. This strongly suggests that the centrally trained dataset models tend to overfit to the training dataset, especially the model trained only using 300 real images. The synthetic dataset model underperforms here, as does the YOLOv5 ensemble model.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">The results for federated learning from Table <a href="#S5.T2" title="TABLE II ‣ V Results and Discussion ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> show that federated learning used to create a hybrid model by training two clients, each having only real and synthetic datasets respectively, produced the best results. The results are followed by the FedEnsemble learning, which also succeeded in producing a robust model that performs well on unseen environment datasets by using the same dataset used for centralized training but divided into different subsets as clients’ datasets.
The results of the global federated model are 8% better in terms of PASCAL metrics mAP (mean Average Precision) when compared to the baseline centrally trained model. In terms of COCO metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, federated learning outperforms the baseline model in all AP (Average Precision) types, as seen in the Table. Federated learning is seen to be performing better than all the other algorithms, followed by the FedEnsemble technique, which also performs better on unseen test data within a different environment (As shown in Figure <a href="#A1.F13" title="Figure 13 ‣ Appendix A Output of Models on Test Dataset ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> and <a href="#A1.F17" title="Figure 17 ‣ Appendix A Output of Models on Test Dataset ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a> for centralized and FedEnsemble model respectively).</p>
</div>
<figure id="S5.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2408.08974/assets/images/Hybrid_crop.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="180" height="111" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Output of <span id="S5.F9.2.1" class="ltx_text ltx_font_bold">YOLOv5l model trained using a Hybrid centralized dataset</span> on test dataset with unseen environment</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2408.08974/assets/images/FedEnss_crop.png" id="S5.F9.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="180" height="110" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Output of <span id="S5.F9.4.1" class="ltx_text ltx_font_bold">Global FedEnsemble model</span> on test dataset with unseen environment</figcaption>
</figure>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">The idea of testing models trained with only synthetic or real datasets is to showcase that clients’ models trained with local datasets performed subpar when similar objects were placed in a new unseen environment condition. However, using similar datasets with the help of federated learning, clients can achieve a robust global federated model without sharing any raw private data (Refer Appendix for further details).</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In conclusion, this paper conducted a comparative analysis between federated learning and centralized models trained on a hybrid dataset. The centralized models, trained with a limited dataset of 300 images and 300 synthetic images generated with class balancing, exhibited signs of overfitting. While they demonstrated strong performance on images from a similar distribution as the training dataset, their efficacy declined significantly when confronted with a test dataset featuring novel environmental parameters, such as distinct backgrounds, lighting conditions, blur, and camera angles.
Contrastingly, the federated learning global models for object detection showcased robust performance on this diverse test dataset. The results underscore the resilience of the federated global model and the FedEnsemble model, both of which demonstrated superior performance and did not exhibit the false positives observed in centralized models when tested on the challenging dataset. This suggests the potential of federated learning for enhancing model generalization across varying environmental conditions.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">The main outcome of these results also portray that Federated learning and FedEnsemble act as regularization due to communication constraints and data heterogeneity, producing a robust model that does not overfit to a single dataset, unlike the models trained with a centralized dataset approach in the case of small datasets.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Future Work</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Use of federated learning should be promoted for a robust model creation for machine learning use cases.
Another perspective is that FedEnsemble can be used as an ensemble technique even if a centralized dataset is available.
A proper synthetic dataset can further benefit your machine learning model in manufacturing scenarios where the dataset samples are limited.
This work can be further extended by experimenting with different manufacturing use cases and generic datasets where datasets are small with repetitive samples and comparing the federated learning models with centralized trained models.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
K. Dasaradharami Reddy and A. S, “Security and privacy in federated
learning: A survey,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Trends in Computer Science and Information
Technology</em>, vol. 8, no. 2, pp. 029–037, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
L. Lyu, H. Yu, and Q. Yang, “Threats to federated learning: A survey.”
[Online]. Available: http://arxiv.org/pdf/2003.02133v1

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
R. Xu, N. Baracaldo, Y. Zhou, A. Anwar, S. Kadhe, and H. Ludwig, “Detrust-fl:
Privacy-preserving federated learning in decentralized trust setting.”
[Online]. Available: http://arxiv.org/pdf/2207.07779.pdf

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M. Arazzi, M. Conti, A. Nocera, and S. Picek, “Turning privacy-preserving
mechanisms against federated learning,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 ACM
SIGSAC Conference on Computer and Communications Security</em>, ser. CCS
’23.   New York, NY, USA: Association
for Computing Machinery, 2023, p. 1482–1495.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
V. Hegiste, T. Legler, and M. Ruskowski, “Application of federated machine
learning in manufacturing,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">2022 International Conference on
Industry 4.0 Technology (I4Tech)</em>, 2022, pp. 1–8.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
V. Hegiste, T. Legler, K. Fridman, and M. Ruskowski, “Federated object
detection for quality inspection in shared production,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">2023 Eighth
International Conference on Fog and Mobile Edge Computing (FMEC)</em>, 2023, pp.
151–158.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S. Tayebi Arasteh, C. Kuhl, M.-J. Saehn, P. Isfort, D. Truhn, and S. Nebelung,
“Enhancing domain generalization in the ai-based analysis of chest
radiographs with federated learning,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Scientific Reports</em>, vol. 13,
no. 1, Dec. 2023. [Online]. Available:
http://dx.doi.org/10.1038/s41598-023-49956-8

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
V. Hegiste, T. Legler, and M. Ruskowski, “Federated ensemble yolov5 – a
better generalized object detection algorithm,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">2023 Eighth
International Conference on Fog and Mobile Edge Computing (FMEC)</em>, 2023, pp.
7–14.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Antony, V. Hegiste, A. Nazeri, H. Tavakoli, S. Walunj, C. Plociennik, and
M. Ruskowski, “Enhancing object detection performance for small objects
through synthetic data generation and proportional class-balancing technique:
A comparative study in industrial scenarios,” 2024.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S.-A. Precup, S. Walunj, A. Gellert, C. Plociennik, J. Antony, C.-B.
Zamfirescu, and M. Ruskowski, “Recognising worker intentions by assembly
step prediction,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">2023 IEEE 28th International Conference on
Emerging Technologies and Factory Automation (ETFA)</em>.   IEEE, 2023, pp. 1–8.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
L. Yundong, D. Han, L. Hongguang, X. Zhang, B. Zhang, and X. Zhifeng,
“Multi-block ssd based on small object detection for uav railway scene
surveillance,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Chinese Journal of Aeronautics</em>, vol. 33, no. 6, pp.
1747–1755, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Borkman, A. Crespi, S. Dhakad, S. Ganguly, J. Hogins, Y.-C. Jhang,
M. Kamalzadeh, B. Li, S. Leal, P. Parisi <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Unity perception:
Generate synthetic data for computer vision,” <em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2107.04259</em>, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
E. Acar and W. Konen, “Federated learning with dynamic regularization,” in
<em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">OpenReview.org</em>, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
R. Leonardi, F. Ragusa, A. Furnari, and G. M. Farinella, “Exploiting
multimodal synthetic data for egocentric human-object interaction detection
in an industrial scenario,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.12152</em>, 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Y. Liu, P. Sun, N. Wergeles, and Y. Shang, “A survey and performance
evaluation of deep learning methods for small object detection,”
<em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Expert Systems with Applications</em>, vol. 172, p. 114602, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu, “A survey on deep
transfer learning,” 2018.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
X. Yin, W. Chen, X. Wu, and H. Yue, “Fine-tuning and visualization of
convolutional neural networks,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">2017 12th IEEE Conference on
Industrial Electronics and Applications (ICIEA)</em>, 2017, pp. 1310–1315.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence and statistics</em>.   PMLR, 2017, pp. 1273–1282.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
P. Yu and Y. Liu, “Federated object detection: Optimizing object detection
model with federated learning,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 3rd
International Conference on Vision, Image and Signal Processing</em>, 2019, pp.
1–6.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Y. Liu, A. Huang, Y. Luo, H. Huang, Y. Liu, Y. Chen, L. Feng, T. Chen, H. Yu,
and Q. Yang, “Fedvision: An online visual object detection platform powered
by federated learning,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on
artificial intelligence</em>, vol. 34, no. 08, 2020, pp. 13 172–13 179.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J. Zhang, J. Zhou, J. Guo, and X. Sun, “Visual object detection for
privacy-preserving federated learning,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 11, pp.
33 324–33 335, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
C. C. Aggarwal, <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Data mining: patterns and techniques</em>.   Springer, 2015.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
T. Hastie, R. Tibshirani, and J. H. Friedman, <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">The elements of statistical
learning (2nd ed.)</em>.   Springer, 2009.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
H. Li and M. Jaggi, “Fedprox: Towards communication-efficient federated
learning with adaptive gradient clipping,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2004.04900</em>, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Y. Wang and H. Su, “Federated learning with intermediate representation
regularization,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.15827</em>, 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
<em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.6980</em>, 2014.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
I. Loshchilov and F. Hutter, “Fixing weight decay regularization in adam,”
<em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.05101</em>, 2017.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations
by back-propagating errors,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Nature</em>, vol. 323, no. 6088, pp.
533–536, 1986.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
N. Qian, “A gentle introduction to optimization,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Mathematical
Programming</em>, vol. 106, no. 1, pp. 1–35, 2006.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
L. Wood and F. Chollet, “Efficient graph-friendly coco metric computation for
train-time model evaluation,” 2022.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Output of Models on Test Dataset</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">The output results from all the models on unseen test images can be referred to below. In Figures <a href="#A1.F13" title="Figure 13 ‣ Appendix A Output of Models on Test Dataset ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, <a href="#A1.F13" title="Figure 13 ‣ Appendix A Output of Models on Test Dataset ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, <a href="#A1.F13" title="Figure 13 ‣ Appendix A Output of Models on Test Dataset ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, <a href="#A1.F13" title="Figure 13 ‣ Appendix A Output of Models on Test Dataset ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, <a href="#A1.F17" title="Figure 17 ‣ Appendix A Output of Models on Test Dataset ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>, <a href="#A1.F17" title="Figure 17 ‣ Appendix A Output of Models on Test Dataset ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>, <a href="#A1.F17" title="Figure 17 ‣ Appendix A Output of Models on Test Dataset ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>, and <a href="#A1.F17" title="Figure 17 ‣ Appendix A Output of Models on Test Dataset ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>, the left image is a test image consisting of all the objects in a different environment and lighting condition. We can see that the best-performing model outputs on these images are both the federated learning models.
Moving to the image on the right of all these figures, it is just a background image with no target objects in it, to test the false positives produced by the models. However, this background consists of a holed grill, which also looks very similar to a button or buzzer, and hence there are false positives in the output of all the models except for federated learning models (Figure <a href="#A1.F17" title="Figure 17 ‣ Appendix A Output of Models on Test Dataset ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a> and <a href="#A1.F17" title="Figure 17 ‣ Appendix A Output of Models on Test Dataset ‣ Enhancing Object Detection with Hybrid dataset in Manufacturing Environments: Comparing Federated Learning to Conventional Techniques This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009). 979-8-3503-4863-7/ 24/ $31.00 ©2024 IEEE" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>).</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p">This proves that federated learning models are robust in variable testing environments, specially performs better than traditional techniques when the testing environments such as lighting conditions, blur, background, camera angle and distance from the target objects, etc.are not similar to the training image environments.</p>
</div>
<figure id="A1.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2408.08974/assets/images/Hybrid.png" id="A1.F13.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="568" height="156" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Output of <span id="A1.F13.2.1" class="ltx_text ltx_font_bold">YOLOv5l model trained using a Hybrid centralized dataset</span> on test dataset with unseen environment (left) and on image with just background (right)</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2408.08974/assets/images/Transfer.png" id="A1.F13.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="568" height="156" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Output of<span id="A1.F13.4.1" class="ltx_text ltx_font_bold"> Transfer learning model</span> on test dataset with unseen environment (left) and on image with just background (right)</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2408.08974/assets/images/finetuned.png" id="A1.F13.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="568" height="155" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Output of <span id="A1.F13.6.1" class="ltx_text ltx_font_bold">Fine-tuned model</span> on test dataset with unseen environment (left) and on image with just background (right)</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2408.08974/assets/images/Real.png" id="A1.F13.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="568" height="156" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Output of <span id="A1.F13.8.1" class="ltx_text ltx_font_bold">YOLOv5l model trained using only the real images</span> on test dataset with unseen environment (left) and on image with just background (right)</figcaption>
</figure>
<figure id="A1.F17" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2408.08974/assets/images/synthetic.png" id="A1.F17.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="568" height="156" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Output of <span id="A1.F17.2.1" class="ltx_text ltx_font_bold">YOLOv5l model trained using only the synthetic dataset</span> on test dataset with unseen environment (left) and on image with just background (right)</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2408.08974/assets/images/YoloEns.png" id="A1.F17.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="568" height="156" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Output of <span id="A1.F17.4.1" class="ltx_text ltx_font_bold">YOLOv5l real and synthetic ensemble model</span> on test dataset with unseen environment (left) and on image with just background (right)</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2408.08974/assets/images/Fedlearn.png" id="A1.F17.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="568" height="156" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Output of <span id="A1.F17.6.1" class="ltx_text ltx_font_bold">Global Federated model</span> on test dataset with unseen environment (left) and on image with just background (right)</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2408.08974/assets/images/FedEns.png" id="A1.F17.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="568" height="156" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Output of <span id="A1.F17.8.1" class="ltx_text ltx_font_bold">Global FedEnsemble model</span> on test dataset with unseen environment (left) and on image with just background (right)</figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.08973" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.08974" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.08974">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.08974" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.08975" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 13:39:31 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
