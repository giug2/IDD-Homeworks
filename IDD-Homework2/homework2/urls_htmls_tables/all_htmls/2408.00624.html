<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.00624] SynesLM: A Unified Approach for Audio-visual Speech Recognition and Translation via Language Model and Synthetic Data* Equal contributions</title><meta property="og:description" content="In this work, we present SynesLM, an unified model which can perform three multimodal language understanding tasks: audio-visual automatic speech recognition(AV-ASR) and visual-aided speech/machine translation(VST/VMT)…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SynesLM: A Unified Approach for Audio-visual Speech Recognition and Translation via Language Model and Synthetic Data* Equal contributions">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SynesLM: A Unified Approach for Audio-visual Speech Recognition and Translation via Language Model and Synthetic Data* Equal contributions">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.00624">

<!--Generated on Thu Sep  5 15:32:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=1*]YichenLu
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=1*]JiaqiSong
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=1]XuankaiChang
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>[affiliation=1]HengweiBian
<span id="p1.3.4" class="ltx_ERROR undefined">\name</span>[affiliation=1]SoumiMaiti
<span id="p1.3.5" class="ltx_ERROR undefined">\name</span>[affiliation=1]ShinjiWatanabe</p>
</div>
<h1 class="ltx_title ltx_title_document">SynesLM: A Unified Approach for Audio-visual Speech Recognition and Translation via Language Model and Synthetic Data<span id="id2.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>* Equal contributions</span></span></span>
</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">In this work, we present <span id="id4.id1.1" class="ltx_text ltx_font_italic">SynesLM</span>, an unified model which can perform three multimodal language understanding tasks: audio-visual automatic speech recognition(AV-ASR) and visual-aided speech/machine translation(VST/VMT). Unlike previous research that focused on lip motion as visual cues for speech signals, our work explores more general visual information within entire frames, such as objects and actions. Additionally, we use synthetic image data to enhance the correlation between image and speech data. We benchmark SynesLM against the How2 dataset, demonstrating performance on par with state-of-the-art (SOTA) models dedicated to AV-ASR while maintaining our multitasking framework. Remarkably, for zero-shot AV-ASR, SynesLM achieved SOTA performance by lowering the Word Error Rate (WER) from 43.4% to 39.4% on the VisSpeech Dataset. Furthermore, our results in VST and VMT outperform the previous results, improving the BLEU score to 43.5 from 37.2 for VST, and to 54.8 from 54.4 for VMT.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Audio-Visual Automatic Speech Recognition, Speech Translation, Multimodal Language Model, Multitask
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text ltx_font_italic">Synesthesia</span> is a neurological condition where stimulation of one sensory pathway involuntarily triggers experiences in another, such as perceiving colors when hearing sounds. This phenomenon highlights the complex integration of multisensory inputs in human cognition, essential for comprehending the world through combined audio and visual stimuli <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Visual cues, for instance, enhance speech recognition and language understanding, aiding in translation tasks.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Motivated by these insights, we aim to design a unified model for a range of audio-visual tasks that concurrently use audio and visual inputs. Similar to human learning processes, our model benefits from multitask training. Additionally, we found that incorporating pretrained language model weights further improves our model's performance.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2408.00624/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="207" height="100" alt="Refer to caption">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.2.1" class="ltx_text ltx_font_bold">An overview of SynesLM architecture.</span> The definition of the special tokens will be discussed at the end of the Section <a href="#S3" title="3 Method ‣ SynesLM: A Unified Approach for Audio-visual Speech Recognition and Translation via Language Model and Synthetic Data* Equal contributions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Recently, there are several works trying to incorporate visual information with speech to perform automatic speech recognition (ASR). For instance, AV-HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> focuses on audio-visual speech representation, utilizing video lip recordings to learn powerful speech representations. AVATAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> extends this concept by incorporating full visual frames for unconstrained audio-visual automatic speech recognition (AV-ASR). The AVFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> explores injecting vision into frozen speech models for zero-shot AV-ASR, highlighting the potential of lightweight domain adaptation. However, existing research primarily focuses on AV-ASR, with a noticeable shortage of studies investigating a unified approach for all audio-visual language understanding tasks, such as visual-aided speech translation.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">On the other hand, large language models(LLMs) have surged in popularity due to their advanced capabilities in natural language understanding and generation. Several multimodal language models have been developed for more complex tasks necessitating either visual or audio modalities. LLaVa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and BLIP2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> corporate visual modality for visual question answering and image captioning. VoxtLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and VioLA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> utilize speech modality to perform multiple speech tasks, such as speech recognition and text-to-speech. Additionally, works like OneLLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and X-LLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> extend the input set to various modalities, including audio and visual. However, none of these models can concurrently process audio and visual inputs, meaning they cannot perform tasks like AV-ASR.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Since current multimodal language understanding approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> are limited to handling AV-ASR, and existing MLLMs can only process tasks involving text plus one additional modality, this paper introduces the <span id="S1.p5.1.1" class="ltx_text ltx_font_bold">Synes</span>thesia <span id="S1.p5.1.2" class="ltx_text ltx_font_bold">L</span>anguage <span id="S1.p5.1.3" class="ltx_text ltx_font_bold">M</span>odel (SynesLM).
SynesLM is a novel, unified approach capable of performing a variety of audio-visual input tasks within a single model. Drawing inspiration from the phenomenon of synesthesia and leveraging advancements in recent language models, SynesLM is meticulously designed to proficiently handle complex language tasks such as audio-visual automatic speech recognition (AV-ASR), visual speech translation (VST), and visual machine translation (VMT). Additionaly, we proposed a data recovery pipeline using LLM and image generative model for multimodal language understanding dataset like How2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to enhanced its multimodal interaction between speech and visual modality.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We make the following contributions in this work:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">A unified model for multiple audio-visual tasks:</span> We explore a novel framework capable of comprehending and processing visual, speech, and textual data to perform various of multimodal language understanding tasks.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Synthetic visual data recovery pipeline:</span> To address the poor quality of visual data in multimodal speech datasets like How2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, we developed a novel data recovery pipeline. This pipeline significantly improved the integration between visual and speech data.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Performance:</span> We achieved improved performance across all tasks, recording a 4.0% WER absolute improvement in zero-shot AV-ASR and BLEU scores of 43.5 for VST and 54.8 for VMT, showcasing our model's strong audio processing and visual comprehension capabilities (Table <a href="#S4.T3" title="Table 3 ‣ 4.2 Results ‣ 4 Experiments ‣ SynesLM: A Unified Approach for Audio-visual Speech Recognition and Translation via Language Model and Synthetic Data* Equal contributions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p"><span id="S1.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Reproducibility:</span> For reproducibility, we open-source our code and model checkpoints in the form of ESPNet recipe in <a target="_blank" href="https://github.com/espnet/espnet" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/espnet/espnet</a>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Multimodal Language Models (MLMs).</span> Recently, numerous MLMs have been proposed for various modalities. Advances in Visual Language Models (VLMs), such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, have significantly improved the integration of visual information into pre-trained language models. These models use a pre-trained vision encoder for visual feature extraction, excelling in tasks like image captioning and visual question answering. Similarly, recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> have begun exploring unified models for various speech and text tasks, employing self-supervised learning (SSL) feature extractors as audio encoders alongside pre-trained language models to enhance language comprehension. Some recent research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> has attempted to incorporate both visual and audio modalities into language models. However, these models can only process one specific modality with text and do not explore the interaction between audio and visual modalities. The most related work to ours is Video-SALMONN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, but it lacks multilingual capabilities, limiting its ability to perform translation tasks.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">AV-ASR Methods.</span> Several methods have been designed for AV-ASR tasks. AV-HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> uses video recordings to learn robust speech representations through masked multimodal cluster prediction, focusing on lip motion. AVATAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> employs a multimodal encoder with a transformer decoder for natural language speech recognition output. AVFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> explores injecting vision into frozen speech models for zero-shot AVSR, showing the potential for lightweight domain adaptation. Additionally, prompting-whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> designs a cascade model that injects visual prompts for AV-ASR. However, these methods are limited to AV-ASR tasks or utilize cascade structures, while we design an end-to-end approach for multiple speech-visual tasks.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2408.00624/assets/figures/pipeline.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="269" height="252" alt="Refer to caption">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S3.F2.2.1" class="ltx_text ltx_font_bold">Synthetic Data Recovery Pipeline.</span> </figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2408.00624/assets/x2.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="428" height="122" alt="Refer to caption">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S3.F3.2.1" class="ltx_text ltx_font_bold">Qualitative examples on How2 ASR.</span> We show that our audio with original visual (A+OV) and audio with synthetic visual (A+SV) method successfully extract and understand the information from the image and corporate the information with speech representation to perform ASR task.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section initially outlines the architecture of our model, followed by a description of how we recover the multimodal data and tokenize it.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Representation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">Discrete Speech Representation.</span> Recent advances in discrete speech representations for ASR and ST have improved training speed, inference speed, and storage efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. These methods utilize SSL's ability to capture linguistic and acoustic information, surpassing previous techniques like log-Mel filterbanks. By converting continuous audio features into discrete speech tokens, speech input can be handled similarly to text tokens. This unification allows speech and text tokens, sharing similar semantics, to be processed together in a single LM using a unified character set called discrete speech-text tokens.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Visual Encoders and Features.</span> For the visual modality, we randomly select a single frame from each video clip as the visual input, keeping the image whole rather than dividing it into patches. This simplifies the extraction of object and action information, ensuring one frame provides sufficient details. Using a pre-trained visual encoder from CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, we extract features from the entire image. To bridge the visual and language modalities, a Vision-Language connector layer (a Multi-Layer Perceptron) maps these visual features into the same embedding space as the discrete audio-text tokens, aligning them with the linguistic components. We also experiment with other CLIP-like pretrained visual encoders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> to explore their performance in fusing discrete text-speech representations.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Data Format.</span> We use several special tokens to indicate different tasks and different modality. The entire sequence start with a <span id="S3.SS1.p3.1.2" class="ltx_text ltx_font_typewriter">&lt;SOS&gt;</span> token, end with a <span id="S3.SS1.p3.1.3" class="ltx_text ltx_font_typewriter">&lt;EOS&gt;</span> token. We use a <span id="S3.SS1.p3.1.4" class="ltx_text ltx_font_typewriter">&lt;IMG&gt;</span> token to indicate the position of visual information in input sequence. <span id="S3.SS1.p3.1.5" class="ltx_text ltx_font_typewriter">&lt;start-of-text&gt;</span> and <span id="S3.SS1.p3.1.6" class="ltx_text ltx_font_typewriter">&lt;start-of-speech&gt;</span> (<span id="S3.SS1.p3.1.7" class="ltx_text ltx_font_typewriter">&lt;SOT&gt;</span>, <span id="S3.SS1.p3.1.8" class="ltx_text ltx_font_typewriter">&lt;SOSP&gt;</span> in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ SynesLM: A Unified Approach for Audio-visual Speech Recognition and Translation via Language Model and Synthetic Data* Equal contributions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) represent the different input modalities. <span id="S3.SS1.p3.1.9" class="ltx_text ltx_font_typewriter">&lt;generate-text&gt;</span> (<span id="S3.SS1.p3.1.10" class="ltx_text ltx_font_typewriter">&lt;GT&gt;</span> in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ SynesLM: A Unified Approach for Audio-visual Speech Recognition and Translation via Language Model and Synthetic Data* Equal contributions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) indicate the output modality of SynesLM. Moreover, the language token <span id="S3.SS1.p3.1.11" class="ltx_text ltx_font_typewriter">&lt;LANG&gt;</span> (e.g. <span id="S3.SS1.p3.1.12" class="ltx_text ltx_font_typewriter">&lt;EN&gt;</span> and <span id="S3.SS1.p3.1.13" class="ltx_text ltx_font_typewriter">&lt;PT&gt;</span>) in the input and output sequence can indicate the source and target language, which enable the translation capacity of our framework. The arrangement of modality and language tokens in SynesLM facilitates its multitasking capability, where each specific combination signifies a distinct task. For example, the token combination {<span id="S3.SS1.p3.1.14" class="ltx_text ltx_font_typewriter">&lt;IMG&gt;</span>, <span id="S3.SS1.p3.1.15" class="ltx_text ltx_font_typewriter">&lt;SOSP&gt;</span>, <span id="S3.SS1.p3.1.16" class="ltx_text ltx_font_typewriter">&lt;EN&gt;</span>, <span id="S3.SS1.p3.1.17" class="ltx_text ltx_font_typewriter">&lt;GT&gt;</span>, <span id="S3.SS1.p3.1.18" class="ltx_text ltx_font_typewriter">&lt;PT&gt;</span>} represents the task of VST from English to Portuguese.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>SynesLM</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ SynesLM: A Unified Approach for Audio-visual Speech Recognition and Translation via Language Model and Synthetic Data* Equal contributions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the overall architecture of SynesLM. We utilize a transformer-based decoder-only LM as our backbone. We use pre-trained OPT model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> to initialize our weights to achieve better performance and training efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">To process the speech inputs, SynesLM employs
a SSL feature extractor like HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> and k-means to generate speech discrete tokens. The integration of visual information is accomplished through a visual encoder and a Vision-Language MLP projector. Details will be explained later in this section.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.14" class="ltx_p">We concatenate the input text sequence <math id="S3.SS2.p3.1.m1.2" class="ltx_math_unparsed" alttext="Y=(y_{i}\in\mathcal{V}_{\text{txt}}|i=1,...,t_{txt})" display="inline"><semantics id="S3.SS2.p3.1.m1.2a"><mrow id="S3.SS2.p3.1.m1.2b"><mi id="S3.SS2.p3.1.m1.2.3">Y</mi><mo id="S3.SS2.p3.1.m1.2.4">=</mo><mrow id="S3.SS2.p3.1.m1.2.5"><mo stretchy="false" id="S3.SS2.p3.1.m1.2.5.1">(</mo><msub id="S3.SS2.p3.1.m1.2.5.2"><mi id="S3.SS2.p3.1.m1.2.5.2.2">y</mi><mi id="S3.SS2.p3.1.m1.2.5.2.3">i</mi></msub><mo id="S3.SS2.p3.1.m1.2.5.3">∈</mo><msub id="S3.SS2.p3.1.m1.2.5.4"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.1.m1.2.5.4.2">𝒱</mi><mtext id="S3.SS2.p3.1.m1.2.5.4.3">txt</mtext></msub><mo fence="false" rspace="0.167em" stretchy="false" id="S3.SS2.p3.1.m1.2.5.5">|</mo><mi id="S3.SS2.p3.1.m1.2.5.6">i</mi><mo id="S3.SS2.p3.1.m1.2.5.7">=</mo><mn id="S3.SS2.p3.1.m1.1.1">1</mn><mo id="S3.SS2.p3.1.m1.2.5.8">,</mo><mi mathvariant="normal" id="S3.SS2.p3.1.m1.2.2">…</mi><mo id="S3.SS2.p3.1.m1.2.5.9">,</mo><msub id="S3.SS2.p3.1.m1.2.5.10"><mi id="S3.SS2.p3.1.m1.2.5.10.2">t</mi><mrow id="S3.SS2.p3.1.m1.2.5.10.3"><mi id="S3.SS2.p3.1.m1.2.5.10.3.2">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.1.m1.2.5.10.3.1">​</mo><mi id="S3.SS2.p3.1.m1.2.5.10.3.3">x</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.1.m1.2.5.10.3.1a">​</mo><mi id="S3.SS2.p3.1.m1.2.5.10.3.4">t</mi></mrow></msub><mo stretchy="false" id="S3.SS2.p3.1.m1.2.5.11">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.2c">Y=(y_{i}\in\mathcal{V}_{\text{txt}}|i=1,...,t_{txt})</annotation></semantics></math> with the input discrete speech token(dst) sequence <math id="S3.SS2.p3.2.m2.2" class="ltx_math_unparsed" alttext="D=(d_{i}\in\mathcal{V}_{\text{dst}}|i=1,...,t_{dst})" display="inline"><semantics id="S3.SS2.p3.2.m2.2a"><mrow id="S3.SS2.p3.2.m2.2b"><mi id="S3.SS2.p3.2.m2.2.3">D</mi><mo id="S3.SS2.p3.2.m2.2.4">=</mo><mrow id="S3.SS2.p3.2.m2.2.5"><mo stretchy="false" id="S3.SS2.p3.2.m2.2.5.1">(</mo><msub id="S3.SS2.p3.2.m2.2.5.2"><mi id="S3.SS2.p3.2.m2.2.5.2.2">d</mi><mi id="S3.SS2.p3.2.m2.2.5.2.3">i</mi></msub><mo id="S3.SS2.p3.2.m2.2.5.3">∈</mo><msub id="S3.SS2.p3.2.m2.2.5.4"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.2.m2.2.5.4.2">𝒱</mi><mtext id="S3.SS2.p3.2.m2.2.5.4.3">dst</mtext></msub><mo fence="false" rspace="0.167em" stretchy="false" id="S3.SS2.p3.2.m2.2.5.5">|</mo><mi id="S3.SS2.p3.2.m2.2.5.6">i</mi><mo id="S3.SS2.p3.2.m2.2.5.7">=</mo><mn id="S3.SS2.p3.2.m2.1.1">1</mn><mo id="S3.SS2.p3.2.m2.2.5.8">,</mo><mi mathvariant="normal" id="S3.SS2.p3.2.m2.2.2">…</mi><mo id="S3.SS2.p3.2.m2.2.5.9">,</mo><msub id="S3.SS2.p3.2.m2.2.5.10"><mi id="S3.SS2.p3.2.m2.2.5.10.2">t</mi><mrow id="S3.SS2.p3.2.m2.2.5.10.3"><mi id="S3.SS2.p3.2.m2.2.5.10.3.2">d</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.2.5.10.3.1">​</mo><mi id="S3.SS2.p3.2.m2.2.5.10.3.3">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.2.5.10.3.1a">​</mo><mi id="S3.SS2.p3.2.m2.2.5.10.3.4">t</mi></mrow></msub><mo stretchy="false" id="S3.SS2.p3.2.m2.2.5.11">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.2c">D=(d_{i}\in\mathcal{V}_{\text{dst}}|i=1,...,t_{dst})</annotation></semantics></math> as the <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mi id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><ci id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">T</annotation></semantics></math>-length discrete speech-text token sequence <math id="S3.SS2.p3.4.m4.3" class="ltx_math_unparsed" alttext="Z=(z_{i}\in\mathcal{V}=\mathcal{V}_{\text{txt}}\cup\mathcal{V}_{\text{dst}}\cup\mathcal{V}_{\text{special}}|i=1,...,T)" display="inline"><semantics id="S3.SS2.p3.4.m4.3a"><mrow id="S3.SS2.p3.4.m4.3b"><mi id="S3.SS2.p3.4.m4.3.4">Z</mi><mo id="S3.SS2.p3.4.m4.3.5">=</mo><mrow id="S3.SS2.p3.4.m4.3.6"><mo stretchy="false" id="S3.SS2.p3.4.m4.3.6.1">(</mo><msub id="S3.SS2.p3.4.m4.3.6.2"><mi id="S3.SS2.p3.4.m4.3.6.2.2">z</mi><mi id="S3.SS2.p3.4.m4.3.6.2.3">i</mi></msub><mo id="S3.SS2.p3.4.m4.3.6.3">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.4.m4.3.6.4">𝒱</mi><mo id="S3.SS2.p3.4.m4.3.6.5">=</mo><msub id="S3.SS2.p3.4.m4.3.6.6"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.4.m4.3.6.6.2">𝒱</mi><mtext id="S3.SS2.p3.4.m4.3.6.6.3">txt</mtext></msub><mo id="S3.SS2.p3.4.m4.3.6.7">∪</mo><msub id="S3.SS2.p3.4.m4.3.6.8"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.4.m4.3.6.8.2">𝒱</mi><mtext id="S3.SS2.p3.4.m4.3.6.8.3">dst</mtext></msub><mo id="S3.SS2.p3.4.m4.3.6.9">∪</mo><msub id="S3.SS2.p3.4.m4.3.6.10"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.4.m4.3.6.10.2">𝒱</mi><mtext id="S3.SS2.p3.4.m4.3.6.10.3">special</mtext></msub><mo fence="false" rspace="0.167em" stretchy="false" id="S3.SS2.p3.4.m4.3.6.11">|</mo><mi id="S3.SS2.p3.4.m4.3.6.12">i</mi><mo id="S3.SS2.p3.4.m4.3.6.13">=</mo><mn id="S3.SS2.p3.4.m4.1.1">1</mn><mo id="S3.SS2.p3.4.m4.3.6.14">,</mo><mi mathvariant="normal" id="S3.SS2.p3.4.m4.2.2">…</mi><mo id="S3.SS2.p3.4.m4.3.6.15">,</mo><mi id="S3.SS2.p3.4.m4.3.3">T</mi><mo stretchy="false" id="S3.SS2.p3.4.m4.3.6.16">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.3c">Z=(z_{i}\in\mathcal{V}=\mathcal{V}_{\text{txt}}\cup\mathcal{V}_{\text{dst}}\cup\mathcal{V}_{\text{special}}|i=1,...,T)</annotation></semantics></math>, where <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="T=t_{txt}+t_{dst}" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><mrow id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml"><mi id="S3.SS2.p3.5.m5.1.1.2" xref="S3.SS2.p3.5.m5.1.1.2.cmml">T</mi><mo id="S3.SS2.p3.5.m5.1.1.1" xref="S3.SS2.p3.5.m5.1.1.1.cmml">=</mo><mrow id="S3.SS2.p3.5.m5.1.1.3" xref="S3.SS2.p3.5.m5.1.1.3.cmml"><msub id="S3.SS2.p3.5.m5.1.1.3.2" xref="S3.SS2.p3.5.m5.1.1.3.2.cmml"><mi id="S3.SS2.p3.5.m5.1.1.3.2.2" xref="S3.SS2.p3.5.m5.1.1.3.2.2.cmml">t</mi><mrow id="S3.SS2.p3.5.m5.1.1.3.2.3" xref="S3.SS2.p3.5.m5.1.1.3.2.3.cmml"><mi id="S3.SS2.p3.5.m5.1.1.3.2.3.2" xref="S3.SS2.p3.5.m5.1.1.3.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.5.m5.1.1.3.2.3.1" xref="S3.SS2.p3.5.m5.1.1.3.2.3.1.cmml">​</mo><mi id="S3.SS2.p3.5.m5.1.1.3.2.3.3" xref="S3.SS2.p3.5.m5.1.1.3.2.3.3.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.5.m5.1.1.3.2.3.1a" xref="S3.SS2.p3.5.m5.1.1.3.2.3.1.cmml">​</mo><mi id="S3.SS2.p3.5.m5.1.1.3.2.3.4" xref="S3.SS2.p3.5.m5.1.1.3.2.3.4.cmml">t</mi></mrow></msub><mo id="S3.SS2.p3.5.m5.1.1.3.1" xref="S3.SS2.p3.5.m5.1.1.3.1.cmml">+</mo><msub id="S3.SS2.p3.5.m5.1.1.3.3" xref="S3.SS2.p3.5.m5.1.1.3.3.cmml"><mi id="S3.SS2.p3.5.m5.1.1.3.3.2" xref="S3.SS2.p3.5.m5.1.1.3.3.2.cmml">t</mi><mrow id="S3.SS2.p3.5.m5.1.1.3.3.3" xref="S3.SS2.p3.5.m5.1.1.3.3.3.cmml"><mi id="S3.SS2.p3.5.m5.1.1.3.3.3.2" xref="S3.SS2.p3.5.m5.1.1.3.3.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.5.m5.1.1.3.3.3.1" xref="S3.SS2.p3.5.m5.1.1.3.3.3.1.cmml">​</mo><mi id="S3.SS2.p3.5.m5.1.1.3.3.3.3" xref="S3.SS2.p3.5.m5.1.1.3.3.3.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.5.m5.1.1.3.3.3.1a" xref="S3.SS2.p3.5.m5.1.1.3.3.3.1.cmml">​</mo><mi id="S3.SS2.p3.5.m5.1.1.3.3.3.4" xref="S3.SS2.p3.5.m5.1.1.3.3.3.4.cmml">t</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><apply id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1"><eq id="S3.SS2.p3.5.m5.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1.1"></eq><ci id="S3.SS2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2">𝑇</ci><apply id="S3.SS2.p3.5.m5.1.1.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3"><plus id="S3.SS2.p3.5.m5.1.1.3.1.cmml" xref="S3.SS2.p3.5.m5.1.1.3.1"></plus><apply id="S3.SS2.p3.5.m5.1.1.3.2.cmml" xref="S3.SS2.p3.5.m5.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.1.1.3.2.1.cmml" xref="S3.SS2.p3.5.m5.1.1.3.2">subscript</csymbol><ci id="S3.SS2.p3.5.m5.1.1.3.2.2.cmml" xref="S3.SS2.p3.5.m5.1.1.3.2.2">𝑡</ci><apply id="S3.SS2.p3.5.m5.1.1.3.2.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3.2.3"><times id="S3.SS2.p3.5.m5.1.1.3.2.3.1.cmml" xref="S3.SS2.p3.5.m5.1.1.3.2.3.1"></times><ci id="S3.SS2.p3.5.m5.1.1.3.2.3.2.cmml" xref="S3.SS2.p3.5.m5.1.1.3.2.3.2">𝑡</ci><ci id="S3.SS2.p3.5.m5.1.1.3.2.3.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3.2.3.3">𝑥</ci><ci id="S3.SS2.p3.5.m5.1.1.3.2.3.4.cmml" xref="S3.SS2.p3.5.m5.1.1.3.2.3.4">𝑡</ci></apply></apply><apply id="S3.SS2.p3.5.m5.1.1.3.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.1.1.3.3.1.cmml" xref="S3.SS2.p3.5.m5.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p3.5.m5.1.1.3.3.2.cmml" xref="S3.SS2.p3.5.m5.1.1.3.3.2">𝑡</ci><apply id="S3.SS2.p3.5.m5.1.1.3.3.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3.3.3"><times id="S3.SS2.p3.5.m5.1.1.3.3.3.1.cmml" xref="S3.SS2.p3.5.m5.1.1.3.3.3.1"></times><ci id="S3.SS2.p3.5.m5.1.1.3.3.3.2.cmml" xref="S3.SS2.p3.5.m5.1.1.3.3.3.2">𝑑</ci><ci id="S3.SS2.p3.5.m5.1.1.3.3.3.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3.3.3.3">𝑠</ci><ci id="S3.SS2.p3.5.m5.1.1.3.3.3.4.cmml" xref="S3.SS2.p3.5.m5.1.1.3.3.3.4">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">T=t_{txt}+t_{dst}</annotation></semantics></math> and <math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="\mathcal{V}_{\text{special}}" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><msub id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.6.m6.1.1.2" xref="S3.SS2.p3.6.m6.1.1.2.cmml">𝒱</mi><mtext id="S3.SS2.p3.6.m6.1.1.3" xref="S3.SS2.p3.6.m6.1.1.3a.cmml">special</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><apply id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.6.m6.1.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p3.6.m6.1.1.2.cmml" xref="S3.SS2.p3.6.m6.1.1.2">𝒱</ci><ci id="S3.SS2.p3.6.m6.1.1.3a.cmml" xref="S3.SS2.p3.6.m6.1.1.3"><mtext mathsize="70%" id="S3.SS2.p3.6.m6.1.1.3.cmml" xref="S3.SS2.p3.6.m6.1.1.3">special</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">\mathcal{V}_{\text{special}}</annotation></semantics></math> is the special token set e.g. <span id="S3.SS2.p3.14.1" class="ltx_text ltx_font_typewriter">&lt;LANG&gt;</span> etc. We feed the input discrete speech-text sequence <math id="S3.SS2.p3.7.m7.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S3.SS2.p3.7.m7.1a"><mi id="S3.SS2.p3.7.m7.1.1" xref="S3.SS2.p3.7.m7.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.1b"><ci id="S3.SS2.p3.7.m7.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.1c">Z</annotation></semantics></math> into the embedding layer to get the <math id="S3.SS2.p3.8.m8.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS2.p3.8.m8.1a"><mi id="S3.SS2.p3.8.m8.1.1" xref="S3.SS2.p3.8.m8.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m8.1b"><ci id="S3.SS2.p3.8.m8.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m8.1c">D</annotation></semantics></math>-dimensional embedding space expressed by <math id="S3.SS2.p3.9.m9.3" class="ltx_math_unparsed" alttext="\mathbf{E}=(\mathbf{e}_{i}\in\mathbb{R}^{D}|i=1,...,T)" display="inline"><semantics id="S3.SS2.p3.9.m9.3a"><mrow id="S3.SS2.p3.9.m9.3b"><mi id="S3.SS2.p3.9.m9.3.4">𝐄</mi><mo id="S3.SS2.p3.9.m9.3.5">=</mo><mrow id="S3.SS2.p3.9.m9.3.6"><mo stretchy="false" id="S3.SS2.p3.9.m9.3.6.1">(</mo><msub id="S3.SS2.p3.9.m9.3.6.2"><mi id="S3.SS2.p3.9.m9.3.6.2.2">𝐞</mi><mi id="S3.SS2.p3.9.m9.3.6.2.3">i</mi></msub><mo id="S3.SS2.p3.9.m9.3.6.3">∈</mo><msup id="S3.SS2.p3.9.m9.3.6.4"><mi id="S3.SS2.p3.9.m9.3.6.4.2">ℝ</mi><mi id="S3.SS2.p3.9.m9.3.6.4.3">D</mi></msup><mo fence="false" rspace="0.167em" stretchy="false" id="S3.SS2.p3.9.m9.3.6.5">|</mo><mi id="S3.SS2.p3.9.m9.3.6.6">i</mi><mo id="S3.SS2.p3.9.m9.3.6.7">=</mo><mn id="S3.SS2.p3.9.m9.1.1">1</mn><mo id="S3.SS2.p3.9.m9.3.6.8">,</mo><mi mathvariant="normal" id="S3.SS2.p3.9.m9.2.2">…</mi><mo id="S3.SS2.p3.9.m9.3.6.9">,</mo><mi id="S3.SS2.p3.9.m9.3.3">T</mi><mo stretchy="false" id="S3.SS2.p3.9.m9.3.6.10">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS2.p3.9.m9.3c">\mathbf{E}=(\mathbf{e}_{i}\in\mathbb{R}^{D}|i=1,...,T)</annotation></semantics></math>.
Additionally, we add a single visual token in the <math id="S3.SS2.p3.10.m10.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS2.p3.10.m10.1a"><mi id="S3.SS2.p3.10.m10.1.1" xref="S3.SS2.p3.10.m10.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.10.m10.1b"><ci id="S3.SS2.p3.10.m10.1.1.cmml" xref="S3.SS2.p3.10.m10.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.10.m10.1c">D</annotation></semantics></math>-dimensional embedding space as <math id="S3.SS2.p3.11.m11.1" class="ltx_Math" alttext="\mathbf{V}" display="inline"><semantics id="S3.SS2.p3.11.m11.1a"><mi id="S3.SS2.p3.11.m11.1.1" xref="S3.SS2.p3.11.m11.1.1.cmml">𝐕</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.11.m11.1b"><ci id="S3.SS2.p3.11.m11.1.1.cmml" xref="S3.SS2.p3.11.m11.1.1">𝐕</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.11.m11.1c">\mathbf{V}</annotation></semantics></math>.
We feed the visual embedding <math id="S3.SS2.p3.12.m12.1" class="ltx_Math" alttext="\mathbf{V}" display="inline"><semantics id="S3.SS2.p3.12.m12.1a"><mi id="S3.SS2.p3.12.m12.1.1" xref="S3.SS2.p3.12.m12.1.1.cmml">𝐕</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.12.m12.1b"><ci id="S3.SS2.p3.12.m12.1.1.cmml" xref="S3.SS2.p3.12.m12.1.1">𝐕</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.12.m12.1c">\mathbf{V}</annotation></semantics></math> and discrete speech-text embedding <math id="S3.SS2.p3.13.m13.1" class="ltx_Math" alttext="\mathbf{E}" display="inline"><semantics id="S3.SS2.p3.13.m13.1a"><mi id="S3.SS2.p3.13.m13.1.1" xref="S3.SS2.p3.13.m13.1.1.cmml">𝐄</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.13.m13.1b"><ci id="S3.SS2.p3.13.m13.1.1.cmml" xref="S3.SS2.p3.13.m13.1.1">𝐄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.13.m13.1c">\mathbf{E}</annotation></semantics></math> in to our auto-regressive LM. The joint probability of the generated text sequence <math id="S3.SS2.p3.14.m14.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.SS2.p3.14.m14.1a"><mi id="S3.SS2.p3.14.m14.1.1" xref="S3.SS2.p3.14.m14.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.14.m14.1b"><ci id="S3.SS2.p3.14.m14.1.1.cmml" xref="S3.SS2.p3.14.m14.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.14.m14.1c">Y</annotation></semantics></math> can be expressed as:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="p(Y)=\prod_{i=T+1}^{T+1+L}p\left(Y_{i}|\mathbf{V},\mathbf{E}_{1:i-1}\right)," display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1.3" xref="S3.E1.m1.3.3.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.3.2" xref="S3.E1.m1.3.3.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.3.1" xref="S3.E1.m1.3.3.1.1.3.1.cmml">​</mo><mrow id="S3.E1.m1.3.3.1.1.3.3.2" xref="S3.E1.m1.3.3.1.1.3.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.1.1.3.3.2.1" xref="S3.E1.m1.3.3.1.1.3.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">Y</mi><mo stretchy="false" id="S3.E1.m1.3.3.1.1.3.3.2.2" xref="S3.E1.m1.3.3.1.1.3.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.1" xref="S3.E1.m1.3.3.1.1.1.cmml"><munderover id="S3.E1.m1.3.3.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E1.m1.3.3.1.1.1.2.2.2" xref="S3.E1.m1.3.3.1.1.1.2.2.2.cmml">∏</mo><mrow id="S3.E1.m1.3.3.1.1.1.2.2.3" xref="S3.E1.m1.3.3.1.1.1.2.2.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.2.2.3.2" xref="S3.E1.m1.3.3.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E1.m1.3.3.1.1.1.2.2.3.1" xref="S3.E1.m1.3.3.1.1.1.2.2.3.1.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.1.2.2.3.3" xref="S3.E1.m1.3.3.1.1.1.2.2.3.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.2.2.3.3.2" xref="S3.E1.m1.3.3.1.1.1.2.2.3.3.2.cmml">T</mi><mo id="S3.E1.m1.3.3.1.1.1.2.2.3.3.1" xref="S3.E1.m1.3.3.1.1.1.2.2.3.3.1.cmml">+</mo><mn id="S3.E1.m1.3.3.1.1.1.2.2.3.3.3" xref="S3.E1.m1.3.3.1.1.1.2.2.3.3.3.cmml">1</mn></mrow></mrow><mrow id="S3.E1.m1.3.3.1.1.1.2.3" xref="S3.E1.m1.3.3.1.1.1.2.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.2.3.2" xref="S3.E1.m1.3.3.1.1.1.2.3.2.cmml">T</mi><mo id="S3.E1.m1.3.3.1.1.1.2.3.1" xref="S3.E1.m1.3.3.1.1.1.2.3.1.cmml">+</mo><mn id="S3.E1.m1.3.3.1.1.1.2.3.3" xref="S3.E1.m1.3.3.1.1.1.2.3.3.cmml">1</mn><mo id="S3.E1.m1.3.3.1.1.1.2.3.1a" xref="S3.E1.m1.3.3.1.1.1.2.3.1.cmml">+</mo><mi id="S3.E1.m1.3.3.1.1.1.2.3.4" xref="S3.E1.m1.3.3.1.1.1.2.3.4.cmml">L</mi></mrow></munderover><mrow id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">Y</mi><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">𝐕</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">,</mo><msub id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">𝐄</mi><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml"><mn id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml">:</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml">i</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">−</mo><mn id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">1</mn></mrow></mrow></msub></mrow></mrow><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1"><eq id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2"></eq><apply id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.3"><times id="S3.E1.m1.3.3.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.3.1"></times><ci id="S3.E1.m1.3.3.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.3.2">𝑝</ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑌</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1"><apply id="S3.E1.m1.3.3.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.2">superscript</csymbol><apply id="S3.E1.m1.3.3.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.2">subscript</csymbol><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.2.2.2">product</csymbol><apply id="S3.E1.m1.3.3.1.1.1.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.2.2.3"><eq id="S3.E1.m1.3.3.1.1.1.2.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.2.2.3.1"></eq><ci id="S3.E1.m1.3.3.1.1.1.2.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.2.2.3.2">𝑖</ci><apply id="S3.E1.m1.3.3.1.1.1.2.2.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.2.2.3.3"><plus id="S3.E1.m1.3.3.1.1.1.2.2.3.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.2.2.3.3.1"></plus><ci id="S3.E1.m1.3.3.1.1.1.2.2.3.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.2.2.3.3.2">𝑇</ci><cn type="integer" id="S3.E1.m1.3.3.1.1.1.2.2.3.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.2.2.3.3.3">1</cn></apply></apply></apply><apply id="S3.E1.m1.3.3.1.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.2.3"><plus id="S3.E1.m1.3.3.1.1.1.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.2.3.1"></plus><ci id="S3.E1.m1.3.3.1.1.1.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.2.3.2">𝑇</ci><cn type="integer" id="S3.E1.m1.3.3.1.1.1.2.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.2.3.3">1</cn><ci id="S3.E1.m1.3.3.1.1.1.2.3.4.cmml" xref="S3.E1.m1.3.3.1.1.1.2.3.4">𝐿</ci></apply></apply><apply id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1"><times id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2"></times><ci id="S3.E1.m1.3.3.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3">𝑝</ci><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2">conditional</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.2">𝑌</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3">𝑖</ci></apply><list id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1"><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝐕</ci><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2">𝐄</ci><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3"><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1">:</ci><cn type="integer" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2">1</cn><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3"><minus id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.1"></minus><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.2">𝑖</ci><cn type="integer" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.3">1</cn></apply></apply></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">p(Y)=\prod_{i=T+1}^{T+1+L}p\left(Y_{i}|\mathbf{V},\mathbf{E}_{1:i-1}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p3.15" class="ltx_p">where <math id="S3.SS2.p3.15.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS2.p3.15.m1.1a"><mi id="S3.SS2.p3.15.m1.1.1" xref="S3.SS2.p3.15.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.15.m1.1b"><ci id="S3.SS2.p3.15.m1.1.1.cmml" xref="S3.SS2.p3.15.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.15.m1.1c">L</annotation></semantics></math> is the generated text length.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.4" class="ltx_p">During training and inference process, given the previous information <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="\mathbf{V}" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">𝐕</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">𝐕</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\mathbf{V}</annotation></semantics></math>, <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="\mathbf{E}_{1:i-1}" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><msub id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><mi id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml">𝐄</mi><mrow id="S3.SS2.p4.2.m2.1.1.3" xref="S3.SS2.p4.2.m2.1.1.3.cmml"><mn id="S3.SS2.p4.2.m2.1.1.3.2" xref="S3.SS2.p4.2.m2.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.SS2.p4.2.m2.1.1.3.1" xref="S3.SS2.p4.2.m2.1.1.3.1.cmml">:</mo><mrow id="S3.SS2.p4.2.m2.1.1.3.3" xref="S3.SS2.p4.2.m2.1.1.3.3.cmml"><mi id="S3.SS2.p4.2.m2.1.1.3.3.2" xref="S3.SS2.p4.2.m2.1.1.3.3.2.cmml">i</mi><mo id="S3.SS2.p4.2.m2.1.1.3.3.1" xref="S3.SS2.p4.2.m2.1.1.3.3.1.cmml">−</mo><mn id="S3.SS2.p4.2.m2.1.1.3.3.3" xref="S3.SS2.p4.2.m2.1.1.3.3.3.cmml">1</mn></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2">𝐄</ci><apply id="S3.SS2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3"><ci id="S3.SS2.p4.2.m2.1.1.3.1.cmml" xref="S3.SS2.p4.2.m2.1.1.3.1">:</ci><cn type="integer" id="S3.SS2.p4.2.m2.1.1.3.2.cmml" xref="S3.SS2.p4.2.m2.1.1.3.2">1</cn><apply id="S3.SS2.p4.2.m2.1.1.3.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3.3"><minus id="S3.SS2.p4.2.m2.1.1.3.3.1.cmml" xref="S3.SS2.p4.2.m2.1.1.3.3.1"></minus><ci id="S3.SS2.p4.2.m2.1.1.3.3.2.cmml" xref="S3.SS2.p4.2.m2.1.1.3.3.2">𝑖</ci><cn type="integer" id="S3.SS2.p4.2.m2.1.1.3.3.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">\mathbf{E}_{1:i-1}</annotation></semantics></math>, we can use this formula to predict the probability distribution in the next time step <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="\hat{p_{i}}" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><mover accent="true" id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml"><msub id="S3.SS2.p4.3.m3.1.1.2" xref="S3.SS2.p4.3.m3.1.1.2.cmml"><mi id="S3.SS2.p4.3.m3.1.1.2.2" xref="S3.SS2.p4.3.m3.1.1.2.2.cmml">p</mi><mi id="S3.SS2.p4.3.m3.1.1.2.3" xref="S3.SS2.p4.3.m3.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS2.p4.3.m3.1.1.1" xref="S3.SS2.p4.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"><ci id="S3.SS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1.1">^</ci><apply id="S3.SS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p4.3.m3.1.1.2.1.cmml" xref="S3.SS2.p4.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS2.p4.3.m3.1.1.2.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2.2">𝑝</ci><ci id="S3.SS2.p4.3.m3.1.1.2.3.cmml" xref="S3.SS2.p4.3.m3.1.1.2.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">\hat{p_{i}}</annotation></semantics></math> by <math id="S3.SS2.p4.4.m4.3" class="ltx_Math" alttext="\hat{p_{i}}=\operatorname{\mathrm{SynesLM}}\left(Y_{i}|\mathbf{V},\mathbf{E}_{1:i-1}\right)" display="inline"><semantics id="S3.SS2.p4.4.m4.3a"><mrow id="S3.SS2.p4.4.m4.3.3" xref="S3.SS2.p4.4.m4.3.3.cmml"><mover accent="true" id="S3.SS2.p4.4.m4.3.3.3" xref="S3.SS2.p4.4.m4.3.3.3.cmml"><msub id="S3.SS2.p4.4.m4.3.3.3.2" xref="S3.SS2.p4.4.m4.3.3.3.2.cmml"><mi id="S3.SS2.p4.4.m4.3.3.3.2.2" xref="S3.SS2.p4.4.m4.3.3.3.2.2.cmml">p</mi><mi id="S3.SS2.p4.4.m4.3.3.3.2.3" xref="S3.SS2.p4.4.m4.3.3.3.2.3.cmml">i</mi></msub><mo id="S3.SS2.p4.4.m4.3.3.3.1" xref="S3.SS2.p4.4.m4.3.3.3.1.cmml">^</mo></mover><mo id="S3.SS2.p4.4.m4.3.3.2" xref="S3.SS2.p4.4.m4.3.3.2.cmml">=</mo><mrow id="S3.SS2.p4.4.m4.3.3.1.1" xref="S3.SS2.p4.4.m4.3.3.1.2.cmml"><mi id="S3.SS2.p4.4.m4.2.2" xref="S3.SS2.p4.4.m4.2.2.cmml">SynesLM</mi><mo id="S3.SS2.p4.4.m4.3.3.1.1a" xref="S3.SS2.p4.4.m4.3.3.1.2.cmml">⁡</mo><mrow id="S3.SS2.p4.4.m4.3.3.1.1.1" xref="S3.SS2.p4.4.m4.3.3.1.2.cmml"><mo id="S3.SS2.p4.4.m4.3.3.1.1.1.2" xref="S3.SS2.p4.4.m4.3.3.1.2.cmml">(</mo><mrow id="S3.SS2.p4.4.m4.3.3.1.1.1.1" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.cmml"><msub id="S3.SS2.p4.4.m4.3.3.1.1.1.1.3" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.3.cmml"><mi id="S3.SS2.p4.4.m4.3.3.1.1.1.1.3.2" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.3.2.cmml">Y</mi><mi id="S3.SS2.p4.4.m4.3.3.1.1.1.1.3.3" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S3.SS2.p4.4.m4.3.3.1.1.1.1.2" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.2.cmml">|</mo><mrow id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml">𝐕</mi><mo id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.2" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.2.cmml">,</mo><msub id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.2" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.2.cmml">𝐄</mi><mrow id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.cmml"><mn id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.2" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.1" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.1.cmml">:</mo><mrow id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.3" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.3.2" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.3.2.cmml">i</mi><mo id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.3.1" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.3.1.cmml">−</mo><mn id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.3.3" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.3.3.cmml">1</mn></mrow></mrow></msub></mrow></mrow><mo id="S3.SS2.p4.4.m4.3.3.1.1.1.3" xref="S3.SS2.p4.4.m4.3.3.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.3b"><apply id="S3.SS2.p4.4.m4.3.3.cmml" xref="S3.SS2.p4.4.m4.3.3"><eq id="S3.SS2.p4.4.m4.3.3.2.cmml" xref="S3.SS2.p4.4.m4.3.3.2"></eq><apply id="S3.SS2.p4.4.m4.3.3.3.cmml" xref="S3.SS2.p4.4.m4.3.3.3"><ci id="S3.SS2.p4.4.m4.3.3.3.1.cmml" xref="S3.SS2.p4.4.m4.3.3.3.1">^</ci><apply id="S3.SS2.p4.4.m4.3.3.3.2.cmml" xref="S3.SS2.p4.4.m4.3.3.3.2"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m4.3.3.3.2.1.cmml" xref="S3.SS2.p4.4.m4.3.3.3.2">subscript</csymbol><ci id="S3.SS2.p4.4.m4.3.3.3.2.2.cmml" xref="S3.SS2.p4.4.m4.3.3.3.2.2">𝑝</ci><ci id="S3.SS2.p4.4.m4.3.3.3.2.3.cmml" xref="S3.SS2.p4.4.m4.3.3.3.2.3">𝑖</ci></apply></apply><apply id="S3.SS2.p4.4.m4.3.3.1.2.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1"><ci id="S3.SS2.p4.4.m4.2.2.cmml" xref="S3.SS2.p4.4.m4.2.2">SynesLM</ci><apply id="S3.SS2.p4.4.m4.3.3.1.1.1.1.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.p4.4.m4.3.3.1.1.1.1.2.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.2">conditional</csymbol><apply id="S3.SS2.p4.4.m4.3.3.1.1.1.1.3.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m4.3.3.1.1.1.1.3.1.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p4.4.m4.3.3.1.1.1.1.3.2.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.3.2">𝑌</ci><ci id="S3.SS2.p4.4.m4.3.3.1.1.1.1.3.3.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.3.3">𝑖</ci></apply><list id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.2.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1"><ci id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">𝐕</ci><apply id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.2">𝐄</ci><apply id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3"><ci id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.1">:</ci><cn type="integer" id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.2">1</cn><apply id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.3"><minus id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.3.1"></minus><ci id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.3.2">𝑖</ci><cn type="integer" id="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1.1.1.1.1.3.3.3">1</cn></apply></apply></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.3c">\hat{p_{i}}=\operatorname{\mathrm{SynesLM}}\left(Y_{i}|\mathbf{V},\mathbf{E}_{1:i-1}\right)</annotation></semantics></math>. Then the cross entropy (CE) loss of the model can be expressed as:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.5" class="ltx_Math" alttext="\mathcal{L}_{\operatorname{\mathrm{CE}}}\left(p_{i},\hat{p_{i}}\right)=-\sum_{c=1}^{|\mathcal{V}|}p_{i}(c)\log\hat{p_{i}}(c)," display="block"><semantics id="S3.E2.m1.5a"><mrow id="S3.E2.m1.5.5.1" xref="S3.E2.m1.5.5.1.1.cmml"><mrow id="S3.E2.m1.5.5.1.1" xref="S3.E2.m1.5.5.1.1.cmml"><mrow id="S3.E2.m1.5.5.1.1.1" xref="S3.E2.m1.5.5.1.1.1.cmml"><msub id="S3.E2.m1.5.5.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.5.5.1.1.1.3.2" xref="S3.E2.m1.5.5.1.1.1.3.2.cmml">ℒ</mi><mi id="S3.E2.m1.5.5.1.1.1.3.3" xref="S3.E2.m1.5.5.1.1.1.3.3.cmml">CE</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.5.5.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.2.cmml"><mo id="S3.E2.m1.5.5.1.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.2.cmml">(</mo><msub id="S3.E2.m1.5.5.1.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.5.5.1.1.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.cmml">p</mi><mi id="S3.E2.m1.5.5.1.1.1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E2.m1.5.5.1.1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.1.2.cmml">,</mo><mover accent="true" id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><msub id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml"><mi id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml">p</mi><mi id="S3.E2.m1.2.2.2.3" xref="S3.E2.m1.2.2.2.3.cmml">i</mi></msub><mo id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml">^</mo></mover><mo id="S3.E2.m1.5.5.1.1.1.1.1.4" xref="S3.E2.m1.5.5.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.5.5.1.1.2" xref="S3.E2.m1.5.5.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.5.5.1.1.3" xref="S3.E2.m1.5.5.1.1.3.cmml"><mo id="S3.E2.m1.5.5.1.1.3a" xref="S3.E2.m1.5.5.1.1.3.cmml">−</mo><mrow id="S3.E2.m1.5.5.1.1.3.2" xref="S3.E2.m1.5.5.1.1.3.2.cmml"><munderover id="S3.E2.m1.5.5.1.1.3.2.1" xref="S3.E2.m1.5.5.1.1.3.2.1.cmml"><mo movablelimits="false" id="S3.E2.m1.5.5.1.1.3.2.1.2.2" xref="S3.E2.m1.5.5.1.1.3.2.1.2.2.cmml">∑</mo><mrow id="S3.E2.m1.5.5.1.1.3.2.1.2.3" xref="S3.E2.m1.5.5.1.1.3.2.1.2.3.cmml"><mi id="S3.E2.m1.5.5.1.1.3.2.1.2.3.2" xref="S3.E2.m1.5.5.1.1.3.2.1.2.3.2.cmml">c</mi><mo id="S3.E2.m1.5.5.1.1.3.2.1.2.3.1" xref="S3.E2.m1.5.5.1.1.3.2.1.2.3.1.cmml">=</mo><mn id="S3.E2.m1.5.5.1.1.3.2.1.2.3.3" xref="S3.E2.m1.5.5.1.1.3.2.1.2.3.3.cmml">1</mn></mrow><mrow id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">𝒱</mi><mo stretchy="false" id="S3.E2.m1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.2.1.cmml">|</mo></mrow></munderover><mrow id="S3.E2.m1.5.5.1.1.3.2.2" xref="S3.E2.m1.5.5.1.1.3.2.2.cmml"><msub id="S3.E2.m1.5.5.1.1.3.2.2.2" xref="S3.E2.m1.5.5.1.1.3.2.2.2.cmml"><mi id="S3.E2.m1.5.5.1.1.3.2.2.2.2" xref="S3.E2.m1.5.5.1.1.3.2.2.2.2.cmml">p</mi><mi id="S3.E2.m1.5.5.1.1.3.2.2.2.3" xref="S3.E2.m1.5.5.1.1.3.2.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.3.2.2.1" xref="S3.E2.m1.5.5.1.1.3.2.2.1.cmml">​</mo><mrow id="S3.E2.m1.5.5.1.1.3.2.2.3.2" xref="S3.E2.m1.5.5.1.1.3.2.2.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.1.1.3.2.2.3.2.1" xref="S3.E2.m1.5.5.1.1.3.2.2.cmml">(</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">c</mi><mo stretchy="false" id="S3.E2.m1.5.5.1.1.3.2.2.3.2.2" xref="S3.E2.m1.5.5.1.1.3.2.2.cmml">)</mo></mrow><mo lspace="0.167em" rspace="0em" id="S3.E2.m1.5.5.1.1.3.2.2.1a" xref="S3.E2.m1.5.5.1.1.3.2.2.1.cmml">​</mo><mrow id="S3.E2.m1.5.5.1.1.3.2.2.4" xref="S3.E2.m1.5.5.1.1.3.2.2.4.cmml"><mi id="S3.E2.m1.5.5.1.1.3.2.2.4.1" xref="S3.E2.m1.5.5.1.1.3.2.2.4.1.cmml">log</mi><mo lspace="0.167em" id="S3.E2.m1.5.5.1.1.3.2.2.4a" xref="S3.E2.m1.5.5.1.1.3.2.2.4.cmml">⁡</mo><mover accent="true" id="S3.E2.m1.5.5.1.1.3.2.2.4.2" xref="S3.E2.m1.5.5.1.1.3.2.2.4.2.cmml"><msub id="S3.E2.m1.5.5.1.1.3.2.2.4.2.2" xref="S3.E2.m1.5.5.1.1.3.2.2.4.2.2.cmml"><mi id="S3.E2.m1.5.5.1.1.3.2.2.4.2.2.2" xref="S3.E2.m1.5.5.1.1.3.2.2.4.2.2.2.cmml">p</mi><mi id="S3.E2.m1.5.5.1.1.3.2.2.4.2.2.3" xref="S3.E2.m1.5.5.1.1.3.2.2.4.2.2.3.cmml">i</mi></msub><mo id="S3.E2.m1.5.5.1.1.3.2.2.4.2.1" xref="S3.E2.m1.5.5.1.1.3.2.2.4.2.1.cmml">^</mo></mover></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.3.2.2.1b" xref="S3.E2.m1.5.5.1.1.3.2.2.1.cmml">​</mo><mrow id="S3.E2.m1.5.5.1.1.3.2.2.5.2" xref="S3.E2.m1.5.5.1.1.3.2.2.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.1.1.3.2.2.5.2.1" xref="S3.E2.m1.5.5.1.1.3.2.2.cmml">(</mo><mi id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">c</mi><mo stretchy="false" id="S3.E2.m1.5.5.1.1.3.2.2.5.2.2" xref="S3.E2.m1.5.5.1.1.3.2.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.5.5.1.2" xref="S3.E2.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.5b"><apply id="S3.E2.m1.5.5.1.1.cmml" xref="S3.E2.m1.5.5.1"><eq id="S3.E2.m1.5.5.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.2"></eq><apply id="S3.E2.m1.5.5.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1"><times id="S3.E2.m1.5.5.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.2"></times><apply id="S3.E2.m1.5.5.1.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.3.1.cmml" xref="S3.E2.m1.5.5.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.1.3.2.cmml" xref="S3.E2.m1.5.5.1.1.1.3.2">ℒ</ci><ci id="S3.E2.m1.5.5.1.1.1.3.3.cmml" xref="S3.E2.m1.5.5.1.1.1.3.3">CE</ci></apply><interval closure="open" id="S3.E2.m1.5.5.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1"><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2">𝑝</ci><ci id="S3.E2.m1.5.5.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><ci id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1">^</ci><apply id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">𝑝</ci><ci id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.3">𝑖</ci></apply></apply></interval></apply><apply id="S3.E2.m1.5.5.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.3"><minus id="S3.E2.m1.5.5.1.1.3.1.cmml" xref="S3.E2.m1.5.5.1.1.3"></minus><apply id="S3.E2.m1.5.5.1.1.3.2.cmml" xref="S3.E2.m1.5.5.1.1.3.2"><apply id="S3.E2.m1.5.5.1.1.3.2.1.cmml" xref="S3.E2.m1.5.5.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.3.2.1.1.cmml" xref="S3.E2.m1.5.5.1.1.3.2.1">superscript</csymbol><apply id="S3.E2.m1.5.5.1.1.3.2.1.2.cmml" xref="S3.E2.m1.5.5.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.3.2.1.2.1.cmml" xref="S3.E2.m1.5.5.1.1.3.2.1">subscript</csymbol><sum id="S3.E2.m1.5.5.1.1.3.2.1.2.2.cmml" xref="S3.E2.m1.5.5.1.1.3.2.1.2.2"></sum><apply id="S3.E2.m1.5.5.1.1.3.2.1.2.3.cmml" xref="S3.E2.m1.5.5.1.1.3.2.1.2.3"><eq id="S3.E2.m1.5.5.1.1.3.2.1.2.3.1.cmml" xref="S3.E2.m1.5.5.1.1.3.2.1.2.3.1"></eq><ci id="S3.E2.m1.5.5.1.1.3.2.1.2.3.2.cmml" xref="S3.E2.m1.5.5.1.1.3.2.1.2.3.2">𝑐</ci><cn type="integer" id="S3.E2.m1.5.5.1.1.3.2.1.2.3.3.cmml" xref="S3.E2.m1.5.5.1.1.3.2.1.2.3.3">1</cn></apply></apply><apply id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.3"><abs id="S3.E2.m1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.3.1"></abs><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝒱</ci></apply></apply><apply id="S3.E2.m1.5.5.1.1.3.2.2.cmml" xref="S3.E2.m1.5.5.1.1.3.2.2"><times id="S3.E2.m1.5.5.1.1.3.2.2.1.cmml" xref="S3.E2.m1.5.5.1.1.3.2.2.1"></times><apply id="S3.E2.m1.5.5.1.1.3.2.2.2.cmml" xref="S3.E2.m1.5.5.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.3.2.2.2.1.cmml" xref="S3.E2.m1.5.5.1.1.3.2.2.2">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.3.2.2.2.2.cmml" xref="S3.E2.m1.5.5.1.1.3.2.2.2.2">𝑝</ci><ci id="S3.E2.m1.5.5.1.1.3.2.2.2.3.cmml" xref="S3.E2.m1.5.5.1.1.3.2.2.2.3">𝑖</ci></apply><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝑐</ci><apply id="S3.E2.m1.5.5.1.1.3.2.2.4.cmml" xref="S3.E2.m1.5.5.1.1.3.2.2.4"><log id="S3.E2.m1.5.5.1.1.3.2.2.4.1.cmml" xref="S3.E2.m1.5.5.1.1.3.2.2.4.1"></log><apply id="S3.E2.m1.5.5.1.1.3.2.2.4.2.cmml" xref="S3.E2.m1.5.5.1.1.3.2.2.4.2"><ci id="S3.E2.m1.5.5.1.1.3.2.2.4.2.1.cmml" xref="S3.E2.m1.5.5.1.1.3.2.2.4.2.1">^</ci><apply id="S3.E2.m1.5.5.1.1.3.2.2.4.2.2.cmml" xref="S3.E2.m1.5.5.1.1.3.2.2.4.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.3.2.2.4.2.2.1.cmml" xref="S3.E2.m1.5.5.1.1.3.2.2.4.2.2">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.3.2.2.4.2.2.2.cmml" xref="S3.E2.m1.5.5.1.1.3.2.2.4.2.2.2">𝑝</ci><ci id="S3.E2.m1.5.5.1.1.3.2.2.4.2.2.3.cmml" xref="S3.E2.m1.5.5.1.1.3.2.2.4.2.2.3">𝑖</ci></apply></apply></apply><ci id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">𝑐</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.5c">\mathcal{L}_{\operatorname{\mathrm{CE}}}\left(p_{i},\hat{p_{i}}\right)=-\sum_{c=1}^{|\mathcal{V}|}p_{i}(c)\log\hat{p_{i}}(c),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p4.5" class="ltx_p">where <math id="S3.SS2.p4.5.m1.1" class="ltx_Math" alttext="p_{i}" display="inline"><semantics id="S3.SS2.p4.5.m1.1a"><msub id="S3.SS2.p4.5.m1.1.1" xref="S3.SS2.p4.5.m1.1.1.cmml"><mi id="S3.SS2.p4.5.m1.1.1.2" xref="S3.SS2.p4.5.m1.1.1.2.cmml">p</mi><mi id="S3.SS2.p4.5.m1.1.1.3" xref="S3.SS2.p4.5.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m1.1b"><apply id="S3.SS2.p4.5.m1.1.1.cmml" xref="S3.SS2.p4.5.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.5.m1.1.1.1.cmml" xref="S3.SS2.p4.5.m1.1.1">subscript</csymbol><ci id="S3.SS2.p4.5.m1.1.1.2.cmml" xref="S3.SS2.p4.5.m1.1.1.2">𝑝</ci><ci id="S3.SS2.p4.5.m1.1.1.3.cmml" xref="S3.SS2.p4.5.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m1.1c">p_{i}</annotation></semantics></math> is the reference probability distribution.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Data Recovery via Synthetic Image Data</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We mainly utilize two different multimodal datasets in our study:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">How2</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> is a rich multimodal collection of instructional videos with English subtitles and Portuguese translations. We use the 300 hours subset of video content in this project. We utilize the training set contains 182,167 clips, the validation set includes 1,939 clips, and the test set consists of 2,298 clips.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">VisSpeech</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> is constructed by selecting 482 video clips from the extensive HowTo100M dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, aimed at creating a robust test set. The selected clips are characterized by their strong visual-audio correlation, providing an ideal platform to evaluate the effectiveness of our visual modality.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">We use the How2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> for training and testing. To ensure the correlation between visual and textual data, we initially calculate the similarity score between the ground truth transcription and the selected frame. As illustrated in Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Method ‣ SynesLM: A Unified Approach for Audio-visual Speech Recognition and Translation via Language Model and Synthetic Data* Equal contributions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, for a dataset containing image-text pairs, we input the images and text into CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> to calculate the cosine similarity score for each word in the ground truth. The distribution of similarity scores indicates that over 62% of visual data have a similarity score below 20%, reflecting the poor quality of the How2 visual data. To address this issue, we designed a data recovery pipeline to enhance modality interaction within the How2 dataset. If the maximum score for each words in the sentence is below a given threshold (<math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\tau=0.2" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">τ</mi><mo id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><eq id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></eq><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝜏</ci><cn type="float" id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\tau=0.2</annotation></semantics></math> in our experiment), we generate new visual data based on the ground truth. For image data generation, we first input the ground truth data into a Large Language Model (LLM) to extract object or action information and generate an image generation prompt based on that information. Finally, we feed the image generation prompt into our image diffusion model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Experimental results comparing on single-task and multi-task on different visual encoders. The relative improvement rate represents the performance influence of incorporating visual modality to the original model under the certain tasks, the visual influence in single-task or multi-task scenarios are calculated independently.</figcaption>
<table id="S4.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.4.5.1" class="ltx_tr">
<th id="S4.T1.4.5.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S4.T1.4.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.4.5.1.2.1" class="ltx_text ltx_font_bold">ASR (How2)</span></th>
<th id="S4.T1.4.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.4.5.1.3.1" class="ltx_text ltx_font_bold">ASR (VisSpeech)</span></th>
<th id="S4.T1.4.5.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.4.5.1.4.1" class="ltx_text ltx_font_bold">ST</span></th>
<th id="S4.T1.4.5.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.4.5.1.5.1" class="ltx_text ltx_font_bold">MT</span></th>
</tr>
<tr id="S4.T1.4.4" class="ltx_tr">
<th id="S4.T1.4.4.5" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">WER(<math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</th>
<th id="S4.T1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">WER(<math id="S4.T1.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.2.2.2.m1.1a"><mo stretchy="false" id="S4.T1.2.2.2.m1.1.1" xref="S4.T1.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">\downarrow</annotation></semantics></math>)</th>
<th id="S4.T1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">BLEU(<math id="S4.T1.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.3.3.3.m1.1a"><mo stretchy="false" id="S4.T1.3.3.3.m1.1.1" xref="S4.T1.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.m1.1c">\uparrow</annotation></semantics></math>)</th>
<th id="S4.T1.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">BLEU(<math id="S4.T1.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.4.4.4.m1.1a"><mo stretchy="false" id="S4.T1.4.4.4.m1.1.1" xref="S4.T1.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.m1.1b"><ci id="S4.T1.4.4.4.m1.1.1.cmml" xref="S4.T1.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.m1.1c">\uparrow</annotation></semantics></math>)</th>
</tr>
<tr id="S4.T1.4.6.2" class="ltx_tr">
<th id="S4.T1.4.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">How2 Baseline w/ visual  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</th>
<th id="S4.T1.4.6.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">18.0</th>
<th id="S4.T1.4.6.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">-</th>
<th id="S4.T1.4.6.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">37.2</th>
<th id="S4.T1.4.6.2.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">54.4</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.4.7.1" class="ltx_tr">
<th id="S4.T1.4.7.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Single w/o visual</th>
<td id="S4.T1.4.7.1.2" class="ltx_td ltx_align_left ltx_border_t">17.6</td>
<td id="S4.T1.4.7.1.3" class="ltx_td ltx_align_left ltx_border_t">41.6</td>
<td id="S4.T1.4.7.1.4" class="ltx_td ltx_align_left ltx_border_t">40.5</td>
<td id="S4.T1.4.7.1.5" class="ltx_td ltx_align_left ltx_border_t">55.2</td>
</tr>
<tr id="S4.T1.4.8.2" class="ltx_tr">
<th id="S4.T1.4.8.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Single w/ visual (CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>)</th>
<td id="S4.T1.4.8.2.2" class="ltx_td ltx_align_left"><span id="S4.T1.4.8.2.2.1" class="ltx_text ltx_font_bold">17.0 (+3.41%)</span></td>
<td id="S4.T1.4.8.2.3" class="ltx_td ltx_align_left">41.7 (-0.24%)</td>
<td id="S4.T1.4.8.2.4" class="ltx_td ltx_align_left">40.7 (+0.49%)</td>
<td id="S4.T1.4.8.2.5" class="ltx_td ltx_align_left"><span id="S4.T1.4.8.2.5.1" class="ltx_text ltx_font_bold">55.6 (+0.72%)</span></td>
</tr>
<tr id="S4.T1.4.9.3" class="ltx_tr">
<th id="S4.T1.4.9.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Single w/ visual (EVA-CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>)</th>
<td id="S4.T1.4.9.3.2" class="ltx_td ltx_align_left">17.6 (+0.00%)</td>
<td id="S4.T1.4.9.3.3" class="ltx_td ltx_align_left"><span id="S4.T1.4.9.3.3.1" class="ltx_text ltx_font_bold">40.4 (+2.88%)</span></td>
<td id="S4.T1.4.9.3.4" class="ltx_td ltx_align_left">41.3 (+1.98%)</td>
<td id="S4.T1.4.9.3.5" class="ltx_td ltx_align_left">54.7 (-0.91%)</td>
</tr>
<tr id="S4.T1.4.10.4" class="ltx_tr">
<th id="S4.T1.4.10.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Single w/ visual (SigLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>)</th>
<td id="S4.T1.4.10.4.2" class="ltx_td ltx_align_left">17.3 (+1.70%)</td>
<td id="S4.T1.4.10.4.3" class="ltx_td ltx_align_left">42.3 (-1.68%)</td>
<td id="S4.T1.4.10.4.4" class="ltx_td ltx_align_left"><span id="S4.T1.4.10.4.4.1" class="ltx_text ltx_font_bold">41.4 (+2.22%)</span></td>
<td id="S4.T1.4.10.4.5" class="ltx_td ltx_align_left">54.7 (-0.91%)</td>
</tr>
<tr id="S4.T1.4.11.5" class="ltx_tr">
<th id="S4.T1.4.11.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Multi w/o visual</th>
<td id="S4.T1.4.11.5.2" class="ltx_td ltx_align_left ltx_border_t">16.4</td>
<td id="S4.T1.4.11.5.3" class="ltx_td ltx_align_left ltx_border_t">40.8</td>
<td id="S4.T1.4.11.5.4" class="ltx_td ltx_align_left ltx_border_t">42.9</td>
<td id="S4.T1.4.11.5.5" class="ltx_td ltx_align_left ltx_border_t">54.7</td>
</tr>
<tr id="S4.T1.4.12.6" class="ltx_tr">
<th id="S4.T1.4.12.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Multi w/ visual (CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>)</th>
<td id="S4.T1.4.12.6.2" class="ltx_td ltx_align_left">16.1 (+1.83%)</td>
<td id="S4.T1.4.12.6.3" class="ltx_td ltx_align_left">40.1 (+1.72%)</td>
<td id="S4.T1.4.12.6.4" class="ltx_td ltx_align_left">43.0 (+0.23%)</td>
<td id="S4.T1.4.12.6.5" class="ltx_td ltx_align_left">54.0 (-1.30%)</td>
</tr>
<tr id="S4.T1.4.13.7" class="ltx_tr">
<th id="S4.T1.4.13.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Multi w/ visual (EVA-CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>)</th>
<td id="S4.T1.4.13.7.2" class="ltx_td ltx_align_left">15.9 (+3.05%)</td>
<td id="S4.T1.4.13.7.3" class="ltx_td ltx_align_left">40.2 (+1.47%)</td>
<td id="S4.T1.4.13.7.4" class="ltx_td ltx_align_left">43.4 (+1.17%)</td>
<td id="S4.T1.4.13.7.5" class="ltx_td ltx_align_left">53.9 (-1.46%)</td>
</tr>
<tr id="S4.T1.4.14.8" class="ltx_tr">
<th id="S4.T1.4.14.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Multi w/ visual (SigLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>)</th>
<td id="S4.T1.4.14.8.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T1.4.14.8.2.1" class="ltx_text ltx_font_bold">15.7 (+4.27%)</span></td>
<td id="S4.T1.4.14.8.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T1.4.14.8.3.1" class="ltx_text ltx_font_bold">39.4 (+3.43%)</span></td>
<td id="S4.T1.4.14.8.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T1.4.14.8.4.1" class="ltx_text ltx_font_bold">43.5 (+1.40%)</span></td>
<td id="S4.T1.4.14.8.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T1.4.14.8.5.1" class="ltx_text ltx_font_bold">54.8 (+0.18%)</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S4.T2.4.1" class="ltx_text ltx_font_bold">Ablation Study.</span> Experimental AV-ASR task results comparing the visual influence on multi-task SigLIP<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> encoder. Random visual means randomly select the visual features from other video clips.</figcaption>
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.3" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S4.T2.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.1.1.2" class="ltx_tr">
<td id="S4.T2.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.1.1.1.1.2.1.1" class="ltx_text ltx_font_bold">ASR (How2)</span></td>
</tr>
<tr id="S4.T2.1.1.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">WER(<math id="S4.T2.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T2.1.1.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</td>
</tr>
</table>
</th>
<th id="S4.T2.2.2.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S4.T2.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.2.2.2.1.2" class="ltx_tr">
<td id="S4.T2.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.2.2.2.1.2.1.1" class="ltx_text ltx_font_bold">ASR (Visspeech)</span></td>
</tr>
<tr id="S4.T2.2.2.2.1.1" class="ltx_tr">
<td id="S4.T2.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">WER(<math id="S4.T2.2.2.2.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.2.2.2.1.1.1.m1.1a"><mo stretchy="false" id="S4.T2.2.2.2.1.1.1.m1.1.1" xref="S4.T2.2.2.2.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.1.1.m1.1b"><ci id="S4.T2.2.2.2.1.1.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.3.1" class="ltx_tr">
<th id="S4.T2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Multi w/o visual</th>
<td id="S4.T2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">16.4</td>
<td id="S4.T2.2.3.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">40.8</td>
</tr>
<tr id="S4.T2.2.4.2" class="ltx_tr">
<th id="S4.T2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Multi w random visual</th>
<td id="S4.T2.2.4.2.2" class="ltx_td ltx_align_center">16.4</td>
<td id="S4.T2.2.4.2.3" class="ltx_td ltx_nopad_r ltx_align_center">41.0</td>
</tr>
<tr id="S4.T2.2.5.3" class="ltx_tr">
<th id="S4.T2.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Multi w visual</th>
<td id="S4.T2.2.5.3.2" class="ltx_td ltx_align_center">15.9</td>
<td id="S4.T2.2.5.3.3" class="ltx_td ltx_nopad_r ltx_align_center">39.4</td>
</tr>
<tr id="S4.T2.2.6.4" class="ltx_tr">
<th id="S4.T2.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Multi w visual + synthetic</th>
<td id="S4.T2.2.6.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.6.4.2.1" class="ltx_text ltx_font_bold">15.7</span></td>
<td id="S4.T2.2.6.4.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T2.2.6.4.3.1" class="ltx_text ltx_font_bold">39.4</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiment Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">According to the survey of dicrete representation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, we fixed our BPE size to 3K and k-mean cluster number to 2K since How2 only contains about 300 hours of speech data. We did all of our experiment on the open-source E2E speech processing toolkit ESPNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. For the speech encoder, we select XLS-R <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> as our SSL feature extraction method. In all experiments, we use 2<math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mo id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><times id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\times</annotation></semantics></math> V100 GPUs for training our models. Our core model is based on a Decoder-only Transformer architecture, featuring 12 layers with a 768-dimensional feature space and 12 attention heads, culminating in a total of 125 million trainable parameters. For multitask training, we ensured an equal distribution of data across the different tasks. We froze the vision encoder to concentrate training on the vision-language projector and the decoder-only language model, achieving an end-to-end training process.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‣ 4 Experiments ‣ SynesLM: A Unified Approach for Audio-visual Speech Recognition and Translation via Language Model and Synthetic Data* Equal contributions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the experimental results evaluating the impact of visual features in both single-task and multi-task settings. Our methodology is evaluated across three distinct tasks: ASR, ST, and MT. Furthermore, we explore the effectiveness of three different visual encoders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> to determine which encoder best aligns with speech-text discrete representations.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">There are some notable instances where the visual modality enhances ASR task performance. As demonstrated in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Method ‣ SynesLM: A Unified Approach for Audio-visual Speech Recognition and Translation via Language Model and Synthetic Data* Equal contributions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, visual cues are instrumental in recognizing seldom-used vocabulary, especially when there is a strong correlation between the visual content and these specific words. This highlights the potential of language models to comprehend visual information and merge it with speech data for a multimodal understanding. The findings indicate that incorporating SynesLM's visual modality consistently enhances performance across all the tasks when compared to an audio-only baseline (e.g., improving the WER from 16.4% to 15.7% in a multitask setting using SigLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>). Among the visual encoders tested, CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> achieves the highest performance in single-task experiments. On the other hand, in the multitask framework, SigLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> demonstrates superior efficacy, notably achieving a 3.43% relative performance increase in the zero-shot AV-ASR on the VisSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> dataset. Furthermore, to delve deeper into the impact of visual features, we conduct an ablation study where the visual input is replaced with a random image. As shown in Table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ SynesLM: A Unified Approach for Audio-visual Speech Recognition and Translation via Language Model and Synthetic Data* Equal contributions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the WER for the random visual input scenario increases from 15.7% to 16.4%, which is the same as the performance without visual input. In addition, our synthetic data recovery technique further improves the performance from 15.9% to 15.7%, indicating that better audio-visual correlation could further benefit model performance. This outcome underscores the robustness of our model in audio-visual tasks.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison with the state-of-the-art on AV-ASR task. The <span id="S4.T3.3.1" class="ltx_text ltx_font_typewriter">train set</span> section for last two rows indicates that those methods use additional dataset other then How2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> for pre-training. <sup id="S4.T3.4.2" class="ltx_sup">†</sup> denotes initialization with OPT. Results are reported as WER (%, lower is better).</figcaption>
<table id="S4.T3.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.5.1.1" class="ltx_tr">
<th id="S4.T3.5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S4.T3.5.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<td id="S4.T3.5.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T3.5.1.1.2.1" class="ltx_text ltx_font_bold">Train Set</span></td>
<th id="S4.T3.5.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S4.T3.5.1.1.3.1" class="ltx_text ltx_font_bold">How2</span></th>
<td id="S4.T3.5.1.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt"><span id="S4.T3.5.1.1.4.1" class="ltx_text ltx_font_bold">VisSpeech</span></td>
</tr>
<tr id="S4.T3.5.2.2" class="ltx_tr">
<th id="S4.T3.5.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">How2 Base <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</th>
<td id="S4.T3.5.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">300hrs</td>
<th id="S4.T3.5.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">18.0</th>
<td id="S4.T3.5.2.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T3.5.3.3" class="ltx_tr">
<th id="S4.T3.5.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">LLD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</th>
<td id="S4.T3.5.3.3.2" class="ltx_td ltx_align_center ltx_border_r">300hrs</td>
<th id="S4.T3.5.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">16.7</th>
<td id="S4.T3.5.3.3.4" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S4.T3.5.4.4" class="ltx_tr">
<th id="S4.T3.5.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">VAT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</th>
<td id="S4.T3.5.4.4.2" class="ltx_td ltx_align_center ltx_border_r">300hrs</td>
<th id="S4.T3.5.4.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">18.0</th>
<td id="S4.T3.5.4.4.4" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S4.T3.5.5.5" class="ltx_tr">
<th id="S4.T3.5.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">MultiRes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</th>
<td id="S4.T3.5.5.5.2" class="ltx_td ltx_align_center ltx_border_r">300hrs</td>
<th id="S4.T3.5.5.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">20.5</th>
<td id="S4.T3.5.5.5.4" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S4.T3.5.6.6" class="ltx_tr">
<th id="S4.T3.5.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">AVATAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</th>
<td id="S4.T3.5.6.6.2" class="ltx_td ltx_align_center ltx_border_r">300hrs</td>
<th id="S4.T3.5.6.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.5.6.6.3.1" class="ltx_text ltx_font_bold">15.6</span></th>
<td id="S4.T3.5.6.6.4" class="ltx_td ltx_nopad_r ltx_align_center">43.4</td>
</tr>
<tr id="S4.T3.5.7.7" class="ltx_tr">
<th id="S4.T3.5.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.5.7.7.1.1" class="ltx_text ltx_font_bold">Ours<sup id="S4.T3.5.7.7.1.1.1" class="ltx_sup">†</sup></span></th>
<td id="S4.T3.5.7.7.2" class="ltx_td ltx_align_center ltx_border_r">300hrs</td>
<th id="S4.T3.5.7.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">15.7</th>
<td id="S4.T3.5.7.7.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.5.7.7.4.1" class="ltx_text ltx_font_bold">39.4</span></td>
</tr>
<tr id="S4.T3.5.8.8" class="ltx_tr">
<th id="S4.T3.5.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">AVFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</th>
<td id="S4.T3.5.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">960hrs + 6500hrs</td>
<th id="S4.T3.5.8.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">13.6</th>
<td id="S4.T3.5.8.8.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">16.6</td>
</tr>
<tr id="S4.T3.5.9.9" class="ltx_tr">
<th id="S4.T3.5.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">AVATAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</th>
<td id="S4.T3.5.9.9.2" class="ltx_td ltx_align_center ltx_border_r">300hrs + 131k hrs</td>
<th id="S4.T3.5.9.9.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">9.1</th>
<td id="S4.T3.5.9.9.4" class="ltx_td ltx_nopad_r ltx_align_center">11.3</td>
</tr>
<tr id="S4.T3.5.10.10" class="ltx_tr">
<th id="S4.T3.5.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">Prompt-whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</th>
<td id="S4.T3.5.10.10.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">680k hrs</td>
<th id="S4.T3.5.10.10.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">-</th>
<td id="S4.T3.5.10.10.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">7.16</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Compare with the SOTA methods.</span>
In Table <a href="#S4.T3" title="Table 3 ‣ 4.2 Results ‣ 4 Experiments ‣ SynesLM: A Unified Approach for Audio-visual Speech Recognition and Translation via Language Model and Synthetic Data* Equal contributions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we compare our model with state-of-the-art methods on the AV-ASR task. The results demonstrate that our approach surpasses most of the methods when utilizing only the How2 dataset. Note that, both AVATAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and AVFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> pretrain on the vast HowTo100M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> dataset, which makes them perform well explicitly on single AV-ASR task. Contrary to the design purposes of these models, we aim to explore an unified model architecture for different audio-visual related tasks. Under the multitasking scenario, our model not only retains high performance in the AV-ASR task but also surpasses the How2 baseline in ST and MT tasks. Specifically, as shown in Table <a href="#S4.T1" title="Table 1 ‣ 4 Experiments ‣ SynesLM: A Unified Approach for Audio-visual Speech Recognition and Translation via Language Model and Synthetic Data* Equal contributions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we observed a significant improvement in the BLEU score for ST, increasing from 37.2 to 43.5. Similarly, for MT, there was a modest enhancement from 54.4 to 54.8 in the BLEU score.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We introduce SynesLM, a novel multimodal language model designed for multiple audio-visual tasks. SynesLM outperforms existing single-task methods, showcasing the effective integration of audio and visual data. The experiments also reveal SynesLM's ability to synergize auditory, textual and visual information effectively. The results underscore SynesLM's proficiency across all evaluated tasks and highlight its potential for broader applications in audio-visual processing.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgements</h2>

</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:80%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:80%;">
F. N. Newell and K. J. Mitchell, ``Multisensory integration and cross-modal learning in synaesthesia: a unifying model,'' </span><em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">Neuropsychologia</em><span id="bib.bib1.3.3" class="ltx_text" style="font-size:80%;">, vol. 88, pp. 140–150, 2016.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:80%;">
R. Chiou, M. Stelter, and A. N. Rich, ``Beyond colour perception: Auditory–visual synaesthesia induces experiences of geometric objects in specific locations,'' </span><em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">Cortex</em><span id="bib.bib2.3.3" class="ltx_text" style="font-size:80%;">, vol. 49, no. 6, pp. 1750–1763, 2013.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:80%;">
B. Shi, W.-N. Hsu, K. Lakhotia, and A. Mohamed, ``Learning audio-visual speech representation by masked multimodal cluster prediction,'' in </span><em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">International Conference on Learning Representations</em><span id="bib.bib3.3.3" class="ltx_text" style="font-size:80%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:80%;">
V. Gabeur </span><em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib4.3.3" class="ltx_text" style="font-size:80%;">, ``AVATAR: Unconstrained audiovisual speech recognition,'' in </span><em id="bib.bib4.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Interspeech</em><span id="bib.bib4.5.5" class="ltx_text" style="font-size:80%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:80%;">
P. H. Seo, A. Nagrani, and C. Schmid, ``AVFormer: Injecting vision into frozen speech models for zero-shot av-asr,'' in </span><em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">CVPR</em><span id="bib.bib5.3.3" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:80%;">
H. Liu, C. Li, Q. Wu, and Y. J. Lee, ``Visual instruction tuning,'' in </span><em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">Advances in Neural Information Processing Systems</em><span id="bib.bib6.3.3" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:80%;">
J. Li, D. Li, S. Savarese, and S. Hoi, ``Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,'' </span><em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:2301.12597</em><span id="bib.bib7.3.3" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:80%;">
S. Maiti </span><em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib8.3.3" class="ltx_text" style="font-size:80%;">, ``Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks,'' in </span><em id="bib.bib8.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">ICASSP</em><span id="bib.bib8.5.5" class="ltx_text" style="font-size:80%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:80%;">
T. Wang </span><em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib9.3.3" class="ltx_text" style="font-size:80%;">, ``Viola: Unified codec language models for speech recognition, synthesis, and translation,'' </span><em id="bib.bib9.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:2305.16107</em><span id="bib.bib9.5.5" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:80%;">
J. Han </span><em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib10.3.3" class="ltx_text" style="font-size:80%;">, ``Onellm: One framework to align all modalities with language,'' in </span><em id="bib.bib10.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib10.5.5" class="ltx_text" style="font-size:80%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:80%;">
F. Chen </span><em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib11.3.3" class="ltx_text" style="font-size:80%;">, ``X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages,'' 2023.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:80%;">
R. Sanabria </span><em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib12.3.3" class="ltx_text" style="font-size:80%;">, ``How2: A large-scale dataset for multimodal language understanding,'' 2018.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:80%;">
W. Dai </span><em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib13.3.3" class="ltx_text" style="font-size:80%;">, ``Instructblip: Towards general-purpose vision-language models with instruction tuning,'' </span><em id="bib.bib13.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:2305.06500</em><span id="bib.bib13.5.5" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:80%;">
J.-B. Alayrac </span><em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib14.3.3" class="ltx_text" style="font-size:80%;">, ``Flamingo: a visual language model for few-shot learning,'' in </span><em id="bib.bib14.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Advances in Neural Information Processing Systems</em><span id="bib.bib14.5.5" class="ltx_text" style="font-size:80%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:80%;">
J. Bai </span><em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib15.3.3" class="ltx_text" style="font-size:80%;">, ``Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond,'' 2023.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:80%;">
K. Chen </span><em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib16.3.3" class="ltx_text" style="font-size:80%;">, ``Shikra: Unleashing multimodal llm's referential dialogue magic,'' </span><em id="bib.bib16.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:2306.15195</em><span id="bib.bib16.5.5" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:80%;">
J. Chen </span><em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib17.3.3" class="ltx_text" style="font-size:80%;">, ``Minigpt-v2: large language model as a unified interface for vision-language multi-task learning,'' </span><em id="bib.bib17.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:2310.09478</em><span id="bib.bib17.5.5" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:80%;">
D. Driess </span><em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib18.3.3" class="ltx_text" style="font-size:80%;">, ``Palm-e: An embodied multimodal language model,'' </span><em id="bib.bib18.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:2303.03378</em><span id="bib.bib18.5.5" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:80%;">
Z. Borsos </span><em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib19.3.3" class="ltx_text" style="font-size:80%;">, ``Audiolm: a language modeling approach to audio generation,'' in </span><em id="bib.bib19.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">ICASSP</em><span id="bib.bib19.5.5" class="ltx_text" style="font-size:80%;">.   IEEE, 2023.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:80%;">
P. K. Rubenstein </span><em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib20.3.3" class="ltx_text" style="font-size:80%;">, ``Audiopalm: A large language model that can speak and listen,'' </span><em id="bib.bib20.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:2306.12925</em><span id="bib.bib20.5.5" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:80%;">
Q. Dong </span><em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib21.3.3" class="ltx_text" style="font-size:80%;">, ``Polyvoice: Language models for speech to speech translation,'' 2023.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:80%;">
K. Lakhotia </span><em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib22.3.3" class="ltx_text" style="font-size:80%;">, ``Generative spoken language modeling from raw audio,'' 2021.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:80%;">
C. Tang </span><em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib23.3.3" class="ltx_text" style="font-size:80%;">, ``Salmonn: Towards generic hearing abilities for large language models,'' </span><em id="bib.bib23.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:2310.13289</em><span id="bib.bib23.5.5" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:80%;">
J. Wu </span><em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib24.3.3" class="ltx_text" style="font-size:80%;">, ``On decoder-only architecture for speech-to-text and large language model integration,'' in </span><em id="bib.bib24.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Automatic Speech Recognition and Understanding Workshop (ASRU)</em><span id="bib.bib24.5.5" class="ltx_text" style="font-size:80%;">, 2023, pp. 1–8.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:80%;">
Z. Chen </span><em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib25.3.3" class="ltx_text" style="font-size:80%;">, ``Salm: Speech-augmented language model with in-context learning for speech recognition and translation,'' </span><em id="bib.bib25.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:2310.09424</em><span id="bib.bib25.5.5" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:80%;">
C. Lyu </span><em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib26.3.3" class="ltx_text" style="font-size:80%;">, ``Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration,'' </span><em id="bib.bib26.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:2306.09093</em><span id="bib.bib26.5.5" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:80%;">
H. Zhang, X. Li, and L. Bing, ``Video-llama: An instruction-tuned audio-visual language model for video understanding,'' in </span><em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">EMNLP</em><span id="bib.bib27.3.3" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:80%;">
M. Kim </span><em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib28.3.3" class="ltx_text" style="font-size:80%;">, ``Tmt: Tri-modal translation between speech, image, and text by processing different modalities as different languages,'' 2024.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:80%;">
G. Sun </span><em id="bib.bib29.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib29.3.3" class="ltx_text" style="font-size:80%;">, ``video-SALMONN: Speech-enhanced audio-visual large language models,'' in </span><em id="bib.bib29.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Forty-first International Conference on Machine Learning</em><span id="bib.bib29.5.5" class="ltx_text" style="font-size:80%;">, 2024. [Online]. Available: </span><a target="_blank" href="https://openreview.net/forum?id=nYsh5GFIqX" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:80%;">https://openreview.net/forum?id=nYsh5GFIqX</a><span id="bib.bib29.6.6" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:80%;">
P. Peng, B. Yan, S. Watanabe, and D. Harwath, ``Prompting the hidden talent of web-scale speech models for zero-shot task generalization,'' in </span><em id="bib.bib30.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">Interspeech</em><span id="bib.bib30.3.3" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:80%;">
X. Chang </span><em id="bib.bib31.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib31.3.3" class="ltx_text" style="font-size:80%;">, ``Exploring speech recognition, translation, and understanding with discrete speech units: A comparative study,'' in </span><em id="bib.bib31.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">ICASSP</em><span id="bib.bib31.5.5" class="ltx_text" style="font-size:80%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:80%;">
A. Radford </span><em id="bib.bib32.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib32.3.3" class="ltx_text" style="font-size:80%;">, ``Learning transferable visual models from natural language supervision,'' in </span><em id="bib.bib32.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">International conference on machine learning</em><span id="bib.bib32.5.5" class="ltx_text" style="font-size:80%;">.   PMLR, 2021, pp. 8748–8763.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:80%;">
X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer, ``Sigmoid loss for language image pre-training,'' 2023.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:80%;">
Q. Sun </span><em id="bib.bib34.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib34.3.3" class="ltx_text" style="font-size:80%;">, ``Eva-clip: Improved training techniques for clip at scale,'' </span><em id="bib.bib34.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:2303.15389</em><span id="bib.bib34.5.5" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:80%;">
S. Zhang </span><em id="bib.bib35.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib35.3.3" class="ltx_text" style="font-size:80%;">, ``Opt: Open pre-trained transformer language models,'' </span><em id="bib.bib35.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:2205.01068</em><span id="bib.bib35.5.5" class="ltx_text" style="font-size:80%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:80%;">
M. Hassid </span><em id="bib.bib36.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib36.3.3" class="ltx_text" style="font-size:80%;">, ``Textually pretrained speech language models,'' in </span><em id="bib.bib36.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Advances in Neural Information Processing Systems</em><span id="bib.bib36.5.5" class="ltx_text" style="font-size:80%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:80%;">
W.-N. Hsu </span><em id="bib.bib37.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib37.3.3" class="ltx_text" style="font-size:80%;">, ``Hubert: Self-supervised speech representation learning by masked prediction of hidden units,'' </span><em id="bib.bib37.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em><span id="bib.bib37.5.5" class="ltx_text" style="font-size:80%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:80%;">
A. Miech </span><em id="bib.bib38.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib38.3.3" class="ltx_text" style="font-size:80%;">, ``HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips,'' in </span><em id="bib.bib38.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">ICCV</em><span id="bib.bib38.5.5" class="ltx_text" style="font-size:80%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:80%;">
R. Rombach </span><em id="bib.bib39.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib39.3.3" class="ltx_text" style="font-size:80%;">, ``High-resolution image synthesis with latent diffusion models,'' 2022.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:80%;">
S. Watanabe, H. Hori, S. Karita </span><em id="bib.bib40.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib40.3.3" class="ltx_text" style="font-size:80%;">, ``ESPnet: End-to-end speech processing toolkit,'' in </span><em id="bib.bib40.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Interspeech</em><span id="bib.bib40.5.5" class="ltx_text" style="font-size:80%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:80%;">
A. Babu </span><em id="bib.bib41.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib41.3.3" class="ltx_text" style="font-size:80%;">, ``Xls-r: Self-supervised cross-lingual speech representation learning at scale,'' </span><em id="bib.bib41.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:2111.09296</em><span id="bib.bib41.5.5" class="ltx_text" style="font-size:80%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:80%;">
S. Ghorbani, Y. Gaur, Y. Shi, and J. Li, ``Listen, look and deliberate: Visual context-aware speech recognition using pre-trained text-video representations,'' in </span><em id="bib.bib42.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">IEEE Spoken Language Technology Workshop (SLT)</em><span id="bib.bib42.3.3" class="ltx_text" style="font-size:80%;">.   IEEE, 2021, pp. 621–628.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:80%;">
O. Caglayan </span><em id="bib.bib43.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib43.3.3" class="ltx_text" style="font-size:80%;">, ``Multimodal grounding for sequence-to-sequence speech recognition,'' in </span><em id="bib.bib43.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">ICASSP</em><span id="bib.bib43.5.5" class="ltx_text" style="font-size:80%;">.   IEEE, 2019, pp. 8648–8652.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:80%;">
G. Paraskevopoulos, S. Parthasarathy, A. Khare, and S. Sundaram, ``Multimodal and multiresolution speech recognition with transformers,'' in </span><em id="bib.bib44.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em><span id="bib.bib44.3.3" class="ltx_text" style="font-size:80%;">, 2020, pp. 2381–2387.
</span>
</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.00623" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.00624" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.00624">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.00624" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.00626" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 15:32:47 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
