<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Little Human Data Goes A Long Way</title>
<!--Generated on Thu Oct 17 00:03:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.13098v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#S1" title="In A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#S2" title="In A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Synthetic Data Generation from Evidence Texts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#S3" title="In A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Can Synthetic Data Replace Humans?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#S4" title="In A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>When Should We Use Human Data?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#S5" title="In A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#S6" title="In A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#S7" title="In A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#S8" title="In A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#S9" title="In A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Ethical Considerations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#S10" title="In A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>Acknowledgements</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A1" title="In A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Supplemental Figures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A2" title="In A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Synthetic Data Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A3" title="In A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Datasets Used</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A3.SS1" title="In Appendix C Datasets Used â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Fact Verification Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A3.SS2" title="In Appendix C Datasets Used â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Question Answering Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A4" title="In A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Detailed Discussion on Differences Between Synthetic and Human Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5" title="In A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Implementation Details</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">A Little Human Data Goes A Long Way</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dhananjay Ashok<sup class="ltx_sup" id="id1.1.1"><math alttext="\dagger" class="ltx_Math" display="inline" id="id1.1.1.m1.1"><semantics id="id1.1.1.m1.1a"><mo id="id1.1.1.m1.1.1" xref="id1.1.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="id1.1.1.m1.1b"><ci id="id1.1.1.m1.1.1.cmml" xref="id1.1.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="id1.1.1.m1.1d">â€ </annotation></semantics></math></sup>, â€ƒJonathan May<sup class="ltx_sup" id="id2.2.2"><math alttext="\dagger" class="ltx_Math" display="inline" id="id2.2.2.m1.1"><semantics id="id2.2.2.m1.1a"><mo id="id2.2.2.m1.1.1" xref="id2.2.2.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="id2.2.2.m1.1b"><ci id="id2.2.2.m1.1.1.cmml" xref="id2.2.2.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.2.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="id2.2.2.m1.1d">â€ </annotation></semantics></math></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id3.3.3"><math alttext="\dagger" class="ltx_Math" display="inline" id="id3.3.3.m1.1"><semantics id="id3.3.3.m1.1a"><mo id="id3.3.3.m1.1.1" xref="id3.3.3.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="id3.3.3.m1.1b"><ci id="id3.3.3.m1.1.1.cmml" xref="id3.3.3.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="id3.3.3.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="id3.3.3.m1.1d">â€ </annotation></semantics></math></sup>Information Sciences Institute, University of Southern California
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id4.4.id1">{ashokd, jonmay}@isi.edu</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">Faced with an expensive human annotation process, creators of NLP systems increasingly turn to synthetic data generation. While this method shows promise, the extent to which synthetic data can replace human annotation is poorly understood. We investigate the use of synthetic data in Fact Verification (FV) and Question Answering (QA) by studying the effects of incrementally replacing human generated data with synthetic points on eight diverse datasets. Strikingly, replacing up to 90% of the training data only marginally decreases performance, but replacing the final 10% leads to severe declines. We find that models trained on purely synthetic data can be reliably improved by including as few as <span class="ltx_text ltx_font_bold" id="id5.id1.1">125</span> human generated data points. We show that matching the performance gain of just a little additional human data requires an order of magnitude more synthetic data, and we then estimate price ratios at which human annotation would be a more cost-effective solution. Our results suggest that even when human annotation at scale is infeasible, there is great value to having a small proportion of the dataset being human generated.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.3">
<p class="ltx_p" id="p1.3.4"><span class="ltx_text ltx_font_bold" id="p1.3.4.1">A Little Human Data Goes A Long Way</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.3.3" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.3.3.3" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.3.3.3.3">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.2.2.2.2.2">
<span class="ltx_td ltx_align_center" id="p1.2.2.2.2.2.2"><span class="ltx_text ltx_font_bold" id="p1.2.2.2.2.2.2.2">Dhananjay Ashok<sup class="ltx_sup" id="p1.1.1.1.1.1.1.1.1"><math alttext="\dagger" class="ltx_Math" display="inline" id="p1.1.1.1.1.1.1.1.1.m1.1"><semantics id="p1.1.1.1.1.1.1.1.1.m1.1a"><mo id="p1.1.1.1.1.1.1.1.1.m1.1.1" xref="p1.1.1.1.1.1.1.1.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="p1.1.1.1.1.1.1.1.1.m1.1b"><ci id="p1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="p1.1.1.1.1.1.1.1.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.1.1.1.1.1.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="p1.1.1.1.1.1.1.1.1.m1.1d">â€ </annotation></semantics></math></sup>, â€ƒJonathan May<sup class="ltx_sup" id="p1.2.2.2.2.2.2.2.2"><math alttext="\dagger" class="ltx_Math" display="inline" id="p1.2.2.2.2.2.2.2.2.m1.1"><semantics id="p1.2.2.2.2.2.2.2.2.m1.1a"><mo id="p1.2.2.2.2.2.2.2.2.m1.1.1" xref="p1.2.2.2.2.2.2.2.2.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="p1.2.2.2.2.2.2.2.2.m1.1b"><ci id="p1.2.2.2.2.2.2.2.2.m1.1.1.cmml" xref="p1.2.2.2.2.2.2.2.2.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.2.2.2.2.2.2.2.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="p1.2.2.2.2.2.2.2.2.m1.1d">â€ </annotation></semantics></math></sup></span></span></span>
<span class="ltx_tr" id="p1.3.3.3.3.3">
<span class="ltx_td ltx_align_center" id="p1.3.3.3.3.3.1"><sup class="ltx_sup" id="p1.3.3.3.3.3.1.1"><math alttext="\dagger" class="ltx_Math" display="inline" id="p1.3.3.3.3.3.1.1.m1.1"><semantics id="p1.3.3.3.3.3.1.1.m1.1a"><mo id="p1.3.3.3.3.3.1.1.m1.1.1" xref="p1.3.3.3.3.3.1.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="p1.3.3.3.3.3.1.1.m1.1b"><ci id="p1.3.3.3.3.3.1.1.m1.1.1.cmml" xref="p1.3.3.3.3.3.1.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="p1.3.3.3.3.3.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="p1.3.3.3.3.3.1.1.m1.1d">â€ </annotation></semantics></math></sup>Information Sciences Institute, University of Southern California</span></span>
<span class="ltx_tr" id="p1.3.3.3.3.4.1">
<span class="ltx_td ltx_align_center" id="p1.3.3.3.3.4.1.1"><span class="ltx_text ltx_font_typewriter" id="p1.3.3.3.3.4.1.1.1">{ashokd, jonmay}@isi.edu</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">From BERTÂ <cite class="ltx_cite ltx_citemacro_citep">(Devlin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib13" title="">2019</a>)</cite> to GPT-4Â <cite class="ltx_cite ltx_citemacro_citep">(Achiam etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib1" title="">2023</a>)</cite>, the explosive growth of language models (LMs) has been underpinned by exponential increases in the size of available training data. However, the more complex and specialized the task, the more expensive and challenging it is to collect human generated data at scaleÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib58" title="">2021</a>)</cite>. Combined with growing concerns that LMs may soon exhaust the stock of publicly available training dataÂ <cite class="ltx_cite ltx_citemacro_citep">(Villalobos etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib55" title="">2024</a>)</cite>, many turn to synthetic data generation, hoping to eliminate their reliance on human annotation.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="240" id="S1.F1.g1" src="extracted/5929974/images/full_dataset/fv.png" width="299"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Change in model performance as the proportion of synthetic points in the training data is increased. Across datasets, the performance decrease when moving from 0% to 90% synthetic data is often less than that of moving from 90% to purely synthetic data.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Synthetic data generation has long been used to increase the amount of training data availableÂ <cite class="ltx_cite ltx_citemacro_cite">Simard etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib49" title="">2002</a>); Krizhevsky etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib29" title="">2012</a>)</cite>. Early NLP approaches use rule based methodsÂ <cite class="ltx_cite ltx_citemacro_citep">(DeÂ Gispert etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib12" title="">2005</a>; Chen etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib11" title="">2012</a>)</cite>, paraphrasingÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang and Yang, <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib59" title="">2015</a>; Kobayashi, <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib28" title="">2018</a>)</cite>, noisingÂ <cite class="ltx_cite ltx_citemacro_citep">(Xie etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib68" title="">2017</a>; Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib60" title="">2018</a>)</cite>, and backtranslationÂ <cite class="ltx_cite ltx_citemacro_citep">(Sennrich etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib46" title="">2016</a>; Yu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib72" title="">2018</a>)</cite>, but are limited in their capability.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Modern LMs demonstrate the capability to solve myriad NLP tasks with minimal task specific dataÂ <cite class="ltx_cite ltx_citemacro_citep">(Brown, <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib8" title="">2020</a>; Wei etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib61" title="">2022a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib62" title="">b</a>; Ouyang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib41" title="">2022</a>; Ashok and Lipton, <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib4" title="">2023</a>)</cite>, making them more powerful synthetic data generators. Leveraging this, synthetic data approaches have seen increased use in tasksÂ <cite class="ltx_cite ltx_citemacro_citep">(Tan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib51" title="">2024</a>)</cite> such as QAÂ <cite class="ltx_cite ltx_citemacro_citep">(Wu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib67" title="">2021</a>; Yu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib73" title="">2024a</a>)</cite>, NLIÂ <cite class="ltx_cite ltx_citemacro_citep">(Meng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib37" title="">2022</a>; Ye etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib71" title="">2022</a>; Hsieh etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib26" title="">2023</a>)</cite>, text classificationÂ <cite class="ltx_cite ltx_citemacro_citep">(Ye etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib71" title="">2022</a>; Gao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib19" title="">2022</a>; Yu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib75" title="">2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib74" title="">2024b</a>)</cite>, instruction tuningÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib31" title="">2024</a>; Wei etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib63" title="">2023</a>)</cite>, evaluationÂ <cite class="ltx_cite ltx_citemacro_citep">(Dubois etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib15" title="">2024</a>; Fu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib18" title="">2023</a>)</cite>, and moreÂ <cite class="ltx_cite ltx_citemacro_citep">(Tang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib53" title="">2023</a>; Kulkarni etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib30" title="">2024</a>; Chan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib9" title="">2024</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="240" id="S1.F2.g1" src="extracted/5929974/images/zoom_5/qa.png" width="299"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Model performance as the synthetic proportion of the training data varies from 95% to 100%. Across all datasets and random seeds, having <span class="ltx_text ltx_font_bold" id="S1.F2.2.1">just 2.5%</span> of the training dataset being human generated boosts performance.</figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The adoption has been particularly enthusiastic for tasks that require the model to â€˜understandâ€™ knowledge contained in an â€˜evidence textâ€™ e.g. FVÂ <cite class="ltx_cite ltx_citemacro_citep">(Tang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib52" title="">2024</a>)</cite>, factual error correctionÂ <cite class="ltx_cite ltx_citemacro_citep">(Ashok etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib3" title="">2023</a>)</cite>, NLIÂ <cite class="ltx_cite ltx_citemacro_citep">(Hosseini etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib25" title="">2024</a>)</cite> and Context-grounded QAÂ <cite class="ltx_cite ltx_citemacro_citep">(Schimanski etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib44" title="">2024</a>)</cite>. Such tasks are of vital importance in Fake News DetectionÂ <cite class="ltx_cite ltx_citemacro_citep">(Sharma etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib47" title="">2023</a>)</cite>, Retrieval Augmented GenerationÂ <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib20" title="">2023</a>)</cite> and Dialogue SystemsÂ <cite class="ltx_cite ltx_citemacro_citep">(Weston etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib65" title="">2015</a>)</cite>. Recent datasetsÂ <cite class="ltx_cite ltx_citemacro_citep">(Wan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib57" title="">2024</a>; Wu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib67" title="">2021</a>; Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib35" title="">2022</a>; Ni etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib40" title="">2024</a>; Muhlgay etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib39" title="">2023</a>)</cite> and methodsÂ <cite class="ltx_cite ltx_citemacro_citep">(Ye etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib71" title="">2022</a>; Hsieh etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib26" title="">2023</a>; Tang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib52" title="">2024</a>; Ni etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib40" title="">2024</a>)</cite> exploit the plentiful availability of evidence texts (scientific journals, news articles, Wikipedia, books, etc.), using synthetic generation to avoid being bottlenecked by the expensive annotation procedureÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib35" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Varying results from studies across ML tasks suggest that whether completely replacing human annotation with synthetic data shows promiseÂ <cite class="ltx_cite ltx_citemacro_citep">(Fan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib16" title="">2024</a>; Hammoud etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib22" title="">2024</a>)</cite> or leads to catastrophic failuresÂ <cite class="ltx_cite ltx_citemacro_citep">(Bisbee etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib6" title="">2024</a>; Guo etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib21" title="">2023</a>)</cite> is highly task dependentÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib32" title="">2023</a>)</cite>. In this work, we focus on FV and QA, investigating the trade-offs presented by the use of synthetic data generation in these fundamental tasks.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">We study eight diverse FV and QA datasets, using their â€˜evidence textsâ€™ to generate synthetic datasets. By holding the number of data points constant but increasing the percentage of the training data that is synthetic, we can compare the utility of synthetic data to the original human generated data points. Across multiple fine-tuning models, prompt models, and prompting strategies, we find that while increasing the proportion of synthetic data typically causes only minor degradations in model performance, a significant decline occurs at the extremes; i.e., when the percentage of synthetic data exceeds 90%. Focusing on the extremes, we show that purely synthetically trained FV and QA systems can be meaningfully improved by including as few as <span class="ltx_text ltx_font_bold" id="S1.p6.1.1">125</span> human generated datapoints.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Aside from the insight that synthetic data remains inferior to human annotation, our observations have actionable implications for researchers hoping to use synthetic data for FV and QA. Our results suggest that even when human annotation at scale is infeasible, there is great value to having a small proportion of the dataset being generated by humans.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">To help guide this choice, we quantify the performance-cost tradeoff between human and synthetic data. We find that matching the performance gain of
just a little additional human data (only 200 data points) requires an order of magnitude more synthetic data points, empirically showing the per-data point price ratio at which human annotation is the more cost-effective solution.
Finally, we conduct an analysis on the differing properties of synthetic vs. human data. Among other findings, we observe that synthetic generations are longer and more extractive from the evidence texts than their human-produced counterparts.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Synthetic Data Generation from Evidence Texts</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">We study a synthetic data generation pipeline representative of the methods used in the FVÂ <cite class="ltx_cite ltx_citemacro_citep">(Ni etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib40" title="">2024</a>; He etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib23" title="">2023</a>)</cite> and QAÂ <cite class="ltx_cite ltx_citemacro_citep">(Schimanski etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib44" title="">2024</a>; Wan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib57" title="">2024</a>)</cite> literature.
Specifically, we use Few-Shot In-Context LearningÂ <cite class="ltx_cite ltx_citemacro_citep">(Brown, <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib8" title="">2020</a>)</cite> to generate synthetic (claim, label) pairs from an input evidence text. The prompt model is given examples of (evidence text, claim, label) from the real training data, and is then queried with the evidence text we seek to generate data for (QA synthetic data is generated analogously, see additional details in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A2" title="Appendix B Synthetic Data Generation â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">B</span></a>).</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">In total, we use 4 FV/NLI datasets: FEVERÂ <cite class="ltx_cite ltx_citemacro_citep">(Thorne etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib54" title="">2018</a>)</cite>, SciFactÂ <cite class="ltx_cite ltx_citemacro_citep">(Wadden etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib56" title="">2020</a>)</cite>, WANLIÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib35" title="">2022</a>)</cite> and FACTIFY1.0Â <cite class="ltx_cite ltx_citemacro_citep">(Mishra etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib38" title="">2022</a>)</cite>, as well as 4 QA datasets: ROPESÂ <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib34" title="">2019</a>)</cite>, CoQAÂ <cite class="ltx_cite ltx_citemacro_citep">(Reddy etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib43" title="">2019</a>)</cite>, QAConvÂ <cite class="ltx_cite ltx_citemacro_citep">(Wu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib67" title="">2021</a>)</cite> and FairyTaleQAÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib69" title="">2022</a>)</cite>. Together, the datasets span a variety of domains (science, news, social media, reasoning, conversation, fiction). More details, including a discussion on data leakage, can be found in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A3" title="Appendix C Datasets Used â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">FV performance is measured by test accuracy, while QA is measured using Exact Match, String Inclusion, BLEUÂ <cite class="ltx_cite ltx_citemacro_citep">(Papineni etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib42" title="">2002</a>)</cite>, ROUGE-1Â <cite class="ltx_cite ltx_citemacro_citep">(Lin, <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib33" title="">2004</a>)</cite> and BERTScoreÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang* etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib77" title="">2020</a>)</cite>. Evaluation is always conducted on the (human-generated) test split of each dataset.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">We use GPT-3.5-TurboÂ <cite class="ltx_cite ltx_citemacro_citep">(Brown, <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib8" title="">2020</a>)</cite> for prompting and LoRAÂ <cite class="ltx_cite ltx_citemacro_citep">(Hu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib27" title="">2022</a>)</cite> on Llama3-8BÂ <cite class="ltx_cite ltx_citemacro_citep">(Dubey etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib14" title="">2024</a>)</cite> for fine-tuning. Implementation details are provided below (AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5" title="Appendix E Implementation Details â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">E</span></a>).<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/DhananjayAshok/LittleHumanData/" title="">https://github.com/DhananjayAshok/LittleHumanData/</a></span></span></span></p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Can Synthetic Data Replace Humans?</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We investigate the potential of synthetic data to replace human annotation by holding the number of training data points fixed, incrementally increasing the proportion of the data that is synthetic, and fine-tuning a model on each training set.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="225" id="S3.F3.g1" src="extracted/5929974/images/money/wanli.png" width="299"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Adding 200 real data points is as effective as adding an order of magnitude more synthetic data points.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.p2.1.1">Results:</span>
Across all datasets, using purely synthetic data typically leads to worse performance than the same amount of real data (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">1</span></a>).
This confirms that despite advances in synthetic generation, human annotation yields more useful data.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">The performance decline is not uniform as we increase the synthetic proportion. On all datasets, there is only a minor degradation up until 90% replacement, after which the performance drops considerably. We zoom in on the 90%-100% interval, fixing the amount of training data at <math alttext="n=5000" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><mrow id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><mi id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml">n</mi><mo id="S3.p3.1.m1.1.1.1" xref="S3.p3.1.m1.1.1.1.cmml">=</mo><mn id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml">5000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><eq id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.1.1"></eq><ci id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2">ğ‘›</ci><cn id="S3.p3.1.m1.1.1.3.cmml" type="integer" xref="S3.p3.1.m1.1.1.3">5000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">n=5000</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">italic_n = 5000</annotation></semantics></math> (500 for SciFact) and training on datasets with 95%, 97.5% and 100% synthetic data (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">2</span></a>).
Surprisingly, the results show that across all datasets (and 3 random seeds), there is a significant difference between the performance of models on 97.5% and 100% synthetic data. These trends hold robustly over choice of fine-tuning model (Mistral7B), prompt model (GPT4), prompting strategy (Chain-of-Thought) and data scale (AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A1" title="Appendix A Supplemental Figures â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">A</span></a>). The same trend is seen in all cases; when using synthetic training sets, the addition of just <span class="ltx_text ltx_font_bold" id="S3.p3.1.1">125</span> (2.5% of 5000) human generated datapoints reliably improved the performance of FV and QA models.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>When Should We Use Human Data?</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.5">Having observed the disproportionate value added by human data, we ask what the relative cost between human and synthetic data generation must be for us to prefer one over the other. We fine tune models on purely synthetic datasets of varying sizes, and establish the synthetic baseline by fitting a curve of the form <math alttext="y=a_{0}+a_{1}\log(x)" class="ltx_Math" display="inline" id="S4.p1.1.m1.2"><semantics id="S4.p1.1.m1.2a"><mrow id="S4.p1.1.m1.2.3" xref="S4.p1.1.m1.2.3.cmml"><mi id="S4.p1.1.m1.2.3.2" xref="S4.p1.1.m1.2.3.2.cmml">y</mi><mo id="S4.p1.1.m1.2.3.1" xref="S4.p1.1.m1.2.3.1.cmml">=</mo><mrow id="S4.p1.1.m1.2.3.3" xref="S4.p1.1.m1.2.3.3.cmml"><msub id="S4.p1.1.m1.2.3.3.2" xref="S4.p1.1.m1.2.3.3.2.cmml"><mi id="S4.p1.1.m1.2.3.3.2.2" xref="S4.p1.1.m1.2.3.3.2.2.cmml">a</mi><mn id="S4.p1.1.m1.2.3.3.2.3" xref="S4.p1.1.m1.2.3.3.2.3.cmml">0</mn></msub><mo id="S4.p1.1.m1.2.3.3.1" xref="S4.p1.1.m1.2.3.3.1.cmml">+</mo><mrow id="S4.p1.1.m1.2.3.3.3" xref="S4.p1.1.m1.2.3.3.3.cmml"><msub id="S4.p1.1.m1.2.3.3.3.2" xref="S4.p1.1.m1.2.3.3.3.2.cmml"><mi id="S4.p1.1.m1.2.3.3.3.2.2" xref="S4.p1.1.m1.2.3.3.3.2.2.cmml">a</mi><mn id="S4.p1.1.m1.2.3.3.3.2.3" xref="S4.p1.1.m1.2.3.3.3.2.3.cmml">1</mn></msub><mo id="S4.p1.1.m1.2.3.3.3.1" lspace="0.167em" xref="S4.p1.1.m1.2.3.3.3.1.cmml">â¢</mo><mrow id="S4.p1.1.m1.2.3.3.3.3.2" xref="S4.p1.1.m1.2.3.3.3.3.1.cmml"><mi id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">log</mi><mo id="S4.p1.1.m1.2.3.3.3.3.2a" xref="S4.p1.1.m1.2.3.3.3.3.1.cmml">â¡</mo><mrow id="S4.p1.1.m1.2.3.3.3.3.2.1" xref="S4.p1.1.m1.2.3.3.3.3.1.cmml"><mo id="S4.p1.1.m1.2.3.3.3.3.2.1.1" stretchy="false" xref="S4.p1.1.m1.2.3.3.3.3.1.cmml">(</mo><mi id="S4.p1.1.m1.2.2" xref="S4.p1.1.m1.2.2.cmml">x</mi><mo id="S4.p1.1.m1.2.3.3.3.3.2.1.2" stretchy="false" xref="S4.p1.1.m1.2.3.3.3.3.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.2b"><apply id="S4.p1.1.m1.2.3.cmml" xref="S4.p1.1.m1.2.3"><eq id="S4.p1.1.m1.2.3.1.cmml" xref="S4.p1.1.m1.2.3.1"></eq><ci id="S4.p1.1.m1.2.3.2.cmml" xref="S4.p1.1.m1.2.3.2">ğ‘¦</ci><apply id="S4.p1.1.m1.2.3.3.cmml" xref="S4.p1.1.m1.2.3.3"><plus id="S4.p1.1.m1.2.3.3.1.cmml" xref="S4.p1.1.m1.2.3.3.1"></plus><apply id="S4.p1.1.m1.2.3.3.2.cmml" xref="S4.p1.1.m1.2.3.3.2"><csymbol cd="ambiguous" id="S4.p1.1.m1.2.3.3.2.1.cmml" xref="S4.p1.1.m1.2.3.3.2">subscript</csymbol><ci id="S4.p1.1.m1.2.3.3.2.2.cmml" xref="S4.p1.1.m1.2.3.3.2.2">ğ‘</ci><cn id="S4.p1.1.m1.2.3.3.2.3.cmml" type="integer" xref="S4.p1.1.m1.2.3.3.2.3">0</cn></apply><apply id="S4.p1.1.m1.2.3.3.3.cmml" xref="S4.p1.1.m1.2.3.3.3"><times id="S4.p1.1.m1.2.3.3.3.1.cmml" xref="S4.p1.1.m1.2.3.3.3.1"></times><apply id="S4.p1.1.m1.2.3.3.3.2.cmml" xref="S4.p1.1.m1.2.3.3.3.2"><csymbol cd="ambiguous" id="S4.p1.1.m1.2.3.3.3.2.1.cmml" xref="S4.p1.1.m1.2.3.3.3.2">subscript</csymbol><ci id="S4.p1.1.m1.2.3.3.3.2.2.cmml" xref="S4.p1.1.m1.2.3.3.3.2.2">ğ‘</ci><cn id="S4.p1.1.m1.2.3.3.3.2.3.cmml" type="integer" xref="S4.p1.1.m1.2.3.3.3.2.3">1</cn></apply><apply id="S4.p1.1.m1.2.3.3.3.3.1.cmml" xref="S4.p1.1.m1.2.3.3.3.3.2"><log id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"></log><ci id="S4.p1.1.m1.2.2.cmml" xref="S4.p1.1.m1.2.2">ğ‘¥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.2c">y=a_{0}+a_{1}\log(x)</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.2d">italic_y = italic_a start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT roman_log ( italic_x )</annotation></semantics></math> where <math alttext="x" class="ltx_Math" display="inline" id="S4.p1.2.m2.1"><semantics id="S4.p1.2.m2.1a"><mi id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><ci id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.p1.2.m2.1d">italic_x</annotation></semantics></math> is the size of the synthetic dataset and <math alttext="y" class="ltx_Math" display="inline" id="S4.p1.3.m3.1"><semantics id="S4.p1.3.m3.1a"><mi id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><ci id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">y</annotation><annotation encoding="application/x-llamapun" id="S4.p1.3.m3.1d">italic_y</annotation></semantics></math> is the performance. We then take the synthetic training sets with {1000, 2000 â€¦} points and observe the performance (<math alttext="y^{*}" class="ltx_Math" display="inline" id="S4.p1.4.m4.1"><semantics id="S4.p1.4.m4.1a"><msup id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml"><mi id="S4.p1.4.m4.1.1.2" xref="S4.p1.4.m4.1.1.2.cmml">y</mi><mo id="S4.p1.4.m4.1.1.3" xref="S4.p1.4.m4.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><apply id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p1.4.m4.1.1.1.cmml" xref="S4.p1.4.m4.1.1">superscript</csymbol><ci id="S4.p1.4.m4.1.1.2.cmml" xref="S4.p1.4.m4.1.1.2">ğ‘¦</ci><times id="S4.p1.4.m4.1.1.3.cmml" xref="S4.p1.4.m4.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">y^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.4.m4.1d">italic_y start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT</annotation></semantics></math>) when we add 200 human data points. <math alttext="\exp({\frac{y^{*}-a_{0}}{a1}})" class="ltx_Math" display="inline" id="S4.p1.5.m5.2"><semantics id="S4.p1.5.m5.2a"><mrow id="S4.p1.5.m5.2.3.2" xref="S4.p1.5.m5.2.3.1.cmml"><mi id="S4.p1.5.m5.1.1" xref="S4.p1.5.m5.1.1.cmml">exp</mi><mo id="S4.p1.5.m5.2.3.2a" xref="S4.p1.5.m5.2.3.1.cmml">â¡</mo><mrow id="S4.p1.5.m5.2.3.2.1" xref="S4.p1.5.m5.2.3.1.cmml"><mo id="S4.p1.5.m5.2.3.2.1.1" stretchy="false" xref="S4.p1.5.m5.2.3.1.cmml">(</mo><mfrac id="S4.p1.5.m5.2.2" xref="S4.p1.5.m5.2.2.cmml"><mrow id="S4.p1.5.m5.2.2.2" xref="S4.p1.5.m5.2.2.2.cmml"><msup id="S4.p1.5.m5.2.2.2.2" xref="S4.p1.5.m5.2.2.2.2.cmml"><mi id="S4.p1.5.m5.2.2.2.2.2" xref="S4.p1.5.m5.2.2.2.2.2.cmml">y</mi><mo id="S4.p1.5.m5.2.2.2.2.3" xref="S4.p1.5.m5.2.2.2.2.3.cmml">âˆ—</mo></msup><mo id="S4.p1.5.m5.2.2.2.1" xref="S4.p1.5.m5.2.2.2.1.cmml">âˆ’</mo><msub id="S4.p1.5.m5.2.2.2.3" xref="S4.p1.5.m5.2.2.2.3.cmml"><mi id="S4.p1.5.m5.2.2.2.3.2" xref="S4.p1.5.m5.2.2.2.3.2.cmml">a</mi><mn id="S4.p1.5.m5.2.2.2.3.3" xref="S4.p1.5.m5.2.2.2.3.3.cmml">0</mn></msub></mrow><mrow id="S4.p1.5.m5.2.2.3" xref="S4.p1.5.m5.2.2.3.cmml"><mi id="S4.p1.5.m5.2.2.3.2" xref="S4.p1.5.m5.2.2.3.2.cmml">a</mi><mo id="S4.p1.5.m5.2.2.3.1" xref="S4.p1.5.m5.2.2.3.1.cmml">â¢</mo><mn id="S4.p1.5.m5.2.2.3.3" xref="S4.p1.5.m5.2.2.3.3.cmml">1</mn></mrow></mfrac><mo id="S4.p1.5.m5.2.3.2.1.2" stretchy="false" xref="S4.p1.5.m5.2.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.5.m5.2b"><apply id="S4.p1.5.m5.2.3.1.cmml" xref="S4.p1.5.m5.2.3.2"><exp id="S4.p1.5.m5.1.1.cmml" xref="S4.p1.5.m5.1.1"></exp><apply id="S4.p1.5.m5.2.2.cmml" xref="S4.p1.5.m5.2.2"><divide id="S4.p1.5.m5.2.2.1.cmml" xref="S4.p1.5.m5.2.2"></divide><apply id="S4.p1.5.m5.2.2.2.cmml" xref="S4.p1.5.m5.2.2.2"><minus id="S4.p1.5.m5.2.2.2.1.cmml" xref="S4.p1.5.m5.2.2.2.1"></minus><apply id="S4.p1.5.m5.2.2.2.2.cmml" xref="S4.p1.5.m5.2.2.2.2"><csymbol cd="ambiguous" id="S4.p1.5.m5.2.2.2.2.1.cmml" xref="S4.p1.5.m5.2.2.2.2">superscript</csymbol><ci id="S4.p1.5.m5.2.2.2.2.2.cmml" xref="S4.p1.5.m5.2.2.2.2.2">ğ‘¦</ci><times id="S4.p1.5.m5.2.2.2.2.3.cmml" xref="S4.p1.5.m5.2.2.2.2.3"></times></apply><apply id="S4.p1.5.m5.2.2.2.3.cmml" xref="S4.p1.5.m5.2.2.2.3"><csymbol cd="ambiguous" id="S4.p1.5.m5.2.2.2.3.1.cmml" xref="S4.p1.5.m5.2.2.2.3">subscript</csymbol><ci id="S4.p1.5.m5.2.2.2.3.2.cmml" xref="S4.p1.5.m5.2.2.2.3.2">ğ‘</ci><cn id="S4.p1.5.m5.2.2.2.3.3.cmml" type="integer" xref="S4.p1.5.m5.2.2.2.3.3">0</cn></apply></apply><apply id="S4.p1.5.m5.2.2.3.cmml" xref="S4.p1.5.m5.2.2.3"><times id="S4.p1.5.m5.2.2.3.1.cmml" xref="S4.p1.5.m5.2.2.3.1"></times><ci id="S4.p1.5.m5.2.2.3.2.cmml" xref="S4.p1.5.m5.2.2.3.2">ğ‘</ci><cn id="S4.p1.5.m5.2.2.3.3.cmml" type="integer" xref="S4.p1.5.m5.2.2.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.5.m5.2c">\exp({\frac{y^{*}-a_{0}}{a1}})</annotation><annotation encoding="application/x-llamapun" id="S4.p1.5.m5.2d">roman_exp ( divide start_ARG italic_y start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT - italic_a start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG start_ARG italic_a 1 end_ARG )</annotation></semantics></math> is then the size of the purely synthetic dataset that achieves equivalent performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.4"><span class="ltx_text ltx_font_bold" id="S4.p2.4.1">Results:</span>
Across all datasets, adding <span class="ltx_text ltx_font_bold" id="S4.p2.4.2">200</span> human data points is comparable to adding at least an order of magnitude (often multiple orders of magnitude) more synthetic data points. On WANLI (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#S3.F3" title="Figure 3 â€£ 3 Can Synthetic Data Replace Humans? â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">3</span></a>), more than <math alttext="17000" class="ltx_Math" display="inline" id="S4.p2.1.m1.1"><semantics id="S4.p2.1.m1.1a"><mn id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">17000</mn><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><cn id="S4.p2.1.m1.1.1.cmml" type="integer" xref="S4.p2.1.m1.1.1">17000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">17000</annotation><annotation encoding="application/x-llamapun" id="S4.p2.1.m1.1d">17000</annotation></semantics></math> additional synthetic points are needed to achieve the performance gains of <math alttext="200" class="ltx_Math" display="inline" id="S4.p2.2.m2.1"><semantics id="S4.p2.2.m2.1a"><mn id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><cn id="S4.p2.2.m2.1.1.cmml" type="integer" xref="S4.p2.2.m2.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">200</annotation><annotation encoding="application/x-llamapun" id="S4.p2.2.m2.1d">200</annotation></semantics></math> human points. If the price of a synthetic point for WANLI exceeds <math alttext="73" class="ltx_Math" display="inline" id="S4.p2.3.m3.1"><semantics id="S4.p2.3.m3.1a"><mn id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml">73</mn><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><cn id="S4.p2.3.m3.1.1.cmml" type="integer" xref="S4.p2.3.m3.1.1">73</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">73</annotation><annotation encoding="application/x-llamapun" id="S4.p2.3.m3.1d">73</annotation></semantics></math> times the price of a human generated point, then an incremental amount of human annotation would be the more cost-effective solution. In the extreme case, the equation learned on FairyTaleQA suggests that it takes <math alttext="2e5" class="ltx_Math" display="inline" id="S4.p2.4.m4.1"><semantics id="S4.p2.4.m4.1a"><mrow id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml"><mn id="S4.p2.4.m4.1.1.2" xref="S4.p2.4.m4.1.1.2.cmml">2</mn><mo id="S4.p2.4.m4.1.1.1" xref="S4.p2.4.m4.1.1.1.cmml">â¢</mo><mi id="S4.p2.4.m4.1.1.3" xref="S4.p2.4.m4.1.1.3.cmml">e</mi><mo id="S4.p2.4.m4.1.1.1a" xref="S4.p2.4.m4.1.1.1.cmml">â¢</mo><mn id="S4.p2.4.m4.1.1.4" xref="S4.p2.4.m4.1.1.4.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><apply id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1"><times id="S4.p2.4.m4.1.1.1.cmml" xref="S4.p2.4.m4.1.1.1"></times><cn id="S4.p2.4.m4.1.1.2.cmml" type="integer" xref="S4.p2.4.m4.1.1.2">2</cn><ci id="S4.p2.4.m4.1.1.3.cmml" xref="S4.p2.4.m4.1.1.3">ğ‘’</ci><cn id="S4.p2.4.m4.1.1.4.cmml" type="integer" xref="S4.p2.4.m4.1.1.4">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">2e5</annotation><annotation encoding="application/x-llamapun" id="S4.p2.4.m4.1d">2 italic_e 5</annotation></semantics></math> additional synthetic points to match the performance gain of 200 additional real data points. Rather than interpret these numbers literally, we take them to suggest that human data could have unique value in some settings, enabling performance levels that are impossible with purely synthetic datasets. See below (AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A1" title="Appendix A Supplemental Figures â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">A</span></a>) for more results and details.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="225" id="S5.F4.g1" src="extracted/5929974/images/analysis/gpt-3.5-turbo/cot_False/coqa/generated/question_length.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Synthetic questions are longer than human generated ones, a trend also seen in answers.</figcaption>
</figure>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">A deeper analysis of the difference between real and synthetic data exposes interesting trends. Our analysis shows that synthetic data generation produced claims of comparable length to the real datasets, however synthetic questions and answers tended to be longer than human-generated counterparts for all QA datasets (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#S5.F4" title="Figure 4 â€£ 5 Discussion â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">4</span></a>). We find that synthetic generations have a higher n-gram overlap with the evidence sentences. This suggests that synthetic data generation produces data points that are more directly taken from the evidence texts, while humans are more likely to employ rephrasing or different vocabulary than the evidence texts. Surprisingly, we find that synthetic data generation chooses more diverse sources for the question and answer content, with human annotation overwhelmingly more likely to create questions whose answers lie in the start of the evidence texts. We include a detailed discussion below (AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A4" title="Appendix D Detailed Discussion on Differences Between Synthetic and Human Data â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">D</span></a>).</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The replacement of human annotation with synthetic data is extensively studied in the pretraining stage of LMs, where results consistently showÂ <cite class="ltx_cite ltx_citemacro_citep">(Shumailov etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib48" title="">2023</a>; Seddik etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib45" title="">2024</a>; Guo etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib21" title="">2023</a>; Briesch etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib7" title="">2023</a>)</cite> catastrophic forgetting, mode collapse, and performance deterioration.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">While our results show that purely synthetic datasets are relatively worse than mixed datasets, relying only on synthetic data still achieves reasonable performance across all tasks. This suggests that the usage of exclusively synthetic data poses fewer risks in a setting where generations are still grounded in diverse natural â€˜evidence texts.â€™</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Interestingly, conclusions which confirm our findings are found more in the image and multimodal domains, where recent workÂ <cite class="ltx_cite ltx_citemacro_citep">(Singh etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib50" title="">2024</a>; He etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib23" title="">2023</a>; Fan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib16" title="">2024</a>)</cite> finds that synthetic data holds promise, but must be used in conjunction with human data to mitigate its harms.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">There is limited work on understanding whether synthetic data can replace human annotation in a task-specific setting for the language domain.Â <cite class="ltx_cite ltx_citemacro_citet">Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib32" title="">2023</a>)</cite> categorize text classification tasks by subjectivity, showing that synthetic data is less useful when tasks are more subjective. This draws them to focus on different tasks (sentiment classification, relation extraction and spam detection), and they do not study using a mixture of real and synthetic data.
Â <cite class="ltx_cite ltx_citemacro_citet">Bisbee etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib6" title="">2024</a>)</cite> demonstrate that replacing political survey respondents with LMs produces unreliable results, while Â <cite class="ltx_cite ltx_citemacro_citet">Ahmed etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib2" title="">2024</a>)</cite> find that there are specific subtasks in the annotation of software engineering artefacts where synthetic data approaches human performance. Â <cite class="ltx_cite ltx_citemacro_citet">Chen etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib10" title="">2024</a>)</cite> show that instruction-following capabilities are diminished when using data from GPT-4, and present a machine unlearning approach to mitigate this. The diversity of results when evaluating the impact of using purely synthetic data confirms that the feasibility of replacing human annotation with synthetic data is highly task dependant. This work deepens our understanding of the problem by being the first to study whether synthetic data can replace human annotation on the fundamental tasks of fact verification, question answering, and natural language inference.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Showing impressive performance when human data is scarce, synthetic data generation seems poised to remain a key method in FV and QA. Our work sheds light on how the best way to use this method is in conjunction with human data. We show that a little human data goes a long way, with just <span class="ltx_text ltx_font_bold" id="S7.p1.1.1">125</span> points being enough to see reliable gains on all datasets studied. With practical considerations in mind, we show that the alternative to small amounts of additional human data can be an order of magnitude more of synthetic data, suggesting that at times human annotation can be cost-effective relative to synthetic generation. We hope these results better inform design decisions on datasets and methods for fact verification and question answering.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Limitations</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">All datasets studied in this work are in the English language, hence limiting our scope of investigation. We hope future work can explore whether similar claims can be made regarding the impact of replacing human annotation with synthetic data across different languages. We also have a limited ability to control for dataset leakage, with only one dataset from each of the tasks that is surely not leaked to GPT-3.5 (and, even these two datasets may have been seen by GPT-4). This can potentially bias the results in favor of synthetic data. Due to the scarcity of suitable available datasets (i.e., ones that have not been exposed to the prompt models) we are prevented from studying the problem more rigorously. Another limitation is that while we are able to identify clear differences between synthetic vs. real data distributions, our analysis of the errors made by models trained on 0% vs. 100% synthetic data failed to yield any generalizable insights that could inform modelling approaches. A more fine-grained study of the effect of using synthetic data on the behaviour of the downstream model is hence left as a subject of future research. This work focuses on the value of human-labelled data, but due to the nature of benchmarks, the human-labelled <span class="ltx_text ltx_font_italic" id="S8.p1.1.1">test</span> data is drawn from the same distribution as our treatment data. Future work should verify that these results hold under <span class="ltx_text ltx_font_italic" id="S8.p1.1.2">novel</span> human-generated examples, which is beyond the scope of this investigation.</p>
</div>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Ethical Considerations</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">The usage of synthetic data has several important ethical considerations. In the era of LMs trained on internet-wide corpora having poor documentation as to their exact data sources, it becomes challenging to ensure the privacy of individuals whose data may be obtainable via a public crawlÂ <cite class="ltx_cite ltx_citemacro_citep">(Yao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib70" title="">2024</a>)</cite>. Additionally, models trained on massive internet-based data sources may contain implicit biases, illegal and/or highly offensive material that is hard to audit and cleanÂ <cite class="ltx_cite ltx_citemacro_citep">(Bender etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib5" title="">2021</a>)</cite>. This data affects the synthetic data obtained from prompt models, and could unknowingly impose cultural or ethical viewpoints that are unintended or not well aligned with the use case in mind. Specifically, prior work has shown that one of the prompt models studied in this work, GPT-3.5, often disagrees with humans on key ethical questionsÂ <cite class="ltx_cite ltx_citemacro_citep">(Felkner etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib17" title="">2024</a>)</cite>. The endeavour to completely replace human annotation with synthetic data generation also has key implications on the extent to which the field of NLP employs human annotators. It is possible that an increasing reliance on purely synthetic data reduces the demand for human annotation, which would place a downward pressure on the working standards and compensation awarded to the remaining human annotatorsÂ <cite class="ltx_cite ltx_citemacro_citep">(Weidinger etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib64" title="">2022</a>)</cite>. We argue in this work that we should not try to eliminate human annotation from our dataset and method design, showing that their work contributes uniquely helpful data points.</p>
</div>
</section>
<section class="ltx_section" id="S10">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Acknowledgements</h2>
<div class="ltx_para" id="S10.p1">
<p class="ltx_p" id="S10.p1.1">This work was funded by the Defense Advanced Research Projects Agency with award HR00112220046.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam etÂ al. (2023)</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, FlorenciaÂ Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, etÂ al. 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2303.08774</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahmed etÂ al. (2024)</span>
<span class="ltx_bibblock">
Toufique Ahmed, Prem Devanbu, Christoph Treude, and Michael Pradel. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:271855028" title="">Can llms replace manual annotation of software engineering artifacts?</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">ArXiv</em>, abs/2408.05534.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ashok etÂ al. (2023)</span>
<span class="ltx_bibblock">
Dhananjay Ashok, Atharva Kulkarni, Hai Pham, and Barnabas Poczos. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-emnlp.451" title="">The student becomes the master: Outperforming GPT3 on scientific factual error correction</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 6762â€“6778, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ashok and Lipton (2023)</span>
<span class="ltx_bibblock">
Dhananjay Ashok and ZacharyÂ C Lipton. 2023.

</span>
<span class="ltx_bibblock">Promptner: Prompting for named entity recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2305.15444</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bender etÂ al. (2021)</span>
<span class="ltx_bibblock">
EmilyÂ M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021.

</span>
<span class="ltx_bibblock">On the dangers of stochastic parrots: Can language models be too big?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</em>, pages 610â€“623.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisbee etÂ al. (2024)</span>
<span class="ltx_bibblock">
James Bisbee, JoshuaÂ D. Clinton, Cassy Dorff, Brenton Kenkel, and JenniferÂ M. Larson. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:269845858" title="">Synthetic replacements for human survey data? the perils of large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Political Analysis</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Briesch etÂ al. (2023)</span>
<span class="ltx_bibblock">
Martin Briesch, Dominik Sobania, and Franz Rothlauf. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:265466007" title="">Large language models suffer from their own output: An analysis of the self-consuming training loop</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">ArXiv</em>, abs/2311.16822.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown (2020)</span>
<span class="ltx_bibblock">
TomÂ B Brown. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2005.14165</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan etÂ al. (2024)</span>
<span class="ltx_bibblock">
Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:270845490" title="">Scaling synthetic data creation with 1,000,000,000 personas</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">ArXiv</em>, abs/2406.20094.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2024)</span>
<span class="ltx_bibblock">
Jie Chen, Yupeng Zhang, Bingning Wang, WayneÂ Xin Zhao, Ji-Rong Wen, and Weipeng Chen. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:270562788" title="">Unveiling the flaws: Exploring imperfections in synthetic data and mitigation strategies for large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">ArXiv</em>, abs/2406.12397.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2012)</span>
<span class="ltx_bibblock">
Mei-Hua Chen, Shih-Ting Huang, Chung-Chi Huang, Hsien-Chin Liou, and JasonÂ S Chang. 2012.

</span>
<span class="ltx_bibblock">Prefer: using a graph-based approach to generate paraphrases for language learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the Seventh Workshop on Building Educational Applications Using NLP</em>, pages 80â€“85.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeÂ Gispert etÂ al. (2005)</span>
<span class="ltx_bibblock">
Adria DeÂ Gispert, JosÃ©Â B MariÃ±o, and JosepÂ Maria Crego. 2005.

</span>
<span class="ltx_bibblock">Improving statistical machine translation by classifying and generalizing inflected verb forms.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">INTERSPEECH</em>, pages 3193â€“3196.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin etÂ al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1423" title="">BERT: Pre-training of deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171â€“4186, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey etÂ al. (2024)</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, etÂ al. 2024.

</span>
<span class="ltx_bibblock">The llama 3 herd of models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2407.21783</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubois etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yann Dubois, ChenÂ Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, PercyÂ S Liang, and TatsunoriÂ B Hashimoto. 2024.

</span>
<span class="ltx_bibblock">Alpacafarm: A simulation framework for methods that learn from human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan etÂ al. (2024)</span>
<span class="ltx_bibblock">
Lijie Fan, Kaifeng Chen, Dilip Krishnan, Dina Katabi, Phillip Isola, and Yonglong Tian. 2024.

</span>
<span class="ltx_bibblock">Scaling laws of synthetic images for model trainingâ€¦ for now.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 7382â€“7392.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Felkner etÂ al. (2024)</span>
<span class="ltx_bibblock">
Virginia Felkner, Jennifer Thompson, and Jonathan May. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2024.acl-long.760" title="">GPT is not an annotator: The necessity of human annotation in fairness benchmark construction</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 14104â€“14115, Bangkok, Thailand. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:256662188" title="">Gptscore: Evaluate as you desire</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">North American Chapter of the Association for Computational Linguistics</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jiahui Gao, Renjie Pi, Yong Lin, Hang Xu, Jiacheng Ye, Zhiyong Wu, Weizhong Zhang, Xiaodan Liang, Zhenguo Li, and Lingpeng Kong. 2022.

</span>
<span class="ltx_bibblock">Self-guided noise-free data generation for efficient zero-shot learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2205.12679</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, YiÂ Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:266359151" title="">Retrieval-augmented generation for large language models: A survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">ArXiv</em>, abs/2312.10997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yanzhu Guo, Guokan Shang, Michalis Vazirgiannis, and ChloÃ© Clavel. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:265221240" title="">The curious decline of linguistic diversity: Training language models on synthetic text</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">NAACL-HLT</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hammoud etÂ al. (2024)</span>
<span class="ltx_bibblock">
Hasan Hammoud, Hani Itani, Fabio Pizzati, Philip H.Â S. Torr, Adel Bibi, and Bernard Ghanem. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:267411953" title="">Synthclip: Are we ready for a fully synthetic clip training?</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">ArXiv</em>, abs/2402.01832.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and Xiaojuan Qi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=nUmCcZ5RKF" title="">Is synthetic data from generative models ready for image recognition?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">The Eleventh International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honovich etÂ al. (2022)</span>
<span class="ltx_bibblock">
OrÂ Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. 2022.

</span>
<span class="ltx_bibblock">True: Re-evaluating factual consistency evaluation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 3905â€“3920.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hosseini etÂ al. (2024)</span>
<span class="ltx_bibblock">
MohammadÂ Javad Hosseini, Andrey Petrov, Alex Fabrikant, and Annie Louis. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:267750265" title="">A synthetic data approach for domain generalization of nli models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Annual Meeting of the Association for Computational Linguistics</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsieh etÂ al. (2023)</span>
<span class="ltx_bibblock">
Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023.

</span>
<span class="ltx_bibblock">Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 8003â€“8017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2022)</span>
<span class="ltx_bibblock">
EdwardÂ J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, LuÂ Wang, and Weizhu Chen. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="">LoRA: Low-rank adaptation of large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kobayashi (2018)</span>
<span class="ltx_bibblock">
Sosuke Kobayashi. 2018.

</span>
<span class="ltx_bibblock">Contextual augmentation: Data augmentation by words with paradigmatic relations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</em>, pages 452â€“457.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky etÂ al. (2012)</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Ilya Sutskever, and GeoffreyÂ E Hinton. 2012.

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Advances in neural information processing systems</em>, 25.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kulkarni etÂ al. (2024)</span>
<span class="ltx_bibblock">
Atharva Kulkarni, Bo-Hsiang Tseng, Joel Moniz, Dhivya Piraviperumal, Hong Yu, and Shruti Bhargava. 2024.

</span>
<span class="ltx_bibblock">Synthdst: Synthetic data is all you need for few-shot dialog state tracking.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1988â€“2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2024)</span>
<span class="ltx_bibblock">
Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, JasonÂ E Weston, and Mike Lewis. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=1oijHJBRsT" title="">Self-alignment with instruction backtranslation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, and Ming Yin. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=MmBjKmHIND" title="">Synthetic data generation with large language models for text classification: Potential and limitations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">The 2023 Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W04-1013" title="">ROUGE: A package for automatic evaluation of summaries</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Text Summarization Branches Out</em>, pages 74â€“81, Barcelona, Spain. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2019)</span>
<span class="ltx_bibblock">
Kevin Lin, Oyvind Tafjord, Peter Clark, and Matt Gardner. 2019.

</span>
<span class="ltx_bibblock">Reasoning over paragraph effects in situations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">MRQA@EMNLP</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Alisa Liu, Swabha Swayamdipta, NoahÂ A. Smith, and Yejin Choi. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:246016339" title="">Wanli: Worker and ai collaboration for natural language inference dataset creation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lo etÂ al. (2019)</span>
<span class="ltx_bibblock">
Kyle Lo, LucyÂ Lu Wang, Mark Neumann, Rodney Kinney, and DanÂ S Weld. 2019.

</span>
<span class="ltx_bibblock">S2orc: The semantic scholar open research corpus.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:1911.02782</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng etÂ al. (2022)</span>
<span class="ltx_bibblock">
YuÂ Meng, Jiaxin Huang, YuÂ Zhang, and Jiawei Han. 2022.

</span>
<span class="ltx_bibblock">Generating training data with language models: Towards zero-shot language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Advances in Neural Information Processing Systems</em>, 35:462â€“477.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra etÂ al. (2022)</span>
<span class="ltx_bibblock">
Shreyash Mishra, SÂ Suryavardan, Amrit Bhaskar, Parul Chopra, AishwaryaÂ N. Reganti, Parth Patwa, Amitava Das, Tanmoy Chakraborty, A.Â Sheth, and Asif Ekbal. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:252016186" title="">Factify: A multi-modal fact verification dataset</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">DE-FACTIFY@AAAI</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muhlgay etÂ al. (2023)</span>
<span class="ltx_bibblock">
Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:259847758" title="">Generating benchmarks for factuality evaluation of language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Conference of the European Chapter of the Association for Computational Linguistics</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ni etÂ al. (2024)</span>
<span class="ltx_bibblock">
Jingwei Ni, Minjing Shi, Dominik Stammbach, Mrinmaya Sachan, Elliott Ash, and Markus Leippold. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:267750827" title="">Afacta: Assisting the annotation of factual claim detection with reliable llm annotators</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Annual Meeting of the Association for Computational Linguistics</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, XuÂ Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Advances in neural information processing systems</em>, 35:27730â€“27744.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni etÂ al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/1073083.1073135" title="">Bleu: a method for automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, pages 311â€“318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddy etÂ al. (2019)</span>
<span class="ltx_bibblock">
Siva Reddy, Danqi Chen, and ChristopherÂ D Manning. 2019.

</span>
<span class="ltx_bibblock">Coqa: A conversational question answering challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Transactions of the Association for Computational Linguistics</em>, 7:249â€“266.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schimanski etÂ al. (2024)</span>
<span class="ltx_bibblock">
Tobias Schimanski, Jingwei Ni, Mathias Kraus, Elliott Ash, and Markus Leippold. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2024.acl-long.105" title="">Towards faithful and robust LLM specialists for evidence-based question-answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1913â€“1931, Bangkok, Thailand. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seddik etÂ al. (2024)</span>
<span class="ltx_bibblock">
Mohamed ElÂ Amine Seddik, Suei-Wen Chen, Soufiane Hayou, Pierre Youssef, and MÃ©rouane Debbah. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:269005923" title="">How bad is training on synthetic data? a statistical analysis of language model collapse</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">ArXiv</em>, abs/2404.05090.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich etÂ al. (2016)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P16-1009" title="">Improving neural machine translation models with monolingual data</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 86â€“96, Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma etÂ al. (2023)</span>
<span class="ltx_bibblock">
Umang Sharma, Sidarth Saran, and DrÂ ShankarÂ M. Patil. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:235387137" title="">Fake news detection using machine learning algorithms</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">2023 International Conference on New Frontiers in Communication, Automation, Management and Security (ICCAMS)</em>, 1:1â€“7.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shumailov etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:258987240" title="">The curse of recursion: Training on generated data makes models forget</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">ArXiv</em>, abs/2305.17493.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simard etÂ al. (2002)</span>
<span class="ltx_bibblock">
PatriceÂ Y Simard, YannÂ A LeCun, JohnÂ S Denker, and Bernard Victorri. 2002.

</span>
<span class="ltx_bibblock">Transformation invariance in pattern recognitionâ€”tangent distance and tangent propagation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Neural networks: tricks of the trade</em>, pages 239â€“274. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh etÂ al. (2024)</span>
<span class="ltx_bibblock">
Krishnakant Singh, Thanush Navaratnam, Jannik Holmer, Simone Schaub-Meyer, and Stefan Roth. 2024.

</span>
<span class="ltx_bibblock">Is synthetic data all we need? benchmarking the robustness of models trained with synthetic images.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 2505â€“2515.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan etÂ al. (2024)</span>
<span class="ltx_bibblock">
Zhen Tan, Dawei Li, Alimohammad Beigi, Song Wang, Ruocheng Guo, Amrita Bhattacharjee, Bohan Jiang, Mansooreh Karami, Jundong Li, LuÂ Cheng, and Huan Liu. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:267770019" title="">Large language models for data annotation: A survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">ArXiv</em>, abs/2402.13446.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Liyan Tang, Philippe Laban, and Greg Durrett. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:269157443" title="">Minicheck: Efficient fact-checking of llms on grounding documents</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">ArXiv</em>, abs/2404.10774.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:257405132" title="">Does synthetic data generation of llms help clinical text mining?</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">ArXiv</em>, abs/2303.04360.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thorne etÂ al. (2018)</span>
<span class="ltx_bibblock">
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018.

</span>
<span class="ltx_bibblock">Fever: a large-scale dataset for fact extraction and verification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</em>, pages 809â€“819.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Villalobos etÂ al. (2024)</span>
<span class="ltx_bibblock">
Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=ViZcgDQjyG" title="">Position: Will we run out of data? limits of LLM scaling based on human-generated data</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Forty-first International Conference on Machine Learning</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wadden etÂ al. (2020)</span>
<span class="ltx_bibblock">
David Wadden, Shanchuan Lin, Kyle Lo, LucyÂ Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020.

</span>
<span class="ltx_bibblock">Fact or fiction: Verifying scientific claims.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 7534â€“7550.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wan etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yuwei Wan, Yixuan Liu, Aswathy Ajith, Clara Grazian, Bram Hoex, Wenjie Zhang, Chunyu Kit, Tong Xie, and Ian Foster. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:269791295" title="">Sciqag: A framework for auto-generated science question answering dataset with fine-grained evaluation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2021)</span>
<span class="ltx_bibblock">
Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael Zeng. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.findings-emnlp.354" title="">Want to reduce labeling cost? GPT-3 can help</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Findings of the Association for Computational Linguistics: EMNLP 2021</em>, pages 4195â€“4205, Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Yang (2015)</span>
<span class="ltx_bibblock">
WilliamÂ Yang Wang and Diyi Yang. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:3257353" title="">Thatâ€™s so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using #petpeeve tweets</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2018)</span>
<span class="ltx_bibblock">
Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:52078335" title="">Switchout: an efficient data augmentation algorithm for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Jason Wei, YiÂ Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, etÂ al. 2022a.

</span>
<span class="ltx_bibblock">Emergent abilities of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2206.07682</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, EdÂ Huai hsin Chi, F.Â Xia, Quoc Le, and Denny Zhou. 2022b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:246411621" title="">Chain of thought prompting elicits reasoning in large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">ArXiv</em>, abs/2201.11903.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2023)</span>
<span class="ltx_bibblock">
JerryÂ W. Wei, DaÂ Huang, Yifeng Lu, Denny Zhou, and QuocÂ V. Le. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:260704246" title="">Simple synthetic data reduces sycophancy in large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">ArXiv</em>, abs/2308.03958.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weidinger etÂ al. (2022)</span>
<span class="ltx_bibblock">
Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Taxonomy of risks posed by language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>, pages 214â€“229.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weston etÂ al. (2015)</span>
<span class="ltx_bibblock">
Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:3178759" title="">Towards ai-complete question answering: A set of prerequisite toy tasks</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv: Artificial Intelligence</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Williams etÂ al. (2018)</span>
<span class="ltx_bibblock">
Adina Williams, Nikita Nangia, and Samuel Bowman. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://aclweb.org/anthology/N18-1101" title="">A broad-coverage challenge corpus for sentence understanding through inference</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</em>, pages 1112â€“1122. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2021)</span>
<span class="ltx_bibblock">
ChienÂ Sheng Wu, Andrea Madotto, Wenhao Liu, Pascale Fung, and Caiming Xiong. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:234679223" title="">Qaconv: Question answering on informative conversations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">Annual Meeting of the Association for Computational Linguistics</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ziang Xie, SidaÂ I. Wang, Jiwei Li, Daniel LÃ©vy, Aiming Nie, Dan Jurafsky, and AndrewÂ Y. Ng. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=H1VyHY9gg" title="">Data noising as smoothing in neural network language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Ying Xu, Dakuo Wang, MoÂ Yu, Daniel Ritchie, Bingsheng Yao, Tongshuang Wu, Zheng Zhang, Toby Li, Nora Bradford, Branda Sun, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Fantastic questions and where to find them: Fairytaleqaâ€“an authentic dataset for narrative comprehension.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 447â€“460.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang. 2024.

</span>
<span class="ltx_bibblock">A survey on large language model (llm) security and privacy: The good, the bad, and the ugly.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">High-Confidence Computing</em>, page 100211.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong. 2022.

</span>
<span class="ltx_bibblock">Zerogen: Efficient zero-shot learning via dataset generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pages 11653â€“11669.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2018)</span>
<span class="ltx_bibblock">
AdamsÂ Wei Yu, David Dohan, Quoc Le, Thang Luong, Rui Zhao, and Kai Chen. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=B14TlG-RW" title="">Fast and accurate reading comprehension by combining self-attention and convolution</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, YuÂ Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2024a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=N8N0hgNDRt" title="">Metamath: Bootstrap your own mathematical questions for large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Yue Yu, Yuchen Zhuang, Jieyu Zhang, YuÂ Meng, AlexanderÂ J Ratner, Ranjay Krishna, Jiaming Shen, and Chao Zhang. 2024b.

</span>
<span class="ltx_bibblock">Large language model as attributed training data generator: A tale of diversity and bias.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yue Yu, Yuchen Zhuang, Rongzhi Zhang, YuÂ Meng, Jiaming Shen, and Chao Zhang. 2023.

</span>
<span class="ltx_bibblock">Regen: Zero-shot text classification via training data generation with progressive dense retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 11782â€“11805.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zha etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:258947273" title="">Alignscore: Evaluating factual consistency with a unified alignment function</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">Annual Meeting of the Association for Computational Linguistics</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang* etÂ al. (2020)</span>
<span class="ltx_bibblock">
Tianyi Zhang*, Varsha Kishore*, Felix Wu*, KilianÂ Q. Weinberger, and Yoav Artzi. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=SkeHuCVFDr" title="">Bertscore: Evaluating text generation with bert</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">International Conference on Learning Representations</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Supplemental Figures</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">We present a more detailed set of figures and tables to supplement the results presented in the main text.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p2">
<p class="ltx_p" id="A1.p2.1"><span class="ltx_text ltx_font_bold" id="A1.p2.1.1">Synthetic Replacement Experiments:</span> For figures in the main text where only one task is shown (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">1</span></a> and FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">2</span></a>), we provide the complete figures with both tasks (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5.F5" title="Figure 5 â€£ Appendix E Implementation Details â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">5</span></a> and FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5.F6" title="Figure 6 â€£ Appendix E Implementation Details â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">6</span></a>). We also provide the individual performance curves for these experiments (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5.F7" title="Figure 7 â€£ Appendix E Implementation Details â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">7</span></a> and FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5.F8" title="Figure 8 â€£ Appendix E Implementation Details â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">8</span></a>).</p>
</div>
<div class="ltx_para" id="A1.p3">
<p class="ltx_p" id="A1.p3.1">To verify the robustness of the results, we show that the QA results are not an artifact of the choice of metric (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5.T1" title="Table 1 â€£ Appendix E Implementation Details â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">1</span></a> and FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5.T2" title="Table 2 â€£ Appendix E Implementation Details â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">2</span></a>), and that the same trends can be seen (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5.F9" title="Figure 9 â€£ Appendix E Implementation Details â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">9</span></a>, FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5.F10" title="Figure 10 â€£ Appendix E Implementation Details â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">10</span></a> and FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5.F11" title="Figure 11 â€£ Appendix E Implementation Details â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">11</span></a>) when using a different fine-tuning model (Mistral-7B), a more powerful prompting model (GPT-4-Turbo) and a more sophisticated prompting strategy (Chain-of-Thought Prompting). Across all configurations, we see a consistent decrease in performance when moving from 95% to 100% synthetic data, confirming that models trained on purely synthetic data can be improved by including just <span class="ltx_text ltx_font_bold" id="A1.p3.1.1">125</span> real data points. For Chain-of-Thought Prompting, the authors manually annotated 3 examples with rationales per dataset to serve as the prompts. The complete examples and pipeline will be provided with the code once the review period is concluded.</p>
</div>
<div class="ltx_para" id="A1.p4">
<p class="ltx_p" id="A1.p4.1">We additionally show that these trends hold across data scales (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5.F12" title="Figure 12 â€£ Appendix E Implementation Details â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">12</span></a> and FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5.F13" title="Figure 13 â€£ Appendix E Implementation Details â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">13</span></a>), replicating the experiment with n=3000 and n=1000. While the trend is clearly visible in both cases, the results for n=1000 have more variance and hence have a minority of cases where the relationship does not hold.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p5">
<p class="ltx_p" id="A1.p5.3"><span class="ltx_text ltx_font_bold" id="A1.p5.3.1">Tradeoff Experiment:</span> The main text shows results for the experiment detailed in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#S4" title="4 When Should We Use Human Data? â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">4</span></a> on the WANLI dataset (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#S3.F3" title="Figure 3 â€£ 3 Can Synthetic Data Replace Humans? â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">3</span></a>), here we show results on the remaining three datasets (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5.F14" title="Figure 14 â€£ Appendix E Implementation Details â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">14</span></a>) and provide (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5.T3" title="Table 3 â€£ Appendix E Implementation Details â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">3</span></a>)the number of additional synthetic points needed to match the performance gains of 200 additional real points (average, median and standard deviation for each dataset). ROPES shows similar results to WANLI, however FairyTaleQA and FEVER present different trends. On FEVER, we are able to reach the saturation point, after which additional data (whether synthetic or real) does not increase performance. Even in this case, we are able to reach this point of diminishing marginal return more rapidly when using a small amount of synthetic data. On a base synthetic training set of size <math alttext="3000" class="ltx_Math" display="inline" id="A1.p5.1.m1.1"><semantics id="A1.p5.1.m1.1a"><mn id="A1.p5.1.m1.1.1" xref="A1.p5.1.m1.1.1.cmml">3000</mn><annotation-xml encoding="MathML-Content" id="A1.p5.1.m1.1b"><cn id="A1.p5.1.m1.1.1.cmml" type="integer" xref="A1.p5.1.m1.1.1">3000</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p5.1.m1.1c">3000</annotation><annotation encoding="application/x-llamapun" id="A1.p5.1.m1.1d">3000</annotation></semantics></math>, adding <math alttext="200" class="ltx_Math" display="inline" id="A1.p5.2.m2.1"><semantics id="A1.p5.2.m2.1a"><mn id="A1.p5.2.m2.1.1" xref="A1.p5.2.m2.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="A1.p5.2.m2.1b"><cn id="A1.p5.2.m2.1.1.cmml" type="integer" xref="A1.p5.2.m2.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p5.2.m2.1c">200</annotation><annotation encoding="application/x-llamapun" id="A1.p5.2.m2.1d">200</annotation></semantics></math> real data points drives the test accuracy to <math alttext="89.25\%" class="ltx_Math" display="inline" id="A1.p5.3.m3.1"><semantics id="A1.p5.3.m3.1a"><mrow id="A1.p5.3.m3.1.1" xref="A1.p5.3.m3.1.1.cmml"><mn id="A1.p5.3.m3.1.1.2" xref="A1.p5.3.m3.1.1.2.cmml">89.25</mn><mo id="A1.p5.3.m3.1.1.1" xref="A1.p5.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.p5.3.m3.1b"><apply id="A1.p5.3.m3.1.1.cmml" xref="A1.p5.3.m3.1.1"><csymbol cd="latexml" id="A1.p5.3.m3.1.1.1.cmml" xref="A1.p5.3.m3.1.1.1">percent</csymbol><cn id="A1.p5.3.m3.1.1.2.cmml" type="float" xref="A1.p5.3.m3.1.1.2">89.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p5.3.m3.1c">89.25\%</annotation><annotation encoding="application/x-llamapun" id="A1.p5.3.m3.1d">89.25 %</annotation></semantics></math>, a score that is only matched once we add at least 2000 synthetic data points (an order or magnitude larger). On FairyTaleQA, we get enormous estimates for the number of additional synthetic points needed (a mean of 2.8e5). We do not interpret these numbers literally, rather seeing this as a sign that human generated data may occasionally boost performance to an extent that could be fundamentally unachievable by purely synthetic data.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Synthetic Data Generation</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">In our implementation (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5.F15" title="Figure 15 â€£ Appendix E Implementation Details â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">15</span></a>), we use few-shot learning with <math alttext="k=3" class="ltx_Math" display="inline" id="A2.p1.1.m1.1"><semantics id="A2.p1.1.m1.1a"><mrow id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml"><mi id="A2.p1.1.m1.1.1.2" xref="A2.p1.1.m1.1.1.2.cmml">k</mi><mo id="A2.p1.1.m1.1.1.1" xref="A2.p1.1.m1.1.1.1.cmml">=</mo><mn id="A2.p1.1.m1.1.1.3" xref="A2.p1.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.1b"><apply id="A2.p1.1.m1.1.1.cmml" xref="A2.p1.1.m1.1.1"><eq id="A2.p1.1.m1.1.1.1.cmml" xref="A2.p1.1.m1.1.1.1"></eq><ci id="A2.p1.1.m1.1.1.2.cmml" xref="A2.p1.1.m1.1.1.2">ğ‘˜</ci><cn id="A2.p1.1.m1.1.1.3.cmml" type="integer" xref="A2.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.1.m1.1c">k=3</annotation><annotation encoding="application/x-llamapun" id="A2.p1.1.m1.1d">italic_k = 3</annotation></semantics></math>, i.e., three examples per query, with each example drawn randomly (with replacement) from the training set of the specific dataset.</p>
</div>
<div class="ltx_para" id="A2.p2">
<p class="ltx_p" id="A2.p2.2">We generate one synthetic point for every real point in the dataset, using the evidence text it is associated with. This gives us a total of <math alttext="n" class="ltx_Math" display="inline" id="A2.p2.1.m1.1"><semantics id="A2.p2.1.m1.1a"><mi id="A2.p2.1.m1.1.1" xref="A2.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A2.p2.1.m1.1b"><ci id="A2.p2.1.m1.1.1.cmml" xref="A2.p2.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="A2.p2.1.m1.1d">italic_n</annotation></semantics></math> synthetic data points for every <math alttext="n" class="ltx_Math" display="inline" id="A2.p2.2.m2.1"><semantics id="A2.p2.2.m2.1a"><mi id="A2.p2.2.m2.1.1" xref="A2.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A2.p2.2.m2.1b"><ci id="A2.p2.2.m2.1.1.cmml" xref="A2.p2.2.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="A2.p2.2.m2.1d">italic_n</annotation></semantics></math> real data points in a dataset.</p>
</div>
<div class="ltx_para" id="A2.p3">
<p class="ltx_p" id="A2.p3.1">We observed that if we did not correct for label shift, the prompt model would be heavily biased towards True claims, i.e., it would generate a dataset containing 90% True claims, while original datasets have proportions between 33%â€“60% True.</p>
</div>
<div class="ltx_para" id="A2.p4">
<p class="ltx_p" id="A2.p4.1">For the synthetic datasets used in our experiments, we correct for this label shift by specifying the label of the claim we wish to generate and providing only examples of claims with that specific label in the prompt. his setting is generous towards synthetic data generation. In practice, we might only have a fixed three examples to use in the prompt, potentially reducing the diversity of synthetic data generated. We may also not know the correct label proportion to ask for, and suffer a significant label shift when using synthetic data generation.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Datasets Used</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">All datasets used below are released under open use licenses, authorizing their use in this research. For each dataset below, we discuss the potential of dataset leakage (i.e., whether the data has been exposed to GPT-3.5-Turbo during its training) as well as the extent of automation involved in the generation of each dataset. However, across all experiments and ablations, these factors do not seem to have any discernable effect on the trends discovered in this work.</p>
</div>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Fact Verification Datasets</h3>
<div class="ltx_para ltx_noindent" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="A3.SS1.p1.1.1">FEVER <cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_medium" id="A3.SS1.p1.1.1.1.1">(</span>Thorne etÂ al.<span class="ltx_text ltx_font_medium" id="A3.SS1.p1.1.1.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib54" title="">2018</a><span class="ltx_text ltx_font_medium" id="A3.SS1.p1.1.1.3.3">)</span></cite></span> is a dataset of claims about specific entities, generated by altering sentences extracted from Wikipedia. The evidence passages are sentences from Wikipedia articles relevant to the entity in question. This dataset has been well established for a long time before the release of the prompting models used in this work, increasing the chance that it has been exposed to the prompt model ahead of time.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS1.p2">
<p class="ltx_p" id="A3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="A3.SS1.p2.1.1">SciFact</span> <cite class="ltx_cite ltx_citemacro_citep">(Wadden etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib56" title="">2020</a>)</cite> is a fact verification dataset for the scientific domain, which uses the abstracts of scientific articles as evidence texts. The corpus is collected from S2ORCÂ <cite class="ltx_cite ltx_citemacro_citep">(Lo etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib36" title="">2019</a>)</cite>, a publicly-available corpus of millions of
scientific articles. Annotators are shown a source citation
in the context of an article, and are asked to write up
to three claims based on the content of the citation.</p>
</div>
<div class="ltx_para" id="A3.SS1.p3">
<p class="ltx_p" id="A3.SS1.p3.1">The above datasets are popular NLP challenge sets that were well known even before the release of the GPT-3.5-TurboÂ <cite class="ltx_cite ltx_citemacro_citep">(Brown, <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib8" title="">2020</a>)</cite>, the prompting model used in this work. The following two datasets were released after the official training date cut-off, guaranteeing that the data has not been seen ahead of time.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS1.p4">
<p class="ltx_p" id="A3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="A3.SS1.p4.1.1">WANLI</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib35" title="">2022</a>)</cite> is an NLI dataset of 108K examples created through a hybrid worker and AI collaboration approach. The creators first study MultiNLIÂ <cite class="ltx_cite ltx_citemacro_citep">(Williams etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib66" title="">2018</a>)</cite> and use dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns.
and then instruct GPT-3 to compose new examples with similar patterns. Machine generated
examples are then automatically filtered, and
finally revised and labeled by human crowd workers. While GPT-3.5-Turbo has not been trained on this data, it is worth noting that the data is partially synthetic generated.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS1.p5">
<p class="ltx_p" id="A3.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="A3.SS1.p5.1.1">FACTIFY</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Mishra etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib38" title="">2022</a>)</cite> is a dataset on multi-modal fact verification. It contains images, textual claims, reference textual documents and reference images. The dataset marks some examples that can be verified using text only; we use this sample in our experiments. This dataset was released after the training cut-off date for GPT-3.5 and takes its evidence texts / claims from human-written news or editorial articles. This ensures that the prompt model studied have not seen the data before training.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS1.p6">
<p class="ltx_p" id="A3.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="A3.SS1.p6.1.1">Label mapping for NLI and FV:</span>
While all of the above datasets contain labels for Supports, Refutes, and Not Enough Information (or Entails, Contradicts, Neutral), we consider the stricter formulation of Fact Verification used byÂ <cite class="ltx_cite ltx_citemacro_citet">Honovich etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib24" title="">2022</a>)</cite> and Â <cite class="ltx_cite ltx_citemacro_citet">Zha etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib76" title="">2023</a>)</cite>, considering a claim to be factual if the label is Supports (Entails), and non-factual otherwise.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Question Answering Datasets</h3>
<div class="ltx_para ltx_noindent" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="A3.SS2.p1.1.1">ROPES</span> <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib34" title="">2019</a>)</cite> is a QA dataset which tests a systemâ€™s ability to apply knowledge from a passage of text to a new situation. The evidence context contains causal or qualitative relation(s) (e.g., â€œanimal pollinators increase efficiency of fertilization in flowersâ€), and a novel situation that uses this background. The question requires reasoning about effects of the relationships in the background passage in the context of the situation.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS2.p2">
<p class="ltx_p" id="A3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="A3.SS2.p2.1.1">CoQA</span> <cite class="ltx_cite ltx_citemacro_citep">(Reddy etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib43" title="">2019</a>)</cite> is a dataset for building Conversational Question Answering systems. CoQA measures the ability of machines to understand a text passage and answer a series of interconnected questions that appear in a conversation. In our experiments, we extract only the first question in the series and use this to obtain our (context, question, answer) data points.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS2.p3">
<p class="ltx_p" id="A3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="A3.SS2.p3.1.1">QAConv</span> <cite class="ltx_cite ltx_citemacro_citep">(Wu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib67" title="">2021</a>)</cite> focuses on informative conversations, including business emails, panel discussions, and work channels. The creators collect QA pairs with both human-written and machine-generated questions. They use a question generator and a dialogue summarizer as auxiliary tools to collect and recommend questions. While the arXiv version of the paper appears before the GPT-3 cut-off data (April 2021 to the cut-off date of Sept 2021), the paper itself appeared only at ACL2022. It is still possible that the training data was compromised, and owing to the lack of clarity on the training data used for GPT-3 we have no way to confirm or deny this speculation.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS2.p4">
<p class="ltx_p" id="A3.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="A3.SS2.p4.1.1">FairyTaleQA</span> <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#bib.bib69" title="">2022</a>)</cite> is a dataset focusing on narrative comprehension of kindergarten to eighth-grade students. The evidence texts are derived from children-friendly stories which serve as evidence texts. The questions are both explicit and implicit, covering seven types of narrative elements or relations. This dataset was released after the GPT-3 training cut-off date, ensuring that it has not been seen by our prompt model before.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Detailed Discussion on Differences Between Synthetic and Human Data</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">To compute the extent to which the evidence sentences â€˜containsâ€™ the questions, answers, and claims, we measured the BLEU of the generation with <span class="ltx_text ltx_font_bold" id="A4.p1.1.1">each individual sentence</span> of the evidence texts, plotting the maximum of these BLEU scores in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5.F17" title="Figure 17 â€£ Appendix E Implementation Details â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">17</span></a>.
We find that synthetic generations have a far higher n-gram overlap with the evidence sentences than human generations. This suggests that synthetic data generation produces data points that are more extractive, while humans are more likely to abstract from the evidence. We also use the position of the evidence sentence that achieves the highest BLEU score as a proxy for the source location of the synthetic generation, and find that synthetic data generation chooses more diverse sources for the question and answer content, with human annotation overwhelmingly more likely to create questions whose answers lie in the start of the evidence texts (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5.F18" title="Figure 18 â€£ Appendix E Implementation Details â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">18</span></a>). Finally, the main text shows the size length comparison for a single dataset, here we provide a larger sample
(FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5.F16" title="Figure 16 â€£ Appendix E Implementation Details â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">16</span></a>). We explored the errors created by the models trained on 0% and 100% data, searching for trends or divergences between the input instances that achieve a low prediction accuracy or score. Our investigation found no major distinguishing factors between them, leaving a more fine-grained study of the effect of purely synthetic data on model decision-making to future work.</p>
</div>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Implementation Details</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">While our full code implementation can be seen in the GitHub repository (to be released after review), we list the key implementation details below.</p>
</div>
<div class="ltx_para" id="A5.p2">
<p class="ltx_p" id="A5.p2.1"><span class="ltx_text ltx_font_bold" id="A5.p2.1.1">Hardware and Systems Used</span>: The experiments were run on a cluster that included nodes with: 5 A40 GPUs (48GB), 3 RTX 2080Tis, and a separate machine using a single A100 GPU.</p>
</div>
<div class="ltx_para" id="A5.p3">
<p class="ltx_p" id="A5.p3.1"><span class="ltx_text ltx_font_bold" id="A5.p3.1.1">Prompt Models used</span>: We used GPT-3.5-Turbo and GPT-4-Turbo Batch APIs from OpenAI. Generations were obtained at various points from August 2024 to September 2024.</p>
</div>
<div class="ltx_para" id="A5.p4">
<p class="ltx_p" id="A5.p4.1"><span class="ltx_text ltx_font_bold" id="A5.p4.1.1">Fine-Tuning Models Used</span>: We used 2 fine-tuning models in our experiments. Llama3 used the Llama3.1-8B HuggingFace Checkpoint, and Mistral used the Mistral7B-Instruct-v0.2 HuggingFace Checkpoint. We did not conduct an extensive hyperparameter search, however we tried various epochs on smaller samples of the FEVER and ROPES datasets, selecting that number for every dataset on all experiments. Fact verification models used Adam Optimization with a learning rate of 1e-5 for 2 epochs, while QA datasets used a learning rate of 1e-2 for 5 epochs.</p>
</div>
<figure class="ltx_figure" id="A5.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="294" id="A5.F5.g1" src="extracted/5929974/images/full_dataset/fv.png" width="419"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="343" id="A5.F5.g2" src="extracted/5929974/images/full_dataset/qa.png" width="419"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Change in model performance as the proportion of synthetic points in the training data is increased. Across datasets, the performance decrease when moving from 0% to 90% synthetic data is often less than that of moving from 90% to purely synthetic data.</figcaption>
</figure>
<figure class="ltx_figure" id="A5.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="291" id="A5.F6.g1" src="extracted/5929974/images/zoom_5/fv.png" width="419"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="336" id="A5.F6.g2" src="extracted/5929974/images/zoom_5/qa.png" width="419"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Model performance as the synthetic proportion of the training data varies from 95% to 100%. Across all datasets and random seeds, having <span class="ltx_text ltx_font_bold" id="A5.F6.2.1">just 2.5%</span> of the training dataset being human generated boosts performance.</figcaption>
</figure>
<figure class="ltx_table" id="A5.T1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A5.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A5.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A5.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T1.1.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt" id="A5.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A5.T1.1.1.1.2.1">Synthetic Fraction</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A5.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A5.T1.1.1.1.3.1">EM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A5.T1.1.1.1.4.1">Inc</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A5.T1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A5.T1.1.1.1.5.1">R Inc</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="A5.T1.1.1.1.6.1">BLEU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A5.T1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="A5.T1.1.1.1.7.1">ROUGE</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T1.1.1.1.8"><span class="ltx_text ltx_font_bold" id="A5.T1.1.1.1.8.1">BERTScore</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A5.T1.1.2.1.1">CoQA</th>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="A5.T1.1.2.1.2">0</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A5.T1.1.2.1.3">40.6</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T1.1.2.1.4">52.2</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A5.T1.1.2.1.5">60..8</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T1.1.2.1.6">47.85</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A5.T1.1.2.1.7">64.01</th>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A5.T1.1.2.1.8">87.78</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.3.2">
<th class="ltx_td ltx_th ltx_th_row" id="A5.T1.1.3.2.1"></th>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A5.T1.1.3.2.2">0.25</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.3.2.3">35</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.3.2.4">51.8</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.3.2.5">54.8</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.3.2.6">44.05</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.3.2.7">61.95</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T1.1.3.2.8">85.89</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.4.3">
<th class="ltx_td ltx_th ltx_th_row" id="A5.T1.1.4.3.1"></th>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A5.T1.1.4.3.2">0.5</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.4.3.3">31.6</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.4.3.4">42.8</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.4.3.5">60.6</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.4.3.6">38.68</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.4.3.7">54.65</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T1.1.4.3.8">79.88</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.5.4">
<th class="ltx_td ltx_th ltx_th_row" id="A5.T1.1.5.4.1"></th>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A5.T1.1.5.4.2">0.75</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.5.4.3">39.2</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.5.4.4">50.4</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.5.4.5">69</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.5.4.6">46.38</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.5.4.7">62.07</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T1.1.5.4.8">79.94</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.6.5">
<th class="ltx_td ltx_th ltx_th_row" id="A5.T1.1.6.5.1"></th>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A5.T1.1.6.5.2">0.9</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.6.5.3">36.2</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.6.5.4">50.2</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.6.5.5">58.2</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.6.5.6">44.46</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.6.5.7">60.81</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T1.1.6.5.8">85.4</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.7.6">
<th class="ltx_td ltx_th ltx_th_row" id="A5.T1.1.7.6.1"></th>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A5.T1.1.7.6.2">1</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.7.6.3">13.6</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.7.6.4">26</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.7.6.5">58.2</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.7.6.6">18.17</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.7.6.7">26.55</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T1.1.7.6.8">52.76</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A5.T1.1.8.7.1">FairytaleQA</th>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="A5.T1.1.8.7.2">0</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A5.T1.1.8.7.3">0</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T1.1.8.7.4">0</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A5.T1.1.8.7.5">0</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T1.1.8.7.6">39.34</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A5.T1.1.8.7.7">55.3</th>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A5.T1.1.8.7.8">90.82</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.9.8">
<th class="ltx_td ltx_th ltx_th_row" id="A5.T1.1.9.8.1"></th>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A5.T1.1.9.8.2">0.25</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.9.8.3">0</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.9.8.4">0</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.9.8.5">0</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.9.8.6">40.02</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.9.8.7">56.12</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T1.1.9.8.8">90.5</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.10.9">
<th class="ltx_td ltx_th ltx_th_row" id="A5.T1.1.10.9.1"></th>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A5.T1.1.10.9.2">0.5</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.10.9.3">0</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.10.9.4">0</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.10.9.5">0</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.10.9.6">39.22</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.10.9.7">55.3</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T1.1.10.9.8">88.9</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.11.10">
<th class="ltx_td ltx_th ltx_th_row" id="A5.T1.1.11.10.1"></th>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A5.T1.1.11.10.2">0.75</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.11.10.3">0</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.11.10.4">0</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.11.10.5">0</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.11.10.6">39.74</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.11.10.7">55.36</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T1.1.11.10.8">90.56</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.12.11">
<th class="ltx_td ltx_th ltx_th_row" id="A5.T1.1.12.11.1"></th>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A5.T1.1.12.11.2">0.9</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.12.11.3">0</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.12.11.4">0</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.12.11.5">0</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.12.11.6">38.1</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.12.11.7">54.2</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T1.1.12.11.8">89.9</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.13.12">
<th class="ltx_td ltx_th ltx_th_row" id="A5.T1.1.13.12.1"></th>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A5.T1.1.13.12.2">1</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.13.12.3">0</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.13.12.4">0</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.13.12.5">0</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.13.12.6">30.12</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.13.12.7">49.51</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T1.1.13.12.8">88.35</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.14.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A5.T1.1.14.13.1">QAConv</th>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="A5.T1.1.14.13.2">0</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A5.T1.1.14.13.3">29.39</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T1.1.14.13.4">36.3</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A5.T1.1.14.13.5">49.42</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T1.1.14.13.6">35.11</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A5.T1.1.14.13.7">51.62</th>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A5.T1.1.14.13.8">89.89</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.15.14">
<th class="ltx_td ltx_th ltx_th_row" id="A5.T1.1.15.14.1"></th>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A5.T1.1.15.14.2">0.25</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.15.14.3">28.55</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.15.14.4">34.5</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.15.14.5">47.96</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.15.14.6">33.51</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.15.14.7">48.56</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T1.1.15.14.8">89.51</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.16.15">
<th class="ltx_td ltx_th ltx_th_row" id="A5.T1.1.16.15.1"></th>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A5.T1.1.16.15.2">0.5</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.16.15.3">27.95</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.16.15.4">34.04</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.16.15.5">47.1</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.16.15.6">33.1</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.16.15.7">48.67</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T1.1.16.15.8">89.46</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.17.16">
<th class="ltx_td ltx_th ltx_th_row" id="A5.T1.1.17.16.1"></th>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A5.T1.1.17.16.2">0.75</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.17.16.3">28.55</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.17.16.4">35.87</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.17.16.5">48.45</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.17.16.6">34.17</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.17.16.7">50.52</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T1.1.17.16.8">89.97</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.18.17">
<th class="ltx_td ltx_th ltx_th_row" id="A5.T1.1.18.17.1"></th>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A5.T1.1.18.17.2">0.9</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.18.17.3">29.01</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.18.17.4">35.96</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.18.17.5">49.88</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.18.17.6">34.75</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.18.17.7">50.57</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T1.1.18.17.8">88.97</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.19.18">
<th class="ltx_td ltx_th ltx_th_row" id="A5.T1.1.19.18.1"></th>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A5.T1.1.19.18.2">1</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.19.18.3">23.5</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.19.18.4">34.29</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.19.18.5">40.98</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.19.18.6">30.21</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.19.18.7">45.16</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T1.1.19.18.8">87.15</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.20.19">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A5.T1.1.20.19.1">ROPES</th>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="A5.T1.1.20.19.2">0</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A5.T1.1.20.19.3">66.76</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T1.1.20.19.4">67.35</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A5.T1.1.20.19.5">72.15</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T1.1.20.19.6">67.01</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A5.T1.1.20.19.7">72.66</th>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A5.T1.1.20.19.8">95.95</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.21.20">
<th class="ltx_td ltx_th ltx_th_row" id="A5.T1.1.21.20.1"></th>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A5.T1.1.21.20.2">0.25</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.21.20.3">66.82</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.21.20.4">67.53</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.21.20.5">69.78</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.21.20.6">67.15</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.21.20.7">71.12</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T1.1.21.20.8">96.15</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.22.21">
<th class="ltx_td ltx_th ltx_th_row" id="A5.T1.1.22.21.1"></th>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A5.T1.1.22.21.2">0.5</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.22.21.3">62.79</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.22.21.4">63.44</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.22.21.5">65.46</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.22.21.6">63.07</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.22.21.7">66.61</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T1.1.22.21.8">95.17</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.23.22">
<th class="ltx_td ltx_th ltx_th_row" id="A5.T1.1.23.22.1"></th>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A5.T1.1.23.22.2">0.75</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.23.22.3">66.82</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.23.22.4">68</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.23.22.5">68.83</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.23.22.6">67.39</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.23.22.7">70.82</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T1.1.23.22.8">96.2</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.24.23">
<th class="ltx_td ltx_th ltx_th_row" id="A5.T1.1.24.23.1"></th>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A5.T1.1.24.23.2">0.9</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.24.23.3">70.57</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.24.23.4">71.46</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.24.23.5">71.82</th>
<td class="ltx_td ltx_align_center" id="A5.T1.1.24.23.6">70.98</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A5.T1.1.24.23.7">73.01</th>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T1.1.24.23.8">96.84</td>
</tr>
<tr class="ltx_tr" id="A5.T1.1.25.24">
<th class="ltx_td ltx_th ltx_th_row ltx_border_bb" id="A5.T1.1.25.24.1"></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" id="A5.T1.1.25.24.2">1</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A5.T1.1.25.24.3">60.84</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T1.1.25.24.4">63.86</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A5.T1.1.25.24.5">61.19</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T1.1.25.24.6">62.07</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A5.T1.1.25.24.7">64.9</th>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="A5.T1.1.25.24.8">95.29</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Full Results for the QA datasets. There is overwhelming agreement between all metrics on the ranking between models trained on different synthetic fractions. EM: Exact Match, Inc: String Inclusion, R Inc: Reverse String Inclusion</figcaption>
</figure>
<figure class="ltx_table" id="A5.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A5.T2.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A5.T2.3.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T2.3.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T2.3.1.1.1.1">Run</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T2.3.1.1.2"><span class="ltx_text ltx_font_bold" id="A5.T2.3.1.1.2.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T2.3.1.1.3"><span class="ltx_text ltx_font_bold" id="A5.T2.3.1.1.3.1">Synthetic Fraction</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T2.3.1.1.4"><span class="ltx_text ltx_font_bold" id="A5.T2.3.1.1.4.1">BLEU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T2.3.1.1.5"><span class="ltx_text ltx_font_bold" id="A5.T2.3.1.1.5.1">ROUGE</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T2.3.1.1.6"><span class="ltx_text ltx_font_bold" id="A5.T2.3.1.1.6.1">BERTScore</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T2.3.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.2.1.1">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.2.1.2">FairytaleQA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.2.1.3">0.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.2.1.4">38.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.2.1.5">54.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A5.T2.3.2.1.6">90.34</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.3.2">
<td class="ltx_td" id="A5.T2.3.3.2.1"></td>
<td class="ltx_td" id="A5.T2.3.3.2.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.3.2.3">0.975</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.3.2.4">37.19</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.3.2.5">52.09</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.3.2.6">86.17</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.4.3">
<td class="ltx_td" id="A5.T2.3.4.3.1"></td>
<td class="ltx_td" id="A5.T2.3.4.3.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.4.3.3">1</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.4.3.4">26.1</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.4.3.5">43.82</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.4.3.6">77.03</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.5.4">
<td class="ltx_td ltx_border_t" id="A5.T2.3.5.4.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.5.4.2">QAConv</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.5.4.3">0.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.5.4.4">34.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.5.4.5">51.23</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A5.T2.3.5.4.6">90.35</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.6.5">
<td class="ltx_td" id="A5.T2.3.6.5.1"></td>
<td class="ltx_td" id="A5.T2.3.6.5.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.6.5.3">0.975</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.6.5.4">34.31</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.6.5.5">51.89</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.6.5.6">89.8</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.7.6">
<td class="ltx_td" id="A5.T2.3.7.6.1"></td>
<td class="ltx_td" id="A5.T2.3.7.6.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.7.6.3">1</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.7.6.4">32.33</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.7.6.5">49.39</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.7.6.6">89.77</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.8.7">
<td class="ltx_td ltx_border_t" id="A5.T2.3.8.7.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.8.7.2">CoQA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.8.7.3">0.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.8.7.4">25.33</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.8.7.5">35.64</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A5.T2.3.8.7.6">59.39</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.9.8">
<td class="ltx_td" id="A5.T2.3.9.8.1"></td>
<td class="ltx_td" id="A5.T2.3.9.8.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.9.8.3">0.975</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.9.8.4">42.78</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.9.8.5">57.88</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.9.8.6">78.84</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.10.9">
<td class="ltx_td" id="A5.T2.3.10.9.1"></td>
<td class="ltx_td" id="A5.T2.3.10.9.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.10.9.3">1</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.10.9.4">19.11</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.10.9.5">27.57</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.10.9.6">51.44</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.11.10">
<td class="ltx_td ltx_border_t" id="A5.T2.3.11.10.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.11.10.2">ROPES</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.11.10.3">0.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.11.10.4">72.89</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.11.10.5">77.26</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A5.T2.3.11.10.6">97.22</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.12.11">
<td class="ltx_td" id="A5.T2.3.12.11.1"></td>
<td class="ltx_td" id="A5.T2.3.12.11.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.12.11.3">0.975</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.12.11.4">70.74</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.12.11.5">73.83</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.12.11.6">96.58</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.13.12">
<td class="ltx_td" id="A5.T2.3.13.12.1"></td>
<td class="ltx_td" id="A5.T2.3.13.12.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.13.12.3">1</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.13.12.4">58.28</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.13.12.5">60.82</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.13.12.6">94.37</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.14.13">
<td class="ltx_td ltx_align_center ltx_border_tt" id="A5.T2.3.14.13.1">1</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A5.T2.3.14.13.2">FairytaleQA</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A5.T2.3.14.13.3">0.95</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A5.T2.3.14.13.4">39.5</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A5.T2.3.14.13.5">54.67</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="A5.T2.3.14.13.6">90.57</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.15.14">
<td class="ltx_td" id="A5.T2.3.15.14.1"></td>
<td class="ltx_td" id="A5.T2.3.15.14.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.15.14.3">0.975</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.15.14.4">35.95</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.15.14.5">53.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.15.14.6">89.73</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.16.15">
<td class="ltx_td" id="A5.T2.3.16.15.1"></td>
<td class="ltx_td" id="A5.T2.3.16.15.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.16.15.3">1</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.16.15.4">29.75</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.16.15.5">47.05</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.16.15.6">81.86</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.17.16">
<td class="ltx_td ltx_border_t" id="A5.T2.3.17.16.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.17.16.2">QAConv</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.17.16.3">0.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.17.16.4">38.94</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.17.16.5">56.41</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A5.T2.3.17.16.6">90.79</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.18.17">
<td class="ltx_td" id="A5.T2.3.18.17.1"></td>
<td class="ltx_td" id="A5.T2.3.18.17.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.18.17.3">0.975</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.18.17.4">40.06</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.18.17.5">57.64</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.18.17.6">90.45</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.19.18">
<td class="ltx_td" id="A5.T2.3.19.18.1"></td>
<td class="ltx_td" id="A5.T2.3.19.18.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.19.18.3">1</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.19.18.4">37.46</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.19.18.5">54.11</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.19.18.6">88.57</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.20.19">
<td class="ltx_td ltx_border_t" id="A5.T2.3.20.19.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.20.19.2">CoQA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.20.19.3">0.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.20.19.4">34.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.20.19.5">47.16</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A5.T2.3.20.19.6">63.35</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.21.20">
<td class="ltx_td" id="A5.T2.3.21.20.1"></td>
<td class="ltx_td" id="A5.T2.3.21.20.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.21.20.3">0.975</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.21.20.4">41.91</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.21.20.5">56.53</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.21.20.6">85.3</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.22.21">
<td class="ltx_td" id="A5.T2.3.22.21.1"></td>
<td class="ltx_td" id="A5.T2.3.22.21.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.22.21.3">1</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.22.21.4">14.81</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.22.21.5">24.58</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.22.21.6">50.02</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.23.22">
<td class="ltx_td ltx_border_t" id="A5.T2.3.23.22.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.23.22.2">ROPES</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.23.22.3">0.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.23.22.4">69.61</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.23.22.5">71.97</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A5.T2.3.23.22.6">96.23</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.24.23">
<td class="ltx_td" id="A5.T2.3.24.23.1"></td>
<td class="ltx_td" id="A5.T2.3.24.23.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.24.23.3">0.975</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.24.23.4">69.72</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.24.23.5">72.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.24.23.6">96.58</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.25.24">
<td class="ltx_td" id="A5.T2.3.25.24.1"></td>
<td class="ltx_td" id="A5.T2.3.25.24.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.25.24.3">1</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.25.24.4">62.08</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.25.24.5">65.44</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.25.24.6">95.03</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.26.25">
<td class="ltx_td ltx_align_center ltx_border_tt" id="A5.T2.3.26.25.1">2</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A5.T2.3.26.25.2">FairytaleQA</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A5.T2.3.26.25.3">0.95</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A5.T2.3.26.25.4">37.97</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A5.T2.3.26.25.5">53.98</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="A5.T2.3.26.25.6">90.34</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.27.26">
<td class="ltx_td" id="A5.T2.3.27.26.1"></td>
<td class="ltx_td" id="A5.T2.3.27.26.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.27.26.3">0.975</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.27.26.4">37.65</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.27.26.5">52.44</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.27.26.6">87.83</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.28.27">
<td class="ltx_td" id="A5.T2.3.28.27.1"></td>
<td class="ltx_td" id="A5.T2.3.28.27.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.28.27.3">1</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.28.27.4">29.26</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.28.27.5">49.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.28.27.6">88.56</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.29.28">
<td class="ltx_td ltx_border_t" id="A5.T2.3.29.28.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.29.28.2">QAConv</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.29.28.3">0.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.29.28.4">38.07</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.29.28.5">54.49</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A5.T2.3.29.28.6">90.13</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.30.29">
<td class="ltx_td" id="A5.T2.3.30.29.1"></td>
<td class="ltx_td" id="A5.T2.3.30.29.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.30.29.3">0.975</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.30.29.4">37.7</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.30.29.5">54.64</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.30.29.6">88.61</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.31.30">
<td class="ltx_td" id="A5.T2.3.31.30.1"></td>
<td class="ltx_td" id="A5.T2.3.31.30.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.31.30.3">1</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.31.30.4">35.94</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.31.30.5">51.84</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.31.30.6">89.49</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.32.31">
<td class="ltx_td ltx_border_t" id="A5.T2.3.32.31.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.32.31.2">CoQA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.32.31.3">0.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.32.31.4">30.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.32.31.5">42.03</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A5.T2.3.32.31.6">62.72</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.33.32">
<td class="ltx_td" id="A5.T2.3.33.32.1"></td>
<td class="ltx_td" id="A5.T2.3.33.32.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.33.32.3">0.975</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.33.32.4">30.4</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.33.32.5">41.02</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.33.32.6">56.83</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.34.33">
<td class="ltx_td" id="A5.T2.3.34.33.1"></td>
<td class="ltx_td" id="A5.T2.3.34.33.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.34.33.3">1</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.34.33.4">23.42</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.34.33.5">37.2</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.34.33.6">63.31</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.35.34">
<td class="ltx_td ltx_border_t" id="A5.T2.3.35.34.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.35.34.2">ROPES</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.35.34.3">0.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.35.34.4">63.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T2.3.35.34.5">65.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A5.T2.3.35.34.6">95.28</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.36.35">
<td class="ltx_td" id="A5.T2.3.36.35.1"></td>
<td class="ltx_td" id="A5.T2.3.36.35.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.36.35.3">0.975</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.36.35.4">67.94</td>
<td class="ltx_td ltx_align_center" id="A5.T2.3.36.35.5">71.47</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T2.3.36.35.6">96.16</td>
</tr>
<tr class="ltx_tr" id="A5.T2.3.37.36">
<td class="ltx_td ltx_border_bb" id="A5.T2.3.37.36.1"></td>
<td class="ltx_td ltx_border_bb" id="A5.T2.3.37.36.2"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T2.3.37.36.3">1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T2.3.37.36.4">58.34</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T2.3.37.36.5">61.62</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="A5.T2.3.37.36.6">94.16</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results on <math alttext="n=5000" class="ltx_Math" display="inline" id="A5.T2.2.m1.1"><semantics id="A5.T2.2.m1.1b"><mrow id="A5.T2.2.m1.1.1" xref="A5.T2.2.m1.1.1.cmml"><mi id="A5.T2.2.m1.1.1.2" xref="A5.T2.2.m1.1.1.2.cmml">n</mi><mo id="A5.T2.2.m1.1.1.1" xref="A5.T2.2.m1.1.1.1.cmml">=</mo><mn id="A5.T2.2.m1.1.1.3" xref="A5.T2.2.m1.1.1.3.cmml">5000</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.T2.2.m1.1c"><apply id="A5.T2.2.m1.1.1.cmml" xref="A5.T2.2.m1.1.1"><eq id="A5.T2.2.m1.1.1.1.cmml" xref="A5.T2.2.m1.1.1.1"></eq><ci id="A5.T2.2.m1.1.1.2.cmml" xref="A5.T2.2.m1.1.1.2">ğ‘›</ci><cn id="A5.T2.2.m1.1.1.3.cmml" type="integer" xref="A5.T2.2.m1.1.1.3">5000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T2.2.m1.1d">n=5000</annotation><annotation encoding="application/x-llamapun" id="A5.T2.2.m1.1e">italic_n = 5000</annotation></semantics></math> from 95% to 100% for the QA datasets. There is overwhelming agreement between all metrics on the ranking between models trained on different synthetic fractions.</figcaption>
</figure>
<figure class="ltx_figure" id="A5.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F7.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F7.1.g1" src="extracted/5929974/images/full_dataset/factify.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F7.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F7.2.g1" src="extracted/5929974/images/full_dataset/fever.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F7.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F7.3.g1" src="extracted/5929974/images/full_dataset/scifact.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F7.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F7.4.g1" src="extracted/5929974/images/full_dataset/wanli.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F7.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F7.5.g1" src="extracted/5929974/images/full_dataset/fairytaleqa.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F7.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F7.6.g1" src="extracted/5929974/images/full_dataset/ropes.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F7.7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F7.7.g1" src="extracted/5929974/images/full_dataset/coqa.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F7.8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F7.8.g1" src="extracted/5929974/images/full_dataset/qaconv.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Change in model performance as the proportion of synthetic points in the training data is varied. Across datasets, the performance decrease when moving from 0% to 90% synthetic data is often less than that of moving from 90% to purely synthetic data.</figcaption>
</figure>
<figure class="ltx_figure" id="A5.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F8.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F8.1.g1" src="extracted/5929974/images/zoom_5/factify.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F8.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F8.2.g1" src="extracted/5929974/images/zoom_5/fever.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F8.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F8.3.g1" src="extracted/5929974/images/zoom_5/scifact.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F8.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F8.4.g1" src="extracted/5929974/images/zoom_5/wanli.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F8.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F8.5.g1" src="extracted/5929974/images/zoom_5/fairytaleqa.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F8.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F8.6.g1" src="extracted/5929974/images/zoom_5/ropes.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F8.7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F8.7.g1" src="extracted/5929974/images/zoom_5/coqa.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F8.8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F8.8.g1" src="extracted/5929974/images/zoom_5/qaconv.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Model performance as the synthetic proportion of the training data varies from 95% to 100%. Across all datasets and random seeds, having <span class="ltx_text ltx_font_bold" id="A5.F8.10.1">just 2.5%</span> of the training dataset being human generated boosts performance.</figcaption>
</figure>
<figure class="ltx_figure" id="A5.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F9.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F9.1.g1" src="extracted/5929974/images/mistral/fever.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F9.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F9.2.g1" src="extracted/5929974/images/mistral/scifact.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F9.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F9.3.g1" src="extracted/5929974/images/mistral/wanli.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F9.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F9.4.g1" src="extracted/5929974/images/gpt4/fever.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F9.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F9.5.g1" src="extracted/5929974/images/gpt4/scifact.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F9.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F9.6.g1" src="extracted/5929974/images/gpt4/wanli.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Results hold consistently on Fact Verification datasets when using Mistral7B as the fine-tuning model and GPT-4 as the prompting model.</figcaption>
</figure>
<figure class="ltx_figure" id="A5.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F10.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F10.1.g1" src="extracted/5929974/images/mistral/fairytaleqa.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F10.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F10.2.g1" src="extracted/5929974/images/mistral/ropes.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F10.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F10.3.g1" src="extracted/5929974/images/gpt4/fairytaleqa.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F10.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F10.4.g1" src="extracted/5929974/images/gpt4/ropes.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Results hold consistently on Question Answering datasets when using Mistral7B as the fine-tuning model and GPT-4 as the prompting model</figcaption>
</figure>
<figure class="ltx_figure" id="A5.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F11.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F11.1.g1" src="extracted/5929974/images/cot/fever.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F11.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F11.2.g1" src="extracted/5929974/images/cot/scifact.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F11.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F11.3.g1" src="extracted/5929974/images/cot/wanli.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F11.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F11.4.g1" src="extracted/5929974/images/cot/fairytaleqa_bleu.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F11.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F11.5.g1" src="extracted/5929974/images/cot/ropes_bleu.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Results hold when using Chain-Of-Thought Prompting on GPT-3.5</figcaption>
</figure>
<figure class="ltx_figure" id="A5.F12">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F12.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F12.1.g1" src="extracted/5929974/images/zoom_3/factify.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F12.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F12.2.g1" src="extracted/5929974/images/zoom_3/fever.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F12.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F12.3.g1" src="extracted/5929974/images/zoom_3/scifact.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F12.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F12.4.g1" src="extracted/5929974/images/zoom_3/wanli.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F12.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F12.5.g1" src="extracted/5929974/images/zoom_3/fairytaleqa.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F12.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F12.6.g1" src="extracted/5929974/images/zoom_3/ropes.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Model performance as the synthetic proportion of the training data varies from 95% to 100% with total number of points <math alttext="n=3000" class="ltx_Math" display="inline" id="A5.F12.8.m1.1"><semantics id="A5.F12.8.m1.1b"><mrow id="A5.F12.8.m1.1.1" xref="A5.F12.8.m1.1.1.cmml"><mi id="A5.F12.8.m1.1.1.2" xref="A5.F12.8.m1.1.1.2.cmml">n</mi><mo id="A5.F12.8.m1.1.1.1" xref="A5.F12.8.m1.1.1.1.cmml">=</mo><mn id="A5.F12.8.m1.1.1.3" xref="A5.F12.8.m1.1.1.3.cmml">3000</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.F12.8.m1.1c"><apply id="A5.F12.8.m1.1.1.cmml" xref="A5.F12.8.m1.1.1"><eq id="A5.F12.8.m1.1.1.1.cmml" xref="A5.F12.8.m1.1.1.1"></eq><ci id="A5.F12.8.m1.1.1.2.cmml" xref="A5.F12.8.m1.1.1.2">ğ‘›</ci><cn id="A5.F12.8.m1.1.1.3.cmml" type="integer" xref="A5.F12.8.m1.1.1.3">3000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.F12.8.m1.1d">n=3000</annotation><annotation encoding="application/x-llamapun" id="A5.F12.8.m1.1e">italic_n = 3000</annotation></semantics></math>. Across all runs on all datasets including <span class="ltx_text ltx_font_bold" id="A5.F12.10.1">just 75 real datapoints</span> can boost performance.</figcaption>
</figure>
<figure class="ltx_figure" id="A5.F13">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F13.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F13.1.g1" src="extracted/5929974/images/zoom_1/factify.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F13.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F13.2.g1" src="extracted/5929974/images/zoom_1/fever.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F13.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F13.3.g1" src="extracted/5929974/images/zoom_1/scifact.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F13.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F13.4.g1" src="extracted/5929974/images/zoom_1/wanli.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F13.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F13.5.g1" src="extracted/5929974/images/zoom_1/fairytaleqa.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A5.F13.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F13.6.g1" src="extracted/5929974/images/zoom_1/ropes.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Model performance as the synthetic proportion of the training data varies from 95% to 100% with total number of points <math alttext="n=1000" class="ltx_Math" display="inline" id="A5.F13.8.m1.1"><semantics id="A5.F13.8.m1.1b"><mrow id="A5.F13.8.m1.1.1" xref="A5.F13.8.m1.1.1.cmml"><mi id="A5.F13.8.m1.1.1.2" xref="A5.F13.8.m1.1.1.2.cmml">n</mi><mo id="A5.F13.8.m1.1.1.1" xref="A5.F13.8.m1.1.1.1.cmml">=</mo><mn id="A5.F13.8.m1.1.1.3" xref="A5.F13.8.m1.1.1.3.cmml">1000</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.F13.8.m1.1c"><apply id="A5.F13.8.m1.1.1.cmml" xref="A5.F13.8.m1.1.1"><eq id="A5.F13.8.m1.1.1.1.cmml" xref="A5.F13.8.m1.1.1.1"></eq><ci id="A5.F13.8.m1.1.1.2.cmml" xref="A5.F13.8.m1.1.1.2">ğ‘›</ci><cn id="A5.F13.8.m1.1.1.3.cmml" type="integer" xref="A5.F13.8.m1.1.1.3">1000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.F13.8.m1.1d">n=1000</annotation><annotation encoding="application/x-llamapun" id="A5.F13.8.m1.1e">italic_n = 1000</annotation></semantics></math>. While the most common trend is that including real data improves performance, the results are much more unstable.</figcaption>
</figure>
<figure class="ltx_figure" id="A5.F14">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F14.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F14.1.g1" src="extracted/5929974/images/money/fever.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F14.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F14.2.g1" src="extracted/5929974/images/money/wanli.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F14.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F14.3.g1" src="extracted/5929974/images/money/fairytaleqa.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F14.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F14.4.g1" src="extracted/5929974/images/money/ropes.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Adding 200 real data points is as effective as adding an order of magnitude more synthetic data points.</figcaption>
</figure>
<figure class="ltx_table" id="A5.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A5.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A5.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A5.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A5.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.1.2.1">Mean</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A5.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.1.3.1">Median</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A5.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.1.4.1">25th Percentile</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A5.T3.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.1.5.1">75th Percentile</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T3.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.2.1.1">WANLI</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.2.1.2">17671.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.2.1.3">16905.38</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.2.1.4">9711.42</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.2.1.5">22931.17</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.3.2">
<td class="ltx_td ltx_align_center" id="A5.T3.1.3.2.1">ROPES</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.3.2.2">17332.69</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.3.2.3">6006.45</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.3.2.4">3622.95</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.3.2.5">21944.36</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.4.3">
<td class="ltx_td ltx_align_center" id="A5.T3.1.4.3.1">FairyTaleQA</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.4.3.2">281951.01</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.4.3.3">36901.45</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.4.3.4">15128.74</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.4.3.5">813134.81</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_b" id="A5.T3.1.5.4.1">FEVER</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A5.T3.1.5.4.2">1155.13</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A5.T3.1.5.4.3">236.37</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A5.T3.1.5.4.4">-1400.05</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A5.T3.1.5.4.5">7073.19</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Summary statistics for the number of additional synthetic data points needed to match performance gain of 200 human data points (aggregated over various fixed operating points of synthetic data). High values for FairyTaleQA suggests that human generated data may unlock performance that is unachievable with purely synthetic data. Negative values for FEVER are due to a saturation of the performance gains, however human data helps reach the saturation point much faster (see SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A1" title="Appendix A Supplemental Figures â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">A</span></a> and FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.13098v1#A5.F14" title="Figure 14 â€£ Appendix E Implementation Details â€£ A Little Human Data Goes A Long Way"><span class="ltx_text ltx_ref_tag">14</span></a>).</figcaption>
</figure>
<figure class="ltx_figure" id="A5.F15">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="378" id="A5.F15.g1" src="extracted/5929974/images/prompt_fv.jpg" width="419"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="340" id="A5.F15.g2" src="extracted/5929974/images/prompt_qa.jpg" width="419"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Example prompts used to synthetically generate (claim, label) or (question, answer) pairs using a new context / evidence text.</figcaption>
</figure>
<figure class="ltx_figure" id="A5.F16">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F16.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F16.1.g1" src="extracted/5929974/images/analysis/gpt-3.5-turbo/cot_False/ropes/generated/question_length.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F16.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F16.2.g1" src="extracted/5929974/images/analysis/gpt-3.5-turbo/cot_False/wanli/generated/claim_length.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F16.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F16.3.g1" src="extracted/5929974/images/analysis/gpt-3.5-turbo/cot_False/coqa/generated/answer_length.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F16.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F16.4.g1" src="extracted/5929974/images/analysis/gpt-3.5-turbo/cot_False/qaconv/generated/question_length.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F16.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F16.5.g1" src="extracted/5929974/images/analysis/gpt-3.5-turbo/cot_True/fairytaleqa/generated/answer_length.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F16.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F16.6.g1" src="extracted/5929974/images/analysis/gpt-3.5-turbo/cot_True/fever/generated/claim_length.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F16.7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F16.7.g1" src="extracted/5929974/images/analysis/gpt-4-turbo/cot_False/wanli/generated/claim_length.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F16.8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F16.8.g1" src="extracted/5929974/images/analysis/gpt-4-turbo/cot_False/fairytaleqa/generated/question_length.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Synthetic data is, on average, longer than its human generated counterpart. This trend can be seen on FV (claims) and QA (claims and questions), and holds across prompt models and strategies.</figcaption>
</figure>
<figure class="ltx_figure" id="A5.F17">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F17.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F17.1.g1" src="extracted/5929974/images/analysis/gpt-3.5-turbo/cot_False/ropes/generated/question_bleu_with_evidence_text_best.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F17.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F17.2.g1" src="extracted/5929974/images/analysis/gpt-3.5-turbo/cot_False/wanli/generated/claim_bleu_with_evidence_text_best.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F17.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F17.3.g1" src="extracted/5929974/images/analysis/gpt-3.5-turbo/cot_False/coqa/generated/answer_bleu_with_evidence_text_best.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F17.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F17.4.g1" src="extracted/5929974/images/analysis/gpt-3.5-turbo/cot_False/qaconv/generated/question_bleu_with_evidence_text_best.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F17.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F17.5.g1" src="extracted/5929974/images/analysis/gpt-3.5-turbo/cot_True/fairytaleqa/generated/answer_bleu_with_evidence_text_best.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F17.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F17.6.g1" src="extracted/5929974/images/analysis/gpt-3.5-turbo/cot_True/fever/generated/claim_bleu_with_evidence_text_best.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F17.7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F17.7.g1" src="extracted/5929974/images/analysis/gpt-4-turbo/cot_False/wanli/generated/claim_bleu_with_evidence_text_best.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F17.8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F17.8.g1" src="extracted/5929974/images/analysis/gpt-4-turbo/cot_False/fairytaleqa/generated/answer_bleu_with_evidence_text_best.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Synthetic data generally exhibits a higher maximum BLEU score measured against sentences from the context. This suggests that synthetic questions, answers, and claims are more extractive than their human generated counterparts</figcaption>
</figure>
<figure class="ltx_figure" id="A5.F18">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F18.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F18.1.g1" src="extracted/5929974/images/analysis/gpt-3.5-turbo/cot_False/ropes/generated/relative_position_of_answer_in_evidence_text.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F18.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F18.2.g1" src="extracted/5929974/images/analysis/gpt-3.5-turbo/cot_False/wanli/generated/relative_position_of_claim_in_evidence_text.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F18.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F18.3.g1" src="extracted/5929974/images/analysis/gpt-3.5-turbo/cot_False/coqa/generated/relative_position_of_answer_in_evidence_text.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F18.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F18.4.g1" src="extracted/5929974/images/analysis/gpt-3.5-turbo/cot_False/qaconv/generated/relative_position_of_answer_in_evidence_text.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F18.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F18.5.g1" src="extracted/5929974/images/analysis/gpt-3.5-turbo/cot_True/fairytaleqa/generated/relative_position_of_answer_in_evidence_text.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A5.F18.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="A5.F18.6.g1" src="extracted/5929974/images/analysis/gpt-4-turbo/cot_False/fairytaleqa/generated/relative_position_of_answer_in_evidence_text.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>Synthetic data typically chooses more diverse sources (in terms of answer location or claim location in the evidence text), while humans tend to favor the start of the evidence text.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct 17 00:03:48 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
