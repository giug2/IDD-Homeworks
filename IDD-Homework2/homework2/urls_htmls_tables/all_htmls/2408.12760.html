<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.12760] Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification</title><meta property="og:description" content="Hyperspectral image (HSI) and synthetic aperture radar (SAR) data joint classification is a crucial and yet challenging task in the field of remote sensing image interpretation. However, feature modeling in existing me…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.12760">

<!--Generated on Thu Sep  5 16:20:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Hyperspectral image; Multi-source data classification; Hierarchical attention; Parallel filter fusion; Synthetic aperture radar.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Han Luo, Feng Gao, Junyu Dong, and Lin Qi
</span><span class="ltx_author_notes">This work was supported in part by the National Science and Technology Major Project under Grant 2022ZD0117201, and in part by the Natural Science Foundation of Qingdao under Grant 23-2-1-222-ZYYD-JCH. (<span id="id1.1.id1" class="ltx_text ltx_font_italic">Corresponding author: Feng Gao</span>)
Han Luo, Feng Gao, Junyu Dong, Lin Qi are with the School of Information Science and Engineering, Ocean University of China, Qingdao 266100, China.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Hyperspectral image (HSI) and synthetic aperture radar (SAR) data joint classification is a crucial and yet challenging task in the field of remote sensing image interpretation. However, feature modeling in existing methods is deficient to exploit the abundant global, spectral, and local features simultaneously, leading to sub-optimal classification performance. To solve the problem, we propose a hierarchical attention and parallel filter fusion network for multi-source data classification. Concretely, we design a hierarchical attention module for hyperspectral feature extraction. This module integrates global, spectral, and local features simultaneously to provide more comprehensive feature representation. In addition, we develop parallel filter fusion module which enhances cross-modal feature interactions among different spatial locations in the frequency domain. Extensive experiments on two multi-source remote sensing data classification datasets verify the superiority of our proposed method over current state-of-the-art classification approaches. Specifically, our proposed method achieves 91.44% and 80.51% of overall accuracy (OA) on the respective datasets, highlighting its superior performance.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Hyperspectral image; Multi-source data classification; Hierarchical attention; Parallel filter fusion; Synthetic aperture radar.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With the advancement of earth observation technology and satellite sensor platforms, a large amount of remote sensing data has been obtained. Among these data, Hyperspectral images (HSIs) have received a lot of attention due to their rich spectral information and have been widely used for land cover classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. However, spectral mixing occurs when HSI contains multiple land cover types, and it affects the HSI classification performance.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To solve the problem, synthetic aperture radar (SAR) image is often used to provide complementary information for HSI, since the SAR sensor is particularly useful in cloudy or hazy environments when HSI sensors may be limited. By combining HSI with SAR data, land cover classification methods can mitigate the effects of atmospheric interference, leading to more robust classification results. Therefore, in this letter, we mainly focus on HSI and SAR data joint classification.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Traditionally, attribute and extinction profile <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> are used for multisource data feature extraction. Xia et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> presented graph fusion-based method, in which morphological filters were used to the key components of cross-modal data. Then, spectral, spatial, and texture features were projected to a lower subspace to compute joint features. However, traditional feature extraction methods are limited in extracting high-level semantic information from raw data.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Recently, many deep learning-based HSI and SAR data joint classification methods have been proposed. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> presented a multi-scale interactive fusion network for multi-source data classification. Multi-scale features are extracted and fused via global dependence fusion module. Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> presented an asymmetric feature fusion network, in which a feature calibration module is designed to exploit the spatial dependence of multisource features. In addition, graph neural network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, bilinear fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and adversarial learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> are employed for multi-source remote sensing data classification.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2408.12760/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="243" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of the proposed HAPNet. In the HAPNet Block, multi-level spatial signals are fused through Hierarchical Attention Module (HAM), and the ability to interact and fuse feature information is enhanced through the Parallel Filter Fusion Module (PFFM).</figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Although existing methods have achieved remarkable performance, it is non-trivial to build a robust classification model due to the following two challenges: <span id="S1.p5.1.1" class="ltx_text ltx_font_bold">1) Multi-granularity feature modeling for HSI data.</span> Cross-scale feature extraction is important for HSI feature representation. Existing methods can hardly model information at multiple granularities. Hence, how to modeling global, spectral, and local features simultaneously for HSI is a non-trivial task. <span id="S1.p5.1.2" class="ltx_text ltx_font_bold">2) How to enhance cross-modal feature interactions between HSI and SAR data.</span> Existing multi-source image classification methods commonly capture multi-source feature interactions in the spatial domain. Feature interactions in the frequency domain is rarely explored. How to uncover the cross-modal feature interactions in the frequency domain is of great importance.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">To overcome the above limitations, we propose a <span id="S1.p6.1.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">H</span>ierarchical <span id="S1.p6.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">A</span>ttention and <span id="S1.p6.1.3" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">P</span>arallel filter fusion <span id="S1.p6.1.4" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">Net</span>work, dubbed as HAPNet. Specifically, to efficiently extract multi-scale information, we design a Hierarchical Attention Module (HAM) to capture global, spectral and local information from HSI simultaneously. In addition, to enhance cross-modal feature interactions between HSI and SAR data, we propose a Parallel Filter Fusion Module (PFFM) for feature fusion. The feature interactions between HSI and SAR data are modeled as a set of learnable global filters which are applied to the spectrum of the input features. Therefore, cross-modal feature interactions among spatial locations are enhanced in the frequency domain. Extensive experiments on two HSI and SAR datasets have fully validated that our proposed method is superior to other state-of-the-art competitors.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Our main contributions can be summarized as follows:</p>
</div>
<div id="S1.p8" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We present the HAM as an enhancement to the self-attention mechanism. This module integrates global, spectral, and local features to provide more comprehensive feature representation for multi-source remote sensing image classification.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We develop PFFM to enhance cross-modal feature interactions in the frequency domain. The module enhances feature interactions among different spatial locations in the frequency domain, and thus effectively improves the classification performance.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">As illustrated in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the proposed HAPNet comprises HSI feature encoder, SAR feature encoder, feature fusion modules, and a classifier. Three hierarchical attention blocks are employed for HSI feature encoder, and three convolutional layers are used for SAR feature encoder. Subsequently, features from each level, derived from two encoder branches, are processed through feature fusion module. Finally, the fused features are fed into two fully-connected layers to generate the outcome. It should be noted that feature expansion is used here, and the expansion ratio is set to 2. In the HSI feature encoder, Principle Component Analysis (PCA) is employed to select the best 30 spectral bands. As illustrated in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, HAM and PFFM are the key components to improve the classification results. Subsequently, we provide in-depth descriptions of both modules.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2408.12760/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="253" height="316" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Illustration of Hierarchical Attention Module (HAM). Global, spectral and local features are modeled in parallel. The global and spectral features are extracted via anchored self-attention in the spatial and spectral dimensions, respectively. The local features captures local structures via depth-wise convolutions.</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Hierarchical Attention Module (HAM)</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">As depicted in Fig. <a href="#S2.F2" title="Figure 2 ‣ II Methodology ‣ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, HAM is the key component that provides the hierarchical feature modeling capacity in the global, spectral, and local range. HAM first split the input feature into three branches. The global branch models long-range feature dependencies in the spatial dimension via anchored self-attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. The spectral branch models long-range feature dependencies in the spectral dimension via shifted windows attention. The local branch captures local structures in the input feature via depth-wise convolutions. Features maps generated from the global, spectral, and local branch are combined via element-wise summation, and finally fed to the FFN to enhance the non-linear feature transformation. It should be noted that HAM is an extension of the self-attention mechanism. HAM provides a more comprehensive multi-granularity feature modeling approach through multi-scale and parallel feature processing. HAM captures global, spectral, and local information simultaneously, while the self-attention only captures global feature dependencies.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.5" class="ltx_p"><span id="S2.SS1.p2.5.1" class="ltx_text ltx_font_bold">Global and Spectral Feature Modeling.</span> In global and spectral branches, we use anchor self-attention for feature modeling. As depicted in Fig. <a href="#S2.F2" title="Figure 2 ‣ II Methodology ‣ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the triplet of <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{Q}" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">𝐐</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">𝐐</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\mathbf{Q}</annotation></semantics></math>, <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{K}" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">𝐊</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">𝐊</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">\mathbf{K}</annotation></semantics></math>, <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="\mathbf{V}" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><mi id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">𝐕</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><ci id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">𝐕</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">\mathbf{V}</annotation></semantics></math> is generated by linear projections. Then, an average pooling layer is implemented to reduce the spatial/spectral dimensions to generate <span id="S2.SS1.p2.5.2" class="ltx_text ltx_markedasmath ltx_font_bold">A</span>. The spatial/spectral dimension is down-scaled by a factor of <math id="S2.SS1.p2.5.m5.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S2.SS1.p2.5.m5.1a"><mi id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b"><ci id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">s</annotation></semantics></math>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">To be specific, the anchor self-attention is computed as follows:</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_Math" alttext="\mathbf{Y}=\mathbf{M_{e}}\cdot\mathbf{Z}=\mathbf{M_{e}}\cdot\left(\mathbf{M_{d}}\cdot\mathbf{V}\right)," display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mrow id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.3.cmml">𝐘</mi><mo id="S2.E1.m1.1.1.1.1.4" xref="S2.E1.m1.1.1.1.1.4.cmml">=</mo><mrow id="S2.E1.m1.1.1.1.1.5" xref="S2.E1.m1.1.1.1.1.5.cmml"><msub id="S2.E1.m1.1.1.1.1.5.2" xref="S2.E1.m1.1.1.1.1.5.2.cmml"><mi id="S2.E1.m1.1.1.1.1.5.2.2" xref="S2.E1.m1.1.1.1.1.5.2.2.cmml">𝐌</mi><mi id="S2.E1.m1.1.1.1.1.5.2.3" xref="S2.E1.m1.1.1.1.1.5.2.3.cmml">𝐞</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.E1.m1.1.1.1.1.5.1" xref="S2.E1.m1.1.1.1.1.5.1.cmml">⋅</mo><mi id="S2.E1.m1.1.1.1.1.5.3" xref="S2.E1.m1.1.1.1.1.5.3.cmml">𝐙</mi></mrow><mo id="S2.E1.m1.1.1.1.1.6" xref="S2.E1.m1.1.1.1.1.6.cmml">=</mo><mrow id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.1.3.2.cmml">𝐌</mi><mi id="S2.E1.m1.1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.1.3.3.cmml">𝐞</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.E1.m1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.2.cmml">⋅</mo><mrow id="S2.E1.m1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml">𝐌</mi><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml">𝐝</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.E1.m1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml">⋅</mo><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.cmml">𝐕</mi></mrow><mo id="S2.E1.m1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><and id="S2.E1.m1.1.1.1.1a.cmml" xref="S2.E1.m1.1.1.1"></and><apply id="S2.E1.m1.1.1.1.1b.cmml" xref="S2.E1.m1.1.1.1"><eq id="S2.E1.m1.1.1.1.1.4.cmml" xref="S2.E1.m1.1.1.1.1.4"></eq><ci id="S2.E1.m1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.3">𝐘</ci><apply id="S2.E1.m1.1.1.1.1.5.cmml" xref="S2.E1.m1.1.1.1.1.5"><ci id="S2.E1.m1.1.1.1.1.5.1.cmml" xref="S2.E1.m1.1.1.1.1.5.1">⋅</ci><apply id="S2.E1.m1.1.1.1.1.5.2.cmml" xref="S2.E1.m1.1.1.1.1.5.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.5.2.1.cmml" xref="S2.E1.m1.1.1.1.1.5.2">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.5.2.2.cmml" xref="S2.E1.m1.1.1.1.1.5.2.2">𝐌</ci><ci id="S2.E1.m1.1.1.1.1.5.2.3.cmml" xref="S2.E1.m1.1.1.1.1.5.2.3">𝐞</ci></apply><ci id="S2.E1.m1.1.1.1.1.5.3.cmml" xref="S2.E1.m1.1.1.1.1.5.3">𝐙</ci></apply></apply><apply id="S2.E1.m1.1.1.1.1c.cmml" xref="S2.E1.m1.1.1.1"><eq id="S2.E1.m1.1.1.1.1.6.cmml" xref="S2.E1.m1.1.1.1.1.6"></eq><share href="#S2.E1.m1.1.1.1.1.5.cmml" id="S2.E1.m1.1.1.1.1d.cmml" xref="S2.E1.m1.1.1.1"></share><apply id="S2.E1.m1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"><ci id="S2.E1.m1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.2">⋅</ci><apply id="S2.E1.m1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.3.2">𝐌</ci><ci id="S2.E1.m1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.3.3">𝐞</ci></apply><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1"><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1">⋅</ci><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.2">𝐌</ci><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.3">𝐝</ci></apply><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3">𝐕</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\mathbf{Y}=\mathbf{M_{e}}\cdot\mathbf{Z}=\mathbf{M_{e}}\cdot\left(\mathbf{M_{d}}\cdot\mathbf{V}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.1" class="ltx_Math" alttext="\mathbf{M_{d}}=\text{Softmax}\left(\mathbf{A}\cdot\mathbf{K}^{T}/\sqrt{d}\right)," display="block"><semantics id="S2.E2.m1.1a"><mrow id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml"><mrow id="S2.E2.m1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml"><msub id="S2.E2.m1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.1.1.1.1.3.2" xref="S2.E2.m1.1.1.1.1.3.2.cmml">𝐌</mi><mi id="S2.E2.m1.1.1.1.1.3.3" xref="S2.E2.m1.1.1.1.1.3.3.cmml">𝐝</mi></msub><mo id="S2.E2.m1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S2.E2.m1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.cmml"><mtext id="S2.E2.m1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.3a.cmml">Softmax</mtext><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E2.m1.1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E2.m1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E2.m1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.2.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.cmml">𝐀</mi><mo lspace="0.222em" rspace="0.222em" id="S2.E2.m1.1.1.1.1.1.1.1.1.2.1" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.1.cmml">⋅</mo><msup id="S2.E2.m1.1.1.1.1.1.1.1.1.2.3" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.2.cmml">𝐊</mi><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.3.cmml">T</mi></msup></mrow><mo id="S2.E2.m1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.cmml">/</mo><msqrt id="S2.E2.m1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml">d</mi></msqrt></mrow><mo id="S2.E2.m1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E2.m1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"><eq id="S2.E2.m1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.2"></eq><apply id="S2.E2.m1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.1.1.3.2">𝐌</ci><ci id="S2.E2.m1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.1.1.1.1.3.3">𝐝</ci></apply><apply id="S2.E2.m1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1"><times id="S2.E2.m1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1.2"></times><ci id="S2.E2.m1.1.1.1.1.1.3a.cmml" xref="S2.E2.m1.1.1.1.1.1.3"><mtext id="S2.E2.m1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.3">Softmax</mtext></ci><apply id="S2.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1"><divide id="S2.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1"></divide><apply id="S2.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2"><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.1">⋅</ci><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.2">𝐀</ci><apply id="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.3">superscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.2">𝐊</ci><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.3">𝑇</ci></apply></apply><apply id="S2.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.3"><root id="S2.E2.m1.1.1.1.1.1.1.1.1.3a.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.3"></root><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.3.2">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">\mathbf{M_{d}}=\text{Softmax}\left(\mathbf{A}\cdot\mathbf{K}^{T}/\sqrt{d}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.1" class="ltx_Math" alttext="\mathbf{M_{e}}=\text{Softmax}\left(\mathbf{Q}\cdot\mathbf{A}^{T}/\sqrt{d}\right)," display="block"><semantics id="S2.E3.m1.1a"><mrow id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml"><mrow id="S2.E3.m1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml"><msub id="S2.E3.m1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.3.cmml"><mi id="S2.E3.m1.1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.1.3.2.cmml">𝐌</mi><mi id="S2.E3.m1.1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.1.3.3.cmml">𝐞</mi></msub><mo id="S2.E3.m1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.2.cmml">=</mo><mrow id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.cmml"><mtext id="S2.E3.m1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.3a.cmml">Softmax</mtext><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E3.m1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E3.m1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E3.m1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E3.m1.1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.2.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml">𝐐</mi><mo lspace="0.222em" rspace="0.222em" id="S2.E3.m1.1.1.1.1.1.1.1.1.2.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml">⋅</mo><msup id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.2.cmml">𝐀</mi><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.3.cmml">T</mi></msup></mrow><mo id="S2.E3.m1.1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.cmml">/</mo><msqrt id="S2.E3.m1.1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml">d</mi></msqrt></mrow><mo id="S2.E3.m1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E3.m1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.1b"><apply id="S2.E3.m1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"><eq id="S2.E3.m1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.2"></eq><apply id="S2.E3.m1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.1.3">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.1.3.2">𝐌</ci><ci id="S2.E3.m1.1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.1.3.3">𝐞</ci></apply><apply id="S2.E3.m1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1"><times id="S2.E3.m1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.2"></times><ci id="S2.E3.m1.1.1.1.1.1.3a.cmml" xref="S2.E3.m1.1.1.1.1.1.3"><mtext id="S2.E3.m1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.3">Softmax</mtext></ci><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1"><divide id="S2.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1"></divide><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2"><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.1">⋅</ci><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.2">𝐐</ci><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3">superscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.2">𝐀</ci><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2.3.3">𝑇</ci></apply></apply><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3"><root id="S2.E3.m1.1.1.1.1.1.1.1.1.3a.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3"></root><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.3.2">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.1c">\mathbf{M_{e}}=\text{Softmax}\left(\mathbf{Q}\cdot\mathbf{A}^{T}/\sqrt{d}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p4.3" class="ltx_p">where <math id="S2.SS1.p4.1.m1.1" class="ltx_Math" alttext="\mathbf{A}" display="inline"><semantics id="S2.SS1.p4.1.m1.1a"><mi id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml">𝐀</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><ci id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1">𝐀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">\mathbf{A}</annotation></semantics></math> is the anchor, <math id="S2.SS1.p4.2.m2.1" class="ltx_Math" alttext="\mathbf{M}_{e}" display="inline"><semantics id="S2.SS1.p4.2.m2.1a"><msub id="S2.SS1.p4.2.m2.1.1" xref="S2.SS1.p4.2.m2.1.1.cmml"><mi id="S2.SS1.p4.2.m2.1.1.2" xref="S2.SS1.p4.2.m2.1.1.2.cmml">𝐌</mi><mi id="S2.SS1.p4.2.m2.1.1.3" xref="S2.SS1.p4.2.m2.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.m2.1b"><apply id="S2.SS1.p4.2.m2.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.2.m2.1.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p4.2.m2.1.1.2.cmml" xref="S2.SS1.p4.2.m2.1.1.2">𝐌</ci><ci id="S2.SS1.p4.2.m2.1.1.3.cmml" xref="S2.SS1.p4.2.m2.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.m2.1c">\mathbf{M}_{e}</annotation></semantics></math> denotes the attention maps between the query-anchor pair, and <math id="S2.SS1.p4.3.m3.1" class="ltx_Math" alttext="\mathbf{M}_{d}" display="inline"><semantics id="S2.SS1.p4.3.m3.1a"><msub id="S2.SS1.p4.3.m3.1.1" xref="S2.SS1.p4.3.m3.1.1.cmml"><mi id="S2.SS1.p4.3.m3.1.1.2" xref="S2.SS1.p4.3.m3.1.1.2.cmml">𝐌</mi><mi id="S2.SS1.p4.3.m3.1.1.3" xref="S2.SS1.p4.3.m3.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.3.m3.1b"><apply id="S2.SS1.p4.3.m3.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.3.m3.1.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.p4.3.m3.1.1.2.cmml" xref="S2.SS1.p4.3.m3.1.1.2">𝐌</ci><ci id="S2.SS1.p4.3.m3.1.1.3.cmml" xref="S2.SS1.p4.3.m3.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.3.m3.1c">\mathbf{M}_{d}</annotation></semantics></math> denotes the attention maps between the anchor-key pair.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p"><span id="S2.SS1.p5.1.1" class="ltx_text ltx_font_bold">Local Feature Modeling.</span> In local branch, channel-attention enhanced convolution is used for local feature modeling. Specifically, two depth-wise convolution layers are employed for feature extraction, and then channel-wise attention is used for feature enhancement.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Parallel Filter Fusion Module (PFFM)</span>
</h3>

<figure id="S2.F3" class="ltx_figure"><img src="/html/2408.12760/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="253" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Illustration of the Parallel Filter Fusion Network (PFFM).</figcaption>
</figure>
<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.7" class="ltx_p">To enhance cross-model feature interactions, we propose the PFFM to adaptively fuse HSI and SAR features. It integrates Fourier transform-based filters into existing feature fusion network. As shown in Fig. <a href="#S2.F3" title="Figure 3 ‣ II-B Parallel Filter Fusion Module (PFFM) ‣ II Methodology ‣ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the proposed PFFM comprises of three parallel paths. We use <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="F_{h}" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><msub id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mi id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">F</mi><mi id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">𝐹</ci><ci id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">F_{h}</annotation></semantics></math> and <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="F_{s}" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><msub id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.p1.2.m2.1.1.2" xref="S2.SS2.p1.2.m2.1.1.2.cmml">F</mi><mi id="S2.SS2.p1.2.m2.1.1.3" xref="S2.SS2.p1.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2">𝐹</ci><ci id="S2.SS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">F_{s}</annotation></semantics></math> to represent features from HSI and SAR, respectively. First, we use element-wise multiplication between <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="F_{h}" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><msub id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml"><mi id="S2.SS2.p1.3.m3.1.1.2" xref="S2.SS2.p1.3.m3.1.1.2.cmml">F</mi><mi id="S2.SS2.p1.3.m3.1.1.3" xref="S2.SS2.p1.3.m3.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><apply id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m3.1.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.p1.3.m3.1.1.2.cmml" xref="S2.SS2.p1.3.m3.1.1.2">𝐹</ci><ci id="S2.SS2.p1.3.m3.1.1.3.cmml" xref="S2.SS2.p1.3.m3.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">F_{h}</annotation></semantics></math> and <math id="S2.SS2.p1.4.m4.1" class="ltx_Math" alttext="F_{s}" display="inline"><semantics id="S2.SS2.p1.4.m4.1a"><msub id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml"><mi id="S2.SS2.p1.4.m4.1.1.2" xref="S2.SS2.p1.4.m4.1.1.2.cmml">F</mi><mi id="S2.SS2.p1.4.m4.1.1.3" xref="S2.SS2.p1.4.m4.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><apply id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.4.m4.1.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.p1.4.m4.1.1.2.cmml" xref="S2.SS2.p1.4.m4.1.1.2">𝐹</ci><ci id="S2.SS2.p1.4.m4.1.1.3.cmml" xref="S2.SS2.p1.4.m4.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">F_{s}</annotation></semantics></math> to generate <math id="S2.SS2.p1.5.m5.1" class="ltx_Math" alttext="F_{f}" display="inline"><semantics id="S2.SS2.p1.5.m5.1a"><msub id="S2.SS2.p1.5.m5.1.1" xref="S2.SS2.p1.5.m5.1.1.cmml"><mi id="S2.SS2.p1.5.m5.1.1.2" xref="S2.SS2.p1.5.m5.1.1.2.cmml">F</mi><mi id="S2.SS2.p1.5.m5.1.1.3" xref="S2.SS2.p1.5.m5.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.1b"><apply id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.5.m5.1.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS2.p1.5.m5.1.1.2.cmml" xref="S2.SS2.p1.5.m5.1.1.2">𝐹</ci><ci id="S2.SS2.p1.5.m5.1.1.3.cmml" xref="S2.SS2.p1.5.m5.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.1c">F_{f}</annotation></semantics></math>. Then, <math id="S2.SS2.p1.6.m6.1" class="ltx_Math" alttext="F_{f}" display="inline"><semantics id="S2.SS2.p1.6.m6.1a"><msub id="S2.SS2.p1.6.m6.1.1" xref="S2.SS2.p1.6.m6.1.1.cmml"><mi id="S2.SS2.p1.6.m6.1.1.2" xref="S2.SS2.p1.6.m6.1.1.2.cmml">F</mi><mi id="S2.SS2.p1.6.m6.1.1.3" xref="S2.SS2.p1.6.m6.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.6.m6.1b"><apply id="S2.SS2.p1.6.m6.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.6.m6.1.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.p1.6.m6.1.1.2.cmml" xref="S2.SS2.p1.6.m6.1.1.2">𝐹</ci><ci id="S2.SS2.p1.6.m6.1.1.3.cmml" xref="S2.SS2.p1.6.m6.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.6.m6.1c">F_{f}</annotation></semantics></math> is fed into the global filter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> to generate the attention weights <math id="S2.SS2.p1.7.m7.1" class="ltx_Math" alttext="\mathbf{W}" display="inline"><semantics id="S2.SS2.p1.7.m7.1a"><mi id="S2.SS2.p1.7.m7.1.1" xref="S2.SS2.p1.7.m7.1.1.cmml">𝐖</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.7.m7.1b"><ci id="S2.SS2.p1.7.m7.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1">𝐖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.7.m7.1c">\mathbf{W}</annotation></semantics></math>. This process can be formulated as follows:</p>
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1.1" class="ltx_Math" alttext="\mathbf{W}=\mathcal{F}^{-1}(K\odot\mathcal{F}(F_{h}\otimes F_{s}))," display="block"><semantics id="S2.E4.m1.1a"><mrow id="S2.E4.m1.1.1.1" xref="S2.E4.m1.1.1.1.1.cmml"><mrow id="S2.E4.m1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.cmml"><mi id="S2.E4.m1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.3.cmml">𝐖</mi><mo id="S2.E4.m1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.2.cmml">=</mo><mrow id="S2.E4.m1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.cmml"><msup id="S2.E4.m1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.1.1.1.1.1.3.2" xref="S2.E4.m1.1.1.1.1.1.3.2.cmml">ℱ</mi><mrow id="S2.E4.m1.1.1.1.1.1.3.3" xref="S2.E4.m1.1.1.1.1.1.3.3.cmml"><mo id="S2.E4.m1.1.1.1.1.1.3.3a" xref="S2.E4.m1.1.1.1.1.1.3.3.cmml">−</mo><mn id="S2.E4.m1.1.1.1.1.1.3.3.2" xref="S2.E4.m1.1.1.1.1.1.3.3.2.cmml">1</mn></mrow></msup><mo lspace="0em" rspace="0em" id="S2.E4.m1.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E4.m1.1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E4.m1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E4.m1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E4.m1.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.1.1.1.1.1.1.1.1.3.2" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3.2.cmml">K</mi><mo lspace="0.222em" rspace="0.222em" id="S2.E4.m1.1.1.1.1.1.1.1.1.3.1" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3.1.cmml">⊙</mo><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.1.1.1.1.1.1.1.1.3.3" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3.3.cmml">ℱ</mi></mrow><mo lspace="0em" rspace="0em" id="S2.E4.m1.1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">F</mi><mi id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">h</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">⊗</mo><msub id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">F</mi><mi id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">s</mi></msub></mrow><mo stretchy="false" id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E4.m1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E4.m1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.1b"><apply id="S2.E4.m1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1"><eq id="S2.E4.m1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.2"></eq><ci id="S2.E4.m1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.3">𝐖</ci><apply id="S2.E4.m1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1"><times id="S2.E4.m1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.1.2"></times><apply id="S2.E4.m1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.1.1.1.1.1.3">superscript</csymbol><ci id="S2.E4.m1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.1.1.1.1.1.3.2">ℱ</ci><apply id="S2.E4.m1.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.1.1.1.1.1.3.3"><minus id="S2.E4.m1.1.1.1.1.1.3.3.1.cmml" xref="S2.E4.m1.1.1.1.1.1.3.3"></minus><cn type="integer" id="S2.E4.m1.1.1.1.1.1.3.3.2.cmml" xref="S2.E4.m1.1.1.1.1.1.3.3.2">1</cn></apply></apply><apply id="S2.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1"><times id="S2.E4.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.2"></times><apply id="S2.E4.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="latexml" id="S2.E4.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3.1">direct-product</csymbol><ci id="S2.E4.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3.2">𝐾</ci><ci id="S2.E4.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3.3">ℱ</ci></apply><apply id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1">tensor-product</csymbol><apply id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝐹</ci><ci id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.3">ℎ</ci></apply><apply id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝐹</ci><ci id="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑠</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.1c">\mathbf{W}=\mathcal{F}^{-1}(K\odot\mathcal{F}(F_{h}\otimes F_{s})),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p1.10" class="ltx_p">where <math id="S2.SS2.p1.8.m1.1" class="ltx_Math" alttext="\mathcal{F}" display="inline"><semantics id="S2.SS2.p1.8.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.8.m1.1.1" xref="S2.SS2.p1.8.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.8.m1.1b"><ci id="S2.SS2.p1.8.m1.1.1.cmml" xref="S2.SS2.p1.8.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.8.m1.1c">\mathcal{F}</annotation></semantics></math> denotes the 2D FFT, <math id="S2.SS2.p1.9.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.SS2.p1.9.m2.1a"><mi id="S2.SS2.p1.9.m2.1.1" xref="S2.SS2.p1.9.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.9.m2.1b"><ci id="S2.SS2.p1.9.m2.1.1.cmml" xref="S2.SS2.p1.9.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.9.m2.1c">K</annotation></semantics></math> is a learnable filter, <math id="S2.SS2.p1.10.m3.1" class="ltx_Math" alttext="\mathcal{F}^{-1}" display="inline"><semantics id="S2.SS2.p1.10.m3.1a"><msup id="S2.SS2.p1.10.m3.1.1" xref="S2.SS2.p1.10.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.10.m3.1.1.2" xref="S2.SS2.p1.10.m3.1.1.2.cmml">ℱ</mi><mrow id="S2.SS2.p1.10.m3.1.1.3" xref="S2.SS2.p1.10.m3.1.1.3.cmml"><mo id="S2.SS2.p1.10.m3.1.1.3a" xref="S2.SS2.p1.10.m3.1.1.3.cmml">−</mo><mn id="S2.SS2.p1.10.m3.1.1.3.2" xref="S2.SS2.p1.10.m3.1.1.3.2.cmml">1</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.10.m3.1b"><apply id="S2.SS2.p1.10.m3.1.1.cmml" xref="S2.SS2.p1.10.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.10.m3.1.1.1.cmml" xref="S2.SS2.p1.10.m3.1.1">superscript</csymbol><ci id="S2.SS2.p1.10.m3.1.1.2.cmml" xref="S2.SS2.p1.10.m3.1.1.2">ℱ</ci><apply id="S2.SS2.p1.10.m3.1.1.3.cmml" xref="S2.SS2.p1.10.m3.1.1.3"><minus id="S2.SS2.p1.10.m3.1.1.3.1.cmml" xref="S2.SS2.p1.10.m3.1.1.3"></minus><cn type="integer" id="S2.SS2.p1.10.m3.1.1.3.2.cmml" xref="S2.SS2.p1.10.m3.1.1.3.2">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.10.m3.1c">\mathcal{F}^{-1}</annotation></semantics></math> denotes the inverse FFT. Finally, The enhanced features are combined via element-wise summation as follows:</p>
<table id="S2.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E5.m1.1" class="ltx_Math" alttext="F_{fus}=(\mathbf{W}\otimes F_{h})\oplus(\mathbf{W}\otimes F_{s})." display="block"><semantics id="S2.E5.m1.1a"><mrow id="S2.E5.m1.1.1.1" xref="S2.E5.m1.1.1.1.1.cmml"><mrow id="S2.E5.m1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.cmml"><msub id="S2.E5.m1.1.1.1.1.4" xref="S2.E5.m1.1.1.1.1.4.cmml"><mi id="S2.E5.m1.1.1.1.1.4.2" xref="S2.E5.m1.1.1.1.1.4.2.cmml">F</mi><mrow id="S2.E5.m1.1.1.1.1.4.3" xref="S2.E5.m1.1.1.1.1.4.3.cmml"><mi id="S2.E5.m1.1.1.1.1.4.3.2" xref="S2.E5.m1.1.1.1.1.4.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.1.1.1.1.4.3.1" xref="S2.E5.m1.1.1.1.1.4.3.1.cmml">​</mo><mi id="S2.E5.m1.1.1.1.1.4.3.3" xref="S2.E5.m1.1.1.1.1.4.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.1.1.1.1.4.3.1a" xref="S2.E5.m1.1.1.1.1.4.3.1.cmml">​</mo><mi id="S2.E5.m1.1.1.1.1.4.3.4" xref="S2.E5.m1.1.1.1.1.4.3.4.cmml">s</mi></mrow></msub><mo id="S2.E5.m1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.3.cmml">=</mo><mrow id="S2.E5.m1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.2.cmml"><mrow id="S2.E5.m1.1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E5.m1.1.1.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E5.m1.1.1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E5.m1.1.1.1.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.1.1.1.1.2.cmml">𝐖</mi><mo lspace="0.222em" rspace="0.222em" id="S2.E5.m1.1.1.1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.1.1.1.cmml">⊗</mo><msub id="S2.E5.m1.1.1.1.1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E5.m1.1.1.1.1.1.1.1.1.3.2" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.2.cmml">F</mi><mi id="S2.E5.m1.1.1.1.1.1.1.1.1.3.3" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.3.cmml">h</mi></msub></mrow><mo stretchy="false" id="S2.E5.m1.1.1.1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S2.E5.m1.1.1.1.1.2.3" xref="S2.E5.m1.1.1.1.1.2.3.cmml">⊕</mo><mrow id="S2.E5.m1.1.1.1.1.2.2.1" xref="S2.E5.m1.1.1.1.1.2.2.1.1.cmml"><mo stretchy="false" id="S2.E5.m1.1.1.1.1.2.2.1.2" xref="S2.E5.m1.1.1.1.1.2.2.1.1.cmml">(</mo><mrow id="S2.E5.m1.1.1.1.1.2.2.1.1" xref="S2.E5.m1.1.1.1.1.2.2.1.1.cmml"><mi id="S2.E5.m1.1.1.1.1.2.2.1.1.2" xref="S2.E5.m1.1.1.1.1.2.2.1.1.2.cmml">𝐖</mi><mo lspace="0.222em" rspace="0.222em" id="S2.E5.m1.1.1.1.1.2.2.1.1.1" xref="S2.E5.m1.1.1.1.1.2.2.1.1.1.cmml">⊗</mo><msub id="S2.E5.m1.1.1.1.1.2.2.1.1.3" xref="S2.E5.m1.1.1.1.1.2.2.1.1.3.cmml"><mi id="S2.E5.m1.1.1.1.1.2.2.1.1.3.2" xref="S2.E5.m1.1.1.1.1.2.2.1.1.3.2.cmml">F</mi><mi id="S2.E5.m1.1.1.1.1.2.2.1.1.3.3" xref="S2.E5.m1.1.1.1.1.2.2.1.1.3.3.cmml">s</mi></msub></mrow><mo stretchy="false" id="S2.E5.m1.1.1.1.1.2.2.1.3" xref="S2.E5.m1.1.1.1.1.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S2.E5.m1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.1b"><apply id="S2.E5.m1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1"><eq id="S2.E5.m1.1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.3"></eq><apply id="S2.E5.m1.1.1.1.1.4.cmml" xref="S2.E5.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.4.1.cmml" xref="S2.E5.m1.1.1.1.1.4">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.4.2.cmml" xref="S2.E5.m1.1.1.1.1.4.2">𝐹</ci><apply id="S2.E5.m1.1.1.1.1.4.3.cmml" xref="S2.E5.m1.1.1.1.1.4.3"><times id="S2.E5.m1.1.1.1.1.4.3.1.cmml" xref="S2.E5.m1.1.1.1.1.4.3.1"></times><ci id="S2.E5.m1.1.1.1.1.4.3.2.cmml" xref="S2.E5.m1.1.1.1.1.4.3.2">𝑓</ci><ci id="S2.E5.m1.1.1.1.1.4.3.3.cmml" xref="S2.E5.m1.1.1.1.1.4.3.3">𝑢</ci><ci id="S2.E5.m1.1.1.1.1.4.3.4.cmml" xref="S2.E5.m1.1.1.1.1.4.3.4">𝑠</ci></apply></apply><apply id="S2.E5.m1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.2"><csymbol cd="latexml" id="S2.E5.m1.1.1.1.1.2.3.cmml" xref="S2.E5.m1.1.1.1.1.2.3">direct-sum</csymbol><apply id="S2.E5.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E5.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.1">tensor-product</csymbol><ci id="S2.E5.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.2">𝐖</ci><apply id="S2.E5.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.2">𝐹</ci><ci id="S2.E5.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.3">ℎ</ci></apply></apply><apply id="S2.E5.m1.1.1.1.1.2.2.1.1.cmml" xref="S2.E5.m1.1.1.1.1.2.2.1"><csymbol cd="latexml" id="S2.E5.m1.1.1.1.1.2.2.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.2.2.1.1.1">tensor-product</csymbol><ci id="S2.E5.m1.1.1.1.1.2.2.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.2.2.1.1.2">𝐖</ci><apply id="S2.E5.m1.1.1.1.1.2.2.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.2.2.1.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.2.2.1.1.3.1.cmml" xref="S2.E5.m1.1.1.1.1.2.2.1.1.3">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.2.2.1.1.3.2.cmml" xref="S2.E5.m1.1.1.1.1.2.2.1.1.3.2">𝐹</ci><ci id="S2.E5.m1.1.1.1.1.2.2.1.1.3.3.cmml" xref="S2.E5.m1.1.1.1.1.2.2.1.1.3.3">𝑠</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.1c">F_{fus}=(\mathbf{W}\otimes F_{h})\oplus(\mathbf{W}\otimes F_{s}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The proposed PFFM use global filters to capture the feature interactions in the frequency domain. Therefore, cross-modal spectrum interactions are explored, which effectively improves the accuracy of multi-source data joint classification.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2408.12760/assets/x4.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="253" height="260" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Classification results of different methods on the Augsburg dataset.</figcaption>
</figure>
<figure id="S2.F5" class="ltx_figure"><img src="/html/2408.12760/assets/x5.png" id="S2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="326" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Classification results of different methods on the Berlin dataset.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Experimental Results and Analysis</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Datasets and Experimental Setting</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.3" class="ltx_p">We evaluate the performance of the proposed HAPNet on two HSI and SAR datasets. Specifically, the first dataset is the Augsburg dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. The HSI is captured by DAS-EOC, German Aerospace Center (DLR), over the city of Augsburg, Germany. The SAR data is collected by the Sentinel-1 sensor. For the HSI, there are 332 <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mo id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><times id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\times</annotation></semantics></math> 485 pixels and 180 spectral bands ranging from 0.4 to 2.5 <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mu</annotation></semantics></math>m. There are 7 distinct land cover classes in the ground truth. The second dataset is the Berlin dataset. The dataset contains 797 <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mo id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><times id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\times</annotation></semantics></math> 220 pixels, and the HSI contains 244 spectral bands. The are 8 distinct land cover classes in the ground truth. Due to the number of spectral bands in the Augsburg dataset and Berlin dataset being 180 and 244 respectively, to retain key feature information and reduce the input data volume of the neural network to improve its computational efficiency, we choose the PCA method to reduce the dimensionality of the spectral features in hyperspectral images to 30 channels. We use the standard training and test sets in both datasets. Specifically, 761 training samples are used on the Augsburg dataset, and 2820 training samples are used on the Berlin dataset.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">To demonstrate the effectiveness of the proposed HAPNet, six state-of-the-art methods are selected for comparison: TBCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, S<sup id="S3.SS1.p2.1.1" class="ltx_sup">2</sup>ENet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, ExViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, FusAtNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and DFINet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. These methods are evaluated by visual comparison and quantitative metrics of individual class accuracy, overall accuracy (OA), average accuracy (AA), and Kappa coefficient. OA denotes the ratio of the total number of correctly classified pixels to the total number of pixels in the dataset, and it provides an overall assessment of the classification accuracy. AA provides a balanced measure by taking into account the accuracy for each class. Kappa provides a more robust measure of accuracy by taking into account the possibility of agreement by random chance.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The proposed HAPNet was conducted on NVIDIA RTX 3090 GPU. The training phase spanned over 100 epochs. The Adam optimizer is used with the learning rate of 0.0003. The batch size is set as 128. The input patch size for the proposed HAPNet is <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="11\times 11" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mrow id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mn id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">11</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.1.m1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">11</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><times id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">11</cn><cn type="integer" id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">11\times 11</annotation></semantics></math> pixels.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Experimental Results and Discussion</span>
</h3>

<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Experimental results on the Augsburg dataset.</figcaption>
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:219.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(20.1pt,-10.7pt) scale(1.108036159166,1.108036159166) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t">Class</td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">TBCNN</td>
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">S<sup id="S3.T1.1.1.1.1.1" class="ltx_sup">2</sup>ENet</td>
<td id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">FusAtNet</td>
<td id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">DFINet</td>
<td id="S3.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">ExViT</td>
<td id="S3.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">HAPNet</td>
</tr>
<tr id="S3.T1.1.1.2" class="ltx_tr">
<td id="S3.T1.1.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Forest</td>
<td id="S3.T1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">90.88</td>
<td id="S3.T1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.2.3.1" class="ltx_text ltx_font_bold">98.10</span></td>
<td id="S3.T1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">93.78</td>
<td id="S3.T1.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">97.38</td>
<td id="S3.T1.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t">90.04</td>
<td id="S3.T1.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t">96.57</td>
</tr>
<tr id="S3.T1.1.1.3" class="ltx_tr">
<td id="S3.T1.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r">Residential area</td>
<td id="S3.T1.1.1.3.2" class="ltx_td ltx_align_center">93.89</td>
<td id="S3.T1.1.1.3.3" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.3.3.1" class="ltx_text ltx_font_bold">99.08</span></td>
<td id="S3.T1.1.1.3.4" class="ltx_td ltx_align_center">97.58</td>
<td id="S3.T1.1.1.3.5" class="ltx_td ltx_align_center">98.37</td>
<td id="S3.T1.1.1.3.6" class="ltx_td ltx_align_center">95.44</td>
<td id="S3.T1.1.1.3.7" class="ltx_td ltx_align_center">95.78</td>
</tr>
<tr id="S3.T1.1.1.4" class="ltx_tr">
<td id="S3.T1.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r">Industrail area</td>
<td id="S3.T1.1.1.4.2" class="ltx_td ltx_align_center">8.28</td>
<td id="S3.T1.1.1.4.3" class="ltx_td ltx_align_center">12.19</td>
<td id="S3.T1.1.1.4.4" class="ltx_td ltx_align_center">26.48</td>
<td id="S3.T1.1.1.4.5" class="ltx_td ltx_align_center">61.31</td>
<td id="S3.T1.1.1.4.6" class="ltx_td ltx_align_center">34.58</td>
<td id="S3.T1.1.1.4.7" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.4.7.1" class="ltx_text ltx_font_bold">68.02</span></td>
</tr>
<tr id="S3.T1.1.1.5" class="ltx_tr">
<td id="S3.T1.1.1.5.1" class="ltx_td ltx_align_center ltx_border_r">Low plants</td>
<td id="S3.T1.1.1.5.2" class="ltx_td ltx_align_center">91.97</td>
<td id="S3.T1.1.1.5.3" class="ltx_td ltx_align_center">91.78</td>
<td id="S3.T1.1.1.5.4" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.5.4.1" class="ltx_text ltx_font_bold">97.67</span></td>
<td id="S3.T1.1.1.5.5" class="ltx_td ltx_align_center">92.63</td>
<td id="S3.T1.1.1.5.6" class="ltx_td ltx_align_center">90.68</td>
<td id="S3.T1.1.1.5.7" class="ltx_td ltx_align_center">94.83</td>
</tr>
<tr id="S3.T1.1.1.6" class="ltx_tr">
<td id="S3.T1.1.1.6.1" class="ltx_td ltx_align_center ltx_border_r">Allotment</td>
<td id="S3.T1.1.1.6.2" class="ltx_td ltx_align_center">38.24</td>
<td id="S3.T1.1.1.6.3" class="ltx_td ltx_align_center">45.12</td>
<td id="S3.T1.1.1.6.4" class="ltx_td ltx_align_center">52.77</td>
<td id="S3.T1.1.1.6.5" class="ltx_td ltx_align_center">49.33</td>
<td id="S3.T1.1.1.6.6" class="ltx_td ltx_align_center">51.82</td>
<td id="S3.T1.1.1.6.7" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.6.7.1" class="ltx_text ltx_font_bold">64.24</span></td>
</tr>
<tr id="S3.T1.1.1.7" class="ltx_tr">
<td id="S3.T1.1.1.7.1" class="ltx_td ltx_align_center ltx_border_r">Commercial area</td>
<td id="S3.T1.1.1.7.2" class="ltx_td ltx_align_center">1.40</td>
<td id="S3.T1.1.1.7.3" class="ltx_td ltx_align_center">1.22</td>
<td id="S3.T1.1.1.7.4" class="ltx_td ltx_align_center">24.66</td>
<td id="S3.T1.1.1.7.5" class="ltx_td ltx_align_center">3.54</td>
<td id="S3.T1.1.1.7.6" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.7.6.1" class="ltx_text ltx_font_bold">28.63</span></td>
<td id="S3.T1.1.1.7.7" class="ltx_td ltx_align_center">18.08</td>
</tr>
<tr id="S3.T1.1.1.8" class="ltx_tr">
<td id="S3.T1.1.1.8.1" class="ltx_td ltx_align_center ltx_border_r">Water</td>
<td id="S3.T1.1.1.8.2" class="ltx_td ltx_align_center">10.82</td>
<td id="S3.T1.1.1.8.3" class="ltx_td ltx_align_center">24.09</td>
<td id="S3.T1.1.1.8.4" class="ltx_td ltx_align_center">47.51</td>
<td id="S3.T1.1.1.8.5" class="ltx_td ltx_align_center">26.61</td>
<td id="S3.T1.1.1.8.6" class="ltx_td ltx_align_center">17.65</td>
<td id="S3.T1.1.1.8.7" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.8.7.1" class="ltx_text ltx_font_bold">48.00</span></td>
</tr>
<tr id="S3.T1.1.1.9" class="ltx_tr">
<td id="S3.T1.1.1.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OA</td>
<td id="S3.T1.1.1.9.2" class="ltx_td ltx_align_center ltx_border_t">84.53</td>
<td id="S3.T1.1.1.9.3" class="ltx_td ltx_align_center ltx_border_t">88.22</td>
<td id="S3.T1.1.1.9.4" class="ltx_td ltx_align_center ltx_border_t">90.62</td>
<td id="S3.T1.1.1.9.5" class="ltx_td ltx_align_center ltx_border_t">90.66</td>
<td id="S3.T1.1.1.9.6" class="ltx_td ltx_align_center ltx_border_t">86.65</td>
<td id="S3.T1.1.1.9.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.9.7.1" class="ltx_text ltx_font_bold">91.44</span></td>
</tr>
<tr id="S3.T1.1.1.10" class="ltx_tr">
<td id="S3.T1.1.1.10.1" class="ltx_td ltx_align_center ltx_border_r">AA</td>
<td id="S3.T1.1.1.10.2" class="ltx_td ltx_align_center">47.92</td>
<td id="S3.T1.1.1.10.3" class="ltx_td ltx_align_center">53.08</td>
<td id="S3.T1.1.1.10.4" class="ltx_td ltx_align_center">62.92</td>
<td id="S3.T1.1.1.10.5" class="ltx_td ltx_align_center">61.30</td>
<td id="S3.T1.1.1.10.6" class="ltx_td ltx_align_center">58.40</td>
<td id="S3.T1.1.1.10.7" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.10.7.1" class="ltx_text ltx_font_bold">69.36</span></td>
</tr>
<tr id="S3.T1.1.1.11" class="ltx_tr">
<td id="S3.T1.1.1.11.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r">Kappa</td>
<td id="S3.T1.1.1.11.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">77.13</td>
<td id="S3.T1.1.1.11.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">82.61</td>
<td id="S3.T1.1.1.11.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">86.33</td>
<td id="S3.T1.1.1.11.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">86.47</td>
<td id="S3.T1.1.1.11.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">80.79</td>
<td id="S3.T1.1.1.11.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b"><span id="S3.T1.1.1.11.7.1" class="ltx_text ltx_font_bold">87.75</span></td>
</tr>
</table>
</span></div>
</figure>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Experimental results on the Berlin dataset.</figcaption>
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:239.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(20.1pt,-11.7pt) scale(1.108036159166,1.108036159166) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t">Class</td>
<td id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">TBCNN</td>
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">S<sup id="S3.T2.1.1.1.1.1" class="ltx_sup">2</sup>ENet</td>
<td id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">FusAtNet</td>
<td id="S3.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">DFINet</td>
<td id="S3.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">ExViT</td>
<td id="S3.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">HAPNet</td>
</tr>
<tr id="S3.T2.1.1.2" class="ltx_tr">
<td id="S3.T2.1.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Forest</td>
<td id="S3.T2.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">81.75</td>
<td id="S3.T2.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">81.09</td>
<td id="S3.T2.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.1.2.4.1" class="ltx_text ltx_font_bold">86.24</span></td>
<td id="S3.T2.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">80.90</td>
<td id="S3.T2.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t">78.01</td>
<td id="S3.T2.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t">82.84</td>
</tr>
<tr id="S3.T2.1.1.3" class="ltx_tr">
<td id="S3.T2.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r">Residential area</td>
<td id="S3.T2.1.1.3.2" class="ltx_td ltx_align_center">76.26</td>
<td id="S3.T2.1.1.3.3" class="ltx_td ltx_align_center">73.05</td>
<td id="S3.T2.1.1.3.4" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.3.4.1" class="ltx_text ltx_font_bold">91.38</span></td>
<td id="S3.T2.1.1.3.5" class="ltx_td ltx_align_center">72.81</td>
<td id="S3.T2.1.1.3.6" class="ltx_td ltx_align_center">74.05</td>
<td id="S3.T2.1.1.3.7" class="ltx_td ltx_align_center">89.47</td>
</tr>
<tr id="S3.T2.1.1.4" class="ltx_tr">
<td id="S3.T2.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r">Industrial area</td>
<td id="S3.T2.1.1.4.2" class="ltx_td ltx_align_center">39.67</td>
<td id="S3.T2.1.1.4.3" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.4.3.1" class="ltx_text ltx_font_bold">62.61</span></td>
<td id="S3.T2.1.1.4.4" class="ltx_td ltx_align_center">19.76</td>
<td id="S3.T2.1.1.4.5" class="ltx_td ltx_align_center">38.89</td>
<td id="S3.T2.1.1.4.6" class="ltx_td ltx_align_center">39.48</td>
<td id="S3.T2.1.1.4.7" class="ltx_td ltx_align_center">47.43</td>
</tr>
<tr id="S3.T2.1.1.5" class="ltx_tr">
<td id="S3.T2.1.1.5.1" class="ltx_td ltx_align_center ltx_border_r">Low plant</td>
<td id="S3.T2.1.1.5.2" class="ltx_td ltx_align_center">49.78</td>
<td id="S3.T2.1.1.5.3" class="ltx_td ltx_align_center">82.82</td>
<td id="S3.T2.1.1.5.4" class="ltx_td ltx_align_center">20.00</td>
<td id="S3.T2.1.1.5.5" class="ltx_td ltx_align_center">78.09</td>
<td id="S3.T2.1.1.5.6" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.5.6.1" class="ltx_text ltx_font_bold">84.15</span></td>
<td id="S3.T2.1.1.5.7" class="ltx_td ltx_align_center">81.71</td>
</tr>
<tr id="S3.T2.1.1.6" class="ltx_tr">
<td id="S3.T2.1.1.6.1" class="ltx_td ltx_align_center ltx_border_r">Soil</td>
<td id="S3.T2.1.1.6.2" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.6.2.1" class="ltx_text ltx_font_bold">89.42</span></td>
<td id="S3.T2.1.1.6.3" class="ltx_td ltx_align_center">86.41</td>
<td id="S3.T2.1.1.6.4" class="ltx_td ltx_align_center">48.72</td>
<td id="S3.T2.1.1.6.5" class="ltx_td ltx_align_center">73.48</td>
<td id="S3.T2.1.1.6.6" class="ltx_td ltx_align_center">88.03</td>
<td id="S3.T2.1.1.6.7" class="ltx_td ltx_align_center">71.12</td>
</tr>
<tr id="S3.T2.1.1.7" class="ltx_tr">
<td id="S3.T2.1.1.7.1" class="ltx_td ltx_align_center ltx_border_r">Allotment</td>
<td id="S3.T2.1.1.7.2" class="ltx_td ltx_align_center">54.36</td>
<td id="S3.T2.1.1.7.3" class="ltx_td ltx_align_center">54.61</td>
<td id="S3.T2.1.1.7.4" class="ltx_td ltx_align_center">38.89</td>
<td id="S3.T2.1.1.7.5" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.7.5.1" class="ltx_text ltx_font_bold">72.54</span></td>
<td id="S3.T2.1.1.7.6" class="ltx_td ltx_align_center">70.00</td>
<td id="S3.T2.1.1.7.7" class="ltx_td ltx_align_center">67.02</td>
</tr>
<tr id="S3.T2.1.1.8" class="ltx_tr">
<td id="S3.T2.1.1.8.1" class="ltx_td ltx_align_center ltx_border_r">Commercial area</td>
<td id="S3.T2.1.1.8.2" class="ltx_td ltx_align_center">4.65</td>
<td id="S3.T2.1.1.8.3" class="ltx_td ltx_align_center">2.56</td>
<td id="S3.T2.1.1.8.4" class="ltx_td ltx_align_center">18.47</td>
<td id="S3.T2.1.1.8.5" class="ltx_td ltx_align_center">22.80</td>
<td id="S3.T2.1.1.8.6" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.8.6.1" class="ltx_text ltx_font_bold">38.18</span></td>
<td id="S3.T2.1.1.8.7" class="ltx_td ltx_align_center">24.74</td>
</tr>
<tr id="S3.T2.1.1.9" class="ltx_tr">
<td id="S3.T2.1.1.9.1" class="ltx_td ltx_align_center ltx_border_r">Water</td>
<td id="S3.T2.1.1.9.2" class="ltx_td ltx_align_center">41.93</td>
<td id="S3.T2.1.1.9.3" class="ltx_td ltx_align_center">75.96</td>
<td id="S3.T2.1.1.9.4" class="ltx_td ltx_align_center">29.61</td>
<td id="S3.T2.1.1.9.5" class="ltx_td ltx_align_center">68.15</td>
<td id="S3.T2.1.1.9.6" class="ltx_td ltx_align_center">56.41</td>
<td id="S3.T2.1.1.9.7" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.9.7.1" class="ltx_text ltx_font_bold">77.19</span></td>
</tr>
<tr id="S3.T2.1.1.10" class="ltx_tr">
<td id="S3.T2.1.1.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">OA</td>
<td id="S3.T2.1.1.10.2" class="ltx_td ltx_align_center ltx_border_t">67.60</td>
<td id="S3.T2.1.1.10.3" class="ltx_td ltx_align_center ltx_border_t">71.08</td>
<td id="S3.T2.1.1.10.4" class="ltx_td ltx_align_center ltx_border_t">70.91</td>
<td id="S3.T2.1.1.10.5" class="ltx_td ltx_align_center ltx_border_t">70.33</td>
<td id="S3.T2.1.1.10.6" class="ltx_td ltx_align_center ltx_border_t">72.63</td>
<td id="S3.T2.1.1.10.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.1.10.7.1" class="ltx_text ltx_font_bold">80.51</span></td>
</tr>
<tr id="S3.T2.1.1.11" class="ltx_tr">
<td id="S3.T2.1.1.11.1" class="ltx_td ltx_align_center ltx_border_r">AA</td>
<td id="S3.T2.1.1.11.2" class="ltx_td ltx_align_center">54.72</td>
<td id="S3.T2.1.1.11.3" class="ltx_td ltx_align_center">64.88</td>
<td id="S3.T2.1.1.11.4" class="ltx_td ltx_align_center">44.13</td>
<td id="S3.T2.1.1.11.5" class="ltx_td ltx_align_center">63.45</td>
<td id="S3.T2.1.1.11.6" class="ltx_td ltx_align_center">66.04</td>
<td id="S3.T2.1.1.11.7" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.11.7.1" class="ltx_text ltx_font_bold">66.44</span></td>
</tr>
<tr id="S3.T2.1.1.12" class="ltx_tr">
<td id="S3.T2.1.1.12.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r">Kappa</td>
<td id="S3.T2.1.1.12.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">50.96</td>
<td id="S3.T2.1.1.12.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">58.02</td>
<td id="S3.T2.1.1.12.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">51.07</td>
<td id="S3.T2.1.1.12.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">56.90</td>
<td id="S3.T2.1.1.12.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">60.48</td>
<td id="S3.T2.1.1.12.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b"><span id="S3.T2.1.1.12.7.1" class="ltx_text ltx_font_bold">68.77</span></td>
</tr>
</table>
</span></div>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">Results on the Augsburg dataset.</span> The classification maps of different methods on the Augsburg dataset is shown in Fig. <a href="#S2.F4" title="Figure 4 ‣ II-B Parallel Filter Fusion Module (PFFM) ‣ II Methodology ‣ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, and the corresponding quantitative evaluations are illustrated in Table <a href="#S3.T1" title="TABLE I ‣ III-B Experimental Results and Discussion ‣ III Experimental Results and Analysis ‣ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. It should be noted that all the methods for comparison are designed for multi-source image joint classification. As can be seen in Table <a href="#S3.T1" title="TABLE I ‣ III-B Experimental Results and Discussion ‣ III Experimental Results and Analysis ‣ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, the OA value for the proposed HAPNet achieves 91.44%. It surpasses existing methods by at least 1.04%. This indicates that HAPNet effectively integrates global, spectral, and local features, providing a more comprehensive feature representation for multi-source data classification. Specifically, HAPNet performs better than the S<sup id="S3.SS2.p1.1.2" class="ltx_sup">2</sup>ENet, which is a self-attention-based method. It is evident that hierarchical attention provides better feature modeling capabilities than self-attention mechanism. Additionally, HAPNet performs better than the DFINet in which cross-attention fusion is employed. It demonstrated that cross-modal feature fusion in the frequency domain is more efficient than cross-attention.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.3" class="ltx_p"><span id="S3.SS2.p2.3.1" class="ltx_text ltx_font_italic">Results on the Berlin dataset.</span> The classification results on the Berlin dataset is shown in Fig. <a href="#S2.F5" title="Figure 5 ‣ II-B Parallel Filter Fusion Module (PFFM) ‣ II Methodology ‣ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, and the corresponding quantitative evaluations are illustrated in Table <a href="#S3.T2" title="TABLE II ‣ III-B Experimental Results and Discussion ‣ III Experimental Results and Analysis ‣ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. The proposed HAPNet achieves 80.51<math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mo id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><csymbol cd="latexml" id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\%</annotation></semantics></math>, 66.44<math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mo id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><csymbol cd="latexml" id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\%</annotation></semantics></math> and 68.77<math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mo id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><csymbol cd="latexml" id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\%</annotation></semantics></math> for OA, AA and Kappa, respectively. We found that Berlin dataset is more challenging compared to the Ausburg dataset. Nevertheless, the proposed HAPNet achieves the best performance on ‘Water’ class. The reason might be that PFFM complements the water-sensitive features in SAR data with the HSI features. At the same time, the classification results of the other classes also achieve satisfying results. As a result, the proposed HAPNet performs better than the other methods on the Berlin dataset.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_italic">Computational Cost Analysis.</span> The computational costs of different methods are shown in Table <a href="#S3.T3" title="TABLE III ‣ III-B Experimental Results and Discussion ‣ III Experimental Results and Analysis ‣ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. As can be observed that, only ExViT is more computational efficient than the proposed HAPNet. However, the classification results of ExViT is not satisfying on both datasets. Compared with the other methods, the computational cost of the proposed HAPNet is quite competitive.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Computational costs of different methods.</figcaption>
<div id="S3.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:44.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(40.9pt,-4.5pt) scale(1.24797887425676,1.24797887425676) ;">
<table id="S3.T3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<td id="S3.T3.1.1.1.2" class="ltx_td ltx_border_r ltx_border_tt ltx_border_t"></td>
<td id="S3.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">TBCNN</td>
<td id="S3.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">S<sup id="S3.T3.1.1.1.1.1" class="ltx_sup">2</sup>ENet</td>
<td id="S3.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">FusAtNet</td>
<td id="S3.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">DFINet</td>
<td id="S3.T3.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">ExViT</td>
<td id="S3.T3.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">HAPNet</td>
</tr>
<tr id="S3.T3.1.1.2" class="ltx_tr">
<td id="S3.T3.1.1.2.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r ltx_border_t">FLOPs</td>
<td id="S3.T3.1.1.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t">135.2M</td>
<td id="S3.T3.1.1.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t">107.9M</td>
<td id="S3.T3.1.1.2.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t">670.6M</td>
<td id="S3.T3.1.1.2.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t">297.5M</td>
<td id="S3.T3.1.1.2.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t">53.6M</td>
<td id="S3.T3.1.1.2.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t">103.7M</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Ablation Study</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We conduct a series of ablation experiments on two datasets to validate the effectiveness of the HAM and PFFM. We design two variants of HAPNet, i.e. without the HAM (w/o HAM) and without the PFFM (w/o PFFM). The experimental results are shown in Table <a href="#S3.T4" title="TABLE IV ‣ III-C Ablation Study ‣ III Experimental Results and Analysis ‣ Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. We find that the HAPNet always achieves better performance than its two variants on the two datasets. This demonstrates the necessity of the HAM and PFFM designed in HAPNet.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Ablation study of the proposed HAPNet.</figcaption>
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T4.1.1" class="ltx_tr">
<td id="S3.T4.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T4.1.1.2.1" class="ltx_text">Method</span></td>
<td id="S3.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">OA on different datasets (<math id="S3.T4.1.1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S3.T4.1.1.1.m1.1a"><mo id="S3.T4.1.1.1.m1.1.1" xref="S3.T4.1.1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.m1.1b"><csymbol cd="latexml" id="S3.T4.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.m1.1c">\%</annotation></semantics></math>)</td>
</tr>
<tr id="S3.T4.1.2" class="ltx_tr">
<td id="S3.T4.1.2.1" class="ltx_td ltx_align_center ltx_border_t">Ausburg</td>
<td id="S3.T4.1.2.2" class="ltx_td ltx_align_center ltx_border_t">Berlin</td>
</tr>
<tr id="S3.T4.1.3" class="ltx_tr">
<td id="S3.T4.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">HAPNet w/o HAM</td>
<td id="S3.T4.1.3.2" class="ltx_td ltx_align_center ltx_border_t">90.35</td>
<td id="S3.T4.1.3.3" class="ltx_td ltx_align_center ltx_border_t">74.49</td>
</tr>
<tr id="S3.T4.1.4" class="ltx_tr">
<td id="S3.T4.1.4.1" class="ltx_td ltx_align_center ltx_border_r">HAPNet w/o PFFM</td>
<td id="S3.T4.1.4.2" class="ltx_td ltx_align_center">89.80</td>
<td id="S3.T4.1.4.3" class="ltx_td ltx_align_center">76.75</td>
</tr>
<tr id="S3.T4.1.5" class="ltx_tr">
<td id="S3.T4.1.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Proposed HAPNet</td>
<td id="S3.T4.1.5.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.1.5.2.1" class="ltx_text ltx_font_bold">91.44</span></td>
<td id="S3.T4.1.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.1.5.3.1" class="ltx_text ltx_font_bold">80.51</span></td>
</tr>
</table>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Conclusions</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this letter, we developed an HSI and SAR data joint classification method which can model multi-granularity features simultaneously. To achieve this, we design HAM which integrates global, spectral, and local features to provide more comprehensive feature representation. In addition, we develop PFFM to enhance cross-modal feature interactions in the frequency domain. The module enhances feature interactions among different spatial locations in the frequency domain, and thus effectively improves the classification performance. Our HAPNet achieves very competitive performance with state-of-the-art methods on two HSI and SAR joint classification datasets.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
F. Luo, T. Zhou, J. Liu, T. Guo, X. Gong, and X. Gao, “DCENet: Diff-feature contrast enhancement network for semi-supervised hyperspectral change detection,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Geoscience and Remote Sensing</em>, vol. 62, pp. 1–14, 2024.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
P. Ghamisi, B. Höfle, and X. X. Zhu, “Hyperspectral and LiDAR data fusion using extinction profiles and deep convolutional neural network,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em>, vol. 10, no. 6, pp. 3011–3024, 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Xia, W. Liao, and P. Du, “Hyperspectral and LiDAR classification with semisupervised graph fusion,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Geoscience and Remote Sensing Letters</em>, vol. 17, no. 4, pp. 666–670, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J. Wang, W. Li, Y. Gao, M. Zhang, R. Tao, and Q. Du, “Hyperspectral and SAR image classification via multiscale interactive fusion network,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, vol. 34, no. 12, pp. 10 823–10 837, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
W. Li, Y. Gao, M. Zhang, R. Tao, and Q. Du, “Asymmetric feature fusion network for hyperspectral and SAR image classification,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, vol. 34, no. 10, pp. 8057–8070, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J.-Y. Yang, H.-C. Li, J.-H. Yang, L. Pan, Q. Du, and A. Plaza, “Multifrequency graph convolutional network with cross-modality mutual enhancement for multisource remote sensing data classification,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Geoscience and Remote Sensing</em>, vol. 62, pp. 1–14, 2024.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
X. Song, L. Li, L. Jiao, F. Liu, X. Liu, and S. Yang, “A spatial–spectral bilinear representation fusion network for multimodal classification,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Geoscience and Remote Sensing</em>, vol. 61, pp. 1–17, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y. Gao, M. Zhang, W. Li, X. Song, X. Jiang, and Y. Ma, “Adversarial complementary learning for multisource remote sensing classification,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Geoscience and Remote Sensing</em>, vol. 61, pp. 1–13, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Y. Li, Y. Fan, X. Xiang, D. Demandolx, R. Ranjan, R. Timofte, and L. Van Gool, “Efficient and explicit modelling of image hierarchies for image restoration,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023, pp. 18 278–18 289.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y. Rao, W. Zhao, Z. Zhu, J. Zhou, and J. Lu, “GFNet: Global filter networks for visual recognition,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 45, no. 9, pp. 10 960–10 973, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
D. Hong, J. Hu, J. Yao, J. Chanussot, and X. X. Zhu, “Multimodal remote sensing benchmark datasets for land cover classification with a shared and specific feature learning model,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ISPRS Journal of Photogrammetry and Remote Sensing</em>, vol. 178, pp. 68–80, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
X. Xu, W. Li, Q. Ran, Q. Du, L. Gao, and B. Zhang, “Multisource remote sensing data classification based on convolutional neural network,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Geoscience and Remote Sensing</em>, vol. 56, no. 2, pp. 937–949, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S. Fang, K. Li, and Z. Li, “S2ENet: Spatial–spectral cross-modal enhancement network for classification of hyperspectral and LiDAR data,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Geoscience and Remote Sensing Letters</em>, vol. 19, pp. 1–5, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J. Yao, B. Zhang, C. Li, D. Hong, and J. Chanussot, “Extended vision transformer (ExViT) for land use and land cover classification: A multimodal deep learning framework,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Geoscience and Remote Sensing</em>, vol. 61, pp. 1–15, 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
S. Mohla, S. Pande, B. Banerjee, and S. Chaudhuri, “FusAtNet: Dual attention based spectrospatial multimodal fusion network for hyperspectral and LiDAR classification,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, 2020, pp. 416–425.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Y. Gao, W. Li, M. Zhang, J. Wang, W. Sun, R. Tao, and Q. Du, “Hyperspectral and multispectral classification for coastal wetland using depthwise feature interaction network,” <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Geoscience and Remote Sensing</em>, vol. 60, pp. 1–15, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.12759" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.12760" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.12760">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.12760" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.12761" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 16:20:05 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
