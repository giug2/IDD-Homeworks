<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Graph Machine Learning in the Era of Large Language Models (LLMs)</title>
<!--Generated on Tue Jun  4 01:26:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Graph Machine Learning,  Graph Foundation Models,  Graph Learning,  Large Language Models (LLMs),  Pre-training and Fine-tuning,  Prompting,  Representation Learning.
" lang="en" name="keywords"/>
<base href="/html/2404.14928v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S1" title="In Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S2" title="In Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span><span class="ltx_text ltx_font_smallcaps">RELATED WORK</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S2.SS1" title="In 2 RELATED WORK ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span><span class="ltx_text ltx_font_italic">Graph Machine Learning</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S2.SS2" title="In 2 RELATED WORK ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span><span class="ltx_text ltx_font_italic">Foundation Models (FMs)</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S3" title="In Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span><span class="ltx_text ltx_font_smallcaps">Deep Learning on Graphs</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S3.SS1" title="In 3 Deep Learning on Graphs ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span><span class="ltx_text ltx_font_italic">Backbone Architecture</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S3.SS1.SSS1" title="In 3.1 Backbone Architecture ‣ 3 Deep Learning on Graphs ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Neighborhood Aggregation-based Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S3.SS1.SSS2" title="In 3.1 Backbone Architecture ‣ 3 Deep Learning on Graphs ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Graph Transformer-based Model</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S3.SS2" title="In 3 Deep Learning on Graphs ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span><span class="ltx_text ltx_font_italic">Self-Supervised Learning on Graphs</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S3.SS2.SSS1" title="In 3.2 Self-Supervised Learning on Graphs ‣ 3 Deep Learning on Graphs ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Graph Pretext Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S3.SS2.SSS2" title="In 3.2 Self-Supervised Learning on Graphs ‣ 3 Deep Learning on Graphs ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Downstream Adaptation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S4" title="In Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span><span class="ltx_text ltx_font_smallcaps">LLMs for Graph Models</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S4.SS1" title="In 4 LLMs for Graph Models ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span><span class="ltx_text ltx_font_italic">Enhancing Feature Quality</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S4.SS1.SSS1" title="In 4.1 Enhancing Feature Quality ‣ 4 LLMs for Graph Models ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Enhancing Feature Representation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S4.SS1.SSS2" title="In 4.1 Enhancing Feature Quality ‣ 4 LLMs for Graph Models ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Generating Augmented Information</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S4.SS1.SSS3" title="In 4.1 Enhancing Feature Quality ‣ 4 LLMs for Graph Models ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Aligning Feature Space</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S4.SS2" title="In 4 LLMs for Graph Models ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span><span class="ltx_text ltx_font_italic">Solving Vanilla GNN Training Limitations</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S4.SS2.SSS1" title="In 4.2 Solving Vanilla GNN Training Limitations ‣ 4 LLMs for Graph Models ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Ignoring Structural Information</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S4.SS2.SSS2" title="In 4.2 Solving Vanilla GNN Training Limitations ‣ 4 LLMs for Graph Models ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Implicit Structural Information</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S4.SS2.SSS3" title="In 4.2 Solving Vanilla GNN Training Limitations ‣ 4 LLMs for Graph Models ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Explicit Structural Information</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S4.SS3" title="In 4 LLMs for Graph Models ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span><span class="ltx_text ltx_font_italic">Heterophily and Generalization</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S5" title="In Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span><span class="ltx_text ltx_font_smallcaps">Graphs for LLMs</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S5.SS1" title="In 5 Graphs for LLMs ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span><span class="ltx_text ltx_font_italic">KG-enhanced LLM Pre-training</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S5.SS1.SSS1" title="In 5.1 KG-enhanced LLM Pre-training ‣ 5 Graphs for LLMs ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Modifying Input Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S5.SS1.SSS2" title="In 5.1 KG-enhanced LLM Pre-training ‣ 5 Graphs for LLMs ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Modifying Model Structures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S5.SS1.SSS3" title="In 5.1 KG-enhanced LLM Pre-training ‣ 5 Graphs for LLMs ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.3 </span>Modifying Pre-training Tasks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S5.SS2" title="In 5 Graphs for LLMs ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span><span class="ltx_text ltx_font_italic">KG-enhanced LLM Inference</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S6" title="In Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span><span class="ltx_text ltx_font_smallcaps">Applications</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S6.SS1" title="In 6 Applications ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span><span class="ltx_text ltx_font_italic">Recommender Systems</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S6.SS2" title="In 6 Applications ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span><span class="ltx_text ltx_font_italic">Knowledge Graphs</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S6.SS3" title="In 6 Applications ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span><span class="ltx_text ltx_font_italic">AI for Science</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S6.SS4" title="In 6 Applications ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span><span class="ltx_text ltx_font_italic">Robot Task Planning</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S7" title="In Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span><span class="ltx_text ltx_font_smallcaps">Future Directions</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S7.SS1" title="In 7 Future Directions ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span><span class="ltx_text ltx_font_italic">Generalization and Transferability</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S7.SS2" title="In 7 Future Directions ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span><span class="ltx_text ltx_font_italic">Multi-modal Graph Learning</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S7.SS3" title="In 7 Future Directions ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span><span class="ltx_text ltx_font_italic">Trustworthiness</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S7.SS3.SSS1" title="In 7.3 Trustworthiness ‣ 7 Future Directions ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3.1 </span>Robustness&amp;Safety</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S7.SS3.SSS2" title="In 7.3 Trustworthiness ‣ 7 Future Directions ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3.2 </span>Explainability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S7.SS3.SSS3" title="In 7.3 Trustworthiness ‣ 7 Future Directions ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3.3 </span>Fairness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S7.SS3.SSS4" title="In 7.3 Trustworthiness ‣ 7 Future Directions ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3.4 </span>Privacy</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S7.SS4" title="In 7 Future Directions ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4 </span><span class="ltx_text ltx_font_italic">Efficiency</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S8" title="In Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Graph Machine Learning in the Era of 
<br class="ltx_break"/>Large Language Models (LLMs)</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wenqi Fan, Shijie Wang, Jiani Huang, Zhikai Chen, Yu Song, Wenzhuo Tang,
<br class="ltx_break"/>Haitao Mao, Hui Liu, Xiaorui Liu, Dawei Yin, Qing Li
</span><span class="ltx_author_notes">
W. Fan is with the Department of Computing (COMP) and Department of Management and Marketing (MM), The Hong Kong Polytechnic University. E-mail: wenqifan03@gmail.com.
S. Wang, and Q. Li are with the Department of Computing, The Hong
Kong Polytechnic University. E-mail: shijie.wang@connect.polyu.hk;
csqli@comp.polyu.edu.hk.J. Huang is with Wuhan University. E-mail: huangjiani@whu.edu.cn.
Z. Chen, Y. Song, W. Tang, H. Mao, and H. Liu are with Michigan State University. E-mail: {chenzh85, songyu5, tangwen2, haitaoma, liuhui7}@msu.edu.
X. Liu is with North Carolina State University. E-mail: xliu96@ncsu.edu.
D. Yin is Senior Director of Engineering at Baidu Inc. E-mail: yindawei@acm.org.
(Corresponding authors: Wenqi Fan and Qing Li.)</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Graphs play an important role in representing complex relationships in various domains like social networks, knowledge graphs, and molecular discovery. With the advent of deep learning, Graph Neural Networks (GNNs) have emerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the representation and processing of graph structures. Recently, LLMs have demonstrated unprecedented capabilities in language tasks and are widely adopted in a variety of applications such as computer vision and recommender systems. This remarkable success has also attracted interest in applying LLMs to the graph domain. Increasing efforts have been made to explore the potential of LLMs in advancing Graph ML’s generalization, transferability, and few-shot learning ability. Meanwhile, graphs, especially knowledge graphs, are rich in reliable factual knowledge, which can be utilized to enhance the reasoning capabilities of LLMs and potentially alleviate their limitations such as hallucinations and the lack of explainability. Given the rapid progress of this research direction, a systematic review summarizing the latest advancements for Graph ML in the era of LLMs is necessary to provide an in-depth understanding to researchers and practitioners. Therefore, in this survey, we first review the recent developments in Graph ML. We then explore how LLMs can be utilized to enhance the quality of graph features, alleviate the reliance on labeled data, and address challenges such as graph heterogeneity and out-of-distribution (OOD) generalization. Afterward, we delve into how graphs can enhance LLMs, highlighting their abilities to enhance LLM pre-training and inference. Furthermore, we investigate various applications and discuss the potential future directions in this promising field.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Graph Machine Learning, Graph Foundation Models, Graph Learning, Large Language Models (LLMs), Pre-training and Fine-tuning, Prompting, Representation Learning.

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<figure class="ltx_figure ltx_align_center" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="478" id="S1.F1.1.g1" src="x1.png" width="822"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of the application of Large Language Models (LLMs) in graph machine learning. The integration of LLMs with Graph Neural Networks (GNNs) is utilized to model an extensive range of graph data across various downstream tasks.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Graph data are widespread in many real-world applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib2" title="">2</a>]</cite>, including social graphs, knowledge graphs, and recommender systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib5" title="">5</a>]</cite>.
Typically, graphs consist of nodes and edges, e.g., in a social graph, nodes represent users and edges represent relationships <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib7" title="">7</a>]</cite>.
In addition to the topological structure, graphs tend to possess various features of nodes, such as textual description, which provide valuable context and semantic information about nodes.
To effectively model the graph, <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">Graph Machine Learning (Graph ML)</em> has garnered significant interest.
With the advent of deep learning (DL), Graph Neural Networks (GNNs) have become a critical technique in Graph ML due to their message-passing mechanism.
This mechanism allows each node to obtain its representation by recursively receiving and aggregating messages from neighboring nodes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib9" title="">9</a>]</cite>, thereby capturing the high-order relationships and dependencies within the graph structure. To mitigate the reliance on supervised data, many research focused on developing self-supervised Graph ML methods to advance GNNs to capture transferable graph patterns, enhancing their generalization capabilities across various tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib13" title="">13</a>]</cite>.
Given the exponential growth of applications of graph data, researchers are actively working to develop more powerful Graph ML methods.</p>
</div>
<figure class="ltx_figure ltx_align_center" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="379" id="S1.F2.1.g1" src="x2.png" width="813"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The outline of our survey. <span class="ltx_text ltx_font_bold" id="S1.F2.7.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S3" title="3 Deep Learning on Graphs ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_tag">3</span></a> Deep Learning on Graphs</span> explores the development of DNN-based methods, focusing on the Backbone Architecture, Graph Pretext Tasks, and Downstream Adaption three aspects. <span class="ltx_text ltx_font_bold" id="S1.F2.8.2">Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S4" title="4 LLMs for Graph Models ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_tag">4</span></a> LLMs for Graph
Models</span> explore how current LLMs help the current Graph ML towards GFMs from Enhancing Feature Quality, Solving Vanilla GNN Training Limitations, and Heterophily and Generalization three aspects.
<span class="ltx_text ltx_font_bold" id="S1.F2.9.3">Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S5" title="5 Graphs for LLMs ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_tag">5</span></a> Graph for LLMs</span> focuses on Knowledge Graph(KG)-enhanced LLM Pre-training and KG-enhanced LLM Inference. <span class="ltx_text ltx_font_bold" id="S1.F2.10.4">Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S6" title="6 Applications ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_tag">6</span></a> Applications</span> presents various applications, including Recommender System, Knowledge Graph, AI for Science, and Robot Task Planning. <span class="ltx_text ltx_font_bold" id="S1.F2.11.5">Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S7" title="7 Future Directions ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_tag">7</span></a> Future Directions</span> discusses potential future directions for LLMs in graph machine learning from the Generalization and Transferability, Multi-modal Graph Learning, Trustworthiness and Efficiency.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Recently, Large Language Models (LLMs) have started a new trend of AI and have shown remarkable capabilities in natural language processing (NLP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib15" title="">15</a>]</cite>.
With the evolution of these models, LLMs are not only being applied to language tasks but also showcasing great potentials in various applications such as CV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib16" title="">16</a>]</cite>, and Recommender System <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib17" title="">17</a>]</cite>.
The effectiveness of LLMs in complex tasks is attributed to their extensive scale in both architecture and dataset size. For example, GPT-3 with 175 billion parameters demonstrates exciting capabilities by generating human-like text, answering complex questions, and coding.
Furthermore, LLMs are able to grasp extensive general knowledge and sophisticated reasoning due to their vast training datasets.
Therefore, their abilities in linguistic semantics and knowledge reasoning enable them to learn semantic information.
Additionally, LLMs exhibit emergence abilities, excelling in new tasks and domains with limited or no specific training. This attribute is expected to provide high generalisability across different downstream datasets and tasks even in few-shot or zero-shot situations.
Therefore, leveraging the capabilities of LLMs in Graph Machine Learning (Graph ML) has gained increasing interest and is expected to enhance Graph ML towards Graph Foundation Models (GFMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib19" title="">19</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">GFMs are generally trained on extensive data and can be adapted for a wide range of downstream tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib20" title="">20</a>]</cite>.
By exploiting the ability of LLMs, it is expected to enhance the ability of Graph ML to generalize a variety of tasks, thus facilitating GFMs.
Currently, researchers have made several initial efforts to explore the potential of LLMs in advancing Graph ML towards GFMs.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_tag">1</span></a> demonstrates an example of integrating LLMs and GNNs for various graph tasks.
Firstly, some methods leverage LLMs to alleviate the reliance of vanilla Graph ML on labeled data, where they make inferences based on implicit and explicit graph structure information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib23" title="">23</a>]</cite>.
For instance, InstructGLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib21" title="">21</a>]</cite> fine-tunes models like LlaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib24" title="">24</a>]</cite> and T5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib25" title="">25</a>]</cite> by serializing graph data as tokens and encoding structural information about the graph to solve graph tasks.
Secondly, to overcome the challenge of feature quality, some methods further employ LLMs to enhance the quality of graph features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib28" title="">28</a>]</cite>.
For example, SimTeG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib26" title="">26</a>]</cite> fine-tunes LLMs on textual graphs datasets to obtain textual attribute embeddings, which are then utilized to augment the GNN for various downstream tasks.
Additionally, some studies explore using LLMs to address challenges such as heterogeneity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib29" title="">29</a>]</cite> and OOD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib27" title="">27</a>]</cite> of graphs.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">On the other hand, although LLM achieves great success in various fields, it still faces several challenges, including hallucinations, actuality awareness, and lacking explainability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib33" title="">33</a>]</cite>. Graphs, especially knowledge graphs, capture extensive high-quality and reliable factual knowledge in a structured format <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib5" title="">5</a>]</cite>.
Therefore, incorporating graph structure into LLMs could improve the reasoning ability of LLMs and mitigate these limitations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib34" title="">34</a>]</cite>.
To this end, efforts have been made to explore the potential of graphs in augmenting LLMs’ explainability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib36" title="">36</a>]</cite> and mitigating hallucination <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib38" title="">38</a>]</cite>.
Given the rapid evolution and significant potential of this field, a thorough review of recent advancements in graph applications and Graph ML in the era of LLMs is imperative.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Therefore, in this survey, we aim to provide a comprehensive review of Graph Machine Learning in the era of LLMs.
The outline of the survey is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_tag">2</span></a>: Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S2" title="2 RELATED WORK ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_tag">2</span></a> reviews work related to graph machine learning and foundation models.
Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S3" title="3 Deep Learning on Graphs ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_tag">3</span></a> introduces the deep learning methods on graphs, which focus on various GNN models and self-supervised methods.
Subsequently, the survey delves into how LLMs can be used to enhance Graph ML in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S4" title="4 LLMs for Graph Models ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_tag">4</span></a> and how graphs can be adopted for augmenting LLMs in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S5" title="5 Graphs for LLMs ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_tag">5</span></a>.
Finally, some applications and potential future directions for Graph ML in the era of LLMs are discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S6" title="6 Applications ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_tag">6</span></a> and Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S7" title="7 Future Directions ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_tag">7</span></a>, respectively. Our main contributions can be summarized as follows:</p>
</div>
<div class="ltx_para" id="S1.p6">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We detail the evolution from early graph learning methods to the latest GFMs in the era of LLMs;</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We provide a comprehensive analysis of current LLMs enhanced Graph ML methods, highlighting their advantages and limitations, and offering a systematic categorization;</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We thoroughly investigate the potential of graph structures to address the limitations of LLMs;</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We explore the applications and prospective future directions of Graph ML in the era of LLMs, and discuss both research and practical applications in various fields.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Concurrent to our survey, Wei <span class="ltx_text ltx_font_italic" id="S1.p7.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib39" title="">39</a>]</cite> review the development of graph learning.
Zhang <span class="ltx_text ltx_font_italic" id="S1.p7.1.2">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib40" title="">40</a>]</cite> provide a prospective review of large graph models.
Jin <span class="ltx_text ltx_font_italic" id="S1.p7.1.3">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib41" title="">41</a>]</cite> and Li <span class="ltx_text ltx_font_italic" id="S1.p7.1.4">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib42" title="">42</a>]</cite> review different techniques for pre-training language models (in particular LLMs) on graphs and applications to different types of graphs, respectively.
Liu <span class="ltx_text ltx_font_italic" id="S1.p7.1.5">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib43" title="">43</a>]</cite> review the Graph Foundation Models according to the pipelines.
Mao <span class="ltx_text ltx_font_italic" id="S1.p7.1.6">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib19" title="">19</a>]</cite> focus on the fundamental principles and discuss the potential of GFMs.
Different from these concurrent surveys, our survey provides a more comprehensive review with the following differences: (1) we present a more systematic review of the development of Graph Machine Learning and further exploration of LLMs for Graph ML towards GFMs;
(2) we present a more comprehensive and fine-grained taxonomy of recent advancements of Graph ML in the era of LLMs;
(3) we delve into the limitations of recent Graph ML, and provide insights into how to overcome these limitations from LLM’s perspective;
(4) we further explore how graphs can be used to augment LLMs; and
(5) we thoroughly summarize a broad range of applications and present a more forward-looking discussion on the challenges and future directions.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">RELATED WORK</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we briefly review some related works in the fields of graph machine learning and foundation model techniques.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span><span class="ltx_text ltx_font_italic" id="S2.SS1.1.1">Graph Machine Learning</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">As one of the most active fields in artificial intelligence, graph learning has attracted considerable attention to its capability to model complex relationships and structures in data represented as graphs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib44" title="">44</a>]</cite>.
Nowadays, it has been widely adopted in various applications, including social network analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib45" title="">45</a>]</cite>, protein detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib46" title="">46</a>]</cite>, recommender systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib48" title="">48</a>]</cite>, etc.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">The initial phases of graph learning typically use Random Walks, which is a foundational method for exploring graph structures. This technique involves a stochastic process of moving from one node to another within a graph, which is instrumental in understanding node connectivity and influence within networks. Building upon Random Walks, Graph Embedding methods aim to represent nodes (or edges) as low-dimensional vectors while preserving graph topology and node relationships. Representative methods such as LINE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib49" title="">49</a>]</cite>, DeepWalk <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib50" title="">50</a>]</cite>, and Node2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib51" title="">51</a>]</cite> leverage Random Walks to learn node representations, capturing local structures and community information effectively.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Due to the exceptional representation learning and modeling capabilities, GNNs bolstered by deep learning have brought significant advances in graph learning. For example, GCNs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib52" title="">52</a>]</cite> introduce convolutional operations to graph data, enabling effective aggregation of neighborhood information for each node, thus enhancing node representation learning.
GraphSAGE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib53" title="">53</a>]</cite> learns a function to aggregate information from a node’s local neighborhood in an inductive setting, allowing efficient embedding generation for unseen nodes.
GAT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib54" title="">54</a>]</cite> further advances GNNs by integrating attention mechanisms, assigning varying weights to nodes in a neighborhood, thereby sharpening the model’s ability to focus on significant nodes.
Inspired by the success of transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib55" title="">55</a>]</cite> in NLP and CV, several studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib60" title="">60</a>]</cite> adopt self-attention mechanisms to graph data, providing a more global perspective of graph structures and interactions.
Recent works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib65" title="">65</a>]</cite> further leverage transformer architectures to enhance graph data modeling. For example, GraphFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib61" title="">61</a>]</cite> integrates GNN within each layer in the transformer, enabling simultaneous consideration of textual and graph information.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">The advancements in LLMs have given rise to graph learning. Recent works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib67" title="">67</a>]</cite> apply techniques from these advanced language models like LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib24" title="">24</a>]</cite> or ChatGPT to graph data, resulting in models capable of understanding and handling graph structures in a manner similar to natural language processing. A typical approach, GraphGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib23" title="">23</a>]</cite>, tokenizes graph data for insertion into LLMs (i.e. Vicuna <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib68" title="">68</a>]</cite> and LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib24" title="">24</a>]</cite>) thus providing a powerful generalization capability. GLEM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib69" title="">69</a>]</cite> further integrates the graph models and LLMs, specifically DeBERTa <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib70" title="">70</a>]</cite>, within a variational Expectation-Maximization (EM) framework. It alternates between updating LLM and GNN in the E-step and M-step, thereby scaling efficiently and improving effectiveness in downstream tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span><span class="ltx_text ltx_font_italic" id="S2.SS2.1.1">Foundation Models (FMs)</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Foundation Models (FMs) represent a significant breakthrough in the field of artificial intelligence, characterized by their ability to be extensively pre-trained on large-scale datasets and adapted to a variety of downstream tasks.
These models are distinguished by their extensive pre-training on large-scale datasets and their adaptability to a wide range of downstream tasks.
It is worth noting that FMs are not limited to a single field and can be found in natural language <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib15" title="">15</a>]</cite>, vision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib71" title="">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib72" title="">72</a>]</cite>, and graph domains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib19" title="">19</a>]</cite>, serving as a promising research direction.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">In the realm of vision, Visual Foundation Models (VFMs) have gained significant success, making substantial impacts on areas such as image recognition, object detection, and scene understanding.
Specifically, VFMs benefit from pre-training on extensive and diverse image datasets, allowing them to learn intricate patterns and features. For instance, models such as DALL-E <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib73" title="">73</a>]</cite>, and CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib71" title="">71</a>]</cite> leverage self-supervised learning to understand and generate images based on textual descriptions, demonstrating remarkable cross-modal understanding capabilities. Recent Visual ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib72" title="">72</a>]</cite> integrates ChatGPT with a series of Visual Foundation Models (VFMs), making it perform a variety of complex visual tasks.
These VFMs allow models to learn from a broader range of visual data, thereby improving their generalizability and robustness.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">In the sphere of Natural Language Processing (NLP), Large Language Models (LLMs) such as ChatGPT and LLaMA have also revolutionized the field <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib74" title="">74</a>]</cite>.
Characterized by their extensive scale, LLMs are trained on billions of parameters using extensive textual datasets, which enable them to excel in comprehending and generating natural language. The landscape of pre-trained language models is diverse, such as GPT (Generative Pre-trained Transformer) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib14" title="">14</a>]</cite>, BERT (Bidirectional Encoder Representations from Transformers) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib15" title="">15</a>]</cite> and T5 (Text-To-Text Transfer Transformer) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib25" title="">25</a>]</cite>. These models can broadly fall into three categories: encoder-only, decoder-only, and encoder-decoder models. Encoder-only models, such as BERT, specialize in understanding and interpreting language. In contrast, decoder-only models like GPT excel in generating coherent and contextually relevant text. Encoder-decoder models, like T5, combine both abilities, efficiently performing various NLP tasks from translation to summarization.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">As an encoder-only model, BERT introduces a paradigm in NLP with its innovative bi-directional attention mechanism, which analyzes text from both directions simultaneously, unlike its predecessors like transformer which processed text in a single direction (either left-to-right or right-to-left).
This feature allows BERT to attain a comprehensive context understanding, significantly improving its language nuance comprehension.
On the other hand, decoder-only models such as GPT, including variants like ChatGPT, utilize a unidirectional self-attention mechanism. This design makes them particularly effective in predicting subsequent words in a sequence, thus excelling in tasks like text completion, creative writing, language translation, and code generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib75" title="">75</a>]</cite>.
Additionally, as an encoder-decoder model, T5 uniquely transforms a variety of NLP tasks as text generation problems.
For example, it reframes sentiment analysis from a classification task to a text generation task, where input like ”Sentiment: Today is sunny” would prompt T5 to generate an output such as ”Positive”. This text-to-text approach underscores T5’s versatility and adaptability across diverse language tasks.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>A comparison of various DNN-based models. We present Models and their Architecture, Pretext Task, Adaptation Method, and Downstream Tasks.
<span class="ltx_text ltx_font_bold" id="S2.T1.3.1">URL</span> in <span class="ltx_text ltx_font_bold" id="S2.T1.4.2">Adaptation Method</span> indicates Unsupervised Representation Learning.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T1.5" style="width:737.2pt;height:562.2pt;vertical-align:-1.2pt;"><span class="ltx_transformed_inner" style="transform:translate(61.1pt,-46.5pt) scale(1.19862650062663,1.19862650062663) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.5.1">
<tr class="ltx_tr" id="S2.T1.5.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.5.1.1.1"><span class="ltx_text" id="S2.T1.5.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.5.1.1.2">Architecture</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.5.1.1.3">Pretext Task</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.5.1.1.4">Adaptation Method</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.5.1.1.5">Downstream Tasks</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.2.1">DGI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib76" title="">76</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.2.2">GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.2.3">Contrastive Learning</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.2.4">URL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.2.5">Node</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.3">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.3.1">GRACE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib77" title="">77</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.3.2">GNN</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.3.3">Contrastive Learning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.3.4">URL</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.3.5">Node</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.4">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.4.1">GraphMAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib78" title="">78</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.4.2">GNN</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.4.3">Graph Generation</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.4.4">URL</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.4.5">Node, Graph</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.5">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.5.1">MVGRL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib79" title="">79</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.5.2">GNN</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.5.3">Contrastive Learning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.5.4">URL</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.5.5">Node, Graph</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.6">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.6.1">GraphCL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib10" title="">10</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.6.2">GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.6.3">Contrastive Learning</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.6.4">Fine-tuning</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.6.5">Node, Graph</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.7">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.7.1">CSSL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib11" title="">11</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.7.2">GNN</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.7.3">Contrastive Learning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.7.4">Fine-tuning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.7.5">Graph</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.8">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.8.1">GCC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib13" title="">13</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.8.2">GNN</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.8.3">Contrastive Learning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.8.4">URL&amp;Fine-tuning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.8.5">Node, Graph</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.9">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.9.1">G-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib80" title="">80</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.9.2">BERT</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.9.3">Graph Generation</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.9.4">Fine-tuning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.9.5">Recommendation</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.10">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.10.1">AdapterGNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib81" title="">81</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.10.2">GNN</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.10.3">Multi-task</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.10.4">Fine-tuning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.10.5">Graph</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.11">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.11.1">GROVER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib82" title="">82</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.11.2">Graph Transformer</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.11.3">Property Prediction</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.11.4">Fine-tuning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.11.5">Graph</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.12">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.12.1">Graph-Bert <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib60" title="">60</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.12.2">Graph Transformer</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.12.3">Graph Generation</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.12.4">URL&amp;Fine-tuning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.12.5">Node</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.13">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.13.1">G-Adapter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib83" title="">83</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.13.2">Graph Transformer</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.13.3">Multi-task</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.13.4">Fine-tuning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.13.5">Graph</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.14">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.14.1">GraphGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib84" title="">84</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.14.2">Graph Transformer</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.14.3">Graph Generation</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.14.4">Fine-tuning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.14.5">Node, Edge, Graph</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.15">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.15.1">MoMu <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib85" title="">85</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.15.2">BERT, GNN</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.15.3">Contrastive Learning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.15.4">Fine-tuning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.15.5">Graph</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.16">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.16.1">TOUCHUP-G <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib86" title="">86</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.16.2">BERT, ViT, GNN</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.16.3">Contrastive Learning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.16.4">Fine-tuning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.16.5">Node, Edge</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.17">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.17.1">GraphPrompt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib87" title="">87</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.17.2">GNN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.17.3">Contrastive Learning</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.17.4">Prompt Tuning</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.17.5">Node, Graph</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.18">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.18.1">GPPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib88" title="">88</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.18.2">GNN</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.18.3">Contrastive Learning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.18.4">Prompt Tuning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.18.5">Node</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.19">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.19.1">PGCL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib89" title="">89</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.19.2">GNN</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.19.3">Contrastive Learning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.19.4">Prompt Tuning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.19.5">Node, Edge, Graph</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.20">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.20.1">GPF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib90" title="">90</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.20.2">GNN</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.20.3">Multi-task</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.20.4">Prompt Tuning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.20.5">Graph</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.21">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.21.1">ProG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib91" title="">91</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.21.2">GNN, Graph Transformer</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.21.3">Contrastive Learning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.21.4">Prompt Tuning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.21.5">Node, Edge, Graph</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.22">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.22.1">ULTRA-DP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib92" title="">92</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.22.2">GNN</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.22.3">Multi-task</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.22.4">Prompt Tuning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.22.5">Node</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.23">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.23.1">SAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib93" title="">93</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.23.2">GNN</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.23.3">Contrastive Learning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.23.4">Prompt Tuning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.23.5">Node, Graph</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.24">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.24.1">PRODIGY <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib94" title="">94</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.24.2">GNN</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.24.3">Multi-task</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.24.4">Prompt Tuning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.24.5">Node, Edge</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.25">
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.25.1">SGL-PT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib95" title="">95</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.25.2">GNN</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.25.3">Multi-task</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.25.4">Prompt Tuning</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.1.25.5">Node, Graph</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1.26">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.5.1.26.1">DeepGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib96" title="">96</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.5.1.26.2">Graph Transformer</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.5.1.26.3">Graph Regression</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.5.1.26.4">Prompt Tuning</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.5.1.26.5">Graph</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">The evolution of LLMs has seen the emergence of advanced models like GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib97" title="">97</a>]</cite>, LaMDA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib98" title="">98</a>]</cite>, PaLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib99" title="">99</a>]</cite>, and Vicuna <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib68" title="">68</a>]</cite>. These models represent significant advances in NLP, distinguished by their enhanced capabilities in comprehending and generating complex, fine-grained language.
Their training methods are usually more sophisticated, involving larger datasets and more powerful computational resources. This scaling up has led to unprecedented language understanding and generation capabilities, exhibiting emergent properties such as in-context learning (ICL), adaptability, and flexibility.
Furthermore, recent advancements demonstrate the successful integration of LLMs with other models, like recommender system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib17" title="">17</a>]</cite>, reinforcement learning (RL) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib100" title="">100</a>]</cite>, GNNs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib101" title="">101</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib102" title="">102</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib103" title="">103</a>]</cite>.
This integration enables LLMs to tackle both traditional and novel challenges, proposing prospective avenues for applications.</p>
</div>
<div class="ltx_para" id="S2.SS2.p6">
<p class="ltx_p" id="S2.SS2.p6.1">LLMs have found applications in diverse sectors like chemistry <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib104" title="">104</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib105" title="">105</a>]</cite>, education <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib106" title="">106</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib107" title="">107</a>]</cite>, and finance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib108" title="">108</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib109" title="">109</a>]</cite>.
In these fields, they contribute to various tasks from data analysis to personalized learning.
Particularly, LLMs exhibit great potential in graph tasks such as graph classification and link prediction, demonstrating their versatility and broad applicability.
Specifically, several studies like Simteg <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib26" title="">26</a>]</cite>, GraD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib102" title="">102</a>]</cite>, Graph-Toolformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib101" title="">101</a>]</cite>, Graph CoT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib110" title="">110</a>]</cite>, and Graphologue <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib103" title="">103</a>]</cite> have notably advanced graph learning.
These models utilize LLMs for textual graph learning, graph-aware distillation, and graph reasoning, illustrating the potential of LLMs in enhancing the understanding of and interaction with complex graph structures.</p>
</div>
<div class="ltx_para" id="S2.SS2.p7">
<p class="ltx_p" id="S2.SS2.p7.1">Although FMs have revolutionized Vision and NLP domains, the development of Graph Foundation Models (GFMs) is still in the nascent stages.
With the rapid evolution and significant potential of this field, it is imperative to continue exploring and developing advanced techniques that can further enhance Graph ML towards GFMs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Deep Learning on Graphs</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">With the rapid development of deep neural networks (DNNs), GNN techniques modeling graph structure and node attributes for representation learning have been widely explored and have become one key technology in Graph ML.
While vanilla GNNs demonstrate proficiency in various graph tasks, they still encounter several challenges such as scalability, generalization to unseen data, and limited capability in capturing complex graph structures.
To overcome these limitations, many efforts have been made to improve GNN with the self-supervised paradigm.
Therefore, to provide a comprehensive review of these methods, in this section, we first introduce the backbone architecture, including GNN-based models and graph transformer-based models.
After that, we explore two important aspects of self-supervised graph ML models: graph pretext tasks and downstream adaptation.
Note that a comprehensive summary of these methods is presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S2.T1" title="TABLE I ‣ 2.2 Foundation Models (FMs) ‣ 2 RELATED WORK ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span class="ltx_text ltx_font_italic" id="S3.SS1.1.1">Backbone Architecture</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">As one of the most active fields in the artificial intelligence (AI) community,
various GNN methods have been proposed to solve various tasks.
The powerful capability of these models is largely dependent on the development of their backbone architectures.
Therefore, in this subsection, we focus on two broadly used architectures: neighborhood aggregation-based models and graph transformer-based models.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Neighborhood Aggregation-based Model</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.4">Neighborhood aggregation-based models are the most popular graph learning architectures that have been extensively studied and applied in various downstream tasks.
These models operate based on the message-passing mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib111" title="">111</a>]</cite>, which updates a node‘s representation by aggregating the features of its neighboring nodes along with its own features. Formally, this process can be represented as:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S8.EGx1">
<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle m_{u}" class="ltx_Math" display="inline" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><msub id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mi id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">m</mi><mi id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml">u</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2">𝑚</ci><ci id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3">𝑢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle m_{u}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_m start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=Aggregate(f_{v},v\in\mathcal{N}_{u})," class="ltx_Math" display="inline" id="S3.E1.m2.2"><semantics id="S3.E1.m2.2a"><mrow id="S3.E1.m2.2.2.1" xref="S3.E1.m2.2.2.1.1.cmml"><mrow id="S3.E1.m2.2.2.1.1" xref="S3.E1.m2.2.2.1.1.cmml"><mi id="S3.E1.m2.2.2.1.1.3" xref="S3.E1.m2.2.2.1.1.3.cmml"></mi><mo id="S3.E1.m2.2.2.1.1.2" xref="S3.E1.m2.2.2.1.1.2.cmml">=</mo><mrow id="S3.E1.m2.2.2.1.1.1" xref="S3.E1.m2.2.2.1.1.1.cmml"><mi id="S3.E1.m2.2.2.1.1.1.3" xref="S3.E1.m2.2.2.1.1.1.3.cmml">A</mi><mo id="S3.E1.m2.2.2.1.1.1.2" xref="S3.E1.m2.2.2.1.1.1.2.cmml">⁢</mo><mi id="S3.E1.m2.2.2.1.1.1.4" xref="S3.E1.m2.2.2.1.1.1.4.cmml">g</mi><mo id="S3.E1.m2.2.2.1.1.1.2a" xref="S3.E1.m2.2.2.1.1.1.2.cmml">⁢</mo><mi id="S3.E1.m2.2.2.1.1.1.5" xref="S3.E1.m2.2.2.1.1.1.5.cmml">g</mi><mo id="S3.E1.m2.2.2.1.1.1.2b" xref="S3.E1.m2.2.2.1.1.1.2.cmml">⁢</mo><mi id="S3.E1.m2.2.2.1.1.1.6" xref="S3.E1.m2.2.2.1.1.1.6.cmml">r</mi><mo id="S3.E1.m2.2.2.1.1.1.2c" xref="S3.E1.m2.2.2.1.1.1.2.cmml">⁢</mo><mi id="S3.E1.m2.2.2.1.1.1.7" xref="S3.E1.m2.2.2.1.1.1.7.cmml">e</mi><mo id="S3.E1.m2.2.2.1.1.1.2d" xref="S3.E1.m2.2.2.1.1.1.2.cmml">⁢</mo><mi id="S3.E1.m2.2.2.1.1.1.8" xref="S3.E1.m2.2.2.1.1.1.8.cmml">g</mi><mo id="S3.E1.m2.2.2.1.1.1.2e" xref="S3.E1.m2.2.2.1.1.1.2.cmml">⁢</mo><mi id="S3.E1.m2.2.2.1.1.1.9" xref="S3.E1.m2.2.2.1.1.1.9.cmml">a</mi><mo id="S3.E1.m2.2.2.1.1.1.2f" xref="S3.E1.m2.2.2.1.1.1.2.cmml">⁢</mo><mi id="S3.E1.m2.2.2.1.1.1.10" xref="S3.E1.m2.2.2.1.1.1.10.cmml">t</mi><mo id="S3.E1.m2.2.2.1.1.1.2g" xref="S3.E1.m2.2.2.1.1.1.2.cmml">⁢</mo><mi id="S3.E1.m2.2.2.1.1.1.11" xref="S3.E1.m2.2.2.1.1.1.11.cmml">e</mi><mo id="S3.E1.m2.2.2.1.1.1.2h" xref="S3.E1.m2.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m2.2.2.1.1.1.1.1" xref="S3.E1.m2.2.2.1.1.1.1.1.1.cmml"><mo id="S3.E1.m2.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m2.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m2.2.2.1.1.1.1.1.1" xref="S3.E1.m2.2.2.1.1.1.1.1.1.cmml"><mrow id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.2.cmml"><msub id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.2.cmml">f</mi><mi id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.3.cmml">v</mi></msub><mo id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E1.m2.1.1" xref="S3.E1.m2.1.1.cmml">v</mi></mrow><mo id="S3.E1.m2.2.2.1.1.1.1.1.1.2" xref="S3.E1.m2.2.2.1.1.1.1.1.1.2.cmml">∈</mo><msub id="S3.E1.m2.2.2.1.1.1.1.1.1.3" xref="S3.E1.m2.2.2.1.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m2.2.2.1.1.1.1.1.1.3.2" xref="S3.E1.m2.2.2.1.1.1.1.1.1.3.2.cmml">𝒩</mi><mi id="S3.E1.m2.2.2.1.1.1.1.1.1.3.3" xref="S3.E1.m2.2.2.1.1.1.1.1.1.3.3.cmml">u</mi></msub></mrow><mo id="S3.E1.m2.2.2.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m2.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m2.2.2.1.2" xref="S3.E1.m2.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m2.2b"><apply id="S3.E1.m2.2.2.1.1.cmml" xref="S3.E1.m2.2.2.1"><eq id="S3.E1.m2.2.2.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.2"></eq><csymbol cd="latexml" id="S3.E1.m2.2.2.1.1.3.cmml" xref="S3.E1.m2.2.2.1.1.3">absent</csymbol><apply id="S3.E1.m2.2.2.1.1.1.cmml" xref="S3.E1.m2.2.2.1.1.1"><times id="S3.E1.m2.2.2.1.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.1.2"></times><ci id="S3.E1.m2.2.2.1.1.1.3.cmml" xref="S3.E1.m2.2.2.1.1.1.3">𝐴</ci><ci id="S3.E1.m2.2.2.1.1.1.4.cmml" xref="S3.E1.m2.2.2.1.1.1.4">𝑔</ci><ci id="S3.E1.m2.2.2.1.1.1.5.cmml" xref="S3.E1.m2.2.2.1.1.1.5">𝑔</ci><ci id="S3.E1.m2.2.2.1.1.1.6.cmml" xref="S3.E1.m2.2.2.1.1.1.6">𝑟</ci><ci id="S3.E1.m2.2.2.1.1.1.7.cmml" xref="S3.E1.m2.2.2.1.1.1.7">𝑒</ci><ci id="S3.E1.m2.2.2.1.1.1.8.cmml" xref="S3.E1.m2.2.2.1.1.1.8">𝑔</ci><ci id="S3.E1.m2.2.2.1.1.1.9.cmml" xref="S3.E1.m2.2.2.1.1.1.9">𝑎</ci><ci id="S3.E1.m2.2.2.1.1.1.10.cmml" xref="S3.E1.m2.2.2.1.1.1.10">𝑡</ci><ci id="S3.E1.m2.2.2.1.1.1.11.cmml" xref="S3.E1.m2.2.2.1.1.1.11">𝑒</ci><apply id="S3.E1.m2.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1"><in id="S3.E1.m2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.2"></in><list id="S3.E1.m2.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1"><apply id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.2">𝑓</ci><ci id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.3">𝑣</ci></apply><ci id="S3.E1.m2.1.1.cmml" xref="S3.E1.m2.1.1">𝑣</ci></list><apply id="S3.E1.m2.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m2.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m2.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.3.2">𝒩</ci><ci id="S3.E1.m2.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.3.3">𝑢</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m2.2c">\displaystyle=Aggregate(f_{v},v\in\mathcal{N}_{u}),</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m2.2d">= italic_A italic_g italic_g italic_r italic_e italic_g italic_a italic_t italic_e ( italic_f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , italic_v ∈ caligraphic_N start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle f_{u}^{\prime}" class="ltx_Math" display="inline" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><msubsup id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi id="S3.E2.m1.1.1.2.2" xref="S3.E2.m1.1.1.2.2.cmml">f</mi><mi id="S3.E2.m1.1.1.2.3" xref="S3.E2.m1.1.1.2.3.cmml">u</mi><mo id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml">′</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1">superscript</csymbol><apply id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.2.1.cmml" xref="S3.E2.m1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.2.2">𝑓</ci><ci id="S3.E2.m1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.2.3">𝑢</ci></apply><ci id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle f_{u}^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">italic_f start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=Update(m_{u},f_{u})," class="ltx_Math" display="inline" id="S3.E2.m2.1"><semantics id="S3.E2.m2.1a"><mrow id="S3.E2.m2.1.1.1" xref="S3.E2.m2.1.1.1.1.cmml"><mrow id="S3.E2.m2.1.1.1.1" xref="S3.E2.m2.1.1.1.1.cmml"><mi id="S3.E2.m2.1.1.1.1.4" xref="S3.E2.m2.1.1.1.1.4.cmml"></mi><mo id="S3.E2.m2.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.3.cmml">=</mo><mrow id="S3.E2.m2.1.1.1.1.2" xref="S3.E2.m2.1.1.1.1.2.cmml"><mi id="S3.E2.m2.1.1.1.1.2.4" xref="S3.E2.m2.1.1.1.1.2.4.cmml">U</mi><mo id="S3.E2.m2.1.1.1.1.2.3" xref="S3.E2.m2.1.1.1.1.2.3.cmml">⁢</mo><mi id="S3.E2.m2.1.1.1.1.2.5" xref="S3.E2.m2.1.1.1.1.2.5.cmml">p</mi><mo id="S3.E2.m2.1.1.1.1.2.3a" xref="S3.E2.m2.1.1.1.1.2.3.cmml">⁢</mo><mi id="S3.E2.m2.1.1.1.1.2.6" xref="S3.E2.m2.1.1.1.1.2.6.cmml">d</mi><mo id="S3.E2.m2.1.1.1.1.2.3b" xref="S3.E2.m2.1.1.1.1.2.3.cmml">⁢</mo><mi id="S3.E2.m2.1.1.1.1.2.7" xref="S3.E2.m2.1.1.1.1.2.7.cmml">a</mi><mo id="S3.E2.m2.1.1.1.1.2.3c" xref="S3.E2.m2.1.1.1.1.2.3.cmml">⁢</mo><mi id="S3.E2.m2.1.1.1.1.2.8" xref="S3.E2.m2.1.1.1.1.2.8.cmml">t</mi><mo id="S3.E2.m2.1.1.1.1.2.3d" xref="S3.E2.m2.1.1.1.1.2.3.cmml">⁢</mo><mi id="S3.E2.m2.1.1.1.1.2.9" xref="S3.E2.m2.1.1.1.1.2.9.cmml">e</mi><mo id="S3.E2.m2.1.1.1.1.2.3e" xref="S3.E2.m2.1.1.1.1.2.3.cmml">⁢</mo><mrow id="S3.E2.m2.1.1.1.1.2.2.2" xref="S3.E2.m2.1.1.1.1.2.2.3.cmml"><mo id="S3.E2.m2.1.1.1.1.2.2.2.3" stretchy="false" xref="S3.E2.m2.1.1.1.1.2.2.3.cmml">(</mo><msub id="S3.E2.m2.1.1.1.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m2.1.1.1.1.1.1.1.1.2" xref="S3.E2.m2.1.1.1.1.1.1.1.1.2.cmml">m</mi><mi id="S3.E2.m2.1.1.1.1.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.1.1.1.1.3.cmml">u</mi></msub><mo id="S3.E2.m2.1.1.1.1.2.2.2.4" xref="S3.E2.m2.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.E2.m2.1.1.1.1.2.2.2.2" xref="S3.E2.m2.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E2.m2.1.1.1.1.2.2.2.2.2" xref="S3.E2.m2.1.1.1.1.2.2.2.2.2.cmml">f</mi><mi id="S3.E2.m2.1.1.1.1.2.2.2.2.3" xref="S3.E2.m2.1.1.1.1.2.2.2.2.3.cmml">u</mi></msub><mo id="S3.E2.m2.1.1.1.1.2.2.2.5" stretchy="false" xref="S3.E2.m2.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m2.1.1.1.2" xref="S3.E2.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m2.1b"><apply id="S3.E2.m2.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1"><eq id="S3.E2.m2.1.1.1.1.3.cmml" xref="S3.E2.m2.1.1.1.1.3"></eq><csymbol cd="latexml" id="S3.E2.m2.1.1.1.1.4.cmml" xref="S3.E2.m2.1.1.1.1.4">absent</csymbol><apply id="S3.E2.m2.1.1.1.1.2.cmml" xref="S3.E2.m2.1.1.1.1.2"><times id="S3.E2.m2.1.1.1.1.2.3.cmml" xref="S3.E2.m2.1.1.1.1.2.3"></times><ci id="S3.E2.m2.1.1.1.1.2.4.cmml" xref="S3.E2.m2.1.1.1.1.2.4">𝑈</ci><ci id="S3.E2.m2.1.1.1.1.2.5.cmml" xref="S3.E2.m2.1.1.1.1.2.5">𝑝</ci><ci id="S3.E2.m2.1.1.1.1.2.6.cmml" xref="S3.E2.m2.1.1.1.1.2.6">𝑑</ci><ci id="S3.E2.m2.1.1.1.1.2.7.cmml" xref="S3.E2.m2.1.1.1.1.2.7">𝑎</ci><ci id="S3.E2.m2.1.1.1.1.2.8.cmml" xref="S3.E2.m2.1.1.1.1.2.8">𝑡</ci><ci id="S3.E2.m2.1.1.1.1.2.9.cmml" xref="S3.E2.m2.1.1.1.1.2.9">𝑒</ci><interval closure="open" id="S3.E2.m2.1.1.1.1.2.2.3.cmml" xref="S3.E2.m2.1.1.1.1.2.2.2"><apply id="S3.E2.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.2">𝑚</ci><ci id="S3.E2.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.3">𝑢</ci></apply><apply id="S3.E2.m2.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m2.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m2.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E2.m2.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E2.m2.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E2.m2.1.1.1.1.2.2.2.2.2">𝑓</ci><ci id="S3.E2.m2.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E2.m2.1.1.1.1.2.2.2.2.3">𝑢</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m2.1c">\displaystyle=Update(m_{u},f_{u}),</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m2.1d">= italic_U italic_p italic_d italic_a italic_t italic_e ( italic_m start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.SSS1.p1.3">where, for each node <math alttext="u" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.1.m1.1"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><mi id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.1b"><ci id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.1c">u</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.1.m1.1d">italic_u</annotation></semantics></math>, a message <math alttext="m_{u}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.2.m2.1"><semantics id="S3.SS1.SSS1.p1.2.m2.1a"><msub id="S3.SS1.SSS1.p1.2.m2.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS1.p1.2.m2.1.1.2" xref="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml">m</mi><mi id="S3.SS1.SSS1.p1.2.m2.1.1.3" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.cmml">u</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.2.m2.1b"><apply id="S3.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.2">𝑚</ci><ci id="S3.SS1.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3">𝑢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.2.m2.1c">m_{u}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.2.m2.1d">italic_m start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT</annotation></semantics></math> is generated through the aggregation function from its neighboring nodes. Subsequently, the graph signal <math alttext="f" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.3.m3.1"><semantics id="S3.SS1.SSS1.p1.3.m3.1a"><mi id="S3.SS1.SSS1.p1.3.m3.1.1" xref="S3.SS1.SSS1.p1.3.m3.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.3.m3.1b"><ci id="S3.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.3.m3.1c">f</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.3.m3.1d">italic_f</annotation></semantics></math> is updated with the message.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">GCN is a typical method designed to leverage both the graph structure and the node attributes. This architecture updates node representations by aggregating neighboring features with the node’s own.
As the number of network layers increases, each captures an increasingly larger neighborhood.
Owing to the efficiency and performance, GCN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib52" title="">52</a>]</cite> has been widely applied by several methods such as CSSL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib11" title="">11</a>]</cite> and PRODIGY <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib94" title="">94</a>]</cite>.
GraphSAGE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib53" title="">53</a>]</cite> is another notable neighborhood aggregation-based model.
Due to its inductive paradigms, GraphSAGE can easily generalize to unseen nodes or graphs, making it widely employed by many studies such as PinSage <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib112" title="">112</a>]</cite> for inductive learning.
Additionally, several studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib91" title="">91</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib94" title="">94</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib78" title="">78</a>]</cite> incorporate Graph Attention Networks (GATs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib54" title="">54</a>]</cite> as the backbone architecture. GATs integrate attention mechanisms into GNNs, assigning variable weights to neighboring nodes, thereby focusing on the most relevant parts of the input graph for improved node representations.
As another important model in the family of GNNs, Graph Isomorphism Network (GIN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib113" title="">113</a>]</cite> has also been widely used <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib95" title="">95</a>]</cite>, due to its powerful representation ability.
Its unique architecture guarantees the expressiveness equivalent to the Weisfeiler-Lehman isomorphism test, making it widely chosen as the backbone model for a lot of structure-intensive tasks.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.1">Although these models are widely adopted to solve graph tasks, they still suffer from some inherent limitations, such as over-smoothing and lack of generalization. In addition, the lower amount of parameters also limits the modeling capacity as the backbone model to serve multiple datasets and tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Graph Transformer-based Model</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">While neighborhood aggregation-based GNN models have shown remarkable performance in processing graph-structured data, they suffer from some limitations.
A significant challenge for these models is their difficulty in handling large graphs due to their reliance on local neighborhood information and their limited capacity in capturing long-range dependencies within the graph <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib114" title="">114</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib115" title="">115</a>]</cite>.
To overcome these problems, inspired by the success of the transformer model in various NLP tasks, graph transformer-based models have been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib57" title="">57</a>]</cite>. These models leverage the self-attention mechanism to adaptly capture both local and global graph structures, which allows the model to stack multiple layers without over-smoothing.
Due to the lower inductive bias, graph transformer-based models can learn the structural patterns from data rather than solely relying on the graph structure.
Additionally, transformers have demonstrated great scaling behavior in CV and NLP, suggesting that their performance can keep improving with more data and parameters.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1">Graph transformer-based models have been widely applied as a backbone architecture in various tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib82" title="">82</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib116" title="">116</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib96" title="">96</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib83" title="">83</a>]</cite>. For example, Heterformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib117" title="">117</a>]</cite> introduces a graph-empowered Transformers architecture by adding neighbor tokens into each language Transformer layer. Edgeformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib118" title="">118</a>]</cite> propose to encode text and structure inside each Transformer layer jointly.
Graph-Bert <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib60" title="">60</a>]</cite> employs a transformer to pre-train on the graph dataset with feature and edge reconstruction tasks and then fine-tunes for various downstream tasks.
Similarly, GROVER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib82" title="">82</a>]</cite> introduces a self-supervised graph transformer-based model designed specifically for large-scale molecular data.
It pre-trains on extensive molecular datasets and then fine-tunes for specific downstream tasks.
GraphGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib84" title="">84</a>]</cite> employs a (semi-)Eulerian path to transform the graph into a sequence of tokens, and then feeds the sequence into the transformer. Specifically, it constructs a dataset-specific vocabulary such that each node can correspond to a unique node ID.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p3">
<p class="ltx_p" id="S3.SS1.SSS2.p3.1">Despite graph transformer-based models that can somehow address the limitations of traditional GNNs, they also face several challenges. One of the challenges is the quadratic complexity caused by self-attention, which becomes particularly problematic for large-scale graphs. In addition, there is a risk of losing some information about the original graph structure when serializing the graph.</p>
</div>
<figure class="ltx_figure ltx_align_center" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="439" id="S3.F3.1.g1" src="x3.png" width="788"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A comparison of pre-training, fine-tuning, and prompt tuning. (a) Pre-training involves training the GNN model based on specific pre-training tasks. (b) Fine-tuning updates the parameters of the pre-trained GNN model according to the downstream tasks. (c) Prompt tuning generates and updates the features of the prompt according to the downstream tasks, while keeping the pre-trained GNN model fixed and without any modification.
</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span class="ltx_text ltx_font_italic" id="S3.SS2.1.1">Self-Supervised Learning on Graphs</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To adapt GNNs to various graph tasks, many self-supervised learning methods have been proposed and extensively studied.
These approaches enable GNNs to learn graph representations from the pre-training task and transfer them to various downstream tasks, such as node classification, graph classification, and link prediction.
Therefore, in this subsection, we will introduce graph self-supervised learning methods from pretext tasks and downstream adaptation, respectively.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Graph Pretext Tasks</h4>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p1.1.1">Graph Contrastive Learning</span> aims to learn augmentation representations by contrasting similar and dissimilar graph data pairs, effectively identifying nuanced relationships and structural patterns.
We can review graph contrastive learning from two perspectives: graph augmentations and the scale of contrast.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">Generally, graph augmentations can be broadly categorized into two types: 1) <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p2.1.1">feature perturbation</em> and 2) <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p2.1.2">topology perturbation</em>. They assume that tiny changes in the feature or structural space do not change the semantics of the Node/Edge/(sub)graph.
Feature perturbation involves perturbing the features of the nodes in the graph.
For example, GRACE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib77" title="">77</a>]</cite> randomly masks the node features to learn more robust representations.
On the other hand, topology perturbation mainly involves modifying the structure of the graph.
A typical example is CSSL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib11" title="">11</a>]</cite> which employs strategies like edge perturbation or node dropping to adopt graph-graph level contrast, thereby enhancing the robustness of representations.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1">Regarding the scale of contrast, the approaches can be divided into node-level and graph-level.
For example, GRACE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib77" title="">77</a>]</cite> computes the similarities between node-level embeddings to learn discriminative node representations.
GCC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib13" title="">13</a>]</cite> also works at the node level but learns local structural patterns by sampling a node’s neighbors to obtain subgraphs (positive pairs) and contrasting them with randomly selected non-contextual subgraphs (negative pairs).
In contrast, DGI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib76" title="">76</a>]</cite> contrasts node-level embeddings with graph-level embedding to capture global graph structures.
GraphCL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib10" title="">10</a>]</cite> takes a different approach by implementing graph-to-graph level contrast, thereby learning robust representations.
The scale used for pre-training has a huge impact on the downstream performance. When adopting contrastive learning as the pre-training task, one key challenge is how to design the objective such that the embeddings learned can account for downstream tasks of different scales.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p4">
<p class="ltx_p" id="S3.SS2.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p4.1.1">Graph Generation</span> methods aim to learn the distribution of graph data to enable graph generation or reconstruction.
In contrast to models in CV that predict masked image patches, or in NLP that predict the next token in a sequence, graph data presents a unique challenge due to its interconnected nature.
Consequently, graph generation methods typically work on the feature or structural space.
Feature generation methods focus on masking the features of one or a subset of nodes and then training the model to recover the masked features.
For instance, GraphMAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib78" title="">78</a>]</cite> utilizes a masked autoencoder framework to reconstruct masked graph portions based on their context, effectively capturing the underlying node semantics and their connection patterns.
Alternatively, structure generation methods concentrate on training the model to recover the graph structure.
The method GraphGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib84" title="">84</a>]</cite> encodes the graph into sequences of tokens and then employs a transformer decoder to predict the next token of the sequence to recover the connectivity of the graph.
In addition, Graph-Bert <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib60" title="">60</a>]</cite> is trained on both node attribute recovery and graph structure recovery tasks to ensure that the model captures local node attribute information while maintaining a global view of the graph structure.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p5">
<p class="ltx_p" id="S3.SS2.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p5.1.1">Graph Property Prediction</span>
methods gain guidance from the node-, edge-, and graph-level properties, which are inherently present in the graph data. These methods follow a training approach similar to supervised learning, as both utilize ”sample-label” pairs for training. The key distinction lies in the origin of the labels: in supervised learning, labels are manually annotated by human experts which can be costly in real scenarios, whereas in property-based learning, the labels are automatically generated from the graph using some heuristics or algorithms.
For example, GROVER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib82" title="">82</a>]</cite> utilizes professional software
to extract the information on graph motifs as labels for classification.
Similarly,  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib119" title="">119</a>]</cite> leverage statistical properties of the graph for graph self-supervised learning.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Downstream Adaptation</h4>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p1.1.1">Unsupervised Representation Learning</span> (URL) is a common method due to the scarcity of labeled data in the real world <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib76" title="">76</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib77" title="">77</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib78" title="">78</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib79" title="">79</a>]</cite>.
In URL, the pre-trained graph encoder is frozen and only a task-specific layer is learned during downstream tuning.
The learned representations are then directly fed into decoders.
This pattern allows URLs to be efficiently applied to downstream tasks.
For example, DGI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib76" title="">76</a>]</cite> trains an encoder model to learn node representations within graph-structured.
Node representations can then be used for downstream tasks.
However, due to the gap between the pretext task and downstream tasks, URL can also lead to suboptimal performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p2.1.1">Fine-tuning</span>
is the default method to adapt a pre-trained model to a certain downstream task. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S3.F3" title="Figure 3 ‣ 3.1.2 Graph Transformer-based Model ‣ 3.1 Backbone Architecture ‣ 3 Deep Learning on Graphs ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_tag">3</span></a>, it adds a randomly initialized task header (e.g., a classifier) on top of the pre-trained model, and during fine-tuning, both the backbone model and the header are jointly trained <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib11" title="">11</a>]</cite>.
Compared with URL, fine-tuning provides more flexibility as it allows changes in the backbone parameters, and one can choose the layers to be tuned while keeping others fixed.
Additionally, recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib81" title="">81</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib83" title="">83</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib10" title="">10</a>]</cite> further explore advanced graph fine-tuning methods that go beyond naive fine-tuning.
For instance, AdapterGNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib81" title="">81</a>]</cite> introduces two trainable adapters in parallel before and after the message passing. It freezes the GNN model during fine-tuning while only tuning the adapters, enabling parameter-efficient fine-tuning with minimal influence on the downstream performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p3.1.1">Prompt-tuning:</span>
”Pre-training &amp; fine-tuning” is prevalent in adapting pre-trained models to specific downstream tasks, but it overlooks the gap between pre-training and downstream tasks, potentially limiting generalization capabilities. Moreover, fine-tuning for different tasks also leads to significant time and computational costs.
Inspired by recent advancements in NLP, several methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib95" title="">95</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib88" title="">88</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib91" title="">91</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib89" title="">89</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib90" title="">90</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib93" title="">93</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib92" title="">92</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib96" title="">96</a>]</cite> have presented the potential of introducing prompts to adapt pre-trained models to specific tasks as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S3.F3" title="Figure 3 ‣ 3.1.2 Graph Transformer-based Model ‣ 3.1 Backbone Architecture ‣ 3 Deep Learning on Graphs ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_tag">3</span></a>.
Specifically, Prompt-tuning first unifies the downstream task with the pre-trained task into the same paradigm, followed by the introduction of learnable prompts for tuning.
For example, GPPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib88" title="">88</a>]</cite> first reframe node classification as link predictions.
GraphPrompt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib87" title="">87</a>]</cite> further extends graph classification into link prediction.
On the other hand, Prog <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib91" title="">91</a>]</cite> unifies all the downstream tasks into subgraph classification.
The inserting prompt including vectors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib88" title="">88</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib90" title="">90</a>]</cite>, node <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib95" title="">95</a>]</cite> and sub-graph <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib91" title="">91</a>]</cite>.
By inserting these prompts, the pre-trained parameters can be utilized in a way that aligns more closely with the requirements of the downstream tasks.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="524" id="S3.F4.g1" src="x4.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Illustration of LLMs for Graph ML.
(1) Methods using LLMs for <span class="ltx_text ltx_font_italic" id="S3.F4.4.1">Enhancing Feature Quality</span> by enhancing feature representation, generating augmented information, and aligning feature space.
(2) Explorations for <span class="ltx_text ltx_font_italic" id="S3.F4.5.2">solving Vanilla GNN Training Limitations</span> are categorized based on how structural information in the graph is processed: ignoring structural information, implicit structural information, and explicit structural information.
(3) Research about employing LLMs to alleviate the limitations of <span class="ltx_text ltx_font_italic" id="S3.F4.6.3">Heterophily and Generalization</span>.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">LLMs for Graph Models</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Despite great potential, Graph ML based on the GNNs has its inherent limitations.
Firstly, vanilla GNN models commonly demand labeled data for supervision, and obtaining these annotations can be resource-intensive in terms of time and cost. Secondly, real-world graphs often contain abundant textual information, which is crucial for downstream tasks. However, GNNs typically rely on shallow text embeddings for semantic extraction, thereby limiting their capacity to capture intricate semantics and text features.
Moreover, the diversity of graphs presents challenges for GNN models in terms of generalization across diverse domains and tasks.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Recently, LLMs have achieved remarkable success in handling natural language, with exciting features like (1) conducting zero/few-shot predictions and (2) providing a unified feature space. These capabilities present a potential solution to address the above challenges faced by Graph ML and GFMs.
Therefore, this section aims to investigate the contributions that current LLMs can make to enhance Graph ML’s progress towards GFMs, while also examining their current limitations, as Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S3.F4" title="Figure 4 ‣ 3.2.2 Downstream Adaptation ‣ 3.2 Self-Supervised Learning on Graphs ‣ 3 Deep Learning on Graphs ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_tag">4</span></a> shows.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>A summary of LLM for Graph ML research. We present the GNN model, LLM model, predictor, domain, task, datasets, and project link. <span class="ltx_text ltx_font_bold" id="S4.T2.98.1">FT</span> is Fine-tuning, refers to whether modifications are made to the parameters of the LLM model while <span class="ltx_text ltx_font_bold" id="S4.T2.99.2">PR</span> is Prompting, involves inputting textual prompts to the LLM to obtain responses. In the context of <span class="ltx_text ltx_font_bold" id="S4.T2.100.3">task</span>, ”node” denotes node-level tasks such as node classification, ”edge” signifies edge-level tasks like link prediction, ”graph” represents graph-level tasks such as graph classification, and ”structure” pertains to structure understanding tasks, such as node degree counting.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.94" style="width:433.6pt;height:429.4pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-289.9pt,286.8pt) scale(0.42784485985963,0.42784485985963) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.94.94">
<tr class="ltx_tr" id="S4.T2.94.94.95">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.94.94.95.1">Role</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.94.94.95.2">Sub Category</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.94.94.95.3">Method</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.94.94.95.4">GNN Model</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.94.94.95.5">LLM Model</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.94.94.95.6">FT</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.94.94.95.7">PR</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.94.94.95.8">Domain</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.94.94.95.9">Task</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.94.94.95.10">Datasets</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.94.94.95.11">Link</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.3" rowspan="15"><span class="ltx_text" id="S4.T2.2.2.2.3.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T2.2.2.2.3.1.1" style="width:8.9pt;height:118.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:118.6pt;transform:translate(-54.88pt,-53.9pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T2.2.2.2.3.1.1.1">Enhancing Feature Quality</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.4" rowspan="4"><span class="ltx_text" id="S4.T2.2.2.2.4.1"><span class="ltx_text" id="S4.T2.2.2.2.4.1.1"></span> <span class="ltx_text" id="S4.T2.2.2.2.4.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.2.2.2.4.1.2.1">
<span class="ltx_tr" id="S4.T2.2.2.2.4.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.2.2.2.4.1.2.1.1.1">Enhancing Feature</span></span>
<span class="ltx_tr" id="S4.T2.2.2.2.4.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.2.2.2.4.1.2.1.2.1">Representation</span></span>
</span></span> <span class="ltx_text" id="S4.T2.2.2.2.4.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.5">Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib27" title="">27</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.6">
<span class="ltx_text" id="S4.T2.2.2.2.6.1"></span> <span class="ltx_text" id="S4.T2.2.2.2.6.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.2.2.2.6.2.1">
<span class="ltx_tr" id="S4.T2.2.2.2.6.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.2.2.2.6.2.1.1.1">GCN, GAT, MLP</span></span>
</span></span><span class="ltx_text" id="S4.T2.2.2.2.6.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.7">ChatGPT, LLaMA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><times id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.2"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.2.m1.1a"><mo id="S4.T2.2.2.2.2.m1.1.1" mathcolor="#FF0000" xref="S4.T2.2.2.2.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><times id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.2.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.8">Citation, E-commerce</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.9">Node</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.10">5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.11"><a class="ltx_ref ltx_href" href="https://github.com/CurryTang/Graph-LLM" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.4">
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.3">SimTeG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib26" title="">26</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.4">
<span class="ltx_text" id="S4.T2.4.4.4.4.1"></span> <span class="ltx_text" id="S4.T2.4.4.4.4.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.4.4.4.4.2.1">
<span class="ltx_tr" id="S4.T2.4.4.4.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.4.4.4.2.1.1.1">SAGE, MLP, etc.</span></span>
</span></span><span class="ltx_text" id="S4.T2.4.4.4.4.3"></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.5">
<span class="ltx_text" id="S4.T2.4.4.4.5.1"></span> <span class="ltx_text" id="S4.T2.4.4.4.5.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.4.4.4.5.2.1">
<span class="ltx_tr" id="S4.T2.4.4.4.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.4.4.5.2.1.1.1">all- MiniLM-L6-v2, etc.</span></span>
</span></span><span class="ltx_text" id="S4.T2.4.4.4.5.3"></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.3.3.3.1.m1.1"><semantics id="S4.T2.3.3.3.1.m1.1a"><mi id="S4.T2.3.3.3.1.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.3.3.3.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.1.m1.1b"><ci id="S4.T2.3.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.2"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.4.4.4.2.m1.1"><semantics id="S4.T2.4.4.4.2.m1.1a"><mo id="S4.T2.4.4.4.2.m1.1.1" mathcolor="#FF0000" xref="S4.T2.4.4.4.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.2.m1.1b"><times id="S4.T2.4.4.4.2.m1.1.1.cmml" xref="S4.T2.4.4.4.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.4.2.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.6">Citation, E-commerce</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.7">Node, Edge</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.8">3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.9"><a class="ltx_ref ltx_href" href="https://github.com/vermouthdky/SimTeG" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.6.6">
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.6.3">LKPNR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib120" title="">120</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.6.4">wiki KG</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.6.5">
<span class="ltx_text" id="S4.T2.6.6.6.5.1"></span> <span class="ltx_text" id="S4.T2.6.6.6.5.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.6.6.6.5.2.1">
<span class="ltx_tr" id="S4.T2.6.6.6.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.6.6.6.5.2.1.1.1">ChatGLM2, RWKV,</span></span>
<span class="ltx_tr" id="S4.T2.6.6.6.5.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.6.6.6.5.2.1.2.1">LLaMA2</span></span>
</span></span><span class="ltx_text" id="S4.T2.6.6.6.5.3"></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.5.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.5.5.5.1.m1.1"><semantics id="S4.T2.5.5.5.1.m1.1a"><mi id="S4.T2.5.5.5.1.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.5.5.5.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.1.m1.1b"><ci id="S4.T2.5.5.5.1.m1.1.1.cmml" xref="S4.T2.5.5.5.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.5.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.6.2"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.6.6.6.2.m1.1"><semantics id="S4.T2.6.6.6.2.m1.1a"><mo id="S4.T2.6.6.6.2.m1.1.1" mathcolor="#FF0000" xref="S4.T2.6.6.6.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.2.m1.1b"><times id="S4.T2.6.6.6.2.m1.1.1.cmml" xref="S4.T2.6.6.6.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.6.6.2.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.6.6">Recommendation</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.6.7">Click Prediction</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.6.8">1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.6.9"><a class="ltx_ref ltx_href" href="https://github.com/Xuan-ZW/LKPNR" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.8.8">
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.8.3">GRID <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib121" title="">121</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.8.4">GAT</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.8.5">INSTRUCTOR</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.7.7.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.7.7.7.1.m1.1"><semantics id="S4.T2.7.7.7.1.m1.1a"><mo id="S4.T2.7.7.7.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.7.7.7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.7.1.m1.1b"><times id="S4.T2.7.7.7.1.m1.1.1.cmml" xref="S4.T2.7.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.7.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.7.7.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.8.2"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.8.8.8.2.m1.1"><semantics id="S4.T2.8.8.8.2.m1.1a"><mo id="S4.T2.8.8.8.2.m1.1.1" mathcolor="#FF0000" xref="S4.T2.8.8.8.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.8.2.m1.1b"><times id="S4.T2.8.8.8.2.m1.1.1.cmml" xref="S4.T2.8.8.8.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.8.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.8.8.2.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.8.6">Robotics</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.8.7">Robot Task Planning</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.8.8">2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.8.9"><a class="ltx_ref ltx_href" href="https://jackyzengl.github.io/GRID.github.io/" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.10.10.10">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.10.10.10.3" rowspan="9"><span class="ltx_text" id="S4.T2.10.10.10.3.1"><span class="ltx_text" id="S4.T2.10.10.10.3.1.1"></span> <span class="ltx_text" id="S4.T2.10.10.10.3.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.10.10.10.3.1.2.1">
<span class="ltx_tr" id="S4.T2.10.10.10.3.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.10.10.10.3.1.2.1.1.1">Generating Augmented</span></span>
<span class="ltx_tr" id="S4.T2.10.10.10.3.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.10.10.10.3.1.2.1.2.1">Information</span></span>
</span></span> <span class="ltx_text" id="S4.T2.10.10.10.3.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.10.10.10.4">TAPE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib122" title="">122</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.10.10.10.5">
<span class="ltx_text" id="S4.T2.10.10.10.5.1"></span> <span class="ltx_text" id="S4.T2.10.10.10.5.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.10.10.10.5.2.1">
<span class="ltx_tr" id="S4.T2.10.10.10.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.10.10.10.5.2.1.1.1">GCN, SAGE,MLP,</span></span>
<span class="ltx_tr" id="S4.T2.10.10.10.5.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.10.10.10.5.2.1.2.1">RevGAT</span></span>
</span></span><span class="ltx_text" id="S4.T2.10.10.10.5.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.10.10.10.6">ChatGPT, LLaMA2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.9.9.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.9.9.9.1.m1.1"><semantics id="S4.T2.9.9.9.1.m1.1a"><mo id="S4.T2.9.9.9.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.9.9.9.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.9.1.m1.1b"><times id="S4.T2.9.9.9.1.m1.1.1.cmml" xref="S4.T2.9.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.9.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.9.9.9.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.10.10.10.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.10.10.10.2.m1.1"><semantics id="S4.T2.10.10.10.2.m1.1a"><mi id="S4.T2.10.10.10.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.10.10.10.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.10.2.m1.1b"><ci id="S4.T2.10.10.10.2.m1.1.1.cmml" xref="S4.T2.10.10.10.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.10.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.10.10.10.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.10.10.10.7">Citation, E-commerce</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.10.10.10.8">Node</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.10.10.10.9">5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.10.10.10.10"><a class="ltx_ref ltx_href" href="https://github.com/XiaoxinHe/TAPE" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.12.12.12">
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.12.3">KEA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib27" title="">27</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.12.4">
<span class="ltx_text" id="S4.T2.12.12.12.4.1"></span> <span class="ltx_text" id="S4.T2.12.12.12.4.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.12.12.12.4.2.1">
<span class="ltx_tr" id="S4.T2.12.12.12.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.12.12.12.4.2.1.1.1">GCN, GAT, MLP</span></span>
</span></span><span class="ltx_text" id="S4.T2.12.12.12.4.3"></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.12.5">ChatGPT</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.11.11.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.11.11.11.1.m1.1"><semantics id="S4.T2.11.11.11.1.m1.1a"><mo id="S4.T2.11.11.11.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.11.11.11.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.11.11.11.1.m1.1b"><times id="S4.T2.11.11.11.1.m1.1.1.cmml" xref="S4.T2.11.11.11.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.11.11.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.11.11.11.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.12.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.12.12.12.2.m1.1"><semantics id="S4.T2.12.12.12.2.m1.1a"><mi id="S4.T2.12.12.12.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.12.12.12.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.12.12.12.2.m1.1b"><ci id="S4.T2.12.12.12.2.m1.1.1.cmml" xref="S4.T2.12.12.12.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.12.12.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.12.12.12.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.12.6">Citation, E-commerce</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.12.7">Node</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.12.8">5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.12.9"><a class="ltx_ref ltx_href" href="https://github.com/CurryTang/Graph-LLM" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.14.14.14">
<td class="ltx_td ltx_align_center" id="S4.T2.14.14.14.3">RLMRec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib123" title="">123</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.14.14.14.4">LightGCN</td>
<td class="ltx_td ltx_align_center" id="S4.T2.14.14.14.5">ChatGPT</td>
<td class="ltx_td ltx_align_center" id="S4.T2.13.13.13.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.13.13.13.1.m1.1"><semantics id="S4.T2.13.13.13.1.m1.1a"><mo id="S4.T2.13.13.13.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.13.13.13.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.13.13.13.1.m1.1b"><times id="S4.T2.13.13.13.1.m1.1.1.cmml" xref="S4.T2.13.13.13.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.13.13.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.13.13.13.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.14.14.14.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.14.14.14.2.m1.1"><semantics id="S4.T2.14.14.14.2.m1.1a"><mi id="S4.T2.14.14.14.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.14.14.14.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.14.14.14.2.m1.1b"><ci id="S4.T2.14.14.14.2.m1.1.1.cmml" xref="S4.T2.14.14.14.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.14.14.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.14.14.14.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.14.14.14.6">Recommendation</td>
<td class="ltx_td ltx_align_center" id="S4.T2.14.14.14.7">Recommendatoon</td>
<td class="ltx_td ltx_align_center" id="S4.T2.14.14.14.8">3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.14.14.14.9"><a class="ltx_ref ltx_href" href="https://github.com/HKUDS/RLMRec" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.16.16.16">
<td class="ltx_td ltx_align_center" id="S4.T2.16.16.16.3">LLMRec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib124" title="">124</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.16.16.16.4">LightGCN</td>
<td class="ltx_td ltx_align_center" id="S4.T2.16.16.16.5">
<span class="ltx_text" id="S4.T2.16.16.16.5.1"></span> <span class="ltx_text" id="S4.T2.16.16.16.5.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.16.16.16.5.2.1">
<span class="ltx_tr" id="S4.T2.16.16.16.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.16.16.16.5.2.1.1.1">ChatGPT</span></span>
</span></span><span class="ltx_text" id="S4.T2.16.16.16.5.3"></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.15.15.15.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.15.15.15.1.m1.1"><semantics id="S4.T2.15.15.15.1.m1.1a"><mo id="S4.T2.15.15.15.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.15.15.15.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.15.15.15.1.m1.1b"><times id="S4.T2.15.15.15.1.m1.1.1.cmml" xref="S4.T2.15.15.15.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.15.15.15.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.15.15.15.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.16.16.16.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.16.16.16.2.m1.1"><semantics id="S4.T2.16.16.16.2.m1.1a"><mi id="S4.T2.16.16.16.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.16.16.16.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.16.16.16.2.m1.1b"><ci id="S4.T2.16.16.16.2.m1.1.1.cmml" xref="S4.T2.16.16.16.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.16.16.16.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.16.16.16.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.16.16.16.6">Recommendation</td>
<td class="ltx_td ltx_align_center" id="S4.T2.16.16.16.7">Recommendation</td>
<td class="ltx_td ltx_align_center" id="S4.T2.16.16.16.8">2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.16.16.16.9"><a class="ltx_ref ltx_href" href="https://github.com/HKUDS/LLMRec" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.18.18.18">
<td class="ltx_td ltx_align_center" id="S4.T2.18.18.18.3">LLM-Rec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib125" title="">125</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.18.18.18.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.18.18.18.5">text-davinci-003</td>
<td class="ltx_td ltx_align_center" id="S4.T2.17.17.17.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.17.17.17.1.m1.1"><semantics id="S4.T2.17.17.17.1.m1.1a"><mi id="S4.T2.17.17.17.1.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.17.17.17.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.17.17.17.1.m1.1b"><ci id="S4.T2.17.17.17.1.m1.1.1.cmml" xref="S4.T2.17.17.17.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.17.17.17.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.17.17.17.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.18.18.18.2"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.18.18.18.2.m1.1"><semantics id="S4.T2.18.18.18.2.m1.1a"><mo id="S4.T2.18.18.18.2.m1.1.1" mathcolor="#FF0000" xref="S4.T2.18.18.18.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.18.18.18.2.m1.1b"><times id="S4.T2.18.18.18.2.m1.1.1.cmml" xref="S4.T2.18.18.18.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.18.18.18.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.18.18.18.2.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.18.18.18.6">Recommendation</td>
<td class="ltx_td ltx_align_center" id="S4.T2.18.18.18.7">Recommendation</td>
<td class="ltx_td ltx_align_center" id="S4.T2.18.18.18.8">2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.18.18.18.9">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.20.20.20">
<td class="ltx_td ltx_align_center" id="S4.T2.20.20.20.3">LLM4Mo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib66" title="">66</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.20.20.20.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.20.20.20.5">ChatGPT</td>
<td class="ltx_td ltx_align_center" id="S4.T2.19.19.19.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.19.19.19.1.m1.1"><semantics id="S4.T2.19.19.19.1.m1.1a"><mo id="S4.T2.19.19.19.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.19.19.19.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.19.19.19.1.m1.1b"><times id="S4.T2.19.19.19.1.m1.1.1.cmml" xref="S4.T2.19.19.19.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.19.19.19.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.19.19.19.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.20.20.20.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.20.20.20.2.m1.1"><semantics id="S4.T2.20.20.20.2.m1.1a"><mi id="S4.T2.20.20.20.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.20.20.20.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.20.20.20.2.m1.1b"><ci id="S4.T2.20.20.20.2.m1.1.1.cmml" xref="S4.T2.20.20.20.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.20.20.20.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.20.20.20.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.20.20.20.6">Molecular</td>
<td class="ltx_td ltx_align_center" id="S4.T2.20.20.20.7">Molecular Property Prediction</td>
<td class="ltx_td ltx_align_center" id="S4.T2.20.20.20.8">3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.20.20.20.9"><a class="ltx_ref ltx_href" href="https://github.com/ChnQ/LLM4Mol" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.22.22.22">
<td class="ltx_td ltx_align_center" id="S4.T2.22.22.22.3">GPT-MolBERTa <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib126" title="">126</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.22.22.22.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.22.22.22.5">ChatGPT</td>
<td class="ltx_td ltx_align_center" id="S4.T2.21.21.21.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.21.21.21.1.m1.1"><semantics id="S4.T2.21.21.21.1.m1.1a"><mo id="S4.T2.21.21.21.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.21.21.21.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.21.21.21.1.m1.1b"><times id="S4.T2.21.21.21.1.m1.1.1.cmml" xref="S4.T2.21.21.21.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.21.21.21.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.21.21.21.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.22.22.22.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.22.22.22.2.m1.1"><semantics id="S4.T2.22.22.22.2.m1.1a"><mi id="S4.T2.22.22.22.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.22.22.22.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.22.22.22.2.m1.1b"><ci id="S4.T2.22.22.22.2.m1.1.1.cmml" xref="S4.T2.22.22.22.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.22.22.22.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.22.22.22.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.22.22.22.6">Molecular</td>
<td class="ltx_td ltx_align_center" id="S4.T2.22.22.22.7">Molecular Property Prediction</td>
<td class="ltx_td ltx_align_center" id="S4.T2.22.22.22.8">9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.22.22.22.9">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.24.24.24">
<td class="ltx_td ltx_align_center" id="S4.T2.24.24.24.3">ENG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib127" title="">127</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.24.24.24.4">GCN,GAT</td>
<td class="ltx_td ltx_align_center" id="S4.T2.24.24.24.5">ChatGPT</td>
<td class="ltx_td ltx_align_center" id="S4.T2.23.23.23.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.23.23.23.1.m1.1"><semantics id="S4.T2.23.23.23.1.m1.1a"><mo id="S4.T2.23.23.23.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.23.23.23.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.23.23.23.1.m1.1b"><times id="S4.T2.23.23.23.1.m1.1.1.cmml" xref="S4.T2.23.23.23.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.23.23.23.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.23.23.23.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.24.24.24.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.24.24.24.2.m1.1"><semantics id="S4.T2.24.24.24.2.m1.1a"><mi id="S4.T2.24.24.24.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.24.24.24.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.24.24.24.2.m1.1b"><ci id="S4.T2.24.24.24.2.m1.1.1.cmml" xref="S4.T2.24.24.24.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.24.24.24.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.24.24.24.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.24.24.24.6">Citation</td>
<td class="ltx_td ltx_align_center" id="S4.T2.24.24.24.7">Node</td>
<td class="ltx_td ltx_align_center" id="S4.T2.24.24.24.8">-</td>
<td class="ltx_td" id="S4.T2.24.24.24.9"></td>
</tr>
<tr class="ltx_tr" id="S4.T2.26.26.26">
<td class="ltx_td ltx_align_center" id="S4.T2.26.26.26.3">Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib128" title="">128</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.26.26.26.4">GAT, GCN, etc.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.26.26.26.5">ChatGPT</td>
<td class="ltx_td ltx_align_center" id="S4.T2.25.25.25.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.25.25.25.1.m1.1"><semantics id="S4.T2.25.25.25.1.m1.1a"><mo id="S4.T2.25.25.25.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.25.25.25.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.25.25.25.1.m1.1b"><times id="S4.T2.25.25.25.1.m1.1.1.cmml" xref="S4.T2.25.25.25.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.25.25.25.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.25.25.25.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.26.26.26.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.26.26.26.2.m1.1"><semantics id="S4.T2.26.26.26.2.m1.1a"><mi id="S4.T2.26.26.26.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.26.26.26.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.26.26.26.2.m1.1b"><ci id="S4.T2.26.26.26.2.m1.1.1.cmml" xref="S4.T2.26.26.26.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.26.26.26.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.26.26.26.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.26.26.26.6">Citation</td>
<td class="ltx_td ltx_align_center" id="S4.T2.26.26.26.7">Node</td>
<td class="ltx_td ltx_align_center" id="S4.T2.26.26.26.8">4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.26.26.26.9">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.28.28.28">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.28.28.28.3">Aligning Feature Space</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.28.28.28.4">TouchUp-G <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib86" title="">86</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.28.28.28.5">SAGE, GAT, etc.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.28.28.28.6">BERT, etc.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.27.27.27.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.27.27.27.1.m1.1"><semantics id="S4.T2.27.27.27.1.m1.1a"><mi id="S4.T2.27.27.27.1.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.27.27.27.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.27.27.27.1.m1.1b"><ci id="S4.T2.27.27.27.1.m1.1.1.cmml" xref="S4.T2.27.27.27.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.27.27.27.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.27.27.27.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.28.28.28.2"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.28.28.28.2.m1.1"><semantics id="S4.T2.28.28.28.2.m1.1a"><mo id="S4.T2.28.28.28.2.m1.1.1" mathcolor="#FF0000" xref="S4.T2.28.28.28.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.28.28.28.2.m1.1b"><times id="S4.T2.28.28.28.2.m1.1.1.cmml" xref="S4.T2.28.28.28.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.28.28.28.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.28.28.28.2.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.28.28.28.7">Citation, E-commerce</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.28.28.28.8">Node</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.28.28.28.9">4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.28.28.28.10"><a class="ltx_ref ltx_href" href="https://github.com/jwzhi/TouchUp-G" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.30.30.30">
<td class="ltx_td" id="S4.T2.30.30.30.3"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.30.30.30.4">OFA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib129" title="">129</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.30.30.30.5">GCN,GAT,etc.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.30.30.30.6">LLaMA-2-7B, etc.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.29.29.29.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.29.29.29.1.m1.1"><semantics id="S4.T2.29.29.29.1.m1.1a"><mi id="S4.T2.29.29.29.1.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.29.29.29.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.29.29.29.1.m1.1b"><ci id="S4.T2.29.29.29.1.m1.1.1.cmml" xref="S4.T2.29.29.29.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.29.29.29.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.29.29.29.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.30.30.30.2"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.30.30.30.2.m1.1"><semantics id="S4.T2.30.30.30.2.m1.1a"><mo id="S4.T2.30.30.30.2.m1.1.1" mathcolor="#FF0000" xref="S4.T2.30.30.30.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.30.30.30.2.m1.1b"><times id="S4.T2.30.30.30.2.m1.1.1.cmml" xref="S4.T2.30.30.30.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.30.30.30.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.30.30.30.2.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.30.30.30.7">Citation, Molecular, etc.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.30.30.30.8">Node, Edge, Graph</td>
<td class="ltx_td ltx_align_center" id="S4.T2.30.30.30.9">9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.30.30.30.10"><a class="ltx_ref ltx_href" href="https://github.com/LechengKong/OneForAll" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.32.32.32">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.32.32.32.3" rowspan="25"><span class="ltx_text" id="S4.T2.32.32.32.3.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T2.32.32.32.3.1.1" style="width:8.9pt;height:185.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:185.8pt;transform:translate(-88.48pt,-87.51pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T2.32.32.32.3.1.1.1">Solving Vanilla GNN Training Limitations</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.32.32.32.4">
<span class="ltx_text" id="S4.T2.32.32.32.4.1"></span> <span class="ltx_text" id="S4.T2.32.32.32.4.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.32.32.32.4.2.1">
<span class="ltx_tr" id="S4.T2.32.32.32.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.32.32.32.4.2.1.1.1">Ignoring Structural</span></span>
<span class="ltx_tr" id="S4.T2.32.32.32.4.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.32.32.32.4.2.1.2.1">Information</span></span>
</span></span><span class="ltx_text" id="S4.T2.32.32.32.4.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.32.32.32.5">Hu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib130" title="">130</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.32.32.32.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.32.32.32.7">ChatGPT,GPT-4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.31.31.31.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.31.31.31.1.m1.1"><semantics id="S4.T2.31.31.31.1.m1.1a"><mo id="S4.T2.31.31.31.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.31.31.31.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.31.31.31.1.m1.1b"><times id="S4.T2.31.31.31.1.m1.1.1.cmml" xref="S4.T2.31.31.31.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.31.31.31.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.31.31.31.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.32.32.32.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.32.32.32.2.m1.1"><semantics id="S4.T2.32.32.32.2.m1.1a"><mi id="S4.T2.32.32.32.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.32.32.32.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.32.32.32.2.m1.1b"><ci id="S4.T2.32.32.32.2.m1.1.1.cmml" xref="S4.T2.32.32.32.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.32.32.32.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.32.32.32.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.32.32.32.8">Citation,KG</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.32.32.32.9">Node, Edge</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.32.32.32.10">5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.32.32.32.11">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.34.34.34">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.34.34.34.3" rowspan="14"><span class="ltx_text" id="S4.T2.34.34.34.3.1"><span class="ltx_text" id="S4.T2.34.34.34.3.1.1"></span> <span class="ltx_text" id="S4.T2.34.34.34.3.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.34.34.34.3.1.2.1">
<span class="ltx_tr" id="S4.T2.34.34.34.3.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.34.34.34.3.1.2.1.1.1">Implicit Structural</span></span>
<span class="ltx_tr" id="S4.T2.34.34.34.3.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.34.34.34.3.1.2.1.2.1">Information</span></span>
</span></span> <span class="ltx_text" id="S4.T2.34.34.34.3.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.34.34.34.4">GPT4Graph <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib22" title="">22</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.34.34.34.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.34.34.34.6">text-davinci-003</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.33.33.33.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.33.33.33.1.m1.1"><semantics id="S4.T2.33.33.33.1.m1.1a"><mo id="S4.T2.33.33.33.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.33.33.33.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.33.33.33.1.m1.1b"><times id="S4.T2.33.33.33.1.m1.1.1.cmml" xref="S4.T2.33.33.33.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.33.33.33.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.33.33.33.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.34.34.34.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.34.34.34.2.m1.1"><semantics id="S4.T2.34.34.34.2.m1.1a"><mi id="S4.T2.34.34.34.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.34.34.34.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.34.34.34.2.m1.1b"><ci id="S4.T2.34.34.34.2.m1.1.1.cmml" xref="S4.T2.34.34.34.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.34.34.34.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.34.34.34.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.34.34.34.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.34.34.34.8">Structure,Graph,Node,KGQA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.34.34.34.9">4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.34.34.34.10"><a class="ltx_ref ltx_href" href="https://anonymous.4open.%20science/r/GPT4Graph" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.36.36.36">
<td class="ltx_td ltx_align_center" id="S4.T2.36.36.36.3">GraphText <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib29" title="">29</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.36.36.36.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.36.36.36.5">ChatGPT, LLaMA2-7B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.35.35.35.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.35.35.35.1.m1.1"><semantics id="S4.T2.35.35.35.1.m1.1a"><mi id="S4.T2.35.35.35.1.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.35.35.35.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.35.35.35.1.m1.1b"><ci id="S4.T2.35.35.35.1.m1.1.1.cmml" xref="S4.T2.35.35.35.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.35.35.35.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.35.35.35.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.36.36.36.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.36.36.36.2.m1.1"><semantics id="S4.T2.36.36.36.2.m1.1a"><mi id="S4.T2.36.36.36.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.36.36.36.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.36.36.36.2.m1.1b"><ci id="S4.T2.36.36.36.2.m1.1.1.cmml" xref="S4.T2.36.36.36.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.36.36.36.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.36.36.36.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.36.36.36.6">Citation, Web</td>
<td class="ltx_td ltx_align_center" id="S4.T2.36.36.36.7">Node</td>
<td class="ltx_td ltx_align_center" id="S4.T2.36.36.36.8">7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.36.36.36.9">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.38.38.38">
<td class="ltx_td ltx_align_center" id="S4.T2.38.38.38.3">NLGraph <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib131" title="">131</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.38.38.38.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.38.38.38.5">ChatGPT, GPT-4, etc.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.37.37.37.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.37.37.37.1.m1.1"><semantics id="S4.T2.37.37.37.1.m1.1a"><mo id="S4.T2.37.37.37.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.37.37.37.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.37.37.37.1.m1.1b"><times id="S4.T2.37.37.37.1.m1.1.1.cmml" xref="S4.T2.37.37.37.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.37.37.37.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.37.37.37.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.38.38.38.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.38.38.38.2.m1.1"><semantics id="S4.T2.38.38.38.2.m1.1a"><mi id="S4.T2.38.38.38.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.38.38.38.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.38.38.38.2.m1.1b"><ci id="S4.T2.38.38.38.2.m1.1.1.cmml" xref="S4.T2.38.38.38.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.38.38.38.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.38.38.38.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.38.38.38.6">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.38.38.38.7">Structure</td>
<td class="ltx_td ltx_align_center" id="S4.T2.38.38.38.8">3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.38.38.38.9"><a class="ltx_ref ltx_href" href="https://github.com/Arthur-Heng/NLGraph" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.40.40.40">
<td class="ltx_td ltx_align_center" id="S4.T2.40.40.40.3">InstructGLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib21" title="">21</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.40.40.40.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.40.40.40.5">Flan T5, LLaMA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.39.39.39.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.39.39.39.1.m1.1"><semantics id="S4.T2.39.39.39.1.m1.1a"><mi id="S4.T2.39.39.39.1.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.39.39.39.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.39.39.39.1.m1.1b"><ci id="S4.T2.39.39.39.1.m1.1.1.cmml" xref="S4.T2.39.39.39.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.39.39.39.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.39.39.39.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.40.40.40.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.40.40.40.2.m1.1"><semantics id="S4.T2.40.40.40.2.m1.1a"><mi id="S4.T2.40.40.40.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.40.40.40.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.40.40.40.2.m1.1b"><ci id="S4.T2.40.40.40.2.m1.1.1.cmml" xref="S4.T2.40.40.40.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.40.40.40.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.40.40.40.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.40.40.40.6">Citation</td>
<td class="ltx_td ltx_align_center" id="S4.T2.40.40.40.7">Node</td>
<td class="ltx_td ltx_align_center" id="S4.T2.40.40.40.8">3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.40.40.40.9"><a class="ltx_ref ltx_href" href="https://github.com/Graphlet-AI/llm-graph-ai" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.42.42.42">
<td class="ltx_td ltx_align_center" id="S4.T2.42.42.42.3">LLMtoGraph <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib132" title="">132</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.42.42.42.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.42.42.42.5">
<span class="ltx_text" id="S4.T2.42.42.42.5.1"></span> <span class="ltx_text" id="S4.T2.42.42.42.5.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.42.42.42.5.2.1">
<span class="ltx_tr" id="S4.T2.42.42.42.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.42.42.42.5.2.1.1.1">ChatGPT, GPT-4,</span></span>
<span class="ltx_tr" id="S4.T2.42.42.42.5.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.42.42.42.5.2.1.2.1">Vicuna-13B, etc.</span></span>
</span></span><span class="ltx_text" id="S4.T2.42.42.42.5.3"></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.41.41.41.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.41.41.41.1.m1.1"><semantics id="S4.T2.41.41.41.1.m1.1a"><mo id="S4.T2.41.41.41.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.41.41.41.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.41.41.41.1.m1.1b"><times id="S4.T2.41.41.41.1.m1.1.1.cmml" xref="S4.T2.41.41.41.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.41.41.41.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.41.41.41.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.42.42.42.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.42.42.42.2.m1.1"><semantics id="S4.T2.42.42.42.2.m1.1a"><mi id="S4.T2.42.42.42.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.42.42.42.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.42.42.42.2.m1.1b"><ci id="S4.T2.42.42.42.2.m1.1.1.cmml" xref="S4.T2.42.42.42.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.42.42.42.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.42.42.42.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.42.42.42.6">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.42.42.42.7">Structure</td>
<td class="ltx_td ltx_align_center" id="S4.T2.42.42.42.8">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.42.42.42.9"><a class="ltx_ref ltx_href" href="https://github.com/Ayame1006/LLMtoGraph" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.44.44.44">
<td class="ltx_td ltx_align_center" id="S4.T2.44.44.44.3">Graph Agent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib133" title="">133</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.44.44.44.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.44.44.44.5">
<span class="ltx_text" id="S4.T2.44.44.44.5.1"></span> <span class="ltx_text" id="S4.T2.44.44.44.5.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.44.44.44.5.2.1">
<span class="ltx_tr" id="S4.T2.44.44.44.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.44.44.44.5.2.1.1.1">GPT-4,</span></span>
<span class="ltx_tr" id="S4.T2.44.44.44.5.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.44.44.44.5.2.1.2.1">embedding-ada-002</span></span>
</span></span><span class="ltx_text" id="S4.T2.44.44.44.5.3"></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.43.43.43.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.43.43.43.1.m1.1"><semantics id="S4.T2.43.43.43.1.m1.1a"><mo id="S4.T2.43.43.43.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.43.43.43.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.43.43.43.1.m1.1b"><times id="S4.T2.43.43.43.1.m1.1.1.cmml" xref="S4.T2.43.43.43.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.43.43.43.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.43.43.43.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.44.44.44.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.44.44.44.2.m1.1"><semantics id="S4.T2.44.44.44.2.m1.1a"><mi id="S4.T2.44.44.44.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.44.44.44.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.44.44.44.2.m1.1b"><ci id="S4.T2.44.44.44.2.m1.1.1.cmml" xref="S4.T2.44.44.44.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.44.44.44.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.44.44.44.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.44.44.44.6">Citation, Bioinformatics</td>
<td class="ltx_td ltx_align_center" id="S4.T2.44.44.44.7">Node, Edge</td>
<td class="ltx_td ltx_align_center" id="S4.T2.44.44.44.8">2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.44.44.44.9">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.46.46.46">
<td class="ltx_td ltx_align_center" id="S4.T2.46.46.46.3">LLM-Prop <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib134" title="">134</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.46.46.46.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.46.46.46.5">T5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.45.45.45.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.45.45.45.1.m1.1"><semantics id="S4.T2.45.45.45.1.m1.1a"><mi id="S4.T2.45.45.45.1.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.45.45.45.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.45.45.45.1.m1.1b"><ci id="S4.T2.45.45.45.1.m1.1.1.cmml" xref="S4.T2.45.45.45.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.45.45.45.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.45.45.45.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.46.46.46.2"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.46.46.46.2.m1.1"><semantics id="S4.T2.46.46.46.2.m1.1a"><mo id="S4.T2.46.46.46.2.m1.1.1" mathcolor="#FF0000" xref="S4.T2.46.46.46.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.46.46.46.2.m1.1b"><times id="S4.T2.46.46.46.2.m1.1.1.cmml" xref="S4.T2.46.46.46.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.46.46.46.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.46.46.46.2.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.46.46.46.6">materials science</td>
<td class="ltx_td ltx_align_center" id="S4.T2.46.46.46.7">Crystal Property Prediction</td>
<td class="ltx_td ltx_align_center" id="S4.T2.46.46.46.8">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.46.46.46.9"><a class="ltx_ref ltx_href" href="https://github.com/vertaix/LLM-Prop" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.48.48.48">
<td class="ltx_td ltx_align_center" id="S4.T2.48.48.48.3">GLRec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib135" title="">135</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.48.48.48.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.48.48.48.5">BELLE-LLaMA-7B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.47.47.47.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.47.47.47.1.m1.1"><semantics id="S4.T2.47.47.47.1.m1.1a"><mi id="S4.T2.47.47.47.1.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.47.47.47.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.47.47.47.1.m1.1b"><ci id="S4.T2.47.47.47.1.m1.1.1.cmml" xref="S4.T2.47.47.47.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.47.47.47.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.47.47.47.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.48.48.48.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.48.48.48.2.m1.1"><semantics id="S4.T2.48.48.48.2.m1.1a"><mi id="S4.T2.48.48.48.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.48.48.48.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.48.48.48.2.m1.1b"><ci id="S4.T2.48.48.48.2.m1.1.1.cmml" xref="S4.T2.48.48.48.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.48.48.48.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.48.48.48.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.48.48.48.6">Recommendation</td>
<td class="ltx_td ltx_align_center" id="S4.T2.48.48.48.7">Recommendation</td>
<td class="ltx_td ltx_align_center" id="S4.T2.48.48.48.8">1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.48.48.48.9">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.50.50.50">
<td class="ltx_td ltx_align_center" id="S4.T2.50.50.50.3">ReLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib136" title="">136</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.50.50.50.4">TAG,GCN</td>
<td class="ltx_td ltx_align_center" id="S4.T2.50.50.50.5">GPT-3.5, Vicuna</td>
<td class="ltx_td ltx_align_center" id="S4.T2.49.49.49.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.49.49.49.1.m1.1"><semantics id="S4.T2.49.49.49.1.m1.1a"><mo id="S4.T2.49.49.49.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.49.49.49.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.49.49.49.1.m1.1b"><times id="S4.T2.49.49.49.1.m1.1.1.cmml" xref="S4.T2.49.49.49.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.49.49.49.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.49.49.49.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.50.50.50.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.50.50.50.2.m1.1"><semantics id="S4.T2.50.50.50.2.m1.1a"><mi id="S4.T2.50.50.50.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.50.50.50.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.50.50.50.2.m1.1b"><ci id="S4.T2.50.50.50.2.m1.1.1.cmml" xref="S4.T2.50.50.50.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.50.50.50.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.50.50.50.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.50.50.50.6">Chemistry</td>
<td class="ltx_td ltx_align_center" id="S4.T2.50.50.50.7">Chemical Reaction Prediction</td>
<td class="ltx_td ltx_align_center" id="S4.T2.50.50.50.8">5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.50.50.50.9"><a class="ltx_ref ltx_href" href="https://github.com/syr-cn/ReLM" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.52.52.52">
<td class="ltx_td ltx_align_center" id="S4.T2.52.52.52.3">Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib27" title="">27</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.52.52.52.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.52.52.52.5">ChatGPT</td>
<td class="ltx_td ltx_align_center" id="S4.T2.51.51.51.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.51.51.51.1.m1.1"><semantics id="S4.T2.51.51.51.1.m1.1a"><mo id="S4.T2.51.51.51.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.51.51.51.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.51.51.51.1.m1.1b"><times id="S4.T2.51.51.51.1.m1.1.1.cmml" xref="S4.T2.51.51.51.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.51.51.51.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.51.51.51.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.52.52.52.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.52.52.52.2.m1.1"><semantics id="S4.T2.52.52.52.2.m1.1a"><mi id="S4.T2.52.52.52.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.52.52.52.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.52.52.52.2.m1.1b"><ci id="S4.T2.52.52.52.2.m1.1.1.cmml" xref="S4.T2.52.52.52.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.52.52.52.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.52.52.52.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.52.52.52.6">Citation, E-commerce</td>
<td class="ltx_td ltx_align_center" id="S4.T2.52.52.52.7">Node</td>
<td class="ltx_td ltx_align_center" id="S4.T2.52.52.52.8">5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.52.52.52.9"><a class="ltx_ref ltx_href" href="https://github.com/CurryTang/Graph-LLM" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.54.54.54">
<td class="ltx_td ltx_align_center" id="S4.T2.54.54.54.3">Hu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib130" title="">130</a>]</cite>
</td>
<td class="ltx_td" id="S4.T2.54.54.54.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.54.54.54.5">ChatGPT,GPT-4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.53.53.53.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.53.53.53.1.m1.1"><semantics id="S4.T2.53.53.53.1.m1.1a"><mo id="S4.T2.53.53.53.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.53.53.53.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.53.53.53.1.m1.1b"><times id="S4.T2.53.53.53.1.m1.1.1.cmml" xref="S4.T2.53.53.53.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.53.53.53.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.53.53.53.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.54.54.54.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.54.54.54.2.m1.1"><semantics id="S4.T2.54.54.54.2.m1.1a"><mi id="S4.T2.54.54.54.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.54.54.54.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.54.54.54.2.m1.1b"><ci id="S4.T2.54.54.54.2.m1.1.1.cmml" xref="S4.T2.54.54.54.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.54.54.54.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.54.54.54.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.54.54.54.6">Citation,KG</td>
<td class="ltx_td ltx_align_center" id="S4.T2.54.54.54.7">Node, Edge</td>
<td class="ltx_td ltx_align_center" id="S4.T2.54.54.54.8">5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.54.54.54.9">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.56.56.56">
<td class="ltx_td ltx_align_center" id="S4.T2.56.56.56.3">Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib137" title="">137</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.56.56.56.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.56.56.56.5">ChatGPT</td>
<td class="ltx_td ltx_align_center" id="S4.T2.55.55.55.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.55.55.55.1.m1.1"><semantics id="S4.T2.55.55.55.1.m1.1a"><mo id="S4.T2.55.55.55.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.55.55.55.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.55.55.55.1.m1.1b"><times id="S4.T2.55.55.55.1.m1.1.1.cmml" xref="S4.T2.55.55.55.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.55.55.55.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.55.55.55.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.56.56.56.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.56.56.56.2.m1.1"><semantics id="S4.T2.56.56.56.2.m1.1a"><mi id="S4.T2.56.56.56.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.56.56.56.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.56.56.56.2.m1.1b"><ci id="S4.T2.56.56.56.2.m1.1.1.cmml" xref="S4.T2.56.56.56.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.56.56.56.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.56.56.56.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.56.56.56.6">Citation, E-commerce</td>
<td class="ltx_td ltx_align_center" id="S4.T2.56.56.56.7">Node</td>
<td class="ltx_td ltx_align_center" id="S4.T2.56.56.56.8">5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.56.56.56.9"><a class="ltx_ref ltx_href" href="https://github.com/TRAIS-Lab/LLM-Structured-Data" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.58.58.58">
<td class="ltx_td ltx_align_center" id="S4.T2.58.58.58.3">Fatemi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib138" title="">138</a>]</cite>
</td>
<td class="ltx_td" id="S4.T2.58.58.58.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.58.58.58.5">PaLM2 XXS, PaLM 62B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.57.57.57.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.57.57.57.1.m1.1"><semantics id="S4.T2.57.57.57.1.m1.1a"><mo id="S4.T2.57.57.57.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.57.57.57.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.57.57.57.1.m1.1b"><times id="S4.T2.57.57.57.1.m1.1.1.cmml" xref="S4.T2.57.57.57.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.57.57.57.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.57.57.57.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.58.58.58.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.58.58.58.2.m1.1"><semantics id="S4.T2.58.58.58.2.m1.1a"><mi id="S4.T2.58.58.58.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.58.58.58.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.58.58.58.2.m1.1b"><ci id="S4.T2.58.58.58.2.m1.1.1.cmml" xref="S4.T2.58.58.58.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.58.58.58.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.58.58.58.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.58.58.58.6">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.58.58.58.7">Structure</td>
<td class="ltx_td ltx_align_center" id="S4.T2.58.58.58.8">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.58.58.58.9">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.60.60.60">
<td class="ltx_td ltx_align_center" id="S4.T2.60.60.60.3">MolReGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib139" title="">139</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.60.60.60.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.60.60.60.5">ChatGPT</td>
<td class="ltx_td ltx_align_center" id="S4.T2.59.59.59.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.59.59.59.1.m1.1"><semantics id="S4.T2.59.59.59.1.m1.1a"><mo id="S4.T2.59.59.59.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.59.59.59.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.59.59.59.1.m1.1b"><times id="S4.T2.59.59.59.1.m1.1.1.cmml" xref="S4.T2.59.59.59.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.59.59.59.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.59.59.59.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.60.60.60.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.60.60.60.2.m1.1"><semantics id="S4.T2.60.60.60.2.m1.1a"><mi id="S4.T2.60.60.60.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.60.60.60.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.60.60.60.2.m1.1b"><ci id="S4.T2.60.60.60.2.m1.1.1.cmml" xref="S4.T2.60.60.60.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.60.60.60.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.60.60.60.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.60.60.60.6">Molecular</td>
<td class="ltx_td ltx_align_center" id="S4.T2.60.60.60.7">molecule-caption translation</td>
<td class="ltx_td ltx_align_center" id="S4.T2.60.60.60.8">1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.60.60.60.9"><a class="ltx_ref ltx_href" href="https://github.com/phenixace/MolReGPT" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.62.62.62">
<td class="ltx_td" id="S4.T2.62.62.62.3"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.62.62.62.4">LLaGA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib140" title="">140</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.62.62.62.5">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.62.62.62.6">Vicuna-7B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.61.61.61.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.61.61.61.1.m1.1"><semantics id="S4.T2.61.61.61.1.m1.1a"><mo id="S4.T2.61.61.61.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.61.61.61.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.61.61.61.1.m1.1b"><times id="S4.T2.61.61.61.1.m1.1.1.cmml" xref="S4.T2.61.61.61.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.61.61.61.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.61.61.61.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.62.62.62.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.62.62.62.2.m1.1"><semantics id="S4.T2.62.62.62.2.m1.1a"><mi id="S4.T2.62.62.62.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.62.62.62.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.62.62.62.2.m1.1b"><ci id="S4.T2.62.62.62.2.m1.1.1.cmml" xref="S4.T2.62.62.62.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.62.62.62.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.62.62.62.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.62.62.62.7">Citation, E-commerce</td>
<td class="ltx_td ltx_align_center" id="S4.T2.62.62.62.8">Node, Edge</td>
<td class="ltx_td ltx_align_center" id="S4.T2.62.62.62.9">4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.62.62.62.10"><a class="ltx_ref ltx_href" href="https://github.com/VITA-Group/LLaGA" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.64.64.64">
<td class="ltx_td" id="S4.T2.64.64.64.3"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.64.64.64.4">GraphEdit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib141" title="">141</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.64.64.64.5">GCN</td>
<td class="ltx_td ltx_align_center" id="S4.T2.64.64.64.6">Vicuna-v1.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.63.63.63.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.63.63.63.1.m1.1"><semantics id="S4.T2.63.63.63.1.m1.1a"><mi id="S4.T2.63.63.63.1.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.63.63.63.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.63.63.63.1.m1.1b"><ci id="S4.T2.63.63.63.1.m1.1.1.cmml" xref="S4.T2.63.63.63.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.63.63.63.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.63.63.63.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.64.64.64.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.64.64.64.2.m1.1"><semantics id="S4.T2.64.64.64.2.m1.1a"><mi id="S4.T2.64.64.64.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.64.64.64.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.64.64.64.2.m1.1b"><ci id="S4.T2.64.64.64.2.m1.1.1.cmml" xref="S4.T2.64.64.64.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.64.64.64.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.64.64.64.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.64.64.64.7">Citation</td>
<td class="ltx_td ltx_align_center" id="S4.T2.64.64.64.8">Node</td>
<td class="ltx_td ltx_align_center" id="S4.T2.64.64.64.9">3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.64.64.64.10"><a class="ltx_ref ltx_href" href="https://github.com/HKUDS/GraphEdit" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.66.66.66">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.66.66.66.3" rowspan="10"><span class="ltx_text" id="S4.T2.66.66.66.3.1"><span class="ltx_text" id="S4.T2.66.66.66.3.1.1"></span> <span class="ltx_text" id="S4.T2.66.66.66.3.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.66.66.66.3.1.2.1">
<span class="ltx_tr" id="S4.T2.66.66.66.3.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.66.66.66.3.1.2.1.1.1">Explicit Structure</span></span>
<span class="ltx_tr" id="S4.T2.66.66.66.3.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.66.66.66.3.1.2.1.2.1">Information</span></span>
</span></span> <span class="ltx_text" id="S4.T2.66.66.66.3.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.66.66.66.4">GraphGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib23" title="">23</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.66.66.66.5">Graph Transformer</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.66.66.66.6">Vicuna-7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.65.65.65.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.65.65.65.1.m1.1"><semantics id="S4.T2.65.65.65.1.m1.1a"><mi id="S4.T2.65.65.65.1.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.65.65.65.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.65.65.65.1.m1.1b"><ci id="S4.T2.65.65.65.1.m1.1.1.cmml" xref="S4.T2.65.65.65.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.65.65.65.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.65.65.65.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.66.66.66.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.66.66.66.2.m1.1"><semantics id="S4.T2.66.66.66.2.m1.1a"><mi id="S4.T2.66.66.66.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.66.66.66.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.66.66.66.2.m1.1b"><ci id="S4.T2.66.66.66.2.m1.1.1.cmml" xref="S4.T2.66.66.66.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.66.66.66.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.66.66.66.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.66.66.66.7">Citation</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.66.66.66.8">Node</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.66.66.66.9">3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.66.66.66.10"><a class="ltx_ref ltx_href" href="https://graphgpt.github.io/c" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.68.68.68">
<td class="ltx_td ltx_align_center" id="S4.T2.68.68.68.3">GraphLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib142" title="">142</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.68.68.68.4">Graph Transformer</td>
<td class="ltx_td ltx_align_center" id="S4.T2.68.68.68.5">LLaMA2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.67.67.67.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.67.67.67.1.m1.1"><semantics id="S4.T2.67.67.67.1.m1.1a"><mo id="S4.T2.67.67.67.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.67.67.67.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.67.67.67.1.m1.1b"><times id="S4.T2.67.67.67.1.m1.1.1.cmml" xref="S4.T2.67.67.67.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.67.67.67.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.67.67.67.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.68.68.68.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.68.68.68.2.m1.1"><semantics id="S4.T2.68.68.68.2.m1.1a"><mi id="S4.T2.68.68.68.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.68.68.68.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.68.68.68.2.m1.1b"><ci id="S4.T2.68.68.68.2.m1.1.1.cmml" xref="S4.T2.68.68.68.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.68.68.68.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.68.68.68.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.68.68.68.6">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.68.68.68.7">Structure</td>
<td class="ltx_td ltx_align_center" id="S4.T2.68.68.68.8">4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.68.68.68.9"><a class="ltx_ref ltx_href" href="https://github.com/mistyreed63849/Graph-LLM" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.70.70.70">
<td class="ltx_td ltx_align_center" id="S4.T2.70.70.70.3">GNP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib143" title="">143</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.70.70.70.4">GAT</td>
<td class="ltx_td ltx_align_center" id="S4.T2.70.70.70.5">Flan T5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.69.69.69.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.69.69.69.1.m1.1"><semantics id="S4.T2.69.69.69.1.m1.1a"><mi id="S4.T2.69.69.69.1.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.69.69.69.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.69.69.69.1.m1.1b"><ci id="S4.T2.69.69.69.1.m1.1.1.cmml" xref="S4.T2.69.69.69.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.69.69.69.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.69.69.69.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.70.70.70.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.70.70.70.2.m1.1"><semantics id="S4.T2.70.70.70.2.m1.1a"><mi id="S4.T2.70.70.70.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.70.70.70.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.70.70.70.2.m1.1b"><ci id="S4.T2.70.70.70.2.m1.1.1.cmml" xref="S4.T2.70.70.70.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.70.70.70.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.70.70.70.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.70.70.70.6">KG</td>
<td class="ltx_td ltx_align_center" id="S4.T2.70.70.70.7">KGQA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.70.70.70.8">4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.70.70.70.9">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.72.72.72">
<td class="ltx_td ltx_align_center" id="S4.T2.72.72.72.3">DrugChat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib144" title="">144</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.72.72.72.4">GAT, etc.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.72.72.72.5">Vicuna-13B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.71.71.71.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.71.71.71.1.m1.1"><semantics id="S4.T2.71.71.71.1.m1.1a"><mo id="S4.T2.71.71.71.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.71.71.71.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.71.71.71.1.m1.1b"><times id="S4.T2.71.71.71.1.m1.1.1.cmml" xref="S4.T2.71.71.71.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.71.71.71.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.71.71.71.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.72.72.72.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.72.72.72.2.m1.1"><semantics id="S4.T2.72.72.72.2.m1.1a"><mi id="S4.T2.72.72.72.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.72.72.72.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.72.72.72.2.m1.1b"><ci id="S4.T2.72.72.72.2.m1.1.1.cmml" xref="S4.T2.72.72.72.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.72.72.72.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.72.72.72.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.72.72.72.6">Drug</td>
<td class="ltx_td ltx_align_center" id="S4.T2.72.72.72.7">QA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.72.72.72.8">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.72.72.72.9"><a class="ltx_ref ltx_href" href="https://github.com/UCSD-AI4H/drugchat" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.74.74.74">
<td class="ltx_td ltx_align_center" id="S4.T2.74.74.74.3">KoPA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib145" title="">145</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.74.74.74.4">RotateE</td>
<td class="ltx_td ltx_align_center" id="S4.T2.74.74.74.5">Alpaca-7B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.73.73.73.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.73.73.73.1.m1.1"><semantics id="S4.T2.73.73.73.1.m1.1a"><mi id="S4.T2.73.73.73.1.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.73.73.73.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.73.73.73.1.m1.1b"><ci id="S4.T2.73.73.73.1.m1.1.1.cmml" xref="S4.T2.73.73.73.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.73.73.73.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.73.73.73.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.74.74.74.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.74.74.74.2.m1.1"><semantics id="S4.T2.74.74.74.2.m1.1a"><mi id="S4.T2.74.74.74.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.74.74.74.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.74.74.74.2.m1.1b"><ci id="S4.T2.74.74.74.2.m1.1.1.cmml" xref="S4.T2.74.74.74.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.74.74.74.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.74.74.74.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.74.74.74.6">Knowledge Graph</td>
<td class="ltx_td ltx_align_center" id="S4.T2.74.74.74.7">Knowledge Graph Completion</td>
<td class="ltx_td ltx_align_center" id="S4.T2.74.74.74.8">3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.74.74.74.9"><a class="ltx_ref ltx_href" href="https://github.com/zjukg/KoPA" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.76.76.76">
<td class="ltx_td ltx_align_center" id="S4.T2.76.76.76.3">GIMLET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib146" title="">146</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.76.76.76.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.76.76.76.5">T5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.75.75.75.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.75.75.75.1.m1.1"><semantics id="S4.T2.75.75.75.1.m1.1a"><mi id="S4.T2.75.75.75.1.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.75.75.75.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.75.75.75.1.m1.1b"><ci id="S4.T2.75.75.75.1.m1.1.1.cmml" xref="S4.T2.75.75.75.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.75.75.75.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.75.75.75.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.76.76.76.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.76.76.76.2.m1.1"><semantics id="S4.T2.76.76.76.2.m1.1a"><mi id="S4.T2.76.76.76.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.76.76.76.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.76.76.76.2.m1.1b"><ci id="S4.T2.76.76.76.2.m1.1.1.cmml" xref="S4.T2.76.76.76.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.76.76.76.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.76.76.76.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.76.76.76.6">Molecular</td>
<td class="ltx_td ltx_align_center" id="S4.T2.76.76.76.7">Molecular Property Prediction</td>
<td class="ltx_td ltx_align_center" id="S4.T2.76.76.76.8">14</td>
<td class="ltx_td ltx_align_center" id="S4.T2.76.76.76.9"><a class="ltx_ref ltx_href" href="https://github.com/zhao-ht/GIMLET" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.78.78.78">
<td class="ltx_td ltx_align_center" id="S4.T2.78.78.78.3">GIT-Mol <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib147" title="">147</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.78.78.78.4">GIN</td>
<td class="ltx_td ltx_align_center" id="S4.T2.78.78.78.5">MolT5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.77.77.77.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.77.77.77.1.m1.1"><semantics id="S4.T2.77.77.77.1.m1.1a"><mi id="S4.T2.77.77.77.1.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.77.77.77.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.77.77.77.1.m1.1b"><ci id="S4.T2.77.77.77.1.m1.1.1.cmml" xref="S4.T2.77.77.77.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.77.77.77.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.77.77.77.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.78.78.78.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.78.78.78.2.m1.1"><semantics id="S4.T2.78.78.78.2.m1.1a"><mi id="S4.T2.78.78.78.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.78.78.78.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.78.78.78.2.m1.1b"><ci id="S4.T2.78.78.78.2.m1.1.1.cmml" xref="S4.T2.78.78.78.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.78.78.78.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.78.78.78.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.78.78.78.6">Molecular</td>
<td class="ltx_td ltx_align_center" id="S4.T2.78.78.78.7">Molecular Generation, etc.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.78.78.78.8">6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.78.78.78.9">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.80.80.80">
<td class="ltx_td ltx_align_center" id="S4.T2.80.80.80.3">BioMedGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib148" title="">148</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.80.80.80.4">GIN</td>
<td class="ltx_td ltx_align_center" id="S4.T2.80.80.80.5">LLaMA2-7B-Chat</td>
<td class="ltx_td ltx_align_center" id="S4.T2.79.79.79.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.79.79.79.1.m1.1"><semantics id="S4.T2.79.79.79.1.m1.1a"><mi id="S4.T2.79.79.79.1.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.79.79.79.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.79.79.79.1.m1.1b"><ci id="S4.T2.79.79.79.1.m1.1.1.cmml" xref="S4.T2.79.79.79.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.79.79.79.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.79.79.79.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.80.80.80.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.80.80.80.2.m1.1"><semantics id="S4.T2.80.80.80.2.m1.1a"><mi id="S4.T2.80.80.80.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.80.80.80.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.80.80.80.2.m1.1b"><ci id="S4.T2.80.80.80.2.m1.1.1.cmml" xref="S4.T2.80.80.80.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.80.80.80.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.80.80.80.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.80.80.80.6">Biomedical</td>
<td class="ltx_td ltx_align_center" id="S4.T2.80.80.80.7">QA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.80.80.80.8">3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.80.80.80.9"><a class="ltx_ref ltx_href" href="https://github.com/PharMolix/OpenBioMed" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.82.82.82">
<td class="ltx_td" id="S4.T2.82.82.82.3"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.82.82.82.4">ProteinChat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib149" title="">149</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.82.82.82.5">GVP-GNN</td>
<td class="ltx_td ltx_align_center" id="S4.T2.82.82.82.6">Vicuna-13B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.81.81.81.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.81.81.81.1.m1.1"><semantics id="S4.T2.81.81.81.1.m1.1a"><mo id="S4.T2.81.81.81.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.81.81.81.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.81.81.81.1.m1.1b"><times id="S4.T2.81.81.81.1.m1.1.1.cmml" xref="S4.T2.81.81.81.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.81.81.81.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.81.81.81.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.82.82.82.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.82.82.82.2.m1.1"><semantics id="S4.T2.82.82.82.2.m1.1a"><mi id="S4.T2.82.82.82.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.82.82.82.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.82.82.82.2.m1.1b"><ci id="S4.T2.82.82.82.2.m1.1.1.cmml" xref="S4.T2.82.82.82.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.82.82.82.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.82.82.82.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.82.82.82.7">Protein</td>
<td class="ltx_td ltx_align_center" id="S4.T2.82.82.82.8">QA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.82.82.82.9">1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.82.82.82.10"><a class="ltx_ref ltx_href" href="https://github.com/UCSD-AI4H/proteinchat" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.84.84.84">
<td class="ltx_td" id="S4.T2.84.84.84.3"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.84.84.84.4">DGTL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib150" title="">150</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.84.84.84.5">Disentangled GNN</td>
<td class="ltx_td ltx_align_center" id="S4.T2.84.84.84.6">LLaMA-2-13B-chat</td>
<td class="ltx_td ltx_align_center" id="S4.T2.83.83.83.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.83.83.83.1.m1.1"><semantics id="S4.T2.83.83.83.1.m1.1a"><mo id="S4.T2.83.83.83.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.83.83.83.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.83.83.83.1.m1.1b"><times id="S4.T2.83.83.83.1.m1.1.1.cmml" xref="S4.T2.83.83.83.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.83.83.83.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.83.83.83.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.84.84.84.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.84.84.84.2.m1.1"><semantics id="S4.T2.84.84.84.2.m1.1a"><mi id="S4.T2.84.84.84.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.84.84.84.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.84.84.84.2.m1.1b"><ci id="S4.T2.84.84.84.2.m1.1.1.cmml" xref="S4.T2.84.84.84.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.84.84.84.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.84.84.84.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.84.84.84.7">Citation, E-commerce</td>
<td class="ltx_td ltx_align_center" id="S4.T2.84.84.84.8">Node</td>
<td class="ltx_td ltx_align_center" id="S4.T2.84.84.84.9">3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.84.84.84.10">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.86.86.86">
<td class="ltx_td" id="S4.T2.86.86.86.3"></td>
<td class="ltx_td" id="S4.T2.86.86.86.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.86.86.86.5">G-Retriever <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib151" title="">151</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.86.86.86.6">GAT</td>
<td class="ltx_td ltx_align_center" id="S4.T2.86.86.86.7">LLaMA-2-7B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.85.85.85.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.85.85.85.1.m1.1"><semantics id="S4.T2.85.85.85.1.m1.1a"><mi id="S4.T2.85.85.85.1.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.85.85.85.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.85.85.85.1.m1.1b"><ci id="S4.T2.85.85.85.1.m1.1.1.cmml" xref="S4.T2.85.85.85.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.85.85.85.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.85.85.85.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.86.86.86.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.86.86.86.2.m1.1"><semantics id="S4.T2.86.86.86.2.m1.1a"><mi id="S4.T2.86.86.86.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.86.86.86.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.86.86.86.2.m1.1b"><ci id="S4.T2.86.86.86.2.m1.1.1.cmml" xref="S4.T2.86.86.86.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.86.86.86.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.86.86.86.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.86.86.86.8">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.86.86.86.9">QA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.86.86.86.10">3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.86.86.86.11"><a class="ltx_ref ltx_href" href="https://github.com/XiaoxinHe/G-Retriever" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.88.88.88">
<td class="ltx_td" id="S4.T2.88.88.88.3"></td>
<td class="ltx_td" id="S4.T2.88.88.88.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.88.88.88.5">GraphToken <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib152" title="">152</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.88.88.88.6">GCN,GIN,etc.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.88.88.88.7">PaLM 2 S</td>
<td class="ltx_td ltx_align_center" id="S4.T2.87.87.87.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.87.87.87.1.m1.1"><semantics id="S4.T2.87.87.87.1.m1.1a"><mo id="S4.T2.87.87.87.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.87.87.87.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.87.87.87.1.m1.1b"><times id="S4.T2.87.87.87.1.m1.1.1.cmml" xref="S4.T2.87.87.87.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.87.87.87.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.87.87.87.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.88.88.88.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.88.88.88.2.m1.1"><semantics id="S4.T2.88.88.88.2.m1.1a"><mi id="S4.T2.88.88.88.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.88.88.88.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.88.88.88.2.m1.1b"><ci id="S4.T2.88.88.88.2.m1.1.1.cmml" xref="S4.T2.88.88.88.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.88.88.88.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.88.88.88.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.88.88.88.8">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.88.88.88.9">Structure</td>
<td class="ltx_td ltx_align_center" id="S4.T2.88.88.88.10">1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.88.88.88.11">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.90.90.90">
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.90.90.90.3" rowspan="2"><span class="ltx_text" id="S4.T2.90.90.90.3.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T2.90.90.90.3.1.1" style="width:6.8pt;height:15.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:15.3pt;transform:translate(-4.26pt,-4.26pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T2.90.90.90.3.1.1.1">HG</span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.90.90.90.4">Heterophily</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.90.90.90.5">Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib27" title="">27</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.90.90.90.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.90.90.90.7">ChatGPT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.89.89.89.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.89.89.89.1.m1.1"><semantics id="S4.T2.89.89.89.1.m1.1a"><mo id="S4.T2.89.89.89.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.89.89.89.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.89.89.89.1.m1.1b"><times id="S4.T2.89.89.89.1.m1.1.1.cmml" xref="S4.T2.89.89.89.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.89.89.89.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.89.89.89.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.90.90.90.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.90.90.90.2.m1.1"><semantics id="S4.T2.90.90.90.2.m1.1a"><mi id="S4.T2.90.90.90.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.90.90.90.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.90.90.90.2.m1.1b"><ci id="S4.T2.90.90.90.2.m1.1.1.cmml" xref="S4.T2.90.90.90.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.90.90.90.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.90.90.90.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.90.90.90.8">Citation, E-commerce</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.90.90.90.9">Node</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.90.90.90.10">5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.90.90.90.11"><a class="ltx_ref ltx_href" href="https://github.com/CurryTang/Graph-LLM" title="">link</a></td>
</tr>
<tr class="ltx_tr" id="S4.T2.92.92.92">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.92.92.92.3" rowspan="2"><span class="ltx_text" id="S4.T2.92.92.92.3.1">Generalization</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.92.92.92.4">GraphText <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib29" title="">29</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.92.92.92.5">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.92.92.92.6">ChatGPT, LLaMA2-7B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.91.91.91.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.91.91.91.1.m1.1"><semantics id="S4.T2.91.91.91.1.m1.1a"><mi id="S4.T2.91.91.91.1.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.91.91.91.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.91.91.91.1.m1.1b"><ci id="S4.T2.91.91.91.1.m1.1.1.cmml" xref="S4.T2.91.91.91.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.91.91.91.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.91.91.91.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.92.92.92.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.92.92.92.2.m1.1"><semantics id="S4.T2.92.92.92.2.m1.1a"><mi id="S4.T2.92.92.92.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.92.92.92.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.92.92.92.2.m1.1b"><ci id="S4.T2.92.92.92.2.m1.1.1.cmml" xref="S4.T2.92.92.92.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.92.92.92.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.92.92.92.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.92.92.92.7">Citation, Web</td>
<td class="ltx_td ltx_align_center" id="S4.T2.92.92.92.8">Node</td>
<td class="ltx_td ltx_align_center" id="S4.T2.92.92.92.9">7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.92.92.92.10">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.94.94.94">
<td class="ltx_td ltx_border_bb" id="S4.T2.94.94.94.3"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.94.94.94.4">OpenGraph <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib153" title="">153</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.94.94.94.5">Graph Transformer</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.94.94.94.6">Not mentioned</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.93.93.93.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.93.93.93.1.m1.1"><semantics id="S4.T2.93.93.93.1.m1.1a"><mo id="S4.T2.93.93.93.1.m1.1.1" mathcolor="#FF0000" xref="S4.T2.93.93.93.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.93.93.93.1.m1.1b"><times id="S4.T2.93.93.93.1.m1.1.1.cmml" xref="S4.T2.93.93.93.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.93.93.93.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.93.93.93.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.94.94.94.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.94.94.94.2.m1.1"><semantics id="S4.T2.94.94.94.2.m1.1a"><mi id="S4.T2.94.94.94.2.m1.1.1" mathcolor="#2CA02C" mathvariant="normal" xref="S4.T2.94.94.94.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.94.94.94.2.m1.1b"><ci id="S4.T2.94.94.94.2.m1.1.1.cmml" xref="S4.T2.94.94.94.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.94.94.94.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.94.94.94.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.94.94.94.7">Citation, Drug, etc.</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.94.94.94.8">Node, Edge</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.94.94.94.9">7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.94.94.94.10"><a class="ltx_ref ltx_href" href="https://github.com/HKUDS/OpenGraph" title="">link</a></td>
</tr>
</table>
</span></div>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span><span class="ltx_text ltx_font_italic" id="S4.SS1.1.1">Enhancing Feature Quality</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Graphs encompass diverse attribute information, spanning text, images, audio, and other multi-modal modes. The semantics of these attributes play a crucial role in a range of downstream tasks. In comparison with earlier pre-trained models, LLMs stand out due to their substantial parameter volume and training on extensive datasets, endowing them with rich open-world knowledge. Consequently, researchers are exploring the potential of LLMs to improve feature quality and align feature space. This section delves into research endeavors aimed at leveraging LLMs to accomplish these goals.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Enhancing Feature Representation</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">Researchers utilize the powerful language understanding capabilities of LLMs to generate better representations for text attributes compared to traditional shallow text embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib120" title="">120</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib121" title="">121</a>]</cite>. For example, Patton <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib154" title="">154</a>]</cite> proposes to pre-train a language model on the target graph to obtain high-quality feature representation.
METERN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib155" title="">155</a>]</cite> introduces a soft prompt-based method to learn node multiplex embeddings for different edge types with one language model encoder.
Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib27" title="">27</a>]</cite> utilize LLMs as text encoders and the GNN model as a predictor, validating the effectiveness of LLMs as an enhancer in node classification tasks. In LKPNR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib120" title="">120</a>]</cite>, an LK-Aug news encoder enhances the news recommender system by concatenating LLM embeddings with entity embeddings within the news text to obtain an enriched news representation. Several researchers explore fine-tuning LLMs to obtain text representations better suited for downstream graph tasks. SimTeG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib26" title="">26</a>]</cite> treats node classification and link prediction tasks as text classification and text similarity tasks, fine-tuning PLMs using LoRA  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib156" title="">156</a>]</cite> on the TAG dataset. The fine-tuned PLMs are then used to generate embeddings for text attributes, followed by GNN training for downstream tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Generating Augmented Information</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">Several studies investigate leveraging the generation capabilities and general knowledge of LLMs to generate augmented information from original textual attributes. TAPE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib122" title="">122</a>]</cite> first leverages LLM to generate potential node labels and explanations, utilizing text attributes (such as title and abstract) as input.
These labels and explanations generated by the LLM are regarded as augmented attributes. Subsequently, these augmented attributes are encoded by a fine-tuned language model (LM) and processed by a GNN model, which integrates the graph structure for making final predictions. In contrast to TAPE, KEA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib27" title="">27</a>]</cite> does not directly predict node labels with LLM.
Instead, LLM extracts terms mentioned in textual attributes and provides detailed descriptions of these terms.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1">In the domain of molecular property prediction, both LLM4Mol <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib66" title="">66</a>]</cite> and GPT-MolBERTa <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib126" title="">126</a>]</cite> adopt a similar approach, where LLMs generate interpretations for input Simplified Molecular-Input Line-Entry System (SMILES) notations as augmented attributes.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p3">
<p class="ltx_p" id="S4.SS1.SSS2.p3.1">In the realm of recommender systems, several methods leverage LLMs to enhance the textual attributes of both users and items. LLM-Rec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib125" title="">125</a>]</cite> enables LLMs to produce more detailed item descriptions by explicitly stating the recommendation intent within the prompt. RLMRec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib123" title="">123</a>]</cite> explores using LLM to enhance user preference. Specifically, the LLM receives user and item information as input, generates user preferences, potential types of users that the item may attract, and the reasoning process. LLMRec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib124" title="">124</a>]</cite> employs a similar approach to enhance item and user attributes in recommender systems. For instance, based on historical behavior information, LLM outputs user profiles like age, gender, country, language, and preferred or disliked genres. For item attributes, taking movie information such as title as input, LLM generates outputs such as movie director, country, and language.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p4">
<p class="ltx_p" id="S4.SS1.SSS2.p4.1">In addition to generating augmented text attributes, researchers also employ LLMs to enhance graph topological structures by generating or refining nodes and edges. In ENG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib127" title="">127</a>]</cite>, LLM is employed to generate new nodes and their corresponding text attributes for each node category. To integrate the generated nodes into the original graph, the authors train an edge predictor using relations in the original dataset as supervised signals. Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib128" title="">128</a>]</cite> leverage LLMs to refine graph structures. Specifically, they let LLMs remove unreliable edges by predicting the semantic similarity between node attributes. Additionally, they utilize pseudo-labels generated by LLMs to aid the GNN in learning proper edge weights.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Aligning Feature Space</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">In real-world scenarios, the text attributes of graphs across different domains exhibit considerable diversity. Additionally, beyond text modal attributes, the graph may encompass various other modal attributes. Employing Pretrained Models (PMs) directly for encoding cross-domain and multi-modal features may not produce satisfactory results. Therefore, LLMs are employed to align feature space and provide better representations. TouchUp-G <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib86" title="">86</a>]</cite> introduces a graph-centric fine-tuning strategy aimed at enhancing multi-modal features for graph-related tasks. Initially, they present a novel feature homophily measure to quantify the alignment between node features and the graph structure. Building upon this measure, the authors devise a structure-aware loss function to optimize the PM by minimizing discrepancies between features and graphs. The work of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib129" title="">129</a>]</cite> introduces OFA, a unified framework for classification tasks in graphs across different domains. OFA collects nine text-attributed graph datasets covering diverse domains and represents nodes and relations in natural language. LLMs are then employed to embed those cross-domain graph information into the same embedding space. Moreover, OFA proposes a graph prompting paradigm, which incorporates a prompt graph containing downstream task information into the original input graph, allowing the GNN model to adaptively perform different tasks based on the prompt graph.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span><span class="ltx_text ltx_font_italic" id="S4.SS2.1.1">Solving Vanilla GNN Training Limitations</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The training of vanilla GNNs relies on labeled data. However, obtaining high-quality labeled data has long been associated with substantial time and costs. In contrast to GNNs, LLMs showcase robust zero/few-shot capabilities and possess expansive open-world knowledge. This unique characteristic empowers LLMs to directly leverage node information for prediction, without relying on extensive annotated data. Therefore, researchers have explored employing LLMs to generate annotations or predictions, alleviating dependence on human supervision signals in Graph ML. According to how structural information in graph data is processed, we categorize the methods into the following three categories:</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Ignoring structural information: utilize node attributes exclusively for constructing textual prompts, disregarding neighboring labels and relations.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Implicit Structural information: describe neighbor information and graph topology structure in natural language;</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">Explicit Structural information: employ GNN models to encode graph structure.</p>
</div>
</li>
</ul>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Ignoring Structural Information</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">The fundamental distinction between graphs and text lies in the structural information inherent in graphs. Given that the LLM processes text as its input, an intuitive approach involves leveraging the textual attributes of the target node, disregarding the structural information within the graph, and making predictions directly. For instance, the work of  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib130" title="">130</a>]</cite> explores the effectiveness of LLMs in solving graph tasks without using structure information. In the citation network, they employ the article’s title and abstract to construct a prompt and instruct the LLM to predict the article’s category. Since this kind of paradigm does not incorporate the structural information of the graph, the actual task performed by the LLM is text classification rather than a graph-related task.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="244" id="S4.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The illustration of employing LLMs with implicit and explicit structural information.
(1) Methods leveraging <span class="ltx_text ltx_font_italic" id="S4.F5.3.1">implicit structural information</span> describe nodes and graph structure information in natural language and combine task-specific instructions to form a textual prompt, which is then input into the LLM to generate prediction results.
(2) Methods employing <span class="ltx_text ltx_font_italic" id="S4.F5.4.2">explicit structural information</span> use GNNs and LLMs to encode graph and instruction information separately. Then, fusion layers are added to align the graph and text modalities, and the fused embedding is input into the LLM for prediction. </figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Implicit Structural Information</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">Researchers implicitly leverage structural information to solve graph tasks by describing graph structure in natural language. For example, Hu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib130" title="">130</a>]</cite> propose two kinds of methods for utilizing structural information. The first method involves directly inputting the data of all neighboring nodes into LLM, while the second method employs a retrieval-based prompt to guide the LLM to focus solely on relevant neighbor data. Similarly, Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib137" title="">137</a>]</cite> employ an LLM to assign scores to neighboring nodes and subsequently choose high-scoring nodes as structural information. NLGraph <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib131" title="">131</a>]</cite> introduces a Build-a-Graph prompting strategy to improve the LLM’s understanding of graph structure. This strategy entails appending “Let’s construct a graph with the nodes and edges first.” after providing the graph data description. The work of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib21" title="">21</a>]</cite> introduces InstructGLM, which utilizes natural language for graph description and fine-tunes Flan-T5 through instruction tuning. They generate a set of 31 prompts by combining four configuration parameters: task type, inclusion of node features, maximum hop order, and utilization of node connections. Notably, maximum hop order and node connections implicitly convey graph structure information to the LLM. GraphEdit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib141" title="">141</a>]</cite> leverages LLMs to understand graph structure and refine it by removing noisy edges and uncovering implicit node connections. Specifically, it employs an edge predictor to identify the top k candidate edges for each node, and these candidate edges, along with the original edges of the graph, are then fed into the LLM. The LLM is prompted to determine which edges should be integrated into the final graph structure.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">In addition to employing natural language expression, several researchers leverage structured languages for graph description. GPT4Graph <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib22" title="">22</a>]</cite>, for instance, utilizes Graph Modelling Language <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib157" title="">157</a>]</cite> and Graph Markup Language <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib158" title="">158</a>]</cite> to represent graph structure in XML format. GraphText <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib29" title="">29</a>]</cite> constructs a graph syntax tree for each graph, containing node attributes and relations information. By traversing this tree, structural graph-text sequences can be generated. The advantage of GraphText lies in the ability to integrate the typical inductive bias of GNNs through the construction of various graph syntax trees.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Explicit Structural Information</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">While implicitly describing structure in natural language has achieved preliminary success, these methods still face certain limitations.
Firstly, due to the constraint of input length, LLMs can only get local structural information, and lengthy contexts might diminish their reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib159" title="">159</a>]</cite> and instruction-following abilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib27" title="">27</a>]</cite>.
Secondly, for different tasks and datasets, substantial effort is often required for prompt engineering. A prompt that performs well on one dataset may not generalize effectively to others, resulting in a lack of robustness.
Consequently, researchers investigate representing graph structure explicitly, typically comprising three essential modules: <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p1.1.1">encoding module, fusion module</em>, and <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS3.p1.1.2">LLM module</em>.
More specifically, the encoding module aims to process the graph-structured and textual information, generating graph embeddings and text embeddings, respectively.
Afterward, the fusion module takes these two embeddings as input, producing a modality fusion embedding.
At last, the modality fusion embedding, which contains both graph information and instruction information, is fed into the LLM to obtain the final answer.
Given the research focus is on how LLMs explicitly utilize graph structure information, we will delve into the encoding and fusion modules of various studies in detail, without primarily focusing on the LLM model itself.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p2.1.1">Encoding Module.</span>
The encoding module is responsible for both graph and text encoding, and we will provide separate summaries for each.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS3.p3">
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i1.p1.1.1">Graph Encoding.</span> Pre-trained GNN models are commonly used for graph encoding. For instance, GIT-Mol <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib147" title="">147</a>]</cite> employs the GIN model from the pre-trained MoMu model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib85" title="">85</a>]</cite> to encode molecular graphs. KoPA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib145" title="">145</a>]</cite> utilizes the pre-trained RotateE model to obtain embeddings for entities and relations in the knowledge graph.
In addition, GIMLET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib146" title="">146</a>]</cite> presents a unified graph-text model without the need for additional graph encoding modules. Particularly, GIMLET proposes a distance-based joint position embedding method, where the shortest graph distance is utilized to represent the relative positions between graph nodes, enabling the Transformer encoder to encode both graph and text. GraphToken <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib152" title="">152</a>]</cite> evaluates a series of GNN models as graph encoders, including GCN, MPNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib111" title="">111</a>]</cite>, GIN, Graph Transformer, HGT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib59" title="">59</a>]</cite>, etc.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i2.p1.1.1">Text Encoding.</span> Due to the tremendous capability of LLMs in understanding textual information, most existing methods, such as ProteinChat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib149" title="">149</a>]</cite> and DrugChat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib144" title="">144</a>]</cite>, directly employ LLMs as text encoders. In GraphLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib142" title="">142</a>]</cite>, the tokenizer and frozen embedding table of LLM are leveraged to obtain the representation of node text attributes, aligning with the downstream frozen LLM.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S4.SS2.SSS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p3.1.1">Fusion Module.</span> The goal of the fusion module is to align the graph and text modalities, generating a fusion embedding as input for the LLM.
To achieve the goal, a straightforward solution is to design a linear projection layer to directly transform the graph representation generated by GNN into an LLM-compatible soft prompt vector  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib144" title="">144</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib145" title="">145</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib148" title="">148</a>]</cite>.
Additionally, inspired by BLIP2’s Q-Former <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib160" title="">160</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib147" title="">147</a>]</cite> propose a GIT-Former, which aligns graph, image, and text with the target text modality using self-attention and cross-attention mechanisms.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p4">
<p class="ltx_p" id="S4.SS2.SSS3.p4.1">In addition to the above methods, G-Retriever is proposed to integrate both explicit and implicit structural information  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib151" title="">151</a>]</cite>. To be specific, GAT is employed to encode the graph structure, while representing node and relationship details through textual prompts.
To accommodate real-world graphs with larger scales, G-Retriever introduces a RAG module specifically designed for retrieving subgraphs relevant to user queries.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span><span class="ltx_text ltx_font_italic" id="S4.SS3.1.1">Heterophily and Generalization</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Despite achieving promising performance in graph tasks, GNNs exhibit several shortcomings. A notable drawback involves the inadequacy of the neighbor information aggregation mechanism, especially when dealing with heterogeneous graphs. GNN performance notably diminishes when faced with instances where adjacent nodes lack similarity. Additionally, GNN encounters challenges in out-of-distribution (OOD) generalization, leading to a degradation in model performance on distributions beyond the training data. This challenge is particularly prevalent in practical applications, primarily due to the inherent difficulty of encompassing all possible graph structures within limited training data. Consequently, when GNNs infer on unseen graph structures, their performance may experience a substantial decline. This reduced generalization capability renders GNNs relatively fragile when confronted with evolving graph data in real-world scenarios. For example, GNNs may encounter difficulties handling newly emergent social relationships in social networks.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">LLMs have been utilized to mitigate the above limitations. In particular, GraphText <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib29" title="">29</a>]</cite> effectively decouples depth and scope by encapsulating node attributes and relationships in the graph syntax tree. This approach yields superior results compared to the GNN baseline, particularly on heterogeneous graphs. Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib27" title="">27</a>]</cite> investigate the LLM’s ability to handle OOD generalization scenarios. They utilize the GOOD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib161" title="">161</a>]</cite> benchmark as the criterion and results demonstrate that LLMs exhibit promising performances in addressing OOD generalization issues. OpenGraph <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib153" title="">153</a>]</cite> aims at solving zero-shot graph tasks across different domains. In this model, LLMs are leveraged to generate synthetic graphs of data scarcity scenarios, thereby enhancing the pre-training process of OpenGraph.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Graphs for LLMs</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">LLMs have demonstrated impressive language generation and understanding capabilities across various domains. Nevertheless, they still face several pressing challenges, including factuality awareness, hallucinations, limited explainability in the reasoning process, and beyond.
To alleviate these issues, one potential approach is to take advantage of the <em class="ltx_emph ltx_font_italic" id="S5.p1.1.1">Knowledge Graphs</em> (KGs), which store high-quality, human-curated factual knowledge in a structured format <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib5" title="">5</a>]</cite>.
Recent reviews <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib162" title="">162</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib163" title="">163</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib164" title="">164</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib30" title="">30</a>]</cite> have summarized the research on using KGs to enhance LMs. Hu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib162" title="">162</a>]</cite> present a review on knowledge-enhanced pre-training language models for natural language understanding and natural language generation. Agrawal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib163" title="">163</a>]</cite> systematically review research on mitigating hallucination in LLMs by leveraging KGs across three dimensions: inference process, learning algorithm, and answer validation. Pan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib164" title="">164</a>]</cite> provides a comprehensive summary of the integration of KGs and LLMs from three distinct perspectives: KG-enhanced LLMs, LLM-augmented KGs, and the synergized LLMs and KGs, where LLMs and KGs mutually reinforce each other.
In this section, we will delve into relevant research that explores the usage of KGs to achieve knowledge-enhanced language model pre-training, mitigate hallucinations, and improve inference explainability.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span><span class="ltx_text ltx_font_italic" id="S5.SS1.1.1">KG-enhanced LLM Pre-training</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">While LLMs excel in text understanding and generation, they may still produce grammatically accurate yet factually incorrect information. Explicitly incorporating knowledge from KGs during LLM pre-training holds promise for augmenting LLM’s learning capacity and factual awareness <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib165" title="">165</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib166" title="">166</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib167" title="">167</a>]</cite>. In this subsection, we will outline the research advancements in KG-enhanced pre-trained language models (PLMs). While there is limited work on KG-enhanced pre-training for LLMs, research on KG-enhanced PLMs can offer insights for LLM pretraining. Existing KG-enhanced pre-training methods can be classified into three main categories: modifying input data, modifying model structures, and modifying pre-training tasks.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Modifying Input Data</h4>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.1">Several researchers investigate integrating KG knowledge by modifying input data while keeping the model architecture unchanged. For instance, Moiseev et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib168" title="">168</a>]</cite> directly train PLMs on mixed corpora consisting of factual triples from KGs and natural language texts. E-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib169" title="">169</a>]</cite> aligns entity vectors with BERT’s wordpiece vector space, preserving the structure and refraining from additional pre-training tasks. KALM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib170" title="">170</a>]</cite> utilizes an entity-name dictionary to identify entities within sentences and employs an entity tokenizer to tokenize them. The input of the Transformer consists of the original word embeddings and entity embeddings. Moreover, K-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib171" title="">171</a>]</cite> integrates the original sentence with relevant triples by constructing a sentence tree, where the trunk represents the original sentence and the branches represent the triples. To convert the sentence tree into model input, K-BERT introduces both a hard-position index and a soft-position index within the embedding layer to differentiate between original tokens and triple tokens.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Modifying Model Structures</h4>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.1">Some research designs knowledge-specific encoders or fusion modules to better inject knowledge into PLMs. ERNIE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib172" title="">172</a>]</cite> introduces a K-Encoder to inject knowledge into representations. This involves feeding token embeddings and the concatenation of token embeddings and entity embeddings into a fusion layer for generating new token embeddings and entity embeddings. In contrast, CokeBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib173" title="">173</a>]</cite> extends this approach by incorporating relation information from KGs during pre-training. It introduces a semantic-driven GNN model to assign relevant scores to relations and entities based on the given text. Finally, it fuses the selected relations and entities with text using a K-Encoder similar to ERNIE. KLMO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib174" title="">174</a>]</cite> propose Knowledge Aggregator to fuse text modality and KG modality during pre-training. To incorporate the structural information in KG embeddings, KLMO utilizes KG attention, which integrates a visibility matrix with a conventional attention mechanism, facilitating interaction among adjacent entities and relations within the KG. Subsequently, the token embeddings and contextual KG embeddings are aggregated with entity-level cross-KG attention.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p2">
<p class="ltx_p" id="S5.SS1.SSS2.p2.1">Several studies refrain from modifying the overall structures of the language model but introduce additional adapters to inject knowledge. To preserve the original knowledge within PLMs, Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib175" title="">175</a>]</cite> propose K-Adapter as a pluggable module to leverage KG knowledge. During pre-training, the parameters of the K-Adapter are updated while the parameters of the PLMs remain frozen. KALA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib176" title="">176</a>]</cite> introduces a Knowledge-conditioned Feature Modulation layer, which functions similarly to an adapter module, by scaling and shifting the intermediate hidden representations of PLMs with retrieved knowledge representations. To further control the activation levels of adapters, DAKI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib177" title="">177</a>]</cite> incorporates an attention-based knowledge controller module, which is an adapter module with additional linear layers.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.3 </span>Modifying Pre-training Tasks</h4>
<div class="ltx_para" id="S5.SS1.SSS3.p1">
<p class="ltx_p" id="S5.SS1.SSS3.p1.1">To explicitly model the interactions between text and KG knowledge, various pre-training tasks are proposed. Three major lines of work in this direction include entity-centric tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib172" title="">172</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib178" title="">178</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib179" title="">179</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib180" title="">180</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib181" title="">181</a>]</cite>, relation-centric tasks  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib165" title="">165</a>]</cite>, and beyond.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS3.p2">
<p class="ltx_p" id="S5.SS1.SSS3.p2.1">For entity-centric tasks, ERNIE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib172" title="">172</a>]</cite> randomly masks some token-entity alignments and then requires the model to predict all corresponding entities based on aligned tokens. LUKE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib178" title="">178</a>]</cite> uses Wikipedia articles as training corpora and treats hyperlinks within them as entity annotations, training the model to predict randomly masked entities. KILM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib179" title="">179</a>]</cite> also utilizes hyperlinks in Wikipedia articles as entities. However, it inserts entity descriptions after corresponding entities, tasking the model with reconstructing the masked description tokens rather than directly masking entities. In addition to predicting masked entities, GLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib180" title="">180</a>]</cite> further introduces a distractor-suppressed ranking task. This task leverages negative entity samples from KGs as distractors, enhancing the model’s ability to distinguish various entities.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS3.p3">
<p class="ltx_p" id="S5.SS1.SSS3.p3.1">Relation-centric tasks are also commonly utilized in KG-enhanced PLMs. For instance, JAKET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib182" title="">182</a>]</cite> proposes relation prediction and entity category prediction tasks for enhancing knowledge modeling. Dragon <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib183" title="">183</a>]</cite> is pre-trained in a KG link prediction task. Given a text-KG pair, the model needs to predict the masked relations in KG and the masked tokens in the sentence. ERICA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib184" title="">184</a>]</cite> introduces a relation discrimination task aiming at semantically distinguishing the proximity between two relations. Specifically, it adopts a contrastive learning manner, wherein the relation representations of entity pairs belonging to the same relations are encouraged to be closer.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS3.p4">
<p class="ltx_p" id="S5.SS1.SSS3.p4.1">Additionally, there are several innovative pre-training tasks for KG-enhanced pre-training. KEPLER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib185" title="">185</a>]</cite> proposes a knowledge embedding task to enhance knowledge-awareness of PLMs. Specifically, it uses PLMs to encode entity descriptions as entity embeddings and jointly train the knowledge embedding and masked language modeling tasks on the same PLM. ERNIE 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib186" title="">186</a>]</cite> constructs a series of continuous pre-training tasks from word, structure, and semantic perspectives.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span><span class="ltx_text ltx_font_italic" id="S5.SS2.1.1">KG-enhanced LLM Inference</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Knowledge within KGs can be dynamically updated, whereas updating the knowledge in LLMs often necessitates adjustment of model parameters, which demands substantial computational resources and time. Therefore, many studies opt to utilize KGs during LLMs inference stage. The “black-box” nature of LLMs poses a significant challenge in understanding how the model made a specific prediction or generated a specific text. Additionally, LLMs have often been criticized for generating false, erroneous, or misleading content, typically referred to as hallucination <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib187" title="">187</a>]</cite>. Given the structured and fact-based nature of KGs, integrating them during the inference stage can enhance the explainability of LLM answers and consequently mitigate hallucinations.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">While several methods extract relevant triples from KGs based on user queries and describe these triples in natural language within prompts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib188" title="">188</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib189" title="">189</a>]</cite>, these approaches overlook the structured information inherent in KGs, and still fail to elucidate how LLMs arrive at their answers. Consequently, extensive studies utilize KGs to aid LLMs in reasoning and generate intermediary information like relation paths, evidence subgraphs, and rationales, forming the basis for explaining the LLM’s decision-making process and checking for hallucinations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib190" title="">190</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib191" title="">191</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib192" title="">192</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib38" title="">38</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">Several researchers investigate enabling LLMs to directly reason on KGs and generate relation paths to interpret LLM’s answers. The reasoning path at each step helps to enhance the explainability of the answer and the transparency of the reasoning process. Through observing the reasoning decisions made at each step, it becomes possible to identify and address hallucinations arising from LLMs’ reasoning.
Both RoG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib35" title="">35</a>]</cite>, Knowledge Solver <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib191" title="">191</a>]</cite>, and Keqing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib36" title="">36</a>]</cite> employ relation paths as explanations for LLM’s responses. Specifically, given the KG schema and user query, RoG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib35" title="">35</a>]</cite> guides LLMs to predict multiple relation paths using textual prompts like “Please generate helpful relation paths for answering the question”. Subsequently, LLMs generate the final answer based on the retrieving results of the valid relation path. Conversely, the Knowledge Solver method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib191" title="">191</a>]</cite> differs in that it enables LLMs to generate the relation path step by step. Keqing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib36" title="">36</a>]</cite> initially decomposes complex questions into several sub-questions, each of which can be addressed by pre-defined logical chains on KGs, and then LLMs will generate final answers with relation paths based on the answers of sub-questions.
Mindmap <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib190" title="">190</a>]</cite> uses evident subgraphs to explain the answers generated by LLMs,
where path-based and neighbor-based methods are introduced to obtain several evident subgraphs. The LLM in Mindmap is prompted to merge these evident subgraphs, utilizing the merged graph to generate the final answer. In contrast to previous methods which involve gradually retrieving knowledge and obtaining answers, KGR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib37" title="">37</a>]</cite> takes a different approach. Initially, the LLM directly generates a draft answer. Subsequently, it extracts the claims requiring verification from this answer and retrieves KG’s information to correct claims with hallucinations. Based on the corrected claims, the LLM adjusts the draft answer to get the final answer.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">The above research employs relation paths or evident graphs as the basis for explaining the LLM’s decision-making process and checking hallucinations.
In contrast, several research explore using inherently interpretable models rather than LLMs to make final predictions.
ChatGraph <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib193" title="">193</a>]</cite> presents an innovative approach to enhance both the text classification capabilities and explainability of ChatGPT.
It utilizes ChatGPT to extract triples from unstructured text and subsequently constructs KGs based on these triples. To ensure the explainability of the classified results, ChatGraph avoids employing LLMs directly for predictions. Instead, it leverages a graph model without non-linear activation functions and trains the model on text graphs to get predictions.
Given a question and a list of possible answers, XplainLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib194" title="">194</a>]</cite> propose an explainer model to explain why LLMs choose a particular answer while rejecting others. Specifically, the approach involves constructing an element graph based on the entities present in the question and the candidate answers. Subsequently, a GCN model is employed to assign attention scores to each node within the element graph. Nodes exhibiting high attention scores are identified as reason elements, and LLMs are then prompted to provide explanations based on these selected reason elements.</p>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1">To assess the transparency and interpretability of LLMs, various benchmarks have been proposed.
For example, Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib38" title="">38</a>]</cite> introduce a novel task named Knowledge-aware Language Model Attribution (KaLMA) and develop a corresponding benchmark dataset. This benchmark evaluates the LLM’s capability to derive citation information from KG to support its answers. KaLMA also provides an automatic evaluation covering aspects such as text quality, citation quality, and text-citation alignment of the answers.
In addition, XplainLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib194" title="">194</a>]</cite> introduces a dataset for better understanding LLMs’ decision-making from the perspectives of “why-choose” and “why-not-choose”.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Applications</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this section, we will present practical applications that demonstrate the potential and value of GFMs and LLMs. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#S4.T2" title="TABLE II ‣ 4 LLMs for Graph Models ‣ Graph Machine Learning in the Era of Large Language Models (LLMs)"><span class="ltx_text ltx_ref_tag">II</span></a>, recommender systems, knowledge graphs, AI for science, and robot task planning emerge as the most prevalent domains. We will provide a comprehensive summary of each of these applications.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span><span class="ltx_text ltx_font_italic" id="S6.SS1.1.1">Recommender Systems</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">Recommender systems leverage user historical behaviors to predict items that users are likely to appreciate <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib195" title="">195</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib196" title="">196</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib197" title="">197</a>]</cite>. Graphs play a crucial role in recommender systems, wherein items can be regarded as nodes and collaborative behaviors such as clicks and purchases can be viewed as edges.
Recently, an increasing amount of research is exploring the use of LLMs for direct recommendation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib198" title="">198</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib199" title="">199</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib200" title="">200</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib201" title="">201</a>]</cite> or leveraging LLMs to enhance graph models or datasets for recommendation tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib124" title="">124</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib202" title="">202</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib123" title="">123</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib203" title="">203</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib120" title="">120</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">For directly using LLMs as recommendation models, liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib204" title="">204</a>]</cite> construct task-specific prompts to evaluate ChatGPT’s performance on five common recommendation tasks, encompassing rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Bao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib205" title="">205</a>]</cite> employ prompt templates to guide LLM to decide whether the user will like the target item based on their historical interactions and perform instruction tuning on the LLM to improve its recommendation capability.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1">For using LLMs to enhance traditional recommendation methods or datasets, KAR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib202" title="">202</a>]</cite> leverages LLMs to generate factual knowledge of items and reasoning basis of user preferences; these knowledge texts are then encoded into vectors and integrated into existing recommendation models. Methods like LLM-Rec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib125" title="">125</a>]</cite>, RLMRec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib123" title="">123</a>]</cite>, and LLMRec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib124" title="">124</a>]</cite> enrich recommendation datasets by incorporating LLM-generated descriptions. In contrast, Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib203" title="">203</a>]</cite> utilize LLMs to condense recommendation datasets, in which LLMs are employed to synthesize a condensed dataset for the content-based recommendation, aiming at addressing the challenge of resource-intensive training on large datasets.</p>
</div>
<div class="ltx_para" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1">While the previously discussed methods have explored utilizing LLMs for certain recommendation tasks or domains, an emerging research direction aims to develop foundation models for recommendation. Tang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib199" title="">199</a>]</cite> propose an LLM-based domain-agnostic framework for sequential recommendation. Their approach integrates user behavior across domains, and leverages LLMs to model user behaviors based on multi-domain historical interactions and item titles. Hua et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib206" title="">206</a>]</cite> attempt to address the potential unfairness of recommender systems introduced by LLM bias. They propose a Counterfactually Fair Prompting method to develop an unbiased foundation model for recommendation. To summarize the progress in the area of recommendation foundation model, Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib207" title="">207</a>]</cite> provide a systematic overview of the existing approaches, categorizing them into three main types: language foundation models, personalized agent foundation models, and multi-modal foundation models.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span><span class="ltx_text ltx_font_italic" id="S6.SS2.1.1">Knowledge Graphs</span>
</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">LLMs with robust text generation and language understanding capabilities have found extensive applications in KG-related tasks, including KG completion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib208" title="">208</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib145" title="">145</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib209" title="">209</a>]</cite>, KG question answering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib210" title="">210</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib211" title="">211</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib189" title="">189</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib191" title="">191</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib212" title="">212</a>]</cite>, KG reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib213" title="">213</a>]</cite> and beyond.
Meyer et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib214" title="">214</a>]</cite> introduce LLM-KG-Bench, a framework that automatically evaluates the model’s proficiency in KG engineering tasks such as fixing errors in Turtle files, facts extraction, and dataset generation.
KG-LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib209" title="">209</a>]</cite> is proposed to evaluate LLMs’ performance on KG completion, including triple classification, relation prediction, and link prediction tasks.
Kim et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib210" title="">210</a>]</cite> propose KG-GPT, using LLMs for complex reasoning tasks on knowledge graphs. ChatKBQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib211" title="">211</a>]</cite> introduces a generate-then-retrieve framework for LLMs on knowledge base question answering.
Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib189" title="">189</a>]</cite> present a KG-enhanced LLM framework for KG question answering, which involves fine-tuning an LLM to convert structured triples into free-form text, enhancing LLMs’ understanding of KG data.
The successful application of LLMs in tasks such as KG construction, completion, and question answering offers robust support for advancing the understanding and exploration of KGs.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">Drawing inspiration from foundation models in language and vision, researchers are delving into the development of foundation models tailored for KGs. These GFMs aim to generalize to any unseen relations and entities within KGs. Galkin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib215" title="">215</a>]</cite> propose Ultra, which learns universal graph representations by leveraging interactions between relations. This study is based on the insight that those interactions remain similar and transferable across different datasets.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span><span class="ltx_text ltx_font_italic" id="S6.SS3.1.1">AI for Science</span>
</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">The rapid advancement of AI has led to an increasing number of studies leveraging AI to assist scientific research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib216" title="">216</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib217" title="">217</a>]</cite>.
Recent research has applied LLMs and GFMs
for scientific purposes, such as drug discovery, molecular property prediction, and material design.
Notably, these applications encompass scenarios involving graph-structured data.</p>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">The molecular graph is a way of representing molecules, where the nodes represent atoms, and the edges represent the bonds between the atoms.
With the emergence of LLMs, researchers have explored their performance in tasks related to molecular graphs. Methods like MolReGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib139" title="">139</a>]</cite> and GPT-MolBERTa <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib126" title="">126</a>]</cite> adopt a similar approach, converting molecular graphs into textual descriptions using SMILES language.
They create prompts based on SMILES data, asking the LLM to provide detailed information about functional groups, shapes, chemical properties, etc. This information is then used to train a smaller LM for molecular property prediction.
In contrast to methods directly using LLMs for prediction, ReLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib136" title="">136</a>]</cite> first uses GNNs to predict high-probability candidate products, and then leverages LLMs to make the final selection from these candidates.</p>
</div>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1">In addition to the above research, LLMs are further utilized in drug discovery and materials design. Bran et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib105" title="">105</a>]</cite> present ChemCrow, a chemistry agent integrating LLMs and 18 specialized tools for diverse tasks across drug discovery, materials design, and organic synthesis.
InstructMol <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib218" title="">218</a>]</cite> presents a two-stage framework for aligning language and molecule graph modalities in drug discovery. Initially, the framework maintains the LLM and graph encoder parameters fixed, focusing on training the projector to align molecule-graph representations. Subsequently, instruction tuning is conducted on the LLM to address drug discovery tasks. Zhao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib219" title="">219</a>]</cite> propose ChemDFM, the first dialogue foundation model for chemistry. Trained on extensive chemistry literature and general data, ChemDFM exhibits proficiency in various chemistry tasks such as molecular recognition, molecular design, and beyond.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span><span class="ltx_text ltx_font_italic" id="S6.SS4.1.1">Robot Task Planning</span>
</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">Robot task planning aims to decompose tasks into a series of high-level operations for the step-by-step completion by a robot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib220" title="">220</a>]</cite>. During task execution, the robot needs to perceive information about the surrounding environment, typically represented using <em class="ltx_emph ltx_font_italic" id="S6.SS4.p1.1.1">scene graphs</em>. In a scene graph, nodes represent scene objects like people and tables, while edges describe the spatial or functional relationships between objects.
Enabling LLMs for robot task planning crucially depends on how to represent the environmental information in the scene graph.</p>
</div>
<div class="ltx_para" id="S6.SS4.p2">
<p class="ltx_p" id="S6.SS4.p2.1">Many studies have explored using textual descriptions of scene information and constructing prompts for LLMs to generate task plans. Chalvatzaki et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib221" title="">221</a>]</cite> introduce the Graph2NL mapping table, representing attributes with different numerical ranges using corresponding textual expressions. For instance, a distance value greater than 5 is represented as “distant”, and smaller than 3 is represented as “reachable”.
SayPlan <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib222" title="">222</a>]</cite> describes the scene graph in JSON as a text sequence, iteratively invoking LLM to generate plans and allowing for self-correction. Zhen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib223" title="">223</a>]</cite> propose an effective prompt template, Think_Net_Prompt, to enhance LLM performance in task planning. In contrast to methods that rely on language to describe scene graph information, GRID <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib121" title="">121</a>]</cite> employs the graph transformer to encode scene graphs.
It utilizes cross-modal attention to align the graph modality with user instruction, ultimately outputting action tokens through a decoder layer.
The powerful understanding and reasoning
capabilities of LLMs showcase significant potential in robot task planning. However, as task complexity increases, the search space explosively expands, posing a challenge in efficiently generating viable task plans with LLMs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Future Directions</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this survey, we have thoroughly reviewed the latest developments of Graph Machine Learning in the era of LLMs, revealing significant advancements and potential in this field.
By harnessing the power of LLMs, it is potential to enhance Graph ML to enable GFMs.
As this research direction is still in the exploratory stage, future directions in this field can be diverse and innovative.
Therefore, in this section, we delve into several potential future directions of this promising field.</p>
</div>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span><span class="ltx_text ltx_font_italic" id="S7.SS1.1.1">Generalization and Transferability</span>
</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">While Graph ML has deployed for various graph tasks, a notable problem is their limited capacity for generalization and transferability across different graph domains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib40" title="">40</a>]</cite>.
Different from fields such as NLP and CV, where data often adhere to a uniform format (e.g., a sequence of tokens or a grid of pixels), graphs can be highly heterogeneous in nature.
This heterogeneity manifests in varying graph sizes, densities, and types of nodes and edges, which presents a significant challenge in developing a universal model capable of performing optimally across various graph structure data.
Currently, LLMs have demonstrated great potentials in improving the generalization ability of the graph model. For example, OFA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib129" title="">129</a>]</cite> provides a solution for classification tasks across several certain domains.
Nevertheless, there is still scarce exploration of the generalizability of GFMs compared to LLMs.
Therefore, future research should aim to develop more adaptable and flexible models that can effectively apply learned patterns from one graph type, such as social networks, to another, like molecular structures, without extensive retraining.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span><span class="ltx_text ltx_font_italic" id="S7.SS2.1.1">Multi-modal Graph Learning</span>
</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">Recent LLMs have shown significant potential in advancing GFMs.
Many efforts have been made to transform graph data into formats suitable for LLM input, such as tokens or text <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib84" title="">84</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib131" title="">131</a>]</cite>.
However, many nodes in graphs are enriched with diverse modalities of information, including text, images, and videos.
Understanding this multi-modal data can potentially benefit graph learning.
For example, on social media platforms, a user’s post could include textual content, images, and videos, all of which are valuable for comprehensive user modeling.
Given the importance of multi-modal data, a promising direction for future research is to empower LLMs to process and integrate graph structure with multi-modal data.
Currently, TOUCHUP-G <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib86" title="">86</a>]</cite> makes an initial exploration of multi-modal ( i.e., texts, images) in graph learning.
In the future, we expect the development of a unified model capable of modeling universal modalities for more advanced GFMs.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span><span class="ltx_text ltx_font_italic" id="S7.SS3.1.1">Trustworthiness</span>
</h3>
<div class="ltx_para" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.1">The recent applications of LLMs for Graph ML have significantly enhanced graph modeling capabilities and broadened their utility in various fields.
Despite these advancements, with the growing reliance on these models, it is important to ensure their trustworthiness, particularly in critical areas like healthcare, finance, and social network analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib224" title="">224</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib225" title="">225</a>]</cite>.
Robustness is fundamental in safeguarding the models against adversarial attacks, ensuring consistent reliability.
Explainability is essential for users to understand and trust the decisions made by these models.
Fairness is crucial for the model’s ethical and effective use in various applications.
Privacy is important for legal compliance and key to maintaining user trust.
Therefore, the development of trustworthy LLMs on graphs must be equipped with <em class="ltx_emph ltx_font_italic" id="S7.SS3.p1.1.1">Robustness</em>&amp;<em class="ltx_emph ltx_font_italic" id="S7.SS3.p1.1.2">Safety</em>, <em class="ltx_emph ltx_font_italic" id="S7.SS3.p1.1.3">Explainability</em>, <em class="ltx_emph ltx_font_italic" id="S7.SS3.p1.1.4">Fairness</em>, and <em class="ltx_emph ltx_font_italic" id="S7.SS3.p1.1.5">Privacy</em>, ensuring their safe and effective use in various applications.</p>
</div>
<section class="ltx_subsubsection" id="S7.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.1 </span>Robustness&amp;Safety</h4>
<div class="ltx_para" id="S7.SS3.SSS1.p1">
<p class="ltx_p" id="S7.SS3.SSS1.p1.1">Recently, integrating LLMs into Graph ML has shown promising performance in various downstream tasks, but they are also highly vulnerable to adversarial perturbations, raising significant concerns about their robustness and safety.
To enhance the resilience of these models, some studies add adversarial perturbations to GNNs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib226" title="">226</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib227" title="">227</a>]</cite> or LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib228" title="">228</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib229" title="">229</a>]</cite> for adversarial training.
However, these methods may not be effective for the new paradigm of Graph ML integrating LLMs, as vulnerabilities can arise from both graphs, such as graph poisoning attacks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib230" title="">230</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib231" title="">231</a>]</cite> and graph modification attacks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib232" title="">232</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib233" title="">233</a>]</cite>, and from the language model, like prompt attacks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib234" title="">234</a>]</cite> and misleading text data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib235" title="">235</a>]</cite>. To address these issues, more sophisticated detection and defense mechanisms need to be developed by considering both the intricacies of LLMs and graphs to ensure the comprehensive safety and robustness of Graph ML.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.2 </span>Explainability</h4>
<div class="ltx_para" id="S7.SS3.SSS2.p1">
<p class="ltx_p" id="S7.SS3.SSS2.p1.1">Nowadays, LLMs are increasingly employed in Graph ML across various applications, such as recommender systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib236" title="">236</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib15" title="">15</a>]</cite> and molecular discovery <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib139" title="">139</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib85" title="">85</a>]</cite>. However, due to privacy and security concerns, an application provider may prefer to provide an API version without revealing the architecture and parameters of the LLM, such as with ChatGPT. This lack of transparency can make it challenging for users to understand the model’s results, leading to confusion and dissatisfaction.
Therefore, it’s important to enhance the explainability of Graph ML, especially with LLMs. Owing to their reasoning and interpretive capabilities, LLMs are promising to provide better explainability in graph-related tasks.
For example, P5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib236" title="">236</a>]</cite> can provide reasons for its recommendations in recommendation tasks. Future efforts should be directed toward making the inner workings of these models more transparent and explainable to better comprehend their decision-making processes.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.3 </span>Fairness</h4>
<div class="ltx_para" id="S7.SS3.SSS3.p1">
<p class="ltx_p" id="S7.SS3.SSS3.p1.1">As LLMs become prevalent in enhancing Graph ML towards GFMs, concerns about their fairness are growing. Fairness is crucial to ensure these models operate without biases or discrimination, especially when dealing with complex, interconnected graph data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib225" title="">225</a>]</cite>. Recent studies demonstrate that both language models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib237" title="">237</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib238" title="">238</a>]</cite> and GNN models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib239" title="">239</a>]</cite> can potentially be discriminatory and unfair <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib42" title="">42</a>]</cite>.
Therefore, it is necessary to maintain fairness in both textual and graph contexts.
To enhance the fairness of LLMs, recent studies include retraining strategies that adjust model parameters for unbiased outputs<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib240" title="">240</a>]</cite>, implementing alignment constraints <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib241" title="">241</a>]</cite>, and adopting contrastive learning to diminish bias in model training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib242" title="">242</a>]</cite>.
Concurrently, studies like FairNeg <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib239" title="">239</a>]</cite> also explore improving the fairness of recommendation data.
Despite these efforts, achieving fairness in GFMs is still a significant challenge that needs further exploration.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.4 </span>Privacy</h4>
<div class="ltx_para" id="S7.SS3.SSS4.p1">
<p class="ltx_p" id="S7.SS3.SSS4.p1.1">Privacy is a critical issue in Graph ML, particularly given the risk of these models inadvertently leaking sensitive information contained in graph data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib243" title="">243</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib244" title="">244</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib245" title="">245</a>]</cite>.
For example, Graph ML integrated with LLMs could potentially expose private user data, like browsing histories or social connections when generating outputs.
This concern is especially pressing in highly data-sensitive areas such as healthcare or finance. To mitigate these privacy risks,  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib246" title="">246</a>]</cite> introduces Privacy-Preserving Prompt Tuning (RAPT) to protect user privacy through local differential privacy.
Future exploration in LLM-enhanced Graph ML should also focus on integrating privacy-preserving technologies like differential privacy and federated learning to strengthen data security and user privacy.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S7.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4 </span><span class="ltx_text ltx_font_italic" id="S7.SS4.1.1">Efficiency</span>
</h3>
<div class="ltx_para" id="S7.SS4.p1">
<p class="ltx_p" id="S7.SS4.p1.1">While LLMs have proven effective in constructing GFMs, their operational efficiency, particularly in processing large and complex graphs, is still a significant challenge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib247" title="">247</a>]</cite>.
For example, the use of APIs like GPT4 for large-scale graph tasks can lead to high costs under current billing models.
Additionally, deploying open-source large models (e.g., LLaMa) for parameter updates or just inference in local environments demands substantial computational resources and storage.
Therefore, enhancing the efficiency of LLMs for graph tasks remains a critical issue. Recent studies introduce techniques like LoRA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib156" title="">156</a>]</cite> and QLoRA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib248" title="">248</a>]</cite> to fine-tune LLM parameters more efficiently.
Furthermore, model pruning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib249" title="">249</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14928v2#bib.bib250" title="">250</a>]</cite> is also a promising method to increase efficiency by removing redundant parameters or structures from LLMs, thereby simplifying their application in graph machine learning.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span><span class="ltx_text ltx_font_smallcaps" id="S8.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this survey, we have thoroughly reviewed the recent progress of graph applications and Graph ML in the era of LLMs, an emerging field in graph learning.
We first review the evolution of Graph ML, and then delve into various methods of LLMs enhancing Graph ML.
Due to the remarkable capabilities in various fields, LLMs have great potential to enhance Graph ML towards GFMs.
We further explore the augmenting of LLMs with graphs, highlighting their ability to enhance LLM pre-training and inference.
Additionally, we demonstrate their potential in diverse applications such as molecule discovery, knowledge graphs, and recommender systems.
Despite their success, this field is still evolving and presents numerous opportunities for further advancements.
Therefore, we further discuss several challenges and potential future directions.
Overall, our survey aims to provide a systematic and comprehensive review to researchers and practitioners, inspiring future explorations in this promising field.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
H. Mao, J. Li, H. Shomer, B. Li, W. Fan, Y. Ma, T. Zhao, N. Shah, and J. Tang, “Revisiting link prediction: A data perspective,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2310.00793</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. Hashemi, S. Gong, J. Ni, W. Fan, B. A. Prakash, and W. Jin, “A comprehensive survey on graph reduction: Sparsification, coarsening, and condensation,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2402.03358</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
W. Fan, X. Zhao, Q. Li, T. Derr, Y. Ma, H. Liu, J. Wang, and J. Tang, “Adversarial attacks for black-box recommender systems via copying transferable cross-domain user profiles,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">IEEE Transactions on Knowledge and Data Engineering</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J. Wu, W. Fan, J. Chen, S. Liu, Q. Li, and K. Tang, “Disentangled contrastive learning for social recommendation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</em>, 2022, pp. 4570–4574.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Chen, W. Fan, G. Zhu, X. Zhao, C. Yuan, Q. Li, and Y. Huang, “Knowledge-enhanced black-box attacks for recommendations,” in <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>, 2022, pp. 108–117.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
T. Derr, Y. Ma, W. Fan, X. Liu, C. Aggarwal, and J. Tang, “Epidemic graph convolutional network,” in <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 13th International Conference on Web Search and Data Mining (WSDM)</em>, 2020, pp. 160–168.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y. Ma and J. Tang, <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Deep learning on graphs</em>.   Cambridge University Press, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, “A comprehensive survey on graph neural networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">IEEE transactions on neural networks and learning systems</em>, vol. 32, no. 1, pp. 4–24, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Zhang, R. Xue, W. Fan, X. Xu, Q. Li, J. Pei, and X. Liu, “Linear-time graph neural networks for scalable recommendations,” <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2402.13973</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y. You, T. Chen, Y. Sui, T. Chen, Z. Wang, and Y. Shen, “Graph contrastive learning with augmentations,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Advances in neural information processing systems</em>, vol. 33, pp. 5812–5823, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J. Zeng and P. Xie, “Contrastive self-supervised learning for graph classification,” in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the AAAI conference on Artificial Intelligence</em>, vol. 35, no. 12, 2021, pp. 10 824–10 832.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
X. Xu, F. Zhou, K. Zhang, and S. Liu, “Ccgl: Contrastive cascade graph learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">IEEE Transactions on Knowledge and Data Engineering</em>, vol. 35, no. 5, pp. 4539–4554, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. Qiu, Q. Chen, Y. Dong, J. Zhang, H. Yang, M. Ding, K. Wang, and J. Tang, “Gcc: Graph contrastive coding for graph neural network pre-training,” in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>, 2020, pp. 1150–1160.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A. Radford, K. Narasimhan, T. Salimans, I. Sutskever <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">et al.</em>, “Improving language understanding by generative pre-training,” 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:1810.04805</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
L. Zhou, H. Palangi, L. Zhang, H. Hu, J. Corso, and J. Gao, “Unified vision-language pre-training for image captioning and vqa,” in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the AAAI conference on artificial intelligence</em>, vol. 34, no. 07, 2020, pp. 13 041–13 049.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
W. Fan, Z. Zhao, J. Li, Y. Liu, X. Mei, Y. Wang, Z. Wen, F. Wang, X. Zhao, J. Tang, and Q. Li, “Recommender systems in the era of large language models (llms),” Aug. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Y. Li, P. Wang, Z. Li, J. X. Yu, and J. Li, “Zerog: Investigating cross-dataset zero-shot transferability in graphs,” <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2402.11235</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
H. Mao, Z. Chen, W. Tang, J. Zhao, Y. Ma, T. Zhao, N. Shah, M. Galkin, and J. Tang, “Graph foundation models,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2402.02216</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">et al.</em>, “On the opportunities and risks of foundation models,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.2.2">arXiv preprint arXiv:2108.07258</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
R. Ye, C. Zhang, R. Wang, S. Xu, and Y. Zhang, “Natural language is all a graph needs,” Aug. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J. Guo, L. Du, H. Liu, M. Zhou, X. He, and S. Han, “Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking,” Jul. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J. Tang, Y. Yang, W. Wei, L. Shi, L. Su, S. Cheng, D. Yin, and C. Huang, “Graphgpt: Graph instruction tuning for large language models,” Oct. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">et al.</em>, “Llama: Open and efficient foundation language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib24.2.2">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a unified text-to-text transformer,” <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">The Journal of Machine Learning Research</em>, vol. 21, no. 1, pp. 5485–5551, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
K. Duan, Q. Liu, T.-S. Chua, S. Yan, W. T. Ooi, Q. Xie, and J. He, “Simteg: A frustratingly simple approach improves textual graph learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2308.02565</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Z. Chen, H. Mao, H. Li, W. Jin, H. Wen, X. Wei, S. Wang, D. Yin, W. Fan, H. Liu, and J. Tang, “Exploring the potential of large language models (llms) in learning on graphs,” Aug. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
E. Chien, W.-C. Chang, C.-J. Hsieh, H.-F. Yu, J. Zhang, O. Milenkovic, and I. S. Dhillon, “Node feature extraction by self-supervised multi-scale neighborhood prediction,” <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2111.00064</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J. Zhao, L. Zhuo, Y. Shen, M. Qu, K. Liu, M. Bronstein, Z. Zhu, and J. Tang, “Graphtext: Graph reasoning in text space,” Oct. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Y. Ding, W. Fan, L. Ning, S. Wang, H. Li, D. Yin, T.-S. Chua, and Q. Li, “A survey on rag meets llms: Towards retrieval-augmented large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2405.06211</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
R. L. Logan IV, N. F. Liu, M. E. Peters, M. Gardner, and S. Singh, “Barack’s wife hillary: Using knowledge-graphs for fact-aware language modeling,” <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:1906.07241</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">et al.</em>, “Sparks of artificial general intelligence: Early experiments with gpt-4,” <em class="ltx_emph ltx_font_italic" id="bib.bib32.2.2">arXiv preprint arXiv:2303.12712</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
H. Zhao, H. Chen, F. Yang, N. Liu, H. Deng, H. Cai, S. Wang, D. Yin, and M. Du, “Explainability for large language models: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">ACM Transactions on Intelligent Systems and Technology</em>, vol. 15, no. 2, pp. 1–38, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
S. Xiong, A. Payani, R. Kompella, and F. Fekri, “Large language models can learn temporal reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2401.06853</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
L. Luo, Y.-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faithful and interpretable large language model reasoning,” Oct. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
C. Wang, Y. Xu, Z. Peng, C. Zhang, B. Chen, X. Wang, L. Feng, and B. An, “keqing: knowledge-based question answering is a nature chain-of-thought mentor of llm,” <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2401.00426</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
X. Guan, Y. Liu, H. Lin, Y. Lu, B. He, X. Han, and L. Sun, “Mitigating large language model hallucinations via autonomous knowledge graph-based retrofitting,” <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2311.13314</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
X. Li, Y. Cao2, L. Pan, Y. Ma, and A. Sun, “Towards verifiable generation: A benchmark for knowledge-aware language model attribution,” Oct. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
S. Wei, Y. Zhao, X. Chen, Q. Li, F. Zhuang, J. Liu, F. Ren, and G. Kou, “Graph learning and its advancements on large language models: A holistic survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2212.08966</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Z. Zhang, H. Li, Z. Zhang, Y. Qin, X. Wang, and W. Zhu, “Large graph models: A perspective,” Aug. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
B. Jin, G. Liu, C. Han, M. Jiang, H. Ji, and J. Han, “Large language models on graphs: A comprehensive survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2312.02783</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Y. Li, Z. Li, P. Wang, J. Li, X. Sun, H. Cheng, and J. X. Yu, “A survey of graph meets large language model: Progress and future directions,” <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2311.12399</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
J. Liu, C. Yang, Z. Lu, J. Chen, Y. Li, M. Zhang, T. Bai, Y. Fang, L. Sun, P. S. Yu <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">et al.</em>, “Towards graph foundation models: A survey and beyond,” <em class="ltx_emph ltx_font_italic" id="bib.bib43.2.2">arXiv preprint arXiv:2310.11829</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
L. Wang, W. Fan, J. Li, Y. Ma, and Q. Li, “Fast graph condensation with structure-based neural tangent kernel,” <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2310.11046</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
W. Fan, S. Wang, X.-y. Wei, X. Mei, and Q. Li, “Untargeted black-box attacks for social recommendations,” <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2311.07127</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
M. Tsubaki, K. Tomii, and J. Sese, “Compound–protein interaction prediction with end-to-end learning of neural networks for graphs and sequences,” <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Bioinformatics</em>, vol. 35, no. 2, pp. 309–318, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
W. Fan, Y. Ma, Q. Li, Y. He, E. Zhao, J. Tang, and D. Yin, “Graph neural networks for social recommendation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">The world wide web conference</em>, 2019, pp. 417–426.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
W. Fan, Y. Ma, Q. Li, J. Wang, G. Cai, J. Tang, and D. Yin, “A graph neural network framework for social recommendations,” <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">IEEE Transactions on Knowledge and Data Engineering</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “Line: Large-scale information network embedding,” in <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the 24th international conference on world wide web</em>, 2015, pp. 1067–1077.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning of social representations,” in <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</em>, 2014, pp. 701–710.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</em>, 2016, pp. 855–864.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
T. N. Kipf and M. Welling, “Semi-supervised classification with graph convolutional networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:1609.02907</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
W. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Advances in neural information processing systems</em>, vol. 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio, “Graph attention networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:1710.10903</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Advances in neural information processing systems</em>, vol. 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Y. Li, X. Liang, Z. Hu, Y. Chen, and E. P. Xing, “Graph transformer,” 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
S. Yun, M. Jeong, R. Kim, J. Kang, and H. J. Kim, “Graph transformer networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Advances in neural information processing systems</em>, vol. 32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
J. Baek, M. Kang, and S. J. Hwang, “Accurate learning of graph representations with graph multiset pooling,” in <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">ICLR</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Z. Hu, Y. Dong, K. Wang, and Y. Sun, “Heterogeneous graph transformer,” in <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the web conference 2020</em>, 2020, pp. 2704–2710.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
J. Zhang, H. Zhang, C. Xia, and L. Sun, “Graph-bert: Only attention is needed for learning graph representations,” <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">arXiv preprint arXiv:2001.05140</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
J. Yang, Z. Liu, S. Xiao, C. Li, D. Lian, S. Agrawal, A. Singh, G. Sun, and X. Xie, “Graphformers: Gnn-nested transformers for representation learning on textual graph,” <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Advances in Neural Information Processing Systems</em>, vol. 34, pp. 28 798–28 810, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, and T.-Y. Liu, “Do transformers really perform badly for graph representation?” <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Advances in Neural Information Processing Systems</em>, vol. 34, pp. 28 877–28 888, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
S. Mitheran, A. Java, S. K. Sahu, and A. Shaikh, “Introducing self-attention to target attentive graph neural networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2107.01516</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
D. Kreuzer, D. Beaini, W. Hamilton, V. Létourneau, and P. Tossou, “Rethinking graph transformers with spectral attention,” <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Advances in Neural Information Processing Systems</em>, vol. 34, pp. 21 618–21 629, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
V. P. Dwivedi and X. Bresson, “A generalization of transformer networks to graphs,” <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2012.09699</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
C. Qian, H. Tang, Z. Yang, H. Liang, and Y. Liu, “Can large language models empower molecular property prediction?” Jul. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
N. Chen, Y. Li, J. Tang, and J. Li, “Graphwiz: An instruction-following language model for graph problems,” <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">arXiv preprint arXiv:2402.16029</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">et al.</em>, “Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality,” <em class="ltx_emph ltx_font_italic" id="bib.bib68.2.2">See https://vicuna. lmsys. org (accessed 14 April 2023)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
J. Zhao, M. Qu, C. Li, H. Yan, Q. Liu, R. Li, X. Xie, and J. Tang, “Learning on large-scale text-attributed graphs via variational inference,” <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2210.14709</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
P. He, X. Liu, J. Gao, and W. Chen, “Deberta: Decoding-enhanced bert with disentangled attention,” <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">arXiv preprint arXiv:2006.03654</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">et al.</em>, “Learning transferable visual models from natural language supervision,” in <em class="ltx_emph ltx_font_italic" id="bib.bib71.2.2">ICML</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, and N. Duan, “Visual chatgpt: Talking, drawing and editing with visual foundation models,” <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">arXiv preprint arXiv:2303.04671</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever, “Zero-shot text-to-image generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">ICML</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Y. Ding, Y. Ma, W. Fan, Y. Yao, T.-S. Chua, and Q. Li, “Fashionregen: Llm-empowered fashion report generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">arXiv preprint arXiv:2403.06660</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Y. Yang, S. Xiong, A. Payani, E. Shareghi, and F. Fekri, “Harnessing the power of large language models for natural language to first-order logic translation,” <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">arXiv preprint arXiv:2305.15541</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
P. Veličković, W. Fedus, W. L. Hamilton, P. Liò, Y. Bengio, and R. D. Hjelm, “Deep graph infomax,” <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">arXiv preprint arXiv:1809.10341</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Y. Zhu, Y. Xu, F. Yu, Q. Liu, S. Wu, and L. Wang, “Deep graph contrastive representation learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">arXiv preprint arXiv:2006.04131</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Z. Hou, X. Liu, Y. Cen, Y. Dong, H. Yang, C. Wang, and J. Tang, “Graphmae: Self-supervised masked graph autoencoders,” in <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>, 2022, pp. 594–604.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
K. Hassani and A. H. Khasahmadi, “Contrastive multi-view representation learning on graphs,” in <em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">International conference on machine learning</em>.   PMLR, 2020, pp. 4116–4126.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
J. Shang, T. Ma, C. Xiao, and J. Sun, “Pre-training of graph augmented transformers for medication recommendation,” <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">arXiv preprint arXiv:1906.00346</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
S. Li, X. Han, and J. Bai, “Adaptergnn: Efficient delta tuning improves generalization ability in graph neural networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">arXiv preprint arXiv:2304.09595</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Y. Rong, Y. Bian, T. Xu, W. Xie, Y. Wei, W. Huang, and J. Huang, “Self-supervised graph transformer on large-scale molecular data,” <em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">Advances in Neural Information Processing Systems</em>, vol. 33, pp. 12 559–12 571, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
A. Gui, J. Ye, and H. Xiao, “G-adapter: Towards structure-aware parameter-efficient transfer learning for graph transformer networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">arXiv preprint arXiv:2305.10329</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Q. Zhao, W. Ren, T. Li, X. Xu, and H. Liu, “Graphgpt: Graph learning with generative pre-trained transformers,” <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">arXiv preprint arXiv:2401.00529</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
B. Su, D. Du, Z. Yang, Y. Zhou, J. Li, A. Rao, H. Sun, Z. Lu, and J.-R. Wen, “A molecular multimodal foundation model associating molecule graphs with natural language,” <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">arXiv preprint arXiv:2209.05481</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
J. Zhu, X. Song, V. N. Ioannidis, D. Koutra, and C. Faloutsos, “Touchup-g: Improving feature representation through graph-centric finetuning,” <em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">arXiv preprint arXiv:2309.13885</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Z. Liu, X. Yu, Y. Fang, and X. Zhang, “Graphprompt: Unifying pre-training and downstream tasks for graph neural networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">Proceedings of the ACM Web Conference 2023</em>, 2023, pp. 417–428.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
M. Sun, K. Zhou, X. He, Y. Wang, and X. Wang, “Gppt: Graph pre-training and prompt tuning to generalize graph neural networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>, 2022, pp. 1717–1727.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
C. Gong, X. Li, J. Yu, C. Yao, J. Tan, C. Yu, and D. Yin, “Prompt tuning for multi-view graph contrastive learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">arXiv preprint arXiv:2310.10362</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
T. Fang, Y. M. Zhang, Y. Yang, C. Wang, and C. Lei, “Universal prompt tuning for graph neural networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
X. Sun, H. Cheng, J. Li, B. Liu, and J. Guan, “All in one: Multi-task prompting for graph neural networks,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
M. Chen, Z. Liu, C. Liu, J. Li, Q. Mao, and J. Sun, “Ultra-dp: Unifying graph pre-training with multi-task graph dual prompt,” <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">arXiv preprint arXiv:2310.14845</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Q. Ge, Z. Zhao, Y. Liu, A. Cheng, X. Li, S. Wang, and D. Yin, “Enhancing graph neural networks with structure-based prompt,” <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">arXiv preprint arXiv:2310.17394</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Q. Huang, H. Ren, P. Chen, G. Kržmanc, D. Zeng, P. Liang, and J. Leskovec, “Prodigy: Enabling in-context learning over graphs,” <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">arXiv preprint arXiv:2305.12600</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Y. Zhu, J. Guo, and S. Tang, “Sgl-pt: A strong graph learner with graph prompt tuning,” <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">arXiv preprint arXiv:2302.12449</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
R. Shirkavand and H. Huang, “Deep prompt tuning for graph transformers,” <em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">arXiv preprint arXiv:2309.10131</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell <em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">et al.</em>, “Language models are few-shot learners,” <em class="ltx_emph ltx_font_italic" id="bib.bib97.2.2">NeurIPS</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">et al.</em>, “Lamda: Language models for dialog applications,” <em class="ltx_emph ltx_font_italic" id="bib.bib98.2.2">arXiv preprint arXiv:2201.08239</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann <em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">et al.</em>, “Palm: Scaling language modeling with pathways,” <em class="ltx_emph ltx_font_italic" id="bib.bib99.2.2">arXiv preprint arXiv:2204.02311</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
N. Shinn, F. Cassano, A. Gopinath, K. R. Narasimhan, and S. Yao, “Reflexion: Language agents with verbal reinforcement learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
J. Zhang, “Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt,” <em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">arXiv preprint arXiv:2304.11116</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
C. Mavromatis, V. N. Ioannidis, S. Wang, D. Zheng, S. Adeshina, J. Ma, H. Zhao, C. Faloutsos, and G. Karypis, “Train your own gnn teacher: Graph-aware distillation on textual graphs,” <em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">arXiv preprint arXiv:2304.10668</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
P. Jiang, J. Rayan, S. P. Dow, and H. Xia, “Graphologue: Exploring large language model responses with interactive diagrams,” <em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">arXiv preprint arXiv:2305.11473</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
C. M. Castro Nascimento and A. S. Pimentel, “Do large language models understand chemistry? a conversation with chatgpt,” <em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">Journal of Chemical Information and Modeling</em>, vol. 63, no. 6, pp. 1649–1655, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
A. M. Bran, S. Cox, A. D. White, and P. Schwaller, “Chemcrow: Augmenting large-language models with chemistry tools,” <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">arXiv preprint arXiv:2304.05376</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
E. Kasneci, K. Seßler, S. Küchemann, M. Bannert, D. Dementieva, F. Fischer, U. Gasser, G. Groh, S. Günnemann, E. Hüllermeier <em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">et al.</em>, “Chatgpt for good? on opportunities and challenges of large language models for education,” <em class="ltx_emph ltx_font_italic" id="bib.bib106.2.2">Learning and Individual Differences</em>, vol. 103, p. 102274, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
A. Gilson, C. W. Safranek, T. Huang, V. Socrates, L. Chi, R. A. Taylor, D. Chartash <em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">et al.</em>, “How does chatgpt perform on the united states medical licensing examination? the implications of large language models for medical education and knowledge assessment,” <em class="ltx_emph ltx_font_italic" id="bib.bib107.2.2">JMIR Medical Education</em>, vol. 9, no. 1, p. e45312, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann, P. Kambadur, D. Rosenberg, and G. Mann, “Bloomberggpt: A large language model for finance,” <em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">arXiv preprint arXiv:2303.17564</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
Y. Yang, M. C. S. Uy, and A. Huang, “Finbert: A pretrained language model for financial communications,” <em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">arXiv preprint arXiv:2006.08097</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
B. Jin, C. Xie, J. Zhang, K. K. Roy, Y. Zhang, S. Wang, Y. Meng, and J. Han, “Graph chain-of-thought: Augmenting large language models by reasoning on graphs,” <em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">arXiv preprint arXiv:2404.07103</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl, “Neural message passing for quantum chemistry,” in <em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">International conference on machine learning</em>.   PMLR, 2017, pp. 1263–1272.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and J. Leskovec, “Graph convolutional neural networks for web-scale recommender systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>, 2018, pp. 974–983.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph neural networks?” <em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">arXiv preprint arXiv:1810.00826</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
H. Maron, H. Ben-Hamu, H. Serviansky, and Y. Lipman, “Provably powerful graph networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib114.1.1">Advances in neural information processing systems</em>, vol. 32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
C. Morris, M. Ritzert, M. Fey, W. L. Hamilton, J. E. Lenssen, G. Rattan, and M. Grohe, “Weisfeiler and leman go neural: Higher-order graph neural networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib115.1.1">Proceedings of the AAAI conference on artificial intelligence</em>, vol. 33, no. 01, 2019, pp. 4602–4609.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
P. Li, J. Wang, Y. Qiao, H. Chen, Y. Yu, X. Yao, P. Gao, G. Xie, and S. Song, “An effective self-supervised framework for learning expressive molecular global representations to drug discovery,” <em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">Briefings in Bioinformatics</em>, vol. 22, no. 6, p. bbab109, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
B. Jin, Y. Zhang, Q. Zhu, and J. Han, “Heterformer: Transformer-based deep node representation learning on heterogeneous text-rich networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>, 2023, pp. 1020–1031.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
B. Jin, Y. Zhang, Y. Meng, and J. Han, “Edgeformers: Graph-empowered transformers for representation learning on textual-edge networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib118.1.1">arXiv preprint arXiv:2302.11050</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
Z. Hu, C. Fan, T. Chen, K.-W. Chang, and Y. Sun, “Pre-training graph neural networks for generic structural feature extraction,” <em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">arXiv preprint arXiv:1905.13728</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
C. hao, X. Runfeng, C. Xiangyang, Y. Zhou, W. Xin, X. Zhanwei, and Z. Kai, “Lkpnr: Llm and kg for personalized news recommendation framework,” Aug. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
Z. Ni, X.-X. Deng, C. Tai, X.-Y. Zhu, X. Wu, Y.-J. Liu, and L. Zeng, “Grid: Scene-graph-based instruction-driven robotic task planning,” Sep. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
X. He, X. Bresson, T. Laurent, A. Perold, Y. LeCun, and B. Hooi, “Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation learning,” Oct. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
X. Ren, W. Wei, L. Xia, L. Su, S. Cheng, J. Wang, D. Yin, and C. Huang, “Representation learning with large language models for recommendation,” Oct. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
W. Wei, X. Ren, J. Tang, Q. Wang, L. Su, S. Cheng, J. Wang, D. Yin, and C. Huang, “Llmrec: Large language models with graph augmentation for recommendation.”

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
H. Lyu, S. Jiang, H. Zeng, Y. Xia, and J. Luo, “Llm-rec: Personalized recommendation via prompting large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib125.1.1">arXiv preprint arXiv:2307.15780</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
S. Balaji, R. Magar, Y. Jadhav, and A. B. Farimani, “Gpt-molberta: Gpt molecular features language model for molecular property prediction,” Oct. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
J. Yu, Y. Ren, C. Gong, J. Tan, X. Li, and X. Zhang, “Empower text-attributed graphs learning with large language models (llms),” Oct. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
S. Sun, Y. Ren, C. Ma, and X. Zhang, “Large language models as topological structure enhancers for text-attributed graphs,” <em class="ltx_emph ltx_font_italic" id="bib.bib128.1.1">arXiv preprint arXiv:2311.14324</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
H. Liu, J. Feng, L. Kong, N. Liang, D. Tao, Y. Chen, and M. Zhang, “One for all: Towards training one graph model for all classification tasks,” <em class="ltx_emph ltx_font_italic" id="bib.bib129.1.1">arXiv preprint arXiv:2310.00149</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
Y. Hu, Z. Zhang, and L. Zhao, “Beyond text: A deep dive into large language models’ ability on understanding graph data,” Oct. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
H. Wang, S. Feng, T. He, Z. Tan, X. Han, and Y. Tsvetkov, “Can language models solve graph problems in natural language?” <em class="ltx_emph ltx_font_italic" id="bib.bib131.1.1">arXiv preprint arXiv:2305.10037</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
C. Liu and B. Wu, “Evaluating large language models on graphs: Performance insights and comparative analysis,” Sep. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
Q. Wang, Z. Gao, and R. Xu, “Graph agent: Explicit reasoning agent for graphs,” <em class="ltx_emph ltx_font_italic" id="bib.bib133.1.1">arXiv preprint arXiv:2310.16421</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
A. N. Rubungo, C. Arnold, B. P. Rand, and A. B. Dieng, “Llm-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions,” Oct. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
L. Wu, Z. Qiu, Z. Zheng, H. Zhu, and E. Chen, “Exploring large language model for graph data understanding in online job recommendations,” <em class="ltx_emph ltx_font_italic" id="bib.bib135.1.1">arXiv preprint arXiv:2307.05722</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
Y. Shi, A. Zhang, E. Zhang, Z. Liu, and X. Wang, “Relm: Leveraging language models for enhanced chemical reaction prediction,” <em class="ltx_emph ltx_font_italic" id="bib.bib136.1.1">arXiv preprint arXiv:2310.13590</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
J. Huang, X. Zhang, Q. Mei, and J. Ma, “Can llms effectively leverage graph structural information: When and why,” Sep. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
B. Fatemi, J. Halcrow, and B. Perozzi, “Talk like a graph: Encoding graphs for large language models,” Oct. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
J. Li, Y. Liu, W. Fan, X.-Y. Wei, H. Liu, J. Tang, and Q. Li, “Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective,” <em class="ltx_emph ltx_font_italic" id="bib.bib139.1.1">arXiv preprint arXiv:2306.06615</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
R. Chen, T. Zhao, A. Jaiswal, N. Shah, and Z. Wang, “Llaga: Large language and graph assistant,” <em class="ltx_emph ltx_font_italic" id="bib.bib140.1.1">arXiv preprint arXiv:2402.08170</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
Z. Guo, L. Xia, Y. Yu, Y. Wang, Z. Yang, W. Wei, L. Pang, T.-S. Chua, and C. Huang, “Graphedit: Large language models for graph structure learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib141.1.1">arXiv preprint arXiv:2402.15183</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
Z. Chai, T. Zhang, L. Wu, K. Han, X. Hu, X. Huang, and Y. Yang, “Graphllm: Boosting graph reasoning ability of large language model,” Oct. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
Y. Tian, H. Song, Z. Wang, H. Wang, Z. Hu, F. Wang, N. V. Chawla, and P. Xu, “Graph neural prompting with large language models,” Sep. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
Y. Liang, R. Zhang, L. Zhang, and P. Xie, “Drugchat: Towards enabling chatgpt-like capabilities on drug molecule graphs,” May 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
Y. Zhang, Z. Chen, W. Zhang, and H. Chen, “Making large language models perform better in knowledge graph completion,” Oct. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
H. Zhao, S. Liu, C. Ma, H. Xu, J. Fu, Z.-H. Deng, L. Kong, and Q. Liu, “Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning,” Bioinformatics, Preprint, Jun. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
P. Liu, Y. Ren, and Z. Ren, “Git-mol: A multi-modal large language model for molecular science with graph, image, and text,” Aug. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
Y. Luo, J. Zhang, S. Fan, K. Yang, Y. Wu, M. Qiao, and Z. Nie, “Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine,” <em class="ltx_emph ltx_font_italic" id="bib.bib148.1.1">arXiv preprint arXiv:2308.09442</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
H. Guo, M. Huo, R. Zhang, and P. Xie, “Proteinchat: Towards achieving chatgpt-like functionalities on protein 3d structures,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
Y. Qin, X. Wang, Z. Zhang, and W. Zhu, “Disentangled representation learning with large language models for text-attributed graphs,” <em class="ltx_emph ltx_font_italic" id="bib.bib150.1.1">arXiv preprint arXiv:2310.18152</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
X. He, Y. Tian, Y. Sun, N. V. Chawla, T. Laurent, Y. LeCun, X. Bresson, and B. Hooi, “G-retriever: Retrieval-augmented generation for textual graph understanding and question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib151.1.1">arXiv preprint arXiv:2402.07630</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
B. Perozzi, B. Fatemi, D. Zelle, A. Tsitsulin, M. Kazemi, R. Al-Rfou, and J. Halcrow, “Let your graph do the talking: Encoding structured data for llms,” <em class="ltx_emph ltx_font_italic" id="bib.bib152.1.1">arXiv preprint arXiv:2402.05862</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
L. Xia, B. Kao, and C. Huang, “Opengraph: Towards open graph foundation models,” <em class="ltx_emph ltx_font_italic" id="bib.bib153.1.1">arXiv preprint arXiv:2403.01121</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
B. Jin, W. Zhang, Y. Zhang, Y. Meng, X. Zhang, Q. Zhu, and J. Han, “Patton: Language model pretraining on text-rich networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib154.1.1">arXiv preprint arXiv:2305.12268</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
B. Jin, W. Zhang, Y. Zhang, Y. Meng, H. Zhao, and J. Han, “Learning multiplex embeddings on text-rich networks with one text encoder,” <em class="ltx_emph ltx_font_italic" id="bib.bib155.1.1">arXiv preprint arXiv:2310.06684</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, “Lora: Low-rank adaptation of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib156.1.1">arXiv preprint arXiv:2106.09685</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
M. Himsolt, “Gml: Graph modelling language,” <em class="ltx_emph ltx_font_italic" id="bib.bib157.1.1">University of Passau</em>, 1997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
U. Brandes, M. Eiglsperger, J. Lerner, and C. Pich, “Graph markup language (graphml),” 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang, “Lost in the middle: How language models use long contexts,” <em class="ltx_emph ltx_font_italic" id="bib.bib159.1.1">arXiv preprint arXiv:2307.03172</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib160.1.1">arXiv preprint arXiv:2301.12597</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
S. Gui, X. Li, L. Wang, and S. Ji, “Good: A graph out-of-distribution benchmark,” <em class="ltx_emph ltx_font_italic" id="bib.bib161.1.1">Advances in Neural Information Processing Systems</em>, vol. 35, pp. 2059–2073, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
L. Hu, Z. Liu, Z. Zhao, L. Hou, L. Nie, and J. Li, “A survey of knowledge enhanced pre-trained language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib162.1.1">IEEE Transactions on Knowledge and Data Engineering</em>, pp. 1–19, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
G. Agrawal, T. Kumarage, Z. Alghami, and H. Liu, “Can knowledge graphs reduce hallucinations in llms?: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib163.1.1">arXiv preprint arXiv:2311.07914</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu, “Unifying large language models and knowledge graphs: A roadmap,” Jun. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen, Y. Zhao, Y. Lu <em class="ltx_emph ltx_font_italic" id="bib.bib165.1.1">et al.</em>, “Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib165.2.2">arXiv e-prints</em>, pp. arXiv–2107, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
L. Fang, Y. Luo, K. Feng, K. Zhao, and A. Hu, “A knowledge-enriched ensemble method for word embedding and multi-sense embedding,” <em class="ltx_emph ltx_font_italic" id="bib.bib166.1.1">IEEE Transactions on Knowledge and Data Engineering</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
Q. Li, D. Wang, S. F. K. Song, Y. Zhang, and G. Yu, “Oerl: Enhanced representation learning via open knowledge graphs,” <em class="ltx_emph ltx_font_italic" id="bib.bib167.1.1">IEEE Transactions on Knowledge and Data Engineering</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
F. Moiseev, Z. Dong, E. Alfonseca, and M. Jaggi, “Skill: Structured knowledge infusion for large language models,” May 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
N. Poerner, U. Waltinger, and H. Schütze, “E-bert: Efficient-yet-effective entity embeddings for bert,” <em class="ltx_emph ltx_font_italic" id="bib.bib169.1.1">arXiv preprint arXiv:1911.03681</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
C. Rosset, C. Xiong, M. Phan, X. Song, P. Bennett, and S. Tiwary, “Knowledge-aware language model pretraining,” <em class="ltx_emph ltx_font_italic" id="bib.bib170.1.1">arXiv preprint arXiv:2007.00655</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
W. Liu, P. Zhou, Z. Zhao, Z. Wang, Q. Ju, H. Deng, and P. Wang, “K-bert: Enabling language representation with knowledge graph,” <em class="ltx_emph ltx_font_italic" id="bib.bib171.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 34, no. 03, pp. 2901–2908, Apr. 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu, “Ernie: Enhanced language representation with informative entities,” <em class="ltx_emph ltx_font_italic" id="bib.bib172.1.1">arXiv preprint arXiv:1905.07129</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
Y. Su, X. Han, Z. Zhang, Y. Lin, P. Li, Z. Liu, J. Zhou, and M. Sun, “Cokebert: Contextual knowledge selection and embedding towards enhanced pre-trained language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib173.1.1">AI Open</em>, vol. 2, pp. 127–134, Jan. 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
L. He, S. Zheng, T. Yang, and F. Zhang, “Klmo: Knowledge graph enhanced pretrained language model with fine-grained relationships,” in <em class="ltx_emph ltx_font_italic" id="bib.bib174.1.1">Findings of the Association for Computational Linguistics: EMNLP 2021</em>, M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds.   Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 4536–4542.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
R. Wang, D. Tang, N. Duan, Z. Wei, X. Huang, J. Ji, G. Cao, D. Jiang, and M. Zhou, “K-adapter: Infusing knowledge into pre-trained models with adapters,” in <em class="ltx_emph ltx_font_italic" id="bib.bib175.1.1">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</em>, C. Zong, F. Xia, W. Li, and R. Navigli, Eds.   Online: Association for Computational Linguistics, Aug. 2021, pp. 1405–1418.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
M. Kang, J. Baek, and S. J. Hwang, “Kala: knowledge-augmented language model adaptation,” <em class="ltx_emph ltx_font_italic" id="bib.bib176.1.1">arXiv preprint arXiv:2204.10555</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
Q. Lu, D. Dou, and T. H. Nguyen, “Parameter-efficient domain knowledge integration from multiple sources for biomedical pre-trained language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib177.1.1">Findings of the Association for Computational Linguistics: EMNLP 2021</em>, 2021, pp. 3855–3865.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
I. Yamada, A. Asai, H. Shindo, H. Takeda, and Y. Matsumoto, “Luke: Deep contextualized entity representations with entity-aware self-attention,” <em class="ltx_emph ltx_font_italic" id="bib.bib178.1.1">arXiv preprint arXiv:2010.01057</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
Y. Xu, M. Namazifar, D. Hazarika, A. Padmakumar, Y. Liu, and D. Hakkani-Tür, “Kilm: Knowledge injection into encoder-decoder language models,” Feb. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
T. Shen, Y. Mao, P. He, G. Long, A. Trischler, and W. Chen, “Exploiting structured knowledge in text via graph-guided representation learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib180.1.1">arXiv preprint arXiv:2004.14224</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and O. Levy, “Spanbert: Improving pre-training by representing and predicting spans,” <em class="ltx_emph ltx_font_italic" id="bib.bib181.1.1">Transactions of the association for computational linguistics</em>, vol. 8, pp. 64–77, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
D. Yu, C. Zhu, Y. Yang, and M. Zeng, “Jaket: Joint pre-training of knowledge graph and language understanding,” <em class="ltx_emph ltx_font_italic" id="bib.bib182.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 36, no. 10, pp. 11 630–11 638, Jun. 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
M. Yasunaga, A. Bosselut, H. Ren, X. Zhang, C. D. Manning, P. S. Liang, and J. Leskovec, “Deep bidirectional language-knowledge graph pretraining,” <em class="ltx_emph ltx_font_italic" id="bib.bib183.1.1">Advances in Neural Information Processing Systems</em>, vol. 35, pp. 37 309–37 323, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
Y. Qin, Y. Lin, R. Takanobu, Z. Liu, P. Li, H. Ji, M. Huang, M. Sun, and J. Zhou, “Erica: Improving entity and relation understanding for pre-trained language models via contrastive learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib184.1.1">arXiv preprint arXiv:2012.15022</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
X. Wang, T. Gao, Z. Zhu, Z. Zhang, Z. Liu, J. Li, and J. Tang, “Kepler: A unified model for knowledge embedding and pre-trained language representation,” <em class="ltx_emph ltx_font_italic" id="bib.bib185.1.1">Transactions of the Association for Computational Linguistics</em>, vol. 9, pp. 176–194, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
Y. Sun, S. Wang, Y. Li, S. Feng, H. Tian, H. Wu, and H. Wang, “Ernie 2.0: A continual pre-training framework for language understanding,” in <em class="ltx_emph ltx_font_italic" id="bib.bib186.1.1">Proceedings of the AAAI conference on artificial intelligence</em>, vol. 34, no. 05, 2020, pp. 8968–8975.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong <em class="ltx_emph ltx_font_italic" id="bib.bib187.1.1">et al.</em>, “A survey of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib187.2.2">arXiv preprint arXiv:2303.18223</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
J. Baek, A. F. Aji, and A. Saffari, “Knowledge-augmented language model prompting for zero-shot knowledge graph question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib188.1.1">arXiv preprint arXiv:2306.04136</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
Y. Wu, N. Hu, S. Bi, G. Qi, J. Ren, A. Xie, and W. Song, “Retrieve-rewrite-answer: A kg-to-text enhanced llms framework for knowledge graph question answering,” Sep. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
Y. Wen, Z. Wang, and J. Sun, “Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models,” Sep. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
C. Feng, X. Zhang, and Z. Fei, “Knowledge solver: Teaching llms to search for domain knowledge from knowledge graphs,” Sep. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock">
S. Saxena, S. Prasad, M. Prakash, A. Shankar, V. Vaddina, S. Gopalakrishnan <em class="ltx_emph ltx_font_italic" id="bib.bib192.1.1">et al.</em>, “Minimizing factual inconsistency and hallucination in large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib192.2.2">arXiv preprint arXiv:2311.13878</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_tag_bibitem">[193]</span>
<span class="ltx_bibblock">
Y. Shi, H. Ma, W. Zhong, G. Mai, X. Li, T. Liu, and J. Huang, “Chatgraph: Interpretable text classification by converting chatgpt knowledge to graphs,” <em class="ltx_emph ltx_font_italic" id="bib.bib193.1.1">arXiv preprint arXiv:2305.03513</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_tag_bibitem">[194]</span>
<span class="ltx_bibblock">
Z. Chen, J. Chen, M. Gaidhani, A. Singh, and M. Sra, “Xplainllm: A qa explanation dataset for understanding llm decision-making,” <em class="ltx_emph ltx_font_italic" id="bib.bib194.1.1">arXiv preprint arXiv:2311.08614</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_tag_bibitem">[195]</span>
<span class="ltx_bibblock">
W. Fan, Q. Li, and M. Cheng, “Deep modeling of social relations for recommendation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib195.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 32, no. 1, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_tag_bibitem">[196]</span>
<span class="ltx_bibblock">
W. Fan, T. Derr, Y. Ma, J. Wang, J. Tang, and Q. Li, “Deep adversarial social recommendation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib196.1.1">28th International Joint Conference on Artificial Intelligence (IJCAI-19)</em>.   International Joint Conferences on Artificial Intelligence, 2019, pp. 1351–1357.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_tag_bibitem">[197]</span>
<span class="ltx_bibblock">
W. Fan, Y. Ma, D. Yin, J. Wang, J. Tang, and Q. Li, “Deep social collaborative filtering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib197.1.1">Proceedings of the 13th ACM Conference on Recommender Systems</em>, 2019, pp. 305–313.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_tag_bibitem">[198]</span>
<span class="ltx_bibblock">
B. Yin, J. Xie, Y. Qin, Z. Ding, Z. Feng, X. Li, and W. Lin, “Heterogeneous knowledge fusion: A novel approach for personalized recommendation via llm,” in <em class="ltx_emph ltx_font_italic" id="bib.bib198.1.1">Proceedings of the 17th ACM Conference on Recommender Systems</em>, 2023, pp. 599–601.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_tag_bibitem">[199]</span>
<span class="ltx_bibblock">
Z. Tang, Z. Huan, Z. Li, X. Zhang, J. Hu, C. Fu, J. Zhou, and C. Li, “One model for all: Large language models are domain-agnostic recommendation systems,” Oct. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_tag_bibitem">[200]</span>
<span class="ltx_bibblock">
S. Dai, N. Shao, H. Zhao, W. Yu, Z. Si, C. Xu, Z. Sun, X. Zhang, and J. Xu, “Uncovering chatgpt’s capabilities in recommender systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib200.1.1">arXiv preprint arXiv:2305.02182</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_tag_bibitem">[201]</span>
<span class="ltx_bibblock">
H. Wang, X. Liu, W. Fan, X. Zhao, V. Kini, D. Yadav, F. Wang, Z. Wen, J. Tang, and H. Liu, “Rethinking large language model architectures for sequential recommendations,” <em class="ltx_emph ltx_font_italic" id="bib.bib201.1.1">arXiv preprint arXiv:2402.09543</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_tag_bibitem">[202]</span>
<span class="ltx_bibblock">
Y. Xi, W. Liu, J. Lin, J. Zhu, B. Chen, R. Tang, W. Zhang, R. Zhang, and Y. Yu, “Towards open-world recommendation with knowledge augmentation from large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib202.1.1">arXiv preprint arXiv:2306.10933</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_tag_bibitem">[203]</span>
<span class="ltx_bibblock">
J. Wu, Q. Liu, H. Hu, W. Fan, S. Liu, Q. Li, X.-M. Wu, and K. Tang, “Leveraging large language models (llms) to empower training-free dataset condensation for content-based recommendation,” <em class="ltx_emph ltx_font_italic" id="bib.bib203.1.1">arXiv preprint arXiv:2310.09874</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_tag_bibitem">[204]</span>
<span class="ltx_bibblock">
J. Liu, C. Liu, R. Lv, K. Zhou, and Y. Zhang, “Is chatgpt a good recommender? a preliminary study,” <em class="ltx_emph ltx_font_italic" id="bib.bib204.1.1">arXiv preprint arXiv:2304.10149</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_tag_bibitem">[205]</span>
<span class="ltx_bibblock">
K. Bao, J. Zhang, Y. Zhang, W. Wang, F. Feng, and X. He, “Tallrec: An effective and efficient tuning framework to align large language model with recommendation,” <em class="ltx_emph ltx_font_italic" id="bib.bib205.1.1">arXiv preprint arXiv:2305.00447</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib206">
<span class="ltx_tag ltx_tag_bibitem">[206]</span>
<span class="ltx_bibblock">
W. Hua, Y. Ge, S. Xu, J. Ji, and Y. Zhang, “Up5: Unbiased foundation model for fairness-aware recommendation,” <em class="ltx_emph ltx_font_italic" id="bib.bib206.1.1">arXiv preprint arXiv:2305.12090</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib207">
<span class="ltx_tag ltx_tag_bibitem">[207]</span>
<span class="ltx_bibblock">
C. Huang, T. Yu, K. Xie, S. Zhang, L. Yao, and J. McAuley, “Foundation models for recommender systems: A survey and new perspectives,” <em class="ltx_emph ltx_font_italic" id="bib.bib207.1.1">arXiv preprint arXiv:2402.11143</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib208">
<span class="ltx_tag ltx_tag_bibitem">[208]</span>
<span class="ltx_bibblock">
R. Yang, L. Fang, and Y. Zhou, “Cp-kgc: Constrained-prompt knowledge graph completion with large language models,” Oct. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib209">
<span class="ltx_tag ltx_tag_bibitem">[209]</span>
<span class="ltx_bibblock">
L. Yao, J. Peng, C. Mao, and Y. Luo, “Exploring large language models for knowledge graph completion,” Sep. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib210">
<span class="ltx_tag ltx_tag_bibitem">[210]</span>
<span class="ltx_bibblock">
J. Kim, Y. Kwon, Y. Jo, and E. Choi, “Kg-gpt: A general framework for reasoning on knowledge graphs using large language models,” Oct. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib211">
<span class="ltx_tag ltx_tag_bibitem">[211]</span>
<span class="ltx_bibblock">
H. Luo, H. E, Z. Tang, S. Peng, Y. Guo, W. Zhang, C. Ma, G. Dong, M. Song, and W. Lin, “Chatkbqa: A generate-then-retrieve framework for knowledge base question answering with fine-tuned large language models,” Oct. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib212">
<span class="ltx_tag ltx_tag_bibitem">[212]</span>
<span class="ltx_bibblock">
J. Jiang, K. Zhou, Z. Dong, K. Ye, W. X. Zhao, and J.-R. Wen, “Structgpt: A general framework for large language model to reason over structured data,” <em class="ltx_emph ltx_font_italic" id="bib.bib212.1.1">arXiv preprint arXiv:2305.09645</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib213">
<span class="ltx_tag ltx_tag_bibitem">[213]</span>
<span class="ltx_bibblock">
L. Luo, J. Ju, B. Xiong, Y.-F. Li, G. Haffari, and S. Pan, “Chatrule: Mining logical rules with large language models for knowledge graph reasoning,” Sep. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib214">
<span class="ltx_tag ltx_tag_bibitem">[214]</span>
<span class="ltx_bibblock">
L.-P. Meyer, J. Frey, K. Junghanns, F. Brei, K. Bulert, S. Gründer-Fahrer, and M. Martin, “Developing a scalable benchmark for assessing large language models in knowledge graph engineering,” Aug. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib215">
<span class="ltx_tag ltx_tag_bibitem">[215]</span>
<span class="ltx_bibblock">
M. Galkin, X. Yuan, H. Mostafa, J. Tang, and Z. Zhu, “Towards foundation models for knowledge graph reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib215.1.1">arXiv preprint arXiv:2310.04562</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib216">
<span class="ltx_tag ltx_tag_bibitem">[216]</span>
<span class="ltx_bibblock">
W. Tang, R. Liu, H. Wen, X. Dai, J. Ding, H. Li, W. Fan, Y. Xie, and J. Tang, “A general single-cell analysis framework via conditional diffusion generative models,” <em class="ltx_emph ltx_font_italic" id="bib.bib216.1.1">bioRxiv</em>, pp. 2023–10, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib217">
<span class="ltx_tag ltx_tag_bibitem">[217]</span>
<span class="ltx_bibblock">
Y. Cao, Z.-Q. Yang, X.-L. Zhang, W. Fan, Y. Wang, J. Shen, D.-Q. Wei, Q. Li, and X.-Y. Wei, “Identifying the kind behind smiles—anatomical therapeutic chemical classification using structure-only representations,” <em class="ltx_emph ltx_font_italic" id="bib.bib217.1.1">Briefings in Bioinformatics</em>, vol. 23, no. 5, p. bbac346, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib218">
<span class="ltx_tag ltx_tag_bibitem">[218]</span>
<span class="ltx_bibblock">
H. Cao, Z. Liu, X. Lu, Y. Yao, and Y. Li, “Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery,” <em class="ltx_emph ltx_font_italic" id="bib.bib218.1.1">arXiv preprint arXiv:2311.16208</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib219">
<span class="ltx_tag ltx_tag_bibitem">[219]</span>
<span class="ltx_bibblock">
Z. Zhao, D. Ma, L. Chen, L. Sun, Z. Li, H. Xu, Z. Zhu, S. Zhu, S. Fan, G. Shen <em class="ltx_emph ltx_font_italic" id="bib.bib219.1.1">et al.</em>, “Chemdfm: Dialogue foundation model for chemistry,” <em class="ltx_emph ltx_font_italic" id="bib.bib219.2.2">arXiv preprint arXiv:2401.14818</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib220">
<span class="ltx_tag ltx_tag_bibitem">[220]</span>
<span class="ltx_bibblock">
C. Galindo, J.-A. Fernández-Madrigal, J. González, and A. Saffiotti, “Robot task planning using semantic maps,” <em class="ltx_emph ltx_font_italic" id="bib.bib220.1.1">Robotics and autonomous systems</em>, vol. 56, no. 11, pp. 955–966, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib221">
<span class="ltx_tag ltx_tag_bibitem">[221]</span>
<span class="ltx_bibblock">
G. Chalvatzaki, A. Younes, D. Nandha, A. T. Le, L. F. R. Ribeiro, and I. Gurevych, “Learning to reason over scene graphs: A case study of finetuning gpt-2 into a robot language model for grounded task planning,” <em class="ltx_emph ltx_font_italic" id="bib.bib221.1.1">Frontiers in Robotics and AI</em>, vol. 10, p. 1221739, Aug. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib222">
<span class="ltx_tag ltx_tag_bibitem">[222]</span>
<span class="ltx_bibblock">
K. Rana, J. Haviland, S. Garg, J. Abou-Chakra, I. Reid, and N. Suenderhauf, “Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning,” Sep. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib223">
<span class="ltx_tag ltx_tag_bibitem">[223]</span>
<span class="ltx_bibblock">
Y. Zhen, S. Bi, L. Xing-tong, P. Wei-qin, S. Hai-peng, C. Zi-rui, and F. Yi-shu, “Robot task planning based on large language model representing knowledge with directed graph structures,” Jun. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib224">
<span class="ltx_tag ltx_tag_bibitem">[224]</span>
<span class="ltx_bibblock">
H. Liu, Y. Wang, W. Fan, X. Liu, Y. Li, S. Jain, Y. Liu, A. K. Jain, and J. Tang, “Trustworthy ai: A computational perspective,” <em class="ltx_emph ltx_font_italic" id="bib.bib224.1.1">arXiv preprint arXiv:2107.06641</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib225">
<span class="ltx_tag ltx_tag_bibitem">[225]</span>
<span class="ltx_bibblock">
W. Fan, X. Zhao, X. Chen, J. Su, J. Gao, L. Wang, Q. Liu, Y. Wang, H. Xu, L. Chen <em class="ltx_emph ltx_font_italic" id="bib.bib225.1.1">et al.</em>, “A comprehensive survey on trustworthy recommender systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib225.2.2">arXiv preprint arXiv:2209.10117</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib226">
<span class="ltx_tag ltx_tag_bibitem">[226]</span>
<span class="ltx_bibblock">
W. Fan, W. Jin, X. Liu, H. Xu, X. Tang, S. Wang, Q. Li, J. Tang, J. Wang, and C. Aggarwal, “Jointly attacking graph neural network and its explanations,” in <em class="ltx_emph ltx_font_italic" id="bib.bib226.1.1">2023 IEEE 39th International Conference on Data Engineering (ICDE)</em>.   IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib227">
<span class="ltx_tag ltx_tag_bibitem">[227]</span>
<span class="ltx_bibblock">
W. Fan, T. Derr, X. Zhao, Y. Ma, H. Liu, J. Wang, J. Tang, and Q. Li, “Attacking black-box recommendations via copying cross-domain user profiles,” in <em class="ltx_emph ltx_font_italic" id="bib.bib227.1.1">2021 IEEE 37th International Conference on Data Engineering (ICDE)</em>.   IEEE, 2021, pp. 1583–1594.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib228">
<span class="ltx_tag ltx_tag_bibitem">[228]</span>
<span class="ltx_bibblock">
N. Jain, A. Schwarzschild, Y. Wen, G. Somepalli, J. Kirchenbauer, P.-y. Chiang, M. Goldblum, A. Saha, J. Geiping, and T. Goldstein, “Baseline defenses for adversarial attacks against aligned language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib228.1.1">arXiv preprint arXiv:2309.00614</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib229">
<span class="ltx_tag ltx_tag_bibitem">[229]</span>
<span class="ltx_bibblock">
D. Bespalov, S. Bhabesh, Y. Xiang, L. Zhou, and Y. Qi, “Towards building a robust toxicity predictor,” in <em class="ltx_emph ltx_font_italic" id="bib.bib229.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)</em>, 2023, pp. 581–598.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib230">
<span class="ltx_tag ltx_tag_bibitem">[230]</span>
<span class="ltx_bibblock">
Y. Chen, H. Yang, Y. Zhang, K. Ma, T. Liu, B. Han, and J. Cheng, “Understanding and improving graph injection attack by promoting unnoticeability,” <em class="ltx_emph ltx_font_italic" id="bib.bib230.1.1">arXiv preprint arXiv:2202.08057</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib231">
<span class="ltx_tag ltx_tag_bibitem">[231]</span>
<span class="ltx_bibblock">
X. Zou, Q. Zheng, Y. Dong, X. Guan, E. Kharlamov, J. Lu, and J. Tang, “Tdgia: Effective injection attacks on graph neural networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib231.1.1">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</em>, 2021, pp. 2461–2471.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib232">
<span class="ltx_tag ltx_tag_bibitem">[232]</span>
<span class="ltx_bibblock">
H. Dai, H. Li, T. Tian, X. Huang, L. Wang, J. Zhu, and L. Song, “Adversarial attack on graph structured data,” in <em class="ltx_emph ltx_font_italic" id="bib.bib232.1.1">International conference on machine learning</em>.   PMLR, 2018, pp. 1115–1124.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib233">
<span class="ltx_tag ltx_tag_bibitem">[233]</span>
<span class="ltx_bibblock">
D. Zügner, A. Akbarnejad, and S. Günnemann, “Adversarial attacks on neural networks for graph data,” in <em class="ltx_emph ltx_font_italic" id="bib.bib233.1.1">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>, 2018, pp. 2847–2856.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib234">
<span class="ltx_tag ltx_tag_bibitem">[234]</span>
<span class="ltx_bibblock">
A. Wei, N. Haghtalab, and J. Steinhardt, “Jailbroken: How does llm safety training fail?” <em class="ltx_emph ltx_font_italic" id="bib.bib234.1.1">arXiv preprint arXiv:2307.02483</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib235">
<span class="ltx_tag ltx_tag_bibitem">[235]</span>
<span class="ltx_bibblock">
Z. Zhang, G. Zhang, B. Hou, W. Fan, Q. Li, S. Liu, Y. Zhang, and S. Chang, “Certified robustness for large language models with self-denoising,” <em class="ltx_emph ltx_font_italic" id="bib.bib235.1.1">arXiv preprint arXiv:2307.07171</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib236">
<span class="ltx_tag ltx_tag_bibitem">[236]</span>
<span class="ltx_bibblock">
S. Geng, S. Liu, Z. Fu, Y. Ge, and Y. Zhang, “Recommendation as language processing (rlp): A unified pretrain, personalized prompt &amp; predict paradigm (p5),” in <em class="ltx_emph ltx_font_italic" id="bib.bib236.1.1">Proceedings of the 16th ACM Conference on Recommender Systems</em>, 2022, pp. 299–315.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib237">
<span class="ltx_tag ltx_tag_bibitem">[237]</span>
<span class="ltx_bibblock">
H. Liu, J. Dacon, W. Fan, H. Liu, Z. Liu, and J. Tang, “Does gender matter? towards fairness in dialogue systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib237.1.1">Proceedings of the 28th International Conference on Computational Linguistics</em>, 2020, pp. 4403–4416.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib238">
<span class="ltx_tag ltx_tag_bibitem">[238]</span>
<span class="ltx_bibblock">
G. Zhang, Y. Zhang, Y. Zhang, W. Fan, Q. Li, S. Liu, and S. Chang, “Fairness reprogramming,” in <em class="ltx_emph ltx_font_italic" id="bib.bib238.1.1">Thirty-sixth Conference on Neural Information Processing Systems</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib239">
<span class="ltx_tag ltx_tag_bibitem">[239]</span>
<span class="ltx_bibblock">
X. Chen, W. Fan, J. Chen, H. Liu, Z. Liu, Z. Zhang, and Q. Li, “Fairly adaptive negative sampling for recommendations,” in <em class="ltx_emph ltx_font_italic" id="bib.bib239.1.1">Proceedings of the ACM Web Conference 2023</em>, 2023, pp. 3723–3733.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib240">
<span class="ltx_tag ltx_tag_bibitem">[240]</span>
<span class="ltx_bibblock">
K. Webster, X. Wang, I. Tenney, A. Beutel, E. Pitler, E. Pavlick, J. Chen, E. Chi, and S. Petrov, “Measuring and reducing gendered correlations in pre-trained models,” <em class="ltx_emph ltx_font_italic" id="bib.bib240.1.1">arXiv preprint arXiv:2010.06032</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib241">
<span class="ltx_tag ltx_tag_bibitem">[241]</span>
<span class="ltx_bibblock">
Y. Guo, Y. Yang, and A. Abbasi, “Auto-debias: Debiasing masked language models with automated biased prompts,” in <em class="ltx_emph ltx_font_italic" id="bib.bib241.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 2022, pp. 1012–1023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib242">
<span class="ltx_tag ltx_tag_bibitem">[242]</span>
<span class="ltx_bibblock">
J. He, M. Xia, C. Fellbaum, and D. Chen, “Mabel: Attenuating gender bias using textual entailment data,” <em class="ltx_emph ltx_font_italic" id="bib.bib242.1.1">arXiv preprint arXiv:2210.14975</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib243">
<span class="ltx_tag ltx_tag_bibitem">[243]</span>
<span class="ltx_bibblock">
N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. B. Brown, D. Song, U. Erlingsson <em class="ltx_emph ltx_font_italic" id="bib.bib243.1.1">et al.</em>, “Extracting training data from large language models.” in <em class="ltx_emph ltx_font_italic" id="bib.bib243.2.2">USENIX Security Symposium</em>, vol. 6, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib244">
<span class="ltx_tag ltx_tag_bibitem">[244]</span>
<span class="ltx_bibblock">
J. Huang, H. Shao, and K. C.-C. Chang, “Are large pre-trained language models leaking your personal information?” <em class="ltx_emph ltx_font_italic" id="bib.bib244.1.1">arXiv preprint arXiv:2205.12628</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib245">
<span class="ltx_tag ltx_tag_bibitem">[245]</span>
<span class="ltx_bibblock">
J. Zhang, L. Wang, S. Wang, and W. Fan, “Graph unlearning with efficient partial retraining,” <em class="ltx_emph ltx_font_italic" id="bib.bib245.1.1">arXiv preprint arXiv:2403.07353</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib246">
<span class="ltx_tag ltx_tag_bibitem">[246]</span>
<span class="ltx_bibblock">
Y. Li, Z. Tan, and Y. Liu, “Privacy-preserving prompt tuning for large language model services,” <em class="ltx_emph ltx_font_italic" id="bib.bib246.1.1">arXiv preprint arXiv:2305.06212</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib247">
<span class="ltx_tag ltx_tag_bibitem">[247]</span>
<span class="ltx_bibblock">
R. Xue, X. Shen, R. Yu, and X. Liu, “Efficient large language models fine-tuning on graphs,” <em class="ltx_emph ltx_font_italic" id="bib.bib247.1.1">arXiv preprint arXiv:2312.04737</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib248">
<span class="ltx_tag ltx_tag_bibitem">[248]</span>
<span class="ltx_bibblock">
T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Efficient finetuning of quantized llms,” <em class="ltx_emph ltx_font_italic" id="bib.bib248.1.1">arXiv preprint arXiv:2305.14314</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib249">
<span class="ltx_tag ltx_tag_bibitem">[249]</span>
<span class="ltx_bibblock">
X. Ma, G. Fang, and X. Wang, “Llm-pruner: On the structural pruning of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib249.1.1">arXiv preprint arXiv:2305.11627</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib250">
<span class="ltx_tag ltx_tag_bibitem">[250]</span>
<span class="ltx_bibblock">
M. Xia, T. Gao, Z. Zeng, and D. Chen, “Sheared llama: Accelerating language model pre-training via structured pruning,” <em class="ltx_emph ltx_font_italic" id="bib.bib250.1.1">arXiv preprint arXiv:2310.06694</em>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun  4 01:26:34 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
