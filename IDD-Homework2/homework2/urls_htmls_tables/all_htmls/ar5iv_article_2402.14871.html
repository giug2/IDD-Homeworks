<article class="ltx_document ltx_authors_1line">
 <span class="ltx_note ltx_role_institutetext" id="id1">
  <sup class="ltx_note_mark">
   1
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     1
    </sup>
    <span class="ltx_note_type">
     institutetext:
    </span>
    Dept. of Computer, Control, and Management Engineering
    <br class="ltx_break"/>
    Sapienza University of Rome, Rome (Italy),
    <span class="ltx_note ltx_role_email" id="id1.1">
     <sup class="ltx_note_mark">
      1
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        1
       </sup>
       <span class="ltx_note_type">
        email:
       </span>
       {lastname}@diag.uniroma1.it
      </span>
     </span>
    </span>
   </span>
  </span>
 </span>
 <span class="ltx_note ltx_role_institutetext" id="id2">
  <sup class="ltx_note_mark">
   2
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     2
    </sup>
    <span class="ltx_note_type">
     institutetext:
    </span>
    UNINT Univeristy, Via Cristoforo Colombo, 200 - 00147 Rome (Italy),
    <span class="ltx_note ltx_role_email" id="id2.1">
     <sup class="ltx_note_mark">
      2
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        2
       </sup>
       <span class="ltx_note_type">
        email:
       </span>
       domenico.bloisi@unint.eu
      </span>
     </span>
    </span>
   </span>
  </span>
 </span>
 <h1 class="ltx_title ltx_title_document">
  LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Emanuele Musumeci
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_orcid">
     <a class="ltx_ref" href="https://orcid.org/0009-0004-2359-5032" target="_blank" title="ORCID identifier">
      0009-0004-2359-5032
     </a>
    </span>
    11
   </span>
  </span>
  <span class="ltx_author_before">
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Michele Brienza
   </span>
   <span class="ltx_author_notes">
    11
    <span class="ltx_contact ltx_role_orcid">
     <a class="ltx_ref" href="https://orcid.org/0009-0000-1549-9500" target="_blank" title="ORCID identifier">
      0009-0000-1549-9500
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <br class="ltx_break"/>
    Vincenzo Suriani
   </span>
   <span class="ltx_author_notes">
    11
    <span class="ltx_contact ltx_role_orcid">
     <a class="ltx_ref" href="https://orcid.org/0000%E2%88%920003%E2%88%921199%E2%88%928358" target="_blank" title="ORCID identifier">
      0000−0003−1199−8358
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Daniele Nardi
   </span>
   <span class="ltx_author_notes">
    11
    <span class="ltx_contact ltx_role_orcid">
     <a class="ltx_ref" href="https://orcid.org/0000-0001-6606-200X" target="_blank" title="ORCID identifier">
      0000-0001-6606-200X
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Domenico Daniele Bloisi
   </span>
   <span class="ltx_author_notes">
    22
    <span class="ltx_contact ltx_role_orcid">
     <a class="ltx_ref" href="https://orcid.org/0000%E2%88%920003%E2%88%920339%E2%88%928651" target="_blank" title="ORCID identifier">
      0000−0003−0339−8651
     </a>
    </span>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id1.id1">
   In the last years’ digitalization process, the creation and management of documents in various domains, particularly in Public Administration (PA), have become increasingly complex and diverse. This complexity arises from the need to handle a wide range of document types, often characterized by semi-structured forms.
Semi-structured documents present a fixed set of data without a fixed format. As a consequence, a template-based solution cannot be used, as understanding a document requires the extraction of the data structure. The recent introduction of Large Language Models (LLMs) has enabled the creation of customized text output satisfying user requests.
In this work, we propose a novel approach that combines the LLMs with prompt engineering and multi-agent systems for generating new documents
compliant with a desired structure.The main contribution of this work concerns replacing the commonly used manual prompting with a task description generated by semantic retrieval from an LLM. The potential of this approach is demonstrated through a series of experiments and case studies, showcasing its effectiveness in real-world PA scenarios.
  </p>
 </div>
 <div class="ltx_keywords">
  <h6 class="ltx_title ltx_title_keywords">
   Keywords:
  </h6>
  Human-Centered AI Public Administration Task optimization
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <figure class="ltx_figure" id="S1.F1">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="310" id="S1.F1.g1" src="/html/2402.14871/assets/images/Process_condensed.png" width="568"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 1:
    </span>
    The presented multi-agent architecture with the LLMs used in prompt engineering and multi-agent fashion for generating new documents.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Document creation is a typical task in the Public Administration (PA) setting, requiring repetitive sub-tasks that offer the potential for automation. For instance, when writing official certificates required in public offices, the required personal information from the requesting user is often very schematic and constitutes only a small percentage of the text present in the document.
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    The use of pre-made document templates manages only marginally to reduce the effort spent and applies only to rigidly structured documents, where the semantics of missing information can be perfectly defined in the templates themselves. Automation of the writing of this kind of document, which amounts to field-filling, is straightforward.
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    Instead, semi-structured documents offer a flexible format, in which missing information cannot be clearly tagged and is usually determined by the semantics of the surrounding context. For instance, standard fields like the current date can appear in different positions in the document with no pre-defined criteria.
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    Producing this kind of document requires additional effort to adapt their flexible structure to the current use case and the available and required information, with additional effort to recover missing information. The semantics associated with the necessary information require some contextual knowledge, which can be gained by the surrounding context and sometimes the overall semantics of the whole document. For instance, different terms can be used for the qualification of the same field: e.g. "Invoice," "Invoice No." "Bill" and "Purchase Order" can all label the same information (the invoice number).
For this reason, structure and semantics in a semi-structured document are usually intertwined, and developing an efficient document generation pipeline that can run for a wide variety of documents is a very challenging task.
   </p>
  </div>
  <div class="ltx_para" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    On these premises, the specific task of adapting semi-structured document templates can be supported by the use of Artificial Intelligence and in particular Language Models, to reduce the time spent automating the process and as a means to improve the generality of the automatic approach.
   </p>
  </div>
  <div class="ltx_para" id="S1.p6">
   <p class="ltx_p" id="S1.p6.1">
    A straightforward solution could be to generate and therefore refine a document in several incremental steps, with a separate prompt for each one, but this could hinder the quality of the result due to the potential dependencies between data in different parts of the same document, that might be located in structurally unrelated sections. A fully unsupervised approach would therefore run the risk of incurring in hallucination due to the limited effectiveness of long context windows for long document structures.
   </p>
  </div>
  <div class="ltx_para" id="S1.p7">
   <p class="ltx_p" id="S1.p7.1">
    Prompt engineering guidelines usually require providing preliminary and accurate context to the role of the LLM agent in the required task. Therefore it is possible to alter and improve the result on the same user request by prepending an accurate description of the role of the LLM agent to the actual user prompt.
   </p>
  </div>
  <div class="ltx_para" id="S1.p8">
   <p class="ltx_p" id="S1.p8.1">
    Under these assumptions, it is possible to improve the incremental trial-and-error document generation by introducing several agents, each with a defined fine-grained role in the generation process. In this framework, the capabilities of each agent can be tuned by providing it with an accurate description of its role. Moreover, each agent can be augmented with a memory component local to the agent, sampled and applied specifically for its role, and with additional capabilities that compensate for the lacks of Language Models, such as interaction with the World Wide Web in real-time, access to private custom knowledge bases and information feeds to enhance agents with domain-specific knowledge, or the execution of specialized code as in a Function-as-a-Service framework. This kind of architecture is perfectly compatible with the emergent AI-as-a-Service (AIaaS) paradigm
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib3" title="">
      3
     </a>
     ]
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p9">
   <p class="ltx_p" id="S1.p9.1">
    The multi-agent process assists the user by iteratively refining the prompt with the guidance of a pre-existing structure extracted from similar documents. Then, context-specific prompts are provided to the various agents during the process, depending on their specific role and the original document structure, with little to no human supervision. Interaction between the agents, which is conversational in nature, can include direct interaction with the user in cases in which intervention is needed.
   </p>
  </div>
  <div class="ltx_para" id="S1.p10">
   <p class="ltx_p" id="S1.p10.1">
    We present a workflow and interaction framework for the LLM-assisted multi-agent generation of a semi-structured document in the PA domain, with limited human supervision. We then show the prompt refinement process necessary to obtain the required results for each specific agent role in the current workflow.
    <br class="ltx_break"/>
    The code and the additional results obtained from this work can be found at the following webpage
    <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://sites.google.com/uniroma1.it/multi-agent-documentgeneration/home-page" target="_blank" title="">
     https://sites.google.com/uniroma1.it/multi-agent-documentgeneration/home-page
    </a>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p11">
   <p class="ltx_p" id="S1.p11.1">
    The remainder of the paper is organized as follows. Section
    <a class="ltx_ref" href="#S2" title="2 Related Work ‣ LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    contains a description of the state of the art. Section
    <a class="ltx_ref" href="#S3" title="3 Proposed Approach ‣ LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain">
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    presents the description of the workflow and interaction framework. The prompt refinement process is then shown in Section
    <a class="ltx_ref" href="#S4" title="4 Experimental Evaluation ‣ LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain">
     <span class="ltx_text ltx_ref_tag">
      4
     </span>
    </a>
    . Finally, conclusions are drawn in Section
    <a class="ltx_ref" href="#S5" title="5 Conclusions ‣ 4.4 Prompt-engineered results ‣ 4.3 Content Generation Agent ‣ 4.1 Semantics Identification Agent ‣ 4 Experimental Evaluation ‣ LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain">
     <span class="ltx_text ltx_ref_tag">
      5
     </span>
    </a>
    .
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Related Work
  </h2>
  <div class="ltx_para" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    Since their release to the public, Large Language Models (LLMs) have shown great potential for a wide range of daily tasks
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib10" title="">
      10
     </a>
     ]
    </cite>
    . In particular, their capabilities in document editing and generation use-cases offer great potential for their successful application to the PA domain. Most LLMs have shown greater performance in zero-shot
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib5" title="">
      5
     </a>
     ]
    </cite>
    and in particular few-shot
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib4" title="">
      4
     </a>
     ]
    </cite>
    tasks, where examples of acceptable results are provided along with the instructions for the task to be executed.
   </p>
  </div>
  <div class="ltx_para" id="S2.p2">
   <p class="ltx_p" id="S2.p2.1">
    It has been shown that better results can be obtained by applying guidelines for prompt engineering
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib16" title="">
      16
     </a>
     ]
    </cite>
    . Usually, when a human is involved, the prompt refinement becomes a trial-and-error process, by improving the final result by incremental changes to the initial prompt, based on the generated output.
   </p>
  </div>
  <div class="ltx_para" id="S2.p3">
   <p class="ltx_p" id="S2.p3.1">
    Prompt engineering proved to be a crucial step both for the average and advanced users in applications of LLMs to the production of semi-structured documents, where the original structure requires subsequent adaptations through a trial-and-error process. Through prompt engineering, it is possible to improve the quality of the output obtained and especially its compliance with contextual specifications regarding the required content and style. For this reason, the main challenge of allowing PA entities to successfully integrate LLMs in their workflows is to enable the inexperienced user to create efficient prompts and to minimize the time spent improving the task description provided as a prompt to the Large Language Model.
   </p>
  </div>
  <div class="ltx_para" id="S2.p4">
   <p class="ltx_p" id="S2.p4.1">
    As a trade-off for their versatility, LLMs incur in the problem of hallucination, causing results to be skewed and inaccurate or biased with respect to the original requests, especially with longer context windows. In the document generation task, especially for the generation of longer documents, prompts might tend to be long and rich in information, with the risk of causing hallucinations.
   </p>
  </div>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1
    </span>
    LLMs in the PA Domain
   </h3>
   <div class="ltx_para" id="S2.SS1.p1">
    <p class="ltx_p" id="S2.SS1.p1.1">
     The integration of Large Language Models (LLMs) in automating document generation processes, particularly in the domain of PA, has seen significant interest due to the amount of document manipulation required in this domain. Some works are helping in information extraction from those documents, for example, when dealing with extracting and classifying relations from tenders of the PA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib12" title="">
       12
      </a>
      ]
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p2">
    <p class="ltx_p" id="S2.SS1.p2.1">
     Prior studies are nowadays predominantly focused on leveraging LLMs for structured data extraction, text summarization, and content customization
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib7" title="">
       7
      </a>
      ]
     </cite>
     to enhance administrative efficiency and user engagement. Prompt engineering has shown relevant results in improving the LLM’s generation capabilities
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib16" title="">
       16
      </a>
      ]
     </cite>
     , and, with guiding principles, LLMs can meet requirements and allow for enhanced quality in response
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib1" title="">
       1
      </a>
      ]
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p3">
    <p class="ltx_p" id="S2.SS1.p3.1">
     An approach for information extraction for unstructured documents is presented in
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib9" title="">
       9
      </a>
      ]
     </cite>
     , where an embedding-based retrieval system with LLM is used for effective agriculture information extraction from unstructured data. The system features an embedding-based retrieval system along with LLM question-answering to automatically extract entities and attributes from the documents, and transform them into structured data.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p4">
    <p class="ltx_p" id="S2.SS1.p4.1">
     When dealing with novel documents, also Retrieval-augmented generation (RAG) approaches allow large language models (LLM) to retrieve relevant knowledge, showing promising potential in mitigating LLM hallucinations and enhancing response quality, and, chance, facilitating the adoption of LLMs in practice
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib2" title="">
       2
      </a>
      ]
     </cite>
     . However, existing RAG systems are often inadequate in answering multi-hop queries, which require retrieving and reasoning over iteratively. An improvement of this technology has been proposed in
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib14" title="">
       14
      </a>
      ]
     </cite>
     where multi-hop reasoning steps are introduced in the RAG system.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p5">
    <p class="ltx_p" id="S2.SS1.p5.1">
     The difficulties are even more common when dealing with the application of LLMs in generating semi-structured documents from semantically similar examples. This remains relatively unsolved and they still struggle with tasks that require generating complex, structured outputs
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib13" title="">
       13
      </a>
      ]
     </cite>
     . This gap is primarily due to the inherent complexity of semi-structured documents, which defy conventional template-based approaches. Several approaches have been proposed to handle them.
A notable example is represented by
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib15" title="">
       15
      </a>
      ]
     </cite>
     , where Chain-Of-Thought is presented as a series of intermediate reasoning steps that significantly improve the ability of large language models to perform complex reasoning. In particular, it is shown how such reasoning abilities emerge naturally in sufficiently large language models.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p6">
    <p class="ltx_p" id="S2.SS1.p6.1">
     A closer step in the generation of semi-structured documents is represented by the Directional Stimulus Prompting Technology
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib8" title="">
       8
      </a>
      ]
     </cite>
     , where the LLM output is conditioned to generate desired outcomes, such as including specific keywords.
The research on semantic understanding and context-aware generation provides foundational insights but stops short of addressing the specific challenges posed by semi-structured documents in PA.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p7">
    <p class="ltx_p" id="S2.SS1.p7.1">
     Furthermore, the Artificial Intelligence as a Service (’AIaaS’) trend
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib3" title="">
       3
      </a>
      ]
     </cite>
     is going to play a growing role in society’s technological infrastructure, enabling, facilitating, and underpinning functionality in many applications. AIaaS providers therefore hold significant power at this infrastructural level and with the upcoming legislations in Europe, their role can easily be diffused in the workflow of the public offices.
The
     <span class="ltx_text ltx_font_italic" id="S2.SS1.p7.1.1">
      AIaaS
     </span>
     approach aligns also with our proposed multi-agent approach in document generation tasks. The multi-agent approach has been demonstrated to improve problem-solving in overcoming the limitations of individual models
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib11" title="">
       11
      </a>
      ]
     </cite>
     .
In the PA domain, this distributed approach offers modularity and scalability, potentially suitable for handling various document types and complexities within PA settings.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p8">
    <p class="ltx_p" id="S2.SS1.p8.1">
     In summary, while the literature provides valuable perspectives on the capabilities and applications of LLMs in various contexts, our work contributes a novel methodology and interaction framework for the LLM-assisted semi-structured document generation in PA, including multi-agent assistance in document generation and paving the way for further innovation and exploration in this domain.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Proposed Approach
  </h2>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    The usual interaction model between an LLM and an inexperienced user for document generation features a trial-and-error process, aimed at refining the prompt until a satisfying result is reached. Longer prompts are required for longer documents, making it even more difficult to obtain a satisfying result.
Moreover, in detail-rich documents, the performance of LLM agents is bound to decrease when document generation requires many tasks to be completed.
   </p>
  </div>
  <div class="ltx_para" id="S3.p2">
   <p class="ltx_p" id="S3.p2.1">
    Given the requirement for user supervision in a trial-and-error process, we propose a different workflow aimed at minimizing user intervention. The proposed process, iterative in nature, follows the overall structure of a document template, which can be extracted from a pre-existing template document provided by the user as input.
   </p>
  </div>
  <div class="ltx_para" id="S3.p3">
   <p class="ltx_p" id="S3.p3.1">
    The user is allowed to provide an initial prompt to describe the overall expected result. The initial prompt is then refined throughout iterations, to hold all the missing data required to generate the document, accumulating in the original prompt the outcomes of user interventions whenever requested by the agents. The
    <em class="ltx_emph ltx_font_italic" id="S3.p3.1.1">
     accumulated prompt
    </em>
    will serve as a data source throughout the document generation steps.
Following the structure of the template document, the output document is then generated section-by-section, in reading order.
   </p>
  </div>
  <div class="ltx_para" id="S3.p4">
   <p class="ltx_p" id="S3.p4.1">
    During the generic generation step, LLM agents are interrogated to solve fine-grained tasks depending on the availability of the information required to generate semantically suitable content for the current section, according to the semantics provided in the corresponding section in the template document. Each agent is instructed with a previously engineered prompt, describing its task, which is then completed by context-dependent information, depending on the semantics of the current section and the data extracted from the
    <em class="ltx_emph ltx_font_italic" id="S3.p4.1.1">
     accumulated prompt
    </em>
    .
   </p>
  </div>
  <div class="ltx_para" id="S3.p5">
   <p class="ltx_p" id="S3.p5.1">
    The multi-agent framework allows specializing agents as much as necessary to prevent hallucinations, in sections where available contextual pieces of information are prone to provide undesired results (like what could happen in case the provided context is very short or the text is very schematic). Post-processing may be applied to improve the results, especially if a schematic output is expected, by explicitly asking the LLM agents to return specific tokens in case some conditions are met, as a way to force them to not hallucinate and comply with their role in the workflow. Detecting these tokens in the output may help in managing limit cases that would otherwise disrupt the workflow, improving the overall system robustness.
   </p>
  </div>
  <div class="ltx_para" id="S3.p6">
   <p class="ltx_p" id="S3.p6.1">
    User intervention is required only in case the pieces of information retrievable from the
    <em class="ltx_emph ltx_font_italic" id="S3.p6.1.1">
     accumulated prompt
    </em>
    are not enough to comply with the semantics of the current document section, so the frequency of user intervention depends on the quality of the initial prompt. The advantage of this approach is that the
    <em class="ltx_emph ltx_font_italic" id="S3.p6.1.2">
     accumulated prompt
    </em>
    is used only as a data source: in this way, the agent tasked with extracting data from the prompt is less prone to hallucinate when the user prompt is not complete or clear. After user intervention, the new data provided by the user is added to the
    <em class="ltx_emph ltx_font_italic" id="S3.p6.1.3">
     accumulated prompt
    </em>
    , to be stored for future retrievals.
   </p>
  </div>
  <figure class="ltx_figure" id="S3.F2">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="384" id="S3.F2.g1" src="/html/2402.14871/assets/images/Process_horizontal.png" width="598"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 2:
    </span>
    Representation of our multi-agent architecture. The workflow for the generic generation step is highlighted by the bold black arrows.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S3.p7">
   <p class="ltx_p" id="S3.p7.1">
    To ensure flexibility in the emulation of the original document template, the user is allowed to optionally skip the generation of a document section at any time, leaving the accumulated prompt unaltered.
A representation of the workflow obtained according to this interaction model is shown in Fig.
    <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3 Proposed Approach ‣ LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    .
   </p>
  </div>
  <div class="ltx_para" id="S3.p8">
   <p class="ltx_p" id="S3.p8.1">
    Although our work focuses on the components atomically necessary to generate a document section-by-section, the presented interaction model allows the integration of additional agents to manage different aspects of document editing, depending on the level of structuring of the document and the level of expertise required by the specific task assigned to the agent.
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Template Pre-processing
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     The document structure can be extracted on a format-dependent basis, using pre-existing tools. In our case, we used a REST API interface for cloud-based processing using the Adobe Extraction API
     <span class="ltx_note ltx_role_footnote" id="footnote1">
      <sup class="ltx_note_mark">
       1
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         1
        </sup>
        <span class="ltx_tag ltx_tag_note">
         1
        </span>
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.adobe.com/document-services/docs/overview/pdf-extract-api/" target="_blank" title="">
         https://developer.adobe.com/document-services/docs/overview/pdf-extract-api/
        </a>
       </span>
      </span>
     </span>
     , to extract bounding boxes and contents from figures, text blocks, and tables in the document. It is not important to deduce the field semantics at this stage as we only need structural cues for the next steps.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Multi-agent Interaction
   </h3>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     The user is invited to provide an initial prompt giving an overall description and directives for the generated document, such as more general qualities like style or tone of the text or more specific information and data necessary for document generation. It is possible to leave the initial prompt empty, in which case the maximum level of user intervention will be required throughout the generation process.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p2">
    <p class="ltx_p" id="S3.SS2.p2.1">
     The current workflow features a set of three LLM-based agents, each corresponding to a phase of the generic content generation step for a single document section, in order:
     <em class="ltx_emph ltx_font_italic" id="S3.SS2.p2.1.1">
      Section Semantics Identification
     </em>
     ,
     <em class="ltx_emph ltx_font_italic" id="S3.SS2.p2.1.2">
      Information Retrieval
     </em>
     ,
     <em class="ltx_emph ltx_font_italic" id="S3.SS2.p2.1.3">
      Content Generation
     </em>
     .
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p3">
    <p class="ltx_p" id="S3.SS2.p3.1">
     The
     <em class="ltx_emph ltx_font_italic" id="S3.SS2.p3.1.1">
      Section Semantics Identification
     </em>
     step is aimed at identifying the semantics of the current section from the document template. In case the template section contains tokens that need to be replaced in the current document section, which can appear as placeholders, such as "Name", "Surname", "Birthday", "City" or explicitly already populated with data, such as "John", "Doe", "01/01/1970", "Washington", the usual Natural Language Processing pipeline to perform Entity recognition and Semantic parsing on a sentence would generally require performing several preliminary tasks such as Part-of-Speech tagging, Named Entity Recognition, Relationship Extraction, before the actual semantic tagging of tokens, to build a semantic representation of the sentence good enough to extract the tokens of interest correctly.
Using LLMs for this task allows instead exploiting their Commonsense Knowledge
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib6" title="">
       6
      </a>
      ]
     </cite>
     . This step is therefore managed by the first LLM agent, the
     <em class="ltx_emph ltx_font_italic" id="S3.SS2.p3.1.2">
      Semantics Identification agent
     </em>
     , which autonomously identifies the semantics of the current section from the template document, identifying replaceable data in the provided template section.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p4">
    <p class="ltx_p" id="S3.SS2.p4.1">
     The agent output is a list of directives and instructions on how to reproduce the semantics of the corresponding template, whenever the text allows deducing it, serving as instructions for the
     <em class="ltx_emph ltx_font_italic" id="S3.SS2.p4.1.1">
      Content Generation
     </em>
     phase, along with a list of identified replaceable data. In case the semantics of any of such data are identified, the list of instructions will contain directives on how to add them to the text generated for the corresponding section in the output document. Using only a schematic representation of the semantics as an output causes a degradation in the quality of the generated content downstream, therefore the instructions provided by the agent are discursive and verbose. The output of this phase will be provided to the
     <em class="ltx_emph ltx_font_italic" id="S3.SS2.p4.1.2">
      Information Retrieval agent
     </em>
     , to identify data required in the current section, and the
     <em class="ltx_emph ltx_font_italic" id="S3.SS2.p4.1.3">
      Content Generation agent
     </em>
     , enabling it to reconstruct the semantics of this template section in the output document.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p5">
    <p class="ltx_p" id="S3.SS2.p5.1">
     It should be noted that the list of replaceable data might contain data that is already available in the accumulated user prompt as well as missing data.
For this reason, the second phase, destined to
     <em class="ltx_emph ltx_font_italic" id="S3.SS2.p5.1.1">
      Information Retrieval
     </em>
     , is aimed at using the available information to retrieve the data specifically required by the current document section, according to the semantic cues previously extracted.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p6">
    <p class="ltx_p" id="S3.SS2.p6.1">
     The second agent, the
     <em class="ltx_emph ltx_font_italic" id="S3.SS2.p6.1.1">
      Information Retrieval agent
     </em>
     , is specialized in extracting the required information from the accumulated prompt, which at the first step coincides with the initial prompt, and determining which data could not be retrieved. In case it is not possible to find all the required data (according to the instructions from the
     <em class="ltx_emph ltx_font_italic" id="S3.SS2.p6.1.2">
      Semantics Identification agent
     </em>
     ), user intervention is required, to specify through a textual prompt the actual replacement values for the missing data. The result of this interaction is added to the
     <em class="ltx_emph ltx_font_italic" id="S3.SS2.p6.1.3">
      accumulated prompt
     </em>
     , to be used by the
     <em class="ltx_emph ltx_font_italic" id="S3.SS2.p6.1.4">
      Content Generation agent
     </em>
     as a data source, or to be stored for the
     <em class="ltx_emph ltx_font_italic" id="S3.SS2.p6.1.5">
      Information Retrieval
     </em>
     phase of later iterations. Therefore, if the user decides not to specify some information, they will not be integrated into the result and will be ignored in the output content.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p7">
    <p class="ltx_p" id="S3.SS2.p7.1">
     As a last step, during the
     <em class="ltx_emph ltx_font_italic" id="S3.SS2.p7.1.1">
      Content Generation
     </em>
     phase, the
     <em class="ltx_emph ltx_font_italic" id="S3.SS2.p7.1.2">
      Content Generation agent
     </em>
     is therefore instructed to generate the textual content for the current document section, using the
     <em class="ltx_emph ltx_font_italic" id="S3.SS2.p7.1.3">
      accumulated prompt
     </em>
     , now enriched with the required information, and the instructions coming from the
     <em class="ltx_emph ltx_font_italic" id="S3.SS2.p7.1.4">
      Semantics Identification agent
     </em>
     .
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F3">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="588" id="S3.F3.g1" src="/html/2402.14871/assets/images/Chat_instance.png" width="568"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 3:
     </span>
     Representation of a generation step instance. Notice how the
     <em class="ltx_emph ltx_font_italic" id="S3.F3.2.1">
      accumulated prompt
     </em>
     is enriched with the missing data provided by the user.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S3.SS2.p8">
    <p class="ltx_p" id="S3.SS2.p8.1">
     At the end, both an output document and a refined prompt are obtained. In particular, the refined prompt will contain all the missing data identified throughout the generation process. An example of a workflow instance is shown in Fig.
     <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3.2 Multi-agent Interaction ‣ 3 Proposed Approach ‣ LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     .
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.3
    </span>
    Document Post-processing
   </h3>
   <div class="ltx_para" id="S3.SS3.p1">
    <p class="ltx_p" id="S3.SS3.p1.1">
     During the document generation, only text content is processed, while the same structure of the original template document is used, including figures and other graphical elements, which can be skipped by the user during the generation process to avoid including them. Thanks to the modularity of this framework, additional agents can be added to improve the graphical appearance and improve its dependence on the context and the semantics of the user requirements, and additional processing steps can be added downstream to improve the graphical appearance of the result (for example by generating context-dependent images) but this is outside of the scope of the current work.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Experimental Evaluation
  </h2>
  <div class="ltx_para" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    Agents were built using the latest OpenAI
    <em class="ltx_emph ltx_font_italic" id="S4.p1.1.1">
     GPT 3.5 Turbo
    </em>
    model (
    <em class="ltx_emph ltx_font_italic" id="S4.p1.1.2">
     gpt-3.5-turbo-1106
    </em>
    ) with a context window of 16,385 tokens and a maximum of 4,096 output tokens.
   </p>
  </div>
  <figure class="ltx_table" id="S4.T1">
   <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
    <thead class="ltx_thead">
     <tr class="ltx_tr" id="S4.T1.1.1.1">
      <th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_t" colspan="2" id="S4.T1.1.1.1.1">
       <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1">
        Prompt engineering for the Semantics Identification Agent
       </span>
      </th>
     </tr>
    </thead>
    <tbody class="ltx_tbody">
     <tr class="ltx_tr" id="S4.T1.1.2.1">
      <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="2" id="S4.T1.1.2.1.1">
       <span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.1.1">
        Template text
       </span>
       :
       <em class="ltx_emph ltx_font_italic" id="S4.T1.1.2.1.1.2">
        Your Name
       </em>
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.1.3.2">
      <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.1.3.2.1">
       <span class="ltx_inline-block ltx_align_top" id="S4.T1.1.3.2.1.1">
        <span class="ltx_p" id="S4.T1.1.3.2.1.1.1" style="width:303.5pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.1.1.1.1">
          Agent task
         </span>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.1.3.2.2">
       <span class="ltx_inline-block ltx_align_top" id="S4.T1.1.3.2.2.1">
        <span class="ltx_p" id="S4.T1.1.3.2.2.1.1" style="width:130.1pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.2.1.1.1">
          Agent answer
         </span>
        </span>
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.1.4.3">
      <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.1.4.3.1">
       <span class="ltx_inline-block ltx_align_top" id="S4.T1.1.4.3.1.1">
        <span class="ltx_p" id="S4.T1.1.4.3.1.1.1" style="width:303.5pt;">
         You are an assistant helping to understand the structure content of a template document. Respond with the content that might be written in the document.
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.1.4.3.2">
       <span class="ltx_inline-block ltx_align_top" id="S4.T1.1.4.3.2.1">
        <span class="ltx_p" id="S4.T1.1.4.3.2.1.1" style="width:130.1pt;">
         [Your Name]
         <br class="ltx_break"/>
         [Your Address]
         <br class="ltx_break"/>
         [City, State, Zip Code]
         <br class="ltx_break"/>
         [Email Address]
         <br class="ltx_break"/>
         [Phone Number]
        </span>
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.1.5.4">
      <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.1.5.4.1">
       <span class="ltx_inline-block ltx_align_top" id="S4.T1.1.5.4.1.1">
        <span class="ltx_p" id="S4.T1.1.5.4.1.1.1" style="width:303.5pt;">
         You are an assistant helping to understand the structure content of a template document. Respond with the content that might be written in the document. Give just the action to do.
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.1.5.4.2">
       <span class="ltx_inline-block ltx_align_top" id="S4.T1.1.5.4.2.1">
        <span class="ltx_p" id="S4.T1.1.5.4.2.1.1" style="width:130.1pt;">
         Fill in your full legal name.
        </span>
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.1.6.5">
      <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.6.5.1">
       <span class="ltx_inline-block ltx_align_top" id="S4.T1.1.6.5.1.1">
        <span class="ltx_p" id="S4.T1.1.6.5.1.1.1" style="width:303.5pt;">
         You are an assistant helping to understand the structure content of a template document. Respond with the content that might be written in the document. Give just the action to do. For example, if you read "Location", the output will be "Add the location"; if you read "Dear Someone", the output will be "Add salutation".
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="S4.T1.1.6.5.2">
       <span class="ltx_inline-block ltx_align_top" id="S4.T1.1.6.5.2.1">
        <span class="ltx_p" id="S4.T1.1.6.5.2.1.1" style="width:130.1pt;">
         Add your full name
        </span>
       </span>
      </td>
     </tr>
    </tbody>
   </table>
   <br class="ltx_break"/>
   <figcaption class="ltx_caption">
    <span class="ltx_tag ltx_tag_table">
     Table 1:
    </span>
    Agent task prompt refinement for Semantics Identification Agent, with template text "
    <em class="ltx_emph ltx_font_italic" id="S4.T1.3.1">
     Your name
    </em>
    "
    <br class="ltx_break"/>
    .
   </figcaption>
  </figure>
  <div class="ltx_para" id="S4.p2">
   <p class="ltx_p" id="S4.p2.1">
    Experiments are aimed at designing an effective prompt for each agent, by performing a trial-and-error process to obtain a correct and contextually appropriate response from the LLMs.
   </p>
  </div>
  <div class="ltx_para" id="S4.p3">
   <p class="ltx_p" id="S4.p3.1">
    The evaluation process tests the agents’ ability to understand the assigned tasks, fine-tuning each agent’s ability to conform to the expected outputs, given their specific role in the generation workflow.
   </p>
  </div>
  <div class="ltx_para" id="S4.p4">
   <p class="ltx_p" id="S4.p4.1">
    The process of crafting an effective prompt that maximizes conformity of the output of a Large Language Model to the original requirements has the potential to dramatically improve the quality of the generated output, especially when this output is used in an intermediate step of a processing pipeline.
The baseline version of the prompt point is obtained by following general prompt engineering guidelines
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib16" title="">
      16
     </a>
     ]
    </cite>
    , the most important one consisting of writing in a clear and imperative tone the task assignment for the LLM. Starting from the baseline and analyzing the response, it is then possible to refine the prompt in incremental steps, by adding instructions to force the model toward a more desirable output.
   </p>
  </div>
  <div class="ltx_para" id="S4.p5">
   <p class="ltx_p" id="S4.p5.1">
    The most important undesirable behavior to keep under control is the chance for hallucination, which in this case can alter the data in the generated content and therefore provide false and unsatisfying textual content for the document section being generated.
   </p>
  </div>
  <div class="ltx_para" id="S4.p6">
   <p class="ltx_p" id="S4.p6.1">
    Each prompt consists of two components: a "
    <em class="ltx_emph ltx_font_italic" id="S4.p6.1.1">
     system prompt
    </em>
    " containing the pre-assigned instructions used to inform the agent of its specific task, instructing it with its role and the guidelines for the generation of its output, and a "
    <em class="ltx_emph ltx_font_italic" id="S4.p6.1.2">
     context-specific prompt
    </em>
    ", which contains the actual context-specific text for the current instance of the agent’s task.
   </p>
  </div>
  <div class="ltx_para" id="S4.p7">
   <p class="ltx_p" id="S4.p7.1">
    Obtaining a robustly engineered prompt requires careful tuning, considering the variety of possible inputs in a semi-structured document. On one hand, very schematic text consisting of single entities (such as "First name", "Last name" and other specific data) poses a challenge and requires more carefully engineered prompts. On the other hand, output should still be desirable in the case of less schematic, more discursive sections.
The generic input for the generation step, which will be forwarded to the
    <em class="ltx_emph ltx_font_italic" id="S4.p7.1.1">
     Semantics identification agent
    </em>
    , is the textual content of the currently processed template document section, which is necessary to obtain semantic cues about the desired output text.
   </p>
  </div>
  <div class="ltx_para" id="S4.p8">
   <p class="ltx_p" id="S4.p8.1">
    The output of the generation step coincides with the output from the
    <em class="ltx_emph ltx_font_italic" id="S4.p8.1.1">
     Content generation agent
    </em>
    .
   </p>
  </div>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    Semantics Identification Agent
   </h3>
   <div class="ltx_para" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     The
     <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.1.1">
      Semantics Identification agent
     </em>
     is tasked with producing a discursive and fluent explanation of the semantics of the current document section and most importantly of the instructions and directives for the generation of the content to be included in the output document, based on the expected semantics.
To avoid hallucinations it is extremely important to have a non-schematic and unspecific description of the semantics of the current document section.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p2">
    <p class="ltx_p" id="S4.SS1.p2.1">
     We show the prompt engineering process based on the answers of the same agent to different
     <em class="ltx_emph ltx_font_italic" id="S4.SS1.p2.1.1">
      system prompts
     </em>
     , progressively refined in incremental steps, for different template texts, in order of increasing verbosity of the
     <em class="ltx_emph ltx_font_italic" id="S4.SS1.p2.1.2">
      context-specific prompt
     </em>
     (therefore in decreasing order of schematism).
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p3">
    <p class="ltx_p" id="S4.SS1.p3.1">
     The first more schematic template text is "
     <em class="ltx_emph ltx_font_italic" id="S4.SS1.p3.1.1">
      Your name
     </em>
     ", as could be found in any template document for a generic letter, in a field where the name of the user is required. The relevant results for this refinement process are provided in Tables
     <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4 Experimental Evaluation ‣ LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     .
     <br class="ltx_break"/>
    </p>
   </div>
   <figure class="ltx_table" id="S4.T2">
    <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="S4.T2.1.1.1">
       <th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_t" colspan="2" id="S4.T2.1.1.1.1">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1">
         Prompt engineering for the Semantics Identification Agent
        </span>
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S4.T2.1.2.1">
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="2" id="S4.T2.1.2.1.1">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.2.1.1.1">
         Template text
        </span>
        :
        <em class="ltx_emph ltx_font_italic" id="S4.T2.1.2.1.1.2">
         Dear Mr./Ms. (Lastname):
        </em>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.1.3.2">
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.1.3.2.1">
        <span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.2.1.1">
         <span class="ltx_p" id="S4.T2.1.3.2.1.1.1" style="width:238.5pt;">
          <span class="ltx_text ltx_font_bold" id="S4.T2.1.3.2.1.1.1.1">
           Agent task
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.3.2.2">
        <span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.2.2.1">
         <span class="ltx_p" id="S4.T2.1.3.2.2.1.1" style="width:195.1pt;">
          <span class="ltx_text ltx_font_bold" id="S4.T2.1.3.2.2.1.1.1">
           Agent answer
          </span>
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.1.4.3">
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.1.4.3.1">
        <span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.3.1.1">
         <span class="ltx_p" id="S4.T2.1.4.3.1.1.1" style="width:238.5pt;">
          You are an assistant helping to understand the structure content of a template document. Respond with the content that might be written in the document.
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.4.3.2">
        <span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.3.2.1">
         <span class="ltx_p" id="S4.T2.1.4.3.2.1.1" style="width:195.1pt;">
          I hope this message finds you well. I am writing to inform you about the upcoming meeting scheduled for (date) at (time) in (location). The purpose of this meeting is to discuss (agenda items). Your presence and input at this meeting are crucial, so please make every effort to attend. If you are unable to attend, please notify me as soon as possible. Thank you for your attention to this matter, and I look forward to seeing you at the meeting.
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.1.5.4">
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.1.5.4.1">
        <span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.4.1.1">
         <span class="ltx_p" id="S4.T2.1.5.4.1.1.1" style="width:238.5pt;">
          You are an assistant helping to understand the structure content of a template document. Respond with the content that might be written in the document. Give just the action to do.
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.5.4.2">
        <span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.4.2.1">
         <span class="ltx_p" id="S4.T2.1.5.4.2.1.1" style="width:195.1pt;">
          Insert the recipient’s last name in the salutation.
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.1.6.5">
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.6.5.1">
        <span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.5.1.1">
         <span class="ltx_p" id="S4.T2.1.6.5.1.1.1" style="width:238.5pt;">
          You are an assistant helping to understand the structure content of a template document. Respond with the content that might be written in the document. Give just the action to do. For example, if you read "Location", the output will be "Add the location"; if you read "Dear Someone", the output will be "Add salutation".
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="S4.T2.1.6.5.2">
        <span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.5.2.1">
         <span class="ltx_p" id="S4.T2.1.6.5.2.1.1" style="width:195.1pt;">
          Add salutation
         </span>
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <br class="ltx_break"/>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_table">
      Table 2:
     </span>
     Agent task prompt refinement for Semantics Identification Agent, with template text "
     <em class="ltx_emph ltx_font_italic" id="S4.T2.3.1">
      Dear Mr./Ms. (Lastname):
     </em>
     ".
     <br class="ltx_break"/>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S4.SS1.p4">
    <p class="ltx_p" id="S4.SS1.p4.1">
     The initial baseline version of the prompt describes the agent’s task without specifying that the output should contain instructions on how to reproduce the semantics of the provided template text.
The agent response is heavily affected by hallucinations, answering with plausible text for a generic letter, without giving a generic description of its semantics, clearly missing its objective. From the analysis of the answer, it can be assumed that the agent misinterprets the request "
     <em class="ltx_emph ltx_font_italic" id="S4.SS1.p4.1.1">
      Respond with the content that could be written
     </em>
     " as a request to provide the actual content of the generated document.
    </p>
   </div>
   <figure class="ltx_table" id="S4.T3">
    <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="S4.T3.1.1.1">
       <th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_t" colspan="2" id="S4.T3.1.1.1.1">
        <span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1">
         Prompt engineering for the Semantics Identification Agent
        </span>
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S4.T3.1.2.1">
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="2" id="S4.T3.1.2.1.1">
        <span class="ltx_inline-block ltx_align_top" id="S4.T3.1.2.1.1.1">
         <span class="ltx_p" id="S4.T3.1.2.1.1.1.1" style="width:433.6pt;">
          <span class="ltx_text ltx_font_bold" id="S4.T3.1.2.1.1.1.1.1">
           Template text:
          </span>
          <em class="ltx_emph ltx_font_italic" id="S4.T3.1.2.1.1.1.1.2">
           First paragraphs : In your opening paragraph, concisely convey your enthusiasm for the role
and the organization, while emphasizing your pertinent educational background. Aim to engage the
reader’s interest, prompting them to delve deeper into your resume. This section serves as a preview,
showcasing your qualifications and setting the stage for the detailed skills and experiences outlined in
your resume. Highlight your top skills that align with the job requirements, and showcase your
familiarity with the organization by mentioning specific aspects such as their mission, ongoing
projects, or strategic objectives, linking them to how your abilities and interests can add value
to their team.
          </em>
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.1.3.2">
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T3.1.3.2.1">
        <span class="ltx_inline-block ltx_align_top" id="S4.T3.1.3.2.1.1">
         <span class="ltx_p" id="S4.T3.1.3.2.1.1.1" style="width:195.1pt;">
          <span class="ltx_text ltx_font_bold" id="S4.T3.1.3.2.1.1.1.1">
           Agent task
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.1.3.2.2">
        <span class="ltx_inline-block ltx_align_top" id="S4.T3.1.3.2.2.1">
         <span class="ltx_p" id="S4.T3.1.3.2.2.1.1" style="width:238.5pt;">
          <span class="ltx_text ltx_font_bold" id="S4.T3.1.3.2.2.1.1.1">
           Agent answer
          </span>
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.1.4.3">
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T3.1.4.3.1">
        <span class="ltx_inline-block ltx_align_top" id="S4.T3.1.4.3.1.1">
         <span class="ltx_p" id="S4.T3.1.4.3.1.1.1" style="width:195.1pt;">
          You are an assistant helping to understand the structure content of a template document. Respond with the content that might be written in the document.
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.1.4.3.2">
        <span class="ltx_inline-block ltx_align_top" id="S4.T3.1.4.3.2.1">
         <span class="ltx_p" id="S4.T3.1.4.3.2.1.1" style="width:238.5pt;">
          Dear [Hiring Manager’s Name],
I am writing to express my genuine enthusiasm for the [Job Title] position at [Organization Name]. With a strong educational background in [relevant field], I am confident in my ability to contribute to the success of your team. The innovative projects and strategic objectives of [Organization Name] resonate with my professional interests, and I am eager to bring my skills and expertise to support your mission.
My top skills in [specific skills] and [specific skills] align closely
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.1.5.4">
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T3.1.5.4.1">
        <span class="ltx_inline-block ltx_align_top" id="S4.T3.1.5.4.1.1">
         <span class="ltx_p" id="S4.T3.1.5.4.1.1.1" style="width:195.1pt;">
          You are an assistant helping to understand the structure content of a template document. Respond with the content that might be written in the document. Give just the action to do.
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.1.5.4.2">
        <span class="ltx_inline-block ltx_align_top" id="S4.T3.1.5.4.2.1">
         <span class="ltx_p" id="S4.T3.1.5.4.2.1.1" style="width:238.5pt;">
          Express enthusiasm for the role and the organization, emphasizing your educational background and top skills that align with the job requirements. Showcase your familiarity with the organization by mentioning specific aspects such as their mission, ongoing projects, or strategic objectives, and link them to how your abilities and interests can add value to their team.
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.1.6.5">
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.1.6.5.1">
        <span class="ltx_inline-block ltx_align_top" id="S4.T3.1.6.5.1.1">
         <span class="ltx_p" id="S4.T3.1.6.5.1.1.1" style="width:195.1pt;">
          You are an assistant helping to understand the structure content of a template document. Respond with the content that might be written in the document. Give just the action to do. For example, if you read "Location", the output will be "Add the location"; if you read "Dear Someone", the output will be "Add salutation".
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="S4.T3.1.6.5.2">
        <span class="ltx_inline-block ltx_align_top" id="S4.T3.1.6.5.2.1">
         <span class="ltx_p" id="S4.T3.1.6.5.2.1.1" style="width:238.5pt;">
          Write an enthusiastic opening paragraph that emphasizes your relevant educational background and showcases your qualifications and skills that align with the job requirements. Mention specific aspects of the organization such as their mission, ongoing projects, or strategic objectives, and link them to how your abilities and interests can add value to their team.
         </span>
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <br class="ltx_break"/>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_table">
      Table 3:
     </span>
     Agent task prompt refinement for Semantics Identification Agent, with a long and very discursive template text.
     <br class="ltx_break"/>
    </figcaption>
   </figure>
   <figure class="ltx_table ltx_figure_panel" id="S4.SS1.1">
    <div class="ltx_flex_figure ltx_flex_table">
     <div class="ltx_flex_cell ltx_flex_size_1">
      <table class="ltx_tabular ltx_figure_panel ltx_align_middle" id="S4.SS1.1.1">
       <tbody class="ltx_tbody">
        <tr class="ltx_tr" id="S4.SS1.1.1.1.1">
         <td class="ltx_td ltx_align_center ltx_align_top ltx_border_t" id="S4.SS1.1.1.1.1.1">
          <span class="ltx_text ltx_font_bold" id="S4.SS1.1.1.1.1.1.1">
           Prompt Engineering for the Information retrieval Agent
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.SS1.1.1.2.2">
         <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="S4.SS1.1.1.2.2.1">
          <span class="ltx_inline-block ltx_align_top" id="S4.SS1.1.1.2.2.1.1">
           <span class="ltx_p" id="S4.SS1.1.1.2.2.1.1.1" style="width:433.6pt;">
            <span class="ltx_text ltx_font_bold" id="S4.SS1.1.1.2.2.1.1.1.1">
             Accumulated prompt
            </span>
            :
"My name is John Doe, i want write a letter for Random University, i am a student in Computer Science"
           </span>
          </span>
         </td>
        </tr>
       </tbody>
      </table>
     </div>
     <div class="ltx_flex_break">
     </div>
     <div class="ltx_flex_cell ltx_flex_size_1">
      <p class="ltx_p ltx_figure_panel" id="S4.SS1.1.2">
       The addition of a clear instruction "
       <em class="ltx_emph ltx_font_italic" id="S4.SS1.1.2.1">
        Give just the action to do
       </em>
       " in the second version of the system prompt definition gives the agent a clear explanation on how to perform its task. This sentence is crucial to improve the conformity of the agent’s response to its original task, instructing it to provide a description of the action to perform to reproduce the semantics of the template section.
      </p>
     </div>
     <div class="ltx_flex_break">
     </div>
     <div class="ltx_flex_cell ltx_flex_size_1">
      <p class="ltx_p ltx_figure_panel" id="S4.SS1.1.3">
       The relevant results for template text "
       <em class="ltx_emph ltx_font_italic" id="S4.SS1.1.3.1">
        Dear Mr./Ms.(Lastname)
       </em>
       " are provided in Table
       <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4.1 Semantics Identification Agent ‣ 4 Experimental Evaluation ‣ LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain">
        <span class="ltx_text ltx_ref_tag">
         2
        </span>
       </a>
       . Here, the output to the second version of the prompt could be considered correct but it could be even more synthetic, considering that the optimal response would just be and instruction to "
       <em class="ltx_emph ltx_font_italic" id="S4.SS1.1.3.2">
        Add a salutation
       </em>
       ".
      </p>
     </div>
     <div class="ltx_flex_break">
     </div>
     <div class="ltx_flex_break">
     </div>
     <div class="ltx_flex_cell ltx_flex_size_1">
      <p class="ltx_p ltx_figure_panel" id="S4.SS1.1.4">
       A particularly interesting study case consists of using the engineered prompt and providing a text without a semantic value, such as a placeholder text:
       <span class="ltx_text ltx_font_bold" id="S4.SS1.1.4.1">
        Template text:
       </span>
       <em class="ltx_emph ltx_font_italic" id="S4.SS1.1.4.2">
        Lorem ipsum dolor sit amet, consectetur adipiscing elit…
       </em>
       The template text is the placeholder text ("
       <em class="ltx_emph ltx_font_italic" id="S4.SS1.1.4.3">
        Lorem Ipsum …
       </em>
       ") typically used to test graphical templates. The agent answer perfectly conforms to the prescribed task as this placeholder is usually found in the discursive sections of document templates:
       <span class="ltx_text ltx_font_bold" id="S4.SS1.1.4.4">
        Agent Answer
       </span>
       :
       <em class="ltx_emph ltx_font_italic" id="S4.SS1.1.4.5">
        The missing information to satisfy the request is the content of the main body of the document.
       </em>
      </p>
     </div>
     <div class="ltx_flex_break">
     </div>
     <div class="ltx_flex_cell ltx_flex_size_1">
      <section class="ltx_subsection ltx_figure_panel" id="S4.SS3">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         4.3
        </span>
        Content Generation Agent
       </h3>
       <div class="ltx_para" id="S4.SS3.p1">
        <p class="ltx_p" id="S4.SS3.p1.1">
         The
         <em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.1">
          Content Generation Agent
         </em>
         is tasked with generating the text in the output document following the instructions of the other agents and using the information contained in the
         <em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.2">
          accumulated prompt
         </em>
         .
The
         <em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.3">
          system prompt
         </em>
         for this agent, as in the cases of the previous agents, contains clear instructions and constraints to obtain a desirable output.
The baseline prompt is:
         <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.4">
          Agent task
         </span>
         :
        </p>
       </div>
       <div class="ltx_para" id="S4.SS3.p2">
        <p class="ltx_p" id="S4.SS3.p2.1">
         <em class="ltx_emph ltx_font_italic" id="S4.SS3.p2.1.1">
          You are an assistant with the purpose of generating a document
with the available information.
You have the following information:
          <br class="ltx_break"/>
         </em>
         <code class="ltx_verbatim ltx_font_typewriter" id="S4.SS3.p2.1.2">
          {Accumulated prompt}
         </code>
         <br class="ltx_break"/>
         <em class="ltx_emph ltx_font_italic" id="S4.SS3.p2.1.3">
          For example, if you read "Add salutation name", write the salutation only.
         </em>
         where
         <code class="ltx_verbatim ltx_font_typewriter" id="S4.SS3.p2.1.4">
          {Accumulated prompt}
         </code>
         is the currently
         <em class="ltx_emph ltx_font_italic" id="S4.SS3.p2.1.5">
          accumulated prompt
         </em>
         .
With this baseline, the model tends to add initial greetings and a conclusion. This deviation from the original task could be solved by forbidding the addition of out-of-context information, by adding the following sentence
         <em class="ltx_emph ltx_font_italic" id="S4.SS3.p2.1.6">
          Remember that in an opening paragraph, it is absolutely forbidden
to write "dear someone", or "sincerely" at the end of the document.
         </em>
         As for the previous agents, adding examples of the desired result drives the agent’s output to the desired format.
        </p>
       </div>
       <figure class="ltx_figure" id="S4.F4">
        <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="377" id="S4.F4.g1" src="/html/2402.14871/assets/images/Salutation_1_example.png" width="556"/>
        <figcaption class="ltx_caption ltx_centering">
         <span class="ltx_tag ltx_tag_figure">
          Figure 4:
         </span>
         Agent responses throughout a single generation step, starting from the template text: "
         <em class="ltx_emph ltx_font_italic" id="S4.F4.2.1">
          Your name
         </em>
         ". The figure represents a single generation step for a section of the document, showing an example of successful generation using the engineered prompt. In this case all information is retrieved in the original user prompt, so no information is asked to the user.
        </figcaption>
       </figure>
       <section class="ltx_subsection" id="S4.SS4">
        <h3 class="ltx_title ltx_title_subsection">
         <span class="ltx_tag ltx_tag_subsection">
          4.4
         </span>
         Prompt-engineered results
        </h3>
        <div class="ltx_para" id="S4.SS4.p1">
         <p class="ltx_p" id="S4.SS4.p1.1">
          Examples of a full processing iteration of template sections with the refined prompts are shown in Figures
          <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.3 Content Generation Agent ‣ 4.1 Semantics Identification Agent ‣ 4 Experimental Evaluation ‣ LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain">
           <span class="ltx_text ltx_ref_tag">
            4
           </span>
          </a>
          and
          <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.4 Prompt-engineered results ‣ 4.3 Content Generation Agent ‣ 4.1 Semantics Identification Agent ‣ 4 Experimental Evaluation ‣ LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain">
           <span class="ltx_text ltx_ref_tag">
            5
           </span>
          </a>
          .
         </p>
        </div>
        <div class="ltx_para" id="S4.SS4.p2">
         <p class="ltx_p" id="S4.SS4.p2.1">
          Fig.
          <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.3 Content Generation Agent ‣ 4.1 Semantics Identification Agent ‣ 4 Experimental Evaluation ‣ LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain">
           <span class="ltx_text ltx_ref_tag">
            4
           </span>
          </a>
          in particular presents a case in which, thanks to the prompt engineering process, the
          <em class="ltx_emph ltx_font_italic" id="S4.SS4.p2.1.1">
           Content Generation agent
          </em>
          is able to retrieve the necessary data from the
          <em class="ltx_emph ltx_font_italic" id="S4.SS4.p2.1.2">
           accumulated prompt
          </em>
          without asking for user intervention, then using the instructions for the reproduction of the semantics of the document section, provided by the
          <em class="ltx_emph ltx_font_italic" id="S4.SS4.p2.1.3">
           Semantics Identification agent
          </em>
          , to generate a desirable result.
         </p>
        </div>
        <div class="ltx_para" id="S4.SS4.p3">
         <p class="ltx_p" id="S4.SS4.p3.1">
          Fig.
          <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.4 Prompt-engineered results ‣ 4.3 Content Generation Agent ‣ 4.1 Semantics Identification Agent ‣ 4 Experimental Evaluation ‣ LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain">
           <span class="ltx_text ltx_ref_tag">
            5
           </span>
          </a>
          instead presents a case in which some missing information is detected, which requires user intervention to provide such missing data, which is then added to the
          <em class="ltx_emph ltx_font_italic" id="S4.SS4.p3.1.1">
           accumulated prompt
          </em>
          .
         </p>
        </div>
        <div class="ltx_para" id="S4.SS4.p4">
         <p class="ltx_p" id="S4.SS4.p4.1">
          In both cases, a dramatic improvement in the quality of the agents’ responses was obtained by adding examples for the structure of the expected result.
         </p>
        </div>
        <figure class="ltx_figure" id="S4.F5">
         <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="364" id="S4.F5.g1" src="/html/2402.14871/assets/images/Salutation_2_example.png" width="538"/>
         <figcaption class="ltx_caption ltx_centering">
          <span class="ltx_tag ltx_tag_figure">
           Figure 5:
          </span>
          Agent responses throughout a single generation step, starting from the template text: "
          <em class="ltx_emph ltx_font_italic" id="S4.F5.2.1">
           Dear Mr./Ms.(Lastname)
          </em>
          ". In this case, user intervention is required to add missing information.
         </figcaption>
        </figure>
        <section class="ltx_section" id="S5">
         <h2 class="ltx_title ltx_title_section">
          <span class="ltx_tag ltx_tag_section">
           5
          </span>
          Conclusions
         </h2>
         <div class="ltx_para" id="S5.p1">
          <p class="ltx_p" id="S5.p1.1">
           This work presents a workflow and an interaction framework for the LLM-assisted multi-agent generation of a semi-structured document in the PA domain, with limited human supervision.
          </p>
         </div>
         <div class="ltx_para" id="S5.p2">
          <p class="ltx_p" id="S5.p2.1">
           We exploited the capabilities of a Large Language Model, taking advantage of a multi-agent architecture, involving three roles, namely, a
           <span class="ltx_text ltx_font_italic" id="S5.p2.1.1">
            Semantics Identification Agent
           </span>
           , an
           <span class="ltx_text ltx_font_italic" id="S5.p2.1.2">
            Information Retrieval Agent
           </span>
           , and, a
           <span class="ltx_text ltx_font_italic" id="S5.p2.1.3">
            Content Generation Agent
           </span>
           . With the resulting architecture, we highlighted the necessary prompt refinement process to obtain desirable results for each specific agent role in the presented workflow. The approach is successful in both schematic and verbose document sections providing enough cues about their semantics.
          </p>
         </div>
         <div class="ltx_para" id="S5.p3">
          <p class="ltx_p" id="S5.p3.1">
           Example-based document generation proves to be perfectly adaptable to the PA scenario, where a progressive transition to Artificial Intelligence tools is expected in the next years.
          </p>
         </div>
         <div class="ltx_para" id="S5.p4">
          <p class="ltx_p" id="S5.p4.1">
           The paradigm shown in the present work could easily be adapted to a plethora of contexts, reducing the workload for human experts who can act as supervisors of the job that a pool of synthetic agents can carry out. This pushes forward the boundaries of the contribution that a large language model can provide in an office context and also represents a notable starting point for research in the field of multi-role architecture for everyday tasks.
          </p>
         </div>
         <section class="ltx_bibliography" id="bib">
          <h2 class="ltx_title ltx_title_bibliography">
           References
          </h2>
          <ul class="ltx_biblist">
           <li class="ltx_bibitem" id="bib.bib1">
            <span class="ltx_tag ltx_tag_bibitem">
             [1]
            </span>
            <span class="ltx_bibblock">
             Bsharat, S.M., Myrzakhan, A., Shen, Z.: Principled instructions are all you need for questioning llama-1/2, gpt-3.5/4 (2024)
            </span>
           </li>
           <li class="ltx_bibitem" id="bib.bib2">
            <span class="ltx_tag ltx_tag_bibitem">
             [2]
            </span>
            <span class="ltx_bibblock">
             Chen, J., Lin, H., Han, X., Sun, L.: Benchmarking large language models in retrieval-augmented generation. arXiv preprint arXiv:2309.01431 (2023)
            </span>
           </li>
           <li class="ltx_bibitem" id="bib.bib3">
            <span class="ltx_tag ltx_tag_bibitem">
             [3]
            </span>
            <span class="ltx_bibblock">
             Cobbe, J., Singh, J.: Artificial intelligence as a service: Legal responsibilities, liabilities, and policy challenges. Computer Law &amp; Security Review
             <span class="ltx_text ltx_font_bold" id="bib.bib3.1.1">
              42
             </span>
             , 105573 (2021)
            </span>
           </li>
           <li class="ltx_bibitem" id="bib.bib4">
            <span class="ltx_tag ltx_tag_bibitem">
             [4]
            </span>
            <span class="ltx_bibblock">
             Hegselmann, S., Buendia, A., Lang, H., Agrawal, M., Jiang, X., Sontag, D.: Tabllm: Few-shot classification of tabular data with large language models. In: Ruiz, F., Dy, J., van de Meent, J.W. (eds.) Proceedings of The 26th International Conference on Artificial Intelligence and Statistics. Proceedings of Machine Learning Research, vol. 206, pp. 5549–5581. PMLR (25–27 Apr 2023),
             <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v206/hegselmann23a.html" target="_blank" title="">
              https://proceedings.mlr.press/v206/hegselmann23a.html
             </a>
            </span>
           </li>
           <li class="ltx_bibitem" id="bib.bib5">
            <span class="ltx_tag ltx_tag_bibitem">
             [5]
            </span>
            <span class="ltx_bibblock">
             Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language models are zero-shot reasoners. In: Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., Oh, A. (eds.) Advances in Neural Information Processing Systems. vol. 35, pp. 22199–22213. Curran Associates, Inc. (2022)
            </span>
           </li>
           <li class="ltx_bibitem" id="bib.bib6">
            <span class="ltx_tag ltx_tag_bibitem">
             [6]
            </span>
            <span class="ltx_bibblock">
             Krause, S., Stolzenburg, F.: Commonsense reasoning and explainable artificial intelligence using large language models. In: European Conference on Artificial Intelligence. pp. 302–319. Springer (2023)
            </span>
           </li>
           <li class="ltx_bibitem" id="bib.bib7">
            <span class="ltx_tag ltx_tag_bibitem">
             [7]
            </span>
            <span class="ltx_bibblock">
             Li, J., Tang, T., Zhao, W.X., Nie, J.Y., Wen, J.R.: Pretrained language models for text generation: A survey. arXiv preprint arXiv:2201.05273 (2022)
            </span>
           </li>
           <li class="ltx_bibitem" id="bib.bib8">
            <span class="ltx_tag ltx_tag_bibitem">
             [8]
            </span>
            <span class="ltx_bibblock">
             Li, Z., Peng, B., He, P., Galley, M., Gao, J., Yan, X.: Guiding large language models via directional stimulus prompting. arXiv preprint arXiv:2302.11520 (2023)
            </span>
           </li>
           <li class="ltx_bibitem" id="bib.bib9">
            <span class="ltx_tag ltx_tag_bibitem">
             [9]
            </span>
            <span class="ltx_bibblock">
             Peng, R., Liu, K., Yang, P., Yuan, Z., Li, S.: Embedding-based retrieval with llm for effective agriculture information extracting from unstructured data. arXiv preprint arXiv:2308.03107 (2023)
            </span>
           </li>
           <li class="ltx_bibitem" id="bib.bib10">
            <span class="ltx_tag ltx_tag_bibitem">
             [10]
            </span>
            <span class="ltx_bibblock">
             Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners. OpenAI blog
             <span class="ltx_text ltx_font_bold" id="bib.bib10.1.1">
              1
             </span>
             (8),  9 (2019)
            </span>
           </li>
           <li class="ltx_bibitem" id="bib.bib11">
            <span class="ltx_tag ltx_tag_bibitem">
             [11]
            </span>
            <span class="ltx_bibblock">
             Rasal, S.: Llm harmony: Multi-agent communication for problem solving. arXiv preprint arXiv:2401.01312 (2024)
            </span>
           </li>
           <li class="ltx_bibitem" id="bib.bib12">
            <span class="ltx_tag ltx_tag_bibitem">
             [12]
            </span>
            <span class="ltx_bibblock">
             Siciliani, L., Ghizzota, E., Basile, P., Lops, P.: Oie4pa: open information extraction for the public administration. Journal of Intelligent Information Systems pp. 1–22 (2023)
            </span>
           </li>
           <li class="ltx_bibitem" id="bib.bib13">
            <span class="ltx_tag ltx_tag_bibitem">
             [13]
            </span>
            <span class="ltx_bibblock">
             Tang, X., Zong, Y., Zhao, Y., Cohan, A., Gerstein, M.: Struc-bench: Are large language models really good at generating complex structured data? arXiv preprint arXiv:2309.08963 (2023)
            </span>
           </li>
           <li class="ltx_bibitem" id="bib.bib14">
            <span class="ltx_tag ltx_tag_bibitem">
             [14]
            </span>
            <span class="ltx_bibblock">
             Tang, Y., Yang, Y.: Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries. arXiv preprint arXiv:2401.15391 (2024)
            </span>
           </li>
           <li class="ltx_bibitem" id="bib.bib15">
            <span class="ltx_tag ltx_tag_bibitem">
             [15]
            </span>
            <span class="ltx_bibblock">
             Wei, J., Wang, X., Schuurmans, D., Bosma, M., ichter, b., Xia, F., Chi, E., Le, Q.V., Zhou, D.: Chain-of-thought prompting elicits reasoning in large language models. In: Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., Oh, A. (eds.) Advances in Neural Information Processing Systems. vol. 35, pp. 24824–24837. Curran Associates, Inc. (2022)
            </span>
           </li>
           <li class="ltx_bibitem" id="bib.bib16">
            <span class="ltx_tag ltx_tag_bibitem">
             [16]
            </span>
            <span class="ltx_bibblock">
             White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar, A., Spencer-Smith, J., Schmidt, D.C.: A prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv preprint arXiv:2302.11382 (2023)
            </span>
           </li>
          </ul>
         </section>
        </section>
       </section>
      </section>
     </div>
    </div>
   </figure>
  </section>
 </section>
</article>
