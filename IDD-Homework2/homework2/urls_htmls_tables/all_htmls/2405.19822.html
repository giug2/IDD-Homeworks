<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.19822] Improving Object Detector Training on Synthetic Data by Starting With a Strong Baseline Methodology</title><meta property="og:description" content="Collecting and annotating real-world data for the development of object detection models is a time-consuming and expensive process. In the military domain in particular, data collection can also be dangerous or infeasi…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Improving Object Detector Training on Synthetic Data by Starting With a Strong Baseline Methodology">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Improving Object Detector Training on Synthetic Data by Starting With a Strong Baseline Methodology">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.19822">

<!--Generated on Wed Jun  5 19:02:27 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\authorinfo</span>
<p id="p1.2" class="ltx_p">Please send further correspondence to Frank Ruis or Alma Liezenga
<br class="ltx_break">Frank A. Ruis: E-mail: frank.ruis@tno.nl, Telephone: +31 (0)6 11 28 04 89 
<br class="ltx_break">Alma M. Liezenga: E-mail: alma.liezenga@tno.nl, Telephone: +31 (0)6 27 86 46 08</p>
</div>
<h1 class="ltx_title ltx_title_document">Improving Object Detector Training on Synthetic Data by Starting With a Strong Baseline Methodology</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Frank A. Ruis
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">TNO, Oude Waalsdorperweg 63, The Hague, The Netherlands
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alma M. Liezenga
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">TNO, Oude Waalsdorperweg 63, The Hague, The Netherlands
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Friso G. Heslinga
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">TNO, Oude Waalsdorperweg 63, The Hague, The Netherlands
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Luca Ballan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">TNO, Oude Waalsdorperweg 63, The Hague, The Netherlands
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Thijs A. Eker
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">TNO, Oude Waalsdorperweg 63, The Hague, The Netherlands
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Richard J. M. den Hollander
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">TNO, Oude Waalsdorperweg 63, The Hague, The Netherlands
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Martin C. van Leeuwen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">TNO, Oude Waalsdorperweg 63, The Hague, The Netherlands
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Judith Dijk
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">TNO, Oude Waalsdorperweg 63, The Hague, The Netherlands
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wyke Huizinga
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">TNO, Oude Waalsdorperweg 63, The Hague, The Netherlands
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Collecting and annotating real-world data for the development of object detection models is a time-consuming and expensive process. In the military domain in particular, data collection can also be dangerous or infeasible. Training models on synthetic data may provide a solution for cases where access to real-world training data is restricted. However, bridging the reality gap between synthetic and real data remains a challenge. Existing methods usually build on top of baseline Convolutional Neural Network (CNN) models that have been shown to perform well when trained on real data, but have limited ability to perform well when trained on synthetic data. For example, some architectures allow for fine-tuning with the expectation of large quantities of training data and are prone to overfitting on synthetic data. Related work usually ignores various best practices from object detection on real data, e.g. by training on synthetic data from a single environment with relatively little variation. In this paper we propose a methodology for improving the performance of a pre-trained object detector when training on synthetic data. Our approach focuses on extracting the salient information from synthetic data without forgetting useful features learned from pre-training on real images. Based on the state of the art, we incorporate data augmentation methods and a Transformer backbone. Besides reaching relatively strong performance without any specialized synthetic data transfer methods, we show that our methods improve the state of the art on synthetic data trained object detection for the RarePlanes and DGTA-VisDrone datasets, and reach near-perfect performance on an in-house vehicle detection dataset.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>object detection, synthetic data, transformers
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>INTRODUCTION</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The development of accurate object detection models remains a challenge, especially when dealing with data scarcity. The proliferation of synthetic datasets, driven by advancements in graphics technology and simulation, has opened new avenues for training object detection models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Synthetic data offers the advantage of scalability, cost-effectiveness, and control over various environmental factors. However, bridging the reality gap, the difference between synthetic or simulated and real(-world) data, remains a significant hurdle. A substantial body of research has addressed the domain gap, focusing on domain transfer methods such as domain randomization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> or domain adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. While such approaches have yielded promising results, there has been a notable imbalance between the attention given to domain adaptation methods and the integration of foundational training practices. Many existing works tend to emphasize domain adaptation techniques while neglecting well-established best practices derived from training on real data. These practices encompass designing a robust augmentation pipeline, optimization and regularization of hyperparameters, tailored to the downstream task, and ensuring data diversity. All of these practices have proven instrumental in enhancing the generalization capability of object detection models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Another crucial aspect of object detection model design pertains to the choice of neural network architecture. CNNs have become the de facto standard backbone for various computer vision tasks, including object detection trained on synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. However, over the past years research has shed light on a notable bias exhibited by CNNs. Empirical observations and theoretical analyses suggest that CNNs behave akin to high-pass filters, emphasizing texture-related features over shape-based features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. While these properties can be controlled for through e.g. augmentations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, without tailored interventions they can significantly impact model generalization. In contrast, Transformer architectures exhibit a distinct ‘shape bias’ behavior, akin to a low-pass filter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. In the context of object detection, the texture bias of CNNs may hinder their ability to generalize effectively when trained on synthetic data, as synthetic data often struggles to faithfully capture fine-grained textures present in real-world scenes. Conversely, the shape bias inherent to Transformer architectures aligns more closely with the shared characteristics of synthetic and real data. Synthetic datasets tend to excel in accurately representing geometric shapes and structural layouts. Thus, the use of synthetic datasets as training data for a Transformer backbone is a potentially powerful combination.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">This research paper aims to establish a strong baseline approach for training object detection models on synthetic data, through exploring the potential of leveraging shape bias in Transformer architectures and integrating traditional best practices for training object detection models, such as including strong data augmentation techniques.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>RELATED WORK</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">A scan of the state of the art has been conducted to gain a deeper understanding of methods for object detection, and the use of synthetic data and data augmentation for improved model performance.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Object detection</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Deep Learning (DL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> has been a game-changer for object detection, achieving state-of-the-art results on a variety of benchmarks and applications, such as autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, people counting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, search and rescue operations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, and military object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. DL-based object detectors typically employ CNNs to extract features from images. Most of these follow a two-stage <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> or a single-stage <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> object detection approach, adapted to the number of steps needed to solve the task. Attention to multi-scale features is often fundamental, and has led to developments from Feature Pyramid Networks (FPN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> to more elaborate versions such as the BiFPN component of EfficientDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. Recent advances show that the adoption of different architectures such as the computer vision adaptation of Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> may result in improvements to the state-of-the-art results. An example is ViT-Yolo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, which is a hybrid model combining convolution with the self-attention mechanism. DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> also exploits an encoder-decoder structure to achieve end-to-end detection. Lastly, the Swin Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> has proven to be a reliable backbone for several computer vision tasks. Due to the distinct ‘shape bias’ behaviour that Transformers exhibit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, Transformer-based methods are arguably more robust to occlusion, perturbations and domain shifts compared to CNNs, and are more suitable when it comes to giving importance to the object’s shape and geometrical features rather than its texture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. This ‘shape bias’ is likely caused by Transformers having a very high receptive field early in the network, allowing the formation of long-range connections early, while CNNs start with a local receptive field that grows gradually each layer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Synthetic data</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Synthetic data has been adopted as a valuable resource to address a lack of large quantities of annotated real-world training data, saving time in acquisition and manual annotation. Its use has been shown to improve the performance of object detection models in a number of different studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. In the context of computer vision, synthetic data can be defined as images or videos (i) generated from scratch, for example through computer graphics or rendering engines, or generative models like GANs or diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, (ii) modified versions of existing real data, for example employing DL-based augmentations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> or style transfer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, or (iii) a combination of the two, for example inpainting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. Besides saving time on data gathering and annotation, synthetic data can also offer the advantage of a large degree of precision and control over the environment through which synthetic datasets are built <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. However, synthetic data also comes with its challenges. Bridging the reality gap, as well as building datasets with enough variability to represent all possible target scenarios and characteristics sufficiently, can be a challenge in creating and using synthetic data. To avoid performance degradation caused by the reality gap, recent literature explores data diversity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, coherent in-context placement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, coverage in viewpoint and scale distributions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, variation in clutter and occlusions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, unrealistic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> or structured <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> domain randomization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, photorealism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> or a combination of the last two <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Data augmentation</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">One of the most effective ways to improve detection performance is the use of data augmentation techniques during training. However, we find that related work often retains the default augmentations provided by their framework or baseline model of choice, occasionally supplemented by photometric distortions such as color jitter and blurring. Certain object detection frameworks such as YOLOv5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> are optimized for fine-tuning on relatively small amounts of task-specific data, and thus have a strong augmentation pipeline, and are continuously updated to follow new state-of-the-art approaches. For the Transformer backbone, we find that most common implementations such as those in MMDetection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> and Detectron2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> have a default set of augmentations optimized for large-scale pretraining, usually limited to random flips and some resizing and cropping. This stands in contrast with the recent advances in image augmentation, which have proven that model performance may increasing significantly when artificially increasing the size and diversity of a dataset, creating new images from existing ones <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. On top of well-known ‘traditional’ augmentations (flipping, cropping, rotating, scaling, blurring), one can work on specific color space and intensity transformations, photometric distortions, blending, or use methods like AutoAugment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, MixUp <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, RandAugment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, CutMix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>, or GAN-based augmentations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. These methods have shown to improve performance on object detection tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. This is especially important when used in conjunction with a Transformer backbone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, as the weaker inductive biases of this backbone cause an increased reliance on regularization or augmentation, compared to CNNs. Due to the large search space of possible augmentations and their hyperparameters, it is computationally infeasible to do a full sweep over all possible permutations. RandAugment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> is an automatic augmentation optimization scheme that is able to reduce the search space while automatically optimizing the applied augmentations with respect to validation accuracy.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>METHODOLOGY</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section highlights the methods used in our experiments in terms of backbone architecture and data augmentation methods, and elaborates on the selected datasets and their quality.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Backbone Architecture</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">As highlighted in the Introduction, CNNs behave similar to high-pass filters, emphasizing high-frequency texture-related features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> while Transformers behave similar to low-pass filters, exhibiting a distinct bias to detect low-frequency shape-based features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. This is especially relevant in the domain of synthetic data, where it is often much easier to faithfully capture the shape of an object than it is to implement photo-realistic textures and lighting. We propose to leverage the aforementioned properties of the Transformer architecture. Thus, we employ several pre-trained Transformers and compare their performance when trained on synthetic data and evaluated on real data, to the performance of baseline convolutional models. The Swin Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> was selected to operationalize the Transformer backbone. This popular backbone is characterized by window-based self-attention: it performs local attention within a window. Additionally, Swin reaches similar or better performance on object detection tasks compared to other, more computationally expensive, Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>. The Swin Transformer does exhibit a significantly smaller shape bias than a full attention vision Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>, but that is considered to be a reasonable trade-off because the full attention computational costs remain prohibitive for the dense prediction tasks. For fair comparison with the baseline convolutional model, the size of the Swin Transformer was selected to match the parameter count of the baseline convolutional model as closely as possible.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Data quality is crucial in training Deep Learning models. We find that many synthetic object detection datasets are limited in terms of variety in the 3D models used, the quality of their textures, and environmental factors such as lighting and backgrounds. Besides variation, we find that label quality is also a common concern. A common way to generate object detection labels is to draw a bounding box around every part of the target object that is visible from the camera viewpoint. While proper implementation should result in perfect ground truth labels, a manual inspection of the data revealed that mistakes were still present in the datasets. For example, DGTA-VisDrone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> contains objects that are completely obscured by trees or foliage, presumably due to raycasts being used for calculating visibility ignoring those surfaces even though they are opaque. We perform experiments on the challenging VisDrone dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> with DGTA-VisDrone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> as a synthetic training counterpart, the synthetic-to-real RarePlanes dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, and a synthetic and real in-house vehicle detection dataset from concurrent work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The details of each dataset are listed below.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>VisDrone</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">The VisDrone dataset is a large-scale visual object detection and tracking benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>. The dataset was captured using drones over various Chinese urban and suburban areas and consists of 263 videos and 10,209 images. The dataset was recorded over a variety of scenes across 14 cities in China, across various weather and lighting conditions. The dataset also contains a wide variety of viewpoints, including oblique angles and overhead viewpoints, from a range of low to high altitudes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>. The images are annotated using 10 fine-grained classes, including vehicles such as cars, vans, or busses, separate classes for pedestrians and drivers/cyclists, and fine-grained distinctions between bicycles, tricycles, and awning-tricycles <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>. The dataset has been used in multiple challenges for evaluating detector performance on small objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. When trained on real data, the challenge has reached a performance of mAP 38-39%, and mAP@50 of 62-65% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. As a counterpart to this real-world dataset, the synthetic DGTA-VisDrone dataset has been developed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. This dataset contains 50,000 images from drone viewpoints, with a great variety in altitude and angle. The dataset was created using the open-source framework DeepGTAV, based on the video game GTAV. The annotations of this dataset are limited to 8 out of 10 VisDrone classes, excluding the classes tricycles and awning-tricycles <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. When training on this synthetic dataset, performance has thusfar reached a maximum mAP@50 of 10.2%<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. <a href="#S3.F1" title="Figure 1 ‣ 3.2.1 VisDrone ‣ 3.2 Data ‣ 3 METHODOLOGY ‣ Improving Object Detector Training on Synthetic Data by Starting With a Strong Baseline Methodology" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a> shows an image from the original VisDrone dataset alongside an example from the DGTA-VisDrone set.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.19822/assets/figures/VisDroneSynthetic.png" id="S3.F1.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="375" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F1.sf1.3.2" class="ltx_text" style="font-size:90%;">An example image of the DGTA-Visdrone dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.19822/assets/figures/VisDroneReal.png" id="S3.F1.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="373" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F1.sf2.3.2" class="ltx_text" style="font-size:90%;">An example image of the VisDrone dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.3.2" class="ltx_text" style="font-size:90%;">Example images for the synthetic/train dataset (a) and the real-world/validation dataset (b)</span></figcaption>
</figure>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>RarePlanes</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">The RarePlanes dataset is a collection of WorldView-3 satellite imagery capturing a wide variety of airplanes at 0.3m resolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. The dataset includes both real and synthetic images. The real portion of the dataset consists of 253 satellite scenes taken over 112 locations with 14,700 annotations. The synthetic portion of the dataset consists of 50,000 synthetic satellite images with 630,000 aircraft annotations from a wide variety of locations. AI.Reverie’s simulation platform was used to capture the images at the same 0.3m resolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. The RarePlanes datasets are annotated with 10 attributes rather than classes, such as the presence of canards, the propulsion system, and number of engines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Related work has thus focused on classification by role <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, specifically for the subset of civilian airplanes classified by their size (small, medium, or large). We train our model on this classification task as well. Previous studies have reached a maximum performance of 75% mAP and 93% mAP@50 for this same task when training on real data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> and 35.9% mAP and 59.1% mAP@50 when training solely on synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. <a href="#S3.F2" title="Figure 2 ‣ 3.2.2 RarePlanes ‣ 3.2 Data ‣ 3 METHODOLOGY ‣ Improving Object Detector Training on Synthetic Data by Starting With a Strong Baseline Methodology" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a> shows one image from the real alongside one image of the synthetic portion of the RarePlanes dataset.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.19822/assets/figures/RarePlanesSynthetic.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="334" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">An example image for the RarePlanes dataset, synthetic portion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.19822/assets/figures/RarePlanesReal.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="343" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">An example image of the RarePlanes dataset, real-world portion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Example images for the synthetic/train dataset (a) and the real-world/validation dataset (b).</span></figcaption>
</figure>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Vehicle Detection</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">The last dataset we evaluated models on is one developed in-house for concurrent work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The dataset was designed through iterative variation of relevant axes, such as the number of images per object class, object-camera distance, and object pitch, to reach optimal mAP performance when training a vehicle object detector on synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The resulting dataset contains 1,600 synthetic and 333 real-world images of 4 types of vehicles. While the data construction method is relatively crude, consisting of just one 3D model per vehicle type superimposed on real photographs of various environments, the careful tuning of relevant simulation axes make it a very potent synthetic training set. The original publication managed to reach a performance of mAP 76.7% and mAP@50 of 95.4% when training on synthetic data and evaluating on real data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. <a href="#S3.F3" title="Figure 3 ‣ 3.2.3 Vehicle Detection ‣ 3.2 Data ‣ 3 METHODOLOGY ‣ Improving Object Detector Training on Synthetic Data by Starting With a Strong Baseline Methodology" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a> shows one image from the real alongside one image of the synthetic portion of the vehicle detection dataset.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.19822/assets/figures/synthetic_truck.png" id="S3.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="302" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">An example image of a truck from the vehicles dataset, synthetic portion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.19822/assets/figures/truck_real.png" id="S3.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="437" height="304" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">An example image of a truck from the vehicles dataset, real-world portion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Example images for the synthetic/train dataset (a) and the real-world/validation dataset (b).</span></figcaption>
</figure>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Augmentations</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Unless stated otherwise, we will apply data mixing augmentation methods MixUp <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>, Mosaic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, and large scale jittering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. An example of MixUp and Mosaic is provided in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.3 Augmentations ‣ 3 METHODOLOGY ‣ Improving Object Detector Training on Synthetic Data by Starting With a Strong Baseline Methodology" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. We will show that adopting a strong augmentation pipeline can significantly increase performance for the Transformer baseline. However, we show through the experiment described in section <a href="#S4.SS2" title="4.2 RarePlanes ‣ 4 EXPERIMENTS ‣ Improving Object Detector Training on Synthetic Data by Starting With a Strong Baseline Methodology" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> that these improvements are not universal, and that the specific set of augmentations should always be carefully adjusted to the task at hand. We use RandAugment in our experiments to select a good augmentation policy from a set of photometric distortions: brightness, contrast, pixelation, jpeg compression, and gaussian blur.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.19822/assets/figures/mixup.png" id="S3.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="324" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F4.sf1.3.2" class="ltx_text" style="font-size:90%;">An example of the MixUp augmentation.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.19822/assets/figures/mosaic.png" id="S3.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="437" height="415" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F4.sf2.3.2" class="ltx_text" style="font-size:90%;">An example of the Mosaic augmentation.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">Examples of the MixUp and Mosaic data augmentation methods, applied to the DGTA-VisDrone dataset.</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>EXPERIMENTS</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We perform experiments on three pairs of datasets of synthetic and real-world images. The baseline comparison models and their performance have been taken from the original publications in the case of DGTA-Visdrone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and RarePlanes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, or have been trained specifically for this publication if there was no baseline available, as was the case for the vehicle detection dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. For fair comparison, the Swin Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> counterpart of the convolutional benchmark models were selected to have a roughly equal or lower parameter count. Our results are reported using either mAP, mAP@50, or both, based on the availability of comparative metrics in related work, as well as how representative these metrics are for the results of the specific experiment. In the following subsections we will detail the experimental setup for each individual dataset.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>VisDrone</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">For the VisDrone experiment, the (DGTA-)VisDrone dataset described in <a href="#S3.SS2.SSS1" title="3.2.1 VisDrone ‣ 3.2 Data ‣ 3 METHODOLOGY ‣ Improving Object Detector Training on Synthetic Data by Starting With a Strong Baseline Methodology" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsubsection 3.2.1</span></a> was used. We compare to the ResNeXt-101 and YOLOv5 models from the original publication as benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. The ResNeXt-101 had been trained with limited data augmentations, while the YOLOv5-X model has a lower parameter count but more elaborate data augmentations. For our experiments we employed a Swin-S Transformer due to its similarity in size, with 66.1M parameters. The larger Swin-B would be a closer fit, but we did not find pretrained Faster-RCNN models utilizing this backbone. All data augmentations described in section <a href="#S3.SS3" title="3.3 Augmentations ‣ 3 METHODOLOGY ‣ Improving Object Detector Training on Synthetic Data by Starting With a Strong Baseline Methodology" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> were applied to our dataset for training. To evaluate whether a potential improvement in performance was caused by the data augmentations or the Transformer backbone, we trained a second Swin-S Transformer on the dataset without applying data augmentations. Finally, we train a DINO model with Swin-L backbone to see the impact of a much larger model on detection performance. We report all our results using mAP@50, due to the unavailability of mAP for the original publication.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>RarePlanes</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">For the Rareplanes experiment, the RarePlanes dataset described in <a href="#S3.SS2.SSS2" title="3.2.2 RarePlanes ‣ 3.2 Data ‣ 3 METHODOLOGY ‣ Improving Object Detector Training on Synthetic Data by Starting With a Strong Baseline Methodology" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsubsection 3.2.2</span></a> was used. We compare to the benchmark ResNet-50 model from the original publication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, focusing on the object detection based on role, excluding military classes in accordance with the original publication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>. We performed the same object detection task with our model, which in essence is a classification based on size (small, medium, large) of civilian airplanes. This classification objective, combined with the fact that all imagery is taken at the same resolution of 0.3m per pixel and no cloud cover or other occlusions are present. This meant that most of the augmentations that were indispensable earlier, such as large scale jittering and MixUp, now harm detection accuracy. Therefore, we only used RandAugment in our best training run. We report our results using mAP, as this was the metric provided in the original publication.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Vehicle Detection</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">For the vehicle detection experiment, the vehicles dataset described in <a href="#S3.SS2.SSS3" title="3.2.3 Vehicle Detection ‣ 3.2 Data ‣ 3 METHODOLOGY ‣ Improving Object Detector Training on Synthetic Data by Starting With a Strong Baseline Methodology" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsubsection 3.2.3</span></a> was used. There are no existing benchmarks since this is a newly published dataset developed in concurrent work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Therefore, we first trained a Transformer model which balanced efficiency in terms of parameter count with performance, and selected a convolutional model, ResNet-101, that had a similar parameter count as our final model. We additionally used a higher-performance backbone and detector architecture in the form of ResNeXt-101 64x4d and Cascade R-CNN to see if more compute may close the gap in model performance. All data augmentations were applied to the training data for both models. We report our results using both mAP and mAP@50. Due to the nature of our dataset, reporting solely the mAP would provide an inaccurate representation of the results, because a low mAP but high mAP@50 is observed to be the result of the exclusion of specific parts sticking out of the vehicle, such as an antenna, camera, or sometimes even the entire truck cabin. We do not consider the detection of these subparts to be a crucial feature of the object detector, and therefore provide both metrics but focus on the mAP@50 in our reporting.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>RESULTS</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section provides the results of the experiments one by one, followed by a discussion of the results and suggestions for future research.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>VisDrone</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Table <a href="#S5.T1" title="Table 1 ‣ 5.1 VisDrone ‣ 5 RESULTS ‣ Improving Object Detector Training on Synthetic Data by Starting With a Strong Baseline Methodology" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the results of the VisDrone experiment. The results demonstrate improvements to the benchmark performance both through applying the Transformer architecture as well as enhancing augmentations. The benchmark ResNeXt-101 in particular had resulted in low performance, potentially caused by the lack of data augmentations in the orginal setup. The YOLOv5 benchmark already achieved better results, leveraging strong augmentation methods and thus preventing overfitting on synthetic data through domain randomization, to some extend <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. However, the Transformer architecture, by leveraging a combination of data augmentation to prevent overfitting and leaning into the shape bias to make effective use of the consistency in shape from synthetic to real data, achieves significantly higher results with a lower parameter count. Our experiment with a combination of Transformer backbone and little data augmentations, demonstrated that proper augmentation remains crucial for high detection performance. Lastly, the experiment with the high-capacity DINO architecture and Swin-L backbone shows that even with suboptimal synthetic training data the model is able to reach impressive performance on the real VisDrone data.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.2.3.1" class="ltx_tr">
<th id="S5.T1.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T1.2.3.1.1.1" class="ltx_text ltx_font_bold">Backbone</span></th>
<td id="S5.T1.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.2.3.1.2.1" class="ltx_text ltx_font_bold">Architecture</span></td>
<td id="S5.T1.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.2.3.1.3.1" class="ltx_text ltx_font_bold">Parameters</span></td>
<td id="S5.T1.2.3.1.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.2.3.1.4.1" class="ltx_text ltx_font_bold">Data augmentation</span></td>
<td id="S5.T1.2.3.1.5" class="ltx_td ltx_align_center"><span id="S5.T1.2.3.1.5.1" class="ltx_text ltx_font_bold">mAP@50</span></td>
</tr>
<tr id="S5.T1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ResNeXt-101 64x4d<sup id="S5.T1.1.1.1.1" class="ltx_sup">∗</sup>
</th>
<td id="S5.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_r">Faster R-CNN</td>
<td id="S5.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_r">127M</td>
<td id="S5.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_r">low</td>
<td id="S5.T1.1.1.5" class="ltx_td ltx_align_center">2.4</td>
</tr>
<tr id="S5.T1.2.2" class="ltx_tr">
<th id="S5.T1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">CSP-Darknet53<sup id="S5.T1.2.2.1.1" class="ltx_sup">∗</sup>
</th>
<td id="S5.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_r">YOLOv5-X</td>
<td id="S5.T1.2.2.3" class="ltx_td ltx_align_center ltx_border_r">86.7M</td>
<td id="S5.T1.2.2.4" class="ltx_td ltx_align_center ltx_border_r">high</td>
<td id="S5.T1.2.2.5" class="ltx_td ltx_align_center">10.2</td>
</tr>
<tr id="S5.T1.2.4.2" class="ltx_tr">
<th id="S5.T1.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Swin-S</th>
<td id="S5.T1.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r">Faster R-CNN</td>
<td id="S5.T1.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r">66.1M</td>
<td id="S5.T1.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r">low</td>
<td id="S5.T1.2.4.2.5" class="ltx_td ltx_align_center">7.8</td>
</tr>
<tr id="S5.T1.2.5.3" class="ltx_tr">
<th id="S5.T1.2.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Swin-S</th>
<td id="S5.T1.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r">Faster R-CNN</td>
<td id="S5.T1.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r">66.1M</td>
<td id="S5.T1.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r">high</td>
<td id="S5.T1.2.5.3.5" class="ltx_td ltx_align_center">16.2</td>
</tr>
<tr id="S5.T1.2.6.4" class="ltx_tr">
<th id="S5.T1.2.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Swin-L</th>
<td id="S5.T1.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r">DINO 5-scale</td>
<td id="S5.T1.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r">218M</td>
<td id="S5.T1.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r">high</td>
<td id="S5.T1.2.6.4.5" class="ltx_td ltx_align_center">26.1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.8.2.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S5.T1.4.1" class="ltx_text" style="font-size:90%;">The results of the VisDrone experiment, including benchmark results 
<br class="ltx_break"><sup id="S5.T1.4.1.1" class="ltx_sup"><span id="S5.T1.4.1.1.1" class="ltx_text" style="font-size:89%;">∗</span></sup><span id="S5.T1.4.1.2" class="ltx_text" style="font-size:89%;"> results taken from Kiefer et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</span></span></figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>RarePlanes</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Table <a href="#S5.T2" title="Table 2 ‣ 5.2 RarePlanes ‣ 5 RESULTS ‣ Improving Object Detector Training on Synthetic Data by Starting With a Strong Baseline Methodology" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the results of the RarePlanes experiment. The results demonstrate a slight improvement to the baseline when using a Transformer backbone. As previously stated, a limited set of data augmentations was used for this experiment, thus we only evaluate the use of the Transformer backbone here. The Transformer backbone does show some improvement in mAP, but the performance gain is limited. The shape of an airplane is very distinctive, potentially working to the advantage of a Transformer backbone. However, the distinction between airplane classes is only based on size and therefore the shape bias does not offer as strong of an advantage here, leaving limited room for improvement by the Transformer backbone. Furthermore, even though the task seems relatively simple, the detection performance has been observed to be lowered predominantly due to the test dataset being tiled. This has resulted in a large number of planes for which just a small part of the wing or fuselage is visible. In such cases it is difficult to judge the actual size of the airplane, even for humans, and the coordinates of the bounding box corners are ambiguous due to the remainder of the plane not being visible. In addition to this, there is no occlusion or cloud cover in the real and the synthetic dataset, which are two other factors under which Transformers would be expected to perform relatively well.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.1.2.1" class="ltx_tr">
<th id="S5.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T2.1.2.1.1.1" class="ltx_text ltx_font_bold">Backbone</span></th>
<td id="S5.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.1.2.1.2.1" class="ltx_text ltx_font_bold">Architecture</span></td>
<td id="S5.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.1.2.1.3.1" class="ltx_text ltx_font_bold">Parameters</span></td>
<td id="S5.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.1.2.1.4.1" class="ltx_text ltx_font_bold">Data augmentation</span></td>
<td id="S5.T2.1.2.1.5" class="ltx_td ltx_align_center"><span id="S5.T2.1.2.1.5.1" class="ltx_text ltx_font_bold">mAP</span></td>
</tr>
<tr id="S5.T2.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ResNet-50<sup id="S5.T2.1.1.1.1" class="ltx_sup">∗</sup>
</th>
<td id="S5.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_r">Faster R-CNN</td>
<td id="S5.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_r">41.4M</td>
<td id="S5.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_r">medium</td>
<td id="S5.T2.1.1.5" class="ltx_td ltx_align_center">35.9</td>
</tr>
<tr id="S5.T2.1.3.2" class="ltx_tr">
<th id="S5.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Swin-T</th>
<td id="S5.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">Faster R-CNN</td>
<td id="S5.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">45.2M</td>
<td id="S5.T2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">medium</td>
<td id="S5.T2.1.3.2.5" class="ltx_td ltx_align_center">40.8</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.7.2.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S5.T2.3.1" class="ltx_text" style="font-size:90%;">The results of the RarePlanes experiment, including benchmark results 
<br class="ltx_break"><sup id="S5.T2.3.1.1" class="ltx_sup"><span id="S5.T2.3.1.1.1" class="ltx_text" style="font-size:89%;">∗</span></sup><span id="S5.T2.3.1.2" class="ltx_text" style="font-size:89%;"> results taken from Shermeyer et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</span></span></figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Vehicles Detection</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Table <a href="#S5.T3" title="Table 3 ‣ 5.3 Vehicles Detection ‣ 5 RESULTS ‣ Improving Object Detector Training on Synthetic Data by Starting With a Strong Baseline Methodology" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the results of the experiments with the in-house vehicle detection dataset. In this case, a strong improvement is demonstrated over the ResNet and ResNeXt architectures, reaching near-perfect object detection performance on the real dataset for a model trained solely with synthetic data. The high performance of the Transformer model could be the result of the strong emphasis on shape for classifying vehicles. This method focused strongly on shape and the images did not include depth or shadow, making them a particularly good fit for the Transfomer backbone and potentially harming their usefulness for a convolutional backbone. Even a larger convolutional model, the ResNeXt-101 64x4d, was not able to reach a similar performance to the Transformer, which was almost half its size in terms of parameter count.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<td id="S5.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.1.2.1.1.1" class="ltx_text ltx_font_bold">Backbone</span></td>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.1.2.1.2.1" class="ltx_text ltx_font_bold">Architecture</span></td>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.1.2.1.3.1" class="ltx_text ltx_font_bold">Parameters</span></td>
<td id="S5.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.1.2.1.4.1" class="ltx_text ltx_font_bold">Data augmentation</span></td>
<td id="S5.T3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.1.2.1.5.1" class="ltx_text ltx_font_bold">mAP@50</span></td>
<td id="S5.T3.1.2.1.6" class="ltx_td ltx_align_center"><span id="S5.T3.1.2.1.6.1" class="ltx_text ltx_font_bold">mAP</span></td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<td id="S5.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r">ResNet-101</td>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">Faster R-CNN</td>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">60.4M</td>
<td id="S5.T3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">high</td>
<td id="S5.T3.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r">78.0</td>
<td id="S5.T3.1.3.2.6" class="ltx_td ltx_align_center">55.0</td>
</tr>
<tr id="S5.T3.1.4.3" class="ltx_tr">
<td id="S5.T3.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r">ResNeXt-101 64x4d</td>
<td id="S5.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">Cascade R-CNN</td>
<td id="S5.T3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">127M</td>
<td id="S5.T3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">high</td>
<td id="S5.T3.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r">87.5</td>
<td id="S5.T3.1.4.3.6" class="ltx_td ltx_align_center">66.0</td>
</tr>
<tr id="S5.T3.1.5.4" class="ltx_tr">
<td id="S5.T3.1.5.4.1" class="ltx_td ltx_align_center ltx_border_r">Swin-S</td>
<td id="S5.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">Faster R-CNN</td>
<td id="S5.T3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">66.1M</td>
<td id="S5.T3.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">high</td>
<td id="S5.T3.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r">94.8</td>
<td id="S5.T3.1.5.4.6" class="ltx_td ltx_align_center">80.1</td>
</tr>
<tr id="S5.T3.1.1" class="ltx_tr">
<td id="S5.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_r">Swin-T<sup id="S5.T3.1.1.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S5.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_r">Faster R-CNN</td>
<td id="S5.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_r">45.2M</td>
<td id="S5.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_r">high</td>
<td id="S5.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_r">95.4</td>
<td id="S5.T3.1.1.6" class="ltx_td ltx_align_center">-</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.7.2.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.3.1" class="ltx_text" style="font-size:90%;">The results of the vehicle detection experiment. 
<br class="ltx_break"><sup id="S5.T3.3.1.1" class="ltx_sup"><span id="S5.T3.3.1.1.1" class="ltx_text" style="font-size:89%;">∗</span></sup><span id="S5.T3.3.1.2" class="ltx_text" style="font-size:89%;"> result taken from Eker et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite></span></span></figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This study demonstrated the capabilities of the Transformer architecture as well as data augmentation techniques to significantly improve object detectors trained on synthetic data. The VisDrone experiment showed how the existing benchmark performance could be improved both through data augmentations and a Transformer backbone, individually and combined. The performance gain could be explained by the strong shape bias of Transformer backbones, versus the texture bias of convolutional backbones. This causes the network to perform well in tasks that are heavily focused on the object shape, such as the detection of persons and (different types of) vehicles. The RarePlanes experiment showed that, while strong augmentations and choice of backbone are important, they need to be well-aligned with the task at hand. Notably, augmentations that were crucial for good performance on the VisDrone dataset had a negative effect on the RarePlanes dataset. Lastly, we presented object detection on a vehicle detection dataset developed in concurrent work. The Transformer backbone far outperformed the convolutional backbone for this task, even when using a convolutional backbone with nearly twice as many parameters. The Transformer backbone’s shape bias is expected to have had a a strong impact on this result in particular due to the crude data construction method which featured 3D models that were limited both in quantity and quality. Nevertheless, the near-perfect performance on the real dataset with a model trained solely on synthetic data is impressive.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">In conclusion, our work demonstrates that a Transformer architecture can provide a potent backbone when training on synthetic data, potentially prior to domain transfer to achieve optimal results. The Transformer backbone’s inherent shape bias makes it useful for object detection tasks that are heavily focused on shape, and less for object detection tasks with ambiguous shapes or tasks that are heavily focused on texture. In addition to this, data augmentation methods are useful for object detection focused on shape as well, though they should be carefully selected based on the specifics of the dataset. Both these methods contribute to our suggestion to improve object detectors trained with synthetic data by starting with a stronger baseline methodology.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Limitations</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">Several limitations of the research should be taken into consideration. First of all, the dataset pairs of synthetic and real data were limited in availability, size, and quality. The VisDrone dataset, though the largest dataset presented here, is limited in its synthetic counterpart, which lacks a proper class to class mapping and presents significant label noise. The RarePlanes dataset presents a relatively simple task, where the distinction between classes is made solely based on size with a consistent resolution and no cases of obstruction of the view or occlusion. There is also significant ambiguity in test set labels due to tiling of the images, resulting in many bounding boxes that only show a small part of a fuselage or wing. Lastly, the synthetic vehicle detection dataset was limited in terms of the quality of the original 3D models used. Besides these limitations in the existing datasets, there also exists a lack of high-quality object detection benchmark models adapted to these and other datasets. This makes it difficult to make fair comparisons between baseline and proposed methodologies. Given the existing benchmark models we have tried to create fair comparisons through using similar parameter count and presenting even larger convolutional models when it added value.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Future work</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">We identified several potential courses for future research. The creation of high-quality and large synthetic and real dataset pairs would be a valuable contribution to this field. Improvement in quality could be made both through ensuring proper class-to-class mapping, improving the annotation quality, and improving the richness of context through using custom simulations. Additionally, the data construction method for synthetic datasets provides for interesting topics of research. There is a clear distinction between datasets that are based on the simulation of a single 3D object, such as the vehicle detection dataset, versus datasets that represent an entire 3D world, such as the DGTA-VisDrone and RarePlanes datasets. Leaning into this distinction and its impact on the usefulness of synthetic data for training could also be a topic of further investigation.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">The creation of more and stronger benchmark models for synthetic to real data would be a valuable addition to this field. The lack of such models limited the ability to make fair comparisons in this paper, and thus hinders the development of good practices for training on synthetic data. In this study, we focus on a type of object detection problem for which shape is the most salient factor. However, there are cases imaginable where a texture bias would be more relevant, for example when segmenting water, sky, or other surfaces with no well-defined shape. For these situations there are methods available that, e.g., propose hybrid architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, or add-on modules to inject high-frequency features for Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>. These methods present interesting topics for further study and we strongly recommend comparison between Transformer, convolutional, and hybrid architectures to gain understanding of the most potent fits between backbones and object detection tasks. In particular, recent progress in applying full attention Transformers to dense prediction tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> may improve results even further due to their superior shape bias over local attention Transformers.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Vanherle, B., Moonen, S., Van Reeth, F., and Michiels, N., “Analysis of
training object detection models with synthetic data,” <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2211.16066</span> (2022).

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Eker, T. A., Heslinga, F. G., Ballan, L., den Hollander, R. J., and Schutte,
K., “The effect of simulation variety on a deep learning-based military
vehicle detector,” in [<span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Artificial Intelligence for Security and Defence
Applications</span> ], <span id="bib.bib2.2.2" class="ltx_text ltx_font_bold">12742</span>, 183–196, SPIE
(2023).

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Rajpura, P. S., Bojinov, H., and Hegde, R. S., “Object detection using Deep
CNNs trained on synthetic images,” <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1706.06782</span>
(2017).

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Prakash, A., Boochoon, S., Brophy, M., Acuna, D., Cameracci, E., State, G.,
Shapira, O., and Birchfield, S., “Structured domain randomization: Bridging
the reality gap by context-aware synthetic data,” in [<span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">2019
International Conference on Robotics and Automation
(ICRA)</span> ], 7249–7255, IEEE (2019).

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P.,
“Domain randomization for transferring deep neural networks from simulation
to the real world,” in [<span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">2017 IEEE/RSJ international conference on
intelligent robots and systems (IROS)</span> ],
23–30, IEEE (2017).

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Hinterstoisser, S., Pauly, O., Heibel, H., Marek, M., and Bokeloh, M., “An
annotation saved is an annotation earned: Using fully synthetic training for
object instance detection,” (2019).

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Jhang, Y.-C., Palmar, A., Li, B., Dhakad, S., Vishwakarma, S. K., Hogins, J.,
Crespi, A., Kerr, C., Chockalingam, S., Romero, C., Thaman, A., and Ganguly,
S., “Training a performant object detection ML model on synthetic data
using Unity Perception tools.”
<a target="_blank" href="https://blogs.unity3d.com/2020/09/17/training-a-performant-object-detection-ml-model-on-synthetic-data-using-unity-computer-vision-tools/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://blogs.unity3d.com/2020/09/17/training-a-performant-object-detection-ml-model-on-synthetic-data-using-unity-computer-vision-tools/</a>
(Sep 2020).

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Yang, J., Yan, R., and Hauptmann, A. G., “Cross-domain video concept detection
using adaptive svms,” in [<span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proceedings of the 15th ACM international
conference on Multimedia</span> ], 188–197 (2007).

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Yang, Y. and Soatto, S., “Proceedings of the ieee/cvf conference on computer
vision and pattern recognition,” 4085–4095 (2020).

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Wang, Q., Dai, D., Hoyer, L., Van Gool, L., and Fink, O., “Domain adaptive
semantic segmentation with self-supervised depth estimation,” in [<span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer
Vision</span> ], (2021).

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
He, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M., “Bag of tricks
for image classification with convolutional neural networks,” in [<span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</span> ], 558–567 (2019).

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Steiner, A., Kolesnikov, A., , Zhai, X., Wightman, R., Uszkoreit, J., and
Beyer, L., “How to train your ViT? Data, augmentation, and regularization
in Vision Transformers,” <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.10270</span> (2021).

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Shermeyer, J., Hossler, T., Van Etten, A., Hogan, D., Lewis, R., and Kim, D.,
“Rareplanes: Synthetic data takes flight,” in [<span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
Vision</span> ], 207–217 (2021).

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Kiefer, B., Ott, D., and Zell, A., “Leveraging synthetic data in object
detection on unmanned aerial vehicles,” in [<span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">2022 26th International
Conference on Pattern Recognition (ICPR)</span> ],
3564–3571, IEEE (2022).

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Vanherle, B., Moonen, S., Reeth, F. V., and Michiels, N., “Analysis of
training object detection models with synthetic data,” in [<span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">33rd British
Machine Vision Conference 2022, BMVC 2022, London, UK, November 21-24,
2022</span> ], BMVA Press (2022).

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F. A., and
Brendel, W., “Imagenet-trained CNNs are biased towards texture; increasing
shape bias improves accuracy and robustness.,” in [<span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">International
Conference on Learning Representations</span> ],
(2019).

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Naseer, M. M., Ranasinghe, K., Khan, S. H., Hayat, M., Shahbaz Khan, F., and
Yang, M.-H., “Intriguing properties of vision transformers,” <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Advances
in Neural Information Processing Systems</span> <span id="bib.bib17.2.2" class="ltx_text ltx_font_bold">34</span>, 23296–23308 (2021).

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Park, N. and Kim, S., “How do vision transformers work?,” in [<span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">International Conference on Learning
Representations</span> ], (2022).

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
LeCun, Y., Bengio, Y., and Hinton, G., “Deep learning,” <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Nature</span> <span id="bib.bib19.2.2" class="ltx_text ltx_font_bold">521</span>(7553), 436–444 (2015).

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Balasubramaniam, A. and Pasricha, S., “Object detection in autonomous
vehicles: Status and open challenges,” <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2201.07706</span>
(2022).

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Krishnachaithanya, N., Singh, G., Sharma, S., Dinesh, R., Sihag, S. R.,
Solanki, K., Agarwal, A., Rana, M., and Makkar, U., “People counting in
public spaces using deep learning-based object detection and tracking
techniques,” in [<span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">2023 International Conference on Computational
Intelligence and Sustainable Engineering Solutions
(CISES)</span> ], 784–788, IEEE (2023).

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Domozi, Z., Stojcsics, D., Benhamida, A., Kozlovszky, M., and Molnar, A.,
“Real time object detection for aerial search and rescue missions for
missing persons,” in [<span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">2020 IEEE 15th International Conference of System
of Systems Engineering (SoSE)</span> ],
000519–000524, IEEE (2020).

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Kong, L., Wang, J., and Zhao, P., “YOLO-G: A lightweight network model for
improving the performance of military targets detection,” <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">IEEE
Access</span> <span id="bib.bib23.2.2" class="ltx_text ltx_font_bold">10</span>, 55546–55564 (2022).

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Cai, Z. and Vasconcelos, N., “Cascade R-CNN: Delving into high quality
object detection,” in [<span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer
vision and pattern recognition</span> ], 6154–6162
(2018).

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Girshick, R., “Fast R-CNN,” in [<span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international
conference on computer vision</span> ], 1440–1448
(2015).

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick, R., and Sun, J., “Faster R-CNN: Towards real-time
object detection with region proposal networks,” <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Advances in neural
information processing systems</span> <span id="bib.bib26.2.2" class="ltx_text ltx_font_bold">28</span> (2015).

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
He, K., Gkioxari, G., Dollár, P., and Girshick, R., “Mask R-CNN,” in
[<span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</span> ], 2961–2969 (2017).

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Redmon, J., Divvala, S., Girshick, R., and Farhadi, A., “You only look once:
Unified, real-time object detection,” in [<span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern
recognition</span> ], 779–788 (2016).

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Redmon, J. and Farhadi, A., “YOLO9000: better, faster, stronger,” in [<span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern
recognition</span> ], 7263–7271 (2017).

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Redmon, J. and Farhadi, A., “YOLOv3: An incremental improvement,” <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1804.02767</span> (2018).

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Bochkovskiy, A., Wang, C.-Y., and Liao, H.-Y. M., “YOLOv4: Optimal speed and
accuracy of object detection,” <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.10934</span>
(2020).

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Wang, C.-Y., Bochkovskiy, A., and Liao, H.-Y. M., “YOLOv7: Trainable
bag-of-freebies sets new state-of-the-art for real-time object detectors,”
in [<span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span> ], 7464–7475 (2023).

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Lin, T.-Y., Dollár, P., Girshick, R., He, K., Hariharan, B., and Belongie,
S., “Feature pyramid networks for object detection,” in [<span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Proceedings
of the IEEE conference on computer vision and pattern
recognition</span> ], 2117–2125 (2017).

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Tan, M., Pang, R., and Le, Q. V., “EfficientDet: Scalable and efficient
object detection,” in [<span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition</span> ],
10781–10790 (2020).

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.,
“An image is worth 16x16 words: Transformers for image recognition at
scale,” <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.11929</span> (2020).

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,
Rault, T., Louf, R., Funtowicz, M., et al., “Transformers: State-of-the-art
natural language processing,” in [<span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2020 conference on
empirical methods in natural language processing: system
demonstrations</span> ], 38–45 (2020).

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Zhang, Z., Lu, X., Cao, G., Yang, Y., Jiao, L., and Liu, F., “ViT-YOLO:
Transformer-based YOLO for object detection,” in [<span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Proceedings of the
IEEE/CVF international conference on computer
vision</span> ], 2799–2808 (2021).

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko,
S., “End-to-end object detection with transformers,” in [<span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">European
conference on computer vision</span> ], 213–229,
Springer (2020).

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B.,
“Swin transformer: Hierarchical vision transformer using shifted windows,”
in [<span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer
Vision</span> ], 10012–10022 (2021).

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Raghu, M., Unterthiner, T., Kornblith, S., Zhang, C., and Dosovitskiy, A., “Do
vision transformers see like convolutional neural networks?,” (2022).

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Azizi, S., Kornblith, S., Saharia, C., Norouzi, M., and Fleet, D. J.,
“Synthetic data from diffusion models improves imagenet classification,”
<span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.08466</span> (2023).

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Trabucco, B., Doherty, K., Gurinas, M., and Salakhutdinov, R., “Effective data
augmentation with diffusion models,” <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.07944</span>
(2023).

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Ho, D., Rao, K., Xu, Z., Jang, E., Khansari, M., and Bai, Y., “Retinagan: An
object-aware approach to sim-to-real transfer,” (2020).

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
He, S., Zou, Y., Li, B., Peng, F., Lu, X., Guo, H., Tan, X., and Chen, Y., “An
image inpainting-based data augmentation method for improved sclerosed
glomerular identification performance with the segmentation model
efficientnetb3-unet,” <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">Scientific Reports</span> <span id="bib.bib44.2.2" class="ltx_text ltx_font_bold">14</span>(1), 1033 (2024).

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Wang, Q., Gao, J., Lin, W., and Yuan, Y., “Learning from synthetic data for
crowd counting in the wild,” in [<span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition</span> ],
8198–8207 (2019).

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Georgakis, G., Mousavian, A., Berg, A. C., and Kosecka, J., “Synthesizing
training data for object detection in indoor scenes,” <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:1702.07836</span> (2017).

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Dwibedi, D., Misra, I., and Hebert, M., “Cut, paste and learn: Surprisingly
easy synthesis for instance detection,” in [<span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE
international conference on computer vision</span> ],
1301–1310 (2017).

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Tremblay, J., To, T., Sundaralingam, B., Xiang, Y., Fox, D., and Birchfield,
S., “Deep object pose estimation for semantic robotic grasping of household
objects,” <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1809.10790</span> (2018).

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Borrego, J., Dehban, A., Figueiredo, R., Moreno, P., Bernardino, A., and
Santos-Victor, J., “Applying domain randomization to synthetic data for
object category detection,” <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1807.09834</span> (2018).

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Movshovitz-Attias, Y., Kanade, T., and Sheikh, Y., “How useful is
photo-realistic rendering for visual learning?,” in [<span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">Computer
Vision–ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and
15-16, 2016, Proceedings, Part III 14</span> ],
202–217, Springer (2016).

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Wang, X., Xie, L., Dong, C., and Shan, Y., “Real-ESRGAN: Training real-world
blind super-resolution with pure synthetic data,” in [<span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">Proceedings of
the IEEE/CVF international conference on computer
vision</span> ], 1905–1914 (2021).

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Jocher, G., Chaurasia, A., Stoken, A., Borovec, J., NanoCode012, Kwon, Y.,
TaoXie, Fang, J., imyhxy, Michael, K., Lorna, V, A., Montes, D., Nadar, J.,
Laughing, tkianai, yxNONG, Skalski, P., Wang, Z., Hogan, A., Fati, C.,
Mammana, L., AlexWang1900, Patel, D., Yiwei, D., You, F., Hajek, J., Diaconu,
L., and Minh, M. T., “ultralytics/yolov5: v6.1 - TensorRT, TensorFlow Edge
TPU and OpenVINO Export and Inference,” (Feb. 2022).

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Chen, K., Wang, J., Pang, J., Cao, Y., Xiong, Y., Li, X., Sun, S., Feng, W.,
Liu, Z., Xu, J., Zhang, Z., Cheng, D., Zhu, C., Cheng, T., Zhao, Q., Li, B.,
Lu, X., Zhu, R., Wu, Y., Dai, J., Wang, J., Shi, J., Ouyang, W., Loy, C. C.,
and Lin, D., “MMDetection: Open mmlab detection toolbox and benchmark,”
<span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1906.07155</span> (2019).

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., and Girshick, R., “Detectron2.”
<a target="_blank" href="https://github.com/facebookresearch/detectron2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/detectron2</a> (2019).

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q. V., “Autoaugment:
Learning augmentation strategies from data,” in [<span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">Proceedings of the
IEEE/CVF conference on computer vision and pattern
recognition</span> ], 113–123 (2019).

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D., “mixup: Beyond
empirical risk minimization,” <span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1710.09412</span> (2017).

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Cubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V., “Randaugment: Practical
automated data augmentation with a reduced search space,” in [<span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition workshops</span> ], 702–703 (2020).

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., and Yoo, Y., “Cutmix:
Regularization strategy to train strong classifiers with localizable
features,” in [<span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF international conference on
computer vision</span> ], 6023–6032 (2019).

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Frid-Adar, M., Klang, E., Amitai, M., Goldberger, J., and Greenspan, H.,
“Synthetic data augmentation using gan for improved liver lesion
classification,” in [<span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">2018 IEEE 15th international symposium on
biomedical imaging (ISBI 2018)</span> ], 289–293,
IEEE (2018).

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y., Xiao, A.,
Xu, C., Xu, Y., et al., “A survey on vision transformer,” <span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">IEEE
transactions on pattern analysis and machine intelligence</span> <span id="bib.bib60.2.2" class="ltx_text ltx_font_bold">45</span>(1),
87–110 (2022).

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Morrison, K., Gilby, B., Lipchak, C., Mattioli, A., and Kovashka, A.,
“Exploring corruption robustness: inductive biases in vision transformers
and mlp-mixers,” <span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.13122</span> (2021).

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Cao, Y., He, Z., Wang, L., Wang, W., Yuan, Y., Zhang, D., Zhang, J., Zhu, P.,
Van Gool, L., Han, J., Hoi, S., Hu, Q., and Liu, M., “Visdrone-det2021: The
vision meets drone object detection challenge results,” in [<span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">Proceedings
of the IEEE/CVF International Conference on Computer Vision (ICCV)
Workshops</span> ], 2847–2854 (October 2021).

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Zhu, P., Wen, L., Bian, X., Ling, H., and Hu, Q., “Vision meets drones: A
challenge,” <span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1804.07437</span> (2018).

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Downes, J., Gleave, W., and Nakada, D., “Rareplanes soar higher:
Self-supervised pretraining for resource constrained and synthetic
datasets,” in [<span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision</span> ], 370–378
(2023).

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D., “mixup: Beyond
empirical risk minimization,” in [<span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">International Conference on Learning
Representations</span> ], (2018).

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Ghiasi, G., Cui, Y., Srinivas, A., Qian, R., Lin, T.-Y., Cubuk, E. D., Le,
Q. V., and Zoph, B., “Simple copy-paste is a strong data augmentation method
for instance segmentation,” in [<span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition</span> ],
2918–2928 (2021).

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Yao, J., Wang, X., Yang, S., and Wang, B., “Vitmatte: Boosting image matting
with pretrained plain vision transformers,” <span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2305.15272</span> (2023).

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Chen, Z., Duan, Y., Wang, W., He, J., Lu, T., Dai, J., and Qiao, Y., “Vision
transformer adapter for dense predictions,” <span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2205.08534</span> (2022).

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Li, Y., Mao, H., Girshick, R., and He, K., “Exploring plain vision transformer
backbones for object detection,” in [<span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">European Conference on Computer
Vision</span> ], 280–296, Springer (2022).

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.19820" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.19822" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.19822">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.19822" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.19823" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 19:02:27 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
