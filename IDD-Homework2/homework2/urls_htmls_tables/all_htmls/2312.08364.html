<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2312.08364] View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data</title><meta property="og:description" content="Procedural synthetic data generation has received increasing attention in computer vision. Procedural signed distance functions (SDFs) are a powerful tool for modeling large-scale detailed scenes, but existing mesh ext…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2312.08364">

<!--Generated on Tue Feb 27 14:03:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zeyu Ma  Alexander Raistrick  Lahav Lipson  Jia Deng 
<br class="ltx_break">Department of Computer Science, Princeton University
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{zeyum, araistrick, llipson, jiadeng}@princeton.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Procedural synthetic data generation has received increasing attention in computer vision. Procedural signed distance functions (SDFs) are a powerful tool for modeling large-scale detailed scenes, but existing mesh extraction methods have artifacts or performance profiles that limit their use for synthetic data. We propose OcMesher, a mesh extraction algorithm that efficiently handles high-detail unbounded scenes with perfect view-consistency, with easy export to downstream real-time engines. The main novelty of our solution is an algorithm to construct an octree based on a given SDF and multiple camera views. We performed extensive experiments, and show our solution produces better synthetic data for training and evaluation of computer vision models.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2312.08364/assets/figures/colors.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="93" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;"> The Infinigen solution has low poly artifacts if the camera changes its location significantly, while our solution has no artifacts.</span></figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Synthetic visual data rendered from conventional computer graphics has seen increasing use in computer vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>, <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>. In 3D vision and embodied AI, synthetic data has been frequently used for training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>, <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>, <a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>, <a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>, evaluation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>, <a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>, and constructing virtual environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>. One advantage of synthetic data is that they can be procedurally generated, that is, created algorithmically by compact randomized mathematical rules. Procedural generation can greatly improve the diversity of synthetic data through infinite random variations and is a promising research direction that has received increasing attention. Procedural generation has been employed in generating both indoor scenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> as well as outdoor scenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>, <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Procedural signed distance functions (SDF) are a common technique for procedural scene generation. An SDF is a function that maps a 3D point to a real number that represents the signed distance to a 3D surface. A procedural SDF is an SDF expressed by compact mathematical rules. For example, a mountain range can be modelled as a procedural SDF using Perlin noise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite>. Procedural SDFs are powerful because just a few lines of code using primitive math functions can represent an arbitrarily large surface with arbitrarily fine details. For example, Infinigen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>, a recently released procedural generator of 3D scenes, makes extensive use of procedural SDFs to generate varied and detailed terrains.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To be used for synthetic data generation, an SDF must first be converted to a mesh, which is the universally supported format for common rendering engines. This process is called “mesh extraction”. The standard approach is to evaluate the SDF on a uniformly-spaced regular 3D grid and use the “Marching Cubes” algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>, which extracts a mesh from the cells that straddle the zero crossings of the SDF (i.e. iso-surface extraction).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">However, this standard approach does not work for unbounded scenes, which are scenes that contain visible geometry that is arbitrarily far away. For example, an ocean that extends to the horizon is unbounded. For such unbounded scenes, a uniformly spaced regular SDF grid will be prohibitively large to cover far-away geometry. An existing solution is to use a regular 3D grid that is spaced non-uniformly in a view-dependent way, with smaller spacing closer to the camera. This allows far-away geometry to be represented by large faces in the mesh, vastly improving efficiency. This is the solution adopted by Infinigen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>, which uses a view-dependent non-uniform spacing based on spherical coordinates. For convenience, we will refer to this solution as the “Infinigen solution”.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The Infinigen solution, however, still suffers from a major drawback in terms of view consistency. In this solution, the spacing of the grid is centered around a single camera location, with small cells near the camera and large cells from far away, such that the polygonal faces of the extracted mesh at different distances will project to about the same size in the pixel space. This works well if the camera stays in the same location, but if the camera changes its location significantly, visual artifacts will occur if we render the same mesh in the new camera view: coarse parts of the mesh can become newly visible or closer to the camera, creating an undesirable “low-poly” appearance as shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">These artifacts can be reduced by extracting a new view-dependent mesh when the camera changes position, which is the fix adopted by Infinigen. But this introduces two new issues: (1) generating a new mesh for each new view is costly and impractical for real-time rendering, which is desirable for simulated environments for embodied AI; and (2) switching between meshes causes severe flickering artifacts, unless they are extracted at prohibitively high resolution.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In this paper, we propose OcMesher, a new mesh extraction solution for unbounded scenes that avoids the pitfalls of existing solutions. OcMesher can generate a single mesh that represents unbounded geometry efficiently and can be rendered without artifacts across any pre-defined range of camera views. Our solution achieves a <em id="S1.p7.1.1" class="ltx_emph ltx_font_italic">combination</em> of capabilities that was not possible with existing solutions:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><em id="S1.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">Unbounded scenes: </em> Our solution can efficiently handle unbounded scenes that include arbitrarily distant geometry, without intractable memory requirements. This is not possible with the naive solution of mesh extraction from a uniformly spaced regular grid.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><em id="S1.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">Real-time rendering: </em> Our solution generates a single mesh that can be directly used by a real-time rendering engine like Unreal Engine <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>. The mesh renders well across a range of camera views, as long as the camera stays within an area predefined by the user. This is not possible with the Infinigen solution that uses a view-dependent non-uniformly-spaced regular grid.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><em id="S1.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">View consistency: </em> Because only a single mesh is needed, our solution has perfect view consistency, with zero flickering artifacts across the rendered video frames, regardless of the poly count of the mesh. Our perfect view consistency improves data quality. This is not possible with the Infinigen solution, which is not able to completely eliminate flickering even with a very high poly count.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">The core idea of our solution is to construct an <em id="S1.p8.1.1" class="ltx_emph ltx_font_italic">octree</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite> instead of a regular grid for mesh extraction. An octree can be understood as an irregular, multi-resolution grid. Given a user-defined range of camera views and a procedural SDF, we construct an octree that is high resolution around surfaces close to any camera view, but low resolution for locations in empty space or far away from all camera views. Our algorithm seeks to construct an <em id="S1.p8.1.2" class="ltx_emph ltx_font_italic">efficient</em> octree, minimizing the number of total cells used while maintaining visual quality of the renders. Once the octree is constructed, we perform dual contouring <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> to extract a mesh. Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the idea in comparison with existing solutions.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">The main novelty of our solution lies in the algorithm that constructs an octree based on a given SDF and <em id="S1.p9.1.1" class="ltx_emph ltx_font_italic">multiple</em> camera views. Existing techniques in the literature, including the computer graphics literature, have only addressed the octree construction dependent on a single view. To the best of our knowledge, we are the first to construct octrees dependent on multiple views.</p>
</div>
<figure id="S1.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F2.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/2Duniform.png" id="S1.F2.1.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(a)</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F2.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/2Dinfinigen.png" id="S1.F2.2.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(b)</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F2.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/2Dquad.png" id="S1.F2.3.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(c)</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.6.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.7.2" class="ltx_text" style="font-size:90%;">(a) Uniform marching cubes mesh extraction from cannot efficiently represent high-detail unbounded scenes; (b) Infinigen’s solution requires different meshes for new views, so introduces artifacts; (c) Our algorithm solves these issues by constructing an <em id="S1.F2.7.2.1" class="ltx_emph ltx_font_italic">efficient</em> octree and extracts the mesh from it to solve the issues.</span></figcaption>
</figure>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">We perform extensive experiments to validate the effectiveness of our solution. Experiments show that our solution can indeed eliminate the flickering artifacts present in Infinigen’s solution, leading to better synthetic data. In addition, we show our solution can generate unbounded environments that can be rendered at 50 FPS in Unreal Engine, seamlessly and without artifacts even with large view changes, a capability not available from existing solutions.
We encourage the reader to view the video <a target="_blank" href="https://youtu.be/YA1c5L0Ncuw" title="" class="ltx_ref ltx_href">here</a> for more qualitative results and please visit <a target="_blank" href="https://github.com/princeton-vl/OcMesher" title="" class="ltx_ref ltx_href">https://github.com/princeton-vl/OcMesher</a> for the code.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Procedural signed distance functions have been widely used in realistic scene generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>, <a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>, <a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>, from height maps to increasingly complex 3D compositions. Approaches to visualize or render scenes modelled as SDFs fall into many categories.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">SDF Rendering Algorithms</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">A potential alternative to our approach is to directly render an SDF without first extracting a mesh, using algorithms such as sphere tracing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>, segment tracing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>, quasi-analytic error-bounded (QAEB) ray tracing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>, or others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>, or even commercial engines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>. However, these generally require extra assumptions (e.g. Lipschitz continuity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>, or that the SDF represents a real Euclidean distance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>), and are generally less performant and widely adopted than mesh-based renderers. Our mesh-extraction solution can be used in popular mesh-based engines and simulators (e.g. Blender <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>, Unreal Engine <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>) without code additions, and trivially integrates with non-SDF-based assets (such as all assets besides terrain in Infinigen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>).</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Multi-Resolution Mesh Extraction</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Uniform iso-surface extraction algorithms like Marching Cubes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> and Marching Tetrahedra <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> are the standard approach to mesh extraction for implicit functions. However, they are less well suited when scene content is unbounded or not all equally important. Therefore, many existing works divide space into multiresolution tetrahedra <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>, or even polyhedra <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>, <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite>. Some works also divide space into an octree and use dual contouring algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>. Please refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite> for a more complete discussion. However, these works do not answer the question of how to determine the level of detail (LOD) when we have multiple cameras.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">View Dependent Mesh Extraction</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">To the best of our knowledge, all existing works focus on a single camera at a time and reuse the mesh for very few neighboring cameras. Many construct volume grids in world space and subdivide them recursively <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>. These works usually do not optimize the mesh size based on criteria such as occlusion and do not scale well for high-resolution images. For example, Scholz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite> focuses on real-time approaches and their generated mesh contains at most 700k triangles for a <math id="S2.SS0.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="1920\times 1080" display="inline"><semantics id="S2.SS0.SSS0.Px3.p1.1.m1.1a"><mrow id="S2.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml"><mn id="S2.SS0.SSS0.Px3.p1.1.m1.1.1.2" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml">1920</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS0.SSS0.Px3.p1.1.m1.1.1.1" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS0.SSS0.Px3.p1.1.m1.1.1.3" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.1.m1.1b"><apply id="S2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1"><times id="S2.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1.1"></times><cn type="integer" id="S2.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1.2">1920</cn><cn type="integer" id="S2.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.1.m1.1c">1920\times 1080</annotation></semantics></math> resolution image of more than 2 million pixels. Considering the deep depth dimension and the fact that they do not coarsen occluded triangles, their meshes are far from achieving pixel-level details in unbounded scenes.</p>
</div>
<div id="S2.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p2.1" class="ltx_p">Infinigen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> uses Marching Cubes in camera-space spherical coordinates, which is more scalable to high-resolution images. However, as mentioned previously, it is impractical for real-time rendering and suffers flickering and low poly artifacts due to switching between view-dependent meshes as the camera moves.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Flickering Removal</h4>

<div id="S2.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px4.p1.1" class="ltx_p">Besides generating a static mesh, alternative flickering removal approaches attempt to hide or smooth out LOD transitions by continuously deforming the mesh <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>. However, these techniques usually require the geometry to be a height map <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> or given as an initial high-poly mesh <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>, which is not applicable in our case with an SDF as input. Other blending algorithms only consider changes in image space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>, ignoring 3D geometry. These solutions also cannot export a static mesh for use in do real-time rendering engines.</p>
</div>
<figure id="S2.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F3.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/lodcri_none.png" id="S2.F3.1.g1" class="ltx_graphics ltx_img_square" width="401" height="401" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(a) Uniform grid</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F3.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/lodcri1.png" id="S2.F3.2.g1" class="ltx_graphics ltx_img_square" width="401" height="401" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(b) + Angular diameter</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F3.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/lodcri2.png" id="S2.F3.3.g1" class="ltx_graphics ltx_img_square" width="401" height="401" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(c) + Occupancy</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F3.4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/lodcri3.png" id="S2.F3.4.g1" class="ltx_graphics ltx_img_square" width="401" height="401" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(d) + Visibility</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.6.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.7.2" class="ltx_text" style="font-size:90%;">The mesh representation (2D analogy) gets increasingly efficient as we apply the 3 LOD criteria one by one.</span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Given an SDF and a set of camera poses, we aim to extract an iso-surface mesh that is high-detail when viewed from any camera but has minimum overall polygon-count and rendering cost. Our strategy is to construct an octree by recursively subdividing nodes until several level of detail criteria are met (Sec. <a href="#S3.SS1" title="3.1 Octree Level of Detail Criteria ‣ 3 Method ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), using a coarse-to-fine strategy to ensure computing the criterion is feasible (Sec. <a href="#S3.SS2" title="3.2 Coarse-to-Fine Octree Construction ‣ 3 Method ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>). We provide an overview in Fig.  <a href="#S3.F4" title="Figure 4 ‣ LOD based on Visibility ‣ 3.1 Octree Level of Detail Criteria ‣ 3 Method ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Octree Level of Detail Criteria</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We use an octree to achieve different levels of detail for different parts of the scene. Octrees represent an irregular, multi-resolution grid using a tree structure, where each node represents a cube in the space, and nodes can be recursively subdivided into eight octants. To construct an octree, we recursively subdivide based on the 3 criteria described below and in Fig. <a href="#S2.F3" title="Figure 3 ‣ Flickering Removal ‣ 2 Related Work ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, using a coarse to fine strategy as described in Sec. <a href="#S3.SS2" title="3.2 Coarse-to-Fine Octree Construction ‣ 3 Method ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">LOD based on Projected Angular Diameter</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">The most important LOD criterion is the projected angular diameter of the octree node in the camera views, which is inversely proportional to the distance to the cameras. We compute the maximum angular diameter <math id="S3.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="A_{\text{node}}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.1.m1.1a"><msub id="S3.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">A</mi><mtext id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3a.cmml">node</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2">𝐴</ci><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3a.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3">node</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.1.m1.1c">A_{\text{node}}</annotation></semantics></math> considering all cameras:</p>
</div>
<div id="S3.SS1.SSS0.Px1.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="A_{\text{node}}=L_{\text{node}}/\min\limits_{\forall\text{cam}}\text{dist}(\text{node},\text{cam})" display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.3" xref="S3.E1.m1.2.3.cmml"><msub id="S3.E1.m1.2.3.2" xref="S3.E1.m1.2.3.2.cmml"><mi id="S3.E1.m1.2.3.2.2" xref="S3.E1.m1.2.3.2.2.cmml">A</mi><mtext id="S3.E1.m1.2.3.2.3" xref="S3.E1.m1.2.3.2.3a.cmml">node</mtext></msub><mo id="S3.E1.m1.2.3.1" xref="S3.E1.m1.2.3.1.cmml">=</mo><mrow id="S3.E1.m1.2.3.3" xref="S3.E1.m1.2.3.3.cmml"><mrow id="S3.E1.m1.2.3.3.2" xref="S3.E1.m1.2.3.3.2.cmml"><msub id="S3.E1.m1.2.3.3.2.2" xref="S3.E1.m1.2.3.3.2.2.cmml"><mi id="S3.E1.m1.2.3.3.2.2.2" xref="S3.E1.m1.2.3.3.2.2.2.cmml">L</mi><mtext id="S3.E1.m1.2.3.3.2.2.3" xref="S3.E1.m1.2.3.3.2.2.3a.cmml">node</mtext></msub><mo id="S3.E1.m1.2.3.3.2.1" xref="S3.E1.m1.2.3.3.2.1.cmml">/</mo><mrow id="S3.E1.m1.2.3.3.2.3" xref="S3.E1.m1.2.3.3.2.3.cmml"><munder id="S3.E1.m1.2.3.3.2.3.1" xref="S3.E1.m1.2.3.3.2.3.1.cmml"><mi id="S3.E1.m1.2.3.3.2.3.1.2" xref="S3.E1.m1.2.3.3.2.3.1.2.cmml">min</mi><mrow id="S3.E1.m1.2.3.3.2.3.1.3" xref="S3.E1.m1.2.3.3.2.3.1.3.cmml"><mo rspace="0.167em" id="S3.E1.m1.2.3.3.2.3.1.3.1" xref="S3.E1.m1.2.3.3.2.3.1.3.1.cmml">∀</mo><mtext id="S3.E1.m1.2.3.3.2.3.1.3.2" xref="S3.E1.m1.2.3.3.2.3.1.3.2a.cmml">cam</mtext></mrow></munder><mo lspace="0.167em" id="S3.E1.m1.2.3.3.2.3a" xref="S3.E1.m1.2.3.3.2.3.cmml">⁡</mo><mtext id="S3.E1.m1.2.3.3.2.3.2" xref="S3.E1.m1.2.3.3.2.3.2a.cmml">dist</mtext></mrow></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.3.3.1" xref="S3.E1.m1.2.3.3.1.cmml">​</mo><mrow id="S3.E1.m1.2.3.3.3.2" xref="S3.E1.m1.2.3.3.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.3.3.3.2.1" xref="S3.E1.m1.2.3.3.3.1.cmml">(</mo><mtext id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1a.cmml">node</mtext><mo id="S3.E1.m1.2.3.3.3.2.2" xref="S3.E1.m1.2.3.3.3.1.cmml">,</mo><mtext id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2a.cmml">cam</mtext><mo stretchy="false" id="S3.E1.m1.2.3.3.3.2.3" xref="S3.E1.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.3.cmml" xref="S3.E1.m1.2.3"><eq id="S3.E1.m1.2.3.1.cmml" xref="S3.E1.m1.2.3.1"></eq><apply id="S3.E1.m1.2.3.2.cmml" xref="S3.E1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.2.1.cmml" xref="S3.E1.m1.2.3.2">subscript</csymbol><ci id="S3.E1.m1.2.3.2.2.cmml" xref="S3.E1.m1.2.3.2.2">𝐴</ci><ci id="S3.E1.m1.2.3.2.3a.cmml" xref="S3.E1.m1.2.3.2.3"><mtext mathsize="70%" id="S3.E1.m1.2.3.2.3.cmml" xref="S3.E1.m1.2.3.2.3">node</mtext></ci></apply><apply id="S3.E1.m1.2.3.3.cmml" xref="S3.E1.m1.2.3.3"><times id="S3.E1.m1.2.3.3.1.cmml" xref="S3.E1.m1.2.3.3.1"></times><apply id="S3.E1.m1.2.3.3.2.cmml" xref="S3.E1.m1.2.3.3.2"><divide id="S3.E1.m1.2.3.3.2.1.cmml" xref="S3.E1.m1.2.3.3.2.1"></divide><apply id="S3.E1.m1.2.3.3.2.2.cmml" xref="S3.E1.m1.2.3.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.2.2.1.cmml" xref="S3.E1.m1.2.3.3.2.2">subscript</csymbol><ci id="S3.E1.m1.2.3.3.2.2.2.cmml" xref="S3.E1.m1.2.3.3.2.2.2">𝐿</ci><ci id="S3.E1.m1.2.3.3.2.2.3a.cmml" xref="S3.E1.m1.2.3.3.2.2.3"><mtext mathsize="70%" id="S3.E1.m1.2.3.3.2.2.3.cmml" xref="S3.E1.m1.2.3.3.2.2.3">node</mtext></ci></apply><apply id="S3.E1.m1.2.3.3.2.3.cmml" xref="S3.E1.m1.2.3.3.2.3"><apply id="S3.E1.m1.2.3.3.2.3.1.cmml" xref="S3.E1.m1.2.3.3.2.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.2.3.1.1.cmml" xref="S3.E1.m1.2.3.3.2.3.1">subscript</csymbol><min id="S3.E1.m1.2.3.3.2.3.1.2.cmml" xref="S3.E1.m1.2.3.3.2.3.1.2"></min><apply id="S3.E1.m1.2.3.3.2.3.1.3.cmml" xref="S3.E1.m1.2.3.3.2.3.1.3"><csymbol cd="latexml" id="S3.E1.m1.2.3.3.2.3.1.3.1.cmml" xref="S3.E1.m1.2.3.3.2.3.1.3.1">for-all</csymbol><ci id="S3.E1.m1.2.3.3.2.3.1.3.2a.cmml" xref="S3.E1.m1.2.3.3.2.3.1.3.2"><mtext mathsize="70%" id="S3.E1.m1.2.3.3.2.3.1.3.2.cmml" xref="S3.E1.m1.2.3.3.2.3.1.3.2">cam</mtext></ci></apply></apply><ci id="S3.E1.m1.2.3.3.2.3.2a.cmml" xref="S3.E1.m1.2.3.3.2.3.2"><mtext id="S3.E1.m1.2.3.3.2.3.2.cmml" xref="S3.E1.m1.2.3.3.2.3.2">dist</mtext></ci></apply></apply><interval closure="open" id="S3.E1.m1.2.3.3.3.1.cmml" xref="S3.E1.m1.2.3.3.3.2"><ci id="S3.E1.m1.1.1a.cmml" xref="S3.E1.m1.1.1"><mtext id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">node</mtext></ci><ci id="S3.E1.m1.2.2a.cmml" xref="S3.E1.m1.2.2"><mtext id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">cam</mtext></ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">A_{\text{node}}=L_{\text{node}}/\min\limits_{\forall\text{cam}}\text{dist}(\text{node},\text{cam})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS0.Px1.p3" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p3.8" class="ltx_p">Where <math id="S3.SS1.SSS0.Px1.p3.1.m1.1" class="ltx_Math" alttext="L_{\text{node}}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p3.1.m1.1a"><msub id="S3.SS1.SSS0.Px1.p3.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.2" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.2.cmml">L</mi><mtext id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.3" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.3a.cmml">node</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p3.1.m1.1b"><apply id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.2">𝐿</ci><ci id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.3a.cmml" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.3">node</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p3.1.m1.1c">L_{\text{node}}</annotation></semantics></math> is the side length of the cube represented by the node, <span id="S3.SS1.SSS0.Px1.p3.8.1" class="ltx_text ltx_font_typewriter">cam</span> iterates through all the cameras, and <span id="S3.SS1.SSS0.Px1.p3.8.2" class="ltx_text ltx_font_typewriter">dist</span> computes the distance of the center of the node to each camera. We compare <math id="S3.SS1.SSS0.Px1.p3.2.m2.1" class="ltx_Math" alttext="A_{\text{node}}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p3.2.m2.1a"><msub id="S3.SS1.SSS0.Px1.p3.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.2" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.2.cmml">A</mi><mtext id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.3" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.3a.cmml">node</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p3.2.m2.1b"><apply id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.2">𝐴</ci><ci id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.3a.cmml" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.3">node</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p3.2.m2.1c">A_{\text{node}}</annotation></semantics></math> to our target angular diameter <math id="S3.SS1.SSS0.Px1.p3.3.m3.1" class="ltx_Math" alttext="\hat{A}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p3.3.m3.1a"><mover accent="true" id="S3.SS1.SSS0.Px1.p3.3.m3.1.1" xref="S3.SS1.SSS0.Px1.p3.3.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p3.3.m3.1.1.2" xref="S3.SS1.SSS0.Px1.p3.3.m3.1.1.2.cmml">A</mi><mo id="S3.SS1.SSS0.Px1.p3.3.m3.1.1.1" xref="S3.SS1.SSS0.Px1.p3.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p3.3.m3.1b"><apply id="S3.SS1.SSS0.Px1.p3.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.3.m3.1.1"><ci id="S3.SS1.SSS0.Px1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.3.m3.1.1.1">^</ci><ci id="S3.SS1.SSS0.Px1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p3.3.m3.1.1.2">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p3.3.m3.1c">\hat{A}</annotation></semantics></math>, which is computed based on the field of view (FOV) of the cameras and the image resolution.
If <math id="S3.SS1.SSS0.Px1.p3.4.m4.1" class="ltx_Math" alttext="A_{\text{node}}&gt;\hat{A}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p3.4.m4.1a"><mrow id="S3.SS1.SSS0.Px1.p3.4.m4.1.1" xref="S3.SS1.SSS0.Px1.p3.4.m4.1.1.cmml"><msub id="S3.SS1.SSS0.Px1.p3.4.m4.1.1.2" xref="S3.SS1.SSS0.Px1.p3.4.m4.1.1.2.cmml"><mi id="S3.SS1.SSS0.Px1.p3.4.m4.1.1.2.2" xref="S3.SS1.SSS0.Px1.p3.4.m4.1.1.2.2.cmml">A</mi><mtext id="S3.SS1.SSS0.Px1.p3.4.m4.1.1.2.3" xref="S3.SS1.SSS0.Px1.p3.4.m4.1.1.2.3a.cmml">node</mtext></msub><mo id="S3.SS1.SSS0.Px1.p3.4.m4.1.1.1" xref="S3.SS1.SSS0.Px1.p3.4.m4.1.1.1.cmml">&gt;</mo><mover accent="true" id="S3.SS1.SSS0.Px1.p3.4.m4.1.1.3" xref="S3.SS1.SSS0.Px1.p3.4.m4.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px1.p3.4.m4.1.1.3.2" xref="S3.SS1.SSS0.Px1.p3.4.m4.1.1.3.2.cmml">A</mi><mo id="S3.SS1.SSS0.Px1.p3.4.m4.1.1.3.1" xref="S3.SS1.SSS0.Px1.p3.4.m4.1.1.3.1.cmml">^</mo></mover></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p3.4.m4.1b"><apply id="S3.SS1.SSS0.Px1.p3.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.4.m4.1.1"><gt id="S3.SS1.SSS0.Px1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.4.m4.1.1.1"></gt><apply id="S3.SS1.SSS0.Px1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p3.4.m4.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px1.p3.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p3.4.m4.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px1.p3.4.m4.1.1.2.2">𝐴</ci><ci id="S3.SS1.SSS0.Px1.p3.4.m4.1.1.2.3a.cmml" xref="S3.SS1.SSS0.Px1.p3.4.m4.1.1.2.3"><mtext mathsize="70%" id="S3.SS1.SSS0.Px1.p3.4.m4.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px1.p3.4.m4.1.1.2.3">node</mtext></ci></apply><apply id="S3.SS1.SSS0.Px1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p3.4.m4.1.1.3"><ci id="S3.SS1.SSS0.Px1.p3.4.m4.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px1.p3.4.m4.1.1.3.1">^</ci><ci id="S3.SS1.SSS0.Px1.p3.4.m4.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px1.p3.4.m4.1.1.3.2">𝐴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p3.4.m4.1c">A_{\text{node}}&gt;\hat{A}</annotation></semantics></math>, we either subdivide the node into 8 children and compute the angular diameter of each child recursively, or subdivide the node into a denser grid. In practice, there will always be some node containing one of the cameras, and <math id="S3.SS1.SSS0.Px1.p3.5.m5.1" class="ltx_Math" alttext="A_{\text{node}}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p3.5.m5.1a"><msub id="S3.SS1.SSS0.Px1.p3.5.m5.1.1" xref="S3.SS1.SSS0.Px1.p3.5.m5.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p3.5.m5.1.1.2" xref="S3.SS1.SSS0.Px1.p3.5.m5.1.1.2.cmml">A</mi><mtext id="S3.SS1.SSS0.Px1.p3.5.m5.1.1.3" xref="S3.SS1.SSS0.Px1.p3.5.m5.1.1.3a.cmml">node</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p3.5.m5.1b"><apply id="S3.SS1.SSS0.Px1.p3.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p3.5.m5.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p3.5.m5.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p3.5.m5.1.1.2">𝐴</ci><ci id="S3.SS1.SSS0.Px1.p3.5.m5.1.1.3a.cmml" xref="S3.SS1.SSS0.Px1.p3.5.m5.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS0.Px1.p3.5.m5.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p3.5.m5.1.1.3">node</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p3.5.m5.1c">A_{\text{node}}</annotation></semantics></math> from Eq. <a href="#S3.E1" title="Equation 1 ‣ LOD based on Projected Angular Diameter ‣ 3.1 Octree Level of Detail Criteria ‣ 3 Method ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> can never be less than <math id="S3.SS1.SSS0.Px1.p3.6.m6.1" class="ltx_Math" alttext="\hat{A}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p3.6.m6.1a"><mover accent="true" id="S3.SS1.SSS0.Px1.p3.6.m6.1.1" xref="S3.SS1.SSS0.Px1.p3.6.m6.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p3.6.m6.1.1.2" xref="S3.SS1.SSS0.Px1.p3.6.m6.1.1.2.cmml">A</mi><mo id="S3.SS1.SSS0.Px1.p3.6.m6.1.1.1" xref="S3.SS1.SSS0.Px1.p3.6.m6.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p3.6.m6.1b"><apply id="S3.SS1.SSS0.Px1.p3.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.6.m6.1.1"><ci id="S3.SS1.SSS0.Px1.p3.6.m6.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.6.m6.1.1.1">^</ci><ci id="S3.SS1.SSS0.Px1.p3.6.m6.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p3.6.m6.1.1.2">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p3.6.m6.1c">\hat{A}</annotation></semantics></math>. To avoid this issue, we clamp all distances to be at least <math id="S3.SS1.SSS0.Px1.p3.7.m7.1" class="ltx_Math" alttext="D_{\text{min}}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p3.7.m7.1a"><msub id="S3.SS1.SSS0.Px1.p3.7.m7.1.1" xref="S3.SS1.SSS0.Px1.p3.7.m7.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p3.7.m7.1.1.2" xref="S3.SS1.SSS0.Px1.p3.7.m7.1.1.2.cmml">D</mi><mtext id="S3.SS1.SSS0.Px1.p3.7.m7.1.1.3" xref="S3.SS1.SSS0.Px1.p3.7.m7.1.1.3a.cmml">min</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p3.7.m7.1b"><apply id="S3.SS1.SSS0.Px1.p3.7.m7.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p3.7.m7.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p3.7.m7.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p3.7.m7.1.1.2">𝐷</ci><ci id="S3.SS1.SSS0.Px1.p3.7.m7.1.1.3a.cmml" xref="S3.SS1.SSS0.Px1.p3.7.m7.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS0.Px1.p3.7.m7.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p3.7.m7.1.1.3">min</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p3.7.m7.1c">D_{\text{min}}</annotation></semantics></math> (a hyperparameter set by the user), and assert that no is actually within distance <math id="S3.SS1.SSS0.Px1.p3.8.m8.1" class="ltx_Math" alttext="D_{\text{min}}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p3.8.m8.1a"><msub id="S3.SS1.SSS0.Px1.p3.8.m8.1.1" xref="S3.SS1.SSS0.Px1.p3.8.m8.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p3.8.m8.1.1.2" xref="S3.SS1.SSS0.Px1.p3.8.m8.1.1.2.cmml">D</mi><mtext id="S3.SS1.SSS0.Px1.p3.8.m8.1.1.3" xref="S3.SS1.SSS0.Px1.p3.8.m8.1.1.3a.cmml">min</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p3.8.m8.1b"><apply id="S3.SS1.SSS0.Px1.p3.8.m8.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p3.8.m8.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p3.8.m8.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p3.8.m8.1.1.2">𝐷</ci><ci id="S3.SS1.SSS0.Px1.p3.8.m8.1.1.3a.cmml" xref="S3.SS1.SSS0.Px1.p3.8.m8.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS0.Px1.p3.8.m8.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p3.8.m8.1.1.3">min</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p3.8.m8.1c">D_{\text{min}}</annotation></semantics></math> of the surface.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">LOD based on Occupancy</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">To save computation, we avoid subdividing any node determined to be wholly unoccupied, as these nodes should not contain zero crossings or affect the final mesh.</p>
</div>
<div id="S3.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p2.1" class="ltx_p">Exactly determining whether a node is occupied is expensive. For example, we could completely subdivide every node to meet the angular diameter criterion, and then merge empty nodes, but this is prohibitively expensive. Instead, we determine node occupancy using heuristics based on the SDF value of its 8 bounding vertices, without performing a full subdivision. This misses some nodes that are actually occupied if we further subdivide, and unless handled carefully, will cause dual contouring to create spiky meshes near any sharp LOD transitions. Therefore, we need to propagate the occupancy from the existing surface to its neighbors, as discussed later in Sec. <a href="#S3.SS2" title="3.2 Coarse-to-Fine Octree Construction ‣ 3 Method ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">LOD based on Visibility</h4>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">Finally, we avoid subdividing nodes which are either occluded by a surface or outside the camera frustrum. In either case, we say that the octree node is not <span id="S3.SS1.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">visible</span>, since it can only affect the image through shadows, reflections, or bounce lighting. For these not-directly-visible areas, we stop subdivision earlier, i.e. we use <math id="S3.SS1.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="\hat{A}_{\text{inv}}&gt;\hat{A}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.1.m1.1a"><mrow id="S3.SS1.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml"><msub id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml"><mover accent="true" id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.2.cmml"><mi id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.2.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.2.2.cmml">A</mi><mo id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.2.1" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.2.1.cmml">^</mo></mover><mtext id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.3" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.3a.cmml">inv</mtext></msub><mo id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.1" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.1.cmml">&gt;</mo><mover accent="true" id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.2.cmml">A</mi><mo id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.1" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.1.cmml">^</mo></mover></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1"><gt id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.1"></gt><apply id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2">subscript</csymbol><apply id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.2"><ci id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.2.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.2.1">^</ci><ci id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.2.2">𝐴</ci></apply><ci id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.3a.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.3"><mtext mathsize="70%" id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.3">inv</mtext></ci></apply><apply id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3"><ci id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.1">^</ci><ci id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.2">𝐴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.1.m1.1c">\hat{A}_{\text{inv}}&gt;\hat{A}</annotation></semantics></math> as the diameter criterion. This reduces overall computation and final mesh size, since the sum of angular sizes of nodes visible to the camera is bounded and should not grow with depth. However, similarly to occupancy checking, computing precise visibility would require completely subdividing all nodes, which is too expensive. We instead decide which node is visible in the middle of the subdivision.</p>
</div>
<div id="S3.SS1.SSS0.Px3.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p2.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ LOD based on Visibility ‣ 3.1 Octree Level of Detail Criteria ‣ 3 Method ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> evaluates the effect of the 3 criteria for the scene in Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.4 Computational Complexity ‣ 3 Method ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> in a low-resolution setting, and for the uniform grid baseline, we could only give the theoretical prediction. We can see all the criteria, especially the angular diameter criterion and the visibility criterion, can significantly reduce redundancy in the octree nodes and the resulting mesh.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.5.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.6.2" class="ltx_text" style="font-size:90%;">Ablations on LOD Criteria</span></figcaption>
<div id="S3.T1.3" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:160.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(63.4pt,-23.4pt) scale(1.41348239125695,1.41348239125695) ;">
<table id="S3.T1.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.3.3.4.1" class="ltx_tr">
<th id="S3.T1.3.3.4.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S3.T1.3.3.4.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">
<table id="S3.T1.3.3.4.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.3.3.4.1.2.1.1" class="ltx_tr">
<td id="S3.T1.3.3.4.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"># Octree</td>
</tr>
<tr id="S3.T1.3.3.4.1.2.1.2" class="ltx_tr">
<td id="S3.T1.3.3.4.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Leaf Nodes</td>
</tr>
</table>
</th>
<th id="S3.T1.3.3.4.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">
<table id="S3.T1.3.3.4.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.3.3.4.1.3.1.1" class="ltx_tr">
<td id="S3.T1.3.3.4.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"># Mesh</td>
</tr>
<tr id="S3.T1.3.3.4.1.3.1.2" class="ltx_tr">
<td id="S3.T1.3.3.4.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Vertices</td>
</tr>
</table>
</th>
<th id="S3.T1.3.3.4.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">
<table id="S3.T1.3.3.4.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.3.3.4.1.4.1.1" class="ltx_tr">
<td id="S3.T1.3.3.4.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"># Mesh</td>
</tr>
<tr id="S3.T1.3.3.4.1.4.1.2" class="ltx_tr">
<td id="S3.T1.3.3.4.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Faces</td>
</tr>
</table>
</th>
</tr>
<tr id="S3.T1.3.3.3" class="ltx_tr">
<th id="S3.T1.3.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Ang. ✗  Occup. ✗  Vis. ✗</th>
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><math id="S3.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="3.5\times 10^{13}" display="inline"><semantics id="S3.T1.1.1.1.1.m1.1a"><mrow id="S3.T1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.cmml"><mn id="S3.T1.1.1.1.1.m1.1.1.2" xref="S3.T1.1.1.1.1.m1.1.1.2.cmml">3.5</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T1.1.1.1.1.m1.1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.1.cmml">×</mo><msup id="S3.T1.1.1.1.1.m1.1.1.3" xref="S3.T1.1.1.1.1.m1.1.1.3.cmml"><mn id="S3.T1.1.1.1.1.m1.1.1.3.2" xref="S3.T1.1.1.1.1.m1.1.1.3.2.cmml">10</mn><mn id="S3.T1.1.1.1.1.m1.1.1.3.3" xref="S3.T1.1.1.1.1.m1.1.1.3.3.cmml">13</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><apply id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1"><times id="S3.T1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1.1"></times><cn type="float" id="S3.T1.1.1.1.1.m1.1.1.2.cmml" xref="S3.T1.1.1.1.1.m1.1.1.2">3.5</cn><apply id="S3.T1.1.1.1.1.m1.1.1.3.cmml" xref="S3.T1.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.T1.1.1.1.1.m1.1.1.3.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S3.T1.1.1.1.1.m1.1.1.3.2.cmml" xref="S3.T1.1.1.1.1.m1.1.1.3.2">10</cn><cn type="integer" id="S3.T1.1.1.1.1.m1.1.1.3.3.cmml" xref="S3.T1.1.1.1.1.m1.1.1.3.3">13</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">3.5\times 10^{13}</annotation></semantics></math></th>
<th id="S3.T1.2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><math id="S3.T1.2.2.2.2.m1.1" class="ltx_Math" alttext="&gt;1\times 10^{9}" display="inline"><semantics id="S3.T1.2.2.2.2.m1.1a"><mrow id="S3.T1.2.2.2.2.m1.1.1" xref="S3.T1.2.2.2.2.m1.1.1.cmml"><mi id="S3.T1.2.2.2.2.m1.1.1.2" xref="S3.T1.2.2.2.2.m1.1.1.2.cmml"></mi><mo id="S3.T1.2.2.2.2.m1.1.1.1" xref="S3.T1.2.2.2.2.m1.1.1.1.cmml">&gt;</mo><mrow id="S3.T1.2.2.2.2.m1.1.1.3" xref="S3.T1.2.2.2.2.m1.1.1.3.cmml"><mn id="S3.T1.2.2.2.2.m1.1.1.3.2" xref="S3.T1.2.2.2.2.m1.1.1.3.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T1.2.2.2.2.m1.1.1.3.1" xref="S3.T1.2.2.2.2.m1.1.1.3.1.cmml">×</mo><msup id="S3.T1.2.2.2.2.m1.1.1.3.3" xref="S3.T1.2.2.2.2.m1.1.1.3.3.cmml"><mn id="S3.T1.2.2.2.2.m1.1.1.3.3.2" xref="S3.T1.2.2.2.2.m1.1.1.3.3.2.cmml">10</mn><mn id="S3.T1.2.2.2.2.m1.1.1.3.3.3" xref="S3.T1.2.2.2.2.m1.1.1.3.3.3.cmml">9</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.m1.1b"><apply id="S3.T1.2.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.2.m1.1.1"><gt id="S3.T1.2.2.2.2.m1.1.1.1.cmml" xref="S3.T1.2.2.2.2.m1.1.1.1"></gt><csymbol cd="latexml" id="S3.T1.2.2.2.2.m1.1.1.2.cmml" xref="S3.T1.2.2.2.2.m1.1.1.2">absent</csymbol><apply id="S3.T1.2.2.2.2.m1.1.1.3.cmml" xref="S3.T1.2.2.2.2.m1.1.1.3"><times id="S3.T1.2.2.2.2.m1.1.1.3.1.cmml" xref="S3.T1.2.2.2.2.m1.1.1.3.1"></times><cn type="integer" id="S3.T1.2.2.2.2.m1.1.1.3.2.cmml" xref="S3.T1.2.2.2.2.m1.1.1.3.2">1</cn><apply id="S3.T1.2.2.2.2.m1.1.1.3.3.cmml" xref="S3.T1.2.2.2.2.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.T1.2.2.2.2.m1.1.1.3.3.1.cmml" xref="S3.T1.2.2.2.2.m1.1.1.3.3">superscript</csymbol><cn type="integer" id="S3.T1.2.2.2.2.m1.1.1.3.3.2.cmml" xref="S3.T1.2.2.2.2.m1.1.1.3.3.2">10</cn><cn type="integer" id="S3.T1.2.2.2.2.m1.1.1.3.3.3.cmml" xref="S3.T1.2.2.2.2.m1.1.1.3.3.3">9</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.m1.1c">&gt;1\times 10^{9}</annotation></semantics></math></th>
<th id="S3.T1.3.3.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><math id="S3.T1.3.3.3.3.m1.1" class="ltx_Math" alttext="&gt;1\times 10^{9}" display="inline"><semantics id="S3.T1.3.3.3.3.m1.1a"><mrow id="S3.T1.3.3.3.3.m1.1.1" xref="S3.T1.3.3.3.3.m1.1.1.cmml"><mi id="S3.T1.3.3.3.3.m1.1.1.2" xref="S3.T1.3.3.3.3.m1.1.1.2.cmml"></mi><mo id="S3.T1.3.3.3.3.m1.1.1.1" xref="S3.T1.3.3.3.3.m1.1.1.1.cmml">&gt;</mo><mrow id="S3.T1.3.3.3.3.m1.1.1.3" xref="S3.T1.3.3.3.3.m1.1.1.3.cmml"><mn id="S3.T1.3.3.3.3.m1.1.1.3.2" xref="S3.T1.3.3.3.3.m1.1.1.3.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T1.3.3.3.3.m1.1.1.3.1" xref="S3.T1.3.3.3.3.m1.1.1.3.1.cmml">×</mo><msup id="S3.T1.3.3.3.3.m1.1.1.3.3" xref="S3.T1.3.3.3.3.m1.1.1.3.3.cmml"><mn id="S3.T1.3.3.3.3.m1.1.1.3.3.2" xref="S3.T1.3.3.3.3.m1.1.1.3.3.2.cmml">10</mn><mn id="S3.T1.3.3.3.3.m1.1.1.3.3.3" xref="S3.T1.3.3.3.3.m1.1.1.3.3.3.cmml">9</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.m1.1b"><apply id="S3.T1.3.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.3.m1.1.1"><gt id="S3.T1.3.3.3.3.m1.1.1.1.cmml" xref="S3.T1.3.3.3.3.m1.1.1.1"></gt><csymbol cd="latexml" id="S3.T1.3.3.3.3.m1.1.1.2.cmml" xref="S3.T1.3.3.3.3.m1.1.1.2">absent</csymbol><apply id="S3.T1.3.3.3.3.m1.1.1.3.cmml" xref="S3.T1.3.3.3.3.m1.1.1.3"><times id="S3.T1.3.3.3.3.m1.1.1.3.1.cmml" xref="S3.T1.3.3.3.3.m1.1.1.3.1"></times><cn type="integer" id="S3.T1.3.3.3.3.m1.1.1.3.2.cmml" xref="S3.T1.3.3.3.3.m1.1.1.3.2">1</cn><apply id="S3.T1.3.3.3.3.m1.1.1.3.3.cmml" xref="S3.T1.3.3.3.3.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.T1.3.3.3.3.m1.1.1.3.3.1.cmml" xref="S3.T1.3.3.3.3.m1.1.1.3.3">superscript</csymbol><cn type="integer" id="S3.T1.3.3.3.3.m1.1.1.3.3.2.cmml" xref="S3.T1.3.3.3.3.m1.1.1.3.3.2">10</cn><cn type="integer" id="S3.T1.3.3.3.3.m1.1.1.3.3.3.cmml" xref="S3.T1.3.3.3.3.m1.1.1.3.3.3">9</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.m1.1c">&gt;1\times 10^{9}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.3.3.5.1" class="ltx_tr">
<th id="S3.T1.3.3.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ang. ✓  Occup. ✗  Vis. ✗</th>
<th id="S3.T1.3.3.5.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">8,686,602</th>
<td id="S3.T1.3.3.5.1.3" class="ltx_td ltx_align_left">143,726</td>
<td id="S3.T1.3.3.5.1.4" class="ltx_td ltx_align_left">287,560</td>
</tr>
<tr id="S3.T1.3.3.6.2" class="ltx_tr">
<th id="S3.T1.3.3.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ang. ✓  Occup. ✓  Vis. ✗</th>
<th id="S3.T1.3.3.6.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">1,035,232</th>
<td id="S3.T1.3.3.6.2.3" class="ltx_td ltx_align_left">121,802</td>
<td id="S3.T1.3.3.6.2.4" class="ltx_td ltx_align_left">243,616</td>
</tr>
<tr id="S3.T1.3.3.7.3" class="ltx_tr">
<th id="S3.T1.3.3.7.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">Ang. ✓  Occup. ✓  Vis. ✓</th>
<th id="S3.T1.3.3.7.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">736</th>
<td id="S3.T1.3.3.7.3.3" class="ltx_td ltx_align_left ltx_border_b">1,535</td>
<td id="S3.T1.3.3.7.3.4" class="ltx_td ltx_align_left ltx_border_b">3,079</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F4.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/cflegend.png" id="S3.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="180" height="71" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F4.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/cf1.png" id="S3.F4.2.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(a) Coarse full octree</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F4.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/cf2.png" id="S3.F4.3.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(b) Occup. and vis. test</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F4.4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/lodcri3.png" id="S3.F4.4.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(c) Fine visible octree</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.6.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.7.2" class="ltx_text" style="font-size:90%;">We construct the octree from coarse to fine.</span></figcaption>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Coarse-to-Fine Octree Construction</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We use a coarse-to-fine strategy to compute occupancy and visibility criteria efficiently. As illustrated in Fig. <a href="#S3.F4" title="Figure 4 ‣ LOD based on Visibility ‣ 3.1 Octree Level of Detail Criteria ‣ 3 Method ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we construct the octree in 3 steps: the coarse full octree, the occupancy and visibility test, and the fine visible octree. We explain them step by step.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Coarse Full Octree</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.4" class="ltx_p">First, we construct a coarse full octree based solely on the projected angular diameter criterion. We initialize the octree with a single root node with size <math id="S3.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="L_{\mathrm{root}}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><msub id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml">L</mi><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml">root</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2">𝐿</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3">root</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">L_{\mathrm{root}}</annotation></semantics></math>. We use <math id="S3.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="L_{\mathrm{root}}=1000\mathrm{m}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a"><mrow id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><msub id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2.cmml">L</mi><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3.cmml">root</mi></msub><mo id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml">=</mo><mrow id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml"><mn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.cmml">1000</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1"><eq id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1"></eq><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2">𝐿</ci><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3">root</ci></apply><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3"><times id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.1"></times><cn type="integer" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2">1000</cn><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3">m</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">L_{\mathrm{root}}=1000\mathrm{m}</annotation></semantics></math> for our Infinigen experiments, but this value is unbounded - one could equally use 5000km (horizon at sea-level) or 50,000km (approx maximum horizon distance on earth) with minimal overhead. We initialize a max-priority queue with the root node, which is sorted by key <math id="S3.SS2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="A_{\text{node}}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.3.m3.1a"><msub id="S3.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml">A</mi><mtext id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3a.cmml">node</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2">𝐴</ci><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3a.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3"><mtext mathsize="70%" id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3">node</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.1c">A_{\text{node}}</annotation></semantics></math>, then repeatedly subdivide the top node while <math id="S3.SS2.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="A_{\text{node}}&gt;\hat{A}_{\text{inv}}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.4.m4.1a"><mrow id="S3.SS2.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml"><msub id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.2.cmml">A</mi><mtext id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.3a.cmml">node</mtext></msub><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.cmml">&gt;</mo><msub id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml"><mover accent="true" id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.2.cmml">A</mi><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.1.cmml">^</mo></mover><mtext id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.3a.cmml">inv</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.4.m4.1b"><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1"><gt id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1"></gt><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.2">𝐴</ci><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.3a.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.3"><mtext mathsize="70%" id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.3">node</mtext></ci></apply><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3">subscript</csymbol><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2"><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.1">^</ci><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.2">𝐴</ci></apply><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.3a.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.3"><mtext mathsize="70%" id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.3">inv</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.4.m4.1c">A_{\text{node}}&gt;\hat{A}_{\text{inv}}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p2.5" class="ltx_p">We limit the size of the coarse octree to <math id="S3.SS2.SSS0.Px1.p2.1.m1.2" class="ltx_Math" alttext="S_{\text{max}}=500,000" display="inline"><semantics id="S3.SS2.SSS0.Px1.p2.1.m1.2a"><mrow id="S3.SS2.SSS0.Px1.p2.1.m1.2.3" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3.cmml"><msub id="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2.cmml"><mi id="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2.2" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2.2.cmml">S</mi><mtext id="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2.3" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2.3a.cmml">max</mtext></msub><mo id="S3.SS2.SSS0.Px1.p2.1.m1.2.3.1" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3.1.cmml">=</mo><mrow id="S3.SS2.SSS0.Px1.p2.1.m1.2.3.3.2" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3.3.1.cmml"><mn id="S3.SS2.SSS0.Px1.p2.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p2.1.m1.1.1.cmml">500</mn><mo id="S3.SS2.SSS0.Px1.p2.1.m1.2.3.3.2.1" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3.3.1.cmml">,</mo><mn id="S3.SS2.SSS0.Px1.p2.1.m1.2.2" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.2.cmml">000</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.1.m1.2b"><apply id="S3.SS2.SSS0.Px1.p2.1.m1.2.3.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3"><eq id="S3.SS2.SSS0.Px1.p2.1.m1.2.3.1.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3.1"></eq><apply id="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2.1.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2.2.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2.2">𝑆</ci><ci id="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2.3a.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2.3"><mtext mathsize="70%" id="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2.3.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2.3">max</mtext></ci></apply><list id="S3.SS2.SSS0.Px1.p2.1.m1.2.3.3.1.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3.3.2"><cn type="integer" id="S3.SS2.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.1.1">500</cn><cn type="integer" id="S3.SS2.SSS0.Px1.p2.1.m1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.2">000</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.1.m1.2c">S_{\text{max}}=500,000</annotation></semantics></math>. Once this value is reached, we no longer subdivide and instead mark unprocessed nodes as having a dense grid of <math id="S3.SS2.SSS0.Px1.p2.2.m2.1" class="ltx_Math" alttext="N\times N\times N" display="inline"><semantics id="S3.SS2.SSS0.Px1.p2.2.m2.1a"><mrow id="S3.SS2.SSS0.Px1.p2.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p2.2.m2.1.1.2" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px1.p2.2.m2.1.1.1" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS0.Px1.p2.2.m2.1.1.3" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1.3.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px1.p2.2.m2.1.1.1a" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS0.Px1.p2.2.m2.1.1.4" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1.4.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.2.m2.1b"><apply id="S3.SS2.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1"><times id="S3.SS2.SSS0.Px1.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1.1"></times><ci id="S3.SS2.SSS0.Px1.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1.2">𝑁</ci><ci id="S3.SS2.SSS0.Px1.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1.3">𝑁</ci><ci id="S3.SS2.SSS0.Px1.p2.2.m2.1.1.4.cmml" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1.4">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.2.m2.1c">N\times N\times N</annotation></semantics></math> “virtual” children. <math id="S3.SS2.SSS0.Px1.p2.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.SSS0.Px1.p2.3.m3.1a"><mi id="S3.SS2.SSS0.Px1.p2.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p2.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.3.m3.1b"><ci id="S3.SS2.SSS0.Px1.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.3.m3.1c">N</annotation></semantics></math> is the smallest power of two such that each virtual child has <math id="S3.SS2.SSS0.Px1.p2.4.m4.1" class="ltx_Math" alttext="A_{\text{node}}\leq\hat{A}_{\text{inv}}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p2.4.m4.1a"><mrow id="S3.SS2.SSS0.Px1.p2.4.m4.1.1" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.cmml"><msub id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.2" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.2.cmml"><mi id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.2.2" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.2.2.cmml">A</mi><mtext id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.2.3" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.2.3a.cmml">node</mtext></msub><mo id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.1" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.1.cmml">≤</mo><msub id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.cmml"><mover accent="true" id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.2" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.2.cmml"><mi id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.2.2" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.2.2.cmml">A</mi><mo id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.2.1" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.2.1.cmml">^</mo></mover><mtext id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.3" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.3a.cmml">inv</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.4.m4.1b"><apply id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1"><leq id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.1"></leq><apply id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.2.2">𝐴</ci><ci id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.2.3a.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.2.3"><mtext mathsize="70%" id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.2.3">node</mtext></ci></apply><apply id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3">subscript</csymbol><apply id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.2"><ci id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.2.1.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.2.1">^</ci><ci id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.2.2.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.2.2">𝐴</ci></apply><ci id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.3a.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.3"><mtext mathsize="70%" id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.3.3">inv</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.4.m4.1c">A_{\text{node}}\leq\hat{A}_{\text{inv}}</annotation></semantics></math>. Crucially, we avoid storing virtual child nodes in memory, since we can cheaply compute them on the fly given just <math id="S3.SS2.SSS0.Px1.p2.5.m5.1" class="ltx_Math" alttext="A_{\text{node}}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p2.5.m5.1a"><msub id="S3.SS2.SSS0.Px1.p2.5.m5.1.1" xref="S3.SS2.SSS0.Px1.p2.5.m5.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p2.5.m5.1.1.2" xref="S3.SS2.SSS0.Px1.p2.5.m5.1.1.2.cmml">A</mi><mtext id="S3.SS2.SSS0.Px1.p2.5.m5.1.1.3" xref="S3.SS2.SSS0.Px1.p2.5.m5.1.1.3a.cmml">node</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.5.m5.1b"><apply id="S3.SS2.SSS0.Px1.p2.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p2.5.m5.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p2.5.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p2.5.m5.1.1.2">𝐴</ci><ci id="S3.SS2.SSS0.Px1.p2.5.m5.1.1.3a.cmml" xref="S3.SS2.SSS0.Px1.p2.5.m5.1.1.3"><mtext mathsize="70%" id="S3.SS2.SSS0.Px1.p2.5.m5.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p2.5.m5.1.1.3">node</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.5.m5.1c">A_{\text{node}}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Occupancy and Visibility Test</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.7" class="ltx_p">We use a flood-fill algorithm to avoid querying the occupancy of nodes that are unlikely to be occupied. We start with a subset <math id="S3.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{N}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">𝒩</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1">𝒩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.1c">\mathcal{N}</annotation></semantics></math> of nodes in the coarse octree (specifically, only the nodes on the boundaries of the <math id="S3.SS2.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="N\times N\times N" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.2.m2.1a"><mrow id="S3.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1a" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.4" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.4.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1"><times id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1"></times><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2">𝑁</ci><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3">𝑁</ci><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.4.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.4">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.2.m2.1c">N\times N\times N</annotation></semantics></math> virtual grids). For each node in <math id="S3.SS2.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{N}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml">𝒩</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.3.m3.1b"><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1">𝒩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.3.m3.1c">\mathcal{N}</annotation></semantics></math>, we compute the SDF value on the 8 vertices of the node. If both positive (<math id="S3.SS2.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="\geq 0" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.4.m4.1a"><mrow id="S3.SS2.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.cmml"></mi><mo id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.cmml">≥</mo><mn id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.4.m4.1b"><apply id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1"><geq id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1"></geq><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2">absent</csymbol><cn type="integer" id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.4.m4.1c">\geq 0</annotation></semantics></math>) and negative (<math id="S3.SS2.SSS0.Px2.p1.5.m5.1" class="ltx_Math" alttext="&lt;0" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.5.m5.1a"><mrow id="S3.SS2.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.2" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.2.cmml"></mi><mo id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.cmml">&lt;</mo><mn id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.5.m5.1b"><apply id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1"><lt id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1"></lt><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.2">absent</csymbol><cn type="integer" id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.5.m5.1c">&lt;0</annotation></semantics></math>) vertices exist, we mark this node as occupied and put its neighboring nodes into <math id="S3.SS2.SSS0.Px2.p1.6.m6.1" class="ltx_Math" alttext="\mathcal{N}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml">𝒩</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.6.m6.1b"><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1">𝒩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.6.m6.1c">\mathcal{N}</annotation></semantics></math>. We iterate until we completely test <math id="S3.SS2.SSS0.Px2.p1.7.m7.1" class="ltx_Math" alttext="\mathcal{N}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.7.m7.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS0.Px2.p1.7.m7.1.1" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.cmml">𝒩</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.7.m7.1b"><ci id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1">𝒩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.7.m7.1c">\mathcal{N}</annotation></semantics></math>. Because the iso-surface of the SDF is continuous, we will not miss a node if any node in its connected component is checked. This approach may miss small floating islands (i.e. if they dont intersect the corner of any coarse node), but we accept this as a tradeoff.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p2.2" class="ltx_p">Next, we compute whether each occupied node is also visible or not. For each camera, we project all nodes onto a depth buffer and mark nodes as visible in this camera if the projected depth is smaller than any of the values within a neighborhood of its projected pixel. A node is then visible overall if it is visible in at least one camera camera. To further avoid missing potentially visible nodes in the final result, we dilate the set of visible nodes by including all <math id="S3.SS2.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="&lt;k" display="inline"><semantics id="S3.SS2.SSS0.Px2.p2.1.m1.1a"><mrow id="S3.SS2.SSS0.Px2.p2.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml"></mi><mo id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.1" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.1.cmml">&lt;</mo><mi id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.1.m1.1b"><apply id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1"><lt id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2">absent</csymbol><ci id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.1.m1.1c">&lt;k</annotation></semantics></math>-th degree neighbors, where <math id="S3.SS2.SSS0.Px2.p2.2.m2.1" class="ltx_Math" alttext="k=2" display="inline"><semantics id="S3.SS2.SSS0.Px2.p2.2.m2.1a"><mrow id="S3.SS2.SSS0.Px2.p2.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.2" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.2.cmml">k</mi><mo id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.1" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.3" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.2.m2.1b"><apply id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1"><eq id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.1"></eq><ci id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.2">𝑘</ci><cn type="integer" id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.2.m2.1c">k=2</annotation></semantics></math> is a user-specified hyperparameter.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Fine Visible Octree</h4>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.1" class="ltx_p">Finally, we divide all visible nodes into high resolution octrees until all of these octrees’s leaf nodes (referred to as subnodes) have <math id="S3.SS2.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="A_{\text{subnode}}\leq\hat{A}" display="inline"><semantics id="S3.SS2.SSS0.Px3.p1.1.m1.1a"><mrow id="S3.SS2.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.cmml"><msub id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.cmml"><mi id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2.cmml">A</mi><mtext id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3a.cmml">subnode</mtext></msub><mo id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.1" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.1.cmml">≤</mo><mover accent="true" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.2.cmml">A</mi><mo id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.1" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.1.cmml">^</mo></mover></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1"><leq id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.1"></leq><apply id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2">𝐴</ci><ci id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3a.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3"><mtext mathsize="70%" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3">subnode</mtext></ci></apply><apply id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3"><ci id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.1">^</ci><ci id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.2">𝐴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.1.m1.1c">A_{\text{subnode}}\leq\hat{A}</annotation></semantics></math>. This process can cause sharp LOD transitions if a neighboring node that was not considered occupied turns out to be occupied. Therefore, if we find a sign change while dividing a node on the border of a node marked unoccupied, we mark it as occupied such that it is meshed at high resolution too. This propagation without restriction can undo the work done by the visibility test. Therefore if a neighbor node is already marked as invisible, we don’t convert it to be visible.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Mesh Extraction</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We use the dual contouring algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> to extract a mesh for each component from the octree with the <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="\text{SDF}_{\text{comp.}}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mtext id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2a.cmml">SDF</mtext><mtext id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3a.cmml">comp.</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2a.cmml" xref="S3.SS3.p1.1.m1.1.1.2"><mtext id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">SDF</mtext></ci><ci id="S3.SS3.p1.1.m1.1.1.3a.cmml" xref="S3.SS3.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">comp.</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\text{SDF}_{\text{comp.}}</annotation></semantics></math> on each vertex. But, instead of quadratic error functions (QEF), we use bisection to locate the center vertex of each node, which is more robust to discontinuous SDFs.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Computational Complexity</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The majority of the time and the memory are spent in the fine octree step, where we need to densely query the SDFs and extract dense meshes. Such SDFs are usually optimized for parallel computation. Therefore we do these operations in batches on parallel devices such as GPU to save time. The total complexity depends on many factors:</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\text{Time and Memory}\propto K_{\text{scene}}K_{\text{cam}}N_{\text{cam}}\frac{S_{\text{fov}}}{\hat{A}^{2}}" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mtext id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2a.cmml">Time and Memory</mtext><mo id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">∝</mo><mrow id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><msub id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml"><mi id="S3.E2.m1.1.1.3.2.2" xref="S3.E2.m1.1.1.3.2.2.cmml">K</mi><mtext id="S3.E2.m1.1.1.3.2.3" xref="S3.E2.m1.1.1.3.2.3a.cmml">scene</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.1" xref="S3.E2.m1.1.1.3.1.cmml">​</mo><msub id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.3.3.2" xref="S3.E2.m1.1.1.3.3.2.cmml">K</mi><mtext id="S3.E2.m1.1.1.3.3.3" xref="S3.E2.m1.1.1.3.3.3a.cmml">cam</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.1a" xref="S3.E2.m1.1.1.3.1.cmml">​</mo><msub id="S3.E2.m1.1.1.3.4" xref="S3.E2.m1.1.1.3.4.cmml"><mi id="S3.E2.m1.1.1.3.4.2" xref="S3.E2.m1.1.1.3.4.2.cmml">N</mi><mtext id="S3.E2.m1.1.1.3.4.3" xref="S3.E2.m1.1.1.3.4.3a.cmml">cam</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.1b" xref="S3.E2.m1.1.1.3.1.cmml">​</mo><mfrac id="S3.E2.m1.1.1.3.5" xref="S3.E2.m1.1.1.3.5.cmml"><msub id="S3.E2.m1.1.1.3.5.2" xref="S3.E2.m1.1.1.3.5.2.cmml"><mi id="S3.E2.m1.1.1.3.5.2.2" xref="S3.E2.m1.1.1.3.5.2.2.cmml">S</mi><mtext id="S3.E2.m1.1.1.3.5.2.3" xref="S3.E2.m1.1.1.3.5.2.3a.cmml">fov</mtext></msub><msup id="S3.E2.m1.1.1.3.5.3" xref="S3.E2.m1.1.1.3.5.3.cmml"><mover accent="true" id="S3.E2.m1.1.1.3.5.3.2" xref="S3.E2.m1.1.1.3.5.3.2.cmml"><mi id="S3.E2.m1.1.1.3.5.3.2.2" xref="S3.E2.m1.1.1.3.5.3.2.2.cmml">A</mi><mo id="S3.E2.m1.1.1.3.5.3.2.1" xref="S3.E2.m1.1.1.3.5.3.2.1.cmml">^</mo></mover><mn id="S3.E2.m1.1.1.3.5.3.3" xref="S3.E2.m1.1.1.3.5.3.3.cmml">2</mn></msup></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><csymbol cd="latexml" id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1">proportional-to</csymbol><ci id="S3.E2.m1.1.1.2a.cmml" xref="S3.E2.m1.1.1.2"><mtext id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">Time and Memory</mtext></ci><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><times id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3.1"></times><apply id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.3.2.2">𝐾</ci><ci id="S3.E2.m1.1.1.3.2.3a.cmml" xref="S3.E2.m1.1.1.3.2.3"><mtext mathsize="70%" id="S3.E2.m1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.3.2.3">scene</mtext></ci></apply><apply id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.2">𝐾</ci><ci id="S3.E2.m1.1.1.3.3.3a.cmml" xref="S3.E2.m1.1.1.3.3.3"><mtext mathsize="70%" id="S3.E2.m1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3">cam</mtext></ci></apply><apply id="S3.E2.m1.1.1.3.4.cmml" xref="S3.E2.m1.1.1.3.4"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.4.1.cmml" xref="S3.E2.m1.1.1.3.4">subscript</csymbol><ci id="S3.E2.m1.1.1.3.4.2.cmml" xref="S3.E2.m1.1.1.3.4.2">𝑁</ci><ci id="S3.E2.m1.1.1.3.4.3a.cmml" xref="S3.E2.m1.1.1.3.4.3"><mtext mathsize="70%" id="S3.E2.m1.1.1.3.4.3.cmml" xref="S3.E2.m1.1.1.3.4.3">cam</mtext></ci></apply><apply id="S3.E2.m1.1.1.3.5.cmml" xref="S3.E2.m1.1.1.3.5"><divide id="S3.E2.m1.1.1.3.5.1.cmml" xref="S3.E2.m1.1.1.3.5"></divide><apply id="S3.E2.m1.1.1.3.5.2.cmml" xref="S3.E2.m1.1.1.3.5.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.5.2.1.cmml" xref="S3.E2.m1.1.1.3.5.2">subscript</csymbol><ci id="S3.E2.m1.1.1.3.5.2.2.cmml" xref="S3.E2.m1.1.1.3.5.2.2">𝑆</ci><ci id="S3.E2.m1.1.1.3.5.2.3a.cmml" xref="S3.E2.m1.1.1.3.5.2.3"><mtext mathsize="70%" id="S3.E2.m1.1.1.3.5.2.3.cmml" xref="S3.E2.m1.1.1.3.5.2.3">fov</mtext></ci></apply><apply id="S3.E2.m1.1.1.3.5.3.cmml" xref="S3.E2.m1.1.1.3.5.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.5.3.1.cmml" xref="S3.E2.m1.1.1.3.5.3">superscript</csymbol><apply id="S3.E2.m1.1.1.3.5.3.2.cmml" xref="S3.E2.m1.1.1.3.5.3.2"><ci id="S3.E2.m1.1.1.3.5.3.2.1.cmml" xref="S3.E2.m1.1.1.3.5.3.2.1">^</ci><ci id="S3.E2.m1.1.1.3.5.3.2.2.cmml" xref="S3.E2.m1.1.1.3.5.3.2.2">𝐴</ci></apply><cn type="integer" id="S3.E2.m1.1.1.3.5.3.3.cmml" xref="S3.E2.m1.1.1.3.5.3.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\text{Time and Memory}\propto K_{\text{scene}}K_{\text{cam}}N_{\text{cam}}\frac{S_{\text{fov}}}{\hat{A}^{2}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.7" class="ltx_p">Where <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="K_{\text{scene}}" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><msub id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">K</mi><mtext id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3a.cmml">scene</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">𝐾</ci><ci id="S3.SS4.p3.1.m1.1.1.3a.cmml" xref="S3.SS4.p3.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS4.p3.1.m1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3">scene</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">K_{\text{scene}}</annotation></semantics></math> is the complexity of the scene, <math id="S3.SS4.p3.2.m2.1" class="ltx_Math" alttext="K_{\text{cam}}" display="inline"><semantics id="S3.SS4.p3.2.m2.1a"><msub id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml"><mi id="S3.SS4.p3.2.m2.1.1.2" xref="S3.SS4.p3.2.m2.1.1.2.cmml">K</mi><mtext id="S3.SS4.p3.2.m2.1.1.3" xref="S3.SS4.p3.2.m2.1.1.3a.cmml">cam</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><apply id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.2.m2.1.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p3.2.m2.1.1.2.cmml" xref="S3.SS4.p3.2.m2.1.1.2">𝐾</ci><ci id="S3.SS4.p3.2.m2.1.1.3a.cmml" xref="S3.SS4.p3.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS4.p3.2.m2.1.1.3.cmml" xref="S3.SS4.p3.2.m2.1.1.3">cam</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">K_{\text{cam}}</annotation></semantics></math> describes how fast the camera moves, i.e., how much new content each camera has (for completely non-overlapping cameras, <math id="S3.SS4.p3.3.m3.1" class="ltx_Math" alttext="K_{\text{cam}}=1" display="inline"><semantics id="S3.SS4.p3.3.m3.1a"><mrow id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml"><msub id="S3.SS4.p3.3.m3.1.1.2" xref="S3.SS4.p3.3.m3.1.1.2.cmml"><mi id="S3.SS4.p3.3.m3.1.1.2.2" xref="S3.SS4.p3.3.m3.1.1.2.2.cmml">K</mi><mtext id="S3.SS4.p3.3.m3.1.1.2.3" xref="S3.SS4.p3.3.m3.1.1.2.3a.cmml">cam</mtext></msub><mo id="S3.SS4.p3.3.m3.1.1.1" xref="S3.SS4.p3.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS4.p3.3.m3.1.1.3" xref="S3.SS4.p3.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><apply id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1"><eq id="S3.SS4.p3.3.m3.1.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1.1"></eq><apply id="S3.SS4.p3.3.m3.1.1.2.cmml" xref="S3.SS4.p3.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p3.3.m3.1.1.2.1.cmml" xref="S3.SS4.p3.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS4.p3.3.m3.1.1.2.2.cmml" xref="S3.SS4.p3.3.m3.1.1.2.2">𝐾</ci><ci id="S3.SS4.p3.3.m3.1.1.2.3a.cmml" xref="S3.SS4.p3.3.m3.1.1.2.3"><mtext mathsize="70%" id="S3.SS4.p3.3.m3.1.1.2.3.cmml" xref="S3.SS4.p3.3.m3.1.1.2.3">cam</mtext></ci></apply><cn type="integer" id="S3.SS4.p3.3.m3.1.1.3.cmml" xref="S3.SS4.p3.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">K_{\text{cam}}=1</annotation></semantics></math>, but in practice, <math id="S3.SS4.p3.4.m4.1" class="ltx_Math" alttext="K_{\text{cam}}\ll 1" display="inline"><semantics id="S3.SS4.p3.4.m4.1a"><mrow id="S3.SS4.p3.4.m4.1.1" xref="S3.SS4.p3.4.m4.1.1.cmml"><msub id="S3.SS4.p3.4.m4.1.1.2" xref="S3.SS4.p3.4.m4.1.1.2.cmml"><mi id="S3.SS4.p3.4.m4.1.1.2.2" xref="S3.SS4.p3.4.m4.1.1.2.2.cmml">K</mi><mtext id="S3.SS4.p3.4.m4.1.1.2.3" xref="S3.SS4.p3.4.m4.1.1.2.3a.cmml">cam</mtext></msub><mo id="S3.SS4.p3.4.m4.1.1.1" xref="S3.SS4.p3.4.m4.1.1.1.cmml">≪</mo><mn id="S3.SS4.p3.4.m4.1.1.3" xref="S3.SS4.p3.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.1b"><apply id="S3.SS4.p3.4.m4.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1"><csymbol cd="latexml" id="S3.SS4.p3.4.m4.1.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1.1">much-less-than</csymbol><apply id="S3.SS4.p3.4.m4.1.1.2.cmml" xref="S3.SS4.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p3.4.m4.1.1.2.1.cmml" xref="S3.SS4.p3.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS4.p3.4.m4.1.1.2.2.cmml" xref="S3.SS4.p3.4.m4.1.1.2.2">𝐾</ci><ci id="S3.SS4.p3.4.m4.1.1.2.3a.cmml" xref="S3.SS4.p3.4.m4.1.1.2.3"><mtext mathsize="70%" id="S3.SS4.p3.4.m4.1.1.2.3.cmml" xref="S3.SS4.p3.4.m4.1.1.2.3">cam</mtext></ci></apply><cn type="integer" id="S3.SS4.p3.4.m4.1.1.3.cmml" xref="S3.SS4.p3.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.1c">K_{\text{cam}}\ll 1</annotation></semantics></math>), <math id="S3.SS4.p3.5.m5.1" class="ltx_Math" alttext="N_{\text{cam}}" display="inline"><semantics id="S3.SS4.p3.5.m5.1a"><msub id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml"><mi id="S3.SS4.p3.5.m5.1.1.2" xref="S3.SS4.p3.5.m5.1.1.2.cmml">N</mi><mtext id="S3.SS4.p3.5.m5.1.1.3" xref="S3.SS4.p3.5.m5.1.1.3a.cmml">cam</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.1b"><apply id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.5.m5.1.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS4.p3.5.m5.1.1.2.cmml" xref="S3.SS4.p3.5.m5.1.1.2">𝑁</ci><ci id="S3.SS4.p3.5.m5.1.1.3a.cmml" xref="S3.SS4.p3.5.m5.1.1.3"><mtext mathsize="70%" id="S3.SS4.p3.5.m5.1.1.3.cmml" xref="S3.SS4.p3.5.m5.1.1.3">cam</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.1c">N_{\text{cam}}</annotation></semantics></math> is the number of cameras, and <math id="S3.SS4.p3.6.m6.1" class="ltx_Math" alttext="S_{\text{fov}}" display="inline"><semantics id="S3.SS4.p3.6.m6.1a"><msub id="S3.SS4.p3.6.m6.1.1" xref="S3.SS4.p3.6.m6.1.1.cmml"><mi id="S3.SS4.p3.6.m6.1.1.2" xref="S3.SS4.p3.6.m6.1.1.2.cmml">S</mi><mtext id="S3.SS4.p3.6.m6.1.1.3" xref="S3.SS4.p3.6.m6.1.1.3a.cmml">fov</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.6.m6.1b"><apply id="S3.SS4.p3.6.m6.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.6.m6.1.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS4.p3.6.m6.1.1.2.cmml" xref="S3.SS4.p3.6.m6.1.1.2">𝑆</ci><ci id="S3.SS4.p3.6.m6.1.1.3a.cmml" xref="S3.SS4.p3.6.m6.1.1.3"><mtext mathsize="70%" id="S3.SS4.p3.6.m6.1.1.3.cmml" xref="S3.SS4.p3.6.m6.1.1.3">fov</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.6.m6.1c">S_{\text{fov}}</annotation></semantics></math> is the solid angle of the FOV. The last factor <math id="S3.SS4.p3.7.m7.1" class="ltx_Math" alttext="\frac{S_{\text{fov}}}{\hat{A}^{2}}" display="inline"><semantics id="S3.SS4.p3.7.m7.1a"><mfrac id="S3.SS4.p3.7.m7.1.1" xref="S3.SS4.p3.7.m7.1.1.cmml"><msub id="S3.SS4.p3.7.m7.1.1.2" xref="S3.SS4.p3.7.m7.1.1.2.cmml"><mi id="S3.SS4.p3.7.m7.1.1.2.2" xref="S3.SS4.p3.7.m7.1.1.2.2.cmml">S</mi><mtext id="S3.SS4.p3.7.m7.1.1.2.3" xref="S3.SS4.p3.7.m7.1.1.2.3a.cmml">fov</mtext></msub><msup id="S3.SS4.p3.7.m7.1.1.3" xref="S3.SS4.p3.7.m7.1.1.3.cmml"><mover accent="true" id="S3.SS4.p3.7.m7.1.1.3.2" xref="S3.SS4.p3.7.m7.1.1.3.2.cmml"><mi id="S3.SS4.p3.7.m7.1.1.3.2.2" xref="S3.SS4.p3.7.m7.1.1.3.2.2.cmml">A</mi><mo id="S3.SS4.p3.7.m7.1.1.3.2.1" xref="S3.SS4.p3.7.m7.1.1.3.2.1.cmml">^</mo></mover><mn id="S3.SS4.p3.7.m7.1.1.3.3" xref="S3.SS4.p3.7.m7.1.1.3.3.cmml">2</mn></msup></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.7.m7.1b"><apply id="S3.SS4.p3.7.m7.1.1.cmml" xref="S3.SS4.p3.7.m7.1.1"><divide id="S3.SS4.p3.7.m7.1.1.1.cmml" xref="S3.SS4.p3.7.m7.1.1"></divide><apply id="S3.SS4.p3.7.m7.1.1.2.cmml" xref="S3.SS4.p3.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p3.7.m7.1.1.2.1.cmml" xref="S3.SS4.p3.7.m7.1.1.2">subscript</csymbol><ci id="S3.SS4.p3.7.m7.1.1.2.2.cmml" xref="S3.SS4.p3.7.m7.1.1.2.2">𝑆</ci><ci id="S3.SS4.p3.7.m7.1.1.2.3a.cmml" xref="S3.SS4.p3.7.m7.1.1.2.3"><mtext mathsize="50%" id="S3.SS4.p3.7.m7.1.1.2.3.cmml" xref="S3.SS4.p3.7.m7.1.1.2.3">fov</mtext></ci></apply><apply id="S3.SS4.p3.7.m7.1.1.3.cmml" xref="S3.SS4.p3.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p3.7.m7.1.1.3.1.cmml" xref="S3.SS4.p3.7.m7.1.1.3">superscript</csymbol><apply id="S3.SS4.p3.7.m7.1.1.3.2.cmml" xref="S3.SS4.p3.7.m7.1.1.3.2"><ci id="S3.SS4.p3.7.m7.1.1.3.2.1.cmml" xref="S3.SS4.p3.7.m7.1.1.3.2.1">^</ci><ci id="S3.SS4.p3.7.m7.1.1.3.2.2.cmml" xref="S3.SS4.p3.7.m7.1.1.3.2.2">𝐴</ci></apply><cn type="integer" id="S3.SS4.p3.7.m7.1.1.3.3.cmml" xref="S3.SS4.p3.7.m7.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.7.m7.1c">\frac{S_{\text{fov}}}{\hat{A}^{2}}</annotation></semantics></math> comes from the fact that we only construct the fine octree for the visible part.</p>
</div>
<figure id="S3.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F5.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:238.5pt;"><img src="/html/2312.08364/assets/figures/infinigensmesher_compressed.jpg" id="S3.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="368" alt="Refer to caption">
<figcaption class="ltx_caption">(a) Infinigen</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F5.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:238.5pt;"><img src="/html/2312.08364/assets/figures/ours_compressed.jpg" id="S3.F5.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="368" alt="Refer to caption">
<figcaption class="ltx_caption">(b) Ours</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.4.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.5.2" class="ltx_text" style="font-size:90%;"> Two adjacent frames of meshes extracted with (a) The Infinigen solution (b) OcMesher (Ours). In the zoomed-in images, we can see more visual inconsistency in (a). The heatmap shows the quantitative measurement (via flow ground truth, explained in Sec. <a href="#S4.SS1" title="4.1 View Consistency ‣ 4 Experiments ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>).</span></figcaption>
</figure>
<figure id="S3.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F6.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/ssim0.png" id="S3.F6.1.g1" class="ltx_graphics ltx_img_portrait" width="598" height="1028" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(a) Scene overview</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F6.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/ssim1.jpg" id="S3.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="339" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(b) Infinigen with increasing resolution</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F6.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/ssim2.png" id="S3.F6.3.g1" class="ltx_graphics ltx_img_portrait" width="598" height="1028" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(c) Ours</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.5.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.6.2" class="ltx_text" style="font-size:90%;">Quantitative measurement of view consistency for the first pair of transition frames. The brighter, the worse.</span></figcaption>
</figure>
<figure id="S3.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F7.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/ssim_tradeoff1.png" id="S3.F7.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="311" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(a) Comparison of frame-by-frame SSIM</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F7.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/ssim_tradeoff2.png" id="S3.F7.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="311" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(b) Comparison of first-to-nth frame SSIM</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.6.2.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S3.F7.4.1" class="ltx_text" style="font-size:90%;">Quantitative measurement of view consistency (a) for adjacent frames and (b) along a pixel trajectory. On the left, Infinigen’s transition frames introduce sharp spikes in frame-by-frame inconsistency. On the right, Infinigen suffers worse <math id="S3.F7.4.1.m1.1" class="ltx_Math" alttext="0\rightarrow i" display="inline"><semantics id="S3.F7.4.1.m1.1b"><mrow id="S3.F7.4.1.m1.1.1" xref="S3.F7.4.1.m1.1.1.cmml"><mn id="S3.F7.4.1.m1.1.1.2" xref="S3.F7.4.1.m1.1.1.2.cmml">0</mn><mo stretchy="false" id="S3.F7.4.1.m1.1.1.1" xref="S3.F7.4.1.m1.1.1.1.cmml">→</mo><mi id="S3.F7.4.1.m1.1.1.3" xref="S3.F7.4.1.m1.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F7.4.1.m1.1c"><apply id="S3.F7.4.1.m1.1.1.cmml" xref="S3.F7.4.1.m1.1.1"><ci id="S3.F7.4.1.m1.1.1.1.cmml" xref="S3.F7.4.1.m1.1.1.1">→</ci><cn type="integer" id="S3.F7.4.1.m1.1.1.2.cmml" xref="S3.F7.4.1.m1.1.1.2">0</cn><ci id="S3.F7.4.1.m1.1.1.3.cmml" xref="S3.F7.4.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F7.4.1.m1.1d">0\rightarrow i</annotation></semantics></math> consistency except for the first 8 before any transition frames occur. In both cases, our method provides a superior SSIM-vs.-runtime tradeoff.</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.2" class="ltx_p">To generate synthetic scenes and videos, we integrated our method with Infinigen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>’s scene generator but replace their mesh extracting solution with ours. Because Infinigen’s assets besides terrains do not use SDFs, we turn them off in the experiments.
We randomly generated 42 scenes and for each scene, we rendered a video consisting of <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="N=192" display="inline"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">N</mi><mo id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><eq id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1"></eq><ci id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">𝑁</ci><cn type="integer" id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">N=192</annotation></semantics></math> frames of resolution <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="H\times W=720\times 1280" display="inline"><semantics id="S4.p1.2.m2.1a"><mrow id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml"><mrow id="S4.p1.2.m2.1.1.2" xref="S4.p1.2.m2.1.1.2.cmml"><mi id="S4.p1.2.m2.1.1.2.2" xref="S4.p1.2.m2.1.1.2.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S4.p1.2.m2.1.1.2.1" xref="S4.p1.2.m2.1.1.2.1.cmml">×</mo><mi id="S4.p1.2.m2.1.1.2.3" xref="S4.p1.2.m2.1.1.2.3.cmml">W</mi></mrow><mo id="S4.p1.2.m2.1.1.1" xref="S4.p1.2.m2.1.1.1.cmml">=</mo><mrow id="S4.p1.2.m2.1.1.3" xref="S4.p1.2.m2.1.1.3.cmml"><mn id="S4.p1.2.m2.1.1.3.2" xref="S4.p1.2.m2.1.1.3.2.cmml">720</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p1.2.m2.1.1.3.1" xref="S4.p1.2.m2.1.1.3.1.cmml">×</mo><mn id="S4.p1.2.m2.1.1.3.3" xref="S4.p1.2.m2.1.1.3.3.cmml">1280</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><apply id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1"><eq id="S4.p1.2.m2.1.1.1.cmml" xref="S4.p1.2.m2.1.1.1"></eq><apply id="S4.p1.2.m2.1.1.2.cmml" xref="S4.p1.2.m2.1.1.2"><times id="S4.p1.2.m2.1.1.2.1.cmml" xref="S4.p1.2.m2.1.1.2.1"></times><ci id="S4.p1.2.m2.1.1.2.2.cmml" xref="S4.p1.2.m2.1.1.2.2">𝐻</ci><ci id="S4.p1.2.m2.1.1.2.3.cmml" xref="S4.p1.2.m2.1.1.2.3">𝑊</ci></apply><apply id="S4.p1.2.m2.1.1.3.cmml" xref="S4.p1.2.m2.1.1.3"><times id="S4.p1.2.m2.1.1.3.1.cmml" xref="S4.p1.2.m2.1.1.3.1"></times><cn type="integer" id="S4.p1.2.m2.1.1.3.2.cmml" xref="S4.p1.2.m2.1.1.3.2">720</cn><cn type="integer" id="S4.p1.2.m2.1.1.3.3.cmml" xref="S4.p1.2.m2.1.1.3.3">1280</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">H\times W=720\times 1280</annotation></semantics></math> on a cluster of GPUs including NVIDIA GeForce RTX 3090, RTX A6000 and A40 . Each scene is generated under several settings:</p>
</div>
<div id="S4.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">With the original Infinigen solution, at 3 target resolutions. Higher mesh resolution reduces flickering, but incurs more computational cost. We show the 3 target resolutions to demonstrate this trade-off. We re-generate the mesh every 8 frames, which is the default setting from Infinigen and is a generous comparison due to the low-poly artifacts at image edges during camera motion, as shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">With our method using a moderate resolution. We create a single mesh for all the <math id="S4.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.I1.i2.p1.1.m1.1a"><mi id="S4.I1.i2.p1.1.m1.1.1" xref="S4.I1.i2.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.1.m1.1b"><ci id="S4.I1.i2.p1.1.m1.1.1.cmml" xref="S4.I1.i2.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.m1.1c">N</annotation></semantics></math> frames except for when the resulting mesh exceeds Blender’s (our rendering Engine) capacity, which happens for 5 scenes out of the 42 scenes in total. In those cases, we split the entire video clip into two clips. This makes the video of these 5 scenes flicker in the middle frame, but it has negligible effects on the experiment results compared with the theoretical results.</p>
</div>
</li>
</ul>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.9" class="ltx_p">We render RGB images with Blender’s Cycles Engine, with 8192 samples per pixel. We also save ground-truth depth maps and camera parameters for each frame. Because we only consider static terrain, we can compute the ground truth optical flow <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{F}_{i\rightarrow j}" display="inline"><semantics id="S4.p3.1.m1.1a"><msub id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mi id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">𝐅</mi><mrow id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml"><mi id="S4.p3.1.m1.1.1.3.2" xref="S4.p3.1.m1.1.1.3.2.cmml">i</mi><mo stretchy="false" id="S4.p3.1.m1.1.1.3.1" xref="S4.p3.1.m1.1.1.3.1.cmml">→</mo><mi id="S4.p3.1.m1.1.1.3.3" xref="S4.p3.1.m1.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1">subscript</csymbol><ci id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2">𝐅</ci><apply id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3"><ci id="S4.p3.1.m1.1.1.3.1.cmml" xref="S4.p3.1.m1.1.1.3.1">→</ci><ci id="S4.p3.1.m1.1.1.3.2.cmml" xref="S4.p3.1.m1.1.1.3.2">𝑖</ci><ci id="S4.p3.1.m1.1.1.3.3.cmml" xref="S4.p3.1.m1.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">\mathbf{F}_{i\rightarrow j}</annotation></semantics></math> (<math id="S4.p3.2.m2.2" class="ltx_Math" alttext="1\leq i,j\leq N" display="inline"><semantics id="S4.p3.2.m2.2a"><mrow id="S4.p3.2.m2.2.2.2" xref="S4.p3.2.m2.2.2.3.cmml"><mrow id="S4.p3.2.m2.1.1.1.1" xref="S4.p3.2.m2.1.1.1.1.cmml"><mn id="S4.p3.2.m2.1.1.1.1.2" xref="S4.p3.2.m2.1.1.1.1.2.cmml">1</mn><mo id="S4.p3.2.m2.1.1.1.1.1" xref="S4.p3.2.m2.1.1.1.1.1.cmml">≤</mo><mi id="S4.p3.2.m2.1.1.1.1.3" xref="S4.p3.2.m2.1.1.1.1.3.cmml">i</mi></mrow><mo id="S4.p3.2.m2.2.2.2.3" xref="S4.p3.2.m2.2.2.3a.cmml">,</mo><mrow id="S4.p3.2.m2.2.2.2.2" xref="S4.p3.2.m2.2.2.2.2.cmml"><mi id="S4.p3.2.m2.2.2.2.2.2" xref="S4.p3.2.m2.2.2.2.2.2.cmml">j</mi><mo id="S4.p3.2.m2.2.2.2.2.1" xref="S4.p3.2.m2.2.2.2.2.1.cmml">≤</mo><mi id="S4.p3.2.m2.2.2.2.2.3" xref="S4.p3.2.m2.2.2.2.2.3.cmml">N</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.2b"><apply id="S4.p3.2.m2.2.2.3.cmml" xref="S4.p3.2.m2.2.2.2"><csymbol cd="ambiguous" id="S4.p3.2.m2.2.2.3a.cmml" xref="S4.p3.2.m2.2.2.2.3">formulae-sequence</csymbol><apply id="S4.p3.2.m2.1.1.1.1.cmml" xref="S4.p3.2.m2.1.1.1.1"><leq id="S4.p3.2.m2.1.1.1.1.1.cmml" xref="S4.p3.2.m2.1.1.1.1.1"></leq><cn type="integer" id="S4.p3.2.m2.1.1.1.1.2.cmml" xref="S4.p3.2.m2.1.1.1.1.2">1</cn><ci id="S4.p3.2.m2.1.1.1.1.3.cmml" xref="S4.p3.2.m2.1.1.1.1.3">𝑖</ci></apply><apply id="S4.p3.2.m2.2.2.2.2.cmml" xref="S4.p3.2.m2.2.2.2.2"><leq id="S4.p3.2.m2.2.2.2.2.1.cmml" xref="S4.p3.2.m2.2.2.2.2.1"></leq><ci id="S4.p3.2.m2.2.2.2.2.2.cmml" xref="S4.p3.2.m2.2.2.2.2.2">𝑗</ci><ci id="S4.p3.2.m2.2.2.2.2.3.cmml" xref="S4.p3.2.m2.2.2.2.2.3">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.2c">1\leq i,j\leq N</annotation></semantics></math>) for any pair of frames # <math id="S4.p3.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.p3.3.m3.1a"><mi id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><ci id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">i</annotation></semantics></math> and # <math id="S4.p3.4.m4.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.p3.4.m4.1a"><mi id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.1b"><ci id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.1c">j</annotation></semantics></math>.
Each vector <math id="S4.p3.5.m5.2" class="ltx_Math" alttext="\mathbf{F}_{i\rightarrow j}[x,y]" display="inline"><semantics id="S4.p3.5.m5.2a"><mrow id="S4.p3.5.m5.2.3" xref="S4.p3.5.m5.2.3.cmml"><msub id="S4.p3.5.m5.2.3.2" xref="S4.p3.5.m5.2.3.2.cmml"><mi id="S4.p3.5.m5.2.3.2.2" xref="S4.p3.5.m5.2.3.2.2.cmml">𝐅</mi><mrow id="S4.p3.5.m5.2.3.2.3" xref="S4.p3.5.m5.2.3.2.3.cmml"><mi id="S4.p3.5.m5.2.3.2.3.2" xref="S4.p3.5.m5.2.3.2.3.2.cmml">i</mi><mo stretchy="false" id="S4.p3.5.m5.2.3.2.3.1" xref="S4.p3.5.m5.2.3.2.3.1.cmml">→</mo><mi id="S4.p3.5.m5.2.3.2.3.3" xref="S4.p3.5.m5.2.3.2.3.3.cmml">j</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.p3.5.m5.2.3.1" xref="S4.p3.5.m5.2.3.1.cmml">​</mo><mrow id="S4.p3.5.m5.2.3.3.2" xref="S4.p3.5.m5.2.3.3.1.cmml"><mo stretchy="false" id="S4.p3.5.m5.2.3.3.2.1" xref="S4.p3.5.m5.2.3.3.1.cmml">[</mo><mi id="S4.p3.5.m5.1.1" xref="S4.p3.5.m5.1.1.cmml">x</mi><mo id="S4.p3.5.m5.2.3.3.2.2" xref="S4.p3.5.m5.2.3.3.1.cmml">,</mo><mi id="S4.p3.5.m5.2.2" xref="S4.p3.5.m5.2.2.cmml">y</mi><mo stretchy="false" id="S4.p3.5.m5.2.3.3.2.3" xref="S4.p3.5.m5.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.5.m5.2b"><apply id="S4.p3.5.m5.2.3.cmml" xref="S4.p3.5.m5.2.3"><times id="S4.p3.5.m5.2.3.1.cmml" xref="S4.p3.5.m5.2.3.1"></times><apply id="S4.p3.5.m5.2.3.2.cmml" xref="S4.p3.5.m5.2.3.2"><csymbol cd="ambiguous" id="S4.p3.5.m5.2.3.2.1.cmml" xref="S4.p3.5.m5.2.3.2">subscript</csymbol><ci id="S4.p3.5.m5.2.3.2.2.cmml" xref="S4.p3.5.m5.2.3.2.2">𝐅</ci><apply id="S4.p3.5.m5.2.3.2.3.cmml" xref="S4.p3.5.m5.2.3.2.3"><ci id="S4.p3.5.m5.2.3.2.3.1.cmml" xref="S4.p3.5.m5.2.3.2.3.1">→</ci><ci id="S4.p3.5.m5.2.3.2.3.2.cmml" xref="S4.p3.5.m5.2.3.2.3.2">𝑖</ci><ci id="S4.p3.5.m5.2.3.2.3.3.cmml" xref="S4.p3.5.m5.2.3.2.3.3">𝑗</ci></apply></apply><interval closure="closed" id="S4.p3.5.m5.2.3.3.1.cmml" xref="S4.p3.5.m5.2.3.3.2"><ci id="S4.p3.5.m5.1.1.cmml" xref="S4.p3.5.m5.1.1">𝑥</ci><ci id="S4.p3.5.m5.2.2.cmml" xref="S4.p3.5.m5.2.2">𝑦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.5.m5.2c">\mathbf{F}_{i\rightarrow j}[x,y]</annotation></semantics></math> means pixel <math id="S4.p3.6.m6.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S4.p3.6.m6.2a"><mrow id="S4.p3.6.m6.2.3.2" xref="S4.p3.6.m6.2.3.1.cmml"><mo stretchy="false" id="S4.p3.6.m6.2.3.2.1" xref="S4.p3.6.m6.2.3.1.cmml">(</mo><mi id="S4.p3.6.m6.1.1" xref="S4.p3.6.m6.1.1.cmml">x</mi><mo id="S4.p3.6.m6.2.3.2.2" xref="S4.p3.6.m6.2.3.1.cmml">,</mo><mi id="S4.p3.6.m6.2.2" xref="S4.p3.6.m6.2.2.cmml">y</mi><mo stretchy="false" id="S4.p3.6.m6.2.3.2.3" xref="S4.p3.6.m6.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.6.m6.2b"><interval closure="open" id="S4.p3.6.m6.2.3.1.cmml" xref="S4.p3.6.m6.2.3.2"><ci id="S4.p3.6.m6.1.1.cmml" xref="S4.p3.6.m6.1.1">𝑥</ci><ci id="S4.p3.6.m6.2.2.cmml" xref="S4.p3.6.m6.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.6.m6.2c">(x,y)</annotation></semantics></math> in frame # <math id="S4.p3.7.m7.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.p3.7.m7.1a"><mi id="S4.p3.7.m7.1.1" xref="S4.p3.7.m7.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.p3.7.m7.1b"><ci id="S4.p3.7.m7.1.1.cmml" xref="S4.p3.7.m7.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.7.m7.1c">i</annotation></semantics></math> goes to pixel <math id="S4.p3.8.m8.4" class="ltx_Math" alttext="(x,y)+\mathbf{F}_{i\rightarrow j}[x,y]" display="inline"><semantics id="S4.p3.8.m8.4a"><mrow id="S4.p3.8.m8.4.5" xref="S4.p3.8.m8.4.5.cmml"><mrow id="S4.p3.8.m8.4.5.2.2" xref="S4.p3.8.m8.4.5.2.1.cmml"><mo stretchy="false" id="S4.p3.8.m8.4.5.2.2.1" xref="S4.p3.8.m8.4.5.2.1.cmml">(</mo><mi id="S4.p3.8.m8.1.1" xref="S4.p3.8.m8.1.1.cmml">x</mi><mo id="S4.p3.8.m8.4.5.2.2.2" xref="S4.p3.8.m8.4.5.2.1.cmml">,</mo><mi id="S4.p3.8.m8.2.2" xref="S4.p3.8.m8.2.2.cmml">y</mi><mo stretchy="false" id="S4.p3.8.m8.4.5.2.2.3" xref="S4.p3.8.m8.4.5.2.1.cmml">)</mo></mrow><mo id="S4.p3.8.m8.4.5.1" xref="S4.p3.8.m8.4.5.1.cmml">+</mo><mrow id="S4.p3.8.m8.4.5.3" xref="S4.p3.8.m8.4.5.3.cmml"><msub id="S4.p3.8.m8.4.5.3.2" xref="S4.p3.8.m8.4.5.3.2.cmml"><mi id="S4.p3.8.m8.4.5.3.2.2" xref="S4.p3.8.m8.4.5.3.2.2.cmml">𝐅</mi><mrow id="S4.p3.8.m8.4.5.3.2.3" xref="S4.p3.8.m8.4.5.3.2.3.cmml"><mi id="S4.p3.8.m8.4.5.3.2.3.2" xref="S4.p3.8.m8.4.5.3.2.3.2.cmml">i</mi><mo stretchy="false" id="S4.p3.8.m8.4.5.3.2.3.1" xref="S4.p3.8.m8.4.5.3.2.3.1.cmml">→</mo><mi id="S4.p3.8.m8.4.5.3.2.3.3" xref="S4.p3.8.m8.4.5.3.2.3.3.cmml">j</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.p3.8.m8.4.5.3.1" xref="S4.p3.8.m8.4.5.3.1.cmml">​</mo><mrow id="S4.p3.8.m8.4.5.3.3.2" xref="S4.p3.8.m8.4.5.3.3.1.cmml"><mo stretchy="false" id="S4.p3.8.m8.4.5.3.3.2.1" xref="S4.p3.8.m8.4.5.3.3.1.cmml">[</mo><mi id="S4.p3.8.m8.3.3" xref="S4.p3.8.m8.3.3.cmml">x</mi><mo id="S4.p3.8.m8.4.5.3.3.2.2" xref="S4.p3.8.m8.4.5.3.3.1.cmml">,</mo><mi id="S4.p3.8.m8.4.4" xref="S4.p3.8.m8.4.4.cmml">y</mi><mo stretchy="false" id="S4.p3.8.m8.4.5.3.3.2.3" xref="S4.p3.8.m8.4.5.3.3.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.8.m8.4b"><apply id="S4.p3.8.m8.4.5.cmml" xref="S4.p3.8.m8.4.5"><plus id="S4.p3.8.m8.4.5.1.cmml" xref="S4.p3.8.m8.4.5.1"></plus><interval closure="open" id="S4.p3.8.m8.4.5.2.1.cmml" xref="S4.p3.8.m8.4.5.2.2"><ci id="S4.p3.8.m8.1.1.cmml" xref="S4.p3.8.m8.1.1">𝑥</ci><ci id="S4.p3.8.m8.2.2.cmml" xref="S4.p3.8.m8.2.2">𝑦</ci></interval><apply id="S4.p3.8.m8.4.5.3.cmml" xref="S4.p3.8.m8.4.5.3"><times id="S4.p3.8.m8.4.5.3.1.cmml" xref="S4.p3.8.m8.4.5.3.1"></times><apply id="S4.p3.8.m8.4.5.3.2.cmml" xref="S4.p3.8.m8.4.5.3.2"><csymbol cd="ambiguous" id="S4.p3.8.m8.4.5.3.2.1.cmml" xref="S4.p3.8.m8.4.5.3.2">subscript</csymbol><ci id="S4.p3.8.m8.4.5.3.2.2.cmml" xref="S4.p3.8.m8.4.5.3.2.2">𝐅</ci><apply id="S4.p3.8.m8.4.5.3.2.3.cmml" xref="S4.p3.8.m8.4.5.3.2.3"><ci id="S4.p3.8.m8.4.5.3.2.3.1.cmml" xref="S4.p3.8.m8.4.5.3.2.3.1">→</ci><ci id="S4.p3.8.m8.4.5.3.2.3.2.cmml" xref="S4.p3.8.m8.4.5.3.2.3.2">𝑖</ci><ci id="S4.p3.8.m8.4.5.3.2.3.3.cmml" xref="S4.p3.8.m8.4.5.3.2.3.3">𝑗</ci></apply></apply><interval closure="closed" id="S4.p3.8.m8.4.5.3.3.1.cmml" xref="S4.p3.8.m8.4.5.3.3.2"><ci id="S4.p3.8.m8.3.3.cmml" xref="S4.p3.8.m8.3.3">𝑥</ci><ci id="S4.p3.8.m8.4.4.cmml" xref="S4.p3.8.m8.4.4">𝑦</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.8.m8.4c">(x,y)+\mathbf{F}_{i\rightarrow j}[x,y]</annotation></semantics></math> in frame # <math id="S4.p3.9.m9.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.p3.9.m9.1a"><mi id="S4.p3.9.m9.1.1" xref="S4.p3.9.m9.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.p3.9.m9.1b"><ci id="S4.p3.9.m9.1.1.cmml" xref="S4.p3.9.m9.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.9.m9.1c">j</annotation></semantics></math>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>View Consistency</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">First, we show that our method has less flickering in the rendered videos, i.e., ours has better view consistency. Qualitatively, we show a zoomed-in region of a muddy mountain scene for a pair of mesh-switching frames (we call such frames transition frames) rendered with both methods in Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.4 Computational Complexity ‣ 3 Method ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Because the Infinigen solution switches between meshes, you can see the differences between the two images, focusing on those reflective regions.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.4" class="ltx_p">In addition, we can quantitatively measure the view consistency score <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="S_{i\rightarrow j}" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><msub id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">S</mi><mrow id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml"><mi id="S4.SS1.p2.1.m1.1.1.3.2" xref="S4.SS1.p2.1.m1.1.1.3.2.cmml">i</mi><mo stretchy="false" id="S4.SS1.p2.1.m1.1.1.3.1" xref="S4.SS1.p2.1.m1.1.1.3.1.cmml">→</mo><mi id="S4.SS1.p2.1.m1.1.1.3.3" xref="S4.SS1.p2.1.m1.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">𝑆</ci><apply id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3"><ci id="S4.SS1.p2.1.m1.1.1.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3.1">→</ci><ci id="S4.SS1.p2.1.m1.1.1.3.2.cmml" xref="S4.SS1.p2.1.m1.1.1.3.2">𝑖</ci><ci id="S4.SS1.p2.1.m1.1.1.3.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">S_{i\rightarrow j}</annotation></semantics></math> between two frames <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mi id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><ci id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">i</annotation></semantics></math> and <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mi id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><ci id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">j</annotation></semantics></math> given the ground truth optical flow <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="\mathbf{F}_{i\rightarrow j}" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><msub id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml"><mi id="S4.SS1.p2.4.m4.1.1.2" xref="S4.SS1.p2.4.m4.1.1.2.cmml">𝐅</mi><mrow id="S4.SS1.p2.4.m4.1.1.3" xref="S4.SS1.p2.4.m4.1.1.3.cmml"><mi id="S4.SS1.p2.4.m4.1.1.3.2" xref="S4.SS1.p2.4.m4.1.1.3.2.cmml">i</mi><mo stretchy="false" id="S4.SS1.p2.4.m4.1.1.3.1" xref="S4.SS1.p2.4.m4.1.1.3.1.cmml">→</mo><mi id="S4.SS1.p2.4.m4.1.1.3.3" xref="S4.SS1.p2.4.m4.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><apply id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.4.m4.1.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.p2.4.m4.1.1.2.cmml" xref="S4.SS1.p2.4.m4.1.1.2">𝐅</ci><apply id="S4.SS1.p2.4.m4.1.1.3.cmml" xref="S4.SS1.p2.4.m4.1.1.3"><ci id="S4.SS1.p2.4.m4.1.1.3.1.cmml" xref="S4.SS1.p2.4.m4.1.1.3.1">→</ci><ci id="S4.SS1.p2.4.m4.1.1.3.2.cmml" xref="S4.SS1.p2.4.m4.1.1.3.2">𝑖</ci><ci id="S4.SS1.p2.4.m4.1.1.3.3.cmml" xref="S4.SS1.p2.4.m4.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">\mathbf{F}_{i\rightarrow j}</annotation></semantics></math>:</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.2" class="ltx_Math" alttext="S_{i\rightarrow j}=\text{SSIM}(\mathbf{I}_{i},\text{warp}(\mathbf{I}_{j},\mathbf{F}_{i\rightarrow j}))" display="block"><semantics id="S4.E3.m1.2a"><mrow id="S4.E3.m1.2.2" xref="S4.E3.m1.2.2.cmml"><msub id="S4.E3.m1.2.2.4" xref="S4.E3.m1.2.2.4.cmml"><mi id="S4.E3.m1.2.2.4.2" xref="S4.E3.m1.2.2.4.2.cmml">S</mi><mrow id="S4.E3.m1.2.2.4.3" xref="S4.E3.m1.2.2.4.3.cmml"><mi id="S4.E3.m1.2.2.4.3.2" xref="S4.E3.m1.2.2.4.3.2.cmml">i</mi><mo stretchy="false" id="S4.E3.m1.2.2.4.3.1" xref="S4.E3.m1.2.2.4.3.1.cmml">→</mo><mi id="S4.E3.m1.2.2.4.3.3" xref="S4.E3.m1.2.2.4.3.3.cmml">j</mi></mrow></msub><mo id="S4.E3.m1.2.2.3" xref="S4.E3.m1.2.2.3.cmml">=</mo><mrow id="S4.E3.m1.2.2.2" xref="S4.E3.m1.2.2.2.cmml"><mtext id="S4.E3.m1.2.2.2.4" xref="S4.E3.m1.2.2.2.4a.cmml">SSIM</mtext><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.2.2.3" xref="S4.E3.m1.2.2.2.3.cmml">​</mo><mrow id="S4.E3.m1.2.2.2.2.2" xref="S4.E3.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.E3.m1.2.2.2.2.2.3" xref="S4.E3.m1.2.2.2.2.3.cmml">(</mo><msub id="S4.E3.m1.1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.cmml"><mi id="S4.E3.m1.1.1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.1.1.2.cmml">𝐈</mi><mi id="S4.E3.m1.1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.E3.m1.2.2.2.2.2.4" xref="S4.E3.m1.2.2.2.2.3.cmml">,</mo><mrow id="S4.E3.m1.2.2.2.2.2.2" xref="S4.E3.m1.2.2.2.2.2.2.cmml"><mtext id="S4.E3.m1.2.2.2.2.2.2.4" xref="S4.E3.m1.2.2.2.2.2.2.4a.cmml">warp</mtext><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.2.2.2.2.2.3" xref="S4.E3.m1.2.2.2.2.2.2.3.cmml">​</mo><mrow id="S4.E3.m1.2.2.2.2.2.2.2.2" xref="S4.E3.m1.2.2.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.E3.m1.2.2.2.2.2.2.2.2.3" xref="S4.E3.m1.2.2.2.2.2.2.2.3.cmml">(</mo><msub id="S4.E3.m1.2.2.2.2.2.2.1.1.1" xref="S4.E3.m1.2.2.2.2.2.2.1.1.1.cmml"><mi id="S4.E3.m1.2.2.2.2.2.2.1.1.1.2" xref="S4.E3.m1.2.2.2.2.2.2.1.1.1.2.cmml">𝐈</mi><mi id="S4.E3.m1.2.2.2.2.2.2.1.1.1.3" xref="S4.E3.m1.2.2.2.2.2.2.1.1.1.3.cmml">j</mi></msub><mo id="S4.E3.m1.2.2.2.2.2.2.2.2.4" xref="S4.E3.m1.2.2.2.2.2.2.2.3.cmml">,</mo><msub id="S4.E3.m1.2.2.2.2.2.2.2.2.2" xref="S4.E3.m1.2.2.2.2.2.2.2.2.2.cmml"><mi id="S4.E3.m1.2.2.2.2.2.2.2.2.2.2" xref="S4.E3.m1.2.2.2.2.2.2.2.2.2.2.cmml">𝐅</mi><mrow id="S4.E3.m1.2.2.2.2.2.2.2.2.2.3" xref="S4.E3.m1.2.2.2.2.2.2.2.2.2.3.cmml"><mi id="S4.E3.m1.2.2.2.2.2.2.2.2.2.3.2" xref="S4.E3.m1.2.2.2.2.2.2.2.2.2.3.2.cmml">i</mi><mo stretchy="false" id="S4.E3.m1.2.2.2.2.2.2.2.2.2.3.1" xref="S4.E3.m1.2.2.2.2.2.2.2.2.2.3.1.cmml">→</mo><mi id="S4.E3.m1.2.2.2.2.2.2.2.2.2.3.3" xref="S4.E3.m1.2.2.2.2.2.2.2.2.2.3.3.cmml">j</mi></mrow></msub><mo stretchy="false" id="S4.E3.m1.2.2.2.2.2.2.2.2.5" xref="S4.E3.m1.2.2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E3.m1.2.2.2.2.2.5" xref="S4.E3.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.2b"><apply id="S4.E3.m1.2.2.cmml" xref="S4.E3.m1.2.2"><eq id="S4.E3.m1.2.2.3.cmml" xref="S4.E3.m1.2.2.3"></eq><apply id="S4.E3.m1.2.2.4.cmml" xref="S4.E3.m1.2.2.4"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.4.1.cmml" xref="S4.E3.m1.2.2.4">subscript</csymbol><ci id="S4.E3.m1.2.2.4.2.cmml" xref="S4.E3.m1.2.2.4.2">𝑆</ci><apply id="S4.E3.m1.2.2.4.3.cmml" xref="S4.E3.m1.2.2.4.3"><ci id="S4.E3.m1.2.2.4.3.1.cmml" xref="S4.E3.m1.2.2.4.3.1">→</ci><ci id="S4.E3.m1.2.2.4.3.2.cmml" xref="S4.E3.m1.2.2.4.3.2">𝑖</ci><ci id="S4.E3.m1.2.2.4.3.3.cmml" xref="S4.E3.m1.2.2.4.3.3">𝑗</ci></apply></apply><apply id="S4.E3.m1.2.2.2.cmml" xref="S4.E3.m1.2.2.2"><times id="S4.E3.m1.2.2.2.3.cmml" xref="S4.E3.m1.2.2.2.3"></times><ci id="S4.E3.m1.2.2.2.4a.cmml" xref="S4.E3.m1.2.2.2.4"><mtext id="S4.E3.m1.2.2.2.4.cmml" xref="S4.E3.m1.2.2.2.4">SSIM</mtext></ci><interval closure="open" id="S4.E3.m1.2.2.2.2.3.cmml" xref="S4.E3.m1.2.2.2.2.2"><apply id="S4.E3.m1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2">𝐈</ci><ci id="S4.E3.m1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S4.E3.m1.2.2.2.2.2.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2"><times id="S4.E3.m1.2.2.2.2.2.2.3.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3"></times><ci id="S4.E3.m1.2.2.2.2.2.2.4a.cmml" xref="S4.E3.m1.2.2.2.2.2.2.4"><mtext id="S4.E3.m1.2.2.2.2.2.2.4.cmml" xref="S4.E3.m1.2.2.2.2.2.2.4">warp</mtext></ci><interval closure="open" id="S4.E3.m1.2.2.2.2.2.2.2.3.cmml" xref="S4.E3.m1.2.2.2.2.2.2.2.2"><apply id="S4.E3.m1.2.2.2.2.2.2.1.1.1.cmml" xref="S4.E3.m1.2.2.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.2.2.2.2.1.1.1.1.cmml" xref="S4.E3.m1.2.2.2.2.2.2.1.1.1">subscript</csymbol><ci id="S4.E3.m1.2.2.2.2.2.2.1.1.1.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2.1.1.1.2">𝐈</ci><ci id="S4.E3.m1.2.2.2.2.2.2.1.1.1.3.cmml" xref="S4.E3.m1.2.2.2.2.2.2.1.1.1.3">𝑗</ci></apply><apply id="S4.E3.m1.2.2.2.2.2.2.2.2.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.2.2.2.2.2.2.2.1.cmml" xref="S4.E3.m1.2.2.2.2.2.2.2.2.2">subscript</csymbol><ci id="S4.E3.m1.2.2.2.2.2.2.2.2.2.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2.2.2.2.2">𝐅</ci><apply id="S4.E3.m1.2.2.2.2.2.2.2.2.2.3.cmml" xref="S4.E3.m1.2.2.2.2.2.2.2.2.2.3"><ci id="S4.E3.m1.2.2.2.2.2.2.2.2.2.3.1.cmml" xref="S4.E3.m1.2.2.2.2.2.2.2.2.2.3.1">→</ci><ci id="S4.E3.m1.2.2.2.2.2.2.2.2.2.3.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2.2.2.2.3.2">𝑖</ci><ci id="S4.E3.m1.2.2.2.2.2.2.2.2.2.3.3.cmml" xref="S4.E3.m1.2.2.2.2.2.2.2.2.2.3.3">𝑗</ci></apply></apply></interval></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.2c">S_{i\rightarrow j}=\text{SSIM}(\mathbf{I}_{i},\text{warp}(\mathbf{I}_{j},\mathbf{F}_{i\rightarrow j}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.4" class="ltx_p">Where <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="\mathbf{I}_{i}" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><msub id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mi id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml">𝐈</mi><mi id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2">𝐈</ci><ci id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">\mathbf{I}_{i}</annotation></semantics></math> and <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="\mathbf{I}_{j}" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><msub id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml"><mi id="S4.SS1.p4.2.m2.1.1.2" xref="S4.SS1.p4.2.m2.1.1.2.cmml">𝐈</mi><mi id="S4.SS1.p4.2.m2.1.1.3" xref="S4.SS1.p4.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><apply id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.1.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p4.2.m2.1.1.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2">𝐈</ci><ci id="S4.SS1.p4.2.m2.1.1.3.cmml" xref="S4.SS1.p4.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">\mathbf{I}_{j}</annotation></semantics></math> are the RGB images of frame <math id="S4.SS1.p4.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS1.p4.3.m3.1a"><mi id="S4.SS1.p4.3.m3.1.1" xref="S4.SS1.p4.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.1b"><ci id="S4.SS1.p4.3.m3.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.1c">i</annotation></semantics></math> and frame <math id="S4.SS1.p4.4.m4.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.SS1.p4.4.m4.1a"><mi id="S4.SS1.p4.4.m4.1.1" xref="S4.SS1.p4.4.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.4.m4.1b"><ci id="S4.SS1.p4.4.m4.1.1.cmml" xref="S4.SS1.p4.4.m4.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.4.m4.1c">j</annotation></semantics></math> respectively, <span id="S4.SS1.p4.4.1" class="ltx_text">warp</span> is a function that warps back the input frame according to the forward flow, and <span id="S4.SS1.p4.4.2" class="ltx_text">SSIM</span> computes the structural similarity score (SSIM)  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite> of two images. This produces a 2D map the same size as the image, with values proportional to how consistent the two frames are. Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.4 Computational Complexity ‣ 3 Method ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the SSIM map for the zoomed-in regions.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">We analyze the complete video sequence for 3 scenes, and provide more in the supplementary material. In Fig. <a href="#S3.F6" title="Figure 6 ‣ 3.4 Computational Complexity ‣ 3 Method ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, column (a) shows an overview of the scene for reference; column (b)(c) shows the SSIM score map <math id="S4.SS1.p5.1.m1.1" class="ltx_Math" alttext="S_{8\rightarrow 9}" display="inline"><semantics id="S4.SS1.p5.1.m1.1a"><msub id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml"><mi id="S4.SS1.p5.1.m1.1.1.2" xref="S4.SS1.p5.1.m1.1.1.2.cmml">S</mi><mrow id="S4.SS1.p5.1.m1.1.1.3" xref="S4.SS1.p5.1.m1.1.1.3.cmml"><mn id="S4.SS1.p5.1.m1.1.1.3.2" xref="S4.SS1.p5.1.m1.1.1.3.2.cmml">8</mn><mo stretchy="false" id="S4.SS1.p5.1.m1.1.1.3.1" xref="S4.SS1.p5.1.m1.1.1.3.1.cmml">→</mo><mn id="S4.SS1.p5.1.m1.1.1.3.3" xref="S4.SS1.p5.1.m1.1.1.3.3.cmml">9</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.1b"><apply id="S4.SS1.p5.1.m1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p5.1.m1.1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p5.1.m1.1.1.2.cmml" xref="S4.SS1.p5.1.m1.1.1.2">𝑆</ci><apply id="S4.SS1.p5.1.m1.1.1.3.cmml" xref="S4.SS1.p5.1.m1.1.1.3"><ci id="S4.SS1.p5.1.m1.1.1.3.1.cmml" xref="S4.SS1.p5.1.m1.1.1.3.1">→</ci><cn type="integer" id="S4.SS1.p5.1.m1.1.1.3.2.cmml" xref="S4.SS1.p5.1.m1.1.1.3.2">8</cn><cn type="integer" id="S4.SS1.p5.1.m1.1.1.3.3.cmml" xref="S4.SS1.p5.1.m1.1.1.3.3">9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.1c">S_{8\rightarrow 9}</annotation></semantics></math> between the first pair of mesh-switching frames, i.e., frame #8 and frame #9. Column (b) shows 3 different resolution settings with the Infinigen solution; (c) shows ours. The brighter the color, the worse the SSIM score. From left to right in (b), as the resolution increases, the score gets better. Yet, ours has much better view consistency even compared with the best in Infinigen. The only significant inconsistency (yellow area) lies in occlusion boundaries and where volume scattering makes the rendering noisy.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p">Fig. <a href="#S3.F7" title="Figure 7 ‣ 3.4 Computational Complexity ‣ 3 Method ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> (a) shows the SSIM score for adjacent frames, which details how much flickering is experience when we watch the video continuously. It shows the frame-wise average against time on the left. The sharp comb-like peaks indicate periodical flickering in the Infinigen solution during transition frames. On the right, we show scene-wise average end-point-error (EPE) for transition frames versus the mesh extraction time. We do not include rendering time because it is very dependent on the rendering engine. Infinigen’s flickering is reduced by additional mesh resolution, but this incurs increased runtime. However, even in its highest resolution, it is still much worse than our method.</p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p">Fig. <a href="#S3.F7" title="Figure 7 ‣ 3.4 Computational Complexity ‣ 3 Method ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> (b) shows SSIM score along many individual pixel trajectories, comparing frame 1 to frame <math id="S4.SS1.p7.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS1.p7.1.m1.1a"><mi id="S4.SS1.p7.1.m1.1.1" xref="S4.SS1.p7.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p7.1.m1.1b"><ci id="S4.SS1.p7.1.m1.1.1.cmml" xref="S4.SS1.p7.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p7.1.m1.1c">i</annotation></semantics></math> at every timestep. It also shows the frame-wise average against time and the scene-wise average against the generation time. We can see as soon as the frame goes beyond the first group of frames where the mesh is reused, the score drops significantly in Infinigen, so it is much harder to match corresponding points there. The trade-off curve is similar.</p>
</div>
<figure id="S4.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F8.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/flow_benchmark1_1.png" id="S4.F8.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="154" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(a) Gunnar Farneback’s algorithm</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F8.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/flow_benchmark1_2.png" id="S4.F8.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="154" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(b) RAFT</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F8.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/flow_benchmark1_3.png" id="S4.F8.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="171" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(c) VideoFlow</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.5.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S4.F8.6.2" class="ltx_text" style="font-size:90%;">End-point-error (EPE) for 3 pre-trained optical flow methods evaluated on meshes from OcMesher vs. Infinigen at various resolutions.</span></figcaption>
</figure>
<figure id="S4.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2312.08364/assets/figures/train_epe.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="479" height="240" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2312.08364/assets/figures/train_1px.png" id="S4.F9.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="479" height="240" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S4.F9.3.2" class="ltx_text" style="font-size:90%;">Models trained on our dataset achieve better or comparable results with a lower cost.</span></figcaption>
</figure>
<figure id="S4.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F10.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:238.5pt;"><img src="/html/2312.08364/assets/figures/realtime_scene_thickcam.jpg" id="S4.F10.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption">(a) The scene and wireframe visualization</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F10.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:238.5pt;"><img src="/html/2312.08364/assets/figures/realtime_screenshots.png" id="S4.F10.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption">(b) Screenshots of a real-time video. FPS: 53.46</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F10.4.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S4.F10.5.2" class="ltx_text" style="font-size:90%;"> Our method enables real-time (&gt;50 FPS) scene exploration in Unreal Engine</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Improved Benchmarks</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We compare these datasets as benchmarks for the optical flow estimation task. With the same method and the same scene, if a dataset setting produces stable and reasonable errors as the camera moves smoothly, this means the dataset has fewer distracting factors and can serve as a better benchmark. We evaluate optical flow for each pair of adjacent frames using 3 existing methods: Gunnar Farneback’s algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> from OpenCV, RAFT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>, and VideoFlow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>. Fig. <a href="#S4.F8" title="Figure 8 ‣ 4.1 View Consistency ‣ 4 Experiments ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the results of each method for one scene (see the supplement for more). On the left, we show frame-wise mean endpoint error (EPE). Again,we see spikes during transition frames in Infinigen, especially in the first two methods which estimate flow independently without neighbor information. This demonstrates that Infinigen has significant measurement noise when used as a benchmark dataset. In contrast, evaluating on our data produces consistent and smooth errors, except for the middle frame where the camera motion suddenly changes direction, which is an expected phenomenon.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Improved Model Training</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We also use the generated datasets as training sets for the optical flow task. Particularly, we focus on wide baseline scenarios, which is a more challenging task and more useful for relevant applications like stereo matching. We use RAFT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>, and train the model from scratch for 50k iterations with batch size 2 on image pairs from 32 out of the 42 scenes, with the rest as a validation set. We train on images sampled 10 frames apart, and exclude images with median optical flows greater than 50px. We compare the training results versus the mesh generation cost of the dataset in Fig <a href="#S4.F9" title="Figure 9 ‣ 4.1 View Consistency ‣ 4 Experiments ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. We can see that models trained on our dataset attain better EPE and % &lt;1 px metrics and at a lower lower cost. Infinigen is prohibitively expensive to achieve similar performance.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Creating Embodied AI Environments</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Most importantly, our method can create a simulated virtual environment in real-time rendering engines like Unity or Unreal Engine, where an embodied AI agent can explore and learn. To demonstrate this, we took a scene from the previous dataset, and extracted a mesh that is valid for a range of camera rotations and translations along a path of interest. This results in an unbounded mesh that can be explored interactively within a certain range of camera poses. We export the resulting mesh to Unreal Engine. We experimented on a 4K (<math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="3840\times 2160" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mrow id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mn id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">3840</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS4.p1.1.m1.1.1.1" xref="S4.SS4.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3.cmml">2160</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><times id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2">3840</cn><cn type="integer" id="S4.SS4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3">2160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">3840\times 2160</annotation></semantics></math>) display and with one NVIDIA GeForce RTX 3090 GPU, and achieved &gt;50 FPS rendering frame rate. Fig. <a href="#S4.F10" title="Figure 10 ‣ 4.1 View Consistency ‣ 4 Experiments ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>(a) shows the scene and the wireframe visualization, and we can see the mesh is denser on those surfaces closer to the camera view and sparser far away. Fig. <a href="#S4.F10" title="Figure 10 ‣ 4.1 View Consistency ‣ 4 Experiments ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>(b) shows the screenshots of the real-time video. In contrast, Infinigen needs to create a new mesh for each new view, which takes about 0.5 seconds even in such low resolution as shown in Fig. <a href="#S4.F11" title="Figure 11 ‣ 4.4 Creating Embodied AI Environments ‣ 4 Experiments ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>. Even ignoring mesh loading overhead, this would give a theoretical framerate of 2.41 FPS. This could be increased by refreshing the mesh only every few frames, but this would produce visible seams and low poly faces.</p>
</div>
<figure id="S4.F11" class="ltx_figure"><img src="/html/2312.08364/assets/figures/nonrealtime_fps.jpg" id="S4.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F11.2.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S4.F11.3.2" class="ltx_text" style="font-size:90%;">Re-extracting new view-dependent meshes with Infinigen’s solution cannot achieve interactive frame-rates, even at extremely low detail ignoring any mesh load-time.</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We proposed a method to extract meshes for unbounded scenes based on a given SDF and a given set of cameras. Unlike previous methods, we generate a high-resolution mesh that can be reused for all predefined cameras. This is very useful in both generating view-consistent procedural synthetic datasets and providing a real-time virtual training environment for embodied computer vision AI.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgements</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This work was partially supported by the National Science Foundation and Amazon.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.3.3.1" class="ltx_text" style="font-size:90%;">[1]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.5.1" class="ltx_text" style="font-size:90%;">
Vectron volume for cinema 4d: https://www.machina-infinitum.com/vectron-volume.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Bai et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Shaojie Bai, Zhengyang Geng, Yash Savani, and J Zico Kolter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">Deep equilibrium optical flow estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib2.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib2.11.3" class="ltx_text" style="font-size:90%;">, pages 620–630, 2022.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Butler et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Daniel J Butler, Jonas Wulff, Garrett B Stanley, and Michael J Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">A naturalistic open source movie for optical flow evaluation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib3.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part VI 12</em><span id="bib.bib3.11.3" class="ltx_text" style="font-size:90%;">, pages 611–625. Springer, 2012.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.4.4.1" class="ltx_text" style="font-size:90%;">Community [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.6.1" class="ltx_text" style="font-size:90%;">
Blender Online Community.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib4.7.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Blender - a 3D modelling and rendering package</em><span id="bib.bib4.8.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.9.1" class="ltx_text" style="font-size:90%;">Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Crassin et al. [2009]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Cyril Crassin, Fabrice Neyret, Sylvain Lefebvre, and Elmar Eisemann.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Gigavoxels: Ray-guided streaming for efficient and detailed voxel rendering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib5.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2009 symposium on Interactive 3D graphics and games</em><span id="bib.bib5.11.3" class="ltx_text" style="font-size:90%;">, pages 15–22, 2009.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Deitke et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Jordi Salvador, Kiana Ehsani, Winson Han, Eric Kolve, Ali Farhadi, Aniruddha Kembhavi, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">ProcTHOR: Large-scale embodied AI using procedural generation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib6.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2206.06994</em><span id="bib.bib6.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Devaranjan et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
Jeevan Devaranjan, Amlan Kar, and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">Meta-sim2: Unsupervised learning of scene structure for synthetic data generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib7.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVII 16</em><span id="bib.bib7.11.3" class="ltx_text" style="font-size:90%;">, pages 715–733. Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.4.4.1" class="ltx_text" style="font-size:90%;">Doi and Koide [1991]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.6.1" class="ltx_text" style="font-size:90%;">
Akio Doi and Akio Koide.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">An efficient method of triangulating equi-valued surfaces by using tetrahedral cells.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib8.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEICE Transactions on Information and Systems</em><span id="bib.bib8.9.2" class="ltx_text" style="font-size:90%;">, 74:214–224, 1991.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Ebert et al. [2003]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
David S Ebert, F Kenton Musgrave, Darwyn Peachey, Ken Perlin, and Steven Worley.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib9.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Texturing &amp; modeling: a procedural approach</em><span id="bib.bib9.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.10.1" class="ltx_text" style="font-size:90%;">Morgan Kaufmann, 2003.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.3.3.1" class="ltx_text" style="font-size:90%;">[10]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.5.1" class="ltx_text" style="font-size:90%;">
Epic Games.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.6.1" class="ltx_text" style="font-size:90%;">Unreal engine.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.4.4.1" class="ltx_text" style="font-size:90%;">Farnebäck [2003]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.6.1" class="ltx_text" style="font-size:90%;">
Gunnar Farnebäck.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">Two-frame motion estimation based on polynomial expansion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib11.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Image Analysis: 13th Scandinavian Conference, SCIA 2003 Halmstad, Sweden, June 29–July 2, 2003 Proceedings 13</em><span id="bib.bib11.10.3" class="ltx_text" style="font-size:90%;">, pages 363–370. Springer, 2003.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Galin et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Eric Galin, Eric Guérin, Axel Paris, and Adrien Peytavie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Segment tracing using local lipschitz bounds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib12.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Graphics Forum</em><span id="bib.bib12.11.3" class="ltx_text" style="font-size:90%;">, pages 545–554. Wiley Online Library, 2020.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.4.4.1" class="ltx_text" style="font-size:90%;">Geiss [2007]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.6.1" class="ltx_text" style="font-size:90%;">
Ryan Geiss.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">Generating complex procedural terrains using the gpu.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib13.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">GPU gems</em><span id="bib.bib13.9.2" class="ltx_text" style="font-size:90%;">, 3(7):37, 2007.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.4.4.1" class="ltx_text" style="font-size:90%;">Gerstner and Pajarola [2000]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.6.1" class="ltx_text" style="font-size:90%;">
Thomas Gerstner and Renato Pajarola.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib14.7.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Topology preserving and controlled topology simplifying multiresolution isosurface extraction</em><span id="bib.bib14.8.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.9.1" class="ltx_text" style="font-size:90%;">IEEE, 2000.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.4.4.1" class="ltx_text" style="font-size:90%;">Giegl and Wimmer [2007]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.6.1" class="ltx_text" style="font-size:90%;">
Markus Giegl and Michael Wimmer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">Unpopping: Solving the image-space blend problem for smooth discrete lod transitions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib15.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Graphics Forum</em><span id="bib.bib15.10.3" class="ltx_text" style="font-size:90%;">, pages 46–49. Wiley Online Library, 2007.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Gobbetti et al. [2008]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
Enrico Gobbetti, Fabio Marton, and José Antonio Iglesias Guitián.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">A single-pass gpu ray casting framework for interactive out-of-core rendering of massive volumetric datasets.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib16.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">The Visual Computer</em><span id="bib.bib16.10.2" class="ltx_text" style="font-size:90%;">, 24:797–806, 2008.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Greff et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Kubric: A scalable dataset generator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib17.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib17.11.3" class="ltx_text" style="font-size:90%;">, pages 3749–3761, 2022.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">Gregorski et al. [2002]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Benjamin Gregorski, Mark Duchaineau, Peter Lindstrom, Valerio Pascucci, and Kenneth I Joy.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib18.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Interactive view-dependent rendering of large isosurfaces</em><span id="bib.bib18.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.10.1" class="ltx_text" style="font-size:90%;">IEEE, 2002.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.4.4.1" class="ltx_text" style="font-size:90%;">Hart [1996]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.6.1" class="ltx_text" style="font-size:90%;">
John C Hart.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">Sphere tracing: A geometric method for the antialiased ray tracing of implicit surfaces.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib19.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">The Visual Computer</em><span id="bib.bib19.9.2" class="ltx_text" style="font-size:90%;">, 12(10):527–545, 1996.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.4.4.1" class="ltx_text" style="font-size:90%;">Hoppe [1997]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.6.1" class="ltx_text" style="font-size:90%;">
Hugues Hoppe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">View-dependent refinement of progressive meshes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib20.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 24th annual conference on Computer graphics and interactive techniques</em><span id="bib.bib20.10.3" class="ltx_text" style="font-size:90%;">, pages 189–198, 1997.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Jiang et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Chenfanfu Jiang, Siyuan Qi, Yixin Zhu, Siyuan Huang, Jenny Lin, Lap-Fai Yu, Demetri Terzopoulos, and Song-Chun Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">Configurable 3d scene synthesis and 2d image rendering with per-pixel ground truth using stochastic grammars.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib21.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</em><span id="bib.bib21.10.2" class="ltx_text" style="font-size:90%;">, 126:920–941, 2018.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.4.4.1" class="ltx_text" style="font-size:90%;">Ju and Udeshi [2006]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.6.1" class="ltx_text" style="font-size:90%;">
Tao Ju and Tushar Udeshi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">Intersection-free contouring on an octree grid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib22.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of Pacific graphics</em><span id="bib.bib22.10.3" class="ltx_text" style="font-size:90%;">. Citeseer, 2006.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Ju et al. [2002]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Tao Ju, Frank Losasso, Scott Schaefer, and Joe Warren.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">Dual contouring of hermite data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib23.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 29th annual conference on Computer graphics and interactive techniques</em><span id="bib.bib23.11.3" class="ltx_text" style="font-size:90%;">, pages 339–346, 2002.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Khan et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Samin Khan, Buu Phan, Rick Salay, and Krzysztof Czarnecki.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Procsy: Procedural synthetic dataset generation towards influence factor studies of semantic segmentation networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib24.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR workshops</em><span id="bib.bib24.11.3" class="ltx_text" style="font-size:90%;">, page 4, 2019.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.4.4.1" class="ltx_text" style="font-size:90%;">Law and Deng [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.6.1" class="ltx_text" style="font-size:90%;">
Hei Law and Jia Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">Label-free synthetic pretraining of object detectors.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib25.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2208.04268</em><span id="bib.bib25.9.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Jiankun Li, Peisen Wang, Pengfei Xiong, Tao Cai, Ziwei Yan, Lei Yang, Jiangyu Liu, Haoqiang Fan, and Shuaicheng Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Practical stereo matching via cascaded recurrent network with adaptive correlation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib26.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib26.11.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Wenbin Li, Sajad Saeedi, John McCormac, Ronald Clark, Dimos Tzoumanikas, Qing Ye, Yuzhong Huang, Rui Tang, and Stefan Leutenegger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">Interiornet: Mega-scale multi-sensor photo-realistic indoor scenes dataset.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib27.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1809.00716</em><span id="bib.bib27.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Lindstrom et al. [1996]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
Peter Lindstrom, David Koller, William Ribarsky, Larry F Hodges, Nick Faust, and Gregory A Turner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">Real-time, continuous level of detail rendering of height fields.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib28.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 23rd annual conference on Computer graphics and interactive techniques</em><span id="bib.bib28.11.3" class="ltx_text" style="font-size:90%;">, pages 109–118, 1996.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Lipson et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Lahav Lipson, Zachary Teed, Ankit Goyal, and Jia Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Coupled iterative refinement for 6d multi-object pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib29.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib29.11.3" class="ltx_text" style="font-size:90%;">, pages 6728–6737, 2022.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2001]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">
Zhiyan Liu, Adam Finkelstein, and Kai Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">Progressive view-dependent isosurface propagation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib30.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Data Visualization 2001: Proceedings of the Joint Eurographics—IEEE TCVG Symposium on Visualization in Ascona, Switzerland, May 28–30, 2001</em><span id="bib.bib30.11.3" class="ltx_text" style="font-size:90%;">, pages 223–232. Springer, 2001.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.4.4.1" class="ltx_text" style="font-size:90%;">Livnat and Hansen [1998]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.6.1" class="ltx_text" style="font-size:90%;">
Yarden Livnat and Charles Hansen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">View dependent isosurface extraction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib31.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings Visualization’98 (Cat. No. 98CB36276)</em><span id="bib.bib31.10.3" class="ltx_text" style="font-size:90%;">, pages 175–180. IEEE, 1998.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.4.4.1" class="ltx_text" style="font-size:90%;">Lorensen and Cline [1998]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.6.1" class="ltx_text" style="font-size:90%;">
William E Lorensen and Harvey E Cline.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">Marching cubes: A high resolution 3d surface construction algorithm.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib32.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Seminal graphics: pioneering efforts that shaped the field</em><span id="bib.bib32.10.3" class="ltx_text" style="font-size:90%;">, pages 347–353. 1998.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.5.5.1" class="ltx_text" style="font-size:90%;">Ma et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">
Zeyu Ma, Zachary Teed, and Jia Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">Multiview stereo with cascaded epipolar raft.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib33.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXI</em><span id="bib.bib33.11.3" class="ltx_text" style="font-size:90%;">, pages 734–750. Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.4.4.1" class="ltx_text" style="font-size:90%;">Meagher [1980]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.6.1" class="ltx_text" style="font-size:90%;">
Donald Meagher.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">Octree encoding: A new technique for the representation, manipulation and display of arbitrary 3-d objects by computer, 1980.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.4.4.1" class="ltx_text" style="font-size:90%;">Musgrave [1993]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.6.1" class="ltx_text" style="font-size:90%;">
Forest Kenton Musgrave.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib35.7.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Methods for realistic landscape imaging</em><span id="bib.bib35.8.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.9.1" class="ltx_text" style="font-size:90%;">Yale University, 1993.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.4.4.1" class="ltx_text" style="font-size:90%;">Musgrave [2003]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.6.1" class="ltx_text" style="font-size:90%;">
F Kenton Musgrave.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">Qaeb rendering for procedural models.
</span>
</span>
<span class="ltx_bibblock"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Ebert et al.</span> <span id="bib.bib36.8.1.1.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a><span id="bib.bib36.9.2.2.2.1" class="ltx_text" style="font-size:90%;">]</span></cite><span id="bib.bib36.10.3" class="ltx_text" style="font-size:90%;">, pages 509–528.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.4.4.1" class="ltx_text" style="font-size:90%;">Pascucci and Cole-McLaughlin [2002]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.6.1" class="ltx_text" style="font-size:90%;">
Valerio Pascucci and Kree Cole-McLaughlin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">Efficient computation of the topology of level sets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib37.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Visualization, 2002. VIS 2002.</em><span id="bib.bib37.10.3" class="ltx_text" style="font-size:90%;">, pages 187–194. IEEE, 2002.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.4.4.1" class="ltx_text" style="font-size:90%;">Perlin [1985]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.6.1" class="ltx_text" style="font-size:90%;">
Ken Perlin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">An image synthesizer.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib38.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM Siggraph Computer Graphics</em><span id="bib.bib38.9.2" class="ltx_text" style="font-size:90%;">, 19(3):287–296, 1985.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.4.4.1" class="ltx_text" style="font-size:90%;">Perlin [2003]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.6.1" class="ltx_text" style="font-size:90%;">
Ken Perlin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">Noise, hypertexture, antialiasing and gesture.
</span>
</span>
<span class="ltx_bibblock"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Ebert et al.</span> <span id="bib.bib39.8.1.1.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a><span id="bib.bib39.9.2.2.2.1" class="ltx_text" style="font-size:90%;">]</span></cite><span id="bib.bib39.10.3" class="ltx_text" style="font-size:90%;">, pages 337–412.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.4.4.1" class="ltx_text" style="font-size:90%;">Perry and Frisken [2001]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.6.1" class="ltx_text" style="font-size:90%;">
Ronald N Perry and Sarah F Frisken.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">Kizamu: A system for sculpting digital characters.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib40.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</em><span id="bib.bib40.10.3" class="ltx_text" style="font-size:90%;">, pages 47–56, 2001.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Raistrick et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
Alexander Raistrick, Lahav Lipson, Zeyu Ma, Lingjie Mei, Mingzhe Wang, Yiming Zuo, Karhan Kayan, Hongyu Wen, Beining Han, Yihan Wang, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">Infinite photorealistic worlds using procedural generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib41.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib41.11.3" class="ltx_text" style="font-size:90%;">, pages 12630–12641, 2023.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Richter et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">Playing for data: Ground truth from computer games.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib42.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ECCV</em><span id="bib.bib42.11.3" class="ltx_text" style="font-size:90%;">, pages 102–118. Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text" style="font-size:90%;">Scholz et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">
Manuel Scholz, Jan Bender, and Carsten Dachsbacher.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="font-size:90%;">Real-time isosurface extraction with view-dependent level of detail and applications.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib43.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Graphics Forum</em><span id="bib.bib43.11.3" class="ltx_text" style="font-size:90%;">, pages 103–115. Wiley Online Library, 2015.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text" style="font-size:90%;">Shi et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">
Xiaoyu Shi, Zhaoyang Huang, Weikang Bian, Dasong Li, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text" style="font-size:90%;">Videoflow: Exploiting temporal cues for multi-frame optical flow estimation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib44.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.08340</em><span id="bib.bib44.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.4.4.1" class="ltx_text" style="font-size:90%;">Teed and Deng [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.6.1" class="ltx_text" style="font-size:90%;">
Zachary Teed and Jia Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="font-size:90%;">Raft: Recurrent all-pairs field transforms for optical flow.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib45.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16</em><span id="bib.bib45.10.3" class="ltx_text" style="font-size:90%;">, pages 402–419. Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.5.5.1" class="ltx_text" style="font-size:90%;">Tsirikoglou et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="font-size:90%;">
Apostolia Tsirikoglou, Joel Kronander, Magnus Wrenninge, and Jonas Unger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.8.1" class="ltx_text" style="font-size:90%;">Procedural modeling and physically based rendering for synthetic data generation in automotive applications.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib46.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1710.06270</em><span id="bib.bib46.10.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text" style="font-size:90%;">
Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.8.1" class="ltx_text" style="font-size:90%;">Tartanair: A dataset to push the limits of visual slam.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib47.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em><span id="bib.bib47.11.3" class="ltx_text" style="font-size:90%;">, pages 4909–4916. IEEE, 2020.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2004]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text" style="font-size:90%;">
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text" style="font-size:90%;">Image quality assessment: from error visibility to structural similarity.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib48.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE transactions on image processing</em><span id="bib.bib48.10.2" class="ltx_text" style="font-size:90%;">, 13(4):600–612, 2004.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.5.5.1" class="ltx_text" style="font-size:90%;">Weber et al. [2003]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.7.1" class="ltx_text" style="font-size:90%;">
Gunther H Weber, Oliver Kreylos, Terry J Ligocki, John M Shalf, Hans Hagen, Bernd Hamann, and Kenneth I Joy.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib49.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Extraction of crack-free isosurfaces from adaptive mesh refinement data</em><span id="bib.bib49.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.10.1" class="ltx_text" style="font-size:90%;">Springer, 2003.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.4.4.1" class="ltx_text" style="font-size:90%;">Wenger [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.6.1" class="ltx_text" style="font-size:90%;">
Rephael Wenger.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib50.7.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Isosurfaces: geometry, topology, and algorithms</em><span id="bib.bib50.8.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.9.1" class="ltx_text" style="font-size:90%;">CRC Press, 2013.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.4.4.1" class="ltx_text" style="font-size:90%;">Wrenninge and Unger [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.6.1" class="ltx_text" style="font-size:90%;">
Magnus Wrenninge and Jonas Unger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.7.1" class="ltx_text" style="font-size:90%;">Synscapes: A photorealistic synthetic dataset for street scene parsing.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib51.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1810.08705</em><span id="bib.bib51.9.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.5.5.1" class="ltx_text" style="font-size:90%;">Yao et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.7.1" class="ltx_text" style="font-size:90%;">
Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.8.1" class="ltx_text" style="font-size:90%;">Blendedmvs: A large-scale dataset for generalized multi-view stereo networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib52.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span id="bib.bib52.11.3" class="ltx_text" style="font-size:90%;">, pages 1790–1799, 2020.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.5.5.1" class="ltx_text" style="font-size:90%;">Zhou et al. [1997]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.7.1" class="ltx_text" style="font-size:90%;">
Yong Zhou, Baoquan Chen, and Arie Kaufman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.8.1" class="ltx_text" style="font-size:90%;">Multiresolution tetrahedral framework for visualizing regular volume data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib53.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings. Visualization’97 (Cat. No. 97CB36155)</em><span id="bib.bib53.11.3" class="ltx_text" style="font-size:90%;">, pages 135–142. IEEE, 1997.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p1" class="ltx_para ltx_align_center">
<span id="p1.1" class="ltx_ERROR undefined">\thetitle</span>
<br class="ltx_break">
<p id="p1.2" class="ltx_p"><span id="p1.2.1" class="ltx_text" style="font-size:144%;">Supplementary Material 
<br class="ltx_break">
</span></p>
</div>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>More Quantitative Measurements of View Consistency</h2>

<figure id="S7.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S7.F12.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/ssim3.png" id="S7.F12.1.g1" class="ltx_graphics ltx_img_portrait" width="598" height="1028" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(a) Scene overview</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S7.F12.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/ssim4.jpg" id="S7.F12.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="339" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(b) Infinigen with increasing resolution</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S7.F12.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/ssim5.png" id="S7.F12.3.g1" class="ltx_graphics ltx_img_portrait" width="598" height="1028" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(c) Ours</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F12.5.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="S7.F12.6.2" class="ltx_text" style="font-size:90%;">Quantitative measurement of view consistency for the first pair of transition frames. The brighter, the worse.</span></figcaption>
</figure>
<figure id="S7.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S7.F13.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/ssim_tradeoff3.png" id="S7.F13.1.g1" class="ltx_graphics ltx_img_landscape" width="275" height="143" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(a) Comparison of frame-by-frame SSIM</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S7.F13.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/ssim_tradeoff4.png" id="S7.F13.2.g1" class="ltx_graphics ltx_img_landscape" width="275" height="143" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(b) Comparison of first-to-nth frame SSIM</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F13.4.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="S7.F13.5.2" class="ltx_text" style="font-size:90%;">Quantitative measurement of view consistency (a) for adjacent frames and (b) along a pixel trajectory. Extension of Fig. <a href="#S3.F7" title="Figure 7 ‣ 3.4 Computational Complexity ‣ 3 Method ‣ View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for Procedural Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a></span></figcaption>
</figure>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>More Evaluation Results of 3 Pre-trained Optical Flow Methods</h2>

<figure id="S8.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S8.F14.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/flow_benchmark_scenes_supp.jpg" id="S8.F14.1.g1" class="ltx_graphics ltx_img_portrait" width="598" height="1719" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(a) Scene overview</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S8.F14.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.08364/assets/figures/flow_benchmark_supp.png" id="S8.F14.2.g1" class="ltx_graphics ltx_img_square" width="598" height="529" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(b) From left to right: Gunnar Farneback’s algorithm, RAFT and VideoFlow</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F14.4.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>: </span><span id="S8.F14.5.2" class="ltx_text" style="font-size:90%;">End-point-error (EPE) for 3 pre-trained optical flow methods evaluated on meshes from OcMesher vs. Infinigen at various resolutions.</span></figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2312.08363" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2312.08364" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2312.08364">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2312.08364" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2312.08365" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 14:03:47 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
