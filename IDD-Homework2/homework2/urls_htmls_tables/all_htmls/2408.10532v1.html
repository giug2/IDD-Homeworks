<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024</title>
<!--Generated on Tue Aug 20 04:09:27 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
computer vision,  YOLO model,  artificial intelligence,  machine learning,  food recognition
" lang="en" name="keywords"/>
<base href="/html/2408.10532v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S1" title="In NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S1.SS1" title="In I Introduction ‣ NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">I-A</span> </span><span class="ltx_text ltx_font_italic">YOLO Models</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S2" title="In NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Literature Review</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S3" title="In NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S3.SS1" title="In III Methodology ‣ NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Overview</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S3.SS2" title="In III Methodology ‣ NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Data Collection and Preprocessing</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S3.SS2.SSS1" title="In III-B Data Collection and Preprocessing ‣ III Methodology ‣ NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>1 </span>Training Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S3.SS2.SSS1.Px1" title="In III-B1 Training Datasets ‣ III-B Data Collection and Preprocessing ‣ III Methodology ‣ NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title">Open Images V6-Food Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S3.SS2.SSS1.Px2" title="In III-B1 Training Datasets ‣ III-B Data Collection and Preprocessing ‣ III Methodology ‣ NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title">School Lunch Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S3.SS2.SSS1.Px3" title="In III-B1 Training Datasets ‣ III-B Data Collection and Preprocessing ‣ III Methodology ‣ NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title">Vietnamese Food Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S3.SS2.SSS1.Px4" title="In III-B1 Training Datasets ‣ III-B Data Collection and Preprocessing ‣ III Methodology ‣ NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title">MAFood-121 Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S3.SS2.SSS1.Px5" title="In III-B1 Training Datasets ‣ III-B Data Collection and Preprocessing ‣ III Methodology ‣ NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title">Food-101 Dataset</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S3.SS3" title="In III Methodology ‣ NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Model Architecture</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S3.SS4" title="In III Methodology ‣ NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Model Evaluation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S3.SS5" title="In III Methodology ‣ NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-E</span> </span><span class="ltx_text ltx_font_italic">API Integration</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S3.SS5.SSS0.Px1" title="In III-E API Integration ‣ III Methodology ‣ NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title">Edamam Nutrient Analysis API</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S3.SS5.SSS0.Px2" title="In III-E API Integration ‣ III Methodology ‣ NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title">Edamam Recipe and Meal Planning API</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S3.SS5.SSS0.Px3" title="In III-E API Integration ‣ III Methodology ‣ NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title">Google Sheets API</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S3.SS5.SSS0.Px4" title="In III-E API Integration ‣ III Methodology ‣ NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title">Server-Side Implementation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S3.SS5.SSS0.Px5" title="In III-E API Integration ‣ III Methodology ‣ NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title">Client-Server Implementation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S4" title="In NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Results</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S5" title="In NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S5.SS1" title="In V Discussion ‣ NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Future Improvements</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S6" title="In NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations
<br class="ltx_break"/><span class="ltx_note ltx_role_thanks" id="id1.id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>Date: August 12, 2024</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michelle Han
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id2.1.id1">New York University</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id3.2.id2">Department of Technology, Operations, and Statistics 
<br class="ltx_break"/></span>New York, New York 
<br class="ltx_break"/>mh7793@nyu.edu
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Junyao Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id4.1.id1">New York University</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id5.2.id2">Department of Technology, Operations, and Statistics 
<br class="ltx_break"/></span>New York, New York 
<br class="ltx_break"/>junyao.chen@nyu.edu
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id6.id1">With diet and nutrition apps reaching 1.4 billion users in 2022 [1], it’s no surprise that health apps like MyFitnessPal, Noom, and Calorie Counter, are surging in popularity. However, one major setback [2] of nearly all nutrition applications is that users must enter food data manually, which is time-consuming and tedious. Thus, there has been an increasing demand for applications that can accurately identify food items, analyze their nutritional content, and offer dietary recommendations in real-time. This paper introduces a comprehensive system that combines advanced computer vision techniques with nutrition analysis, implemented in a versatile mobile and web application. The system is divided into three key components: 1) food detection using the YOLOv8 model, 2) nutrient analysis via the Edamam Nutrition Analysis API, and 3) personalized meal recommendations using the Edamam Meal Planning and Recipe Search APIs. Designed for both mobile and web platforms, the application ensures fast processing times with an intuitive user interface, with features such as data visualizations using Chart.js, a login system, and personalized settings for dietary preferences, allergies, and cuisine choices. Preliminary results showcase the system’s effectiveness, making it a valuable tool for users to make informed dietary decisions.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
computer vision, YOLO model, artificial intelligence, machine learning, food recognition

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The rise of artificial intelligence has significantly impacted various aspects of human daily life, ranging from healthcare to entertainment. Among the numerous advancements in artificial intelligence, computer vision stands out for its potential in how users interact and interpret visual data. Particularly, computer vision offers a promising application in the domain of food recognition and nutrient analysis [3]. As the global population is becoming increasingly health conscious, there is a growing demand for technologies that can accurately identify food items and subsequently provide detailed nutritional information. This research aims to explore the capabilities of a computer vision model, specifically a YOLO (You Only Look Once) model, to accurately recognize food dishes and perform a reliable analysis of their nutritional content.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S1.SS1.4.1.1">I-A</span> </span><span class="ltx_text ltx_font_italic" id="S1.SS1.5.2">YOLO Models</span>
</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">YOLO (You Only Look Once) models are a family of real-time object detection algorithms that excel at processing images in a single evaluation, predicting bounding boxes and class probabilities simultaneously. Unlike traditional object detection models, which process images in multiple stages, YOLO’s single-stage approach allows for much faster detection times without compromising accuracy [4]. This makes YOLO models particularly suitable for this application, which requires rapid and precise object detection. Other fields YOLO models are applicable include autonomous driving, surveillance [5], and notably, food recognition, where quick and accurate results are essential.</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p" id="S1.SS1.p2.1">YOLOv5 and YOLOv8 are two advanced versions of YOLO models, each incorporating techniques such as multi-scale predictions and anchor boxes [6], along with various architectural improvements to enhance detection performance. YOLOv5 is known for its balance between speed and accuracy, making it versatile for diverse applications, while YOLOv8 offers even better accuracy and efficiency through further optimization. In this paper, we test variations of both models.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Literature Review</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2408.10532v1#S2.T1" title="TABLE I ‣ II Literature Review ‣ NutrifyAI: An AI-Powered System for Real-Time Food Detection, Nutritional Analysis, and Personalized Meal Recommendations Date: August 12, 2024"><span class="ltx_text ltx_ref_tag">I</span></a> reviews key studies relevant to the development of NutrifyAI, focusing on food detection using YOLO models, the integration of nutritional APIs for real-time analysis, and AI-driven meal recommendations.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Summary of Related Work in Food Detection and Nutritional Analysis</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.1.1">
<span class="ltx_p" id="S2.T1.1.1.1.1.1.1" style="width:128.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1.1.1">Title</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.2.1">
<span class="ltx_p" id="S2.T1.1.1.1.2.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.2.1.1.1">Authors</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.3.1">
<span class="ltx_p" id="S2.T1.1.1.1.3.1.1" style="width:128.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.3.1.1.1">Description</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.4.1">
<span class="ltx_p" id="S2.T1.1.1.1.4.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.4.1.1.1">Benefits</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.5.1">
<span class="ltx_p" id="S2.T1.1.1.1.5.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.5.1.1.1">Limitations</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.2.1.1">
<span class="ltx_p" id="S2.T1.1.2.2.1.1.1" style="width:128.0pt;">Precision and Adaptability of YOLOv5 and YOLOv8 in Dynamic Robotic Environments</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.2.2.1">
<span class="ltx_p" id="S2.T1.1.2.2.2.1.1" style="width:71.1pt;">Kich, V., Muttaqien, M., Toyama, J., Miyoshi, R., Ida, Y., Ohya, A., &amp; Date, H.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.2.3.1">
<span class="ltx_p" id="S2.T1.1.2.2.3.1.1" style="width:128.0pt;">Explores the use of YOLO models in detecting food items in real-time applications. [10]</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.2.4.1">
<span class="ltx_p" id="S2.T1.1.2.2.4.1.1" style="width:71.1pt;">High accuracy with YOLOv8; optimized for real-time detection.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.2.2.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.2.5.1">
<span class="ltx_p" id="S2.T1.1.2.2.5.1.1" style="width:71.1pt;">Limited performance in detecting less common food items; high computational resources needed.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.3.1.1">
<span class="ltx_p" id="S2.T1.1.3.3.1.1.1" style="width:128.0pt;">RecipeIS—Recipe Recommendation System Based on Recognition of Food Ingredients</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.3.2.1">
<span class="ltx_p" id="S2.T1.1.3.3.2.1.1" style="width:71.1pt;">Rodrigues, M., &amp; Fidalgo, F., &amp; Oliveira, A.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.3.3.1">
<span class="ltx_p" id="S2.T1.1.3.3.3.1.1" style="width:128.0pt;">Discusses the integration of nutritional APIs, like Edamam, for real-time nutrient analysis. [11]</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.3.4.1">
<span class="ltx_p" id="S2.T1.1.3.3.4.1.1" style="width:71.1pt;">Comprehensive nutrient breakdowns; enhances user experience.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.3.3.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.3.5.1">
<span class="ltx_p" id="S2.T1.1.3.3.5.1.1" style="width:71.1pt;">Depends on correct food identification; API call rate limits.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.4.4.1.1">
<span class="ltx_p" id="S2.T1.1.4.4.1.1.1" style="width:128.0pt;">AI-powered in the digital age: Ensemble innovation personalizes the food recommendations</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.4.4.2.1">
<span class="ltx_p" id="S2.T1.1.4.4.2.1.1" style="width:71.1pt;">Yaiprasert, C., &amp; Hidayanto, A.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.4.4.3.1">
<span class="ltx_p" id="S2.T1.1.4.4.3.1.1" style="width:128.0pt;">Explores AI-driven approaches to creating personalized meal recommendations. [12]</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.4.4.4.1">
<span class="ltx_p" id="S2.T1.1.4.4.4.1.1" style="width:71.1pt;">High customization based on user data; supports dietary goals.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.4.4.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.4.4.5.1">
<span class="ltx_p" id="S2.T1.1.4.4.5.1.1" style="width:71.1pt;">Potential inaccuracies if input data is incomplete; may not accommodate all dietary needs.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.5.5.1.1">
<span class="ltx_p" id="S2.T1.1.5.5.1.1.1" style="width:128.0pt;">A Comprehensive Survey of Image-Based Food Recognition and Volume Estimation Methods for Dietary Assessment</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.5.5.2.1">
<span class="ltx_p" id="S2.T1.1.5.5.2.1.1" style="width:71.1pt;">Tahir, G., &amp; Loo, C.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.5.5.3.1">
<span class="ltx_p" id="S2.T1.1.5.5.3.1.1" style="width:128.0pt;">Analyzes challenges in food image recognition, particularly focusing on class imbalance. [13]</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.5.5.4.1">
<span class="ltx_p" id="S2.T1.1.5.5.4.1.1" style="width:71.1pt;">Insights into overcoming class imbalance; improves model robustness.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.5.5.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.5.5.5.1">
<span class="ltx_p" id="S2.T1.1.5.5.5.1.1" style="width:71.1pt;">Requires large, diverse datasets; complex model tuning.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.6.6.1.1">
<span class="ltx_p" id="S2.T1.1.6.6.1.1.1" style="width:128.0pt;">Development and User Evaluation of a Food-recognition app (FoodRec): Experimental Data and Qualitative Analysis</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.1.6.6.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.6.6.2.1">
<span class="ltx_p" id="S2.T1.1.6.6.2.1.1" style="width:71.1pt;">Battiato, S., &amp; Caponnetto, S., &amp; Leotta, R., et al.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.1.6.6.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.6.6.3.1">
<span class="ltx_p" id="S2.T1.1.6.6.3.1.1" style="width:128.0pt;">Surveys AI-powered food recognition systems, evaluating their impact on user health. [14]</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.1.6.6.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.6.6.4.1">
<span class="ltx_p" id="S2.T1.1.6.6.4.1.1" style="width:71.1pt;">Improves engagement through accurate food recognition; aids dietary management.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.1.6.6.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.6.6.5.1">
<span class="ltx_p" id="S2.T1.1.6.6.5.1.1" style="width:71.1pt;">Performance can be affected by lighting conditions and image quality; privacy concerns.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="398" id="S2.F1.g1" src="extracted/5799295/yolo.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>YOLO Model Demonstration [9]</figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Methodology</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Overview</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In this research, we set out to explore the potential of AI technologies in enhancing daily food tracking through detection, analysis, and recommendations. Thus, we developed NutrifyAI, a web application, as the medium for this research to highlights its practicality and usability in people’s daily lives. By offering a user-friendly interface, the app allows individuals to seamlessly integrate nutritional tracking into their routines. Users can simply capture an image of their meal, which the app processes to detect and identify the food items present.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Leveraging the power of the YOLOv8 computer vision model, NutrifyAI accurately recognizes various food items within the image. Once detected, the app retrieves detailed nutritional information for each identified food item using the Edamam API. This data is then aggregated to provide users with a comprehensive nutritional breakdown, including key metrics such as total calories, fat, protein, carbohydrate, and fiber content.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Beyond just tracking, NutrifyAI builds a history of the foods scanned by the user. Using this data, the app offers personalized meal recommendations tailored to the user’s nutritional goals.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="620" id="S3.F2.g1" src="extracted/5799295/login.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>End-to-end User Pipeline for NutrifyAI</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Data Collection and Preprocessing</span>
</h3>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS1.4.1.1">III-B</span>1 </span>Training Datasets</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">The training of our food detection model was based on datasets that had already been aggregated and preprocessed by the repository available at GitHub [7]. This repository integrates multiple datasets, that were publically accessible and available, to provide a comprehensive collection of food images for training. 5 datasets were used:</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Open Images V6-Food Dataset</h5>
<div class="ltx_para" id="S3.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px1.p1.1">We began by extracting relevant food-related images from the Open Images V6 dataset, a comprehensive dataset developed by Google for computer vision tasks. Our extraction focused on 18 specific food labels, resulting in a subset of over 20,000 images. Each image was associated with detailed annotations, including bounding boxes and labels corresponding to the food items present. This dataset served as the primary source of food images, becoming a foundation for the model’s initial training.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">School Lunch Dataset</h5>
<div class="ltx_para" id="S3.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p1.1">The next dataset integrated into our training data was the School Lunch Dataset, which includes 3,940 images of Japanese high school students’ lunches. Each image in this dataset is captured from a consistent frontal angle, with precise labels indicating the coordinates and types of dishes. The dataset categorizes dishes into 21 distinct classes, with an additional ”Other Foods” category for items that do not fit into the predefined classes. This dataset was valuable for introducing variations in food presentation and ensuring the model’s ability to handle real-world school lunch scenarios.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">Vietnamese Food Dataset</h5>
<div class="ltx_para" id="S3.SS2.SSS1.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px3.p1.1">To incorporate cultural diversity, we included a self-collected Vietnamese Food dataset. This dataset contains images of 10 traditional Vietnamese dishes, such as Pho, Com Tam, Hu Tieu, and Banh Mi. Each dish category comprises approximately 20-30 images, which were split 80-20 for training and evaluation. This dataset’s inclusion was essential for improving the model’s recognition capabilities across different cultural contexts.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px4">
<h5 class="ltx_title ltx_title_paragraph">MAFood-121 Dataset</h5>
<div class="ltx_para" id="S3.SS2.SSS1.Px4.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px4.p1.1">This dataset was included to introduce a broader range of global cuisines. This dataset contains 21,175 images of 121 different dishes, selected from the top 11 most popular cuisines worldwide based on Google Trends. The dishes are categorized into 10 food types, such as Bread, Eggs, Fried, Meat, Noodles, Rice, Seafood, Soup, Dumplings, and Vegetables. We used 85% of these images for training and the remaining 15% for evaluation.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px5">
<h5 class="ltx_title ltx_title_paragraph">Food-101 Dataset</h5>
<div class="ltx_para" id="S3.SS2.SSS1.Px5.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px5.p1.1">We also integrated the Food-101 dataset, which includes 101,000 images of 101 different dish types. For each dish, 750 images were designated for training and 250 for testing.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.Px5.p2">
<p class="ltx_p" id="S3.SS2.SSS1.Px5.p2.1">The final expanded dataset comprised 93,748 training images and 26,825 evaluation images, covering a total of 180 distinct dishes. This comprehensive dataset ensured that the model was exposed to a wide variety of food items, enhancing its classification performance.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="47" id="S3.F3.g1" src="extracted/5799295/gsheet2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Food tracking example in Google Sheets</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">Model Architecture</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The YOLOv8 model was trained using the aforementioned datasets, each labeled with food categories and bounding boxes. The dataset was split into 80% for training and 20% for validation. To enhance the model’s generalization ability, we applied data augmentation techniques such as rotations, scaling, and flips to simulate different viewing conditions.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">The training process utilized of Stochastic Gradient Descent with a momentum optimizer. We started with an initial learning rate of 0.01, which was adjusted throughout the training process using a learning rate scheduler to ensure steady improvement and prevent overfitting. The model was trained for 50 epochs, with early stopping used to halt training if the validation loss did not improve after 10 consecutive epochs.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">In addition to YOLOv8, we used EfficientNet-B4 for further classification. EfficientNet-B4 was fine-tuned on our dataset, starting with pre-trained weights from ImageNet. The fine-tuning process involved training for 30 epochs using the Adam optimizer with a learning rate of 0.001, with similar data augmentation techniques applied to improve the model’s robustness in recognizing diverse food items.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.4.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.5.2">Model Evaluation</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">The evaluation of our food detection model was conducted using the Food Recognition 2022 dataset [8], a comprehensive annotated dataset specifically designed for semantic segmentation tasks, which includes 43,962 images with 95,009 labeled objects belonging to 498 different classes. The GitHub repository [7] from which we obtained our training data tested five variations of the YOLO model (YOLOv5s, YOLOv5m, YOLOv5l, YOLOv5x, YOLOv8s) with results measured based on mAP at 0.5 IoU. The chart in Fig. 8 presents the mAP results for each YOLO variant, highlighting the superior performance of YOLOv8s with a mAP of 0.963. This high accuracy is why we chose YOLOv8s to power the NutrifyAI application.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="366" id="S3.F4.g1" src="extracted/5799295/chart.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparison of mAP results across YOLO model variations at 0.5 IoU [7]</figcaption>
</figure>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">To assess our model’s performance, we selected a subset of the Food Recognition 2022 dataset that intersected with the classes present in our training data. This resulted in a test set comprising 55 food categories. Given that the Food Recognition 2022 dataset contains 498 different classes, many of which were not present in our training data, a crucial step in the evaluation process was label mapping. This involved aligning the labels from the Food Recognition 2022 dataset with those used during training. For example, while our original training dataset had a single class labeled as ”wine,” the Food Recognition 2022 dataset included more specific classes such as ”rose-wine” and ”red-wine.” In cases like these, we mapped these specific classes back to the more general ”wine” label to ensure consistency in evaluation.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="345" id="S3.F5.g1" src="extracted/5799295/label.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Examples of label mappings used</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS5.4.1.1">III-E</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS5.5.2">API Integration</span>
</h3>
<section class="ltx_paragraph" id="S3.SS5.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Edamam Nutrient Analysis API</h5>
<div class="ltx_para" id="S3.SS5.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS5.SSS0.Px1.p1.1">This API was used to retrieve detailed nutritional information for the detected food items. After the YOLOv8 model identifies the food in an image, a request is sent to the Edamam API with the name of the detected food item. The API returns comprehensive nutritional data, including calories, fat, protein, fiber, and other key nutrients. In our user interface, the nutritional information retrived from the Edamam API is visualized using Chart.js. This allows users to quickly see the nutritional breakdown of their meals. This information is then stored in a Google Sheets database via the Google Sheets API for further processing and display.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="213" id="S3.F6.g1" src="extracted/5799295/nutrient.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Nutrient analysis chart on web-app interface</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS5.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Edamam Recipe and Meal Planning API</h5>
<div class="ltx_para" id="S3.SS5.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS5.SSS0.Px2.p1.1">To provide personalized meal recommendations based on the user’s nutritional goals, the Edamam Recipe and Meal Planning APIs were integrated into the application. These APIs allow NutrifyAI to suggest recipes and meal plans that align with the user’s dietary preferences, restrictions, and nutritional targets. Recommendations are generated based on the history of foods scanned by the user, offering tailored meal suggestions to help users meet their health objectives.</p>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="287" id="S3.F7.g1" src="extracted/5799295/recs.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Meal Recommendation Prompting for User</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS5.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Google Sheets API</h5>
<div class="ltx_para" id="S3.SS5.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS5.SSS0.Px3.p1.1">This API was used to store and manage the nutritional data and meal recommendations retrieved from the Edamam APIs. Each time a food item is detected and analyzed, the nutritional information is automatically logged into a Google Sheet. This approach ensures that all user data is securely stored and easily accessible for future reference and analysis.</p>
</div>
<figure class="ltx_figure" id="S3.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="151" id="S3.F8.g1" src="extracted/5799295/gsheet.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>User information storage in Google Sheets</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS5.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Server-Side Implementation</h5>
<div class="ltx_para" id="S3.SS5.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS5.SSS0.Px4.p1.1">The entire process is managed by a Flask server, which handles communication between the client-side web application, the YOLOv8 model, and the various APIs. Flask serves as the backbone of the server-side logic, managing requests, processing data, and returning results to the user. The Flask server is connected to a YOLOv8 model hosted on Google Colab via ngrok, which enables secure tunneling and facilitates real-time image processing.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS5.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Client-Server Implementation</h5>
<div class="ltx_para" id="S3.SS5.SSS0.Px5.p1">
<p class="ltx_p" id="S3.SS5.SSS0.Px5.p1.1">As illustrated in the attached image, the web client interacts with the server by sending images, videos, or URLs for food detection. Once the image is processed by the YOLOv8 model on the server, the detected food items are sent to the Edamam APIs for nutritional analysis and meal recommendations. The results are then returned to the web client, where they are displayed to the user through a user-friendly interface that utilizes Chart.js for visualizing nutritional data.</p>
</div>
<figure class="ltx_figure" id="S3.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="340" id="S3.F9.g1" src="extracted/5799295/pipeline.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>NutrifyAI’s Client-Server and Server-Side Interaction [7]</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Results</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">After evaluation on the Food Recognition 2022 dataset as previously mentioned, we noted that while the original authors of the GitHub repository conducted mAP testing on five YOLO model variations, achieving the highest mAP of 0.963 with YOLOv8s at 0.5 IoU, we did not specifically test our model on mAP for this dataset during the final testing phase. Instead, we assessed the model’s accuracy using metrics such as precision, recall, F1 score, and overall accuracy.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">The evaluation script was designed to compare the model’s predictions against ground truth labels on a per-image basis. For each image, the script counted true positives, false positives, and false negatives, which were then aggregated across all images to calculate the overall metrics.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Accuracy was determined by calculating the ratio of correctly predicted labels to the total number of ground truth labels across all images. The model achieved an accuracy of 75.4%, indicating that the majority of the food items were correctly identified.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">Precision measured the proportion of true positive predictions out of all positive predictions (both true and false positives). The model achieved an overall precision of 78.5%, reflecting its ability to minimize false positives.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">Recall was calculated as the proportion of true positives out of the total number of actual positives (true positives plus false negatives). The model demonstrated an overall recall of 72.8%, which indicates how well it detected relevant food items.</p>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1">F1 Score, the harmonic mean of precision and recall, was also calculated to provide a balanced measure of the model’s performance. The model achieved an F1 score of 75.5%.</p>
</div>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p" id="S4.p7.1">In addition to overall metrics, we also separated the results based on food classes to see which foods the model performed the best on. It was observed that the model performed exceptionally well on images containing distinct-looking food items, such as pomegranates and waffles, which were correctly identified with near 100% accuracy. However, the model struggled with more common or visually similar items like different types of apples and pears, likely due to class imbalance in the training data. The distribution for some sample classes are shown in Fig. 10 below.</p>
</div>
<figure class="ltx_figure" id="S4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="412" id="S4.F10.g1" src="extracted/5799295/result1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Precision, Recall and F1 Scores Grouped by Food Classes</figcaption>
</figure>
<div class="ltx_para" id="S4.p8">
<p class="ltx_p" id="S4.p8.1">We also evaluated the model on other aspects including detection confidence scores and detection speeds. As seen in Fig. 11, the histogram reveals a bimodal distribution, with peaks around 0.2 and 0.8. This indicates that the model often either had high confidence in its predictions or was relatively uncertain. In Fig. 12, the median time taken by the model to process each image and produce a result was approximately 1.5 seconds. This indicates that the model is efficient enough for real-time applications, ensuring a smooth user experience.</p>
</div>
<figure class="ltx_figure" id="S4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="529" id="S4.F11.g1" src="extracted/5799295/result2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>NutrifyAI’s Detection Confidence Score Histogram</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="505" id="S4.F12.g1" src="extracted/5799295/result3.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>NutrifyAI’s Detection Speed Histogram</figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Discussion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The primary objective of this study was to develop and evaluate a system that utilizes computer vision models and nutritional APIs for advanced food profiling and recommendations. The findings from our evaluation indicate that the system performs well in identifying food items and providing detailed nutritional analysis, supporting our original hypothesis that an AI-powered tool can streamline and enhance the process of dietary tracking.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">The data moderately supports our hypothesis. NutrifyAI achieved a fair accuracy of 75.4%, with precision, recall, and F1 scores that indicate an acceptable detection system. The successful integration of the Edamam Nutrient Analysis API and the real-time visualization of nutritional data through Chart.js further reinforce the effectiveness of our approach. The personalized meal recommendations generated by the Edamam Recipe and Meal Planning APIs were also well-received, demonstrating the system’s potential in helping users make informed dietary choices. However, the bimodal distribution of the confidence scores and the variability in the model’s performance across different food categories suggest areas where the system can be further refined.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">When comparing our findings to other research in the field of food recognition and dietary tracking, our results are in line with previous studies that have utilized deep learning for food detection. For instance, other studies employing YOLO models have reported similar accuracy and precision metrics, validating our choice of model and approach. However, most research utilize mean Average Precision (mAP) as a primary evaluation metric, which provides a more nuanced understanding of model performance across different thresholds. We did not measure mAP in this study, which could have provided additional insights into the model’s performance and comparison with other systems. Incorporating mAP as a metric in future evaluations would allow for a more comprehensive assessment of our model’s capabilities and would enable more direct comparisons with existing literature.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.4.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.5.2">Future Improvements</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">There are several ways we could improve the system’s overall performance. Firstly, incorporating mean Average Precision (mAP) can better gauge the model’s precision and recall across various confidence thresholds. This would provide a more detailed understanding of the model’s strengths and weaknesses.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">Furthermore, one of the main limitations identified was the class imbalance and limited diversity in the training dataset. Expanding the dataset to include a broader range of food categories, particularly those that are underrepresented, could improve the model’s generalization and recognition capabilities. A more diverse dataset would also help the model perform better on visually similar items, which currently present a challenge.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">We also noticed that the detection confidence score histogram revealed a significant number of predictions clustered around the 0.2 confidence level, indicating uncertainty in those predictions. To improve the model’s reliability, future work should focus on refining the confidence calibration, potentially through better training technique.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">Finally, introducing feedback mechanisms where users can correct or adjust predictions would allow the system to learn from real-world usage. This could lead to continuous improvement in accuracy and personalization, making the meal recommendation system more relevant and tailored to individual users.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In summary, NutrifyAI demonstrates the potential of integrating computer vision models with nutritional APIs to create an effective tool for food recognition and nutritional analysis. While the system achieved promising results, several areas remain for further exploration and enhancement. Future research should focus on expanding the diversity of training datasets to improve the model’s ability to recognize a wider array of food items. Additionally, refining the model’s confidence calibration and incorporating advanced evaluation metrics, such as mAP, will be crucial for achieving greater reliability. Developing mechanisms for user feedback will enable the system to adapt to real-world usage, offering more personalized and accurate meal recommendations. As the field of AI-powered nutrition continues to evolve, there is significant potential for this technology to be applied in various domains, including healthcare and personalized diet planning, making the pursuit of these improvements highly worthwhile.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This project would not have been possible without the support and guidance of Junyao Chen and Professor Zhengyuan Zhou from the NYU Stern School of Business, Department of Technology, Operations, and Statistics. Their mentorship and insights were crucial in shaping the direction of this research. Furthermore, special thanks to the Winston Data Foundation for their financial support through their merit scholarship for the NYU GSTEM Program.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"> A. Sharma, “Diet and Nutrition Apps Statistics: Exploring the Impact on Health,” Market.us Media, 28 Aug. 2023. [Online]. Available: https://media.market.us/diet-and-nutrition-apps-statistics/.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"> L. Young, “Featured Blogs,” ACSM CMS, 2024. [Online]. Available: https://www.acsm.org/blog-detail/fitness-index-blog/2024/06/12/eating-healthy-nutrition-apps.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"> A. Busad <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">et al.</span>, “Computer Vision Based Food Recognition with Nutrition Analysis,” <span class="ltx_text ltx_font_italic" id="bib.bib3.2.2">IJCRT</span>, vol. 11, no. 1, pp. 2320–882, 2023. [Online]. Available: https://www.ijcrt.org/papers/IJCRT2301042.pdf.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"> “You Only Look Once (YOLO): The Best Model for AI-Integrated Video Analytics?,” I3international.com, 2016. [Online]. Available: https://i3international.com/resources/media/yolo-the-best-model-for-ai-integrated-video-analytics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"> T. Diwan <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">et al.</span>, “Object Detection Using YOLO: Challenges, Architectural Successors, Datasets and Applications,” <span class="ltx_text ltx_font_italic" id="bib.bib5.2.2">Multimedia Tools and Applications</span>, vol. 82, pp. 9243–75, Aug. 2022. [Online]. Available: https://doi.org/10.1007/s11042-022-13644-y.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"> “YOLO Models for Object Detection Explained [Yolov8 Updated],” Encord.com, 4 Apr. 2024. [Online]. Available: https://encord.com/blog/yolo-object-detection-guide/.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"> H.-L. Nguyen, “Meal Analysis with Theseus,” GitHub, 21 Sept. 2022. [Online]. Available: https://github.com/lannguyen0910/food-recognition.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"> “Food Recognition 2022 - Dataset Ninja,” Dataset Ninja, 2022. [Online]. Available: https://datasetninja.com/food-recognition.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"> J. Redmon <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">et al.</span>, “You Only Look Once: Unified, Real-Time Object Detection,” 9 May 2016. [Online]. Available: https://arxiv.org/pdf/1506.02640v5.pdf.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"> K. Victor <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">et al.</span>, “Precision and Adaptability of YOLOv5 and YOLOv8 in Dynamic Robotic Environments,” Arxiv.org, 2023. [Online]. Available: https://arxiv.org/html/2406.00315v1.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"> M. S. Rodrigues <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">et al.</span>, “RecipeIS—Recipe Recommendation System Based on Recognition of Food Ingredients,” <span class="ltx_text ltx_font_italic" id="bib.bib11.2.2">Applied Sciences</span>, vol. 13, no. 13, p. 7880, Jan. 2023. [Online]. Available: https://doi.org/10.3390/app13137880.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"> C. Yaiprasert and A. N. Hidayanto, “AI-powered in the Digital Age: Ensemble Innovation Personalizes the Food Recommendations,” <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Journal of Open Innovation: Technology, Market, and Complexity</span>, vol. 10, no. 2, p. 100261, 2024. [Online]. Available: https://doi.org/10.1016/j.joitmc.2024.100261.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"> G. A. Tahir and C. K. Loo, “A Comprehensive Survey of Image-Based Food Recognition and Volume Estimation Methods for Dietary Assessment,” <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Healthcare</span>, vol. 9, no. 12, p. 1676, Dec. 2021. [Online]. Available: https://doi.org/10.3390/healthcare9121676.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"> S. Battiato <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">et al.</span>, “Development and User Evaluation of a Food-recognition App (FoodRec): Experimental Data and Qualitative Analysis,” <span class="ltx_text ltx_font_italic" id="bib.bib14.2.2">Health Psychology Research</span>, vol. 11, p. 70401, 21 Feb. 2023. [Online]. Available: https://doi.org/10.52965/001c.70401.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Aug 20 04:09:27 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
