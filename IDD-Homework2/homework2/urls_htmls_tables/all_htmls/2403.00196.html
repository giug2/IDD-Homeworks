<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.00196] Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras</title><meta property="og:description" content="Advanced Driver Assistance Systems (ADAS) in intelligent vehicles rely on accurate driver perception within the vehicle cabin, often leveraging a combination of sensing modalities. However, these modalities operate at …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.00196">

<!--Generated on Fri Apr  5 16:01:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
image synthesis,  generative artificial intelligence,  thermal imagery,  pseudo-labeled dataset,  data augmentation
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
Learning to Find Missing Video Frames with Synthetic Data Augmentation: 
<br class="ltx_break">A General Framework and Application in Generating Thermal Images Using RGB Cameras
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Mathias Viborg Andersen
</span><span class="ltx_author_notes">M. Andersen, R. Greer, and M. Trivedi are with the Laboratory for Intelligent and Safe Automobiles. A. Møgelmose is with the Visual Analysis and Perception Lab. 
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">Laboratory for Intelligent &amp; Safe Automobiles (LISA)
<br class="ltx_break"></span>University of California San Diego 
<br class="ltx_break">mvan19@student.aau.dk
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ross Greer



 <span id="id2.1.id1" class="ltx_text"></span><span id="id3.2.id2" class="ltx_text"></span> <span id="id4.3.id3" class="ltx_ERROR undefined">{@IEEEauthorhalign}</span>
           Andreas Møgelmose
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id5.4.id1" class="ltx_text ltx_font_italic">Laboratory for Intelligent &amp; Safe Automobiles (LISA)
<br class="ltx_break"></span>University of California San Diego 
<br class="ltx_break">regreer@ucsd.edu
</span>
<span class="ltx_contact ltx_role_affiliation">
           <span id="id6.5.id1" class="ltx_text ltx_font_italic">Visual Analysis and Perception Lab
<br class="ltx_break"></span>           Aalborg Universitet 
<br class="ltx_break">           anmo@create.aau.dk
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">           Mohan M. Trivedi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
           <span id="id7.1.id1" class="ltx_text ltx_font_italic">Laboratory for Intelligent &amp; Safe Automobiles (LISA)
<br class="ltx_break"></span>           University of California San Diego 
<br class="ltx_break">           mtrivedi@ucsd.edu
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">Advanced Driver Assistance Systems (ADAS) in intelligent vehicles rely on accurate driver perception within the vehicle cabin, often leveraging a combination of sensing modalities. However, these modalities operate at varying rates, posing challenges for real-time, comprehensive driver state monitoring. This paper addresses the issue of missing data due to sensor frame rate mismatches, introducing a generative model approach to create synthetic yet realistic thermal imagery. We propose using conditional generative adversarial networks (cGANs), specifically comparing the pix2pix and CycleGAN architectures. Experimental results demonstrate that pix2pix outperforms CycleGAN, and utilizing multi-view input styles, especially stacked views, enhances the accuracy of thermal image generation. Moreover, the study evaluates the model’s generalizability across different subjects, revealing the importance of individualized training for optimal performance. The findings suggest the potential of generative models in addressing missing frames, advancing driver state monitoring for intelligent vehicles, and underscoring the need for continued research in model generalization and customization.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
image synthesis, generative artificial intelligence, thermal imagery, pseudo-labeled dataset, data augmentation

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2403.00196/assets/media/state_problem-min.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="586" height="155" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Many perspectives and modalities of data may contribute to robust driver state monitoring. Differing frame rates of sensors lead to an unavailability of “complete” sets of data from all modalities for a given instance. Because many driver states are best inferred from temporal patterns, an ideal data stream would have constant availability of all sources at each instance. Without such a stream, models may be limited to instance inference (blue), complete-but-temporally-distant sequences (red), or incomplete-but-temporally-local sequences (yellow). By generating missing data, we can provide synthetic but useful representations to fill in these gray gaps, enabling accurate downstream state estimation models using pseudo-complete, temporally-local sequences.</span></figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Many advanced driver assistance systems (ADAS) rely on accurate perception of the human driver by looking inside the intelligent vehicle cabin <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. No single-view or sensing modality perfectly suits every task or design specification, so often a combination of views can be leveraged for enhanced driver state understanding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. However, different sensing modalities operate at different rates and often perform similarly to RGB using neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. For example, thermal cameras often operate at a fraction of the rate of cameras on the visible light spectrum, but are useful toward a variety of tasks in driver state monitoring by providing useful information for understanding occlusion, circulation, respiration, and other heat-related observations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In many cases, it is useful for models to make observations based on a sequence of observations, rather than a single instance in time; doing so allows the modeling of dynamic events and driver actions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, as well as inference reinforcement by repeated agreement between information observed at nearby times <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Considering again the multi-modal nature of driver observation systems, misaligned sensor rates are not a problem when data from each modality is considered individually, but requires careful synchronization and creative approaches to modeling in cases when data from one sensor may not be provided at the ideal rate for best utilizing the remaining sensors. We illustrate this problem, and associated problems and trade-offs, in Figure <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>; essentially, one can either use a high-frequency model with intermittent missing data from low-frequency sensors, or a low-frequency model which may sacrifice temporal precision of inference. The risk of using low-frequency sensors is the mismatch that occurs when the driver takes an action which changes state in a way that can be observed by one sensor but missed by another; this can create conflicting information and multimodal model confusion if not handled properly, illustrated in Figure <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S1.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2403.00196/assets/nearwheel.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="123" height="125" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2403.00196/assets/handsaway.png" id="S1.F2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="144" height="125" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.3.2" class="ltx_text" style="font-size:90%;">When sensors operate at different rates, it is possible that the temporally-nearest measurement to a given instance may have taken place before a significant action for one sensor, and after the action for another. In the above example, the driver has abruptly moved his hands closer to the wheel; however, the thermal camera has not yet processed another signal to capture this motion. So, if both “most recent” signals are sent to a multimodal model meant to estimate a driver’s takeover readiness (e.g. proximity of hands to the steering wheel), the model would have a large amount of uncertainty from modal disagreement.</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this research, we propose a solution using a generative model to create pseudo-complete data samples, pairing both real (low frequency) thermal imagery and generated samples with high-frequency visible light images captured from multiple perspectives.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Research</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Synthetic Thermal Images for Data Augmentation</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Various modes of image data, from RAW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> to RGB to IR, and even to non-light-based imagery such as temperature, provide utility in a variety of applications; further, cross-modality image synthesis is useful as a data augmentation strategy in a variety of tasks, such as terrain classification (visible to IR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, tissue segmentation (MRI to CT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, and heart observation (various CMR medical imaging techniques) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Specific to human interaction, Hermosilla et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> generate thermal facial images from noise using StyleGAN2, in order to build a robust dataset with tunable features that can be used to train deep learning models to detect faces within thermal images, a task which is generally more difficult than detection from visible light images but may be useful in certain sensing environments. Our research similarly utilizes GAN as the underlying generative learning paradigm, with the common goal of augmenting datasets towards problems in human subject feature extraction and understanding.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Translating between Thermal and Visible Light</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Deep learning has revolutionized image processing and opened up new possibilities for image generation. Deep learning models, particularly generative adversarial networks (GANs), have demonstrated remarkable capabilities in translating images between different domains. Relevant to our domains of interest (visible light and thermal signature), Abdrakhmanova et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> create the SpeakingFaces dataset and explore models to generate visible images from thermal images, since facial features for landmark recognition and detection are too obscure in thermal imagery, reducing possible use cases in HCI, biometric authentication, and other systems. They use CycleGAN and CUT models to map thermal face images to the visual spectrum, evaluating both the generated Fréchet inception distance as well as the ability of the generated models to produce the correct output on downstream facial landmark detection tasks.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> introduced the dual-attention generative adversarial network (DAGAN) for the generation of thermal images from visible light, but outside the realm of human subjects and in the domain of fire safety, useful in estimating locations and temperatures of flames in room fires. Though the temperature range, contrast, and locality on human subjects is significantly different from that of an open flame, their research provides a strong indication that GAN architectures are suited to the task.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Predicting Missing Frames</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">For real-world data, the expected translational motion of objects between frames has allowed for missing individual frames to be estimated using a Kalman filtering approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, and for larger segments of up to 14 frames to be created from a fully convolutional model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. However, such methods which effectively “in-paint” or “in-between” video sequences are ineffective toward replacing larger missing sequences (in this case, approximately 1 frame available in every 5-frame period). Further exacerbating this issue, the frame rate is slow enough that basic human motion changes state at a scale faster than the sensor records– meaning that even knowing the surrounding frames may be insufficient to fill in the activity of the frames in between. Fortunately, in this problem setting, we have at our disposal additional information from the “missing” times, rather than just its surrounding pieces.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Methods</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Synthetic Data Augmentation</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In the case presented in this paper, we consider a downstream goal task of driver state estimation, and we have a set of thermal and visible light images which are used to supervise the training of this task. However, we also have a set of visible light images with no matching thermal imagery; by training an intermediate model which generates associated thermal images, and then using these generated images in the training of a driver state estimation model, we can augment the dataset to enable the use of models which operate over complete sets of sensor data at a high frequency.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Algorithm</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We train and evaluate two conditional generative adversarial network (cGAN) architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>; the pix2pix architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and the CycleGAN architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. These models consist of two competing neural networks: a generator that produces thermal images from RGB inputs and a discriminator that distinguishes between real thermal images and generated ones. Through adversarial training, the generator learns to synthesize realistic thermal images that are indistinguishable from real ones. The cGAN architecture of pix2pix is seen in Figure <a href="#S3.F3" title="Figure 3 ‣ III-B Algorithm ‣ III Methods ‣ Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, and a demonstration of its iterative learning is shown in Figure <a href="#S3.F6" title="Figure 6 ‣ III-C Dataset ‣ III Methods ‣ Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2403.00196/assets/media/pix2pixExample.jpg" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="293" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">The flow of pix2pix applied in this work.</span></figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Our output for all methods is a thermal image, like the example shown in Figure <a href="#S3.F5" title="Figure 5 ‣ III-C Dataset ‣ III Methods ‣ Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>a. We evaluate four types of input for experimental comparison:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Front-View RGB (Figure <a href="#S3.F5" title="Figure 5 ‣ III-C Dataset ‣ III Methods ‣ Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>b)</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Single-Subject Four-View RGB, tesselated (Figure <a href="#S3.F5" title="Figure 5 ‣ III-C Dataset ‣ III Methods ‣ Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>c)</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Single-Subject Four-View RGB, stacked (Figure <a href="#S3.F5" title="Figure 5 ‣ III-C Dataset ‣ III Methods ‣ Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>d)</p>
</div>
</li>
</ol>
<p id="S3.SS2.p2.2" class="ltx_p">Though these input styles may appear unusual, their compactness allows for training on a single GPU system, and on an efficient model architecture, as opposed to creating three additional convolutional “heads” to extract features from the individual images. Though there are some false edges introduced due to the stacking structures, our results show that these collage images are still able to significantly outperform single-view learning. Additionally, we create one further experiment to evaluate for the effects of single-subject training versus multi-subject training for the Front View.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Dataset</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The dataset utilized in this study is notably comprehensive, incorporating various perspectives beneficial for developing and evaluating innovative approaches to generating thermal images from RGB inputs as well as driver state monitoring in general. It consists of distinct viewpoints, including <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">thermal</span>, <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_italic">front</span>, <span id="S3.SS3.p1.1.3" class="ltx_text ltx_font_italic">overhead</span>, <span id="S3.SS3.p1.1.4" class="ltx_text ltx_font_italic">profile</span>, and <span id="S3.SS3.p1.1.5" class="ltx_text ltx_font_italic">tablet</span> orientations, as displayed in Figure <a href="#S3.F4" title="Figure 4 ‣ III-C Dataset ‣ III Methods ‣ Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2403.00196/assets/media/00000_13_named_small.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">Example images showcasing perspectives captured from used cameras within our simulator setup. </span></figcaption>
</figure>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The dataset consists of captures of 17 subjects seated at a simulated driver’s seat. Notably, the RGB images are captured at a rate of approximately 30 frames per second (fps), while the thermal imaging data is sourced from a thermal camera operating at less than 9 fps, and represent a range of -20°C to 300°C, scaled to [0, 255]. Thorough synchronization and preparation procedures have been applied to ensure the optimal integration of the multi-view data.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">From each subject we create a collection of 500 thermal + RGB image synchronous groups. In each experiment, an allocation of 80 % for training, 10 % for validation, and 10 % for testing has been used. In the case of training on the aggregate pool of all subjects, we randomly select 5,000 of the 8,500 samples to account for our available training hardware.</p>
</div>
<figure id="S3.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F5.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:195.1pt;"><img src="/html/2403.00196/assets/media/00000_13_thermal.png" id="S3.F5.1.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(a) Thermal ground truth.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F5.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:195.1pt;"><img src="/html/2403.00196/assets/media/00000_13_front_single.png" id="S3.F5.2.g1" class="ltx_graphics ltx_img_square" width="598" height="601" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(b) Front-View.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F5.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:195.1pt;"><img src="/html/2403.00196/assets/media/00000_13_check_single.png" id="S3.F5.3.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(c) Four-View, Tessellated.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F5.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:195.1pt;"><img src="/html/2403.00196/assets/media/00000_13_sidebyside_single.png" id="S3.F5.4.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(d) Four-View, Stacked.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.6.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.7.2" class="ltx_text" style="font-size:90%;">In our experiments, different inputs are evaluated on their potential for generating an image similar to the thermal ground truth.</span></figcaption>
</figure>
<figure id="S3.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2403.00196/assets/x1.jpg" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="325" height="113" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2403.00196/assets/x2.jpg" id="S3.F6.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="325" height="103" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2403.00196/assets/x3.jpg" id="S3.F6.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="325" height="103" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2403.00196/assets/x4.jpg" id="S3.F6.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="325" height="103" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2403.00196/assets/x5.jpg" id="S3.F6.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="325" height="103" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2403.00196/assets/x6.jpg" id="S3.F6.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="325" height="103" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2403.00196/assets/x7.jpg" id="S3.F6.g7" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="325" height="103" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.3.2" class="ltx_text" style="font-size:90%;">From top to bottom, we show the generator output at different iterations of training. Originally, the generator produces a random image, and refines its output to match the intended thermal image over time. These images are separated by only 10 iterations each, except for the final image which represents a jump to 20,000 iterations.</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experimental Evaluation</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Generative Architecture</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We first compare the performance of the pix2pix architecture versus the CycleGAN architecture, which has proven useful in past data augmentation for driver state monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. As shown in Table <a href="#S4.T1" title="TABLE I ‣ IV-A Generative Architecture ‣ IV Experimental Evaluation ‣ Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, the performance of the CycleGAN is significantly subpar to pix2pix. We expect that this is due to the attempt by CycleGAN to reconstruct the original input from its generated output during training; because the thermal image is significantly lossy compared to the amount of information in the visible light images, this relationship is more difficult to model beyond approximation. For interpretation, we note that all experimental error values are on normalized pixel values in the range [0, 1], meaning that most errors range around 5-6% of the pixel range.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.2.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S4.T1.3.2" class="ltx_text" style="font-size:90%;">Comparison of Model Architectures when training front-view.</span></figcaption>
<table id="S4.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.4.1.1" class="ltx_tr">
<th id="S4.T1.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Method</th>
<th id="S4.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Average Test L1 Error</th>
<th id="S4.T1.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Standard Deviation</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.4.2.1" class="ltx_tr">
<td id="S4.T1.4.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CycleGAN</td>
<td id="S4.T1.4.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.2179</td>
<td id="S4.T1.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.0633</td>
</tr>
<tr id="S4.T1.4.3.2" class="ltx_tr">
<td id="S4.T1.4.3.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.4.3.2.1.1" class="ltx_text ltx_font_bold">pix2pix</span></td>
<td id="S4.T1.4.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.4.3.2.2.1" class="ltx_text ltx_font_bold">0.0676</span></td>
<td id="S4.T1.4.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.3.2.3.1" class="ltx_text ltx_font_bold">0.0106</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Input Style</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The results of our experiments comparing the three different styles of input are presented in Table <a href="#S4.T2" title="TABLE II ‣ IV-B Input Style ‣ IV Experimental Evaluation ‣ Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. We find that a combination of views outperforms generation from a single-view, Figure <a href="#S5.F9" title="Figure 9 ‣ V Concluding Remarks ‣ Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, (perhaps assisting in understanding hand and posture positioning and respective heat signatures), and that the stacked-view and tesselated-view, Figures <a href="#S5.F7" title="Figure 7 ‣ V Concluding Remarks ‣ Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>&amp;<a href="#S5.F10" title="Figure 10 ‣ V Concluding Remarks ‣ Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, of the images provides an efficient and effective input, evidenced by the lowest average L1 error (0.0559) and a comparatively low standard deviation (0.0093). This suggests that considering spatial relationships by multi-view information enhances the model’s accuracy in thermal image generation, as seen in Figure <a href="#S5.F7" title="Figure 7 ‣ V Concluding Remarks ‣ Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.2.1.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S4.T2.3.2" class="ltx_text" style="font-size:90%;">Comparison of Input Style; Average Performance Across 17 Subjects.</span></figcaption>
<table id="S4.T2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.4.1.1" class="ltx_tr">
<th id="S4.T2.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Dataset</th>
<th id="S4.T2.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Average Test L1 Error</th>
<th id="S4.T2.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Standard Deviation</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.4.2.1" class="ltx_tr">
<td id="S4.T2.4.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Front-View</td>
<td id="S4.T2.4.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.0676</td>
<td id="S4.T2.4.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">0.0106</td>
</tr>
<tr id="S4.T2.4.3.2" class="ltx_tr">
<td id="S4.T2.4.3.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Four-View, Tessellated</td>
<td id="S4.T2.4.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0587</td>
<td id="S4.T2.4.3.2.3" class="ltx_td ltx_align_center ltx_border_t">0.0109</td>
</tr>
<tr id="S4.T2.4.4.3" class="ltx_tr">
<td id="S4.T2.4.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.4.4.3.1.1" class="ltx_text ltx_font_bold">Four-View, Stacked</span></td>
<td id="S4.T2.4.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.4.4.3.2.1" class="ltx_text ltx_font_bold">0.0559</span></td>
<td id="S4.T2.4.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.4.4.3.3.1" class="ltx_text ltx_font_bold">0.0093</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Subject Generalizability</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The results of our experiments on model generalizability to multi-subject training data is presented in Table <a href="#S4.T3" title="TABLE III ‣ IV-C Subject Generalizability ‣ IV Experimental Evaluation ‣ Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. We find that though the training dataset size grows significantly (17<math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mo id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><times id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\times</annotation></semantics></math>) and still includes the original training data, the additional subjects seem to contribute more to model confusion than generalized pattern creation, showing a higher average L1 error of 0.1116 and supporting the idea that these models are best trained on an individual basis. The relatively weak performance when trained on the more diverse set of data can be observed in Figure <a href="#S5.F8" title="Figure 8 ‣ V Concluding Remarks ‣ Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.2.1.1" class="ltx_text" style="font-size:90%;">TABLE III</span>: </span><span id="S4.T3.3.2" class="ltx_text" style="font-size:90%;">Comparison of Single vs. Multi-Subject Training on Front View.</span></figcaption>
<table id="S4.T3.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.4.1.1" class="ltx_tr">
<th id="S4.T3.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Dataset</th>
<th id="S4.T3.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Average Test L1 Error</th>
<th id="S4.T3.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Std. Deviation</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.4.2.1" class="ltx_tr">
<td id="S4.T3.4.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T3.4.2.1.1.1" class="ltx_text ltx_font_bold">Single-Subject Training</span></td>
<td id="S4.T3.4.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T3.4.2.1.2.1" class="ltx_text ltx_font_bold">0.0676</span></td>
<td id="S4.T3.4.2.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.4.2.1.3.1" class="ltx_text ltx_font_bold">0.0106</span></td>
</tr>
<tr id="S4.T3.4.3.2" class="ltx_tr">
<td id="S4.T3.4.3.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Multi-Subject Training</td>
<td id="S4.T3.4.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.1116</td>
<td id="S4.T3.4.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.0186</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Concluding Remarks</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Our study on generating thermal images from RGB counterparts highlights promising results with effective prediction approximation. However, the persisting challenge of the missing frames issue underscores the complexity of the task, demanding further research, as predictions must be near perfect. The stacked generation approach proved most successful, emphasizing the importance of spatial relationships. Despite these advancements, the model’s generalization between subjects remains the worst performer, warranting continued efforts for improved adaptability across diverse scenarios so that singular models may be trained and deployed, and motivating research into the potential of small-data fine-tuning for model customization to individual drivers. The value of observing time-varying patterns is beneficial to many autonomous driving applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, providing a means to infer useful, high-frequency cues from temporal dynamics of the observed subject.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">In summary, our generative approach shows potential to address the missing frames problem (caused by sensor frame rate mismatches and intermittent failures), providing a means for higher frequency driver state monitoring for enhanced intelligent vehicle awareness and rapid, safe decision-making.</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2403.00196/assets/media/results/result_sidebyside.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S5.F7.3.2" class="ltx_text" style="font-size:90%;">Single-Subject Four-View, Stacked Prediction Example.</span></figcaption>
</figure>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2403.00196/assets/media/results/result_front_combined.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="218" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S5.F8.3.2" class="ltx_text" style="font-size:90%;">Multi-Subject Front-View Prediction Example.</span></figcaption>
</figure>
<figure id="S5.F9" class="ltx_figure"><img src="/html/2403.00196/assets/media/results/result_front.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="213" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S5.F9.3.2" class="ltx_text" style="font-size:90%;">Single-Subject Front-View Prediction Example.</span></figcaption>
</figure>
<figure id="S5.F10" class="ltx_figure"><img src="/html/2403.00196/assets/media/results/result_check.png" id="S5.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="214" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S5.F10.3.2" class="ltx_text" style="font-size:90%;">Four-View, Tessellated Prediction Example.</span></figcaption>
</figure>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The authors would like to thank Sumega Mandadi for her assistance in the synchronization and management of the experimental dataset.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S. Vora, A. Rangesh, and M. M. Trivedi, “Driver gaze zone estimation using convolutional neural networks: A general framework and ablative analysis,” <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Vehicles</span>, vol. 3, no. 3, pp. 254–265, 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
E. Ohn-Bar, A. Tawari, S. Martin, and M. M. Trivedi, “On surveillance for safety critical events: In-vehicle video networks for predictive driver assistance systems,” <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Computer Vision and Image Understanding</span>, vol. 134, pp. 130–140, 2015.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
R. Greer, L. Rakla, A. Gopalkrishnan, and M. Trivedi, “Multi-view ensemble learning with missing data: Computational framework and evaluations using novel data from the safe autonomous driving domain,” <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2301.12592</span>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
C. Kantas, B. Antoniussen, M. V. Andersen, R. Munksø, S. Kotnala, S. B. Jensen, A. Møgelmose, L. Nørgaard, and T. B. Moeslund, “Raw instinct: Trust your classifiers and skip the conversion,” in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">2023 IEEE 6th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)</span>, pp. 456–460, IEEE, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
S. Kajiwara, “Driver-condition detection using a thermal imaging camera and neural networks,” <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">International journal of automotive technology</span>, vol. 22, pp. 1505–1515, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. Bole, C. Fournier, C. Lavergne, G. Druart, and T. Lépine, “Driver head pose tracking with thermal camera,” in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Infrared Sensors, Devices, and Applications VI</span>, vol. 9974, pp. 158–167, SPIE, 2016.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
V. Mattioli, L. Davoli, L. Belli, G. Ferrari, and R. Raheli, “Thermal camera-based driver monitoring in the automotive scenario,” in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">2023 AEIT International Conference on Electrical and Electronic Technologies for Automotive (AEIT AUTOMOTIVE)</span>, pp. 1–6, IEEE, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S. E. H. Kiashari, A. Nahvi, A. Homayounfard, and H. Bakhoda, “Monitoring the variation in driver respiration rate from wakefulness to drowsiness: a non-intrusive method for drowsiness detection using thermal imaging,” <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Journal of Sleep Sciences</span>, vol. 3, no. 1-2, pp. 1–9, 2018.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
C. Weiss, A. Kirmas, S. Lemcke, S. Böshagen, M. Walter, L. Eckstein, and S. Leonhardt, “Head tracking in automotive environments for driver monitoring using a low resolution thermal camera,” <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Vehicles</span>, vol. 4, no. 1, pp. 219–233, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
C. Palmero, A. Clapés, C. Bahnsen, A. Møgelmose, T. B. Moeslund, and S. Escalera, “Multi-modal rgb–depth–thermal human body segmentation,” <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, vol. 118, pp. 217–239, 2016.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. Rangesh, N. Deo, R. Greer, P. Gunaratne, and M. M. Trivedi, “Autonomous vehicles that alert humans to take-over controls: Modeling with real-world data,” in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">2021 IEEE International Intelligent Transportation Systems Conference (ITSC)</span>, pp. 231–236, IEEE, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. Rangesh, N. Deo, R. Greer, P. Gunaratne, and M. M. Trivedi, “Predicting take-over time for autonomous driving with real-world data: Robust data augmentation, models, and evaluation,” <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2107.12932</span>, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
R. Greer, N. Deo, A. Rangesh, P. Gunaratne, and M. Trivedi, “Safe control transitions: Machine vision based observable readiness index and data-driven takeover time prediction,” <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2301.05805</span>, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
R. Greer, L. Rakla, A. Gopalan, and M. Trivedi, “(safe) smart hands: Hand activity analysis and distraction alerts using a multi-camera framework,” <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2301.05838</span>, 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Y. Iwashita, K. Nakashima, S. Rafol, A. Stoica, and R. Kurazume, “Mu-net: Deep learning-based thermal ir image estimation from rgb image,” in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</span>, pp. 0–0, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
X. Chen, C. Lian, L. Wang, H. Deng, T. Kuang, S. H. Fung, J. Gateno, D. Shen, J. J. Xia, and P.-T. Yap, “Diverse data augmentation for learning image segmentation with cross-modality annotations,” <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Medical image analysis</span>, vol. 71, p. 102060, 2021.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
W. Wang, X. Yu, B. Fang, D.-Y. Zhao, Y. Chen, W. Wei, and J. Chen, “Cross-modality lge-cmr segmentation using image-to-image translation based data augmentation,” <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">IEEE/ACM Transactions on Computational Biology and Bioinformatics</span>, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
G. Hermosilla, D.-I. H. Tapia, H. Allende-Cid, G. F. Castro, and E. Vera, “Thermal face generation using stylegan,” <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 9, pp. 80511–80523, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
M. Abdrakhmanova, A. Kuzdeuov, S. Jarju, Y. Khassanov, M. Lewis, and H. A. Varol, “Speakingfaces: A large-scale multimodal dataset of voice commands with visual and thermal video streams,” <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Sensors</span>, vol. 21, no. 10, p. 3465, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Y. Li, Y. Ko, and W. Lee, “A feasibility study on translation of rgb images to thermal images: Development of a machine learning algorithm,” <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">SN Computer Science</span>, vol. 4, no. 5, p. 555, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
M. Chaubey, L. K. Singh, and M. Gupta, “Estimation of missing video frames using kalman filter,” <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Multimedia Tools and Applications</span>, pp. 1–21, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Y. Li, D. Roblek, and M. Tagliasacchi, “From here to there: Video inbetweening using direct 3d convolutions,” <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1905.10240</span>, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
M. Mirza and S. Osindero, “Conditional generative adversarial nets,” <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1411.1784</span>, 2014.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation with conditional adversarial networks,” 2018.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image translation using cycle-consistent adversarial networks,” in <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer vision</span>, pp. 2223–2232, 2017.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Rangesh, B. Zhang, and M. M. Trivedi, “Gaze preserving cyclegans for eyeglass removal and persistent gaze estimation,” <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Vehicles</span>, vol. 7, no. 2, pp. 377–386, 2022.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
R. Greer, A. Gopalkrishnan, M. Keskar, and M. M. Trivedi, “Patterns of vehicle lights: Addressing complexities of camera-based vehicle light datasets and metrics,” <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Pattern Recognition Letters</span>, 2024.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
A. Doshi and M. M. Trivedi, “Examining the impact of driving style on the predictability and responsiveness of the driver: Real-world and simulator analysis,” in <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">2010 IEEE Intelligent Vehicles Symposium</span>, pp. 232–237, IEEE, 2010.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A. Tawari and M. M. Trivedi, “Robust and continuous estimation of driver gaze zone by dynamic analysis of multiple face videos,” in <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">2014 IEEE Intelligent Vehicles Symposium Proceedings</span>, pp. 344–349, IEEE, 2014.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.00195" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.00196" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.00196">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.00196" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.00197" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 16:01:55 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
