<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2007.08781] Mixing Real and Synthetic Data to Enhance Neural Network Training - A Review of Current Approaches</title><meta property="og:description" content="Deep neural networks have gained tremendous importance in many computer vision tasks.
However, their power comes at the cost of large amounts of annotated data required for supervised training.
In this work we review a…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Mixing Real and Synthetic Data to Enhance Neural Network Training - A Review of Current Approaches">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Mixing Real and Synthetic Data to Enhance Neural Network Training - A Review of Current Approaches">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2007.08781">

<!--Generated on Mon Mar 18 17:56:35 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="C">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<figure id="id2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2007.08781/assets/images/motec.png" id="id1.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="210" height="79" alt="[Uncaptioned image]"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2007.08781/assets/images/ametek.png" id="id2.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="210" height="69" alt="[Uncaptioned image]"></div>
</div>
</figure>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p ltx_align_center"><span id="p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Mixing Real and Synthetic Data to Enhance Neural Network Training - A Review of Current Approaches</span></p>
</div>
<figure id="id3" class="ltx_figure"><img src="/html/2007.08781/assets/images/BMWI.png" id="id3.g1" class="ltx_graphics ltx_img_square" width="180" height="177" alt="[Uncaptioned image]">
</figure>
<div id="p2" class="ltx_para ltx_noindent">
<p id="p2.1" class="ltx_p">Das diesem Bericht zugrundeliegende Vorhaben wurde mit Mitteln des Bundesministeriums für Wirtschaft und Energie unter dem Förderkennzeichen 19A16015D gefördert. Die Verantwortung für den Inhalt dieser Veröffentlichung liegt beim Autor/bei der Autorin.</p>
</div>
<div id="p3" class="ltx_para ltx_noindent">
<p id="p3.1" class="ltx_p">This work has been funded by the German Ministry for Economic Affairs and Energy (reference number: 19A16015D).</p>
</div>
<span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Motec GmbH, Oberweyerer Straße 21, 65589 Hadamar-Steinbach, Germany 
<br class="ltx_break"><span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{viktor.seib, benjamin.lange, stefan.wirtz}@ametek.com</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Mixing Real and Synthetic Data to Enhance Neural Network Training - A Review of Current Approaches</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Viktor Seib
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Benjamin Lange
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Stefan Wirtz
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Deep neural networks have gained tremendous importance in many computer vision tasks.
However, their power comes at the cost of large amounts of annotated data required for supervised training.
In this work we review and compare different techniques available in the literature to improve training results without acquiring additional annotated real-world data.
This goal is mostly achieved by applying annotation-preserving transformations to existing data or by synthetically creating more data.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>C
</div>
<div id="p4" class="ltx_para">
<p id="p4.1" class="ltx_p">onvolutional Neural Networks, Transfer Learning, Data Augmentation, Synthetic Data, Review</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Many tasks in object classification and detection can nowadays be performed by computers with a human level accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.
These advances in computer vision are achieved by creating large annotated datasets for the targeted application domain.
However, not all domains allow the collection of large amounts of data.
This is the case if some rare events need to be detected with high confidence.
Another example are privacy issues that impede data collection for health care applications.
Indeed, recent research indicates that the current limitations to the performance of convolutional neural networks is not the network architecture, but the limited amount of available data for training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
Due to the enormous work needed to annotate data, nowadays large-scale datasets exist only for a limited range of applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.
Consequently, researchers are interested in methods to create powerful networks with less training data – or at least with less real-world data.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The motivation to use less real-world data is, on one hand, the expensive acquisition of the data and, on the other hand, the even more expensive annotation.
In contrast, synthetic data can be generated in arbitrary amounts once a suitable framework is properly setup.
Even better: the annotations are generated along with the data itself at (almost) no additional cost.
Depending on the application domain and size of the artificial dataset it can be used as a large dataset for transfer learning or even to train a neural network from scratch.
There are also some downside to consider.
Neural networks learn a latent feature distribution of the presented training data.
Thus, the resulting network will exhibit a poor performance if the synthetic data does not properly reflect the feature distribution of the data in the target domain.
This performance gap is referred to as domain shift and is a common problem when training with synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work we focus on the domain of urban and traffic scenes.
The goal is to review convolutional neural network training scenarios that improve the network’s performance without additional real-world data.
Please note that this review is not exhaustive.
There is a lot of research currently done in this field and not all approaches, or even groups of approaches, are equally well treated in this review.
Nevertheless, we hope that the exemplary approaches compiled in this review will provide further insights and open new directions for investigations.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We start off by reviewing successful methods for data augmentation and some common approaches in transfer learning in Section <a href="#S2" title="2 Little Data - Smartly Used ‣ Mixing Real and Synthetic Data to Enhance Neural Network Training - A Review of Current Approaches" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
This includes data augmentation methods that were used in prevalent and successful neural networks.
We also discuss different options typically used in transfer learning and fine-tuning and provide a short note on the feedforward design to calculate network weights.
Section <a href="#S3" title="3 Synthetic Data ‣ Mixing Real and Synthetic Data to Enhance Neural Network Training - A Review of Current Approaches" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> is the main part of this work.
Therein we present synthetic urban scene datasets that are currently applied in research.
We discuss the training procedures and experiments performed by the authors of the respective datasets.
Additionally, a short section is dedicated to some examples to diminish the effects of domain shift.
There is one type of synthetic datasets that nowadays is still not available to the research community: synthetic GAN-generated, photo-realistic images of urban scenes.
In Section <a href="#S4" title="4 Outlook on Synthetic Data with GANs ‣ Mixing Real and Synthetic Data to Enhance Neural Network Training - A Review of Current Approaches" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> we review some work into that direction and briefly state why such datasets might be expected in near future.
Finally, Section <a href="#S5" title="5 Summary ‣ Mixing Real and Synthetic Data to Enhance Neural Network Training - A Review of Current Approaches" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> concludes this paper and summarizes some messages learnt from the reviewed approaches.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Little Data - Smartly Used</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This section introduces common approaches to reuse trained networks available online for other purposes.
Further, we review methods to artificially increase the amount of data available for training without collecting or annotating additional images.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Pretrained networks</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Pretrained network weights for all commonly known network architectures and deep learning frameworks are available online for free.
These networks are mostly pretrained on large datasets such as ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, Pascal VOC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> or COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
The size of these datasets allows the neural networks to learn abstract representations for many different object classes and embed generalized feature representations.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">When adapting a neural network to a custom use case, the size of the use case-specific dataset often will be smaller than the size of the dataset used for pretraining.
Therefore, for a customized application based on neural networks one should first check which available network architecture best meets the requirements regarding accuracy and available computational resources.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">In most cases it is advisable to take an existing, pretrained network as the basis for a new custom application.
Surprisingly, this is even the case when the data type and application domain of the target network significantly differ from the original pretrained network.
For instance Schwarz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and Eitel et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> use a network pretrained on ImageNet with RGB images to train a network on range image data.
Another example presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> shows the successful application of a network pretrained on ImageNet for skin cancer classification.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">Starting with a pretrained network is the simplest method to reduce the amount of necessary data for training.
Once a pretrained network is chosen, commonly transfer learning and fine-tuning are applied to adapt the network to a specific target domain.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Transfer Learning and Fine-Tuning</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">While the terms are often used interchangeably, transfer learning and fine-tuning are different techniques that are often applied simultaneously.
Both methods are used to transfer the trained weights from one application domain to another <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.
For example, a network trained for autonomous driving in urban scenes can be applied for autonomous navigation in a harbor environment.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The last layer of a network is responsible for the trained task, for example classification or detection.
The last layer also determines the total number of classes the network can distinguish.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">In transfer learning we replace the last layer by a layer with a desired number of classes.
This modified network is then trained with the application-specific dataset.
All other layers of the network are “frozen” - this means their weights are kept constant and are not changed by the training procedure with the application-specific dataset.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">The term fine-tuning means that the weights in all or some of the other layers are not frozen.
The subsequent training with the custom dataset therefore fine-tunes the existing weights to the new domain.
Since the more shallow layers learn low-level features that differ less across domains, a smaller learning rate should be used than for deeper layers of the network.
If shallow layers are changed too quickly or too much, the network might overfit the training data of the target dataset and lose its generalization ability.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Some techniques used to train neural networks from scratch can be readapted to transfer learning.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> Simonyan et al. describe the training of VGG - one of the commonly used network architectures.
Initially, a shallow version of the network was created and trained on the data.
The network was then extended and the previously trained weights used for initialisation of the shallow layers.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p">A similar strategy can be applied for transfer learning.
A shallow network corresponding to the last few layers of the target pretrained network can be trained from scratch on the application specific dataset.
Then, instead of only replacing the last layer, the last layers are replaced by the shallow network with its weights.
Finally, the whole network is further trained and fine-tuned on the target dataset.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Data Augmentation</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Data augmentation is a common method to artificially increase the size of an existing dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and is often combined with transfer learning and fine-tuning.
While there are different types of methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, they all generate altered data from the existing set of training images.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2007.08781/assets/images/data_augmentation.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="70" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Basic image manipulations for data augmentation. Image taken from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.</figcaption>
</figure>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Most commonly, basic image manipulations such as horizontal flipping, random cropping or rotating are used (Figure <a href="#S2.F1" title="Figure 1 ‣ 2.3 Data Augmentation ‣ 2 Little Data - Smartly Used ‣ Mixing Real and Synthetic Data to Enhance Neural Network Training - A Review of Current Approaches" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
Other methods such as normalization (mean subtraction), color space transformations, photometric distortions and noise injection are also widely used.
In general, any alteration of the input image that does not change the label information is acceptable.
For instance, horizontally flipping an image of a cat still produces a cat.
While applying the same transformation to an image of the digit five produces an image with an invalid number that must not be trained as a five.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">In the training of all the famous architectures, such as AlexNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, InceptionV3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, VGG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> and ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, mean subtraction and random cropping were used to augment the training set.
While for AlexNet random cropping was used with a fixed size, in the training of all other architectures images were cropped with a random size and different aspect ratios.
Out of these networks, Inception was the only one without horizontal flipping augmentation.
Contrary, the augmentations for Inception included photometric distortions and random changes in brightness, contrast, color and even different interpolation methods while cropping images with different sizes.
Out of these transformations, only random brightness was applied to AlexNet, while random changes in color were used for VGG and ResNet.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">Data augmentation is usually applied at training time to enable the network to generalize better by presenting it different variations of the data.
However, when the networks mentioned above entered the ImageNet large scale visual recognition challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> they also applied augmentation at test time.
Test time augmentation does not affect the weights of the network.
The purpose is to present the network with different crops and transformations of the query image and average the results.
This is a viable way for offline computations for a competition.
In practical applications with possible real-time constraints this might not be feasible.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p">Random erasing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> is another strategy for augmentation.
A small part (random size and position) of the input image is replaced by a solid color or noise to simulate occlusion.
This technique has to be used carefully in order not to completely erase the pixels of the image that constitute the object described by its label.</p>
</div>
<div id="S2.SS3.p6" class="ltx_para">
<p id="S2.SS3.p6.1" class="ltx_p">Other methods for data augmentation are less intuitive to humans.
For example, in sample pairing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> two random images of different classes are averaged along each of the RGB channels.
The resulting image gets the label of the first randomly selected image.
Surprisingly, this data augmentation strategy was reported to reduce the error rate <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="S2.SS3.p7" class="ltx_para">
<p id="S2.SS3.p7.1" class="ltx_p">Generative adversarial networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> have also been applied for data augmentation.
GANs have gained recent attention in many fields, for example the generation of photo-realistic face images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> or style transfer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
In the same manner as a GAN can be trained to generate faces, it can also be trained to generate images to extend a dataset.
This was demonstrated in the domain of medical images by Maayan et al. in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
However, training a GAN requires a large amount of images.
If the available dataset is small it will not suffice to train a GAN and traditional data augmentation techniques should be used.</p>
</div>
<div id="S2.SS3.p8" class="ltx_para">
<p id="S2.SS3.p8.1" class="ltx_p">On the other hand, many implementations of style transfer GANs are available online, including pretrained weights.
These can be applied for data augmentation purposes as suggested for example in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.
The resulting images show the same content, but in a different artistic style (Figure <a href="#S2.F2" title="Figure 2 ‣ 2.3 Data Augmentation ‣ 2 Little Data - Smartly Used ‣ Mixing Real and Synthetic Data to Enhance Neural Network Training - A Review of Current Approaches" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
This type of data augmentation is similar as generating synthetic data using domain randomization (see Section <a href="#S3.SS3" title="3.3 Exemplary Approaches Explicitly Addressing Domain Shift ‣ 3 Synthetic Data ‣ Mixing Real and Synthetic Data to Enhance Neural Network Training - A Review of Current Approaches" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2007.08781/assets/images/gan_style_transfer.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="80" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Illustration of style transfer applied to an image. Image taken from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.</figcaption>
</figure>
<div id="S2.SS3.p9" class="ltx_para">
<p id="S2.SS3.p9.1" class="ltx_p">Another important augmentation method using GANs is the possibility to transfer images from one domain to another <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>.
Of special interest for autonomous driving would be transferring images between day and night, summer and winter and sunny and rainy weather <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.
This method does not need paired image examples, but still requires a sufficiently large dataset to train domains of interest.
We hope to see pretrained weights available online for corresponding domain translation models in the near future.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Feedforward Design</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">A recent paper proposes a completely different approach to compute network weights that deviates from traditional training schemes.
The so called <span id="S2.SS4.p1.1.1" class="ltx_text ltx_font_italic">feedforward design</span> omits the backpropagation algorithm used to train neural networks in favor of an algebraic approach that only requires one forward pass <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
The benefits reported are faster training, less data required for training and less vulnerability to adversarial attacks.
The results presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> were applied only on simple datasets and more research is necessary to fully assess the advantages one could gain using that approach.
We imagine that future work could result in some efficient pipelines to generate initial weights for new network architectures that lack pretrained weights.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Synthetic Data</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The previously discussed methods aim at altering and augmenting the available real-world data to artificially increase the dataset size.
Naturally, this real-world data is hard to collect and time consuming to annotate.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">In contrast to previous methods, the methods discussed here create synthetic data for the specific application domain.
Scenarios specific to an application domain can be modelled with 3D engines used to design computer games such as the Unreal Engine <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> or Unity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>.
The generated scenarios can be rendered as photo-realistic images from arbitrary perspectives and with arbitrary scene content.
In the context of urban scenes one can design a street and populate it with cars, pedestrians, motorbikes, bicycles and different types of static objects.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Synthetic data has the advantage that almost any domain can be modelled, extended and adjusted to one’s needs.
Further, even rare events can be modelled with required accuracy and variations that would be infeasible or too dangerous to acquire in real life.
An immense advantage of synthetic data is that annotations come for free.
Apart from a photo-realistic image, the 3D engine can also generate a depth map or a pixel-wise class annotation of the image since the type of the contained objects are known and defined by the modeller.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Certainly, there are domains where synthetic data generation is not possible or at least not as easy as in the case of urban scenes.
For example, synthetic images for medical applications might not depict a realistic distribution of certain types of diseases.
A network trained on such data could only perform well on artificial images and might not at all be able to detect pathological features in real images.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">In this work we focus on the domains of urban and traffic scenes for synthetic data generation.
The goal is to review and compare convolutional neural network training scenarios that do not require large amounts of real-world data.
The question of whether or not training with synthetic images will improve the performance on real-world data is still valid.
It will be examined exemplary on several recent publications in that field.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets With Synthetic Images</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Several synthetic datasets have been composed in the recent years.
In this section we review some of the widely used datasets for urban scenes.
Experiments performed with these datasets and reported in literature will be reviewed in the next section.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Ros et al. present SYNTHIA (SYNTHetic collection of Imagery and Annotations of urban scenarios) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, a photo-realistic dataset of a virtual city created with the Unity 3D engine.
It contains pixel-wise annotations for 13 object classes from multiple view points.
The images have a resolution of 960 <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mo id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><times id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\times</annotation></semantics></math> 720 pixels and horizontal field of view of 100 degrees.
SYNTHIA consists of two distinct image sets.
The first set provides around 13,000 images of random camera positions in the city.
The height of the virtual camera was fixed to be between 1.5 and 2 meters.
The second set provides four video sequences with around 50,000 images each of a simulated car ride across the city.
Each of the sequences depicts the same scenes, but during a different season (Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Datasets With Synthetic Images ‣ 3 Synthetic Data ‣ Mixing Real and Synthetic Data to Enhance Neural Network Training - A Review of Current Approaches" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2007.08781/assets/images/synthia.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="268" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example scene from SYNTHIA during the four different seasons. Image taken from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2007.08781/assets/images/virtualkitti.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="272" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Example scene from KITTI (top) and the corresponding scene in VirtualKITTI (bottom). Image taken from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</figcaption>
</figure>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">While SYNTHIA relies on a completely artificial city, Gaidon et al. present an approach to transfer an existing real-world dataset (KITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>) into the virtual domain, dubbed VirtualKITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> (Figure <a href="#S3.F4" title="Figure 4 ‣ 3.1 Datasets With Synthetic Images ‣ 3 Synthetic Data ‣ Mixing Real and Synthetic Data to Enhance Neural Network Training - A Review of Current Approaches" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).
The resulting dataset is photo-realistic and densely labeled with an image resolution of 1242 <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mo id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><times id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\times</annotation></semantics></math> 375 pixels and a total of around 17,000 frames.
While the dataset is smaller than SYNTHIA, apart from object detection and segmentation, it provides annotations for other tasks, including tracking and optical flow.
In contrast to the real KITTI dataset, VirtualKITTI allows to modify the setup and conditions of the scenes.
Among others, this includes the number and position of objects and different lighting and weather conditions.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Johnson-Roberson et al., the creators of the Driving in the Matrix (DITM) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> take a different approach.
Instead of creating a virtual environment they use a set of tools to capture photo-realistic images from a video game (GTA5, Rockstar games).
They focus on different vehicle classes for the tasks of object detection and segmentation.
The datasets consists of three distinct image sets with around 10,000, 50,000 and 200,000 images under different lighting and weather conditions.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">The same video game is also used by Richter et al., the creators of the VIsual PERception benchmark (VIPER) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
This dataset comprises a total of 254,000 full HD images with annotations for object detection, instance segmentation and optical flow.
Similar as DITM, this dataset provides images in different lighting and weather conditions, but does not solely focus on vehicles.
In a previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> the authors released a predecessor of the VIPER dataset with only around 25,000 images that we will refer to as the GTA5 dataset.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">Finally, the Virtual Environment for Instance Segmentation (VEIS) dataset described by Sadat Saleh et al. in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> focuses on instance segmentation of foreground objects.
The authors describe their dataset as “not highly realistic” regarding the textures of the objects, but with realistic shapes.
The dataset of about 61,000 images is split into a set of multi-class images with complex scenes and a single-class (multiple instance) images with simple scenes.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">Some properties of the mentioned datasets are summarized in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Datasets With Synthetic Images ‣ 3 Synthetic Data ‣ Mixing Real and Synthetic Data to Enhance Neural Network Training - A Review of Current Approaches" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
The authors of these datasets employ different training procedures which we will review below along with other approaches on synthetic datasets.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Summary of some properties of the synthetic datasets mentioned in this work.</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column">Dataset</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column"># images</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column">Variations</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Framework/Source</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_tt">SYNTHIA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>
</td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_tt">13k, 50k</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_tt">seasons</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">Unity</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_left">VirtualKITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_left">17k</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_left">weather, lighting</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center">Unity</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left">DITM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_left">10k, 50k, 200k</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_left">weather, lighting</td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center">GTA5</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<td id="S3.T1.1.5.4.1" class="ltx_td ltx_align_left">GTA5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</td>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_left">25k</td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_left">weather, lighting</td>
<td id="S3.T1.1.5.4.4" class="ltx_td ltx_align_center">GTA5</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<td id="S3.T1.1.6.5.1" class="ltx_td ltx_align_left">VIPER<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_left">254k</td>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_left">weather, lighting</td>
<td id="S3.T1.1.6.5.4" class="ltx_td ltx_align_center">GTA5</td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<td id="S3.T1.1.7.6.1" class="ltx_td ltx_align_left">VEIS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S3.T1.1.7.6.2" class="ltx_td ltx_align_left">61k</td>
<td id="S3.T1.1.7.6.3" class="ltx_td ltx_align_left">multi/single class scenes</td>
<td id="S3.T1.1.7.6.4" class="ltx_td ltx_align_center">Unity</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Approaches Exploiting Synthetic Data</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The authors of the SYNTHIA dataset target the task of semantic segmentation in urban scenes.
They test the hypothesis that additional synthetic data aid in training neural networks with datasets containing real data.
In total they investigate four different datasets and two neural networks (T-NET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and FCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>).
Each of the networks is trained with each of the datasets with real images only.
In a second pass, the training is repeated with the same datasets that are mixed with synthetic images.
The training occurs with mixed batches of 10 images, four of which are synthetic.
For most classes the results of training with real and synthetic images are better than with real images only.
The authors observe an improvement for the network T-NET in average per-class and global accuracy.
On the other hand, the improvements for FCN only occur in average per-class accuracy.
Among the classes that gain most from additional synthetic images are classes that represent individual objects.
These are pedestrians, cyclists, but also cars and even poles.
Only small improvements or even worse results are observed for classes that occupy greater areas, such as sky, buildings, roads, vegetation.
The latter can be regarded as background classes, whereas the individual objects constitute the foreground of a scene.
This distinction will be discussed further below in this section.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The authors of VirtualKITTI primarily benchmark a tracking algorithm, however, following the tracking-by-detection paradigm.
Although their dataset provides more classes, in their publication they focus on cars only.
For their experiments they apply a Faster-R-CNN model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and replace the backbone by VGG16.
The network was pretrained on ImageNet.
The authors fine-tune it to cars from the Pascal VOC 2007 dataset.
In the last step transfer learning is applied with the KITTI and with the VirtualKITTI datasets, respectively.
Out of these two variants, the one trained on real data performs much better.
In a third experiment, transfer learning is again applied with the VirtualKITTI dataset, however, the resulting network is subsequently fine-tuned with the real data.
This last experiment improves over the training with real data, although by a smaller margin than the real data improves over synthetic data alone.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">A different approach is reported by the authors of the DITM dataset.
They also focus on cars only and use Faster-R-CNN with VGG16.
In contrast to other approaches they completely separate real and synthetic data in training to better assess the results.
Training is performed on Cityscapes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and on different variants of their simulated data (with 10,000, 50,000 and 200,000 images).
Each of the 4 trained variants is subsequently evaluated on real data from the training set of the KITTI dataset.
The results were split into easy, moderate and hard to detect cars according to the KITTI dataset (the difficulty depends on the size of the car in the image, occlusion etc.).
In all three categories, training on Cityscapes leads to better results than training with 10,000 synthetic images, although Cityscapes provides only around 3,000 images for training.
However, increasing the synthetic image dataset to 50,000 beats training with Cityscapes by a large margin.
For easy to detect cars 50,000 images are sufficient and lead to best results.
For moderately and hard to detects cars the dataset with 200,000 leads to even better results.
The authors draw three conclusions from these experiments.
First, datasets with real images are too small to transfer the learned weights to other datasets.
Second, the current performance of neural network is limited by the available annotated training data rather than network architectures.
Finally, when training with synthetic images, the number of required training samples is much higher than with real images.
This indicates that real images have a higher variation in features, lighting, color etc. than simulated images.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">The authors of the VIPER dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> discussed above focus on a benchmarking suite for training on synthetic data.
In their paper they describe and measure statistical distributions of their synthetic dataset.
These include among others categories and instances per image, instances per semantic class and the distribution of distances of vehicles to the camera.
They find that the statistics of their proposed dataset closely resemble the statistics of Cityscapes.
In a subsequent evaluation they find that the relative performance of two investigated neural networks on their dataset is consistent with the performance of these networks on Cityscapes.
Unfortunately, in this work they make no comparison of training with real and synthetic images.
However, in a previous publication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> they report such experiments.
Note that the synthetic dataset in the previous paper had around 25,000 images (GTA5 dataset), which is only around 10% of the VIPER dataset.
In the previous work the authors conduct two distinct semantic segmentation experiments.
In the first experiment the authors explore gradually mixing real and synthetic images on the CamVid dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
Training on real images leads to significantly better results than only using synthetic images.
However, training with all synthetic images, but only 25% of all available real images leads to a performance close to training on all real images.
Increasing the amount of real images to 33% already beats the results on real images.
Finally, taking all available real and synthetic training data clearly improves the results.
In the second experiment the authors train on the KITTI dataset and again observe an improvement when synthetic data is added in training.
In both experiments training is performed with mixed batches of four real and four synthetic images.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">A common concern when training a network on synthetic images is the degrading performance on real images.
This performance gap is referred to as domain shift.
Sadat Saleh et al., the authors of the paper presenting the VEIS dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> do not explicitly address domain shift, but take it into account when designing their experiments.
They claim that foreground and background classes are affected differently by the domain shift and propose to treat them accordingly.
Sadat Saleh et al. argue that background classes such as road, sky, buildings and vegetation have realistic textures even in synthetic data.
On the other hand, only the shape, but not the texture of foreground classes such as cars, pedestrians and bicycles looks realistic.
The authors therefore propose to handle foreground classes in a detection-based manner to account for realistic shapes, while applying semantic segmentation to background classes because of their realistic texture.
We have already observed (an implicit) distinction between foreground and background classes in the results reported by the authors of the SYNTHIA dataset.
Mostly, adding synthetic data only improved the results for foreground classes.
Following the observations of Sadat Saleh et al. we conclude that the SYNTHIA results on foreground classes improved due to additional shape variations introduced with the synthetic data.
Sadat Saleh et al. train a VGG16-based DeepLab model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> for semantic segmentation of background classes on the GTA5 dataset.
This dataset was chosen because of its photo-realistic textures of background classes.
Foreground classes are trained on the proposed VEIS dataset using the Mask-RCNN network and the results of both networks are fused.
The trained network is evaluated on the validation set of Cityscapes.
The proposed method is compared with training a single segmentation network on different synthetic datasets and consistently obtains better results.
The results are improved even further when the proposed approach is used to generate pseudo ground truth labels for unlabeled real images.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Exemplary Approaches Explicitly Addressing Domain Shift</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Several papers (for example <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>) report that synthetic data does not possess as much variation in appearance as real data.
Tremblay et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> address this shortcoming by an approach that belongs to the field of domain randomization.
The goal is to create synthetic images that do not look photo-realistic at all.
The authors generate images of cars with random, unrealistic parameters for lighting, pose and textures (Figure <a href="#S3.F5" title="Figure 5 ‣ 3.3 Exemplary Approaches Explicitly Addressing Domain Shift ‣ 3 Synthetic Data ‣ Mixing Real and Synthetic Data to Enhance Neural Network Training - A Review of Current Approaches" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).
A total of 100,000 images are generated and used in the experiments.
The idea behind domain randomization is that the network has to learn to detect objects independently from their texture.
When presented with real images at test time the real texture is regarded as just another variation by the network.
An additional benefit is that time consuming creation of photo-realistic images is not necessary with domain randomization.
Tremblay et al. apply transfer learning with this randomized data on a pretrained network and use fine-tuning with real data.
They observe that it is beneficial to additionally update the weights of the earlier layers when applying transfer learning with synthetic data.
The reported results are better than with real images alone.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2007.08781/assets/images/domain_random.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="155" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Examples for domain randomization. Image taken from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>.</figcaption>
</figure>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> propose to use GANs to decrease the domain shift.
In their approach synthetic data is not just used as additional training data.
Instead, a GAN is trained to distinguish between real and fake images - both, for the source domain of synthetic images and the target domain of real images.
An additional pair of networks is closely linked to the GAN during training and is used to train an image embedding that more closely resembles the target domain.
The presented experiments use SYNTHIA and GTA5 as source domains, respectively.
The target domain is Cityscapes.
The proposed approach leads to a significantly lower domain gap compared to a conventional training on synthetic images alone.
Still, training on real images is reported to lead to best results.
At first, this seems to contradict the other papers discussed above.
However, SYNTHIA and GTA5 are comparatively small datasets and in their respective papers were always mixed with real data to obtain an improvement.
We therefore assume that the reported results can be improved further with a large scale synthetic dataset.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Outlook on Synthetic Data with GANs</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Nowadays, a lot of innovation in the field of deep learning is happening the area of GANs.
Many different approaches for image to image translation have been proposed in the recent years.
A recurrent topic of image to image translation methods is translating semantic maps of urban scenes to photo-realistic images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.
Such semantic maps highly resemble the ground truth annotation data of the Cityscapes dataset.
The work of Park et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> not only translates semantic maps to photographs, but additionally allows to choose different styles for the resulting image.
This property covers both, style transfer (Section <a href="#S2.SS3" title="2.3 Data Augmentation ‣ 2 Little Data - Smartly Used ‣ Mixing Real and Synthetic Data to Enhance Neural Network Training - A Review of Current Approaches" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>) and domain randomization (Section <a href="#S3.SS3" title="3.3 Exemplary Approaches Explicitly Addressing Domain Shift ‣ 3 Synthetic Data ‣ Mixing Real and Synthetic Data to Enhance Neural Network Training - A Review of Current Approaches" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Of course to massively apply such transformations from maps to photos one needs to create semantic maps.
While such maps come from ground truth annotations in these papers, there have also been attempts to create complex semantic maps with GANs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and to augment existing maps with inserted object instances <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">With these recent research papers, we have all building blocks for new types of synthetic datasets.
One could synthesize semantic maps, augment the results by inserting required object instances and finally translate the resulting maps into photo-realistic images.
We expect so see such datasets in the near future and are excited about the possibilities these datasets will offer in the areas of training and augmenting with synthetic data.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Summary</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work we reviewed several current approaches that aim at improving the performance of neural networks.
The presented ideas were twofold.
On one hand, we examined how small datasets can be used effectively to train powerful networks with limited amount of data.
On the other hand, we presented some recently published and successful experiments to train with real and synthetic data.
These experiments achieve better results than with real data alone.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">We identify several guidelines for the development of computer vision applications based on neural networks.
First and most importantly, one should always consider using a pre-trained network, rather than training from scratch.
Second, in general it is recommended to apply transfer learning and fine-tuning.
This is especially the case if the total amount of training data is small.
Data augmentation techniques of various types have been proposed in literature and are a great tool to help a network to generalize better.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Synthetic data is easier to acquire and has a great potential to further improve the performance of neural networks.
However, the synthetic datasets that are available nowadays contain less variations than real data.
As a consequence, significantly more synthetic data is needed to achieve the same training results as with real data.
Nevertheless, mixing synthetic and real data has been shown to increase the performance of neural networks in several experiments.
Mixing is done either by training with mixed real and synthetic batches.
Another popular method is using synthetic data for transfer learning and later real data for fine-tuning.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">The more one relies on synthetic data during training, the more domain shift becomes an issue.
Luckily, there is an indication that more synthetic data diminishes its effects and the resulting networks also perform well on real data.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">For the task of semantic segmentation we saw that synthetic data might be enough to train background classes that comprise large image areas.
Contrary, synthetic data for foreground classes (i.e. classes that represent individual objects) has realistic shapes, but poor texture.
Thus, foreground classes benefit from a training procedure that aims at object detection, rather than semantic segmentation.
We would like to see more research on that topic to gain further insights.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">Finally, GANs are a hot research topic and we expect more exciting datasets in the near future.
These could include automatically generated, photo-realistic urban scenes where relevant intances such as pedestrians or cars are automatically inserted in different poses at relevant positions in the images.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Brostow, G.J., Fauqueur, J., Cipolla, R.: Semantic object classes in video: A
high-definition ground truth database. Pattern Recognition Letters 30(2),
88–97 (2009)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab:
Semantic image segmentation with deep convolutional nets, atrous convolution,
and fully connected crfs. IEEE transactions on pattern analysis and machine
intelligence 40(4), 834–848 (2017)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Chen, Q., Koltun, V.: Photographic image synthesis with cascaded refinement
networks. In: Proceedings of the IEEE international conference on computer
vision. pp. 1511–1520 (2017)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,
Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban
scene understanding. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. pp. 3213–3223 (2016)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A
large-scale hierarchical image database. In: 2009 IEEE conference on computer
vision and pattern recognition. pp. 248–255. Ieee (2009)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Eitel, A., Springenberg, J.T., Spinello, L., Riedmiller, M., Burgard, W.:
Multimodal deep learning for robust rgb-d object recognition. In: 2015
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).
pp. 681–687. IEEE (2015)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Epic Games: Unreal engine, <a target="_blank" href="https://www.unrealengine.com" title="" class="ltx_ref">https://www.unrealengine.com</a>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Esteva, A., Kuprel, B., Novoa, R.A., Ko, J., Swetter, S.M., Blau, H.M., Thrun,
S.: Dermatologist-level classification of skin cancer with deep neural
networks. Nature 542(7639), 115–118 (2017)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Everingham, M., Eslami, S.A., Van Gool, L., Williams, C.K., Winn, J.,
Zisserman, A.: The pascal visual object classes challenge: A retrospective.
International journal of computer vision 111(1), 98–136 (2015)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Frid-Adar, M., Klang, E., Amitai, M., Goldberger, J., Greenspan, H.: Gan-based
data augmentation for improved liver lesion classification (2018)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Gaidon, A., Wang, Q., Cabon, Y., Vig, E.: Virtual worlds as proxy for
multi-object tracking analysis. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. pp. 4340–4349 (2016)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics: The
kitti dataset. The International Journal of Robotics Research 32(11),
1231–1237 (2013)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Ghelfi, E., Galeone, P., De Simoni, M., Di Mattia, F.: Adversarial pixel-level
generation of semantic images. arXiv preprint arXiv:1906.12195 (2019)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
S., Courville, A., Bengio, Y.: Generative adversarial nets. In: Advances in
neural information processing systems. pp. 2672–2680 (2014)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
recognition. In: Proceedings of the IEEE conference on computer vision and
pattern recognition. pp. 770–778 (2016)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Inoue, H.: Data augmentation by pairing samples for images classification.
arXiv preprint arXiv:1801.02929 (2018)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with
conditional adversarial networks. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. pp. 1125–1134 (2017)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style
transfer and super-resolution. In: European conference on computer vision.
pp. 694–711. Springer (2016)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Johnson-Roberson, M., Barto, C., Mehta, R., Sridhar, S.N., Rosaen, K.,
Vasudevan, R.: Driving in the matrix: Can virtual worlds replace
human-generated annotations for real world tasks? arXiv preprint
arXiv:1610.01983 (2016)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of gans for
improved quality, stability, and variation. arXiv preprint arXiv:1710.10196
(2017)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep
convolutional neural networks. In: Advances in neural information processing
systems. pp. 1097–1105 (2012)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Kuo, C.C.J., Zhang, M., Li, S., Duan, J., Chen, Y.: Interpretable convolutional
neural networks via feedforward design. Journal of Visual Communication and
Image Representation 60, 346–359 (2019)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Lai, K., Bo, L., Ren, X., Fox, D.: A large-scale hierarchical multi-view rgb-d
object dataset. In: 2011 IEEE international conference on robotics and
automation. pp. 1817–1824. IEEE (2011)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Lee, D., Liu, S., Gu, J., Liu, M.Y., Yang, M.H., Kautz, J.: Context-aware
synthesis and placement of object instances. In: Advances in Neural
Information Processing Systems. pp. 10393–10403 (2018)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In:
European conference on computer vision. pp. 740–755. Springer (2014)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic
segmentation. In: Proceedings of the IEEE conference on computer vision and
pattern recognition. pp. 3431–3440 (2015)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Mikołajczyk, A., Grochowski, M.: Data augmentation for improving deep
learning in image classification problem. In: 2018 international
interdisciplinary PhD workshop (IIPhDW). pp. 117–122. IEEE (2018)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Oquab, M., Bottou, L., Laptev, I., Sivic, J.: Learning and transferring
mid-level image representations using convolutional neural networks. In:
Proceedings of the IEEE conference on computer vision and pattern
recognition. pp. 1717–1724 (2014)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Park, T., Liu, M.Y., Wang, T.C., Zhu, J.Y.: Semantic image synthesis with
spatially-adaptive normalization. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. pp. 2337–2346 (2019)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object
detection with region proposal networks. In: Advances in neural information
processing systems. pp. 91–99 (2015)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Richter, S.R., Hayder, Z., Koltun, V.: Playing for benchmarks. In: Proceedings
of the IEEE International Conference on Computer Vision. pp. 2213–2222
(2017)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Richter, S.R., Vineet, V., Roth, S., Koltun, V.: Playing for data: Ground truth
from computer games. In: European conference on computer vision. pp.
102–118. Springer (2016)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Ros, G., Sellart, L., Materzynska, J., Vazquez, D., Lopez, A.M.: The synthia
dataset: A large collection of synthetic images for semantic segmentation of
urban scenes. In: Proceedings of the IEEE conference on computer vision and
pattern recognition. pp. 3234–3243 (2016)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Ros, G., Stent, S., Alcantarilla, P.F., Watanabe, T.: Training constrained
deconvolutional networks for road scene semantic segmentation. arXiv preprint
arXiv:1604.01545 (2016)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual
recognition challenge. International journal of computer vision 115(3),
211–252 (2015)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Sadat Saleh, F., Sadegh Aliakbarian, M., Salzmann, M., Petersson, L., Alvarez,
J.M.: Effective use of synthetic data for urban scene semantic segmentation.
In: Proceedings of the European Conference on Computer Vision (ECCV). pp.
84–100 (2018)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Sankaranarayanan, S., Balaji, Y., Jain, A., Nam Lim, S., Chellappa, R.:
Learning from synthetic data: Addressing domain shift for semantic
segmentation. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. pp. 3752–3761 (2018)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Schwarz, M., Schulz, H., Behnke, S.: Rgb-d object recognition and pose
estimation based on pre-trained convolutional neural network features. In:
2015 IEEE international conference on robotics and automation (ICRA). pp.
1329–1335. IEEE (2015)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Shahroudy, A., Liu, J., Ng, T.T., Wang, G.: Ntu rgb+ d: A large scale dataset
for 3d human activity analysis. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. pp. 1010–1019 (2016)

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Shorten, C., Khoshgoftaar, T.M.: A survey on image data augmentation for deep
learning. Journal of Big Data 6(1),  60 (2019)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556 (2014)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In:
Proceedings of the IEEE conference on computer vision and pattern
recognition. pp. 1–9 (2015)

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Tremblay, J., Prakash, A., Acuna, D., Brophy, M., Jampani, V., Anil, C., To,
T., Cameracci, E., Boochoon, S., Birchfield, S.: Training deep networks with
synthetic data: Bridging the reality gap by domain randomization. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
Workshops. pp. 969–977 (2018)

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Unity Technologies: Unity, <a target="_blank" href="https://unity.com" title="" class="ltx_ref">https://unity.com</a>

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Wang, J., Perez, L.: The effectiveness of data augmentation in image
classification using deep learning. arXiv preprint arXiv:1712.04621 (2017)

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.:
High-resolution image synthesis and semantic manipulation with conditional
gans. In: Proceedings of the IEEE conference on computer vision and pattern
recognition. pp. 8798–8807 (2018)

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Yosinski, J., Clune, J., Bengio, Y., Lipson, H.: How transferable are features
in deep neural networks? In: Advances in neural information processing
systems. pp. 3320–3328 (2014)

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Zhong, Z., Zheng, L., Kang, G., Li, S., Yang, Y.: Random erasing data
augmentation. arXiv preprint arXiv:1708.04896 (2017)

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image
translation using cycle-consistent adversarial networks. In: Proceedings of
the IEEE international conference on computer vision. pp. 2223–2232 (2017)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2007.08780" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2007.08781" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2007.08781">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2007.08781" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2007.08782" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 18 17:56:35 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
