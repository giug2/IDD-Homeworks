<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.16707] Visualizing the Shadows: Unveiling Data Poisoning Behaviors in Federated Learning</title><meta property="og:description" content="This demo paper examines the susceptibility of Federated Learning (FL) systems to targeted data poisoning attacks, presenting a novel system for visualizing and mitigating such threats.
We simulate targeted data poison…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Visualizing the Shadows: Unveiling Data Poisoning Behaviors in Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Visualizing the Shadows: Unveiling Data Poisoning Behaviors in Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.16707">

<!--Generated on Wed Jun  5 19:12:07 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Data poisoning,  security analysis,  federated learning.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Visualizing the Shadows: Unveiling Data Poisoning Behaviors in Federated Learning
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xueqing Zhang1, Junkai Zhang2, Ka-Ho Chow3, Juntao Chen1, Ying Mao1, Mohamed Rahouti1, 
<br class="ltx_break">Xiang Li1, Yuchen Liu4, Wenqi Wei15
</span><span class="ltx_author_notes">5The corresponding author thanks the Fordham Faculty Research Grant.
<span class="ltx_contact ltx_role_affiliation">1 Department of Computer and Information Sciences, Fordham University 
<br class="ltx_break">
</span>
<span class="ltx_contact ltx_role_affiliation">2 Department of Applied Analytics, Columbia University 
<br class="ltx_break">
</span>
<span class="ltx_contact ltx_role_affiliation">3 Department of Computer Science, the University of Hong Kong 
<br class="ltx_break">
</span>
<span class="ltx_contact ltx_role_affiliation">4 Department of Computer Science, North Carolina State University
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">This demo paper examines the susceptibility of Federated Learning (FL) systems to targeted data poisoning attacks, presenting a novel system for visualizing and mitigating such threats.
We simulate targeted data poisoning attacks via label flipping and analyze the impact on model performance, employing a five-component system that includes Simulation and Data Generation, Data Collection and Upload, User-friendly Interface, Analysis and Insight, and Advisory System. Observations from three demo modules: label manipulation, attack timing, and malicious attack availability, and two analysis
components: utility and analytical behavior of local model updates highlight the risks to system integrity and
offer insight into the resilience of FL systems. The demo is available at <a target="_blank" href="https://github.com/CathyXueqingZhang/DataPoisoningVis" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/CathyXueqingZhang/DataPoisoningVis</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Data poisoning, security analysis, federated learning.

</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2405.16707/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="410" height="125" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S0.F1.2.1" class="ltx_text" style="font-size:90%;">Overview.</span></figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated Learning (FL) is a promising machine learning framework that allows distributed clients to collaboratively train a model while keeping their data localized. Each client trains a model on its own data and sends the model updates to the server, which then aggregates these updates to improve a global model for the next round. This method effectively addresses the major privacy concerns of traditional centralized models. With the growing adoption of AI across various industries, FL’s capability to harness collective without compromising individual privacy stands out as a significant advantage <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. However, this distributed nature introduces unique vulnerabilities through data poisoning attacks where malicious agents can tamper the training process to compromise the model’s accuracy performance or specific misbehaviors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Malicious participants in FL can exploit the lack of control over the clients by tampering with their local datasets or model updates. Errors will propagate through the federated network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> for model manipulation. Unlike the untargeted poisoning attacks that compromise the overall model performance for Denial-of-Service,
the targeted data poisoning attack is particularly stealthy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
The latter modifies only specific inputs related to chosen victims, leaving the remainder of the model functioning as usual.
Therefore, the targeted data poisoning attack poses
significant risks to the integrity and reliability of the federated model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Highlighting these concerns emphasizes the need for robust defense mechanisms to ensure FL is trustworthy and effective.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this demo paper, we present an empirical investigation into the vulnerability of FL systems to targeted data poisoning attacks, highlighting through visualizing the multifaceted landscape of the threats. Specifically, Our study delves into the extent to which FL systems are susceptible to these malicious activities and introduces a novel advisory system designed to detect and mitigate such threats.
Our analysis sheds light on the countermeasure design, providing insight into robustness strategies. There are five components in the proposed demo system for visualizing targeted data poisoning attacks and presenting the adversarial behavior of the targeted data poisoning attack.
The first component is a simulation of the targeted data poisoning attack via label flipping <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. We explore the effects of different attack timings, and malicious participant percentage and availability. The second component acts as the server and collects clients’ local model updates for analysis. The experimental data are recorded in JSON format and stored on MongoDB. The third component consists of an interactive visualization tool, providing a user-friendly interface for analytical inspection of the local model update shared with the server from the clients. The fourth component aims to offer analytical insights from the behavior of the local model update, and the analysis of the impact of poisoning on F1 scores, the effectiveness of attacker classification, the sensitivity of different FL models to poisoning, and the extent of model drift due to poisoned inputs. Finally, the fifth component will help
translating such analytical insights into actionable recommendations for robust FL system design.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">With extensive experiments, our demo system generates valuable insights for defending against targeted data poisoning attacks. For example,
our findings suggest that while the overall accuracy of the global model can rebound from attacks that occur early in the training process, attacks in later stages—particularly those with significant participation from malicious users—tend to cause more severe damage. In general,
we advocate for rigorous client verification strategies to preclude the participation of malicious agents. Based on the separable model updates from the benign and malicious clients,
we recommend studying further when the incorporation of anomaly detection techniques can help identify and neutralize attacks promptly.
Additionally, we emphasize the importance of designing robust FL models that maintain their performance in the face of such adversarial challenges, a concept we term ”robustness performance co-design.”</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Demo System Design</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The design overview of our demo system is illustrated in
<span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Figure <a href="#S0.F1" title="Figure 1 ‣ Visualizing the Shadows: Unveiling Data Poisoning Behaviors in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a></span>. By simulating various attack scenarios, including
different types of label manipulation, attack timing, and malicious participant percentage and availability  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, the goal is to uncover the malicious behavior of the targeted data poisoning attacks on the shared local models and its impact on attack effectiveness. Our framework integrates an interactive dashboard facilitating the generation, feature extraction, and visualization of the FL model behavior in the presence of malicious clients. We focus on studying the effectiveness of the poisoning attack and the separable phenomenon of local model updates between the benign and the malicious clients.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Simulations and Data Generation.</span>
The demo begins by simulating an FL environment with a central server and multiple clients (i.e., 50) over 200 global training rounds<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/git-disl/DataPoisoning_FL.git" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/git-disl/DataPoisoning_FL.git</a></span></span></span>. We consider 10% participation rate, i.e., 5 participants are selected per round. We implement the label-flipping attack to poison the training dataset at local clients for its effectiveness and readiness to corrupt the FL learning process. Note that other types of targeted data poisoning attacks, e.g., backdoor, demonstrate a similar phenomenon in terms of victim data compromise and separable local model update between the benign and malicious clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. The users are given the choice to specify the victim class and the attack target class. Accordingly, malicious clients will modify the label of their training data from the victim class to the target class, causing misclassification. For each of the victim-target pair, we design three attack modules: Label Manipulation, Attack Timing, and Malicious Participant Availability. In all three modules, users have the flexibility to set the number of malicious workers, thereby adjusting the proportion of compromised participants in the FL network.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In <span id="S2.p3.1.1" class="ltx_text ltx_font_italic">Label Manipulation</span>, we evaluate the robustness against adversarial label manipulations using two benchmark datasets: CIFAR-10 and Fashion MNIST. For CIFAR-10, our experiments include switching labels from ’dog’ to ’cat’ (5 to 3), ’airplane’ to ’bird’ (0 to 2), ’automobile’ to ’truck’ (1 to 9), and ’deer’ to ’horse’ (4 to 7). These label changes are designed to test the FL system’s ability to handle misclassifications between visually or contextually similar categories. In parallel, with Fashion MNIST, we manipulate labels from ’t-shirt’ to ’shirt’ (0 to 6), ’trouser’ to ’dress’ (1 to 3), ’coat’ to ’pullover’ (4 to 2), and ’sneaker’ to ’ankle boot’ (7 to 9), which examines the framework’s response to mislabeling among different types of apparel.
In <span id="S2.p3.1.2" class="ltx_text ltx_font_italic">Attack Timing</span>, we investigate how attacks that occur only during certain phases of the training process can affect system performance. The default scenario uses the CIFAR-10 dataset with labels flipped from ”automobile” (class 1) to ”truck” (class 9). Users can also specify the training rounds during which the malicious workers are active, opting for scenarios where the attack commences after a certain point or ceases before another.
Additionally, we examine the impact of <span id="S2.p3.1.3" class="ltx_text ltx_font_italic">Malicious Participant Availability</span>. The default scenario also uses the CIFAR-10 dataset with labels flipped from ”automobile” (class 1) to ”truck” (class 9).
Given that the client selection process in FL usually relies on the availability of the client <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, the attacker can have the motivation to always make themselves online. Thus, even though the percentage of the attackers among 50 clients can be as low as 10%, the number of attackers selected at each round for global aggregation can be much higher. For instance, setting a parameter to 0.7 means there is a 70% probability that any selected participant will be one of the malicious actors in any given round.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.16707/assets/x2.png" id="S2.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="207" height="142" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>User-friendly Interface-F1 Analysis</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.16707/assets/x3.png" id="S2.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="207" height="142" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>User-friendly Interface-Signature Analysis</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S2.F2.2.1" class="ltx_text" style="font-size:90%;">User-friendly Interface</span></figcaption>
</figure>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Data Collection and Management.</span>
With local model update data collected at the global server throughout the FL learning process, a comprehensive dataset including client participation logs and round-specific model parameters is gathered and saved in JSON format. This dataset is stored at MongoDB as the backend database.</p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">User-friendly Interface.</span> An interactive dashboard is designed to allow users to engage with the visualization data dynamically. Users can scrutinize the effects of data poisoning, the performance of individual clients, and the progression of the model’s learning over time. Our demo system features two aspects of the FL system in the presence of the label flipping attack: f1 score regarding the global model performance, and the three-dimension projection of the client’s high-dimension local model updates under principal component analysis (PCA). In the first visualization aspect,
we plot the F1 score of the global model under the user-specified attack setting will be used to study the global model performance throughout the course of FL learning, as shown in <span id="S2.p5.1.2" class="ltx_text ltx_font_bold">Figure <a href="#S2.F2.sf1" title="In Figure 2 ‣ II Demo System Design ‣ Visualizing the Shadows: Unveiling Data Poisoning Behaviors in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2a</span></a></span>. In this module, the user will be able to zoom in on the F1 score for both the source victim class and the attack target class at the specified epoch round range.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">In the second component of our signature analysis, we generate three-dimensional PCA visualizations for the client’s local model updates during each training round, as shown in <span id="S2.p6.1.1" class="ltx_text ltx_font_bold">Figure <a href="#S2.F2.sf2" title="In Figure 2 ‣ II Demo System Design ‣ Visualizing the Shadows: Unveiling Data Poisoning Behaviors in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2b</span></a></span>. Users are informed about the identities of malicious and benign clients, allowing them to select specific rounds and clients for detailed spatial and temporal examination of the local model updates. Our approach to reducing the high-dimensional model updates to three dimensions is inspired by the observed decoupling effect between poisoned and benign updates as documented in existing literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. These visualizations facilitate an understanding of the learning dynamics and identify the rounds in which model performance begins to deteriorate due to poisoning. Additionally, we offer a separate 3D PCA visualization that aggregates local model updates from all five workers at each epoch round. As the round progresses, each point is iteratively highlighted in blue, illustrating how gradient updates evolve over time.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Observation and Insights</span>
</h2>

<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.fig1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:238.5pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2405.16707/assets/myplot1.png" id="S3.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="180" height="99" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>F1-Early phase</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.16707/assets/myplot2.png" id="S3.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="180" height="99" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>F1-Mid phase</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.16707/assets/myplot3.png" id="S3.F3.sf3.g1" class="ltx_graphics ltx_img_landscape" width="180" height="99" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>F1-Late phase</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S3.F4.fig1.2.1" class="ltx_text" style="font-size:90%;">F1 score graph through different phases.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.fig2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:173.4pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F4.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2405.16707/assets/less.jpg" id="S3.F4.sf1.g1" class="ltx_graphics ltx_img_landscape" width="281" height="99" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>5/50 malicious clients </figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.16707/assets/x4.png" id="S3.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="217" height="75" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>30/50 malicious clients </figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S3.F4.fig2.2.1" class="ltx_text" style="font-size:90%;">Abnormal PCA effect in one round.</span></figcaption>
</figure>
</div>
</div>
</figure>
<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p"><span id="S3.p1.1.1" class="ltx_text ltx_font_bold">Utility analysis: F1 score.</span> To investigate the most effective data poisoning strategies, we vary the attack timing and the probability of malicious client participation. We hypothesize that attacks implemented in the late stages of model convergence are potentially more disruptive compared to early interventions. This hypothesis is examined by comparing the impacts of early-round poisoning with later-round attacks.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Our analysis revealed two key insights regarding the effectiveness of data poisoning attacks. Firstly, the timing of the attack plays a crucial role in maximizing its disruptive impact. Attacks implemented too early in the model’s convergence phase tend to be mitigated as the model continues training and recovers from the initial poisoning, as shown in <span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Figure <a href="#S3.F3.sf1" title="In Figure 4 ‣ III Observation and Insights ‣ Visualizing the Shadows: Unveiling Data Poisoning Behaviors in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3a</span></a></span>. Conversely, attacks executed too late may not allow sufficient time for the poisoned model updates to propagate and influence the global model effectively, as shown in Figure <a href="#S3.F3.sf3" title="In Figure 4 ‣ III Observation and Insights ‣ Visualizing the Shadows: Unveiling Data Poisoning Behaviors in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3c</span></a>. Our experiments demonstrated that attacks initiated around epoch 70, as shown in Figure <a href="#S3.F3.sf2" title="In Figure 4 ‣ III Observation and Insights ‣ Visualizing the Shadows: Unveiling Data Poisoning Behaviors in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3b</span></a>, approximately one-third into the training process, yielded the most substantial degradation in model performance. Secondly, the intermittent nature of the attacks manifested as fluctuations in the F1 scores for both the target class and the victim classes. This observation suggests that the model’s ability to learn and generalize was periodically disrupted by the introduction of poisoned data, leading to oscillations in its predictive performance.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Attack behavior analysis: PCA Signature.</span>
To visualize the attack behavior on the local model update, we employ PCA with orange dots as benign clients and blue crosses as malicious ones. We have made several interesting observations on the proposed demo.</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Existence of separable local model update</span>: In the presence of data poisoning attacks at local clients, local model updates from malicious and benign clients may demonstrate separable phenomena in the PCA-transformed feature space. This separability could allow for the identification of outlier clusters corresponding to poisoned data.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Ineffective Poisoning Attempts</span>: When malicious clients are few, and their poisoning attempts are made early in the training process, the aggregate effect on the global model is minimal. The benign training from subsequent rounds
weakens the poison effect.
As shown in <span id="S3.I1.i2.p1.1.2" class="ltx_text ltx_font_bold">Figure <a href="#S3.F4.sf1" title="In Figure 4 ‣ III Observation and Insights ‣ Visualizing the Shadows: Unveiling Data Poisoning Behaviors in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4a</span></a></span> with 5 malicious clients out of 50 total clients, PCA fails to mark any significant deviation from the norm, as the visual clustering of data points remains largely unaffected. When the two cluster merge together, the attack effect is low.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Overwhelming Poisoning</span>: When the poisoning is aggressive—signified by a large number of malicious clients—the model begins to learn the biases as features. The two clusters combine, and the impact of the attack diminishes. Figure <a href="#S3.F4.sf2" title="In Figure 4 ‣ III Observation and Insights ‣ Visualizing the Shadows: Unveiling Data Poisoning Behaviors in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4b</span></a> shows the result with 30 malicious clients out of 50 total clients. The attackers effectively shift the ’norm’ in the feature space, a phenomenon we observe as the model’s compromised integrity. These results echo the complication scenarios of FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> when relying on the separable phenomenon of benign and malicious model updates for outlier detection.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Spatial-Temporal analysis</span>: Besides the spatial analysis of the separable phenomenon on model updates, we further integrate the temporal dimension into the PCA analysis by tracking gradient updates across training epochs. By mapping data points based on both client group similarity and generation round and sequence, we can visualize not just static snapshots but also dynamic changes. This method enables examing dynamics in the model’s learning trajectory, revealing the progressive impact of poisoned data. The observation that malicious clients’ model updates demonstrate a denser distribution than the benign ones makes it possible to detect the existence of attacks more effectively <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Advisory System</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Through thorough visualization and analysis, we determine the conditions under which model performance is most susceptible to poisoning strategies, employing sensitivity analysis to highlight FL system vulnerabilities.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Enhanced Client Verification Strategies</span>: By tracking the confusion matrix on the class-wise performance of the models, client verification processes can be introduced to maintain data integrity. Clients presenting data that significantly lowers the F1 score will be flagged for further scrutiny or excluded from the model update pool.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Anomaly Detection Mechanisms</span>: Outlier detection via a separable model update from PCA/SVD/clustering can fail to identify malicious clients due to the complication scenarios of FL. Monitoring temporal information with advanced anomaly detection is required to smoothly quarantine malicious participants and undo their negative impact in real-time.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Robust Model Development Practices</span>: We advocate for models that continuously learn and adapt, using insights from F1 scores and the behavior of local model updates. Incorporating techniques like cross-validation, robust aggregation, and data/model sanitization could potentially minimize the impact of poisoned data.</p>
</div>
<div id="S4.p5" class="ltx_para ltx_noindent">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Robustness Performance Co-Design</span>:
Apart from the development of robust approaches, it is equally important to maintain model performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Different aspects of robustness consideration as well as performance guarantee should be embedded right from the design phase.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">Leveraging both the utility analysis and attack behavior analysis, FL systems can establish a more informed defense posture against data poisoning. By
intertwining these analytical tools with proactive and reactive
security measures, the FL framework can not only detect but
also adapt to the evolving landscape of data poisoning attacks.
</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This demo paper presents a visualization and analytical platform that simulates targeted data poisoning attacks via
label flipping and analyzes the impact on model performance. The demo system contains five components: Simulation and Data Generation, Data Collection and Upload, User-friendly Interface, Analysis and Insight, and Advisory System. Our demo system provides valuable insights on label manipulation, attack timing, and malicious attacker availability through F1 analysis and signature analysis, offering strategic recommendations toward the robustness of FL systems.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efficient learning of deep networks from decentralized data,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence and Statistics</em>.   PMLR, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
B. Biggio, B. Nelson, and P. Laskov, “Poisoning attacks against support vector machines,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">International Coference on International Conference on Machine Learning</em>, 2012.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
V. Tolpegin, S. Truex, M. E. Gursoy, and L. Liu, “Data poisoning attacks against federated learning systems,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">European Symposium on Research in Computer Security</em>.   Springer, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
K.-H. Chow, L. Liu, W. Wei, F. Ilhan, and Y. Wu, “Stdlens: Model hijacking-resilient federated learning for object detection,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
B. Tran, J. Li, and A. Madry, “Spectral signatures in backdoor attacks,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">International Conference on Neural Information Processing Systems</em>, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
W. Yuan, C. Yang, L. Qu, G. Ye, Q. V. H. Nguyen, and H. Yin, “Robust federated contrastive recommender system against model poisoning attack,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.20107</em>, 2024.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J. Hayase, W. Kong, R. Somani, and S. Oh, “Spectre: Defending against backdoor attacks using robust statistics,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
W. Wei, K.-H. Chow, Y. Wu, and L. Liu, “Demystifying data poisoning attacks in distributed learning as a service,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Services Computing</em>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Steinhardt, P. W. Koh, and P. Liang, “Certified defenses for data poisoning attacks,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">International Conference on Neural Information Processing Systems</em>, 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
W. Wei and L. Liu, “Trustworthy distributed ai systems: Robustness, privacy, and governance,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, 2024.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.16706" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.16707" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.16707">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.16707" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.16708" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 19:12:07 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
