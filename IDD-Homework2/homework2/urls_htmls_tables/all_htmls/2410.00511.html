<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Pre-training with Synthetic Patterns for Audio</title>
<!--Generated on Sun Sep 29 14:40:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
self-supervised learning, 
masked autoencoder,  audio,  synthetic data
" lang="en" name="keywords"/>
<base href="/html/2410.00511v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S1" title="In Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S2" title="In Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Proposed Framework</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S2.SS1" title="In II Proposed Framework ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Masked Autoencoder</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S2.SS2" title="In II Proposed Framework ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Synthetic Patterns</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S2.SS3" title="In II Proposed Framework ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Transferring MAEs to Audio Tasks</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S3" title="In Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Experiments</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S3.SS1" title="In III Experiments ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Properties of synthetic images</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S3.SS2" title="In III Experiments ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Comparison with image-based pre-training</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S3.SS3" title="In III Experiments ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Comparison with existing methods</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S3.SS4" title="In III Experiments ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Evaluation on HEAR and ARCH benchmark</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S4" title="In Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
Pre-training with Synthetic Patterns for Audio
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">1<sup class="ltx_sup" id="id1.1.id1">st</sup> Yuchi Ishikawa
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id2.2.id1">LY Corporation / Keio University</span>
<br class="ltx_break"/>Tokyo, Japan 
<br class="ltx_break"/>yuchi.ishikawa@lycorp.co.jp
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">2<sup class="ltx_sup" id="id3.1.id1">nd</sup> Tatsuya Komatsu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id4.2.id1">LY Corporation</span>
<br class="ltx_break"/>Tokyo, Japan 
<br class="ltx_break"/>komatsu.tatsuya@lycorp.co.jp
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">3<sup class="ltx_sup" id="id5.1.id1">rd</sup> Yoshimitsu Aoki
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id6.2.id1">Keio University</span>
<br class="ltx_break"/>Kanagawa, Japan 
<br class="ltx_break"/>aoki@elec.keio.ac.jp

</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1">In this paper, we propose to pre-train audio encoders using synthetic patterns instead of real audio data.
Our proposed framework consists of two key elements.
The first one is Masked Autoencoder (MAE), a self-supervised learning framework
that learns from reconstructing data from randomly masked counterparts.
MAEs tend to focus on low-level information such as visual patterns and regularities within data.
Therefore, it is unimportant what is portrayed in the input,
whether it be images, audio mel-spectrograms, or even synthetic patterns.
This leads to the second key element, which is synthetic data.
Synthetic data, unlike real audio, is free from privacy and licensing infringement issues.
By combining MAEs and synthetic patterns,
our framework enables the model to learn generalized feature representations without real data,
while addressing the issues related to real audio.
To evaluate the efficacy of our framework,
we conduct extensive experiments across a total of 13 audio tasks and 17 synthetic datasets.
The experiments provide insights into which types of synthetic patterns are effective for audio.
Our results demonstrate that our framework achieves performance
comparable to models pre-trained on AudioSet-2M and partially outperforms image-based pre-training methods.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
self-supervised learning,
masked autoencoder, audio, synthetic data

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large-scale models have demonstrated high performance in the audio processing field.
Among these, Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib1" title="">1</a>]</cite> have played an important role
in advancing this field,
although they have the drawback of requiring large amounts of labeled data for training.
Moreover, it is challenging to collect high-quality labeled audio data in the real world,
which impedes effective training.
We identify two approaches in existing works aiming to leverage Transformers to solve downstream tasks:
(i) transferring Vision Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib2" title="">2</a>]</cite> (ViTs) pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib3" title="">3</a>]</cite>
to audio tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib4" title="">4</a>]</cite>,
(ii) learning feature representations through self-supervised learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib7" title="">7</a>]</cite>
from large amounts of audio data (e.g., AudioSet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib8" title="">8</a>]</cite>)
and VGGSound <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib9" title="">9</a>]</cite>).</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, these methods harbor problems related to real data, as described below:</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S1.p3.1.1">Privacy issues</span>:
ImageNet and AudioSet consist of images that portray people and human voices, respectively.
Therefore, using these datasets may potentially infringe on privacy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S1.p4.1.1">License infringement issues</span>:
Most of the data included in large-scale datasets are collected from the web,
such as YouTube and search engines.
Some of these data may have licenses that prohibit their use for model training.
Using such data or models trained on such data is legally sensitive.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="501" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
<span class="ltx_text ltx_font_bold" id="S1.F1.6.1">Overview of our proposed framework.</span>
In our framework, we first pre-train a Masked Autoencoder (MAE) using synthetic patterns,
and then finetune its encoder part for downstream audio tasks.
This approach eliminates the need for real data during pre-training.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">One approach to solving these issues is
to synthesize realistic data.
In the audio domain,
while some works have proposed utilizing text-to-speech systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib12" title="">12</a>]</cite>,
others have suggested training models
with sounds generated by synthesizers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib14" title="">14</a>]</cite>.
However, many of these approaches still rely on real data
and have subpar performance when using synthetic data alone
primarily due to a lack of diversity.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">As an alternative approach in computer vision,
existing works have proposed pre-training models with synthetic visual patterns <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib18" title="">18</a>]</cite>.
These methods have been reported to demonstrate high performance in image recognition.
Moreover, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib19" title="">19</a>]</cite> has shown that VideoMAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib20" title="">20</a>]</cite>,
a Masked Autoencoder (MAE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib21" title="">21</a>]</cite> for videos,
can learn spatiotemporal features from videos generated from synthetic images,
although these videos do not contain any humans or realistic objects.
This result implies that MAEs learn domain-agnostic, low-level features like patterns and structures
rather than high-level semantic features such as portrayed objects or actions.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">In this paper, inspired by these methods,
we propose to pre-train audio encoders using synthetic patterns,
addressing issues related to privacy and licensing during audio pre-training.
Our framework (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_tag">1</span></a>) combines two key elements.
The first is an MAE, which is trained to reconstruct the whole input from randomly masked counterparts.
Since MAEs tend to focus on low-level information like visual patterns and regularities within an input,
it is not important to what is portrayed in the input,
whether it be real images, real audio mel-spectrograms, even or synthetic patterns.
This leads to the second key element, which is synthetic data.
Synthetic data, unlike real images and real audio, is free from concerns about privacy and licensing.
By combining MAEs and synthetic patterns,
our framework enables the model to learn transferable feature representations without real audio,
performing well on various audio downstream tasks.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">To demonstrate the efficacy of our framework,
we conduct extensive experiments on a total of 13 audio tasks.
In the experiments, we utilize 17 existing synthetic images as synthetic patterns
and evaluate which types of synthetic patterns are effective for audio.
Our experimental results demonstrate that our framework
achieves performance comparable to those pre-trained with AudioSet-2M
and partially surpasses other pre-training methods using images.
These findings suggest that our framework can be a solution to issues related to real data such as privacy and licensing during pre-training,
without impeding performance on audio tasks.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Proposed Framework</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">To address privacy and licensing concerns in pre-training audio encoders,
we propose pre-training an MAE with synthetic patterns
and then transferring it to audio tasks (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_tag">1</span></a>).
In this section, We first provide an overview of MAEs (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S2.SS1" title="II-A Masked Autoencoder ‣ II Proposed Framework ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a>),
and then describe the synthetic patterns used in our experiments (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S2.SS2" title="II-B Synthetic Patterns ‣ II Proposed Framework ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a>).
Finally, we explain how to transfer MAEs to audio data (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S2.SS3" title="II-C Transferring MAEs to Audio Tasks ‣ II Proposed Framework ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-C</span></span></a>).</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="291" id="S2.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
<span class="ltx_text ltx_font_bold" id="S2.F2.28.1">Examples of synthetic image dataset used in our work.</span>
Datasets (a-n) are proposed in  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib17" title="">17</a>]</cite>.
<span class="ltx_text" id="S2.F2.29.2" style="color:#FF0000;">(a-d)</span> Dead-leave models,
<span class="ltx_text" id="S2.F2.30.3" style="color:#0000FF;">(e-h)</span> Statistical image models,
<span class="ltx_text" id="S2.F2.31.4" style="color:#FF00FF;">(i-l)</span> StyleGAN-based models,
and <span class="ltx_text" id="S2.F2.32.5" style="color:#00FFFF;">(m-n)</span> Feature visualization.
Datasets <span class="ltx_text" id="S2.F2.33.6" style="color:#FF8000;">(o-q)</span> are large-scale synthetic datasets.
(o) Shaders1k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib18" title="">18</a>]</cite>, (p) FractalDB1k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib15" title="">15</a>]</cite>,
and (q) VisualAtom1k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib16" title="">16</a>]</cite>.
</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="593" id="S2.F3.sf1.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.sf1.4.1.1" style="font-size:113%;">(a)</span> </span><span class="ltx_text" id="S2.F3.sf1.5.2" style="font-size:113%;">Color entropy</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="593" id="S2.F3.sf2.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.sf2.4.1.1" style="font-size:113%;">(b)</span> </span><span class="ltx_text" id="S2.F3.sf2.5.2" style="font-size:113%;">Brightness entropy</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="593" id="S2.F3.sf3.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.sf3.4.1.1" style="font-size:113%;">(c)</span> </span><span class="ltx_text" id="S2.F3.sf3.5.2" style="font-size:113%;">Mean total variation</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="593" id="S2.F3.sf4.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.sf4.4.1.1" style="font-size:113%;">(d)</span> </span><span class="ltx_text" id="S2.F3.sf4.5.2" style="font-size:113%;">Mean edge density</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="593" id="S2.F3.sf5.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.sf5.4.1.1" style="font-size:113%;">(e)</span> </span><span class="ltx_text" id="S2.F3.sf5.5.2" style="font-size:113%;">Accuracy on CIFAR100</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="35" id="S2.F3.1.g1" src="x8.png" width="830"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.10.2.1" style="font-size:113%;">Figure 3</span>: </span><span class="ltx_text" id="S2.F3.3.1" style="font-size:113%;">
<span class="ltx_text ltx_font_bold" id="S2.F3.3.1.1">Correlation between synthetic image properties and performance on ESC-50 fold 5.</span>
Note that we use only small-scale datasets (a-n) for calculating the correlation coefficient <math alttext="r" class="ltx_Math" display="inline" id="S2.F3.3.1.m1.1"><semantics id="S2.F3.3.1.m1.1b"><mi id="S2.F3.3.1.m1.1.1" xref="S2.F3.3.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.F3.3.1.m1.1c"><ci id="S2.F3.3.1.m1.1.1.cmml" xref="S2.F3.3.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.3.1.m1.1d">r</annotation><annotation encoding="application/x-llamapun" id="S2.F3.3.1.m1.1e">italic_r</annotation></semantics></math>.
</span></figcaption>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Masked Autoencoder</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Masked Autoencoder (MAE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib21" title="">21</a>]</cite> is a self-supervised learning framework,
where the transformer-based autoencoder aims to reconstruct an input from its masked version.
While MAE was originally developed for image recognition,
it has been successfully applied to audio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.5">The MAE pre-training process (illustrated in the upper half of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_tag">1</span></a>)
begins by splitting the input the input <math alttext="X\in\mathbb{R}^{C\times H\times W}" class="ltx_Math" display="inline" id="S2.SS1.p2.1.m1.1"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">X</mi><mo id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml"><mi id="S2.SS1.p2.1.m1.1.1.3.2" xref="S2.SS1.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS1.p2.1.m1.1.1.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.cmml"><mi id="S2.SS1.p2.1.m1.1.1.3.3.2" xref="S2.SS1.p2.1.m1.1.1.3.3.2.cmml">C</mi><mo id="S2.SS1.p2.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S2.SS1.p2.1.m1.1.1.3.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.3.cmml">H</mi><mo id="S2.SS1.p2.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S2.SS1.p2.1.m1.1.1.3.3.4" xref="S2.SS1.p2.1.m1.1.1.3.3.4.cmml">W</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><in id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1"></in><ci id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">𝑋</ci><apply id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.1.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3">superscript</csymbol><ci id="S2.SS1.p2.1.m1.1.1.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.2">ℝ</ci><apply id="S2.SS1.p2.1.m1.1.1.3.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3"><times id="S2.SS1.p2.1.m1.1.1.3.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3.1"></times><ci id="S2.SS1.p2.1.m1.1.1.3.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3.2">𝐶</ci><ci id="S2.SS1.p2.1.m1.1.1.3.3.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3.3">𝐻</ci><ci id="S2.SS1.p2.1.m1.1.1.3.3.4.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3.4">𝑊</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">X\in\mathbb{R}^{C\times H\times W}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.1.m1.1d">italic_X ∈ blackboard_R start_POSTSUPERSCRIPT italic_C × italic_H × italic_W end_POSTSUPERSCRIPT</annotation></semantics></math>
<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><math alttext="C" class="ltx_Math" display="inline" id="footnote1.m1.1"><semantics id="footnote1.m1.1b"><mi id="footnote1.m1.1.1" xref="footnote1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="footnote1.m1.1c"><ci id="footnote1.m1.1.1.cmml" xref="footnote1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m1.1d">C</annotation><annotation encoding="application/x-llamapun" id="footnote1.m1.1e">italic_C</annotation></semantics></math> represents the number of channels
and <math alttext="H" class="ltx_Math" display="inline" id="footnote1.m2.1"><semantics id="footnote1.m2.1b"><mi id="footnote1.m2.1.1" xref="footnote1.m2.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="footnote1.m2.1c"><ci id="footnote1.m2.1.1.cmml" xref="footnote1.m2.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m2.1d">H</annotation><annotation encoding="application/x-llamapun" id="footnote1.m2.1e">italic_H</annotation></semantics></math> and <math alttext="W" class="ltx_Math" display="inline" id="footnote1.m3.1"><semantics id="footnote1.m3.1b"><mi id="footnote1.m3.1.1" xref="footnote1.m3.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="footnote1.m3.1c"><ci id="footnote1.m3.1.1.cmml" xref="footnote1.m3.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m3.1d">W</annotation><annotation encoding="application/x-llamapun" id="footnote1.m3.1e">italic_W</annotation></semantics></math> each represent the height and width of the input.
</span></span></span>
into non-overlapping <math alttext="P\times P" class="ltx_Math" display="inline" id="S2.SS1.p2.2.m2.1"><semantics id="S2.SS1.p2.2.m2.1a"><mrow id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml"><mi id="S2.SS1.p2.2.m2.1.1.2" xref="S2.SS1.p2.2.m2.1.1.2.cmml">P</mi><mo id="S2.SS1.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p2.2.m2.1.1.1.cmml">×</mo><mi id="S2.SS1.p2.2.m2.1.1.3" xref="S2.SS1.p2.2.m2.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1"><times id="S2.SS1.p2.2.m2.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1.1"></times><ci id="S2.SS1.p2.2.m2.1.1.2.cmml" xref="S2.SS1.p2.2.m2.1.1.2">𝑃</ci><ci id="S2.SS1.p2.2.m2.1.1.3.cmml" xref="S2.SS1.p2.2.m2.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">P\times P</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.2.m2.1d">italic_P × italic_P</annotation></semantics></math> patches,
which are then linearly embedded to form <math alttext="X_{p}\in\mathbb{R}^{N\times D}" class="ltx_Math" display="inline" id="S2.SS1.p2.3.m3.1"><semantics id="S2.SS1.p2.3.m3.1a"><mrow id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml"><msub id="S2.SS1.p2.3.m3.1.1.2" xref="S2.SS1.p2.3.m3.1.1.2.cmml"><mi id="S2.SS1.p2.3.m3.1.1.2.2" xref="S2.SS1.p2.3.m3.1.1.2.2.cmml">X</mi><mi id="S2.SS1.p2.3.m3.1.1.2.3" xref="S2.SS1.p2.3.m3.1.1.2.3.cmml">p</mi></msub><mo id="S2.SS1.p2.3.m3.1.1.1" xref="S2.SS1.p2.3.m3.1.1.1.cmml">∈</mo><msup id="S2.SS1.p2.3.m3.1.1.3" xref="S2.SS1.p2.3.m3.1.1.3.cmml"><mi id="S2.SS1.p2.3.m3.1.1.3.2" xref="S2.SS1.p2.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS1.p2.3.m3.1.1.3.3" xref="S2.SS1.p2.3.m3.1.1.3.3.cmml"><mi id="S2.SS1.p2.3.m3.1.1.3.3.2" xref="S2.SS1.p2.3.m3.1.1.3.3.2.cmml">N</mi><mo id="S2.SS1.p2.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p2.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S2.SS1.p2.3.m3.1.1.3.3.3" xref="S2.SS1.p2.3.m3.1.1.3.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><apply id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1"><in id="S2.SS1.p2.3.m3.1.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1.1"></in><apply id="S2.SS1.p2.3.m3.1.1.2.cmml" xref="S2.SS1.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p2.3.m3.1.1.2.1.cmml" xref="S2.SS1.p2.3.m3.1.1.2">subscript</csymbol><ci id="S2.SS1.p2.3.m3.1.1.2.2.cmml" xref="S2.SS1.p2.3.m3.1.1.2.2">𝑋</ci><ci id="S2.SS1.p2.3.m3.1.1.2.3.cmml" xref="S2.SS1.p2.3.m3.1.1.2.3">𝑝</ci></apply><apply id="S2.SS1.p2.3.m3.1.1.3.cmml" xref="S2.SS1.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p2.3.m3.1.1.3.1.cmml" xref="S2.SS1.p2.3.m3.1.1.3">superscript</csymbol><ci id="S2.SS1.p2.3.m3.1.1.3.2.cmml" xref="S2.SS1.p2.3.m3.1.1.3.2">ℝ</ci><apply id="S2.SS1.p2.3.m3.1.1.3.3.cmml" xref="S2.SS1.p2.3.m3.1.1.3.3"><times id="S2.SS1.p2.3.m3.1.1.3.3.1.cmml" xref="S2.SS1.p2.3.m3.1.1.3.3.1"></times><ci id="S2.SS1.p2.3.m3.1.1.3.3.2.cmml" xref="S2.SS1.p2.3.m3.1.1.3.3.2">𝑁</ci><ci id="S2.SS1.p2.3.m3.1.1.3.3.3.cmml" xref="S2.SS1.p2.3.m3.1.1.3.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">X_{p}\in\mathbb{R}^{N\times D}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.3.m3.1d">italic_X start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>,
where <math alttext="N=\frac{HW}{P^{2}}" class="ltx_Math" display="inline" id="S2.SS1.p2.4.m4.1"><semantics id="S2.SS1.p2.4.m4.1a"><mrow id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml"><mi id="S2.SS1.p2.4.m4.1.1.2" xref="S2.SS1.p2.4.m4.1.1.2.cmml">N</mi><mo id="S2.SS1.p2.4.m4.1.1.1" xref="S2.SS1.p2.4.m4.1.1.1.cmml">=</mo><mfrac id="S2.SS1.p2.4.m4.1.1.3" xref="S2.SS1.p2.4.m4.1.1.3.cmml"><mrow id="S2.SS1.p2.4.m4.1.1.3.2" xref="S2.SS1.p2.4.m4.1.1.3.2.cmml"><mi id="S2.SS1.p2.4.m4.1.1.3.2.2" xref="S2.SS1.p2.4.m4.1.1.3.2.2.cmml">H</mi><mo id="S2.SS1.p2.4.m4.1.1.3.2.1" xref="S2.SS1.p2.4.m4.1.1.3.2.1.cmml">⁢</mo><mi id="S2.SS1.p2.4.m4.1.1.3.2.3" xref="S2.SS1.p2.4.m4.1.1.3.2.3.cmml">W</mi></mrow><msup id="S2.SS1.p2.4.m4.1.1.3.3" xref="S2.SS1.p2.4.m4.1.1.3.3.cmml"><mi id="S2.SS1.p2.4.m4.1.1.3.3.2" xref="S2.SS1.p2.4.m4.1.1.3.3.2.cmml">P</mi><mn id="S2.SS1.p2.4.m4.1.1.3.3.3" xref="S2.SS1.p2.4.m4.1.1.3.3.3.cmml">2</mn></msup></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><apply id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1"><eq id="S2.SS1.p2.4.m4.1.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1.1"></eq><ci id="S2.SS1.p2.4.m4.1.1.2.cmml" xref="S2.SS1.p2.4.m4.1.1.2">𝑁</ci><apply id="S2.SS1.p2.4.m4.1.1.3.cmml" xref="S2.SS1.p2.4.m4.1.1.3"><divide id="S2.SS1.p2.4.m4.1.1.3.1.cmml" xref="S2.SS1.p2.4.m4.1.1.3"></divide><apply id="S2.SS1.p2.4.m4.1.1.3.2.cmml" xref="S2.SS1.p2.4.m4.1.1.3.2"><times id="S2.SS1.p2.4.m4.1.1.3.2.1.cmml" xref="S2.SS1.p2.4.m4.1.1.3.2.1"></times><ci id="S2.SS1.p2.4.m4.1.1.3.2.2.cmml" xref="S2.SS1.p2.4.m4.1.1.3.2.2">𝐻</ci><ci id="S2.SS1.p2.4.m4.1.1.3.2.3.cmml" xref="S2.SS1.p2.4.m4.1.1.3.2.3">𝑊</ci></apply><apply id="S2.SS1.p2.4.m4.1.1.3.3.cmml" xref="S2.SS1.p2.4.m4.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS1.p2.4.m4.1.1.3.3.1.cmml" xref="S2.SS1.p2.4.m4.1.1.3.3">superscript</csymbol><ci id="S2.SS1.p2.4.m4.1.1.3.3.2.cmml" xref="S2.SS1.p2.4.m4.1.1.3.3.2">𝑃</ci><cn id="S2.SS1.p2.4.m4.1.1.3.3.3.cmml" type="integer" xref="S2.SS1.p2.4.m4.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">N=\frac{HW}{P^{2}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.4.m4.1d">italic_N = divide start_ARG italic_H italic_W end_ARG start_ARG italic_P start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG</annotation></semantics></math> is the number of patches and <math alttext="D" class="ltx_Math" display="inline" id="S2.SS1.p2.5.m5.1"><semantics id="S2.SS1.p2.5.m5.1a"><mi id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b"><ci id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">D</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.5.m5.1d">italic_D</annotation></semantics></math> is the embedding dimension.
Subsequently, a high proportion of patches (e.g., 75%) are randomly masked out
using a binary mask, producing visible embeddings.
These embeddings are then fed into a transformer-based autoencoder,
which is trained to reconstruct the original pixel values using mean squared error as the loss function.
After pre-training, the encoder part of the MAE, which is equivalent to a Vision Transformer (ViT),
will be fine-tuned for downstream tasks (in the lower part of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">While existing MAE approaches typically use real-world data,
 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib19" title="">19</a>]</cite> has demonstrated that
VideoMAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib20" title="">20</a>]</cite> can learn spatiotemporal feature representations
for action recognition, from synthetic videos which do not contain humans or objects.
This suggests that MAEs focus on low-level information (e.g. visual patterns and regularities)
and it is unimportant what is portrayed in the input, whether it be images, audio mel-spectrograms, or even synthetic patterns.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">Inspired by this finding, we propose pre-training MAEs with synthetic patterns,
and then transfering them to audio tasks.
This approach can eliminate the need for real audio data during pre-training,
therefore alleviating issues related to real data like privacy and license infringement.
In the following section, we will elaborate on the synthetic patterns that we use in this study.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Synthetic Patterns</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">While various types of synthetic patterns can be used for training MAEs,
it is challenging to explore effective synthetic patterns for MAEs without any constraints.
Therefore, in this study, we focus on synthetic images as one form of synthetic patterns.
Specifically, we utilize 17 synthetic image datasets proposed in computer vision,
as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S2.F2" title="Figure 2 ‣ II Proposed Framework ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Datasets labeled (a-n) contain 100k images, respectively,
and are generated from structured noise <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib17" title="">17</a>]</cite>.
We use these datasets to investigate which types of synthetic patterns
are beneficial for MAE training.
We also use large-scale synthetic image datasets labeled (o-q)
for comparison with existing pre-training methods.
FractalDB1k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib15" title="">15</a>]</cite> and
VisualAtom1k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib16" title="">16</a>]</cite>
are generated from mathematical formulas,
while Shaders1k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib18" title="">18</a>]</cite> is synthesized with OpenGL fragment shaders.
Each of these large-scale datasets includes over 1M images.
Pre-training on these datasets achieves performance on image classification tasks
comparable to that achieved by pre-training on large-scale real image datasets such as ImageNet.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.4.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.5.2">Transferring MAEs to Audio Tasks</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">When we pre-train MAEs with synthetic images and transfer them to audio tasks,
we need to modify its encoder part (i.e., Vision Transformer; ViT)
because the input shape is different between ViTs and Audio Spectrogram Transformers (ASTs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib4" title="">4</a>]</cite>.
We make the following modifications:</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p2.1.1">Patch Embedding:</span>
The input dimension <math alttext="C" class="ltx_Math" display="inline" id="S2.SS3.p2.1.m1.1"><semantics id="S2.SS3.p2.1.m1.1a"><mi id="S2.SS3.p2.1.m1.1.1" xref="S2.SS3.p2.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.1.m1.1b"><ci id="S2.SS3.p2.1.m1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.1.m1.1c">C</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.1.m1.1d">italic_C</annotation></semantics></math> differs between ViTs (3 channels) and ASTs (1 channel).
To enable RGB-image-based ViTs to handle 1-channel inputs,
we sum the weights of the patch embedding along the channel dimension.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p3.1.1">Positional encoding:</span>
The input size of ViTs is typically different from that of ASTs,
while the patch size is the same.
To address this, we directly modify the positional encoding.
Both ViTs and ASTs use sinusoidal positional encoding,
so we simply replace the positional encoding of ViTs with that of ASTs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Experiments</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">To evaluate the efficacy of our framework,
we pre-train MAEs using synthetic images
and fine-tune them on various audio tasks.
We provide the details of the experimental setting as follows.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.p2.1.1">Datasets: </span>
For pre-training, we use 17 synthetic image datasets
described in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S2.SS2" title="II-B Synthetic Patterns ‣ II Proposed Framework ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a>.
For downstream tasks, we use a total of 12 datasets.
Following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib6" title="">6</a>]</cite>,
we evaluate our framework on
AudioSet-20k/2M (AS-20k/2M) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib8" title="">8</a>]</cite>,
ESC50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib22" title="">22</a>]</cite>,
DCASE2019 task1A dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib23" title="">23</a>]</cite>,
OpenMIC2018 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib24" title="">24</a>]</cite>,
and Speech Command V2 (SCV2) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib25" title="">25</a>]</cite>.
Additionally, following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib14" title="">14</a>]</cite>,
we conduct experiments on selected tasks
from HEAR benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib26" title="">26</a>]</cite> and ARCH benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib27" title="">27</a>]</cite>:
UrbanSound 8K (US8K) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib28" title="">28</a>]</cite>,
Variably Intense Vocalizations of Affect and Emotion dataset (VIVAE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib29" title="">29</a>]</cite>,
NSynth Pitch 5h dataset (NSynth) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib13" title="">13</a>]</cite>,
CREMA-D (C-D) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib30" title="">30</a>]</cite>,
FSD50k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib31" title="">31</a>]</cite>,
Vocal Imitations dataset (VI) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib32" title="">32</a>]</cite>,
and LibriCount dataset (LCount) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib33" title="">33</a>]</cite>.
We report mean average precision (mAP) for AS-20k/2M, OpenMIC2018, FSD50k, and VI,
while we use accuracy as an evaluation metric for other datasets.
For datasets with multi-fold splits,
we conduct cross-validation and report the average metric.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.p3.1.1">Implementation Details: </span>
We conducted pre-training MAEs following the setting of  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib21" title="">21</a>]</cite>,
with a mask ratio of 0.75 and the number of epochs set to 800.
For fine-tuning, our experiments are mainly based on MaskSpec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib6" title="">6</a>]</cite>
due to high reproducibility.
For the model architecture,
We adopted a vanilla ViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib2" title="">2</a>]</cite>
with non-overlapped patches as the backbone, especially the ViT-Base variant.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Properties of synthetic images</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">First, we used small-scale synthetic image datasets (labeled a-n in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S2.F2" title="Figure 2 ‣ II Proposed Framework ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_tag">2</span></a>)
to investigate which types of synthetic images are effective for our framework.
We examined the Pearson correlation coefficient <math alttext="r" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">r</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_r</annotation></semantics></math>
between the following four properties of the datasets
and their performance on ESC-50 fold5 (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S2.F3" title="Figure 3 ‣ II Proposed Framework ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">Color Entropy:</span>
To measure the diversity of colors in images, we calculated color histograms for each image and computed the average entropy of these histograms.
Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S2.F3.sf1" title="In Figure 3 ‣ II Proposed Framework ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_tag">3(a)</span></a> shows that color diversity has little correlation with performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Brightness Entropy:</span>
We converted images to grayscale, calculated brightness histograms, and computed their average entropy.
We found there is a weak correlation with performance (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S2.F3.sf2" title="In Figure 3 ‣ II Proposed Framework ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_tag">3(b)</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">Total Variation:</span>
Total Variation (TV) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib34" title="">34</a>]</cite> is typically used as regularization
in image restoration and denoising.
Here, we adopt TV as a metric to evaluate how much noise is in the images
We calculated the sum of TV for each image and reported the average value.
We observed a negative correlation with performance (<math alttext="r=-0.4" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.1"><semantics id="S3.SS1.p4.1.m1.1a"><mrow id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">r</mi><mo id="S3.SS1.p4.1.m1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml"><mo id="S3.SS1.p4.1.m1.1.1.3a" xref="S3.SS1.p4.1.m1.1.1.3.cmml">−</mo><mn id="S3.SS1.p4.1.m1.1.1.3.2" xref="S3.SS1.p4.1.m1.1.1.3.2.cmml">0.4</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><eq id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1"></eq><ci id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2">𝑟</ci><apply id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3"><minus id="S3.SS1.p4.1.m1.1.1.3.1.cmml" xref="S3.SS1.p4.1.m1.1.1.3"></minus><cn id="S3.SS1.p4.1.m1.1.1.3.2.cmml" type="float" xref="S3.SS1.p4.1.m1.1.1.3.2">0.4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">r=-0.4</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.1d">italic_r = - 0.4</annotation></semantics></math>).
This suggests that image datasets with fewer noises and sharp changes of textures
can be more effective for transferring to audio tasks.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.24.2.1" style="font-size:113%;">TABLE I</span>: </span><span class="ltx_text" id="S3.T1.2.1" style="font-size:113%;">
<span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1">Comparison of transferability of image-based pre-training methods.</span>
SL = Supervised Learning, FDSL = Formula-Driven Supervised Learning,
IN-1k = ImageNet1k, exF21k = exFractal21k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib35" title="">35</a>]</cite>
VA = VisualAtom <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib16" title="">16</a>]</cite>.
<sup class="ltx_sup" id="S3.T1.2.1.2">∗</sup> indicates results from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib4" title="">4</a>]</cite>.
Note that we are missing 30k data in the balanced and the eval set in AS-20k.
Therefore there is a performance gap between ours and ImageNet1k SL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib4" title="">4</a>]</cite>.
However, the gap narrows when the same data is used.
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.4.3.1">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T1.4.3.1.1"><span class="ltx_text" id="S3.T1.4.3.1.1.1" style="font-size:80%;">Pre-training setting</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T1.4.3.1.2"><span class="ltx_text" id="S3.T1.4.3.1.2.1" style="font-size:80%;">Downstream tasks</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.4.2">
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.2.1"><span class="ltx_text" id="S3.T1.4.4.2.1.1" style="font-size:80%;">Method</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.2.2"><span class="ltx_text" id="S3.T1.4.4.2.2.1" style="font-size:80%;">Dataset</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.2.3"><span class="ltx_text" id="S3.T1.4.4.2.3.1" style="font-size:80%;">Labels</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.2.4"><span class="ltx_text" id="S3.T1.4.4.2.4.1" style="font-size:80%;">AS-20k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.2.5"><span class="ltx_text" id="S3.T1.4.4.2.5.1" style="font-size:80%;">ESC50</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.2.6"><span class="ltx_text" id="S3.T1.4.4.2.6.1" style="font-size:80%;">IN-1k</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.1">
<span class="ltx_text" id="S3.T1.3.1.1.1" style="font-size:80%;">from scratch</span><sup class="ltx_sup" id="S3.T1.3.1.1.2"><span class="ltx_text" id="S3.T1.3.1.1.2.1" style="font-size:80%;">∗</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.2"><span class="ltx_text" id="S3.T1.3.1.2.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_border_t" id="S3.T1.3.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.4"><span class="ltx_text" id="S3.T1.3.1.4.1" style="font-size:80%;">0.148</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.5"><span class="ltx_text" id="S3.T1.3.1.5.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.1.6"><span class="ltx_text" id="S3.T1.3.1.6.1" style="font-size:80%;">77.9</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.2">
<td class="ltx_td ltx_align_center" id="S3.T1.4.2.1">
<span class="ltx_text" id="S3.T1.4.2.1.1" style="font-size:80%;">SL</span><sup class="ltx_sup" id="S3.T1.4.2.1.2"><span class="ltx_text" id="S3.T1.4.2.1.2.1" style="font-size:80%;">∗</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.2.2"><span class="ltx_text" id="S3.T1.4.2.2.1" style="font-size:80%;">IN-1k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.2.3"><span class="ltx_text" id="S3.T1.4.2.3.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.2.4"><span class="ltx_text" id="S3.T1.4.2.4.1" style="font-size:80%;">0.347</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.2.5"><span class="ltx_text" id="S3.T1.4.2.5.1" style="font-size:80%;">0.887</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.2.6"><span class="ltx_text" id="S3.T1.4.2.6.1" style="font-size:80%;">83.4</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.5.3">
<td class="ltx_td ltx_align_center" id="S3.T1.4.5.3.1"><span class="ltx_text" id="S3.T1.4.5.3.1.1" style="font-size:80%;">SL</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.5.3.2"><span class="ltx_text" id="S3.T1.4.5.3.2.1" style="font-size:80%;">IN-1k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.5.3.3"><span class="ltx_text" id="S3.T1.4.5.3.3.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.5.3.4"><span class="ltx_text" id="S3.T1.4.5.3.4.1" style="font-size:80%;">0.290</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.5.3.5"><span class="ltx_text" id="S3.T1.4.5.3.5.1" style="font-size:80%;">0.870</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.5.3.6"><span class="ltx_text" id="S3.T1.4.5.3.6.1" style="font-size:80%;">83.4</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.6.4">
<td class="ltx_td ltx_align_center" id="S3.T1.4.6.4.1"><span class="ltx_text" id="S3.T1.4.6.4.1.1" style="font-size:80%;">FDSL</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.6.4.2"><span class="ltx_text" id="S3.T1.4.6.4.2.1" style="font-size:80%;">exF-21k</span></td>
<td class="ltx_td" id="S3.T1.4.6.4.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.6.4.4"><span class="ltx_text" id="S3.T1.4.6.4.4.1" style="font-size:80%;">0.236</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.6.4.5"><span class="ltx_text" id="S3.T1.4.6.4.5.1" style="font-size:80%;">0.837</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.6.4.6"><span class="ltx_text" id="S3.T1.4.6.4.6.1" style="font-size:80%;">82.7</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.7.5">
<td class="ltx_td ltx_align_center" id="S3.T1.4.7.5.1"><span class="ltx_text" id="S3.T1.4.7.5.1.1" style="font-size:80%;">FDSL</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.7.5.2"><span class="ltx_text" id="S3.T1.4.7.5.2.1" style="font-size:80%;">VA-21k</span></td>
<td class="ltx_td" id="S3.T1.4.7.5.3"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.7.5.4"><span class="ltx_text" id="S3.T1.4.7.5.4.1" style="font-size:80%;">0.172</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.7.5.5"><span class="ltx_text" id="S3.T1.4.7.5.5.1" style="font-size:80%;">0.681</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.7.5.6"><span class="ltx_text" id="S3.T1.4.7.5.6.1" style="font-size:80%;">83.7</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.8.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.4.8.6.1" rowspan="4"><span class="ltx_text" id="S3.T1.4.8.6.1.1" style="font-size:80%;">MAE</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.8.6.2"><span class="ltx_text" id="S3.T1.4.8.6.2.1" style="font-size:80%;">IN-1k</span></td>
<td class="ltx_td ltx_border_t" id="S3.T1.4.8.6.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.8.6.4"><span class="ltx_text" id="S3.T1.4.8.6.4.1" style="font-size:80%;">0.262</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.8.6.5"><span class="ltx_text" id="S3.T1.4.8.6.5.1" style="font-size:80%;">0.858</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.8.6.6"><span class="ltx_text" id="S3.T1.4.8.6.6.1" style="font-size:80%;">83.6</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.9.7">
<td class="ltx_td ltx_align_center" id="S3.T1.4.9.7.1"><span class="ltx_text" id="S3.T1.4.9.7.1.1" style="font-size:80%;">Shaders1k</span></td>
<td class="ltx_td" id="S3.T1.4.9.7.2"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.9.7.3"><span class="ltx_text" id="S3.T1.4.9.7.3.1" style="font-size:80%;">0.274</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.9.7.4"><span class="ltx_text" id="S3.T1.4.9.7.4.1" style="font-size:80%;">0.873</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.9.7.5"><span class="ltx_text" id="S3.T1.4.9.7.5.1" style="font-size:80%;">82.1</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.10.8">
<td class="ltx_td ltx_align_center" id="S3.T1.4.10.8.1"><span class="ltx_text" id="S3.T1.4.10.8.1.1" style="font-size:80%;">FractalDB1k</span></td>
<td class="ltx_td" id="S3.T1.4.10.8.2"></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.10.8.3"><span class="ltx_text" id="S3.T1.4.10.8.3.1" style="font-size:80%;">0.067</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.10.8.4"><span class="ltx_text" id="S3.T1.4.10.8.4.1" style="font-size:80%;">0.136</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.10.8.5"><span class="ltx_text" id="S3.T1.4.10.8.5.1" style="font-size:80%;">77.1</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.11.9">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.4.11.9.1"><span class="ltx_text" id="S3.T1.4.11.9.1.1" style="font-size:80%;">VA-1k</span></td>
<td class="ltx_td ltx_border_bb" id="S3.T1.4.11.9.2"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.4.11.9.3"><span class="ltx_text" id="S3.T1.4.11.9.3.1" style="font-size:80%;">0.110</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.4.11.9.4"><span class="ltx_text" id="S3.T1.4.11.9.4.1" style="font-size:80%;">0.795</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.4.11.9.5"><span class="ltx_text" id="S3.T1.4.11.9.5.1" style="font-size:80%;">79.9</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.1">Edge Density:</span>
Using the Canny edge detector <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib36" title="">36</a>]</cite>,
we calculated the average ratio of edge pixels in the images.
As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S2.F3.sf4" title="In Figure 3 ‣ II Proposed Framework ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_tag">3(d)</span></a>, we found that edge density showed less correlation than TV.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">CIFAR100 Performance:</span>
We also evaluate each model on the image classification task with CIFAR100 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib37" title="">37</a>]</cite>.
Note that when we fine-tune the MAE pre-trained on AudioSet2M,
we inflate the weights of patch embedding along the channel dimension to handle RGB images.
As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S2.F3.sf5" title="In Figure 3 ‣ II Proposed Framework ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_tag">3(e)</span></a>,
models that perform well on CIFAR100 also tend to perform well on ESC50, and vice versa (<math alttext="r=0.5" class="ltx_Math" display="inline" id="S3.SS1.p6.1.m1.1"><semantics id="S3.SS1.p6.1.m1.1a"><mrow id="S3.SS1.p6.1.m1.1.1" xref="S3.SS1.p6.1.m1.1.1.cmml"><mi id="S3.SS1.p6.1.m1.1.1.2" xref="S3.SS1.p6.1.m1.1.1.2.cmml">r</mi><mo id="S3.SS1.p6.1.m1.1.1.1" xref="S3.SS1.p6.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS1.p6.1.m1.1.1.3" xref="S3.SS1.p6.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.1b"><apply id="S3.SS1.p6.1.m1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1"><eq id="S3.SS1.p6.1.m1.1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1.1"></eq><ci id="S3.SS1.p6.1.m1.1.1.2.cmml" xref="S3.SS1.p6.1.m1.1.1.2">𝑟</ci><cn id="S3.SS1.p6.1.m1.1.1.3.cmml" type="float" xref="S3.SS1.p6.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.1c">r=0.5</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.1.m1.1d">italic_r = 0.5</annotation></semantics></math>).
Notably, the model pre-trained on AudioSet2M struggles with the image classification tasks.
This suggests that it is difficult to transfer audio models to image tasks,
while image models tend to learn feature representations that are transferable to audio tasks.</p>
</div>
<div class="ltx_para" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.1">These trends were also observed in large-scale datasets.
Notably, ImageNet and Shaders, which have lower average TV values,
demonstrated higher performance compared to VisualAtom and FractalDB,
which contain noises and complex edges.</p>
</div>
<div class="ltx_para" id="S3.SS1.p8">
<p class="ltx_p" id="S3.SS1.p8.1">Based on these observations,
we can conclude images with lower TV values are more effective for MAE pre-training with synthetic patterns.
In other words, synthetic patterns should have less noise and smoother changes in textures for efficient MAE pre-training.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T2.6.1.1" style="font-size:113%;">TABLE II</span>: </span><span class="ltx_text" id="S3.T2.7.2" style="font-size:113%;">
<span class="ltx_text ltx_font_bold" id="S3.T2.7.2.1">Comparison with other existing methods.</span>
IN-1k = ImageNet1k, AS = AudioSet.
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.8">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.8.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.8.1.1.1" rowspan="2"><span class="ltx_text" id="S3.T2.8.1.1.1.1" style="font-size:80%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S3.T2.8.1.1.2"><span class="ltx_text" id="S3.T2.8.1.1.2.1" style="font-size:80%;">Pre-training Setting</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="6" id="S3.T2.8.1.1.3"><span class="ltx_text" id="S3.T2.8.1.1.3.1" style="font-size:80%;">Downstream Tasks</span></th>
</tr>
<tr class="ltx_tr" id="S3.T2.8.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.8.2.2.1"><span class="ltx_text" id="S3.T2.8.2.2.1.1" style="font-size:80%;">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.8.2.2.2"><span class="ltx_text" id="S3.T2.8.2.2.2.1" style="font-size:80%;">Real data</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.8.2.2.3"><span class="ltx_text" id="S3.T2.8.2.2.3.1" style="font-size:80%;">Label</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.8.2.2.4"><span class="ltx_text" id="S3.T2.8.2.2.4.1" style="font-size:80%;">AS-2M</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.8.2.2.5"><span class="ltx_text" id="S3.T2.8.2.2.5.1" style="font-size:80%;">AS-20k</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.8.2.2.6"><span class="ltx_text" id="S3.T2.8.2.2.6.1" style="font-size:80%;">ESC50</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.8.2.2.7"><span class="ltx_text" id="S3.T2.8.2.2.7.1" style="font-size:80%;">DCASE2019</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.8.2.2.8"><span class="ltx_text" id="S3.T2.8.2.2.8.1" style="font-size:80%;">OpenMIC18</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.8.2.2.9"><span class="ltx_text" id="S3.T2.8.2.2.9.1" style="font-size:80%;">SCV2</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.8.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.8.3.1.1">
<span class="ltx_text" id="S3.T2.8.3.1.1.1" style="font-size:80%;">CNN14 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T2.8.3.1.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib38" title="">38</a><span class="ltx_text" id="S3.T2.8.3.1.1.3.2" style="font-size:80%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.8.3.1.2"><span class="ltx_text" id="S3.T2.8.3.1.2.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.8.3.1.3"><span class="ltx_text" id="S3.T2.8.3.1.3.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.8.3.1.4"><span class="ltx_text" id="S3.T2.8.3.1.4.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.8.3.1.5"><span class="ltx_text" id="S3.T2.8.3.1.5.1" style="font-size:80%;">0.431</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.8.3.1.6"><span class="ltx_text" id="S3.T2.8.3.1.6.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.8.3.1.7"><span class="ltx_text" id="S3.T2.8.3.1.7.1" style="font-size:80%;">0.833</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.8.3.1.8"><span class="ltx_text" id="S3.T2.8.3.1.8.1" style="font-size:80%;">0.691</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.8.3.1.9"><span class="ltx_text" id="S3.T2.8.3.1.9.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.8.3.1.10"><span class="ltx_text" id="S3.T2.8.3.1.10.1" style="font-size:80%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.8.4.2">
<td class="ltx_td ltx_align_center" id="S3.T2.8.4.2.1">
<span class="ltx_text" id="S3.T2.8.4.2.1.1" style="font-size:80%;">CNN14 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T2.8.4.2.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib38" title="">38</a><span class="ltx_text" id="S3.T2.8.4.2.1.3.2" style="font-size:80%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.4.2.2"><span class="ltx_text" id="S3.T2.8.4.2.2.1" style="font-size:80%;">AS</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.4.2.3"><span class="ltx_text" id="S3.T2.8.4.2.3.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.4.2.4"><span class="ltx_text" id="S3.T2.8.4.2.4.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.4.2.5"><span class="ltx_text" id="S3.T2.8.4.2.5.1" style="font-size:80%;">0.431</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.4.2.6"><span class="ltx_text" id="S3.T2.8.4.2.6.1" style="font-size:80%;">0.278</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.4.2.7"><span class="ltx_text" id="S3.T2.8.4.2.7.1" style="font-size:80%;">0.947</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.4.2.8"><span class="ltx_text" id="S3.T2.8.4.2.8.1" style="font-size:80%;">0.764</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.4.2.9"><span class="ltx_text" id="S3.T2.8.4.2.9.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.4.2.10"><span class="ltx_text" id="S3.T2.8.4.2.10.1" style="font-size:80%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.8.5.3">
<td class="ltx_td ltx_align_center" id="S3.T2.8.5.3.1">
<span class="ltx_text" id="S3.T2.8.5.3.1.1" style="font-size:80%;">PSLA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T2.8.5.3.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib39" title="">39</a><span class="ltx_text" id="S3.T2.8.5.3.1.3.2" style="font-size:80%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.5.3.2"><span class="ltx_text" id="S3.T2.8.5.3.2.1" style="font-size:80%;">AS</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.5.3.3"><span class="ltx_text" id="S3.T2.8.5.3.3.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.5.3.4"><span class="ltx_text" id="S3.T2.8.5.3.4.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.5.3.5"><span class="ltx_text" id="S3.T2.8.5.3.5.1" style="font-size:80%;">0.443</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.5.3.6"><span class="ltx_text" id="S3.T2.8.5.3.6.1" style="font-size:80%;">0.319</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.5.3.7"><span class="ltx_text" id="S3.T2.8.5.3.7.1" style="font-size:80%;">0.877</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.5.3.8"><span class="ltx_text" id="S3.T2.8.5.3.8.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.5.3.9"><span class="ltx_text" id="S3.T2.8.5.3.9.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.5.3.10"><span class="ltx_text" id="S3.T2.8.5.3.10.1" style="font-size:80%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.8.6.4">
<td class="ltx_td ltx_align_center" id="S3.T2.8.6.4.1">
<span class="ltx_text" id="S3.T2.8.6.4.1.1" style="font-size:80%;">AST </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T2.8.6.4.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib4" title="">4</a><span class="ltx_text" id="S3.T2.8.6.4.1.3.2" style="font-size:80%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.6.4.2"><span class="ltx_text" id="S3.T2.8.6.4.2.1" style="font-size:80%;">IN-1k + AS</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.6.4.3"><span class="ltx_text" id="S3.T2.8.6.4.3.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.6.4.4"><span class="ltx_text" id="S3.T2.8.6.4.4.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.6.4.5"><span class="ltx_text" id="S3.T2.8.6.4.5.1" style="font-size:80%;">0.457</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.6.4.6"><span class="ltx_text" id="S3.T2.8.6.4.6.1" style="font-size:80%;">0.347</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.6.4.7"><span class="ltx_text" id="S3.T2.8.6.4.7.1" style="font-size:80%;">0.956</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.6.4.8"><span class="ltx_text" id="S3.T2.8.6.4.8.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.6.4.9"><span class="ltx_text" id="S3.T2.8.6.4.9.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.6.4.10"><span class="ltx_text" id="S3.T2.8.6.4.10.1" style="font-size:80%;">0.981</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.8.7.5">
<td class="ltx_td ltx_align_center" id="S3.T2.8.7.5.1">
<span class="ltx_text" id="S3.T2.8.7.5.1.1" style="font-size:80%;">PaSST </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T2.8.7.5.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib40" title="">40</a><span class="ltx_text" id="S3.T2.8.7.5.1.3.2" style="font-size:80%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.7.5.2"><span class="ltx_text" id="S3.T2.8.7.5.2.1" style="font-size:80%;">IN-1k + AS</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.7.5.3"><span class="ltx_text" id="S3.T2.8.7.5.3.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.7.5.4"><span class="ltx_text" id="S3.T2.8.7.5.4.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.7.5.5"><span class="ltx_text" id="S3.T2.8.7.5.5.1" style="font-size:80%;">0.471</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.7.5.6"><span class="ltx_text" id="S3.T2.8.7.5.6.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.7.5.7"><span class="ltx_text" id="S3.T2.8.7.5.7.1" style="font-size:80%;">0.968</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.7.5.8"><span class="ltx_text" id="S3.T2.8.7.5.8.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.7.5.9"><span class="ltx_text" id="S3.T2.8.7.5.9.1" style="font-size:80%;">0.843</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.7.5.10"><span class="ltx_text" id="S3.T2.8.7.5.10.1" style="font-size:80%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.8.8.6">
<td class="ltx_td ltx_align_center" id="S3.T2.8.8.6.1">
<span class="ltx_text" id="S3.T2.8.8.6.1.1" style="font-size:80%;">SSAST </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T2.8.8.6.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib41" title="">41</a><span class="ltx_text" id="S3.T2.8.8.6.1.3.2" style="font-size:80%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.8.6.2">
<span class="ltx_text" id="S3.T2.8.8.6.2.1" style="font-size:80%;">Librispeech </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T2.8.8.6.2.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib42" title="">42</a><span class="ltx_text" id="S3.T2.8.8.6.2.3.2" style="font-size:80%;">]</span></cite><span class="ltx_text" id="S3.T2.8.8.6.2.4" style="font-size:80%;"> + AS</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.8.6.3"><span class="ltx_text" id="S3.T2.8.8.6.3.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td" id="S3.T2.8.8.6.4"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.8.6.5"><span class="ltx_text" id="S3.T2.8.8.6.5.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.8.6.6"><span class="ltx_text" id="S3.T2.8.8.6.6.1" style="font-size:80%;">0.310</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.8.6.7"><span class="ltx_text" id="S3.T2.8.8.6.7.1" style="font-size:80%;">0.888</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.8.6.8"><span class="ltx_text" id="S3.T2.8.8.6.8.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.8.6.9"><span class="ltx_text" id="S3.T2.8.8.6.9.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.8.6.10"><span class="ltx_text" id="S3.T2.8.8.6.10.1" style="font-size:80%;">0.980</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.8.9.7">
<td class="ltx_td ltx_align_center" id="S3.T2.8.9.7.1">
<span class="ltx_text" id="S3.T2.8.9.7.1.1" style="font-size:80%;">MaskSpec </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T2.8.9.7.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib6" title="">6</a><span class="ltx_text" id="S3.T2.8.9.7.1.3.2" style="font-size:80%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.9.7.2"><span class="ltx_text" id="S3.T2.8.9.7.2.1" style="font-size:80%;">AS</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.9.7.3"><span class="ltx_text" id="S3.T2.8.9.7.3.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td" id="S3.T2.8.9.7.4"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.9.7.5"><span class="ltx_text" id="S3.T2.8.9.7.5.1" style="font-size:80%;">0.471</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.9.7.6"><span class="ltx_text" id="S3.T2.8.9.7.6.1" style="font-size:80%;">0.323</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.9.7.7"><span class="ltx_text" id="S3.T2.8.9.7.7.1" style="font-size:80%;">0.896</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.9.7.8"><span class="ltx_text" id="S3.T2.8.9.7.8.1" style="font-size:80%;">0.801</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.9.7.9"><span class="ltx_text" id="S3.T2.8.9.7.9.1" style="font-size:80%;">0.814</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.9.7.10"><span class="ltx_text" id="S3.T2.8.9.7.10.1" style="font-size:80%;">0.977</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.8.10.8">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.8.10.8.1"><span class="ltx_text" id="S3.T2.8.10.8.1.1" style="font-size:80%;">Ours</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.8.10.8.2"><span class="ltx_text" id="S3.T2.8.10.8.2.1" style="font-size:80%;">Shaders1k</span></td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="S3.T2.8.10.8.3"></td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="S3.T2.8.10.8.4"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.8.10.8.5"><span class="ltx_text" id="S3.T2.8.10.8.5.1" style="font-size:80%;">0.461</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.8.10.8.6"><span class="ltx_text" id="S3.T2.8.10.8.6.1" style="font-size:80%;">0.274</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.8.10.8.7"><span class="ltx_text" id="S3.T2.8.10.8.7.1" style="font-size:80%;">0.873</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.8.10.8.8"><span class="ltx_text" id="S3.T2.8.10.8.8.1" style="font-size:80%;">0.763</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.8.10.8.9"><span class="ltx_text" id="S3.T2.8.10.8.9.1" style="font-size:80%;">0.790</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.8.10.8.10"><span class="ltx_text" id="S3.T2.8.10.8.10.1" style="font-size:80%;">0.968</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.15.2.1" style="font-size:113%;">TABLE III</span>: </span><span class="ltx_text" id="S3.T3.2.1" style="font-size:113%;">
<span class="ltx_text ltx_font_bold" id="S3.T3.2.1.1">Evaluation on datasets from HEAR and ARCH benchmark.</span>
<sup class="ltx_sup" id="S3.T3.2.1.2">∗</sup> indicates results from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib14" title="">14</a>]</cite>.
SL = Supervised Learning.
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T3.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.4.3.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.4.3.1.1" rowspan="2"><span class="ltx_text" id="S3.T3.4.3.1.1.1" style="font-size:80%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T3.4.3.1.2"><span class="ltx_text" id="S3.T3.4.3.1.2.1" style="font-size:80%;">Pre-training Setting</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="8" id="S3.T3.4.3.1.3"><span class="ltx_text" id="S3.T3.4.3.1.3.1" style="font-size:80%;">Downstream Tasks</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4.2">
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.2.1"><span class="ltx_text" id="S3.T3.4.4.2.1.1" style="font-size:80%;">Modality</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.2.2"><span class="ltx_text" id="S3.T3.4.4.2.2.1" style="font-size:80%;">Real Data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.2.3"><span class="ltx_text" id="S3.T3.4.4.2.3.1" style="font-size:80%;">Label</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.2.4"><span class="ltx_text" id="S3.T3.4.4.2.4.1" style="font-size:80%;">ESC50</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.2.5"><span class="ltx_text" id="S3.T3.4.4.2.5.1" style="font-size:80%;">US8K</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.2.6"><span class="ltx_text" id="S3.T3.4.4.2.6.1" style="font-size:80%;">VIVAE</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.2.7"><span class="ltx_text" id="S3.T3.4.4.2.7.1" style="font-size:80%;">NSynth</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.2.8"><span class="ltx_text" id="S3.T3.4.4.2.8.1" style="font-size:80%;">C-D</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.2.9"><span class="ltx_text" id="S3.T3.4.4.2.9.1" style="font-size:80%;">FSD50k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.2.10"><span class="ltx_text" id="S3.T3.4.4.2.10.1" style="font-size:80%;">VI</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.2.11"><span class="ltx_text" id="S3.T3.4.4.2.11.1" style="font-size:80%;">LCount</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.5.3">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="12" id="S3.T3.4.5.3.1"><span class="ltx_text ltx_font_bold" id="S3.T3.4.5.3.1.1" style="font-size:80%;">Linear Probing</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.6.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.6.4.1">
<span class="ltx_text" id="S3.T3.4.6.4.1.1" style="font-size:80%;">MS-CLAP </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.4.6.4.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib43" title="">43</a><span class="ltx_text" id="S3.T3.4.6.4.1.3.2" style="font-size:80%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.6.4.2"><span class="ltx_text" id="S3.T3.4.6.4.2.1" style="font-size:80%;">audio-text</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.6.4.3"><span class="ltx_text" id="S3.T3.4.6.4.3.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.6.4.4"><span class="ltx_text" id="S3.T3.4.6.4.4.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.6.4.5"><span class="ltx_text" id="S3.T3.4.6.4.5.1" style="font-size:80%;">0.931</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.6.4.6"><span class="ltx_text" id="S3.T3.4.6.4.6.1" style="font-size:80%;">0.839</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.6.4.7"><span class="ltx_text" id="S3.T3.4.6.4.7.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.6.4.8"><span class="ltx_text" id="S3.T3.4.6.4.8.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.6.4.9"><span class="ltx_text" id="S3.T3.4.6.4.9.1" style="font-size:80%;">0.283</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.6.4.10"><span class="ltx_text" id="S3.T3.4.6.4.10.1" style="font-size:80%;">0.591</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.6.4.11"><span class="ltx_text" id="S3.T3.4.6.4.11.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.6.4.12"><span class="ltx_text" id="S3.T3.4.6.4.12.1" style="font-size:80%;">0.572</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.1">
<td class="ltx_td ltx_align_center" id="S3.T3.3.1.1">
<span class="ltx_text" id="S3.T3.3.1.1.1" style="font-size:80%;">VGGSound SL </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.3.1.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib9" title="">9</a><span class="ltx_text" id="S3.T3.3.1.1.3.2" style="font-size:80%;">]</span></cite><sup class="ltx_sup" id="S3.T3.3.1.1.4"><span class="ltx_text" id="S3.T3.3.1.1.4.1" style="font-size:80%;">∗</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.1.2"><span class="ltx_text" id="S3.T3.3.1.2.1" style="font-size:80%;">audio</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.1.3"><span class="ltx_text" id="S3.T3.3.1.3.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.1.4"><span class="ltx_text" id="S3.T3.3.1.4.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.1.5"><span class="ltx_text" id="S3.T3.3.1.5.1" style="font-size:80%;">0.875</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.1.6"><span class="ltx_text" id="S3.T3.3.1.6.1" style="font-size:80%;">0.776</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.1.7"><span class="ltx_text" id="S3.T3.3.1.7.1" style="font-size:80%;">0.394</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.1.8"><span class="ltx_text" id="S3.T3.3.1.8.1" style="font-size:80%;">0.438</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.1.9"><span class="ltx_text" id="S3.T3.3.1.9.1" style="font-size:80%;">0.544</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.1.10"><span class="ltx_text" id="S3.T3.3.1.10.1" style="font-size:80%;">0.438</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.1.11"><span class="ltx_text" id="S3.T3.3.1.11.1" style="font-size:80%;">0.141</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.1.12"><span class="ltx_text" id="S3.T3.3.1.12.1" style="font-size:80%;">0.561</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.2">
<td class="ltx_td ltx_align_center" id="S3.T3.4.2.1">
<span class="ltx_text" id="S3.T3.4.2.1.1" style="font-size:80%;">VGGSound SSL</span><sup class="ltx_sup" id="S3.T3.4.2.1.2"><span class="ltx_text" id="S3.T3.4.2.1.2.1" style="font-size:80%;">∗</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.2.2"><span class="ltx_text" id="S3.T3.4.2.2.1" style="font-size:80%;">audio</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.2.3"><span class="ltx_text" id="S3.T3.4.2.3.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td" id="S3.T3.4.2.4"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.2.5"><span class="ltx_text" id="S3.T3.4.2.5.1" style="font-size:80%;">0.530</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.2.6"><span class="ltx_text" id="S3.T3.4.2.6.1" style="font-size:80%;">0.638</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.2.7"><span class="ltx_text" id="S3.T3.4.2.7.1" style="font-size:80%;">0.381</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.2.8"><span class="ltx_text" id="S3.T3.4.2.8.1" style="font-size:80%;">0.142</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.2.9"><span class="ltx_text" id="S3.T3.4.2.9.1" style="font-size:80%;">0.500</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.2.10"><span class="ltx_text" id="S3.T3.4.2.10.1" style="font-size:80%;">0.240</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.2.11"><span class="ltx_text" id="S3.T3.4.2.11.1" style="font-size:80%;">0.343</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.2.12"><span class="ltx_text" id="S3.T3.4.2.12.1" style="font-size:80%;">0.698</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.7.5">
<td class="ltx_td ltx_align_center" id="S3.T3.4.7.5.1">
<span class="ltx_text" id="S3.T3.4.7.5.1.1" style="font-size:80%;">Audio Doppelgängers </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.4.7.5.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib14" title="">14</a><span class="ltx_text" id="S3.T3.4.7.5.1.3.2" style="font-size:80%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.7.5.2"><span class="ltx_text" id="S3.T3.4.7.5.2.1" style="font-size:80%;">audio</span></td>
<td class="ltx_td" id="S3.T3.4.7.5.3"></td>
<td class="ltx_td" id="S3.T3.4.7.5.4"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.7.5.5"><span class="ltx_text" id="S3.T3.4.7.5.5.1" style="font-size:80%;">0.589</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.7.5.6"><span class="ltx_text" id="S3.T3.4.7.5.6.1" style="font-size:80%;">0.667</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.7.5.7"><span class="ltx_text" id="S3.T3.4.7.5.7.1" style="font-size:80%;">0.395</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.7.5.8"><span class="ltx_text" id="S3.T3.4.7.5.8.1" style="font-size:80%;">0.444</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.7.5.9"><span class="ltx_text" id="S3.T3.4.7.5.9.1" style="font-size:80%;">0.484</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.7.5.10"><span class="ltx_text" id="S3.T3.4.7.5.10.1" style="font-size:80%;">0.241</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.7.5.11"><span class="ltx_text" id="S3.T3.4.7.5.11.1" style="font-size:80%;">0.915</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.7.5.12"><span class="ltx_text" id="S3.T3.4.7.5.12.1" style="font-size:80%;">0.586</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.8.6">
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.6.1">
<span class="ltx_text" id="S3.T3.4.8.6.1.1" style="font-size:80%;">MaskSpec </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.4.8.6.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib6" title="">6</a><span class="ltx_text" id="S3.T3.4.8.6.1.3.2" style="font-size:80%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.6.2"><span class="ltx_text" id="S3.T3.4.8.6.2.1" style="font-size:80%;">audio</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.6.3"><span class="ltx_text" id="S3.T3.4.8.6.3.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td" id="S3.T3.4.8.6.4"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.6.5"><span class="ltx_text" id="S3.T3.4.8.6.5.1" style="font-size:80%;">0.451</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.6.6"><span class="ltx_text" id="S3.T3.4.8.6.6.1" style="font-size:80%;">0.561</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.6.7"><span class="ltx_text" id="S3.T3.4.8.6.7.1" style="font-size:80%;">0.362</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.6.8"><span class="ltx_text" id="S3.T3.4.8.6.8.1" style="font-size:80%;">0.486</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.6.9"><span class="ltx_text" id="S3.T3.4.8.6.9.1" style="font-size:80%;">0.409</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.6.10"><span class="ltx_text" id="S3.T3.4.8.6.10.1" style="font-size:80%;">0.142</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.6.11"><span class="ltx_text" id="S3.T3.4.8.6.11.1" style="font-size:80%;">0.044</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.6.12"><span class="ltx_text" id="S3.T3.4.8.6.12.1" style="font-size:80%;">0.467</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.9.7">
<td class="ltx_td ltx_align_center" id="S3.T3.4.9.7.1"><span class="ltx_text" id="S3.T3.4.9.7.1.1" style="font-size:80%;">ImageNet SL</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.9.7.2"><span class="ltx_text" id="S3.T3.4.9.7.2.1" style="font-size:80%;">image</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.9.7.3"><span class="ltx_text" id="S3.T3.4.9.7.3.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.9.7.4"><span class="ltx_text" id="S3.T3.4.9.7.4.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.9.7.5"><span class="ltx_text" id="S3.T3.4.9.7.5.1" style="font-size:80%;">0.357</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.9.7.6"><span class="ltx_text" id="S3.T3.4.9.7.6.1" style="font-size:80%;">0.473</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.9.7.7"><span class="ltx_text" id="S3.T3.4.9.7.7.1" style="font-size:80%;">0.388</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.9.7.8"><span class="ltx_text" id="S3.T3.4.9.7.8.1" style="font-size:80%;">0.090</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.9.7.9"><span class="ltx_text" id="S3.T3.4.9.7.9.1" style="font-size:80%;">0.443</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.9.7.10"><span class="ltx_text" id="S3.T3.4.9.7.10.1" style="font-size:80%;">0.146</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.9.7.11"><span class="ltx_text" id="S3.T3.4.9.7.11.1" style="font-size:80%;">0.049</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.9.7.12"><span class="ltx_text" id="S3.T3.4.9.7.12.1" style="font-size:80%;">0.423</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.10.8">
<td class="ltx_td ltx_align_center" id="S3.T3.4.10.8.1"><span class="ltx_text" id="S3.T3.4.10.8.1.1" style="font-size:80%;">Shaders1k MAE (Ours)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.10.8.2"><span class="ltx_text" id="S3.T3.4.10.8.2.1" style="font-size:80%;">image</span></td>
<td class="ltx_td" id="S3.T3.4.10.8.3"></td>
<td class="ltx_td" id="S3.T3.4.10.8.4"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.10.8.5"><span class="ltx_text" id="S3.T3.4.10.8.5.1" style="font-size:80%;">0.343</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.10.8.6"><span class="ltx_text" id="S3.T3.4.10.8.6.1" style="font-size:80%;">0.508</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.10.8.7"><span class="ltx_text" id="S3.T3.4.10.8.7.1" style="font-size:80%;">0.340</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.10.8.8"><span class="ltx_text" id="S3.T3.4.10.8.8.1" style="font-size:80%;">0.252</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.10.8.9"><span class="ltx_text" id="S3.T3.4.10.8.9.1" style="font-size:80%;">0.399</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.10.8.10"><span class="ltx_text" id="S3.T3.4.10.8.10.1" style="font-size:80%;">0.104</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.10.8.11"><span class="ltx_text" id="S3.T3.4.10.8.11.1" style="font-size:80%;">0.037</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.10.8.12"><span class="ltx_text" id="S3.T3.4.10.8.12.1" style="font-size:80%;">0.443</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.11.9">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="12" id="S3.T3.4.11.9.1"><span class="ltx_text ltx_font_bold" id="S3.T3.4.11.9.1.1" style="font-size:80%;">Fine-tuning</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.12.10">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.12.10.1">
<span class="ltx_text" id="S3.T3.4.12.10.1.1" style="font-size:80%;">MaskSpec </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T3.4.12.10.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib6" title="">6</a><span class="ltx_text" id="S3.T3.4.12.10.1.3.2" style="font-size:80%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.12.10.2"><span class="ltx_text" id="S3.T3.4.12.10.2.1" style="font-size:80%;">audio</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.12.10.3"><span class="ltx_text" id="S3.T3.4.12.10.3.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_border_t" id="S3.T3.4.12.10.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.12.10.5"><span class="ltx_text" id="S3.T3.4.12.10.5.1" style="font-size:80%;">0.896</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.12.10.6"><span class="ltx_text" id="S3.T3.4.12.10.6.1" style="font-size:80%;">0.769</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.12.10.7"><span class="ltx_text" id="S3.T3.4.12.10.7.1" style="font-size:80%;">0.421</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.12.10.8"><span class="ltx_text" id="S3.T3.4.12.10.8.1" style="font-size:80%;">0.810</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.12.10.9"><span class="ltx_text" id="S3.T3.4.12.10.9.1" style="font-size:80%;">0.557</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.12.10.10"><span class="ltx_text" id="S3.T3.4.12.10.10.1" style="font-size:80%;">0.573</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.12.10.11"><span class="ltx_text" id="S3.T3.4.12.10.11.1" style="font-size:80%;">0.107</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.12.10.12"><span class="ltx_text" id="S3.T3.4.12.10.12.1" style="font-size:80%;">0.669</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.13.11">
<td class="ltx_td ltx_align_center" id="S3.T3.4.13.11.1"><span class="ltx_text" id="S3.T3.4.13.11.1.1" style="font-size:80%;">ImageNet SL</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.13.11.2"><span class="ltx_text" id="S3.T3.4.13.11.2.1" style="font-size:80%;">image</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.13.11.3"><span class="ltx_text" id="S3.T3.4.13.11.3.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.13.11.4"><span class="ltx_text" id="S3.T3.4.13.11.4.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.13.11.5"><span class="ltx_text" id="S3.T3.4.13.11.5.1" style="font-size:80%;">0.870</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.13.11.6"><span class="ltx_text" id="S3.T3.4.13.11.6.1" style="font-size:80%;">0.776</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.13.11.7"><span class="ltx_text" id="S3.T3.4.13.11.7.1" style="font-size:80%;">0.467</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.13.11.8"><span class="ltx_text" id="S3.T3.4.13.11.8.1" style="font-size:80%;">0.798</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.13.11.9"><span class="ltx_text" id="S3.T3.4.13.11.9.1" style="font-size:80%;">0.581</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.13.11.10"><span class="ltx_text" id="S3.T3.4.13.11.10.1" style="font-size:80%;">0.573</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.13.11.11"><span class="ltx_text" id="S3.T3.4.13.11.11.1" style="font-size:80%;">0.135</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.13.11.12"><span class="ltx_text" id="S3.T3.4.13.11.12.1" style="font-size:80%;">0.679</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.14.12">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.14.12.1"><span class="ltx_text" id="S3.T3.4.14.12.1.1" style="font-size:80%;">Shaders1k MAE (Ours)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.14.12.2"><span class="ltx_text" id="S3.T3.4.14.12.2.1" style="font-size:80%;">image</span></td>
<td class="ltx_td ltx_border_bb" id="S3.T3.4.14.12.3"></td>
<td class="ltx_td ltx_border_bb" id="S3.T3.4.14.12.4"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.14.12.5"><span class="ltx_text" id="S3.T3.4.14.12.5.1" style="font-size:80%;">0.873</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.14.12.6"><span class="ltx_text" id="S3.T3.4.14.12.6.1" style="font-size:80%;">0.783</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.14.12.7"><span class="ltx_text" id="S3.T3.4.14.12.7.1" style="font-size:80%;">0.422</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.14.12.8"><span class="ltx_text" id="S3.T3.4.14.12.8.1" style="font-size:80%;">0.850</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.14.12.9"><span class="ltx_text" id="S3.T3.4.14.12.9.1" style="font-size:80%;">0.575</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.14.12.10"><span class="ltx_text" id="S3.T3.4.14.12.10.1" style="font-size:80%;">0.563</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.14.12.11"><span class="ltx_text" id="S3.T3.4.14.12.11.1" style="font-size:80%;">0.129</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.14.12.12"><span class="ltx_text" id="S3.T3.4.14.12.12.1" style="font-size:80%;">0.684</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Comparison with image-based pre-training</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To demonstrate the transferability of MAEs,
we compare MAEs pre-trained on synthetic images
with other image-based pre-training methods.
For this comparison, we use ViT pre-trained with supervised learning on ImageNet1k and Formula-Driven Supervised Learning (FDSL)
with exFractal21k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib35" title="">35</a>]</cite> and VisualAtom21k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib16" title="">16</a>]</cite>.
FDSL is a powerful pre-training framework where the model aims to classify images generated from mathematical formulae
into predefined categories.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S3.T1" title="TABLE I ‣ III-A Properties of synthetic images ‣ III Experiments ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_tag">I</span></a> shows the results.
Although supervised learning on ImageNet1k achieves superior performance,
it requires real images and high-quality annotations.
In contrast, our framework with Shaders1k enables effective pre-training solely with synthetic images,
thereby alleviating privacy and licensing issues.
Notably, the MAE pre-trained on Shaders1k demonstrates higher performance than those pre-trained on ImageNet1k.
As observed in our previous experiments (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S2.F3" title="Figure 3 ‣ II Proposed Framework ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_tag">3</span></a>),
pre-training with FractalDB1k and VisualAtom1k also fails to transfer effectively to audio tagging on AudioSet-20k
and environment sound classification on ESC50,
despite their high performance on ImageNet1k.
This indicates that models pre-trained by MAE have higher transferability than models pre-trained by FDSL.
Based on these results, we will use Shaders1k for pre-training in our framework for subsequent experiments.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">Comparison with existing methods</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S3.T2" title="TABLE II ‣ III-A Properties of synthetic images ‣ III Experiments ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_tag">II</span></a> shows the comparison results
between the proposed framework and existing pre-training approaches on
AS-20k, AS-2M, ESC50, DCASE2019 task1A dataset, OpenMIC18 dataset, and SCV2.
Despite not using audio data during pre-training,
our framework achieves performance that closely approaches that of MaskSpec.
This indicates that the image-based MAE can learn highly transferable features.
While it does not match the performance of supervised learning methods,
these approaches use real data and require high-quality labels,
which incurs high costs of data collection and potentially raises privacy and licensing issues.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.4.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.5.2">Evaluation on HEAR and ARCH benchmark</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#S3.T3" title="TABLE III ‣ III-A Properties of synthetic images ‣ III Experiments ‣ Pre-training with Synthetic Patterns for Audio"><span class="ltx_text ltx_ref_tag">III</span></a> presents the results from eight datasets
selected from the HEAR and ARCH benchmarks,
following the setting of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00511v1#bib.bib14" title="">14</a>]</cite>.
Although our model performs well when fully fine-tuned,
it exhibits a significant limitation in the linear probing setting,
where the encoder weights remain fixed and only the linear layer is trained.
However, this limitation is not unique to our framework; it is also observed in MaskSpec,
which is a self-supervised learning method using MAE and real audio.
Therefore, we consider this limitation results from the characteristics of MAE
which focuses on low-level features, rather than high-level features.
To learn more audio-specific feature representations and improve performance in the linear probing setting,
a more sophisticated approach beyond masked visual modeling is necessary,
which remains a challenge for future work.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this work, we propose pre-training audio encoders utilizing synthetic patterns,
addressing challenges associated with real data, such as privacy concerns and licensing issues.
Our framework demonstrates robust performance across diverse audio tasks, even without audio data during pre-training.
Through extensive experiments,
we have revealed what types of synthetic patterns are effective for audio tasks.
Specifically, we have found that smoother images with fewer Total Variations contribute
significantly to MAE pre-training.
In comparison with existing methods,
our framework achieves comparable performance to self-supervised pre-training methods with real audio
and partially outperforms image-based pre-training methods.
We posit that our framework offers a viable solution
to mitigate the costs of audio data collection and
alleviate concerns regarding privacy and license infringements during audio pre-training.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Vaswani, “Attention is all you need,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:1706.03762</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">et al.</em>, “An image is worth 16x16 words: Transformers for image recognition at scale,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.2.2">arXiv preprint arXiv:2010.11929</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">2009 IEEE conference on computer vision and pattern recognition</em>.   Ieee, 2009, pp. 248–255.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y. Gong, Y.-A. Chung, and J. Glass, “Ast: Audio spectrogram transformer,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2104.01778</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
P.-Y. Huang, H. Xu, J. Li, A. Baevski, M. Auli, W. Galuba, F. Metze, and C. Feichtenhofer, “Masked autoencoders that listen,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Advances in Neural Information Processing Systems</em>, vol. 35, pp. 28 708–28 720, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
D. Chong, H. Wang, P. Zhou, and Q. Zeng, “Masked spectrogram prediction for self-supervised audio pre-training,” in <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
M.-I. Georgescu, E. Fonseca, R. T. Ionescu, M. Lucic, C. Schmid, and A. Arnab, “Audiovisual masked autoencoders,” in <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2023, pp. 16 144–16 154.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and human-labeled dataset for audio events,” in <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>.   IEEE, 2017, pp. 776–780.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H. Chen, W. Xie, A. Vedaldi, and A. Zisserman, “Vggsound: A large-scale audio-visual dataset,” in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2020, pp. 721–725.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Rosenberg, Y. Zhang, B. Ramabhadran, Y. Jia, P. Moreno, Y. Wu, and Z. Wu, “Speech recognition with augmented synthesized speech,” in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">2019 IEEE automatic speech recognition and understanding workshop (ASRU)</em>.   IEEE, 2019, pp. 996–1002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
X. Zheng, Y. Liu, D. Gunceler, and D. Willett, “Using synthetic audio to improve the recognition of out-of-vocabulary words in end-to-end asr systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2021, pp. 5674–5678.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
M. Bartelds, N. San, B. McDonnell, D. Jurafsky, and M. Wieling, “Making more of little data: Improving low-resource automatic speech recognition using data augmentation,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2305.10951</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. Engel, C. Resnick, A. Roberts, S. Dieleman, M. Norouzi, D. Eck, and K. Simonyan, “Neural audio synthesis of musical notes with wavenet autoencoders,” in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">International Conference on Machine Learning</em>.   PMLR, 2017, pp. 1068–1077.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. Cherep and N. Singh, “Contrastive learning from synthetic audio doppelgangers,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2406.05923</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
H. Kataoka, K. Okayasu, A. Matsumoto, E. Yamagata, R. Yamada, N. Inoue, A. Nakamura, and Y. Satoh, “Pre-training without natural images,” in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the Asian Conference on Computer Vision</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S. Takashima, R. Hayamizu, N. Inoue, H. Kataoka, and R. Yokota, “Visual atoms: Pre-training vision transformers with sinusoidal waves,” in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023, pp. 18 579–18 588.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
M. Baradad Jurjo, J. Wulff, T. Wang, P. Isola, and A. Torralba, “Learning to see by looking at noise,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Advances in Neural Information Processing Systems</em>, vol. 34, pp. 2556–2569, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
M. Baradad, R. Chen, J. Wulff, T. Wang, R. Feris, A. Torralba, and P. Isola, “Procedural image programs for representation learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Advances in Neural Information Processing Systems</em>, vol. 35, pp. 6450–6462, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Y. Ishikawa, M. Kondo, and Y. Aoki, “Data collection-free masked video modeling,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2409.06665</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Z. Tong, Y. Song, J. Wang, and L. Wang, “Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2203.12602</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, “Masked autoencoders are scalable vision learners,” in <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2022, pp. 16 000–16 009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
K. J. Piczak, “Esc: Dataset for environmental sound classification,” in <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 23rd ACM international conference on Multimedia</em>, 2015, pp. 1015–1018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A. Mesaros, T. Heittola, and T. Virtanen, “Acoustic scene classification in dcase 2019 challenge: Closed and open set classification and data mismatch setups,” 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
E. Humphrey, S. Durand, and B. McFee, “Openmic-2018: An open data-set for multiple instrument recognition.” in <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">ISMIR</em>, 2018, pp. 438–444.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
P. Warden, “Speech commands: A dataset for limited-vocabulary speech recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:1804.03209</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
J. Turian, J. Shier, H. R. Khan, B. Raj, B. W. Schuller, C. J. Steinmetz, C. Malloy, G. Tzanetakis, G. Velarde, K. McNally <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">et al.</em>, “Hear: Holistic evaluation of audio representations,” in <em class="ltx_emph ltx_font_italic" id="bib.bib26.2.2">NeurIPS 2021 Competitions and Demonstrations Track</em>.   PMLR, 2022, pp. 125–145.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
M. La Quatra, A. Koudounas, L. Vaiani, E. Baralis, L. Cagliero, P. Garza, and S. M. Siniscalchi, “Benchmarking representations for speech, music, and acoustic events,” <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2405.00934</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
J. Salamon, C. Jacoby, and J. P. Bello, “A dataset and taxonomy for urban sound research,” in <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 22nd ACM international conference on Multimedia</em>, 2014, pp. 1041–1044.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense vocalizations of affect and emotion (vivae) corpus prompts new perspective on nonspeech perception.” <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Emotion</em>, vol. 22, no. 1, p. 213, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova, and R. Verma, “Crema-d: Crowd-sourced emotional multimodal actors dataset,” <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">IEEE transactions on affective computing</em>, vol. 5, no. 4, pp. 377–390, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
E. Fonseca, X. Favory, J. Pons, F. Font, and X. Serra, “Fsd50k: an open dataset of human-labeled sound events,” <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 30, pp. 829–852, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
B. Kim, M. Ghei, B. Pardo, and Z. Duan, “Vocal imitation set: a dataset of vocally imitated sound events using the audioset ontology.” in <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">DCASE</em>, 2018, pp. 148–152.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
F.-R. Stöter, S. Chakrabarty, E. Habets, and B. Edler, “Libricount, a dataset for speaker count estimation,” Apr. 2018. [Online]. Available: https://doi.org/10.5281/zenodo.1216072

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
L. I. Rudin, S. Osher, and E. Fatemi, “Nonlinear total variation based noise removal algorithms,” <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Physica D: nonlinear phenomena</em>, vol. 60, no. 1-4, pp. 259–268, 1992.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
H. Kataoka, R. Hayamizu, R. Yamada, K. Nakashima, S. Takashima, X. Zhang, E. J. Martinez-Noriega, N. Inoue, and R. Yokota, “Replacing labeled real-image datasets with auto-generated contours,” in <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022, pp. 21 232–21 241.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
J. Canny, “A computational approach to edge detection,” <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">IEEE Transactions on pattern analysis and machine intelligence</em>, no. 6, pp. 679–698, 1986.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
A. Krizhevsky, G. Hinton <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">et al.</em>, “Learning multiple layers of features from tiny images,” 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley, “Panns: Large-scale pretrained audio neural networks for audio pattern recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 28, pp. 2880–2894, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Y. Gong, Y.-A. Chung, and J. Glass, “Psla: Improving audio tagging with pretraining, sampling, labeling, and aggregation,” <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 29, pp. 3292–3306, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
K. Koutini, J. Schlüter, H. Eghbal-Zadeh, and G. Widmer, “Efficient training of audio transformers with patchout,” <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2110.05069</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Y. Gong, C.-I. Lai, Y.-A. Chung, and J. Glass, “Ssast: Self-supervised audio spectrogram transformer,” in <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 36, no. 10, 2022, pp. 10 699–10 709.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an asr corpus based on public domain audio books,” in <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>.   IEEE, 2015, pp. 5206–5210.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
B. Elizalde, S. Deshmukh, M. Al Ismail, and H. Wang, “Clap learning audio concepts from natural language supervision,” in <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Sep 29 14:40:51 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
