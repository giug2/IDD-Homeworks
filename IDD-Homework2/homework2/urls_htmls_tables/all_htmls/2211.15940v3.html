<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2211.15940] PiggyBack: Pretrained Visual Question Answering Environment for Backing up Non-deep Learning Professionals</title><meta property="og:description" content="We propose a PiggyBack, a Visual Question Answering platform that allows users to apply the state-of-the-art visual-language pretrained models easily. The PiggyBack supports the full stack of visual question answering …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PiggyBack: Pretrained Visual Question Answering Environment for Backing up Non-deep Learning Professionals">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="PiggyBack: Pretrained Visual Question Answering Environment for Backing up Non-deep Learning Professionals">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2211.15940">

<!--Generated on Thu Mar 14 06:17:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Web Application,  Visual Question Answering,  Graphic User Interface,  Visual-Language Pre-trained Model">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">PiggyBack: Pretrained Visual Question Answering Environment 
<br class="ltx_break">for Backing up Non-deep Learning Professionals</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Zhihao Zhang<sup id="id14.14.id1" class="ltx_sup"><span id="id14.14.id1.1" class="ltx_text ltx_font_italic">1</span></sup>*,
Siwen Luo<sup id="id15.15.id2" class="ltx_sup"><span id="id15.15.id2.1" class="ltx_text ltx_font_italic">1</span></sup>*,
Junyi Chen<sup id="id16.16.id3" class="ltx_sup"><span id="id16.16.id3.1" class="ltx_text ltx_font_italic">1</span></sup>,
Sijia Lai<sup id="id17.17.id4" class="ltx_sup"><span id="id17.17.id4.1" class="ltx_text ltx_font_italic">1</span></sup>,
<br class="ltx_break">Siqu Long<sup id="id18.18.id5" class="ltx_sup"><span id="id18.18.id5.1" class="ltx_text ltx_font_italic">1</span></sup>,
Hyunsuk Chung<sup id="id19.19.id6" class="ltx_sup"><span id="id19.19.id6.1" class="ltx_text ltx_font_italic">3</span></sup>,
Soyeon Caren Han<sup id="id20.20.id7" class="ltx_sup"><span id="id20.20.id7.1" class="ltx_text ltx_font_italic">1,2</span></sup><math id="id8.8.m8.1" class="ltx_Math" alttext="{\dagger}" display="inline"><semantics id="id8.8.m8.1a"><mo id="id8.8.m8.1.1" xref="id8.8.m8.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id8.8.m8.1b"><ci id="id8.8.m8.1.1.cmml" xref="id8.8.m8.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id8.8.m8.1c">{\dagger}</annotation></semantics></math>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id11.11.3" class="ltx_text ltx_affiliation_institution"><sup id="id11.11.3.1" class="ltx_sup"><span id="id11.11.3.1.1" class="ltx_text ltx_font_italic">1</span></sup>The University of Sydney, <sup id="id11.11.3.2" class="ltx_sup"><span id="id11.11.3.2.1" class="ltx_text ltx_font_italic">2</span></sup>The University of Western Australia, <sup id="id11.11.3.3" class="ltx_sup"><span id="id11.11.3.3.1" class="ltx_text ltx_font_italic">3</span></sup>FortifyEdge</span><span id="id13.13.5" class="ltx_text ltx_affiliation_state"><sup id="id13.13.5.1" class="ltx_sup"><span id="id13.13.5.1.1" class="ltx_text ltx_font_italic">1,3</span></sup>New South Wales, <sup id="id13.13.5.2" class="ltx_sup"><span id="id13.13.5.2.1" class="ltx_text ltx_font_italic">2</span></sup>Western Australia</span><span id="id21.21.id1" class="ltx_text ltx_affiliation_country">Australia</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zhihao.zhang1,%20siwen.luo,%20caren.han@sydney.edu.au,%20slai3926,%20slon6753@uni.sydney.edu.au">zhihao.zhang1, siwen.luo, caren.han@sydney.edu.au, slai3926, slon6753@uni.sydney.edu.au</a>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:david.chung@fortifyedge.com,%20junyi.chen.215@gmail.com">david.chung@fortifyedge.com, junyi.chen.215@gmail.com</a>
</span></span></span>
</div>
<div class="ltx_dates">(2023)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotetext: </span>Both authors contributed equally to this research.</span></span></span><span id="footnotex2" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotetext: </span>Corresponding author.</span></span></span>
<p id="id22.id1" class="ltx_p">We propose a PiggyBack, a Visual Question Answering platform that allows users to apply the state-of-the-art visual-language pretrained models easily. The PiggyBack supports the full stack of visual question answering tasks, specifically data processing, model fine-tuning, and result visualisation. We integrate visual-language models, pretrained by HuggingFace, an open-source API platform of deep learning technologies; however, it cannot be runnable without programming skills or deep learning understanding. Hence, our PiggyBack supports an easy-to-use browser-based user interface with several deep learning visual language pretrained models for general users and domain experts. The PiggyBack includes the following benefits: Free availability under the MIT License, Portability due to web-based and thus runs on almost any platform, A comprehensive data creation and processing technique, and ease of use on deep learning-based visual language pretrained models. The demo video is available on YouTube and can be found at <a target="_blank" href="https://youtu.be/iz44RZ1lF4s" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://youtu.be/iz44RZ1lF4s</a>.</p>
</div>
<div class="ltx_keywords">Web Application, Visual Question Answering, Graphic User Interface, Visual-Language Pre-trained Model
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2023</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining; Feb 27– March 03,
2023; In-person Conference with Virtual Elements, Singapore</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_price"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">price: </span>15.00</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Web applications</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Transfer learning</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing User interface design</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering (VQA) <cite class="ltx_cite ltx_citemacro_citep">(Antol et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite> is a Vision-and-Language task that requires answering natural language questions by referring to relevant regions of a given image. This task has proved its practical assistance in various real-world applications, including automatic medical diagnosis <cite class="ltx_cite ltx_citemacro_citep">(Vu et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2020</a>; Ren and Zhou, <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite>, visual-impaired people guidance <cite class="ltx_cite ltx_citemacro_citep">(Ren and Zhou, <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite>, education assistance <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite> and customer advertising improvement <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite>. To achieve acceptable performances, task-specific models require large-scale datasets to learn the visual and textual features sufficiently <cite class="ltx_cite ltx_citemacro_citep">(Long et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2022a</a>)</cite>. However, domain-related datasets could be low-resource due to the collection difficulties and expensiveness, especially in the medical domain. For example, the largest radiology dataset SLAKE <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite> only contains 14K image-question pairs. The Vision-and-Language Pre-trained Models (VLPMs) become helpful in this case. VLPMs are pretrained on huge image-text dataset collections to learn the generic representations of the visual and textual alignment <cite class="ltx_cite ltx_citemacro_citep">(Long et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2022b</a>)</cite>, which various downstream vision-and-language tasks can then use. Recently, several large VLPMs <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>; Su et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2019</a>; Tan and Bansal, <a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite> have been proposed and have proved their state-of-the-art performances on the downstream VQA task. These large VLPMs empower the merit of transfer learning and can be smoothly adapted to different domains by fine-tuning on small-scale datasets while maintaining competitive performances. Therefore, VLPMs can be an excellent solution to the lack of domain-related data. VLPMs have become popular among deep learning researchers, and many open-source tools and APIs are publicly released. Nevertheless, VLPMs are not vastly applied in industrial domains. This is because such implementation requires solid deep learning and programming skills and thus is challenging for non-deep learning experts.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p"><span id="S1.p2.1.1" class="ltx_text ltx_font_bold">Contribution.</span> With this in mind, we propose PiggyBack, a deep learning web-based interactive VQA platform, to help field experts such as physicians, educators and commercial analysts. Our PiggyBack is mainly for those who lack deep learning expertise or programming skills and easily apply VLPMs on VQA tasks with their dataset. More precisely, Piggyback provides two pre-trained models, and users can freely choose and train one of the models over their training data by interacting with its user interface. It also supports model evaluation directly on users’ testing sets with numerous image-question pairs. It enhances the evaluation results with interpretability by visualising the relevant regions for question-answering on the image. Such interpretation would help users build confidence in the model’s decision, especially for critical fields.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p"><span id="S1.p3.1.1" class="ltx_text ltx_font_bold">Comparison.</span> To the best of our knowledge, PiggyBack is the first web-based deep-learning platform that provides a user-friendly interface for non-deep learning users. It allows the users to train VQA models with their datasets by utilising VLPMs in the manner of transfer learning (also known as fine-tuning) and testing the model with their testing datasets. Some of the existing VQA platforms are not based on VLPMs, such as Simple Baseline for VQA<span id="footnotex3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="http://visualqa.csail.mit.edu/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://visualqa.csail.mit.edu/</a></span></span></span> and Explainable VQA<span id="footnotex4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://lrpserver.hhi.fraunhofer.de/visual-question-answering/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://lrpserver.hhi.fraunhofer.de/visual-question-answering/</a></span></span></span>, which cannot provide the benefit of the generalised pre-trained model. Other VQA platforms only focus on testing the models’ performance by evaluating the single image-question pair, such as CloudCV<span id="footnotex5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="http://visualqa.csail.mit.edu/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://visualqa.csail.mit.edu/</a></span></span></span>, ViLT VQA<span id="footnotex6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://huggingface.co/spaces/nielsr/vilt-vqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/spaces/nielsr/vilt-vqa</a></span></span></span> and OFA-VQA<span id="footnotex7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://huggingface.co/spaces/OFA-Sys/OFA-vqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/spaces/OFA-Sys/OFA-vqa</a></span></span></span>, which cannot be trained towards users’ datasets. Furthermore, none of these platforms combines and simplifies the training and testing procedures to provide the VLPMs’ capability for other field experts.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>PiggyBack System</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">PiggyBack integrates the VLPMs implemented by HuggingFace Transformer <cite class="ltx_cite ltx_citemacro_citep">(Wolf et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite> while keeping all the coding away from users behind the well-designed browser-based Graphic User Interface (GUI). Therefore, we designed both the backend and front-end of the system to standardise the workflow scenario for VQA tasks, so any non-deep learning/non-programming professionals can utilise PiggBack effortlessly. Its design flow is shown in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1. Data Preparation ‣ 3. Backend Architecture ‣ PiggyBack: Pretrained Visual Question Answering Environment for Backing up Non-deep Learning Professionals" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, which illustrates the front-end components in Sec.4. PiggyBack’s backend and front-end are described in the following sections.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Backend Architecture</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The system backend is built upon the Flask framework <cite class="ltx_cite ltx_citemacro_citep">(Grinberg, <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>. Since it contains no database abstraction layer, the input data is handled by Python and saved in the server’s local environment. The backend includes four components that cover all necessary procedures in model fine-tuning and evaluation.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Data Preparation</h3>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2211.15940/assets/samples/piggyback.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="286" height="194" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>PiggyBack platform framework, which consists of four main components: 1)Data Uploader, 2)Model Selector, 3)Fine-Tuner, and 4)Visualiser.</figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">PiggyBack simplifies the users’ data preparation by providing the automatic data cleaning process and asking for simple dataset formats, which can be easily prepared. It includes a zip file including all the images and a CSV file containing all the questions and their ground-truth answers for the associated images. As the backend models require the input data following the VQAv2’s one-question ten-answers format <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite>, we developed a module in our system to clean the imperfect data in users’ uploaded CSV. The cleaning steps include: 1) auto-fill the 10 answers when users did not provide enough answers; 2) remove the duplicated image question pairs accidentally provided by the users or the questions that have no valid image id; 3) remove the images that exceed the required size. The cleaned CSV is then transferred into JSON format, which can be directly loaded into different models. Visual features are extracted from images by a feature extraction module and saved into JSON format. This stand-alone module is containerised by Docker <cite class="ltx_cite ltx_citemacro_citep">(Merkel, <a href="#bib.bib13" title="" class="ltx_ref">2014</a>)</cite> and implements the Bottom-Up, and Top-Down Attention model <cite class="ltx_cite ltx_citemacro_citep">(Anderson et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite>. Such data cleaning processes are all wrapped in the backend, which leaves users an easy and simplified experience in their data preparation step.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Model Structure</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Inspired by V-Doc <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite>, our system includes two state-of-the-art pre-trained models: VisualBERT and LXMERT, which offer the users an opportunity to conduct the performance comparison of models with different structures and enable them to choose the model that suits their data the best. VisualBERT <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite> encodes the visual embedding as the sum of bonding region features, segment embedding and position embedding. In the meantime, it encodes the textual embedding following the BERT format, including token embeddings, segment embeddings and position embedding. A single Transformer structure is proposed in VisualBERT, which uses visual and textural embeddings to discover alignments between vision and language. VisualBERT is pre-trained with Masked Language Modeling with the Image Task and Sentence-image Prediction Task, and it can be fine-tuned with VQA datasets. LXMERT <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite> directly takes a sequence of objects from images as the visual inputs and a sequence of words from sentences as the linguistic inputs. There are three Transformer encoders inside LXMERT, which separately encode image object features, question features and cross-modality interactions. LXMERT is pre-trained with five tasks, including Masked Cross-Modality Langage Model, RoI-Feature Regression, Detected-Label Classification Cross-Modality Matching and Image Question Answering, and it can be fine-tuned for VQA downstream task. Both pre-trained models are built upon the HuggingFace deep-learning API <cite class="ltx_cite ltx_citemacro_citep">(Wolf et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite>, and have proved to be an outstanding performance on the VQA tasks.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Fine Tuning</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Once the model is selected, PiggyBack loads the pre-trained model and finds the answer space from the preprocessed data. Then it feeds the data into the data loader and launches the fine-tuning process with the specific answer space on the pre-trained model. When the fine-tuning operation finishes, the fine-tuned model will be packed into a loadable file, which can be imported for evaluation. All the fine-tuning procedures are handled by the backend, so there is no deep-learning knowledge required from the users.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Visualised Evaluation</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">PiggyBack allows the fine-tuned model to evaluate with numerous image-question pairs and delivers the predictions in a single CSV file. Apart from the predicted answers, PiggyBack embeds a visualisation module, which enhances the model interpretability by annotating the important object regions in the images according to their attention scores. Attention scores have long been used as a feature-based local interpretation method for deep neural networks. Both VisualBERT and LXMERT utilise the Transformer structure with the multi-head self-attention mechanism <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2017</a>)</cite>. For the visual component, the attention mechanism assigns attention weights for each region of the input images. The region with higher attention weights is naturally considered more critical to the model’s outputs <cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>. We sum up the attention weights across all heads for all transformer layers as the final attention score for each object region and visualise the top 5 object regions with the highest attention scores and annotate them with their bounding boxes. Regions with higher attention scores are marked in a darker colour.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Front-end Interactive Web Page</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">PiggyBack provides an interactive web front-end that is built upon the Bootstrap<span id="footnotex8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://getbootstrap.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://getbootstrap.com/</a></span></span></span> framework. We established three pages to cover the four components in the backend. The home page includes Data Uploader, Model Selector and Fine-tuner, which introduces a straightforward interface for input datasets uploading, model choosing and training operating. The progress page shows the fine-tuning progress. The evaluation page includes Visualiser, which illustrates the models’ performance to the users after fine-tuning. Those web pages aim to guide the users in completing the fine-tuning and evaluation process.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Data Uploader</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Our system landing page presents the GUI of the Data Uploader for collecting the user’s training dataset, which is shown in Figure <a href="#S4.F2" title="Figure 2 ‣ 4.1. Data Uploader ‣ 4. Front-end Interactive Web Page ‣ PiggyBack: Pretrained Visual Question Answering Environment for Backing up Non-deep Learning Professionals" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Under the ”Images” and ”Questions and Answers” sections, the users only need to upload a compressed ZIP folder that includes all images as well as a CSV file that contains all the questions and answers with the corresponding image id. There are two constraints placed in the uploaded dataset due to the prerequisite of the VLPMs: 1) the input image’s width and height should be within 1920 pixels; 2) the input question, answer and image id should be legitimate to form one piece of the data. To help the users comprehensively understand the data format, we provide the illustrations on the data uploading page and the <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">”Sample Dataset”</span> files that can be downloaded and even modified by users with their data.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2211.15940/assets/samples/piggyback-interface-1.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="509" height="605" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>This figure shows the Preview of Data Uploader and Model Selector on the front-end interface in our PiggyBack.</figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The interactive web page can provide prompt feedback to the users when the image data is sent into the Data Uploader:
1) if there is no valid image in the uploaded folder, a red Error banner will show up, and it requires a new image folder from the users;
2) if there are some oversized images in the uploaded folder, a yellow Warning banner will show up, and the users can choose to fine-tune without those images or resubmit the image folder after modification;
3) if all the images meet the constraint, a green Success banner will show up, and the users can choose to fine-tune with current images or resubmit the image folder. The input question-and-answer data in CSV is preprocessed in the system backend.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Model Selector and Fine-tuner</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">After uploading the dataset successfully, the users can choose either VisualBERT or LXMERT by simply selecting if from the <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">“Choose a model”</span> drop-down menu in the interface. Once the users click <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">“Start fine-tuning”</span> with the chosen pre-trained model, all the processed data features will be passed to the loaded pre-trained model for the fine-tuning process. Meanwhile, a progress bar appears on the web page, indicating the completion of the fine-tuning step. If the users neglect the model selection but click <span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_italic">“Start fine-tuning”</span>, a red Error banner will show up asking to select the pre-trained model, and the system will hold the fine-tuning process till a pre-trained model has been selected.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Visualiser</h3>

<figure id="S4.F3" class="ltx_figure"><img src="/html/2211.15940/assets/samples/piggyback-interface-2.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="509" height="612" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>This figure shows the Preview of Fine-tuner and Visualiser on the front-end interface in our PiggyBack.</figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Once fine-tuning finishes, users will be automatically redirected to the evaluation page, which incorporates the Visualiser. As shown in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.3. Visualiser ‣ 4. Front-end Interactive Web Page ‣ PiggyBack: Pretrained Visual Question Answering Environment for Backing up Non-deep Learning Professionals" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we provide three different scenarios for users to test the fine-tuned model:</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Sample evaluation.</span> This evaluation section equips with a sample case at the top half of the page. Both visual and textual outputs are shown directly on the page, so the users can have a quick glance at the model’s performance and understand the visualised outputs from the PiggyBack. We put a radiology image and medical-related questions as an example; the users can select different questions in the drop-down menu and click <span id="S4.SS3.p2.1.2" class="ltx_text ltx_font_italic">”Get Answer”</span> to see the predicted answers. Furthermore, the Visualiser annotates the top five regions in the image based on the significance calculated by the fine-tuned model. The insights of the significance calculation are introduced in Sec.<a href="#S3.SS4" title="3.4. Visualised Evaluation ‣ 3. Backend Architecture ‣ PiggyBack: Pretrained Visual Question Answering Environment for Backing up Non-deep Learning Professionals" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Single evaluation.</span> PiggyBack provides a testing GUI for the users, which allows them to upload a single image and ask a question about it. As shown in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.3. Visualiser ‣ 4. Front-end Interactive Web Page ‣ PiggyBack: Pretrained Visual Question Answering Environment for Backing up Non-deep Learning Professionals" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, this section is at the bottom left of the evaluation page. The system shows the uploaded image’s preview on the page, which ensures that they type in the relevant question. Similar to the sample evaluation above, a predicted answer and its corresponding annotated image will appear on the page upon clicking <span id="S4.SS3.p3.1.2" class="ltx_text ltx_font_italic">“Get Answer”</span>.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">Multiple evaluation.</span> Apart from single evaluation, PiggyBack is capable of multiple image-question pairs evaluation, which is more practical in the real-world scenario. This section is at the bottom right of the evaluation page. The required format of the multiple evaluation data is similar to the training data; the only difference is that the answers are not required in the testing CSV. In this evaluation GUI, the instruction of the CSV modification and the hyperlink to the previous sample dataset is provided for the users, which helps them with the testing data preparation. We designed a simple checking mechanism, which shows a red error message when there is no valid image or question entry in the dataset. After uploading the testing data, the users can click <span id="S4.SS3.p4.1.2" class="ltx_text ltx_font_italic">“Get Answers”</span> to get model predictions, and a green banner will show up with the download links for both annotated image ZIP and answer CSV. The annotated images in the ZIP are renamed with their questions, which helps the user easily combine the model predictions with the corresponding images.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">PiggyBack is a web-based vision-and-language modelling platform that aims to support non-deep learning users utilizing the SOTA VLPMs for VQA problems in their specific domains. The PiggyBack system provides a user-friendly interface that simplifies all the data uploading, model fine-tuning and evaluation with only a few clicks. Meanwhile, it accompanies the results with a straightforward interpretation to help users better understand the model’s decision.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Jiasen
Lu, Stanislaw Antol, Margaret Mitchell,
C. Lawrence Zitnick, Dhruv Batra, and
Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">VQA: Visual Question Answering.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/ARXIV.1505.00468" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/ARXIV.1505.00468</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Peter Anderson, Xiaodong
He, Chris Buehler, Damien Teney,
Mark Johnson, Stephen Gould, and
Lei Zhang. 2018.

</span>
<span class="ltx_bibblock">Bottom-Up and Top-Down Attention for Image
Captioning and Visual Question Answering. In
<em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya
Agrawal, Jiasen Lu, Margaret Mitchell,
Dhruv Batra, C Lawrence Zitnick, and
Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering. In
<em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">IEEE international conference on computer vision</em>.
2425–2433.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Yihao Ding, Zhe Huang,
Runlin Wang, YanHang Zhang,
Xianru Chen, Yuzhong Ma,
Hyunsuk Chung, and Soyeon Caren Han.
2022.

</span>
<span class="ltx_bibblock">V-Doc: Visual Questions Answers With Documents. In
<em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR)</em>. 21492–21498.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grinberg (2018)</span>
<span class="ltx_bibblock">
Miguel Grinberg.
2018.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Flask web development: developing web
applications with python</em>.

</span>
<span class="ltx_bibblock">” O’Reilly Media, Inc.”.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Caren Han, Siqu Long,
Siwen Luo, Kunze Wang, and
Josiah Poon. 2020.

</span>
<span class="ltx_bibblock">VICTR: Visual Information Captured Text
Representation for Text-to-Vision Multimodal Tasks. In
<em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th International Conference on
Computational Linguistics</em>. 3107–3117.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Bin He, Meng Xia,
Xinguo Yu, Pengpeng Jian,
Hao Meng, and Zhanwen Chen.
2017.

</span>
<span class="ltx_bibblock">An educational robot system of visual question
answering for preschoolers. In <em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">2017 2nd
International Conference on Robotics and Automation Engineering (ICRAE)</em>.
IEEE, 441–445.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Liunian Harold Li, Mark
Yatskar, Da Yin, Cho-Jui Hsieh, and
Kai-Wei Chang. 2019.

</span>
<span class="ltx_bibblock">Visualbert: A simple and performant baseline for
vision and language.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.03557</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Bo Liu, Li-Ming Zhan,
Li Xu, Lin Ma, Yan
Yang, and Xiao-Ming Wu.
2021.

</span>
<span class="ltx_bibblock">SLAKE: A Semantically-Labeled Knowledge-Enhanced
Dataset for Medical Visual Question Answering.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/ARXIV.2102.09542" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/ARXIV.2102.09542</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Siqu Long, Feiqi Cao,
Soyeon Caren Han, and Haiqing Yang.
2022a.

</span>
<span class="ltx_bibblock">Vision-and-Language Pretrained Models: A Survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.07356</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Siqu Long, Soyeon Caren
Han, Xiaojun Wan, and Josiah Poon.
2022b.

</span>
<span class="ltx_bibblock">Gradual: Graph-based dual-modal representation for
image-text matching. In <em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer Vision</em>.
3459–3468.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Merkel (2014)</span>
<span class="ltx_bibblock">
Dirk Merkel.
2014.

</span>
<span class="ltx_bibblock">Docker: lightweight linux containers for consistent
development and deployment.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Linux journal</em> 2014,
239 (2014), 2.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren and Zhou (2020)</span>
<span class="ltx_bibblock">
Fuji Ren and Yangyang
Zhou. 2020.

</span>
<span class="ltx_bibblock">Cgmvqa: A new classification and generative model
for medical visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em> 8
(2020), 50626–50636.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Weijie Su, Xizhou Zhu,
Yue Cao, Bin Li, Lewei
Lu, Furu Wei, and Jifeng Dai.
2019.

</span>
<span class="ltx_bibblock">VL-BERT: Pre-training of Generic Visual-Linguistic
Representations. In <em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">International Conference on
Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Bansal (2019)</span>
<span class="ltx_bibblock">
Hao Tan and Mohit
Bansal. 2019.

</span>
<span class="ltx_bibblock">LXMERT: Learning Cross-Modality Encoder
Representations from Transformers. In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP)</em>. 5100–5111.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam
Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia
Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing
systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vu et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Minh H Vu, Tommy
Löfstedt, Tufve Nyholm, and Raphael
Sznitman. 2020.

</span>
<span class="ltx_bibblock">A question-centric model for visual question
answering in medical imaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">IEEE transactions on medical imaging</em>
39, 9 (2020),
2856–2868.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre
Debut, Victor Sanh, Julien Chaumond,
Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault,
Rémi Louf, Morgan Funtowicz,
Joe Davison, Sam Shleifer,
Patrick von Platen, Clara Ma,
Yacine Jernite, Julien Plu,
Canwen Xu, Teven Le Scao,
Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush.
2020.

</span>
<span class="ltx_bibblock">Transformers: State-of-the-Art Natural Language
Processing. In <em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing: System Demonstrations</em>.
Association for Computational Linguistics,
Online, 38–45.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.aclweb.org/anthology/2020.emnlp-demos.6" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/2020.emnlp-demos.6</a>

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yichao Zhou, Shaunak
Mishra, Manisha Verma, Narayan
Bhamidipati, and Wei Wang.
2020.

</span>
<span class="ltx_bibblock">Recommending themes for ad creative design via
visual-linguistic representations. In <em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">Proceedings
of The Web Conference 2020</em>. 2521–2527.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2211.15939" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2211.15940" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2211.15940">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2211.15940" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2211.15941" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 06:17:53 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
