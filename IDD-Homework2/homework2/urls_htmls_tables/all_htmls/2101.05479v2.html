<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2101.05479] Understanding the Role of Scene Graphs in Visual Question Answering</title><meta property="og:description" content="Visual Question Answering (VQA) is of tremendous interest to the research community with important applications such as aiding visually impaired users and image-based search. In this work, we explore the use of scene g…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Understanding the Role of Scene Graphs in Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Understanding the Role of Scene Graphs in Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2101.05479">

<!--Generated on Fri Mar  8 00:13:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Understanding the Role of Scene Graphs in Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vinay Damodaran
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">  Sharanya Chakravarthy
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Akshay Kumar
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anjana Umapathy
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_bold">Teruko Mitamura</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Carnegie Mellon University 
<br class="ltx_break"><span id="id2.2.id1" class="ltx_text ltx_font_typewriter">{vdamodar, sharanyc, akshayak, aumapath, teruko}@cs.cmu.edu</span>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuta Nakashima
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Osaka University 
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter">{n-yuta, noagarcia}@ids.osaka-u.ac.jp</span>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Noa Garcia
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Osaka University 
<br class="ltx_break"><span id="id4.1.id1" class="ltx_text ltx_font_typewriter">{n-yuta, noagarcia}@ids.osaka-u.ac.jp</span>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chenhui Chu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Kyoto University 
<br class="ltx_break"><span id="id5.1.id1" class="ltx_text ltx_font_typewriter">chu@i.kyoto-u.ac.jp</span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id6.id1" class="ltx_p">Visual Question Answering (VQA) is of tremendous interest to the research community with important applications such as aiding visually impaired users and image-based search. In this work, we explore the use of scene graphs for solving the VQA task. We conduct experiments on the GQA dataset <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib11" title="" class="ltx_ref">2019b</a>)</cite> which presents a challenging set of questions requiring counting, compositionality and advanced reasoning capability, and provides scene graphs for a large number of images. We adopt image + question architectures for use with scene graphs, evaluate various scene graph generation techniques for unseen images, propose a training curriculum to leverage human annotated and auto-generated scene graphs, and build late fusion architectures to learn from multiple image representations. We present a multi-faceted study into the use of scene graphs for VQA, making this work the first of its kind.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The task of Visual Question Answering (VQA) requires a model to answer a free-form natural language question that is based on an image. It is an important Vision+Language (V+L) task that has numerous real-world applications such as image-based search, search and rescue missions, visually capable AI assistants and aiding visually impaired users. A number of VQA datasets such as VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Antol et al., <a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>, CLEVR <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite> and GQA <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib11" title="" class="ltx_ref">2019b</a>)</cite> have been introduced, fuelling research in the area.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Research in VQA includes works that find biases in the data, demonstrating the need for improved datasets
<cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al., <a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite>, pre-trained models such as <span id="S1.p2.1.1" class="ltx_text ltx_font_smallcaps">lxmert</span> <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite> and <span id="S1.p2.1.2" class="ltx_text ltx_font_smallcaps">uniter</span> <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite> that beat the state of the art on a variety of V+L tasks and models such as neural module networks <cite class="ltx_cite ltx_citemacro_citep">(Andreas et al., <a href="#bib.bib3" title="" class="ltx_ref">2016</a>)</cite> constructed to heighten interpretability, among many others. However, few works have explored the use of scene graphs for the VQA task. Scene graphs provide a graphical representation of the image, containing information about objects and relationships between them. This representation is more advantageous than the typical object features extracted from an image since it is in natural language and allows for greater interpretability. The recently released GQA dataset <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib11" title="" class="ltx_ref">2019b</a>)</cite> contains 85,638 scene graphs for images in the training and validation sets, spurring research into the use of scene graphs for VQA.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we explore a set of research questions related to the use of scene graphs for VQA. Given the success of pre-trained models for other V+L tasks, we evaluate the effectiveness of transfer learning for GQA. The use of scene graphs requires converting them from a graphical representation to one that can be undeerstood by a neural network. We adopt the popular Graph Network model <cite class="ltx_cite ltx_citemacro_citep">(Battaglia et al., <a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite> to produce scene graph encodings and then adopt image-based question answering models to use these encodings. One of the challenges in using scene graphs lies in generating them for unseen images. We compare multiple scene graph generation models and study the impact of scene graph quality on model performance. We devise a novel training curriculum aimed at leveraging both ground truth and generated scene graphs to avoid a mismatch between training and testing scene graphs. Additionally, we explore late-fusion ensembles with the goal of using images as well as scene graphs, and combining the strengths of scene graph use with those of pre-training. We present the first in-depth analysis of the use of scene graphs for VQA. We will release our source code, end-to-end pipelines and pre-trained checkpoints in our GitHub repository<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/sharanyarc96/scene-graphs-for-vqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/sharanyarc96/scene-graphs-for-vqa</a></span></span></span>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The remainder of this paper is structured as follows. In Section <a href="#S2" title="2 Related Work ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we introduce existing approaches to VQA and formally define our task in Section <a href="#S3" title="3 Task Definition ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We describe our methodology and experimental setup in Section <a href="#S4" title="4 Proposed Approach ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S5" title="5 Experimental Setup ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> respectively. In Section <a href="#S6" title="6 Results and Analysis ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we provide an in-depth analysis of our results. We conclude the paper with a summary of our contributions in Section <a href="#S7" title="7 Conclusion and Future Work ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section we discuss VQA datasets, state of the art approaches for VQA, usage of scene graphs, and methods of scene graph generation.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Datasets for VQA</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite> released the VQA 2.0 dataset containing 204,721 real images and 1,105,904 open-ended questions requiring open-ended or multiple-choice answers. Prior works have identified statistical biases in VQA 2.0 with educated guess approaches working remarkably well. The CLEVR dataset <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite> was introduced with the goal of mitigating these biases and testing the visual reasoning capability of models using a large number of compositional questions. However, the number of objects in CLEVR is severely limited and the number of possible answers is just 28. Moreover, the images are synthetically generated. In this work, we conduct experiments on the GQA dataset <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib11" title="" class="ltx_ref">2019b</a>)</cite> which contains few statistical biases, retains the semantic richness of real images, contains open-ended questions of various degrees of compositionality and has a much larger vocabulary than CLEVR. Importantly for this work, it provides cleaned up versions of Visual Genome scene graphs <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al., <a href="#bib.bib14" title="" class="ltx_ref">2017</a>)</cite> for images in the training and validation sets.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Approaches to VQA</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">With the introduction of the VQA 2.0 and CLEVR datasets, many models have been proposed for the VQA task. This section covers some task specific and pre-trained models, each of which have beaten prior state of the art approaches for VQA.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Task Specific Models</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.2" class="ltx_p">We classify the set of models trained from scratch for a specific task on a single dataset as task specific. While the performance of these models may not match that of models which leverage transfer learning, they present interesting architectures that can be modified to use scene graphs. We discuss two such models here. 
<br class="ltx_break"><span id="S2.SS2.SSS1.p1.2.1" class="ltx_text ltx_font_bold">Bottom-Up Top-Down Attention (BUTD):</span>
<cite class="ltx_cite ltx_citemacro_citet">Anderson et al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite> propose a combined bottom-up and top-down mechanism for vision and language representation learning. For VQA, top-down attention uses the question, usually predicting an attention distribution over a uniform grid of equally-sized image regions. In contrast, bottom-up attention proposes a set of salient image regions using Faster R-CNN <cite class="ltx_cite ltx_citemacro_citep">(Ren et al., <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite>, which can be attended on. This architecture has proven beneficial for VQA and image captioning.

<br class="ltx_break"><span id="S2.SS2.SSS1.p1.2.2" class="ltx_text ltx_font_bold">MAC:</span> <cite class="ltx_cite ltx_citemacro_citet">Hudson and Manning (<a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite> introduce a neural network architecture that decomposes a question answering task into a series of attention-based reasoning steps, thus increasing the interpretability of the model’s decision making process. Given a knowledge-base <math id="S2.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.SS2.SSS1.p1.1.m1.1a"><mi id="S2.SS2.SSS1.p1.1.m1.1.1" xref="S2.SS2.SSS1.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.1.m1.1b"><ci id="S2.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.1.m1.1c">K</annotation></semantics></math> (image) and a task description <math id="S2.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S2.SS2.SSS1.p1.2.m2.1a"><mi id="S2.SS2.SSS1.p1.2.m2.1.1" xref="S2.SS2.SSS1.p1.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.2.m2.1b"><ci id="S2.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.2.m2.1c">Q</annotation></semantics></math> (question), the control unit <span id="S2.SS2.SSS1.p1.2.3" class="ltx_text ltx_font_smallcaps">(cu)</span> of the model identifies a series of operations to be performed, the read unit <span id="S2.SS2.SSS1.p1.2.4" class="ltx_text ltx_font_smallcaps">(ru)</span> extracts information needed to perform the operation from the image, and the write unit <span id="S2.SS2.SSS1.p1.2.5" class="ltx_text ltx_font_smallcaps">(wu)</span> integrates the information into an internal cell state to produce an intermediate result. The <span id="S2.SS2.SSS1.p1.2.6" class="ltx_text ltx_font_smallcaps">mac</span> model is one of the baselines introduced with the GQA dataset. Since, it is not a pre-trained model, it can easily be extended to use different knowledge bases without the additional overhead of pre-training.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Pre-Trained Models</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">In recent times, the introduction of pre-trained models such as BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite> has advanced the state of the art on a number of Natural Language Processing tasks. Similar benefits of using Transfer Learning have been seen for vision based tasks. Thus, it is not surprising that pre-training has also had a huge impact on the performance on several Vision + Language tasks. <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite> try to learn a UNiversal Image-TExt Representation (<span id="S2.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_smallcaps">uniter</span>) through large-scale pre-training over four image + text datasets. To this end, they design four pre-training tasks: Masked Language Modeling, Masked Region Modeling, ImageText Matching and Word-Region Alignment. <span id="S2.SS2.SSS2.p1.1.2" class="ltx_text ltx_font_smallcaps">uniter</span> achieves state of the art performance on challenges like VQA 2.0 and NLVR2, beating pre-trained models like <span id="S2.SS2.SSS2.p1.1.3" class="ltx_text ltx_font_smallcaps">lxmert</span> <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite> and task-specific models like <span id="S2.SS2.SSS2.p1.1.4" class="ltx_text ltx_font_smallcaps">mac</span>.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Using Scene Graphs for VQA</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">To the best of our knowledge, only a few prior works have used scene graphs for VQA. <cite class="ltx_cite ltx_citemacro_citet">Hudson and Manning (<a href="#bib.bib11" title="" class="ltx_ref">2019b</a>)</cite> report<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/stanfordnlp/mac-network/issues/22" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/stanfordnlp/mac-network/issues/22</a></span></span></span> a validation set accuracy of 83.5 using a “perfect sight” <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_smallcaps">mac</span> which replaces object features with a representation of the scene graph. The exact details of the scene graph encoding are unclear. <cite class="ltx_cite ltx_citemacro_citet">Lee et al. (<a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite> introduce another perfect sight model where the encoding of the scene graph is learnt using a Graph Network and is passed in place of the image to <span id="S2.SS3.p1.1.2" class="ltx_text ltx_font_smallcaps">mac</span>. Their model obtains a validation accuracy of 96.3 using ground truth scene graphs. <cite class="ltx_cite ltx_citemacro_citet">Hudson and Manning (<a href="#bib.bib9" title="" class="ltx_ref">2019a</a>)</cite> propose the task-specific Neural State Machine (NSM) which generates a probabilistic scene graph for the image and performs a series of reasoning operations over it. The generated scene graph contains a probability distribution for predicted objects, attributes, and relations. This allows the model to make predictions despite errors in the graph. While NSM gets a high accuracy, the lack of an open-source implementation and missing details required for reproducibility make comparisons with this model difficult.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Using scene graphs for other tasks</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Scene graphs have proven to be useful in various domains of computer vision and visual understanding. <cite class="ltx_cite ltx_citemacro_citet">Johnson et al. (<a href="#bib.bib12" title="" class="ltx_ref">2018</a>)</cite> studied the role of scene graphs in generating images using natural language descriptions. In the field of image outpainting, the work by <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite> has shown that the use of scene graphs produces much more effective visual results and quantitative evaluations. <cite class="ltx_cite ltx_citemacro_citet">Lee et al. (<a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite> proposes a unique Neural Design Network that uses scene graphs to great effect in the context of layout generation. The effectiveness of scene graphs displayed in these fields further motivates the need to understand the impact of scene graphs in the domain of visual question answering.</p>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Scene Graph Generation</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">The task of Scene Graph Generation has become an increasingly sought out challenge after discovering its potential to surpass the limits on visual reasoning and question answering tasks. Most works in scene graph generation rely on pre-trained object feature extractors like Faster-RCNN <cite class="ltx_cite ltx_citemacro_citep">(Ren et al., <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite> and Mask-RCNN <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite> to capture the object and attribute properties in the image. One of the earlier works in the field of scene graph generation, by <cite class="ltx_cite ltx_citemacro_citet">Zellers et al. (<a href="#bib.bib25" title="" class="ltx_ref">2018</a>)</cite> first introduced the concept of eliminating bias in the global context of determining relations with regards to relational edges. <cite class="ltx_cite ltx_citemacro_citet">Xu et al. (<a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite> first aimed to solve the task of scene graph generation using standard RNNs which learns to iteratively improve its predictions via message passing. Another top performing model by <cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite> constructs scene graphs in the form of tree structures (using TreeLSTMs) and performs very well on a plethora of visual reasoning tasks. The recent work by <cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite> looks to generate unbiased scene graphs with more specific relations across edges by building causal graphs and uses counterfactual causality from the trained graph to infer the effect from bad bias.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Task Definition</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.4" class="ltx_p">In this work, we tackle the multimodal VQA task which involves answering a question <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">q</annotation></semantics></math> given an image <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">I</annotation></semantics></math>. The question can be expressed as the sequence <math id="S3.p1.3.m3.4" class="ltx_Math" alttext="(w_{1},w_{2},...,w_{n})" display="inline"><semantics id="S3.p1.3.m3.4a"><mrow id="S3.p1.3.m3.4.4.3" xref="S3.p1.3.m3.4.4.4.cmml"><mo stretchy="false" id="S3.p1.3.m3.4.4.3.4" xref="S3.p1.3.m3.4.4.4.cmml">(</mo><msub id="S3.p1.3.m3.2.2.1.1" xref="S3.p1.3.m3.2.2.1.1.cmml"><mi id="S3.p1.3.m3.2.2.1.1.2" xref="S3.p1.3.m3.2.2.1.1.2.cmml">w</mi><mn id="S3.p1.3.m3.2.2.1.1.3" xref="S3.p1.3.m3.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.p1.3.m3.4.4.3.5" xref="S3.p1.3.m3.4.4.4.cmml">,</mo><msub id="S3.p1.3.m3.3.3.2.2" xref="S3.p1.3.m3.3.3.2.2.cmml"><mi id="S3.p1.3.m3.3.3.2.2.2" xref="S3.p1.3.m3.3.3.2.2.2.cmml">w</mi><mn id="S3.p1.3.m3.3.3.2.2.3" xref="S3.p1.3.m3.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.p1.3.m3.4.4.3.6" xref="S3.p1.3.m3.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">…</mi><mo id="S3.p1.3.m3.4.4.3.7" xref="S3.p1.3.m3.4.4.4.cmml">,</mo><msub id="S3.p1.3.m3.4.4.3.3" xref="S3.p1.3.m3.4.4.3.3.cmml"><mi id="S3.p1.3.m3.4.4.3.3.2" xref="S3.p1.3.m3.4.4.3.3.2.cmml">w</mi><mi id="S3.p1.3.m3.4.4.3.3.3" xref="S3.p1.3.m3.4.4.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="S3.p1.3.m3.4.4.3.8" xref="S3.p1.3.m3.4.4.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.4b"><vector id="S3.p1.3.m3.4.4.4.cmml" xref="S3.p1.3.m3.4.4.3"><apply id="S3.p1.3.m3.2.2.1.1.cmml" xref="S3.p1.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.2.2.1.1.1.cmml" xref="S3.p1.3.m3.2.2.1.1">subscript</csymbol><ci id="S3.p1.3.m3.2.2.1.1.2.cmml" xref="S3.p1.3.m3.2.2.1.1.2">𝑤</ci><cn type="integer" id="S3.p1.3.m3.2.2.1.1.3.cmml" xref="S3.p1.3.m3.2.2.1.1.3">1</cn></apply><apply id="S3.p1.3.m3.3.3.2.2.cmml" xref="S3.p1.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S3.p1.3.m3.3.3.2.2.1.cmml" xref="S3.p1.3.m3.3.3.2.2">subscript</csymbol><ci id="S3.p1.3.m3.3.3.2.2.2.cmml" xref="S3.p1.3.m3.3.3.2.2.2">𝑤</ci><cn type="integer" id="S3.p1.3.m3.3.3.2.2.3.cmml" xref="S3.p1.3.m3.3.3.2.2.3">2</cn></apply><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">…</ci><apply id="S3.p1.3.m3.4.4.3.3.cmml" xref="S3.p1.3.m3.4.4.3.3"><csymbol cd="ambiguous" id="S3.p1.3.m3.4.4.3.3.1.cmml" xref="S3.p1.3.m3.4.4.3.3">subscript</csymbol><ci id="S3.p1.3.m3.4.4.3.3.2.cmml" xref="S3.p1.3.m3.4.4.3.3.2">𝑤</ci><ci id="S3.p1.3.m3.4.4.3.3.3.cmml" xref="S3.p1.3.m3.4.4.3.3.3">𝑛</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.4c">(w_{1},w_{2},...,w_{n})</annotation></semantics></math> where <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="w_{i}" display="inline"><semantics id="S3.p1.4.m4.1a"><msub id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml"><mi id="S3.p1.4.m4.1.1.2" xref="S3.p1.4.m4.1.1.2.cmml">w</mi><mi id="S3.p1.4.m4.1.1.3" xref="S3.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.1.cmml" xref="S3.p1.4.m4.1.1">subscript</csymbol><ci id="S3.p1.4.m4.1.1.2.cmml" xref="S3.p1.4.m4.1.1.2">𝑤</ci><ci id="S3.p1.4.m4.1.1.3.cmml" xref="S3.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">w_{i}</annotation></semantics></math> is a single word/token. We work with the following representations of the image:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Spatial Feature: A single vector <math id="S3.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.I1.i1.p1.1.m1.1a"><mi id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><ci id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">S</annotation></semantics></math> for the entire image</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.3" class="ltx_p">Object Features: A set of <math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="N^{v}" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><msup id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.2" xref="S3.I1.i2.p1.1.m1.1.1.2.cmml">N</mi><mi id="S3.I1.i2.p1.1.m1.1.1.3" xref="S3.I1.i2.p1.1.m1.1.1.3.cmml">v</mi></msup><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><apply id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">superscript</csymbol><ci id="S3.I1.i2.p1.1.m1.1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2">𝑁</ci><ci id="S3.I1.i2.p1.1.m1.1.1.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">N^{v}</annotation></semantics></math> vectors <math id="S3.I1.i2.p1.2.m2.4" class="ltx_Math" alttext="O=(o_{1},o_{2},...,o_{N^{v}})" display="inline"><semantics id="S3.I1.i2.p1.2.m2.4a"><mrow id="S3.I1.i2.p1.2.m2.4.4" xref="S3.I1.i2.p1.2.m2.4.4.cmml"><mi id="S3.I1.i2.p1.2.m2.4.4.5" xref="S3.I1.i2.p1.2.m2.4.4.5.cmml">O</mi><mo id="S3.I1.i2.p1.2.m2.4.4.4" xref="S3.I1.i2.p1.2.m2.4.4.4.cmml">=</mo><mrow id="S3.I1.i2.p1.2.m2.4.4.3.3" xref="S3.I1.i2.p1.2.m2.4.4.3.4.cmml"><mo stretchy="false" id="S3.I1.i2.p1.2.m2.4.4.3.3.4" xref="S3.I1.i2.p1.2.m2.4.4.3.4.cmml">(</mo><msub id="S3.I1.i2.p1.2.m2.2.2.1.1.1" xref="S3.I1.i2.p1.2.m2.2.2.1.1.1.cmml"><mi id="S3.I1.i2.p1.2.m2.2.2.1.1.1.2" xref="S3.I1.i2.p1.2.m2.2.2.1.1.1.2.cmml">o</mi><mn id="S3.I1.i2.p1.2.m2.2.2.1.1.1.3" xref="S3.I1.i2.p1.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.I1.i2.p1.2.m2.4.4.3.3.5" xref="S3.I1.i2.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S3.I1.i2.p1.2.m2.3.3.2.2.2" xref="S3.I1.i2.p1.2.m2.3.3.2.2.2.cmml"><mi id="S3.I1.i2.p1.2.m2.3.3.2.2.2.2" xref="S3.I1.i2.p1.2.m2.3.3.2.2.2.2.cmml">o</mi><mn id="S3.I1.i2.p1.2.m2.3.3.2.2.2.3" xref="S3.I1.i2.p1.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.I1.i2.p1.2.m2.4.4.3.3.6" xref="S3.I1.i2.p1.2.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml">…</mi><mo id="S3.I1.i2.p1.2.m2.4.4.3.3.7" xref="S3.I1.i2.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S3.I1.i2.p1.2.m2.4.4.3.3.3" xref="S3.I1.i2.p1.2.m2.4.4.3.3.3.cmml"><mi id="S3.I1.i2.p1.2.m2.4.4.3.3.3.2" xref="S3.I1.i2.p1.2.m2.4.4.3.3.3.2.cmml">o</mi><msup id="S3.I1.i2.p1.2.m2.4.4.3.3.3.3" xref="S3.I1.i2.p1.2.m2.4.4.3.3.3.3.cmml"><mi id="S3.I1.i2.p1.2.m2.4.4.3.3.3.3.2" xref="S3.I1.i2.p1.2.m2.4.4.3.3.3.3.2.cmml">N</mi><mi id="S3.I1.i2.p1.2.m2.4.4.3.3.3.3.3" xref="S3.I1.i2.p1.2.m2.4.4.3.3.3.3.3.cmml">v</mi></msup></msub><mo stretchy="false" id="S3.I1.i2.p1.2.m2.4.4.3.3.8" xref="S3.I1.i2.p1.2.m2.4.4.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.4b"><apply id="S3.I1.i2.p1.2.m2.4.4.cmml" xref="S3.I1.i2.p1.2.m2.4.4"><eq id="S3.I1.i2.p1.2.m2.4.4.4.cmml" xref="S3.I1.i2.p1.2.m2.4.4.4"></eq><ci id="S3.I1.i2.p1.2.m2.4.4.5.cmml" xref="S3.I1.i2.p1.2.m2.4.4.5">𝑂</ci><vector id="S3.I1.i2.p1.2.m2.4.4.3.4.cmml" xref="S3.I1.i2.p1.2.m2.4.4.3.3"><apply id="S3.I1.i2.p1.2.m2.2.2.1.1.1.cmml" xref="S3.I1.i2.p1.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.2.m2.2.2.1.1.1.1.cmml" xref="S3.I1.i2.p1.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.2.m2.2.2.1.1.1.2.cmml" xref="S3.I1.i2.p1.2.m2.2.2.1.1.1.2">𝑜</ci><cn type="integer" id="S3.I1.i2.p1.2.m2.2.2.1.1.1.3.cmml" xref="S3.I1.i2.p1.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S3.I1.i2.p1.2.m2.3.3.2.2.2.cmml" xref="S3.I1.i2.p1.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i2.p1.2.m2.3.3.2.2.2.1.cmml" xref="S3.I1.i2.p1.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S3.I1.i2.p1.2.m2.3.3.2.2.2.2.cmml" xref="S3.I1.i2.p1.2.m2.3.3.2.2.2.2">𝑜</ci><cn type="integer" id="S3.I1.i2.p1.2.m2.3.3.2.2.2.3.cmml" xref="S3.I1.i2.p1.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">…</ci><apply id="S3.I1.i2.p1.2.m2.4.4.3.3.3.cmml" xref="S3.I1.i2.p1.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.I1.i2.p1.2.m2.4.4.3.3.3.1.cmml" xref="S3.I1.i2.p1.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S3.I1.i2.p1.2.m2.4.4.3.3.3.2.cmml" xref="S3.I1.i2.p1.2.m2.4.4.3.3.3.2">𝑜</ci><apply id="S3.I1.i2.p1.2.m2.4.4.3.3.3.3.cmml" xref="S3.I1.i2.p1.2.m2.4.4.3.3.3.3"><csymbol cd="ambiguous" id="S3.I1.i2.p1.2.m2.4.4.3.3.3.3.1.cmml" xref="S3.I1.i2.p1.2.m2.4.4.3.3.3.3">superscript</csymbol><ci id="S3.I1.i2.p1.2.m2.4.4.3.3.3.3.2.cmml" xref="S3.I1.i2.p1.2.m2.4.4.3.3.3.3.2">𝑁</ci><ci id="S3.I1.i2.p1.2.m2.4.4.3.3.3.3.3.cmml" xref="S3.I1.i2.p1.2.m2.4.4.3.3.3.3.3">𝑣</ci></apply></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.4c">O=(o_{1},o_{2},...,o_{N^{v}})</annotation></semantics></math> representing <math id="S3.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="N^{v}" display="inline"><semantics id="S3.I1.i2.p1.3.m3.1a"><msup id="S3.I1.i2.p1.3.m3.1.1" xref="S3.I1.i2.p1.3.m3.1.1.cmml"><mi id="S3.I1.i2.p1.3.m3.1.1.2" xref="S3.I1.i2.p1.3.m3.1.1.2.cmml">N</mi><mi id="S3.I1.i2.p1.3.m3.1.1.3" xref="S3.I1.i2.p1.3.m3.1.1.3.cmml">v</mi></msup><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.3.m3.1b"><apply id="S3.I1.i2.p1.3.m3.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.3.m3.1.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1">superscript</csymbol><ci id="S3.I1.i2.p1.3.m3.1.1.2.cmml" xref="S3.I1.i2.p1.3.m3.1.1.2">𝑁</ci><ci id="S3.I1.i2.p1.3.m3.1.1.3.cmml" xref="S3.I1.i2.p1.3.m3.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.1c">N^{v}</annotation></semantics></math> objects in the image</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Scene Graph: A graphical representation <math id="S3.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S3.I1.i3.p1.1.m1.1a"><mi id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><ci id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">g</annotation></semantics></math> of the contents of the image</p>
</div>
</li>
</ul>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.11" class="ltx_p">We formally define the scene graph in Equation  (<a href="#S3.E1" title="Equation 1 ‣ 3 Task Definition ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), where <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">V</annotation></semantics></math> is the set of nodes representing objects in the image, <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.p2.2.m2.1a"><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">E</annotation></semantics></math> is the set of edges representing relations between objects, and <math id="S3.p2.3.m3.1" class="ltx_Math" alttext="u" display="inline"><semantics id="S3.p2.3.m3.1a"><mi id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><ci id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">u</annotation></semantics></math> is a global feature shared by all components of the graph. <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="N^{v}" display="inline"><semantics id="S3.p2.4.m4.1a"><msup id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml"><mi id="S3.p2.4.m4.1.1.2" xref="S3.p2.4.m4.1.1.2.cmml">N</mi><mi id="S3.p2.4.m4.1.1.3" xref="S3.p2.4.m4.1.1.3.cmml">v</mi></msup><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><apply id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p2.4.m4.1.1.1.cmml" xref="S3.p2.4.m4.1.1">superscript</csymbol><ci id="S3.p2.4.m4.1.1.2.cmml" xref="S3.p2.4.m4.1.1.2">𝑁</ci><ci id="S3.p2.4.m4.1.1.3.cmml" xref="S3.p2.4.m4.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">N^{v}</annotation></semantics></math> is the number of objects in the image, and <math id="S3.p2.5.m5.1" class="ltx_Math" alttext="N^{e}" display="inline"><semantics id="S3.p2.5.m5.1a"><msup id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml"><mi id="S3.p2.5.m5.1.1.2" xref="S3.p2.5.m5.1.1.2.cmml">N</mi><mi id="S3.p2.5.m5.1.1.3" xref="S3.p2.5.m5.1.1.3.cmml">e</mi></msup><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><apply id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p2.5.m5.1.1.1.cmml" xref="S3.p2.5.m5.1.1">superscript</csymbol><ci id="S3.p2.5.m5.1.1.2.cmml" xref="S3.p2.5.m5.1.1.2">𝑁</ci><ci id="S3.p2.5.m5.1.1.3.cmml" xref="S3.p2.5.m5.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">N^{e}</annotation></semantics></math> is the number of relations. <math id="S3.p2.6.m6.1" class="ltx_Math" alttext="v_{i}^{name}" display="inline"><semantics id="S3.p2.6.m6.1a"><msubsup id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml"><mi id="S3.p2.6.m6.1.1.2.2" xref="S3.p2.6.m6.1.1.2.2.cmml">v</mi><mi id="S3.p2.6.m6.1.1.2.3" xref="S3.p2.6.m6.1.1.2.3.cmml">i</mi><mrow id="S3.p2.6.m6.1.1.3" xref="S3.p2.6.m6.1.1.3.cmml"><mi id="S3.p2.6.m6.1.1.3.2" xref="S3.p2.6.m6.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.p2.6.m6.1.1.3.1" xref="S3.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.p2.6.m6.1.1.3.3" xref="S3.p2.6.m6.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.p2.6.m6.1.1.3.1a" xref="S3.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.p2.6.m6.1.1.3.4" xref="S3.p2.6.m6.1.1.3.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.p2.6.m6.1.1.3.1b" xref="S3.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.p2.6.m6.1.1.3.5" xref="S3.p2.6.m6.1.1.3.5.cmml">e</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><apply id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p2.6.m6.1.1.1.cmml" xref="S3.p2.6.m6.1.1">superscript</csymbol><apply id="S3.p2.6.m6.1.1.2.cmml" xref="S3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p2.6.m6.1.1.2.1.cmml" xref="S3.p2.6.m6.1.1">subscript</csymbol><ci id="S3.p2.6.m6.1.1.2.2.cmml" xref="S3.p2.6.m6.1.1.2.2">𝑣</ci><ci id="S3.p2.6.m6.1.1.2.3.cmml" xref="S3.p2.6.m6.1.1.2.3">𝑖</ci></apply><apply id="S3.p2.6.m6.1.1.3.cmml" xref="S3.p2.6.m6.1.1.3"><times id="S3.p2.6.m6.1.1.3.1.cmml" xref="S3.p2.6.m6.1.1.3.1"></times><ci id="S3.p2.6.m6.1.1.3.2.cmml" xref="S3.p2.6.m6.1.1.3.2">𝑛</ci><ci id="S3.p2.6.m6.1.1.3.3.cmml" xref="S3.p2.6.m6.1.1.3.3">𝑎</ci><ci id="S3.p2.6.m6.1.1.3.4.cmml" xref="S3.p2.6.m6.1.1.3.4">𝑚</ci><ci id="S3.p2.6.m6.1.1.3.5.cmml" xref="S3.p2.6.m6.1.1.3.5">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">v_{i}^{name}</annotation></semantics></math> is the name of the object corresponding to the node <math id="S3.p2.7.m7.1" class="ltx_Math" alttext="v_{i}" display="inline"><semantics id="S3.p2.7.m7.1a"><msub id="S3.p2.7.m7.1.1" xref="S3.p2.7.m7.1.1.cmml"><mi id="S3.p2.7.m7.1.1.2" xref="S3.p2.7.m7.1.1.2.cmml">v</mi><mi id="S3.p2.7.m7.1.1.3" xref="S3.p2.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.7.m7.1b"><apply id="S3.p2.7.m7.1.1.cmml" xref="S3.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.p2.7.m7.1.1.1.cmml" xref="S3.p2.7.m7.1.1">subscript</csymbol><ci id="S3.p2.7.m7.1.1.2.cmml" xref="S3.p2.7.m7.1.1.2">𝑣</ci><ci id="S3.p2.7.m7.1.1.3.cmml" xref="S3.p2.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m7.1c">v_{i}</annotation></semantics></math> and <math id="S3.p2.8.m8.1" class="ltx_Math" alttext="v_{i}^{attr}" display="inline"><semantics id="S3.p2.8.m8.1a"><msubsup id="S3.p2.8.m8.1.1" xref="S3.p2.8.m8.1.1.cmml"><mi id="S3.p2.8.m8.1.1.2.2" xref="S3.p2.8.m8.1.1.2.2.cmml">v</mi><mi id="S3.p2.8.m8.1.1.2.3" xref="S3.p2.8.m8.1.1.2.3.cmml">i</mi><mrow id="S3.p2.8.m8.1.1.3" xref="S3.p2.8.m8.1.1.3.cmml"><mi id="S3.p2.8.m8.1.1.3.2" xref="S3.p2.8.m8.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.p2.8.m8.1.1.3.1" xref="S3.p2.8.m8.1.1.3.1.cmml">​</mo><mi id="S3.p2.8.m8.1.1.3.3" xref="S3.p2.8.m8.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.p2.8.m8.1.1.3.1a" xref="S3.p2.8.m8.1.1.3.1.cmml">​</mo><mi id="S3.p2.8.m8.1.1.3.4" xref="S3.p2.8.m8.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.p2.8.m8.1.1.3.1b" xref="S3.p2.8.m8.1.1.3.1.cmml">​</mo><mi id="S3.p2.8.m8.1.1.3.5" xref="S3.p2.8.m8.1.1.3.5.cmml">r</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.p2.8.m8.1b"><apply id="S3.p2.8.m8.1.1.cmml" xref="S3.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.p2.8.m8.1.1.1.cmml" xref="S3.p2.8.m8.1.1">superscript</csymbol><apply id="S3.p2.8.m8.1.1.2.cmml" xref="S3.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.p2.8.m8.1.1.2.1.cmml" xref="S3.p2.8.m8.1.1">subscript</csymbol><ci id="S3.p2.8.m8.1.1.2.2.cmml" xref="S3.p2.8.m8.1.1.2.2">𝑣</ci><ci id="S3.p2.8.m8.1.1.2.3.cmml" xref="S3.p2.8.m8.1.1.2.3">𝑖</ci></apply><apply id="S3.p2.8.m8.1.1.3.cmml" xref="S3.p2.8.m8.1.1.3"><times id="S3.p2.8.m8.1.1.3.1.cmml" xref="S3.p2.8.m8.1.1.3.1"></times><ci id="S3.p2.8.m8.1.1.3.2.cmml" xref="S3.p2.8.m8.1.1.3.2">𝑎</ci><ci id="S3.p2.8.m8.1.1.3.3.cmml" xref="S3.p2.8.m8.1.1.3.3">𝑡</ci><ci id="S3.p2.8.m8.1.1.3.4.cmml" xref="S3.p2.8.m8.1.1.3.4">𝑡</ci><ci id="S3.p2.8.m8.1.1.3.5.cmml" xref="S3.p2.8.m8.1.1.3.5">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.8.m8.1c">v_{i}^{attr}</annotation></semantics></math> is the set of attributes of the object. Similarly, <math id="S3.p2.9.m9.1" class="ltx_Math" alttext="e_{j}^{name}" display="inline"><semantics id="S3.p2.9.m9.1a"><msubsup id="S3.p2.9.m9.1.1" xref="S3.p2.9.m9.1.1.cmml"><mi id="S3.p2.9.m9.1.1.2.2" xref="S3.p2.9.m9.1.1.2.2.cmml">e</mi><mi id="S3.p2.9.m9.1.1.2.3" xref="S3.p2.9.m9.1.1.2.3.cmml">j</mi><mrow id="S3.p2.9.m9.1.1.3" xref="S3.p2.9.m9.1.1.3.cmml"><mi id="S3.p2.9.m9.1.1.3.2" xref="S3.p2.9.m9.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.p2.9.m9.1.1.3.1" xref="S3.p2.9.m9.1.1.3.1.cmml">​</mo><mi id="S3.p2.9.m9.1.1.3.3" xref="S3.p2.9.m9.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.p2.9.m9.1.1.3.1a" xref="S3.p2.9.m9.1.1.3.1.cmml">​</mo><mi id="S3.p2.9.m9.1.1.3.4" xref="S3.p2.9.m9.1.1.3.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.p2.9.m9.1.1.3.1b" xref="S3.p2.9.m9.1.1.3.1.cmml">​</mo><mi id="S3.p2.9.m9.1.1.3.5" xref="S3.p2.9.m9.1.1.3.5.cmml">e</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.p2.9.m9.1b"><apply id="S3.p2.9.m9.1.1.cmml" xref="S3.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.p2.9.m9.1.1.1.cmml" xref="S3.p2.9.m9.1.1">superscript</csymbol><apply id="S3.p2.9.m9.1.1.2.cmml" xref="S3.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.p2.9.m9.1.1.2.1.cmml" xref="S3.p2.9.m9.1.1">subscript</csymbol><ci id="S3.p2.9.m9.1.1.2.2.cmml" xref="S3.p2.9.m9.1.1.2.2">𝑒</ci><ci id="S3.p2.9.m9.1.1.2.3.cmml" xref="S3.p2.9.m9.1.1.2.3">𝑗</ci></apply><apply id="S3.p2.9.m9.1.1.3.cmml" xref="S3.p2.9.m9.1.1.3"><times id="S3.p2.9.m9.1.1.3.1.cmml" xref="S3.p2.9.m9.1.1.3.1"></times><ci id="S3.p2.9.m9.1.1.3.2.cmml" xref="S3.p2.9.m9.1.1.3.2">𝑛</ci><ci id="S3.p2.9.m9.1.1.3.3.cmml" xref="S3.p2.9.m9.1.1.3.3">𝑎</ci><ci id="S3.p2.9.m9.1.1.3.4.cmml" xref="S3.p2.9.m9.1.1.3.4">𝑚</ci><ci id="S3.p2.9.m9.1.1.3.5.cmml" xref="S3.p2.9.m9.1.1.3.5">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.9.m9.1c">e_{j}^{name}</annotation></semantics></math> is the name of the relation between a source object / node <math id="S3.p2.10.m10.1" class="ltx_Math" alttext="e_{j}^{s}" display="inline"><semantics id="S3.p2.10.m10.1a"><msubsup id="S3.p2.10.m10.1.1" xref="S3.p2.10.m10.1.1.cmml"><mi id="S3.p2.10.m10.1.1.2.2" xref="S3.p2.10.m10.1.1.2.2.cmml">e</mi><mi id="S3.p2.10.m10.1.1.2.3" xref="S3.p2.10.m10.1.1.2.3.cmml">j</mi><mi id="S3.p2.10.m10.1.1.3" xref="S3.p2.10.m10.1.1.3.cmml">s</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.p2.10.m10.1b"><apply id="S3.p2.10.m10.1.1.cmml" xref="S3.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.p2.10.m10.1.1.1.cmml" xref="S3.p2.10.m10.1.1">superscript</csymbol><apply id="S3.p2.10.m10.1.1.2.cmml" xref="S3.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.p2.10.m10.1.1.2.1.cmml" xref="S3.p2.10.m10.1.1">subscript</csymbol><ci id="S3.p2.10.m10.1.1.2.2.cmml" xref="S3.p2.10.m10.1.1.2.2">𝑒</ci><ci id="S3.p2.10.m10.1.1.2.3.cmml" xref="S3.p2.10.m10.1.1.2.3">𝑗</ci></apply><ci id="S3.p2.10.m10.1.1.3.cmml" xref="S3.p2.10.m10.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.10.m10.1c">e_{j}^{s}</annotation></semantics></math> and a receiver object <math id="S3.p2.11.m11.1" class="ltx_Math" alttext="e_{j}^{r}" display="inline"><semantics id="S3.p2.11.m11.1a"><msubsup id="S3.p2.11.m11.1.1" xref="S3.p2.11.m11.1.1.cmml"><mi id="S3.p2.11.m11.1.1.2.2" xref="S3.p2.11.m11.1.1.2.2.cmml">e</mi><mi id="S3.p2.11.m11.1.1.2.3" xref="S3.p2.11.m11.1.1.2.3.cmml">j</mi><mi id="S3.p2.11.m11.1.1.3" xref="S3.p2.11.m11.1.1.3.cmml">r</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.p2.11.m11.1b"><apply id="S3.p2.11.m11.1.1.cmml" xref="S3.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S3.p2.11.m11.1.1.1.cmml" xref="S3.p2.11.m11.1.1">superscript</csymbol><apply id="S3.p2.11.m11.1.1.2.cmml" xref="S3.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S3.p2.11.m11.1.1.2.1.cmml" xref="S3.p2.11.m11.1.1">subscript</csymbol><ci id="S3.p2.11.m11.1.1.2.2.cmml" xref="S3.p2.11.m11.1.1.2.2">𝑒</ci><ci id="S3.p2.11.m11.1.1.2.3.cmml" xref="S3.p2.11.m11.1.1.2.3">𝑗</ci></apply><ci id="S3.p2.11.m11.1.1.3.cmml" xref="S3.p2.11.m11.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.11.m11.1c">e_{j}^{r}</annotation></semantics></math>.</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.84" class="ltx_Math" alttext="\begin{gathered}g=(E,V,u)\\
V=(v_{1},v_{2},...,v_{N^{v}});v_{i}=(v_{i}^{name},v_{i}^{attr});\\
E=(e_{1},e_{2},...,e_{N^{e}});e_{j}=(e_{j}^{name},e_{j}^{s},e_{j}^{r});\\
e_{j}^{s},e_{j}^{r}\in V\end{gathered}" display="block"><semantics id="S3.E1.m1.84a"><mtable displaystyle="true" rowspacing="0pt" id="S3.E1.m1.84.84.6"><mtr id="S3.E1.m1.84.84.6a"><mtd id="S3.E1.m1.84.84.6b"><mrow id="S3.E1.m1.9.9.9.9.9"><mi id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml">g</mi><mo id="S3.E1.m1.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.cmml">=</mo><mrow id="S3.E1.m1.9.9.9.9.9.10"><mo stretchy="false" id="S3.E1.m1.3.3.3.3.3.3" xref="S3.E1.m1.80.80.2.3.cmml">(</mo><mi id="S3.E1.m1.4.4.4.4.4.4" xref="S3.E1.m1.4.4.4.4.4.4.cmml">E</mi><mo id="S3.E1.m1.5.5.5.5.5.5" xref="S3.E1.m1.80.80.2.3.cmml">,</mo><mi id="S3.E1.m1.6.6.6.6.6.6" xref="S3.E1.m1.6.6.6.6.6.6.cmml">V</mi><mo id="S3.E1.m1.7.7.7.7.7.7" xref="S3.E1.m1.80.80.2.3.cmml">,</mo><mi id="S3.E1.m1.8.8.8.8.8.8" xref="S3.E1.m1.8.8.8.8.8.8.cmml">u</mi><mo stretchy="false" id="S3.E1.m1.9.9.9.9.9.9" xref="S3.E1.m1.80.80.2.3.cmml">)</mo></mrow></mrow></mtd></mtr><mtr id="S3.E1.m1.84.84.6c"><mtd id="S3.E1.m1.84.84.6d"><mrow id="S3.E1.m1.81.81.3.79.29.29.29"><mrow id="S3.E1.m1.81.81.3.79.29.29.29.1"><mrow id="S3.E1.m1.81.81.3.79.29.29.29.1.1.1"><mi id="S3.E1.m1.10.10.10.1.1.1" xref="S3.E1.m1.10.10.10.1.1.1.cmml">V</mi><mo id="S3.E1.m1.11.11.11.2.2.2" xref="S3.E1.m1.11.11.11.2.2.2.cmml">=</mo><mrow id="S3.E1.m1.81.81.3.79.29.29.29.1.1.1.3.3"><mo stretchy="false" id="S3.E1.m1.12.12.12.3.3.3" xref="S3.E1.m1.80.80.2.3.cmml">(</mo><msub id="S3.E1.m1.81.81.3.79.29.29.29.1.1.1.1.1.1"><mi id="S3.E1.m1.13.13.13.4.4.4" xref="S3.E1.m1.13.13.13.4.4.4.cmml">v</mi><mn id="S3.E1.m1.14.14.14.5.5.5.1" xref="S3.E1.m1.14.14.14.5.5.5.1.cmml">1</mn></msub><mo id="S3.E1.m1.15.15.15.6.6.6" xref="S3.E1.m1.80.80.2.3.cmml">,</mo><msub id="S3.E1.m1.81.81.3.79.29.29.29.1.1.1.2.2.2"><mi id="S3.E1.m1.16.16.16.7.7.7" xref="S3.E1.m1.16.16.16.7.7.7.cmml">v</mi><mn id="S3.E1.m1.17.17.17.8.8.8.1" xref="S3.E1.m1.17.17.17.8.8.8.1.cmml">2</mn></msub><mo id="S3.E1.m1.18.18.18.9.9.9" xref="S3.E1.m1.80.80.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.E1.m1.19.19.19.10.10.10" xref="S3.E1.m1.19.19.19.10.10.10.cmml">…</mi><mo id="S3.E1.m1.20.20.20.11.11.11" xref="S3.E1.m1.80.80.2.3.cmml">,</mo><msub id="S3.E1.m1.81.81.3.79.29.29.29.1.1.1.3.3.3"><mi id="S3.E1.m1.21.21.21.12.12.12" xref="S3.E1.m1.21.21.21.12.12.12.cmml">v</mi><msup id="S3.E1.m1.22.22.22.13.13.13.1" xref="S3.E1.m1.22.22.22.13.13.13.1.cmml"><mi id="S3.E1.m1.22.22.22.13.13.13.1.2" xref="S3.E1.m1.22.22.22.13.13.13.1.2.cmml">N</mi><mi id="S3.E1.m1.22.22.22.13.13.13.1.3" xref="S3.E1.m1.22.22.22.13.13.13.1.3.cmml">v</mi></msup></msub><mo stretchy="false" id="S3.E1.m1.23.23.23.14.14.14" xref="S3.E1.m1.80.80.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.24.24.24.15.15.15" xref="S3.E1.m1.80.80.2.3.cmml">;</mo><mrow id="S3.E1.m1.81.81.3.79.29.29.29.1.2.2"><msub id="S3.E1.m1.81.81.3.79.29.29.29.1.2.2.3"><mi id="S3.E1.m1.25.25.25.16.16.16" xref="S3.E1.m1.25.25.25.16.16.16.cmml">v</mi><mi id="S3.E1.m1.26.26.26.17.17.17.1" xref="S3.E1.m1.26.26.26.17.17.17.1.cmml">i</mi></msub><mo id="S3.E1.m1.27.27.27.18.18.18" xref="S3.E1.m1.27.27.27.18.18.18.cmml">=</mo><mrow id="S3.E1.m1.81.81.3.79.29.29.29.1.2.2.2.2"><mo stretchy="false" id="S3.E1.m1.28.28.28.19.19.19" xref="S3.E1.m1.80.80.2.3.cmml">(</mo><msubsup id="S3.E1.m1.81.81.3.79.29.29.29.1.2.2.1.1.1"><mi id="S3.E1.m1.29.29.29.20.20.20" xref="S3.E1.m1.29.29.29.20.20.20.cmml">v</mi><mi id="S3.E1.m1.30.30.30.21.21.21.1" xref="S3.E1.m1.30.30.30.21.21.21.1.cmml">i</mi><mrow id="S3.E1.m1.31.31.31.22.22.22.1" xref="S3.E1.m1.31.31.31.22.22.22.1.cmml"><mi id="S3.E1.m1.31.31.31.22.22.22.1.2" xref="S3.E1.m1.31.31.31.22.22.22.1.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.31.31.31.22.22.22.1.1" xref="S3.E1.m1.31.31.31.22.22.22.1.1.cmml">​</mo><mi id="S3.E1.m1.31.31.31.22.22.22.1.3" xref="S3.E1.m1.31.31.31.22.22.22.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.31.31.31.22.22.22.1.1a" xref="S3.E1.m1.31.31.31.22.22.22.1.1.cmml">​</mo><mi id="S3.E1.m1.31.31.31.22.22.22.1.4" xref="S3.E1.m1.31.31.31.22.22.22.1.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.31.31.31.22.22.22.1.1b" xref="S3.E1.m1.31.31.31.22.22.22.1.1.cmml">​</mo><mi id="S3.E1.m1.31.31.31.22.22.22.1.5" xref="S3.E1.m1.31.31.31.22.22.22.1.5.cmml">e</mi></mrow></msubsup><mo id="S3.E1.m1.32.32.32.23.23.23" xref="S3.E1.m1.80.80.2.3.cmml">,</mo><msubsup id="S3.E1.m1.81.81.3.79.29.29.29.1.2.2.2.2.2"><mi id="S3.E1.m1.33.33.33.24.24.24" xref="S3.E1.m1.33.33.33.24.24.24.cmml">v</mi><mi id="S3.E1.m1.34.34.34.25.25.25.1" xref="S3.E1.m1.34.34.34.25.25.25.1.cmml">i</mi><mrow id="S3.E1.m1.35.35.35.26.26.26.1" xref="S3.E1.m1.35.35.35.26.26.26.1.cmml"><mi id="S3.E1.m1.35.35.35.26.26.26.1.2" xref="S3.E1.m1.35.35.35.26.26.26.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.35.35.35.26.26.26.1.1" xref="S3.E1.m1.35.35.35.26.26.26.1.1.cmml">​</mo><mi id="S3.E1.m1.35.35.35.26.26.26.1.3" xref="S3.E1.m1.35.35.35.26.26.26.1.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.35.35.35.26.26.26.1.1a" xref="S3.E1.m1.35.35.35.26.26.26.1.1.cmml">​</mo><mi id="S3.E1.m1.35.35.35.26.26.26.1.4" xref="S3.E1.m1.35.35.35.26.26.26.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.35.35.35.26.26.26.1.1b" xref="S3.E1.m1.35.35.35.26.26.26.1.1.cmml">​</mo><mi id="S3.E1.m1.35.35.35.26.26.26.1.5" xref="S3.E1.m1.35.35.35.26.26.26.1.5.cmml">r</mi></mrow></msubsup><mo stretchy="false" id="S3.E1.m1.36.36.36.27.27.27" xref="S3.E1.m1.80.80.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.37.37.37.28.28.28" xref="S3.E1.m1.80.80.2.3.cmml">;</mo></mrow></mtd></mtr><mtr id="S3.E1.m1.84.84.6e"><mtd id="S3.E1.m1.84.84.6f"><mrow id="S3.E1.m1.82.82.4.80.33.33.33"><mrow id="S3.E1.m1.82.82.4.80.33.33.33.1"><mrow id="S3.E1.m1.82.82.4.80.33.33.33.1.1.1"><mi id="S3.E1.m1.38.38.38.1.1.1" xref="S3.E1.m1.38.38.38.1.1.1.cmml">E</mi><mo id="S3.E1.m1.39.39.39.2.2.2" xref="S3.E1.m1.39.39.39.2.2.2.cmml">=</mo><mrow id="S3.E1.m1.82.82.4.80.33.33.33.1.1.1.3.3"><mo stretchy="false" id="S3.E1.m1.40.40.40.3.3.3" xref="S3.E1.m1.80.80.2.3.cmml">(</mo><msub id="S3.E1.m1.82.82.4.80.33.33.33.1.1.1.1.1.1"><mi id="S3.E1.m1.41.41.41.4.4.4" xref="S3.E1.m1.41.41.41.4.4.4.cmml">e</mi><mn id="S3.E1.m1.42.42.42.5.5.5.1" xref="S3.E1.m1.42.42.42.5.5.5.1.cmml">1</mn></msub><mo id="S3.E1.m1.43.43.43.6.6.6" xref="S3.E1.m1.80.80.2.3.cmml">,</mo><msub id="S3.E1.m1.82.82.4.80.33.33.33.1.1.1.2.2.2"><mi id="S3.E1.m1.44.44.44.7.7.7" xref="S3.E1.m1.44.44.44.7.7.7.cmml">e</mi><mn id="S3.E1.m1.45.45.45.8.8.8.1" xref="S3.E1.m1.45.45.45.8.8.8.1.cmml">2</mn></msub><mo id="S3.E1.m1.46.46.46.9.9.9" xref="S3.E1.m1.80.80.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.E1.m1.47.47.47.10.10.10" xref="S3.E1.m1.47.47.47.10.10.10.cmml">…</mi><mo id="S3.E1.m1.48.48.48.11.11.11" xref="S3.E1.m1.80.80.2.3.cmml">,</mo><msub id="S3.E1.m1.82.82.4.80.33.33.33.1.1.1.3.3.3"><mi id="S3.E1.m1.49.49.49.12.12.12" xref="S3.E1.m1.49.49.49.12.12.12.cmml">e</mi><msup id="S3.E1.m1.50.50.50.13.13.13.1" xref="S3.E1.m1.50.50.50.13.13.13.1.cmml"><mi id="S3.E1.m1.50.50.50.13.13.13.1.2" xref="S3.E1.m1.50.50.50.13.13.13.1.2.cmml">N</mi><mi id="S3.E1.m1.50.50.50.13.13.13.1.3" xref="S3.E1.m1.50.50.50.13.13.13.1.3.cmml">e</mi></msup></msub><mo stretchy="false" id="S3.E1.m1.51.51.51.14.14.14" xref="S3.E1.m1.80.80.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.52.52.52.15.15.15" xref="S3.E1.m1.80.80.2.3.cmml">;</mo><mrow id="S3.E1.m1.82.82.4.80.33.33.33.1.2.2"><msub id="S3.E1.m1.82.82.4.80.33.33.33.1.2.2.4"><mi id="S3.E1.m1.53.53.53.16.16.16" xref="S3.E1.m1.53.53.53.16.16.16.cmml">e</mi><mi id="S3.E1.m1.54.54.54.17.17.17.1" xref="S3.E1.m1.54.54.54.17.17.17.1.cmml">j</mi></msub><mo id="S3.E1.m1.55.55.55.18.18.18" xref="S3.E1.m1.55.55.55.18.18.18.cmml">=</mo><mrow id="S3.E1.m1.82.82.4.80.33.33.33.1.2.2.3.3"><mo stretchy="false" id="S3.E1.m1.56.56.56.19.19.19" xref="S3.E1.m1.80.80.2.3.cmml">(</mo><msubsup id="S3.E1.m1.82.82.4.80.33.33.33.1.2.2.1.1.1"><mi id="S3.E1.m1.57.57.57.20.20.20" xref="S3.E1.m1.57.57.57.20.20.20.cmml">e</mi><mi id="S3.E1.m1.58.58.58.21.21.21.1" xref="S3.E1.m1.58.58.58.21.21.21.1.cmml">j</mi><mrow id="S3.E1.m1.59.59.59.22.22.22.1" xref="S3.E1.m1.59.59.59.22.22.22.1.cmml"><mi id="S3.E1.m1.59.59.59.22.22.22.1.2" xref="S3.E1.m1.59.59.59.22.22.22.1.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.59.59.59.22.22.22.1.1" xref="S3.E1.m1.59.59.59.22.22.22.1.1.cmml">​</mo><mi id="S3.E1.m1.59.59.59.22.22.22.1.3" xref="S3.E1.m1.59.59.59.22.22.22.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.59.59.59.22.22.22.1.1a" xref="S3.E1.m1.59.59.59.22.22.22.1.1.cmml">​</mo><mi id="S3.E1.m1.59.59.59.22.22.22.1.4" xref="S3.E1.m1.59.59.59.22.22.22.1.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.59.59.59.22.22.22.1.1b" xref="S3.E1.m1.59.59.59.22.22.22.1.1.cmml">​</mo><mi id="S3.E1.m1.59.59.59.22.22.22.1.5" xref="S3.E1.m1.59.59.59.22.22.22.1.5.cmml">e</mi></mrow></msubsup><mo id="S3.E1.m1.60.60.60.23.23.23" xref="S3.E1.m1.80.80.2.3.cmml">,</mo><msubsup id="S3.E1.m1.82.82.4.80.33.33.33.1.2.2.2.2.2"><mi id="S3.E1.m1.61.61.61.24.24.24" xref="S3.E1.m1.61.61.61.24.24.24.cmml">e</mi><mi id="S3.E1.m1.62.62.62.25.25.25.1" xref="S3.E1.m1.62.62.62.25.25.25.1.cmml">j</mi><mi id="S3.E1.m1.63.63.63.26.26.26.1" xref="S3.E1.m1.63.63.63.26.26.26.1.cmml">s</mi></msubsup><mo id="S3.E1.m1.64.64.64.27.27.27" xref="S3.E1.m1.80.80.2.3.cmml">,</mo><msubsup id="S3.E1.m1.82.82.4.80.33.33.33.1.2.2.3.3.3"><mi id="S3.E1.m1.65.65.65.28.28.28" xref="S3.E1.m1.65.65.65.28.28.28.cmml">e</mi><mi id="S3.E1.m1.66.66.66.29.29.29.1" xref="S3.E1.m1.66.66.66.29.29.29.1.cmml">j</mi><mi id="S3.E1.m1.67.67.67.30.30.30.1" xref="S3.E1.m1.67.67.67.30.30.30.1.cmml">r</mi></msubsup><mo stretchy="false" id="S3.E1.m1.68.68.68.31.31.31" xref="S3.E1.m1.80.80.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.69.69.69.32.32.32" xref="S3.E1.m1.80.80.2.3.cmml">;</mo></mrow></mtd></mtr><mtr id="S3.E1.m1.84.84.6g"><mtd id="S3.E1.m1.84.84.6h"><mrow id="S3.E1.m1.84.84.6.82.11.11"><mrow id="S3.E1.m1.84.84.6.82.11.11.11.2"><msubsup id="S3.E1.m1.83.83.5.81.10.10.10.1.1"><mi id="S3.E1.m1.70.70.70.1.1.1" xref="S3.E1.m1.70.70.70.1.1.1.cmml">e</mi><mi id="S3.E1.m1.71.71.71.2.2.2.1" xref="S3.E1.m1.71.71.71.2.2.2.1.cmml">j</mi><mi id="S3.E1.m1.72.72.72.3.3.3.1" xref="S3.E1.m1.72.72.72.3.3.3.1.cmml">s</mi></msubsup><mo id="S3.E1.m1.73.73.73.4.4.4" xref="S3.E1.m1.80.80.2.3.cmml">,</mo><msubsup id="S3.E1.m1.84.84.6.82.11.11.11.2.2"><mi id="S3.E1.m1.74.74.74.5.5.5" xref="S3.E1.m1.74.74.74.5.5.5.cmml">e</mi><mi id="S3.E1.m1.75.75.75.6.6.6.1" xref="S3.E1.m1.75.75.75.6.6.6.1.cmml">j</mi><mi id="S3.E1.m1.76.76.76.7.7.7.1" xref="S3.E1.m1.76.76.76.7.7.7.1.cmml">r</mi></msubsup></mrow><mo id="S3.E1.m1.77.77.77.8.8.8" xref="S3.E1.m1.77.77.77.8.8.8.cmml">∈</mo><mi id="S3.E1.m1.78.78.78.9.9.9" xref="S3.E1.m1.78.78.78.9.9.9.cmml">V</mi></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E1.m1.84b"><apply id="S3.E1.m1.80.80.2.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.3a.cmml" xref="S3.E1.m1.3.3.3.3.3.3">formulae-sequence</csymbol><apply id="S3.E1.m1.79.79.1.1.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><and id="S3.E1.m1.79.79.1.1.1a.cmml" xref="S3.E1.m1.3.3.3.3.3.3"></and><apply id="S3.E1.m1.79.79.1.1.1b.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><eq id="S3.E1.m1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2"></eq><ci id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">𝑔</ci><apply id="S3.E1.m1.79.79.1.1.1.7.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><times id="S3.E1.m1.79.79.1.1.1.7.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3"></times><vector id="S3.E1.m1.79.79.1.1.1.7.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><ci id="S3.E1.m1.4.4.4.4.4.4.cmml" xref="S3.E1.m1.4.4.4.4.4.4">𝐸</ci><ci id="S3.E1.m1.6.6.6.6.6.6.cmml" xref="S3.E1.m1.6.6.6.6.6.6">𝑉</ci><ci id="S3.E1.m1.8.8.8.8.8.8.cmml" xref="S3.E1.m1.8.8.8.8.8.8">𝑢</ci></vector><ci id="S3.E1.m1.10.10.10.1.1.1.cmml" xref="S3.E1.m1.10.10.10.1.1.1">𝑉</ci></apply></apply><apply id="S3.E1.m1.79.79.1.1.1c.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><eq id="S3.E1.m1.11.11.11.2.2.2.cmml" xref="S3.E1.m1.11.11.11.2.2.2"></eq><share href="#S3.E1.m1.79.79.1.1.1.7.cmml" id="S3.E1.m1.79.79.1.1.1d.cmml" xref="S3.E1.m1.3.3.3.3.3.3"></share><vector id="S3.E1.m1.79.79.1.1.1.3.4.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><apply id="S3.E1.m1.79.79.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.79.79.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.13.13.13.4.4.4.cmml" xref="S3.E1.m1.13.13.13.4.4.4">𝑣</ci><cn type="integer" id="S3.E1.m1.14.14.14.5.5.5.1.cmml" xref="S3.E1.m1.14.14.14.5.5.5.1">1</cn></apply><apply id="S3.E1.m1.79.79.1.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.79.79.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.16.16.16.7.7.7.cmml" xref="S3.E1.m1.16.16.16.7.7.7">𝑣</ci><cn type="integer" id="S3.E1.m1.17.17.17.8.8.8.1.cmml" xref="S3.E1.m1.17.17.17.8.8.8.1">2</cn></apply><ci id="S3.E1.m1.19.19.19.10.10.10.cmml" xref="S3.E1.m1.19.19.19.10.10.10">…</ci><apply id="S3.E1.m1.79.79.1.1.1.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.79.79.1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.21.21.21.12.12.12.cmml" xref="S3.E1.m1.21.21.21.12.12.12">𝑣</ci><apply id="S3.E1.m1.22.22.22.13.13.13.1.cmml" xref="S3.E1.m1.22.22.22.13.13.13.1"><csymbol cd="ambiguous" id="S3.E1.m1.22.22.22.13.13.13.1.1.cmml" xref="S3.E1.m1.22.22.22.13.13.13.1">superscript</csymbol><ci id="S3.E1.m1.22.22.22.13.13.13.1.2.cmml" xref="S3.E1.m1.22.22.22.13.13.13.1.2">𝑁</ci><ci id="S3.E1.m1.22.22.22.13.13.13.1.3.cmml" xref="S3.E1.m1.22.22.22.13.13.13.1.3">𝑣</ci></apply></apply></vector></apply></apply><apply id="S3.E1.m1.80.80.2.2.2.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.3a.cmml" xref="S3.E1.m1.3.3.3.3.3.3">formulae-sequence</csymbol><apply id="S3.E1.m1.80.80.2.2.2.1.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><eq id="S3.E1.m1.27.27.27.18.18.18.cmml" xref="S3.E1.m1.27.27.27.18.18.18"></eq><apply id="S3.E1.m1.80.80.2.2.2.1.1.4.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.1.1.4.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.25.25.25.16.16.16.cmml" xref="S3.E1.m1.25.25.25.16.16.16">𝑣</ci><ci id="S3.E1.m1.26.26.26.17.17.17.1.cmml" xref="S3.E1.m1.26.26.26.17.17.17.1">𝑖</ci></apply><interval closure="open" id="S3.E1.m1.80.80.2.2.2.1.1.2.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><apply id="S3.E1.m1.80.80.2.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">superscript</csymbol><apply id="S3.E1.m1.80.80.2.2.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.29.29.29.20.20.20.cmml" xref="S3.E1.m1.29.29.29.20.20.20">𝑣</ci><ci id="S3.E1.m1.30.30.30.21.21.21.1.cmml" xref="S3.E1.m1.30.30.30.21.21.21.1">𝑖</ci></apply><apply id="S3.E1.m1.31.31.31.22.22.22.1.cmml" xref="S3.E1.m1.31.31.31.22.22.22.1"><times id="S3.E1.m1.31.31.31.22.22.22.1.1.cmml" xref="S3.E1.m1.31.31.31.22.22.22.1.1"></times><ci id="S3.E1.m1.31.31.31.22.22.22.1.2.cmml" xref="S3.E1.m1.31.31.31.22.22.22.1.2">𝑛</ci><ci id="S3.E1.m1.31.31.31.22.22.22.1.3.cmml" xref="S3.E1.m1.31.31.31.22.22.22.1.3">𝑎</ci><ci id="S3.E1.m1.31.31.31.22.22.22.1.4.cmml" xref="S3.E1.m1.31.31.31.22.22.22.1.4">𝑚</ci><ci id="S3.E1.m1.31.31.31.22.22.22.1.5.cmml" xref="S3.E1.m1.31.31.31.22.22.22.1.5">𝑒</ci></apply></apply><apply id="S3.E1.m1.80.80.2.2.2.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.1.1.2.2.2.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">superscript</csymbol><apply id="S3.E1.m1.80.80.2.2.2.1.1.2.2.2.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.1.1.2.2.2.2.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.33.33.33.24.24.24.cmml" xref="S3.E1.m1.33.33.33.24.24.24">𝑣</ci><ci id="S3.E1.m1.34.34.34.25.25.25.1.cmml" xref="S3.E1.m1.34.34.34.25.25.25.1">𝑖</ci></apply><apply id="S3.E1.m1.35.35.35.26.26.26.1.cmml" xref="S3.E1.m1.35.35.35.26.26.26.1"><times id="S3.E1.m1.35.35.35.26.26.26.1.1.cmml" xref="S3.E1.m1.35.35.35.26.26.26.1.1"></times><ci id="S3.E1.m1.35.35.35.26.26.26.1.2.cmml" xref="S3.E1.m1.35.35.35.26.26.26.1.2">𝑎</ci><ci id="S3.E1.m1.35.35.35.26.26.26.1.3.cmml" xref="S3.E1.m1.35.35.35.26.26.26.1.3">𝑡</ci><ci id="S3.E1.m1.35.35.35.26.26.26.1.4.cmml" xref="S3.E1.m1.35.35.35.26.26.26.1.4">𝑡</ci><ci id="S3.E1.m1.35.35.35.26.26.26.1.5.cmml" xref="S3.E1.m1.35.35.35.26.26.26.1.5">𝑟</ci></apply></apply></interval></apply><apply id="S3.E1.m1.80.80.2.2.2.2.2.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.2.2.3a.cmml" xref="S3.E1.m1.3.3.3.3.3.3">formulae-sequence</csymbol><apply id="S3.E1.m1.80.80.2.2.2.2.2.1.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><eq id="S3.E1.m1.39.39.39.2.2.2.cmml" xref="S3.E1.m1.39.39.39.2.2.2"></eq><ci id="S3.E1.m1.38.38.38.1.1.1.cmml" xref="S3.E1.m1.38.38.38.1.1.1">𝐸</ci><vector id="S3.E1.m1.80.80.2.2.2.2.2.1.1.3.4.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><apply id="S3.E1.m1.80.80.2.2.2.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.41.41.41.4.4.4.cmml" xref="S3.E1.m1.41.41.41.4.4.4">𝑒</ci><cn type="integer" id="S3.E1.m1.42.42.42.5.5.5.1.cmml" xref="S3.E1.m1.42.42.42.5.5.5.1">1</cn></apply><apply id="S3.E1.m1.80.80.2.2.2.2.2.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.2.2.1.1.2.2.2.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.44.44.44.7.7.7.cmml" xref="S3.E1.m1.44.44.44.7.7.7">𝑒</ci><cn type="integer" id="S3.E1.m1.45.45.45.8.8.8.1.cmml" xref="S3.E1.m1.45.45.45.8.8.8.1">2</cn></apply><ci id="S3.E1.m1.47.47.47.10.10.10.cmml" xref="S3.E1.m1.47.47.47.10.10.10">…</ci><apply id="S3.E1.m1.80.80.2.2.2.2.2.1.1.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.2.2.1.1.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.49.49.49.12.12.12.cmml" xref="S3.E1.m1.49.49.49.12.12.12">𝑒</ci><apply id="S3.E1.m1.50.50.50.13.13.13.1.cmml" xref="S3.E1.m1.50.50.50.13.13.13.1"><csymbol cd="ambiguous" id="S3.E1.m1.50.50.50.13.13.13.1.1.cmml" xref="S3.E1.m1.50.50.50.13.13.13.1">superscript</csymbol><ci id="S3.E1.m1.50.50.50.13.13.13.1.2.cmml" xref="S3.E1.m1.50.50.50.13.13.13.1.2">𝑁</ci><ci id="S3.E1.m1.50.50.50.13.13.13.1.3.cmml" xref="S3.E1.m1.50.50.50.13.13.13.1.3">𝑒</ci></apply></apply></vector></apply><apply id="S3.E1.m1.80.80.2.2.2.2.2.2.2.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.2.2.2.2.3a.cmml" xref="S3.E1.m1.3.3.3.3.3.3">formulae-sequence</csymbol><apply id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><eq id="S3.E1.m1.55.55.55.18.18.18.cmml" xref="S3.E1.m1.55.55.55.18.18.18"></eq><apply id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.4.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.4.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.53.53.53.16.16.16.cmml" xref="S3.E1.m1.53.53.53.16.16.16">𝑒</ci><ci id="S3.E1.m1.54.54.54.17.17.17.1.cmml" xref="S3.E1.m1.54.54.54.17.17.17.1">𝑗</ci></apply><list id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.2.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><vector id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.1.1.1.4.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><apply id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">superscript</csymbol><apply id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.57.57.57.20.20.20.cmml" xref="S3.E1.m1.57.57.57.20.20.20">𝑒</ci><ci id="S3.E1.m1.58.58.58.21.21.21.1.cmml" xref="S3.E1.m1.58.58.58.21.21.21.1">𝑗</ci></apply><apply id="S3.E1.m1.59.59.59.22.22.22.1.cmml" xref="S3.E1.m1.59.59.59.22.22.22.1"><times id="S3.E1.m1.59.59.59.22.22.22.1.1.cmml" xref="S3.E1.m1.59.59.59.22.22.22.1.1"></times><ci id="S3.E1.m1.59.59.59.22.22.22.1.2.cmml" xref="S3.E1.m1.59.59.59.22.22.22.1.2">𝑛</ci><ci id="S3.E1.m1.59.59.59.22.22.22.1.3.cmml" xref="S3.E1.m1.59.59.59.22.22.22.1.3">𝑎</ci><ci id="S3.E1.m1.59.59.59.22.22.22.1.4.cmml" xref="S3.E1.m1.59.59.59.22.22.22.1.4">𝑚</ci><ci id="S3.E1.m1.59.59.59.22.22.22.1.5.cmml" xref="S3.E1.m1.59.59.59.22.22.22.1.5">𝑒</ci></apply></apply><apply id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">superscript</csymbol><apply id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.61.61.61.24.24.24.cmml" xref="S3.E1.m1.61.61.61.24.24.24">𝑒</ci><ci id="S3.E1.m1.62.62.62.25.25.25.1.cmml" xref="S3.E1.m1.62.62.62.25.25.25.1">𝑗</ci></apply><ci id="S3.E1.m1.63.63.63.26.26.26.1.cmml" xref="S3.E1.m1.63.63.63.26.26.26.1">𝑠</ci></apply><apply id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">superscript</csymbol><apply id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.65.65.65.28.28.28.cmml" xref="S3.E1.m1.65.65.65.28.28.28">𝑒</ci><ci id="S3.E1.m1.66.66.66.29.29.29.1.cmml" xref="S3.E1.m1.66.66.66.29.29.29.1">𝑗</ci></apply><ci id="S3.E1.m1.67.67.67.30.30.30.1.cmml" xref="S3.E1.m1.67.67.67.30.30.30.1">𝑟</ci></apply></vector><apply id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.2.2.2.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">superscript</csymbol><apply id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.2.2.2.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.2.2.2.2.1.1.2.2.2.2.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.70.70.70.1.1.1.cmml" xref="S3.E1.m1.70.70.70.1.1.1">𝑒</ci><ci id="S3.E1.m1.71.71.71.2.2.2.1.cmml" xref="S3.E1.m1.71.71.71.2.2.2.1">𝑗</ci></apply><ci id="S3.E1.m1.72.72.72.3.3.3.1.cmml" xref="S3.E1.m1.72.72.72.3.3.3.1">𝑠</ci></apply></list></apply><apply id="S3.E1.m1.80.80.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><in id="S3.E1.m1.77.77.77.8.8.8.cmml" xref="S3.E1.m1.77.77.77.8.8.8"></in><apply id="S3.E1.m1.80.80.2.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.2.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">superscript</csymbol><apply id="S3.E1.m1.80.80.2.2.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.80.80.2.2.2.2.2.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.74.74.74.5.5.5.cmml" xref="S3.E1.m1.74.74.74.5.5.5">𝑒</ci><ci id="S3.E1.m1.75.75.75.6.6.6.1.cmml" xref="S3.E1.m1.75.75.75.6.6.6.1">𝑗</ci></apply><ci id="S3.E1.m1.76.76.76.7.7.7.1.cmml" xref="S3.E1.m1.76.76.76.7.7.7.1">𝑟</ci></apply><ci id="S3.E1.m1.78.78.78.9.9.9.cmml" xref="S3.E1.m1.78.78.78.9.9.9">𝑉</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.84c">\begin{gathered}g=(E,V,u)\\
V=(v_{1},v_{2},...,v_{N^{v}});v_{i}=(v_{i}^{name},v_{i}^{attr});\\
E=(e_{1},e_{2},...,e_{N^{e}});e_{j}=(e_{j}^{name},e_{j}^{s},e_{j}^{r});\\
e_{j}^{s},e_{j}^{r}\in V\end{gathered}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.7" class="ltx_p">We work in a discriminative setting wherein, given <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">Q</annotation></semantics></math> and one or more representations of <math id="S3.p3.2.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.p3.2.m2.1a"><mi id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">I</annotation></semantics></math> as input, the model must select an answer for <math id="S3.p3.3.m3.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.p3.3.m3.1a"><mi id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><ci id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">Q</annotation></semantics></math> from a fixed set of <math id="S3.p3.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.p3.4.m4.1a"><mi id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><ci id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">n</annotation></semantics></math> answers, A = (<math id="S3.p3.5.m5.1" class="ltx_Math" alttext="A_{1}" display="inline"><semantics id="S3.p3.5.m5.1a"><msub id="S3.p3.5.m5.1.1" xref="S3.p3.5.m5.1.1.cmml"><mi id="S3.p3.5.m5.1.1.2" xref="S3.p3.5.m5.1.1.2.cmml">A</mi><mn id="S3.p3.5.m5.1.1.3" xref="S3.p3.5.m5.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p3.5.m5.1b"><apply id="S3.p3.5.m5.1.1.cmml" xref="S3.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p3.5.m5.1.1.1.cmml" xref="S3.p3.5.m5.1.1">subscript</csymbol><ci id="S3.p3.5.m5.1.1.2.cmml" xref="S3.p3.5.m5.1.1.2">𝐴</ci><cn type="integer" id="S3.p3.5.m5.1.1.3.cmml" xref="S3.p3.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.5.m5.1c">A_{1}</annotation></semantics></math>, <math id="S3.p3.6.m6.1" class="ltx_Math" alttext="A_{2}" display="inline"><semantics id="S3.p3.6.m6.1a"><msub id="S3.p3.6.m6.1.1" xref="S3.p3.6.m6.1.1.cmml"><mi id="S3.p3.6.m6.1.1.2" xref="S3.p3.6.m6.1.1.2.cmml">A</mi><mn id="S3.p3.6.m6.1.1.3" xref="S3.p3.6.m6.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p3.6.m6.1b"><apply id="S3.p3.6.m6.1.1.cmml" xref="S3.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p3.6.m6.1.1.1.cmml" xref="S3.p3.6.m6.1.1">subscript</csymbol><ci id="S3.p3.6.m6.1.1.2.cmml" xref="S3.p3.6.m6.1.1.2">𝐴</ci><cn type="integer" id="S3.p3.6.m6.1.1.3.cmml" xref="S3.p3.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.6.m6.1c">A_{2}</annotation></semantics></math>, …, <math id="S3.p3.7.m7.1" class="ltx_Math" alttext="A_{n}" display="inline"><semantics id="S3.p3.7.m7.1a"><msub id="S3.p3.7.m7.1.1" xref="S3.p3.7.m7.1.1.cmml"><mi id="S3.p3.7.m7.1.1.2" xref="S3.p3.7.m7.1.1.2.cmml">A</mi><mi id="S3.p3.7.m7.1.1.3" xref="S3.p3.7.m7.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.7.m7.1b"><apply id="S3.p3.7.m7.1.1.cmml" xref="S3.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.p3.7.m7.1.1.1.cmml" xref="S3.p3.7.m7.1.1">subscript</csymbol><ci id="S3.p3.7.m7.1.1.2.cmml" xref="S3.p3.7.m7.1.1.2">𝐴</ci><ci id="S3.p3.7.m7.1.1.3.cmml" xref="S3.p3.7.m7.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.7.m7.1c">A_{n}</annotation></semantics></math>).</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Proposed Approach</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section we present the details of our methodology.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Baselines</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.4" class="ltx_p">We define two multimodal baselines for VQA, one using the image spatial features <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">S</annotation></semantics></math>, namely the Concatenation Baseline <span id="S4.SS1.p1.4.1" class="ltx_text ltx_font_smallcaps">(concat)</span>, and another using the image object features <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="O" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">O</annotation></semantics></math>, namely the Attention Baseline <span id="S4.SS1.p1.4.2" class="ltx_text ltx_font_smallcaps">(attn)</span>. For all our baselines, we use mean pooled BERT embeddings <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mi id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">B</annotation></semantics></math> of the question <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mi id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><ci id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">q</annotation></semantics></math>. We also define unimodal baselines.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Multimodal Baselines</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.4" class="ltx_p">In the <span id="S4.SS1.SSS1.p1.4.1" class="ltx_text ltx_font_smallcaps">concat</span> model, mean pooled representations of <math id="S4.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="O" display="inline"><semantics id="S4.SS1.SSS1.p1.1.m1.1a"><mi id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.1b"><ci id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.1c">O</annotation></semantics></math> are passed in parallel with <math id="S4.SS1.SSS1.p1.2.m2.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S4.SS1.SSS1.p1.2.m2.1a"><mi id="S4.SS1.SSS1.p1.2.m2.1.1" xref="S4.SS1.SSS1.p1.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.2.m2.1b"><ci id="S4.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.2.m2.1c">B</annotation></semantics></math> through two fully-connected layers with ReLU activation to get <math id="S4.SS1.SSS1.p1.3.m3.1" class="ltx_Math" alttext="b*1024" display="inline"><semantics id="S4.SS1.SSS1.p1.3.m3.1a"><mrow id="S4.SS1.SSS1.p1.3.m3.1.1" xref="S4.SS1.SSS1.p1.3.m3.1.1.cmml"><mi id="S4.SS1.SSS1.p1.3.m3.1.1.2" xref="S4.SS1.SSS1.p1.3.m3.1.1.2.cmml">b</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS1.p1.3.m3.1.1.1" xref="S4.SS1.SSS1.p1.3.m3.1.1.1.cmml">∗</mo><mn id="S4.SS1.SSS1.p1.3.m3.1.1.3" xref="S4.SS1.SSS1.p1.3.m3.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.3.m3.1b"><apply id="S4.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS1.p1.3.m3.1.1"><times id="S4.SS1.SSS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS1.p1.3.m3.1.1.1"></times><ci id="S4.SS1.SSS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS1.p1.3.m3.1.1.2">𝑏</ci><cn type="integer" id="S4.SS1.SSS1.p1.3.m3.1.1.3.cmml" xref="S4.SS1.SSS1.p1.3.m3.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.3.m3.1c">b*1024</annotation></semantics></math> vectors for both modalities, where <math id="S4.SS1.SSS1.p1.4.m4.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S4.SS1.SSS1.p1.4.m4.1a"><mi id="S4.SS1.SSS1.p1.4.m4.1.1" xref="S4.SS1.SSS1.p1.4.m4.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.4.m4.1b"><ci id="S4.SS1.SSS1.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS1.p1.4.m4.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.4.m4.1c">b</annotation></semantics></math> is the batch size. These vectors are concatenated and passed through two fully-connected layers with ReLU activation in between.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.5" class="ltx_p">In <span id="S4.SS1.SSS1.p2.5.1" class="ltx_text ltx_font_smallcaps">attn</span>, we pass <math id="S4.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S4.SS1.SSS1.p2.1.m1.1a"><mi id="S4.SS1.SSS1.p2.1.m1.1.1" xref="S4.SS1.SSS1.p2.1.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.1.m1.1b"><ci id="S4.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.1.m1.1c">B</annotation></semantics></math> and <math id="S4.SS1.SSS1.p2.2.m2.1" class="ltx_Math" alttext="O" display="inline"><semantics id="S4.SS1.SSS1.p2.2.m2.1a"><mi id="S4.SS1.SSS1.p2.2.m2.1.1" xref="S4.SS1.SSS1.p2.2.m2.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.2.m2.1b"><ci id="S4.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p2.2.m2.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.2.m2.1c">O</annotation></semantics></math> through parallel fully-connected layers. We use the obtained question projection as the <math id="S4.SS1.SSS1.p2.3.m3.1" class="ltx_Math" alttext="query" display="inline"><semantics id="S4.SS1.SSS1.p2.3.m3.1a"><mrow id="S4.SS1.SSS1.p2.3.m3.1.1" xref="S4.SS1.SSS1.p2.3.m3.1.1.cmml"><mi id="S4.SS1.SSS1.p2.3.m3.1.1.2" xref="S4.SS1.SSS1.p2.3.m3.1.1.2.cmml">q</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.3.m3.1.1.1" xref="S4.SS1.SSS1.p2.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.3.m3.1.1.3" xref="S4.SS1.SSS1.p2.3.m3.1.1.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.3.m3.1.1.1a" xref="S4.SS1.SSS1.p2.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.3.m3.1.1.4" xref="S4.SS1.SSS1.p2.3.m3.1.1.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.3.m3.1.1.1b" xref="S4.SS1.SSS1.p2.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.3.m3.1.1.5" xref="S4.SS1.SSS1.p2.3.m3.1.1.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.3.m3.1.1.1c" xref="S4.SS1.SSS1.p2.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.3.m3.1.1.6" xref="S4.SS1.SSS1.p2.3.m3.1.1.6.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.3.m3.1b"><apply id="S4.SS1.SSS1.p2.3.m3.1.1.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1"><times id="S4.SS1.SSS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.1"></times><ci id="S4.SS1.SSS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.2">𝑞</ci><ci id="S4.SS1.SSS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.3">𝑢</ci><ci id="S4.SS1.SSS1.p2.3.m3.1.1.4.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.4">𝑒</ci><ci id="S4.SS1.SSS1.p2.3.m3.1.1.5.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.5">𝑟</ci><ci id="S4.SS1.SSS1.p2.3.m3.1.1.6.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.6">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.3.m3.1c">query</annotation></semantics></math> and the image projection as the <math id="S4.SS1.SSS1.p2.4.m4.1" class="ltx_Math" alttext="key" display="inline"><semantics id="S4.SS1.SSS1.p2.4.m4.1a"><mrow id="S4.SS1.SSS1.p2.4.m4.1.1" xref="S4.SS1.SSS1.p2.4.m4.1.1.cmml"><mi id="S4.SS1.SSS1.p2.4.m4.1.1.2" xref="S4.SS1.SSS1.p2.4.m4.1.1.2.cmml">k</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.4.m4.1.1.1" xref="S4.SS1.SSS1.p2.4.m4.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.4.m4.1.1.3" xref="S4.SS1.SSS1.p2.4.m4.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.4.m4.1.1.1a" xref="S4.SS1.SSS1.p2.4.m4.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.4.m4.1.1.4" xref="S4.SS1.SSS1.p2.4.m4.1.1.4.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.4.m4.1b"><apply id="S4.SS1.SSS1.p2.4.m4.1.1.cmml" xref="S4.SS1.SSS1.p2.4.m4.1.1"><times id="S4.SS1.SSS1.p2.4.m4.1.1.1.cmml" xref="S4.SS1.SSS1.p2.4.m4.1.1.1"></times><ci id="S4.SS1.SSS1.p2.4.m4.1.1.2.cmml" xref="S4.SS1.SSS1.p2.4.m4.1.1.2">𝑘</ci><ci id="S4.SS1.SSS1.p2.4.m4.1.1.3.cmml" xref="S4.SS1.SSS1.p2.4.m4.1.1.3">𝑒</ci><ci id="S4.SS1.SSS1.p2.4.m4.1.1.4.cmml" xref="S4.SS1.SSS1.p2.4.m4.1.1.4">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.4.m4.1c">key</annotation></semantics></math> &amp; <math id="S4.SS1.SSS1.p2.5.m5.1" class="ltx_Math" alttext="value" display="inline"><semantics id="S4.SS1.SSS1.p2.5.m5.1a"><mrow id="S4.SS1.SSS1.p2.5.m5.1.1" xref="S4.SS1.SSS1.p2.5.m5.1.1.cmml"><mi id="S4.SS1.SSS1.p2.5.m5.1.1.2" xref="S4.SS1.SSS1.p2.5.m5.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.5.m5.1.1.1" xref="S4.SS1.SSS1.p2.5.m5.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.5.m5.1.1.3" xref="S4.SS1.SSS1.p2.5.m5.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.5.m5.1.1.1a" xref="S4.SS1.SSS1.p2.5.m5.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.5.m5.1.1.4" xref="S4.SS1.SSS1.p2.5.m5.1.1.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.5.m5.1.1.1b" xref="S4.SS1.SSS1.p2.5.m5.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.5.m5.1.1.5" xref="S4.SS1.SSS1.p2.5.m5.1.1.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.5.m5.1.1.1c" xref="S4.SS1.SSS1.p2.5.m5.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.5.m5.1.1.6" xref="S4.SS1.SSS1.p2.5.m5.1.1.6.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.5.m5.1b"><apply id="S4.SS1.SSS1.p2.5.m5.1.1.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1"><times id="S4.SS1.SSS1.p2.5.m5.1.1.1.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.1"></times><ci id="S4.SS1.SSS1.p2.5.m5.1.1.2.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.2">𝑣</ci><ci id="S4.SS1.SSS1.p2.5.m5.1.1.3.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.3">𝑎</ci><ci id="S4.SS1.SSS1.p2.5.m5.1.1.4.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.4">𝑙</ci><ci id="S4.SS1.SSS1.p2.5.m5.1.1.5.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.5">𝑢</ci><ci id="S4.SS1.SSS1.p2.5.m5.1.1.6.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.6">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.5.m5.1c">value</annotation></semantics></math> in an attention mechanism to attend over the objects. Here, we use an attention mask to deal with the variable number of objects in each image. We concatenate the output of the attention mechanism to the question representation. This is followed by a fully-connected layer, ReLU and a classifying layer to get a distribution over the answers. The model architecture is illustrated in Figure <a href="#S4.F1" title="Figure 1 ‣ 4.1.1 Multimodal Baselines ‣ 4.1 Baselines ‣ 4 Proposed Approach ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Such attention-based techniques for multimodal-fusion have been used successfully in the past <cite class="ltx_cite ltx_citemacro_citep">(Liang et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<figure id="S4.F1" class="ltx_figure">
<p id="S4.F1.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S4.F1.1.1.1" class="ltx_text"><img src="/html/2101.05479/assets/figs/slide_8_bert_attn_baseline_diag.jpg" id="S4.F1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="447" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S4.F1.3.1" class="ltx_text ltx_font_smallcaps">attn</span> model architecture</figcaption>
</figure>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Unimodal Baselines</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">For our unimodal baselines we remove the text component from the Concatenation Baseline to observe the impact of the image modality alone. Similarly, we remove the image component to observe the impact of the text modality alone.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>UNITER</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Given the state of the art performance achieved by the pre-trained <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">uniter</span> model on six different Vision + Language tasks, we use it to extract a cross-modal contextualized embedding <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">E</annotation></semantics></math> for a given image and question. The embedding is passed through a multi-layer perceptron <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_smallcaps">(mlp)</span> which is trained during the process of fine-tuning <span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_smallcaps">uniter</span>. The final layer of the <span id="S4.SS2.p1.1.4" class="ltx_text ltx_font_smallcaps">mlp</span> produces a softmax distribution over the answer set A, which is then used to predict an answer.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Scene Graph Encoding</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Graph Neural Networks are a class of interpretable models successfully used for a range of tasks from relational reasoning to knowledge base completion <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a href="#bib.bib26" title="" class="ltx_ref">2018</a>)</cite>. Following the work of <cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite> we encode scene graphs using the class of graph neural networks introduced by <cite class="ltx_cite ltx_citemacro_citet">Battaglia et al. (<a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite> - Graph Networks <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_smallcaps">(GN)</span>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">As described in Equation <a href="#S3.E1" title="Equation 1 ‣ 3 Task Definition ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, each node in the scene graph consists of a name and zero or more attributes. During pre-processing, we split the name and attributes into their constituent words, and represent a node with the vocabulary indices of these words. Similarly, an edge consists of the vocabulary indices of the relation name while the global vector is made up of the vocabulary indices of the question. These pre-processed representations are passed through a GloVe <cite class="ltx_cite ltx_citemacro_citep">(Pennington et al., <a href="#bib.bib19" title="" class="ltx_ref">2014</a>)</cite> embedding layer followed by a single-layer bi-directional LSTM network to obtain the input graph representation for the Graph Network, <math id="S4.SS3.p2.1.m1.3" class="ltx_Math" alttext="(E_{0},V_{0},u_{0})" display="inline"><semantics id="S4.SS3.p2.1.m1.3a"><mrow id="S4.SS3.p2.1.m1.3.3.3" xref="S4.SS3.p2.1.m1.3.3.4.cmml"><mo stretchy="false" id="S4.SS3.p2.1.m1.3.3.3.4" xref="S4.SS3.p2.1.m1.3.3.4.cmml">(</mo><msub id="S4.SS3.p2.1.m1.1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.1.1.2.cmml">E</mi><mn id="S4.SS3.p2.1.m1.1.1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.1.1.3.cmml">0</mn></msub><mo id="S4.SS3.p2.1.m1.3.3.3.5" xref="S4.SS3.p2.1.m1.3.3.4.cmml">,</mo><msub id="S4.SS3.p2.1.m1.2.2.2.2" xref="S4.SS3.p2.1.m1.2.2.2.2.cmml"><mi id="S4.SS3.p2.1.m1.2.2.2.2.2" xref="S4.SS3.p2.1.m1.2.2.2.2.2.cmml">V</mi><mn id="S4.SS3.p2.1.m1.2.2.2.2.3" xref="S4.SS3.p2.1.m1.2.2.2.2.3.cmml">0</mn></msub><mo id="S4.SS3.p2.1.m1.3.3.3.6" xref="S4.SS3.p2.1.m1.3.3.4.cmml">,</mo><msub id="S4.SS3.p2.1.m1.3.3.3.3" xref="S4.SS3.p2.1.m1.3.3.3.3.cmml"><mi id="S4.SS3.p2.1.m1.3.3.3.3.2" xref="S4.SS3.p2.1.m1.3.3.3.3.2.cmml">u</mi><mn id="S4.SS3.p2.1.m1.3.3.3.3.3" xref="S4.SS3.p2.1.m1.3.3.3.3.3.cmml">0</mn></msub><mo stretchy="false" id="S4.SS3.p2.1.m1.3.3.3.7" xref="S4.SS3.p2.1.m1.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.3b"><vector id="S4.SS3.p2.1.m1.3.3.4.cmml" xref="S4.SS3.p2.1.m1.3.3.3"><apply id="S4.SS3.p2.1.m1.1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.SS3.p2.1.m1.1.1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.1.1.2">𝐸</ci><cn type="integer" id="S4.SS3.p2.1.m1.1.1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.1.1.3">0</cn></apply><apply id="S4.SS3.p2.1.m1.2.2.2.2.cmml" xref="S4.SS3.p2.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.2.2.2.2.1.cmml" xref="S4.SS3.p2.1.m1.2.2.2.2">subscript</csymbol><ci id="S4.SS3.p2.1.m1.2.2.2.2.2.cmml" xref="S4.SS3.p2.1.m1.2.2.2.2.2">𝑉</ci><cn type="integer" id="S4.SS3.p2.1.m1.2.2.2.2.3.cmml" xref="S4.SS3.p2.1.m1.2.2.2.2.3">0</cn></apply><apply id="S4.SS3.p2.1.m1.3.3.3.3.cmml" xref="S4.SS3.p2.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.3.3.3.3.1.cmml" xref="S4.SS3.p2.1.m1.3.3.3.3">subscript</csymbol><ci id="S4.SS3.p2.1.m1.3.3.3.3.2.cmml" xref="S4.SS3.p2.1.m1.3.3.3.3.2">𝑢</ci><cn type="integer" id="S4.SS3.p2.1.m1.3.3.3.3.3.cmml" xref="S4.SS3.p2.1.m1.3.3.3.3.3">0</cn></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.3c">(E_{0},V_{0},u_{0})</annotation></semantics></math>. The Graph Network produces updated vectors following the algorithm described by <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a href="#bib.bib26" title="" class="ltx_ref">2018</a>)</cite> which we show in Algorithm  <a href="#algorithm1" title="Algorithm 1 ‣ 4.3 Scene Graph Encoding ‣ 4 Proposed Approach ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Here, <span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_smallcaps">mlp</span> refers to a multi-layer perceptron with two fully-connected layers of hidden size 512.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.2" class="ltx_p">During iteration <math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mi id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><ci id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">t</annotation></semantics></math>, lines 2-4 update each edge using the value of the edge in iteration <math id="S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="t-1" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><mrow id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml"><mi id="S4.SS3.p3.2.m2.1.1.2" xref="S4.SS3.p3.2.m2.1.1.2.cmml">t</mi><mo id="S4.SS3.p3.2.m2.1.1.1" xref="S4.SS3.p3.2.m2.1.1.1.cmml">−</mo><mn id="S4.SS3.p3.2.m2.1.1.3" xref="S4.SS3.p3.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><apply id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1"><minus id="S4.SS3.p3.2.m2.1.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1.1"></minus><ci id="S4.SS3.p3.2.m2.1.1.2.cmml" xref="S4.SS3.p3.2.m2.1.1.2">𝑡</ci><cn type="integer" id="S4.SS3.p3.2.m2.1.1.3.cmml" xref="S4.SS3.p3.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">t-1</annotation></semantics></math>, the node vectors for the source and receiver, and the global vector. Following the edge update, the nodes are updated in lines 5-9 using the updated values of incoming edges to the node, the previous node vector and the global vector. Finally, the global vector is updated (lines 10-14) using the updated values of all the nodes and edges. This process is repeated for 3 iterations with the output being updated edge, node and global vectors.</p>
</div>
<figure id="algorithm1" class="ltx_float ltx_algorithm">
<div id="algorithm1.27" class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div id="algorithm1.1.1" class="ltx_listingline">

<span id="algorithm1.1.1.1" class="ltx_text"><span id="algorithm1.1.1.1.1" class="ltx_text ltx_font_bold">Precondition:</span> </span>Initial Graph <math id="algorithm1.1.1.m1.3" class="ltx_Math" alttext="(E_{0},V_{0},u_{0})" display="inline"><semantics id="algorithm1.1.1.m1.3a"><mrow id="algorithm1.1.1.m1.3.3.3" xref="algorithm1.1.1.m1.3.3.4.cmml"><mo stretchy="false" id="algorithm1.1.1.m1.3.3.3.4" xref="algorithm1.1.1.m1.3.3.4.cmml">(</mo><msub id="algorithm1.1.1.m1.1.1.1.1" xref="algorithm1.1.1.m1.1.1.1.1.cmml"><mi id="algorithm1.1.1.m1.1.1.1.1.2" xref="algorithm1.1.1.m1.1.1.1.1.2.cmml">E</mi><mn id="algorithm1.1.1.m1.1.1.1.1.3" xref="algorithm1.1.1.m1.1.1.1.1.3.cmml">0</mn></msub><mo id="algorithm1.1.1.m1.3.3.3.5" xref="algorithm1.1.1.m1.3.3.4.cmml">,</mo><msub id="algorithm1.1.1.m1.2.2.2.2" xref="algorithm1.1.1.m1.2.2.2.2.cmml"><mi id="algorithm1.1.1.m1.2.2.2.2.2" xref="algorithm1.1.1.m1.2.2.2.2.2.cmml">V</mi><mn id="algorithm1.1.1.m1.2.2.2.2.3" xref="algorithm1.1.1.m1.2.2.2.2.3.cmml">0</mn></msub><mo id="algorithm1.1.1.m1.3.3.3.6" xref="algorithm1.1.1.m1.3.3.4.cmml">,</mo><msub id="algorithm1.1.1.m1.3.3.3.3" xref="algorithm1.1.1.m1.3.3.3.3.cmml"><mi id="algorithm1.1.1.m1.3.3.3.3.2" xref="algorithm1.1.1.m1.3.3.3.3.2.cmml">u</mi><mn id="algorithm1.1.1.m1.3.3.3.3.3" xref="algorithm1.1.1.m1.3.3.3.3.3.cmml">0</mn></msub><mo stretchy="false" id="algorithm1.1.1.m1.3.3.3.7" xref="algorithm1.1.1.m1.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.1.1.m1.3b"><vector id="algorithm1.1.1.m1.3.3.4.cmml" xref="algorithm1.1.1.m1.3.3.3"><apply id="algorithm1.1.1.m1.1.1.1.1.cmml" xref="algorithm1.1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.1.1.m1.1.1.1.1.1.cmml" xref="algorithm1.1.1.m1.1.1.1.1">subscript</csymbol><ci id="algorithm1.1.1.m1.1.1.1.1.2.cmml" xref="algorithm1.1.1.m1.1.1.1.1.2">𝐸</ci><cn type="integer" id="algorithm1.1.1.m1.1.1.1.1.3.cmml" xref="algorithm1.1.1.m1.1.1.1.1.3">0</cn></apply><apply id="algorithm1.1.1.m1.2.2.2.2.cmml" xref="algorithm1.1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="algorithm1.1.1.m1.2.2.2.2.1.cmml" xref="algorithm1.1.1.m1.2.2.2.2">subscript</csymbol><ci id="algorithm1.1.1.m1.2.2.2.2.2.cmml" xref="algorithm1.1.1.m1.2.2.2.2.2">𝑉</ci><cn type="integer" id="algorithm1.1.1.m1.2.2.2.2.3.cmml" xref="algorithm1.1.1.m1.2.2.2.2.3">0</cn></apply><apply id="algorithm1.1.1.m1.3.3.3.3.cmml" xref="algorithm1.1.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="algorithm1.1.1.m1.3.3.3.3.1.cmml" xref="algorithm1.1.1.m1.3.3.3.3">subscript</csymbol><ci id="algorithm1.1.1.m1.3.3.3.3.2.cmml" xref="algorithm1.1.1.m1.3.3.3.3.2">𝑢</ci><cn type="integer" id="algorithm1.1.1.m1.3.3.3.3.3.cmml" xref="algorithm1.1.1.m1.3.3.3.3.3">0</cn></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.1.1.m1.3c">(E_{0},V_{0},u_{0})</annotation></semantics></math>
</div>
<div id="algorithm1.2.2" class="ltx_listingline">
<span id="algorithm1.2.2.1" class="ltx_text"><span id="algorithm1.2.2.1.1" class="ltx_text ltx_font_bold">Postcondition:</span> </span>Encoding <math id="algorithm1.2.2.m1.3" class="ltx_Math" alttext="(E_{T},V_{t},u_{T})" display="inline"><semantics id="algorithm1.2.2.m1.3a"><mrow id="algorithm1.2.2.m1.3.3.3" xref="algorithm1.2.2.m1.3.3.4.cmml"><mo stretchy="false" id="algorithm1.2.2.m1.3.3.3.4" xref="algorithm1.2.2.m1.3.3.4.cmml">(</mo><msub id="algorithm1.2.2.m1.1.1.1.1" xref="algorithm1.2.2.m1.1.1.1.1.cmml"><mi id="algorithm1.2.2.m1.1.1.1.1.2" xref="algorithm1.2.2.m1.1.1.1.1.2.cmml">E</mi><mi id="algorithm1.2.2.m1.1.1.1.1.3" xref="algorithm1.2.2.m1.1.1.1.1.3.cmml">T</mi></msub><mo id="algorithm1.2.2.m1.3.3.3.5" xref="algorithm1.2.2.m1.3.3.4.cmml">,</mo><msub id="algorithm1.2.2.m1.2.2.2.2" xref="algorithm1.2.2.m1.2.2.2.2.cmml"><mi id="algorithm1.2.2.m1.2.2.2.2.2" xref="algorithm1.2.2.m1.2.2.2.2.2.cmml">V</mi><mi id="algorithm1.2.2.m1.2.2.2.2.3" xref="algorithm1.2.2.m1.2.2.2.2.3.cmml">t</mi></msub><mo id="algorithm1.2.2.m1.3.3.3.6" xref="algorithm1.2.2.m1.3.3.4.cmml">,</mo><msub id="algorithm1.2.2.m1.3.3.3.3" xref="algorithm1.2.2.m1.3.3.3.3.cmml"><mi id="algorithm1.2.2.m1.3.3.3.3.2" xref="algorithm1.2.2.m1.3.3.3.3.2.cmml">u</mi><mi id="algorithm1.2.2.m1.3.3.3.3.3" xref="algorithm1.2.2.m1.3.3.3.3.3.cmml">T</mi></msub><mo stretchy="false" id="algorithm1.2.2.m1.3.3.3.7" xref="algorithm1.2.2.m1.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.2.2.m1.3b"><vector id="algorithm1.2.2.m1.3.3.4.cmml" xref="algorithm1.2.2.m1.3.3.3"><apply id="algorithm1.2.2.m1.1.1.1.1.cmml" xref="algorithm1.2.2.m1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.2.2.m1.1.1.1.1.1.cmml" xref="algorithm1.2.2.m1.1.1.1.1">subscript</csymbol><ci id="algorithm1.2.2.m1.1.1.1.1.2.cmml" xref="algorithm1.2.2.m1.1.1.1.1.2">𝐸</ci><ci id="algorithm1.2.2.m1.1.1.1.1.3.cmml" xref="algorithm1.2.2.m1.1.1.1.1.3">𝑇</ci></apply><apply id="algorithm1.2.2.m1.2.2.2.2.cmml" xref="algorithm1.2.2.m1.2.2.2.2"><csymbol cd="ambiguous" id="algorithm1.2.2.m1.2.2.2.2.1.cmml" xref="algorithm1.2.2.m1.2.2.2.2">subscript</csymbol><ci id="algorithm1.2.2.m1.2.2.2.2.2.cmml" xref="algorithm1.2.2.m1.2.2.2.2.2">𝑉</ci><ci id="algorithm1.2.2.m1.2.2.2.2.3.cmml" xref="algorithm1.2.2.m1.2.2.2.2.3">𝑡</ci></apply><apply id="algorithm1.2.2.m1.3.3.3.3.cmml" xref="algorithm1.2.2.m1.3.3.3.3"><csymbol cd="ambiguous" id="algorithm1.2.2.m1.3.3.3.3.1.cmml" xref="algorithm1.2.2.m1.3.3.3.3">subscript</csymbol><ci id="algorithm1.2.2.m1.3.3.3.3.2.cmml" xref="algorithm1.2.2.m1.3.3.3.3.2">𝑢</ci><ci id="algorithm1.2.2.m1.3.3.3.3.3.cmml" xref="algorithm1.2.2.m1.3.3.3.3.3">𝑇</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.2.2.m1.3c">(E_{T},V_{t},u_{T})</annotation></semantics></math>
</div>
<div id="algorithm1.3.3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">1</span>
<span id="algorithm1.3.3.2" class="ltx_text ltx_font_bold">for</span> <em id="algorithm1.3.3.1" class="ltx_emph ltx_font_italic"><math id="algorithm1.3.3.1.m1.1" class="ltx_Math" alttext="t\in\{1...T\}" display="inline"><semantics id="algorithm1.3.3.1.m1.1a"><mrow id="algorithm1.3.3.1.m1.1.1" xref="algorithm1.3.3.1.m1.1.1.cmml"><mi id="algorithm1.3.3.1.m1.1.1.3" xref="algorithm1.3.3.1.m1.1.1.3.cmml">t</mi><mo id="algorithm1.3.3.1.m1.1.1.2" xref="algorithm1.3.3.1.m1.1.1.2.cmml">∈</mo><mrow id="algorithm1.3.3.1.m1.1.1.1.1" xref="algorithm1.3.3.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="algorithm1.3.3.1.m1.1.1.1.1.2" xref="algorithm1.3.3.1.m1.1.1.1.2.cmml">{</mo><mrow id="algorithm1.3.3.1.m1.1.1.1.1.1" xref="algorithm1.3.3.1.m1.1.1.1.1.1.cmml"><mn id="algorithm1.3.3.1.m1.1.1.1.1.1.2" xref="algorithm1.3.3.1.m1.1.1.1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="algorithm1.3.3.1.m1.1.1.1.1.1.1" xref="algorithm1.3.3.1.m1.1.1.1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="algorithm1.3.3.1.m1.1.1.1.1.1.3" xref="algorithm1.3.3.1.m1.1.1.1.1.1.3.cmml">…</mi><mo lspace="0em" rspace="0em" id="algorithm1.3.3.1.m1.1.1.1.1.1.1a" xref="algorithm1.3.3.1.m1.1.1.1.1.1.1.cmml">​</mo><mi id="algorithm1.3.3.1.m1.1.1.1.1.1.4" xref="algorithm1.3.3.1.m1.1.1.1.1.1.4.cmml">T</mi></mrow><mo stretchy="false" id="algorithm1.3.3.1.m1.1.1.1.1.3" xref="algorithm1.3.3.1.m1.1.1.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.3.3.1.m1.1b"><apply id="algorithm1.3.3.1.m1.1.1.cmml" xref="algorithm1.3.3.1.m1.1.1"><in id="algorithm1.3.3.1.m1.1.1.2.cmml" xref="algorithm1.3.3.1.m1.1.1.2"></in><ci id="algorithm1.3.3.1.m1.1.1.3.cmml" xref="algorithm1.3.3.1.m1.1.1.3">𝑡</ci><set id="algorithm1.3.3.1.m1.1.1.1.2.cmml" xref="algorithm1.3.3.1.m1.1.1.1.1"><apply id="algorithm1.3.3.1.m1.1.1.1.1.1.cmml" xref="algorithm1.3.3.1.m1.1.1.1.1.1"><times id="algorithm1.3.3.1.m1.1.1.1.1.1.1.cmml" xref="algorithm1.3.3.1.m1.1.1.1.1.1.1"></times><cn type="integer" id="algorithm1.3.3.1.m1.1.1.1.1.1.2.cmml" xref="algorithm1.3.3.1.m1.1.1.1.1.1.2">1</cn><ci id="algorithm1.3.3.1.m1.1.1.1.1.1.3.cmml" xref="algorithm1.3.3.1.m1.1.1.1.1.1.3">…</ci><ci id="algorithm1.3.3.1.m1.1.1.1.1.1.4.cmml" xref="algorithm1.3.3.1.m1.1.1.1.1.1.4">𝑇</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.3.3.1.m1.1c">t\in\{1...T\}</annotation></semantics></math></em> <span id="algorithm1.3.3.3" class="ltx_text ltx_font_bold">do</span> 
</div>
<div id="algorithm1.4.4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">2</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span id="algorithm1.4.4.2" class="ltx_text ltx_font_bold">for</span> <em id="algorithm1.4.4.1" class="ltx_emph ltx_font_italic"><math id="algorithm1.4.4.1.m1.1" class="ltx_Math" alttext="k\in\{1...N^{e}\}" display="inline"><semantics id="algorithm1.4.4.1.m1.1a"><mrow id="algorithm1.4.4.1.m1.1.1" xref="algorithm1.4.4.1.m1.1.1.cmml"><mi id="algorithm1.4.4.1.m1.1.1.3" xref="algorithm1.4.4.1.m1.1.1.3.cmml">k</mi><mo id="algorithm1.4.4.1.m1.1.1.2" xref="algorithm1.4.4.1.m1.1.1.2.cmml">∈</mo><mrow id="algorithm1.4.4.1.m1.1.1.1.1" xref="algorithm1.4.4.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="algorithm1.4.4.1.m1.1.1.1.1.2" xref="algorithm1.4.4.1.m1.1.1.1.2.cmml">{</mo><mrow id="algorithm1.4.4.1.m1.1.1.1.1.1" xref="algorithm1.4.4.1.m1.1.1.1.1.1.cmml"><mn id="algorithm1.4.4.1.m1.1.1.1.1.1.2" xref="algorithm1.4.4.1.m1.1.1.1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="algorithm1.4.4.1.m1.1.1.1.1.1.1" xref="algorithm1.4.4.1.m1.1.1.1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="algorithm1.4.4.1.m1.1.1.1.1.1.3" xref="algorithm1.4.4.1.m1.1.1.1.1.1.3.cmml">…</mi><mo lspace="0em" rspace="0em" id="algorithm1.4.4.1.m1.1.1.1.1.1.1a" xref="algorithm1.4.4.1.m1.1.1.1.1.1.1.cmml">​</mo><msup id="algorithm1.4.4.1.m1.1.1.1.1.1.4" xref="algorithm1.4.4.1.m1.1.1.1.1.1.4.cmml"><mi id="algorithm1.4.4.1.m1.1.1.1.1.1.4.2" xref="algorithm1.4.4.1.m1.1.1.1.1.1.4.2.cmml">N</mi><mi id="algorithm1.4.4.1.m1.1.1.1.1.1.4.3" xref="algorithm1.4.4.1.m1.1.1.1.1.1.4.3.cmml">e</mi></msup></mrow><mo stretchy="false" id="algorithm1.4.4.1.m1.1.1.1.1.3" xref="algorithm1.4.4.1.m1.1.1.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.4.4.1.m1.1b"><apply id="algorithm1.4.4.1.m1.1.1.cmml" xref="algorithm1.4.4.1.m1.1.1"><in id="algorithm1.4.4.1.m1.1.1.2.cmml" xref="algorithm1.4.4.1.m1.1.1.2"></in><ci id="algorithm1.4.4.1.m1.1.1.3.cmml" xref="algorithm1.4.4.1.m1.1.1.3">𝑘</ci><set id="algorithm1.4.4.1.m1.1.1.1.2.cmml" xref="algorithm1.4.4.1.m1.1.1.1.1"><apply id="algorithm1.4.4.1.m1.1.1.1.1.1.cmml" xref="algorithm1.4.4.1.m1.1.1.1.1.1"><times id="algorithm1.4.4.1.m1.1.1.1.1.1.1.cmml" xref="algorithm1.4.4.1.m1.1.1.1.1.1.1"></times><cn type="integer" id="algorithm1.4.4.1.m1.1.1.1.1.1.2.cmml" xref="algorithm1.4.4.1.m1.1.1.1.1.1.2">1</cn><ci id="algorithm1.4.4.1.m1.1.1.1.1.1.3.cmml" xref="algorithm1.4.4.1.m1.1.1.1.1.1.3">…</ci><apply id="algorithm1.4.4.1.m1.1.1.1.1.1.4.cmml" xref="algorithm1.4.4.1.m1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="algorithm1.4.4.1.m1.1.1.1.1.1.4.1.cmml" xref="algorithm1.4.4.1.m1.1.1.1.1.1.4">superscript</csymbol><ci id="algorithm1.4.4.1.m1.1.1.1.1.1.4.2.cmml" xref="algorithm1.4.4.1.m1.1.1.1.1.1.4.2">𝑁</ci><ci id="algorithm1.4.4.1.m1.1.1.1.1.1.4.3.cmml" xref="algorithm1.4.4.1.m1.1.1.1.1.1.4.3">𝑒</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.4.4.1.m1.1c">k\in\{1...N^{e}\}</annotation></semantics></math></em> <span id="algorithm1.4.4.3" class="ltx_text ltx_font_bold">do</span> 
</div>
<div id="algorithm1.6.6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">3</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math id="algorithm1.5.5.m1.1" class="ltx_Math" alttext="e_{k}^{{}^{\prime}}" display="inline"><semantics id="algorithm1.5.5.m1.1a"><msubsup id="algorithm1.5.5.m1.1.1" xref="algorithm1.5.5.m1.1.1.cmml"><mi id="algorithm1.5.5.m1.1.1.2.2" xref="algorithm1.5.5.m1.1.1.2.2.cmml">e</mi><mi id="algorithm1.5.5.m1.1.1.2.3" xref="algorithm1.5.5.m1.1.1.2.3.cmml">k</mi><msup id="algorithm1.5.5.m1.1.1.3" xref="algorithm1.5.5.m1.1.1.3.cmml"><mi id="algorithm1.5.5.m1.1.1.3a" xref="algorithm1.5.5.m1.1.1.3.cmml"></mi><mo id="algorithm1.5.5.m1.1.1.3.1" xref="algorithm1.5.5.m1.1.1.3.1.cmml">′</mo></msup></msubsup><annotation-xml encoding="MathML-Content" id="algorithm1.5.5.m1.1b"><apply id="algorithm1.5.5.m1.1.1.cmml" xref="algorithm1.5.5.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.5.5.m1.1.1.1.cmml" xref="algorithm1.5.5.m1.1.1">superscript</csymbol><apply id="algorithm1.5.5.m1.1.1.2.cmml" xref="algorithm1.5.5.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.5.5.m1.1.1.2.1.cmml" xref="algorithm1.5.5.m1.1.1">subscript</csymbol><ci id="algorithm1.5.5.m1.1.1.2.2.cmml" xref="algorithm1.5.5.m1.1.1.2.2">𝑒</ci><ci id="algorithm1.5.5.m1.1.1.2.3.cmml" xref="algorithm1.5.5.m1.1.1.2.3">𝑘</ci></apply><apply id="algorithm1.5.5.m1.1.1.3.cmml" xref="algorithm1.5.5.m1.1.1.3"><ci id="algorithm1.5.5.m1.1.1.3.1.cmml" xref="algorithm1.5.5.m1.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.5.5.m1.1c">e_{k}^{{}^{\prime}}</annotation></semantics></math> = <span id="algorithm1.6.6.1" class="ltx_text ltx_font_smallcaps">mlp</span>([<math id="algorithm1.6.6.m2.4" class="ltx_Math" alttext="e_{k},v_{r_{k}},v_{s_{k}},u" display="inline"><semantics id="algorithm1.6.6.m2.4a"><mrow id="algorithm1.6.6.m2.4.4.3" xref="algorithm1.6.6.m2.4.4.4.cmml"><msub id="algorithm1.6.6.m2.2.2.1.1" xref="algorithm1.6.6.m2.2.2.1.1.cmml"><mi id="algorithm1.6.6.m2.2.2.1.1.2" xref="algorithm1.6.6.m2.2.2.1.1.2.cmml">e</mi><mi id="algorithm1.6.6.m2.2.2.1.1.3" xref="algorithm1.6.6.m2.2.2.1.1.3.cmml">k</mi></msub><mo id="algorithm1.6.6.m2.4.4.3.4" xref="algorithm1.6.6.m2.4.4.4.cmml">,</mo><msub id="algorithm1.6.6.m2.3.3.2.2" xref="algorithm1.6.6.m2.3.3.2.2.cmml"><mi id="algorithm1.6.6.m2.3.3.2.2.2" xref="algorithm1.6.6.m2.3.3.2.2.2.cmml">v</mi><msub id="algorithm1.6.6.m2.3.3.2.2.3" xref="algorithm1.6.6.m2.3.3.2.2.3.cmml"><mi id="algorithm1.6.6.m2.3.3.2.2.3.2" xref="algorithm1.6.6.m2.3.3.2.2.3.2.cmml">r</mi><mi id="algorithm1.6.6.m2.3.3.2.2.3.3" xref="algorithm1.6.6.m2.3.3.2.2.3.3.cmml">k</mi></msub></msub><mo id="algorithm1.6.6.m2.4.4.3.5" xref="algorithm1.6.6.m2.4.4.4.cmml">,</mo><msub id="algorithm1.6.6.m2.4.4.3.3" xref="algorithm1.6.6.m2.4.4.3.3.cmml"><mi id="algorithm1.6.6.m2.4.4.3.3.2" xref="algorithm1.6.6.m2.4.4.3.3.2.cmml">v</mi><msub id="algorithm1.6.6.m2.4.4.3.3.3" xref="algorithm1.6.6.m2.4.4.3.3.3.cmml"><mi id="algorithm1.6.6.m2.4.4.3.3.3.2" xref="algorithm1.6.6.m2.4.4.3.3.3.2.cmml">s</mi><mi id="algorithm1.6.6.m2.4.4.3.3.3.3" xref="algorithm1.6.6.m2.4.4.3.3.3.3.cmml">k</mi></msub></msub><mo id="algorithm1.6.6.m2.4.4.3.6" xref="algorithm1.6.6.m2.4.4.4.cmml">,</mo><mi id="algorithm1.6.6.m2.1.1" xref="algorithm1.6.6.m2.1.1.cmml">u</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.6.6.m2.4b"><list id="algorithm1.6.6.m2.4.4.4.cmml" xref="algorithm1.6.6.m2.4.4.3"><apply id="algorithm1.6.6.m2.2.2.1.1.cmml" xref="algorithm1.6.6.m2.2.2.1.1"><csymbol cd="ambiguous" id="algorithm1.6.6.m2.2.2.1.1.1.cmml" xref="algorithm1.6.6.m2.2.2.1.1">subscript</csymbol><ci id="algorithm1.6.6.m2.2.2.1.1.2.cmml" xref="algorithm1.6.6.m2.2.2.1.1.2">𝑒</ci><ci id="algorithm1.6.6.m2.2.2.1.1.3.cmml" xref="algorithm1.6.6.m2.2.2.1.1.3">𝑘</ci></apply><apply id="algorithm1.6.6.m2.3.3.2.2.cmml" xref="algorithm1.6.6.m2.3.3.2.2"><csymbol cd="ambiguous" id="algorithm1.6.6.m2.3.3.2.2.1.cmml" xref="algorithm1.6.6.m2.3.3.2.2">subscript</csymbol><ci id="algorithm1.6.6.m2.3.3.2.2.2.cmml" xref="algorithm1.6.6.m2.3.3.2.2.2">𝑣</ci><apply id="algorithm1.6.6.m2.3.3.2.2.3.cmml" xref="algorithm1.6.6.m2.3.3.2.2.3"><csymbol cd="ambiguous" id="algorithm1.6.6.m2.3.3.2.2.3.1.cmml" xref="algorithm1.6.6.m2.3.3.2.2.3">subscript</csymbol><ci id="algorithm1.6.6.m2.3.3.2.2.3.2.cmml" xref="algorithm1.6.6.m2.3.3.2.2.3.2">𝑟</ci><ci id="algorithm1.6.6.m2.3.3.2.2.3.3.cmml" xref="algorithm1.6.6.m2.3.3.2.2.3.3">𝑘</ci></apply></apply><apply id="algorithm1.6.6.m2.4.4.3.3.cmml" xref="algorithm1.6.6.m2.4.4.3.3"><csymbol cd="ambiguous" id="algorithm1.6.6.m2.4.4.3.3.1.cmml" xref="algorithm1.6.6.m2.4.4.3.3">subscript</csymbol><ci id="algorithm1.6.6.m2.4.4.3.3.2.cmml" xref="algorithm1.6.6.m2.4.4.3.3.2">𝑣</ci><apply id="algorithm1.6.6.m2.4.4.3.3.3.cmml" xref="algorithm1.6.6.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="algorithm1.6.6.m2.4.4.3.3.3.1.cmml" xref="algorithm1.6.6.m2.4.4.3.3.3">subscript</csymbol><ci id="algorithm1.6.6.m2.4.4.3.3.3.2.cmml" xref="algorithm1.6.6.m2.4.4.3.3.3.2">𝑠</ci><ci id="algorithm1.6.6.m2.4.4.3.3.3.3.cmml" xref="algorithm1.6.6.m2.4.4.3.3.3.3">𝑘</ci></apply></apply><ci id="algorithm1.6.6.m2.1.1.cmml" xref="algorithm1.6.6.m2.1.1">𝑢</ci></list></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.6.6.m2.4c">e_{k},v_{r_{k}},v_{s_{k}},u</annotation></semantics></math>])

</div>
<div id="algorithm1.27.28" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">4</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    end for
</div>
<div id="algorithm1.7.7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">5</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   <span id="algorithm1.7.7.2" class="ltx_text ltx_font_bold">for</span> <em id="algorithm1.7.7.1" class="ltx_emph ltx_font_italic"><math id="algorithm1.7.7.1.m1.1" class="ltx_Math" alttext="i\in\{1...N^{v}\}" display="inline"><semantics id="algorithm1.7.7.1.m1.1a"><mrow id="algorithm1.7.7.1.m1.1.1" xref="algorithm1.7.7.1.m1.1.1.cmml"><mi id="algorithm1.7.7.1.m1.1.1.3" xref="algorithm1.7.7.1.m1.1.1.3.cmml">i</mi><mo id="algorithm1.7.7.1.m1.1.1.2" xref="algorithm1.7.7.1.m1.1.1.2.cmml">∈</mo><mrow id="algorithm1.7.7.1.m1.1.1.1.1" xref="algorithm1.7.7.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="algorithm1.7.7.1.m1.1.1.1.1.2" xref="algorithm1.7.7.1.m1.1.1.1.2.cmml">{</mo><mrow id="algorithm1.7.7.1.m1.1.1.1.1.1" xref="algorithm1.7.7.1.m1.1.1.1.1.1.cmml"><mn id="algorithm1.7.7.1.m1.1.1.1.1.1.2" xref="algorithm1.7.7.1.m1.1.1.1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="algorithm1.7.7.1.m1.1.1.1.1.1.1" xref="algorithm1.7.7.1.m1.1.1.1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="algorithm1.7.7.1.m1.1.1.1.1.1.3" xref="algorithm1.7.7.1.m1.1.1.1.1.1.3.cmml">…</mi><mo lspace="0em" rspace="0em" id="algorithm1.7.7.1.m1.1.1.1.1.1.1a" xref="algorithm1.7.7.1.m1.1.1.1.1.1.1.cmml">​</mo><msup id="algorithm1.7.7.1.m1.1.1.1.1.1.4" xref="algorithm1.7.7.1.m1.1.1.1.1.1.4.cmml"><mi id="algorithm1.7.7.1.m1.1.1.1.1.1.4.2" xref="algorithm1.7.7.1.m1.1.1.1.1.1.4.2.cmml">N</mi><mi id="algorithm1.7.7.1.m1.1.1.1.1.1.4.3" xref="algorithm1.7.7.1.m1.1.1.1.1.1.4.3.cmml">v</mi></msup></mrow><mo stretchy="false" id="algorithm1.7.7.1.m1.1.1.1.1.3" xref="algorithm1.7.7.1.m1.1.1.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.7.7.1.m1.1b"><apply id="algorithm1.7.7.1.m1.1.1.cmml" xref="algorithm1.7.7.1.m1.1.1"><in id="algorithm1.7.7.1.m1.1.1.2.cmml" xref="algorithm1.7.7.1.m1.1.1.2"></in><ci id="algorithm1.7.7.1.m1.1.1.3.cmml" xref="algorithm1.7.7.1.m1.1.1.3">𝑖</ci><set id="algorithm1.7.7.1.m1.1.1.1.2.cmml" xref="algorithm1.7.7.1.m1.1.1.1.1"><apply id="algorithm1.7.7.1.m1.1.1.1.1.1.cmml" xref="algorithm1.7.7.1.m1.1.1.1.1.1"><times id="algorithm1.7.7.1.m1.1.1.1.1.1.1.cmml" xref="algorithm1.7.7.1.m1.1.1.1.1.1.1"></times><cn type="integer" id="algorithm1.7.7.1.m1.1.1.1.1.1.2.cmml" xref="algorithm1.7.7.1.m1.1.1.1.1.1.2">1</cn><ci id="algorithm1.7.7.1.m1.1.1.1.1.1.3.cmml" xref="algorithm1.7.7.1.m1.1.1.1.1.1.3">…</ci><apply id="algorithm1.7.7.1.m1.1.1.1.1.1.4.cmml" xref="algorithm1.7.7.1.m1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="algorithm1.7.7.1.m1.1.1.1.1.1.4.1.cmml" xref="algorithm1.7.7.1.m1.1.1.1.1.1.4">superscript</csymbol><ci id="algorithm1.7.7.1.m1.1.1.1.1.1.4.2.cmml" xref="algorithm1.7.7.1.m1.1.1.1.1.1.4.2">𝑁</ci><ci id="algorithm1.7.7.1.m1.1.1.1.1.1.4.3.cmml" xref="algorithm1.7.7.1.m1.1.1.1.1.1.4.3">𝑣</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.7.7.1.m1.1c">i\in\{1...N^{v}\}</annotation></semantics></math></em> <span id="algorithm1.7.7.3" class="ltx_text ltx_font_bold">do</span> 
</div>
<div id="algorithm1.9.9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">6</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
let <math id="algorithm1.8.8.m1.1" class="ltx_Math" alttext="E_{i}^{{}^{\prime}}" display="inline"><semantics id="algorithm1.8.8.m1.1a"><msubsup id="algorithm1.8.8.m1.1.1" xref="algorithm1.8.8.m1.1.1.cmml"><mi id="algorithm1.8.8.m1.1.1.2.2" xref="algorithm1.8.8.m1.1.1.2.2.cmml">E</mi><mi id="algorithm1.8.8.m1.1.1.2.3" xref="algorithm1.8.8.m1.1.1.2.3.cmml">i</mi><msup id="algorithm1.8.8.m1.1.1.3" xref="algorithm1.8.8.m1.1.1.3.cmml"><mi id="algorithm1.8.8.m1.1.1.3a" xref="algorithm1.8.8.m1.1.1.3.cmml"></mi><mo id="algorithm1.8.8.m1.1.1.3.1" xref="algorithm1.8.8.m1.1.1.3.1.cmml">′</mo></msup></msubsup><annotation-xml encoding="MathML-Content" id="algorithm1.8.8.m1.1b"><apply id="algorithm1.8.8.m1.1.1.cmml" xref="algorithm1.8.8.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.8.8.m1.1.1.1.cmml" xref="algorithm1.8.8.m1.1.1">superscript</csymbol><apply id="algorithm1.8.8.m1.1.1.2.cmml" xref="algorithm1.8.8.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.8.8.m1.1.1.2.1.cmml" xref="algorithm1.8.8.m1.1.1">subscript</csymbol><ci id="algorithm1.8.8.m1.1.1.2.2.cmml" xref="algorithm1.8.8.m1.1.1.2.2">𝐸</ci><ci id="algorithm1.8.8.m1.1.1.2.3.cmml" xref="algorithm1.8.8.m1.1.1.2.3">𝑖</ci></apply><apply id="algorithm1.8.8.m1.1.1.3.cmml" xref="algorithm1.8.8.m1.1.1.3"><ci id="algorithm1.8.8.m1.1.1.3.1.cmml" xref="algorithm1.8.8.m1.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.8.8.m1.1c">E_{i}^{{}^{\prime}}</annotation></semantics></math> = <math id="algorithm1.9.9.m2.5" class="ltx_Math" alttext="{(e_{k}^{{}^{\prime}},r_{k},s_{k})}_{r_{k}=i,k=1:N^{e}}" display="inline"><semantics id="algorithm1.9.9.m2.5a"><msub id="algorithm1.9.9.m2.5.5" xref="algorithm1.9.9.m2.5.5.cmml"><mrow id="algorithm1.9.9.m2.5.5.3.3" xref="algorithm1.9.9.m2.5.5.3.4.cmml"><mo stretchy="false" id="algorithm1.9.9.m2.5.5.3.3.4" xref="algorithm1.9.9.m2.5.5.3.4.cmml">(</mo><msubsup id="algorithm1.9.9.m2.3.3.1.1.1" xref="algorithm1.9.9.m2.3.3.1.1.1.cmml"><mi id="algorithm1.9.9.m2.3.3.1.1.1.2.2" xref="algorithm1.9.9.m2.3.3.1.1.1.2.2.cmml">e</mi><mi id="algorithm1.9.9.m2.3.3.1.1.1.2.3" xref="algorithm1.9.9.m2.3.3.1.1.1.2.3.cmml">k</mi><msup id="algorithm1.9.9.m2.3.3.1.1.1.3" xref="algorithm1.9.9.m2.3.3.1.1.1.3.cmml"><mi id="algorithm1.9.9.m2.3.3.1.1.1.3a" xref="algorithm1.9.9.m2.3.3.1.1.1.3.cmml"></mi><mo id="algorithm1.9.9.m2.3.3.1.1.1.3.1" xref="algorithm1.9.9.m2.3.3.1.1.1.3.1.cmml">′</mo></msup></msubsup><mo id="algorithm1.9.9.m2.5.5.3.3.5" xref="algorithm1.9.9.m2.5.5.3.4.cmml">,</mo><msub id="algorithm1.9.9.m2.4.4.2.2.2" xref="algorithm1.9.9.m2.4.4.2.2.2.cmml"><mi id="algorithm1.9.9.m2.4.4.2.2.2.2" xref="algorithm1.9.9.m2.4.4.2.2.2.2.cmml">r</mi><mi id="algorithm1.9.9.m2.4.4.2.2.2.3" xref="algorithm1.9.9.m2.4.4.2.2.2.3.cmml">k</mi></msub><mo id="algorithm1.9.9.m2.5.5.3.3.6" xref="algorithm1.9.9.m2.5.5.3.4.cmml">,</mo><msub id="algorithm1.9.9.m2.5.5.3.3.3" xref="algorithm1.9.9.m2.5.5.3.3.3.cmml"><mi id="algorithm1.9.9.m2.5.5.3.3.3.2" xref="algorithm1.9.9.m2.5.5.3.3.3.2.cmml">s</mi><mi id="algorithm1.9.9.m2.5.5.3.3.3.3" xref="algorithm1.9.9.m2.5.5.3.3.3.3.cmml">k</mi></msub><mo stretchy="false" id="algorithm1.9.9.m2.5.5.3.3.7" xref="algorithm1.9.9.m2.5.5.3.4.cmml">)</mo></mrow><mrow id="algorithm1.9.9.m2.2.2.2" xref="algorithm1.9.9.m2.2.2.2.cmml"><mrow id="algorithm1.9.9.m2.2.2.2.2.2" xref="algorithm1.9.9.m2.2.2.2.2.3.cmml"><mrow id="algorithm1.9.9.m2.1.1.1.1.1.1" xref="algorithm1.9.9.m2.1.1.1.1.1.1.cmml"><msub id="algorithm1.9.9.m2.1.1.1.1.1.1.2" xref="algorithm1.9.9.m2.1.1.1.1.1.1.2.cmml"><mi id="algorithm1.9.9.m2.1.1.1.1.1.1.2.2" xref="algorithm1.9.9.m2.1.1.1.1.1.1.2.2.cmml">r</mi><mi id="algorithm1.9.9.m2.1.1.1.1.1.1.2.3" xref="algorithm1.9.9.m2.1.1.1.1.1.1.2.3.cmml">k</mi></msub><mo id="algorithm1.9.9.m2.1.1.1.1.1.1.1" xref="algorithm1.9.9.m2.1.1.1.1.1.1.1.cmml">=</mo><mi id="algorithm1.9.9.m2.1.1.1.1.1.1.3" xref="algorithm1.9.9.m2.1.1.1.1.1.1.3.cmml">i</mi></mrow><mo id="algorithm1.9.9.m2.2.2.2.2.2.3" xref="algorithm1.9.9.m2.2.2.2.2.3a.cmml">,</mo><mrow id="algorithm1.9.9.m2.2.2.2.2.2.2" xref="algorithm1.9.9.m2.2.2.2.2.2.2.cmml"><mi id="algorithm1.9.9.m2.2.2.2.2.2.2.2" xref="algorithm1.9.9.m2.2.2.2.2.2.2.2.cmml">k</mi><mo id="algorithm1.9.9.m2.2.2.2.2.2.2.1" xref="algorithm1.9.9.m2.2.2.2.2.2.2.1.cmml">=</mo><mn id="algorithm1.9.9.m2.2.2.2.2.2.2.3" xref="algorithm1.9.9.m2.2.2.2.2.2.2.3.cmml">1</mn></mrow></mrow><mo lspace="0.278em" rspace="0.278em" id="algorithm1.9.9.m2.2.2.2.3" xref="algorithm1.9.9.m2.2.2.2.3.cmml">:</mo><msup id="algorithm1.9.9.m2.2.2.2.4" xref="algorithm1.9.9.m2.2.2.2.4.cmml"><mi id="algorithm1.9.9.m2.2.2.2.4.2" xref="algorithm1.9.9.m2.2.2.2.4.2.cmml">N</mi><mi id="algorithm1.9.9.m2.2.2.2.4.3" xref="algorithm1.9.9.m2.2.2.2.4.3.cmml">e</mi></msup></mrow></msub><annotation-xml encoding="MathML-Content" id="algorithm1.9.9.m2.5b"><apply id="algorithm1.9.9.m2.5.5.cmml" xref="algorithm1.9.9.m2.5.5"><csymbol cd="ambiguous" id="algorithm1.9.9.m2.5.5.4.cmml" xref="algorithm1.9.9.m2.5.5">subscript</csymbol><vector id="algorithm1.9.9.m2.5.5.3.4.cmml" xref="algorithm1.9.9.m2.5.5.3.3"><apply id="algorithm1.9.9.m2.3.3.1.1.1.cmml" xref="algorithm1.9.9.m2.3.3.1.1.1"><csymbol cd="ambiguous" id="algorithm1.9.9.m2.3.3.1.1.1.1.cmml" xref="algorithm1.9.9.m2.3.3.1.1.1">superscript</csymbol><apply id="algorithm1.9.9.m2.3.3.1.1.1.2.cmml" xref="algorithm1.9.9.m2.3.3.1.1.1"><csymbol cd="ambiguous" id="algorithm1.9.9.m2.3.3.1.1.1.2.1.cmml" xref="algorithm1.9.9.m2.3.3.1.1.1">subscript</csymbol><ci id="algorithm1.9.9.m2.3.3.1.1.1.2.2.cmml" xref="algorithm1.9.9.m2.3.3.1.1.1.2.2">𝑒</ci><ci id="algorithm1.9.9.m2.3.3.1.1.1.2.3.cmml" xref="algorithm1.9.9.m2.3.3.1.1.1.2.3">𝑘</ci></apply><apply id="algorithm1.9.9.m2.3.3.1.1.1.3.cmml" xref="algorithm1.9.9.m2.3.3.1.1.1.3"><ci id="algorithm1.9.9.m2.3.3.1.1.1.3.1.cmml" xref="algorithm1.9.9.m2.3.3.1.1.1.3.1">′</ci></apply></apply><apply id="algorithm1.9.9.m2.4.4.2.2.2.cmml" xref="algorithm1.9.9.m2.4.4.2.2.2"><csymbol cd="ambiguous" id="algorithm1.9.9.m2.4.4.2.2.2.1.cmml" xref="algorithm1.9.9.m2.4.4.2.2.2">subscript</csymbol><ci id="algorithm1.9.9.m2.4.4.2.2.2.2.cmml" xref="algorithm1.9.9.m2.4.4.2.2.2.2">𝑟</ci><ci id="algorithm1.9.9.m2.4.4.2.2.2.3.cmml" xref="algorithm1.9.9.m2.4.4.2.2.2.3">𝑘</ci></apply><apply id="algorithm1.9.9.m2.5.5.3.3.3.cmml" xref="algorithm1.9.9.m2.5.5.3.3.3"><csymbol cd="ambiguous" id="algorithm1.9.9.m2.5.5.3.3.3.1.cmml" xref="algorithm1.9.9.m2.5.5.3.3.3">subscript</csymbol><ci id="algorithm1.9.9.m2.5.5.3.3.3.2.cmml" xref="algorithm1.9.9.m2.5.5.3.3.3.2">𝑠</ci><ci id="algorithm1.9.9.m2.5.5.3.3.3.3.cmml" xref="algorithm1.9.9.m2.5.5.3.3.3.3">𝑘</ci></apply></vector><apply id="algorithm1.9.9.m2.2.2.2.cmml" xref="algorithm1.9.9.m2.2.2.2"><ci id="algorithm1.9.9.m2.2.2.2.3.cmml" xref="algorithm1.9.9.m2.2.2.2.3">:</ci><apply id="algorithm1.9.9.m2.2.2.2.2.3.cmml" xref="algorithm1.9.9.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="algorithm1.9.9.m2.2.2.2.2.3a.cmml" xref="algorithm1.9.9.m2.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="algorithm1.9.9.m2.1.1.1.1.1.1.cmml" xref="algorithm1.9.9.m2.1.1.1.1.1.1"><eq id="algorithm1.9.9.m2.1.1.1.1.1.1.1.cmml" xref="algorithm1.9.9.m2.1.1.1.1.1.1.1"></eq><apply id="algorithm1.9.9.m2.1.1.1.1.1.1.2.cmml" xref="algorithm1.9.9.m2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="algorithm1.9.9.m2.1.1.1.1.1.1.2.1.cmml" xref="algorithm1.9.9.m2.1.1.1.1.1.1.2">subscript</csymbol><ci id="algorithm1.9.9.m2.1.1.1.1.1.1.2.2.cmml" xref="algorithm1.9.9.m2.1.1.1.1.1.1.2.2">𝑟</ci><ci id="algorithm1.9.9.m2.1.1.1.1.1.1.2.3.cmml" xref="algorithm1.9.9.m2.1.1.1.1.1.1.2.3">𝑘</ci></apply><ci id="algorithm1.9.9.m2.1.1.1.1.1.1.3.cmml" xref="algorithm1.9.9.m2.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="algorithm1.9.9.m2.2.2.2.2.2.2.cmml" xref="algorithm1.9.9.m2.2.2.2.2.2.2"><eq id="algorithm1.9.9.m2.2.2.2.2.2.2.1.cmml" xref="algorithm1.9.9.m2.2.2.2.2.2.2.1"></eq><ci id="algorithm1.9.9.m2.2.2.2.2.2.2.2.cmml" xref="algorithm1.9.9.m2.2.2.2.2.2.2.2">𝑘</ci><cn type="integer" id="algorithm1.9.9.m2.2.2.2.2.2.2.3.cmml" xref="algorithm1.9.9.m2.2.2.2.2.2.2.3">1</cn></apply></apply><apply id="algorithm1.9.9.m2.2.2.2.4.cmml" xref="algorithm1.9.9.m2.2.2.2.4"><csymbol cd="ambiguous" id="algorithm1.9.9.m2.2.2.2.4.1.cmml" xref="algorithm1.9.9.m2.2.2.2.4">superscript</csymbol><ci id="algorithm1.9.9.m2.2.2.2.4.2.cmml" xref="algorithm1.9.9.m2.2.2.2.4.2">𝑁</ci><ci id="algorithm1.9.9.m2.2.2.2.4.3.cmml" xref="algorithm1.9.9.m2.2.2.2.4.3">𝑒</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.9.9.m2.5c">{(e_{k}^{{}^{\prime}},r_{k},s_{k})}_{r_{k}=i,k=1:N^{e}}</annotation></semantics></math> 
</div>
<div id="algorithm1.11.11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">7</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math id="algorithm1.10.10.m1.1" class="ltx_Math" alttext="\bar{e}_{i}^{{}^{\prime}}" display="inline"><semantics id="algorithm1.10.10.m1.1a"><msubsup id="algorithm1.10.10.m1.1.1" xref="algorithm1.10.10.m1.1.1.cmml"><mover accent="true" id="algorithm1.10.10.m1.1.1.2.2" xref="algorithm1.10.10.m1.1.1.2.2.cmml"><mi id="algorithm1.10.10.m1.1.1.2.2.2" xref="algorithm1.10.10.m1.1.1.2.2.2.cmml">e</mi><mo id="algorithm1.10.10.m1.1.1.2.2.1" xref="algorithm1.10.10.m1.1.1.2.2.1.cmml">¯</mo></mover><mi id="algorithm1.10.10.m1.1.1.2.3" xref="algorithm1.10.10.m1.1.1.2.3.cmml">i</mi><msup id="algorithm1.10.10.m1.1.1.3" xref="algorithm1.10.10.m1.1.1.3.cmml"><mi id="algorithm1.10.10.m1.1.1.3a" xref="algorithm1.10.10.m1.1.1.3.cmml"></mi><mo id="algorithm1.10.10.m1.1.1.3.1" xref="algorithm1.10.10.m1.1.1.3.1.cmml">′</mo></msup></msubsup><annotation-xml encoding="MathML-Content" id="algorithm1.10.10.m1.1b"><apply id="algorithm1.10.10.m1.1.1.cmml" xref="algorithm1.10.10.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.10.10.m1.1.1.1.cmml" xref="algorithm1.10.10.m1.1.1">superscript</csymbol><apply id="algorithm1.10.10.m1.1.1.2.cmml" xref="algorithm1.10.10.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.10.10.m1.1.1.2.1.cmml" xref="algorithm1.10.10.m1.1.1">subscript</csymbol><apply id="algorithm1.10.10.m1.1.1.2.2.cmml" xref="algorithm1.10.10.m1.1.1.2.2"><ci id="algorithm1.10.10.m1.1.1.2.2.1.cmml" xref="algorithm1.10.10.m1.1.1.2.2.1">¯</ci><ci id="algorithm1.10.10.m1.1.1.2.2.2.cmml" xref="algorithm1.10.10.m1.1.1.2.2.2">𝑒</ci></apply><ci id="algorithm1.10.10.m1.1.1.2.3.cmml" xref="algorithm1.10.10.m1.1.1.2.3">𝑖</ci></apply><apply id="algorithm1.10.10.m1.1.1.3.cmml" xref="algorithm1.10.10.m1.1.1.3"><ci id="algorithm1.10.10.m1.1.1.3.1.cmml" xref="algorithm1.10.10.m1.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.10.10.m1.1c">\bar{e}_{i}^{{}^{\prime}}</annotation></semantics></math> = <span id="algorithm1.11.11.1" class="ltx_text ltx_font_smallcaps">mean</span>(<math id="algorithm1.11.11.m2.1" class="ltx_Math" alttext="E_{i}^{{}^{\prime}}" display="inline"><semantics id="algorithm1.11.11.m2.1a"><msubsup id="algorithm1.11.11.m2.1.1" xref="algorithm1.11.11.m2.1.1.cmml"><mi id="algorithm1.11.11.m2.1.1.2.2" xref="algorithm1.11.11.m2.1.1.2.2.cmml">E</mi><mi id="algorithm1.11.11.m2.1.1.2.3" xref="algorithm1.11.11.m2.1.1.2.3.cmml">i</mi><msup id="algorithm1.11.11.m2.1.1.3" xref="algorithm1.11.11.m2.1.1.3.cmml"><mi id="algorithm1.11.11.m2.1.1.3a" xref="algorithm1.11.11.m2.1.1.3.cmml"></mi><mo id="algorithm1.11.11.m2.1.1.3.1" xref="algorithm1.11.11.m2.1.1.3.1.cmml">′</mo></msup></msubsup><annotation-xml encoding="MathML-Content" id="algorithm1.11.11.m2.1b"><apply id="algorithm1.11.11.m2.1.1.cmml" xref="algorithm1.11.11.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.11.11.m2.1.1.1.cmml" xref="algorithm1.11.11.m2.1.1">superscript</csymbol><apply id="algorithm1.11.11.m2.1.1.2.cmml" xref="algorithm1.11.11.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.11.11.m2.1.1.2.1.cmml" xref="algorithm1.11.11.m2.1.1">subscript</csymbol><ci id="algorithm1.11.11.m2.1.1.2.2.cmml" xref="algorithm1.11.11.m2.1.1.2.2">𝐸</ci><ci id="algorithm1.11.11.m2.1.1.2.3.cmml" xref="algorithm1.11.11.m2.1.1.2.3">𝑖</ci></apply><apply id="algorithm1.11.11.m2.1.1.3.cmml" xref="algorithm1.11.11.m2.1.1.3"><ci id="algorithm1.11.11.m2.1.1.3.1.cmml" xref="algorithm1.11.11.m2.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.11.11.m2.1c">E_{i}^{{}^{\prime}}</annotation></semantics></math>) 
</div>
<div id="algorithm1.14.14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">8</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math id="algorithm1.12.12.m1.1" class="ltx_Math" alttext="v_{i}^{{}^{\prime}}" display="inline"><semantics id="algorithm1.12.12.m1.1a"><msubsup id="algorithm1.12.12.m1.1.1" xref="algorithm1.12.12.m1.1.1.cmml"><mi id="algorithm1.12.12.m1.1.1.2.2" xref="algorithm1.12.12.m1.1.1.2.2.cmml">v</mi><mi id="algorithm1.12.12.m1.1.1.2.3" xref="algorithm1.12.12.m1.1.1.2.3.cmml">i</mi><msup id="algorithm1.12.12.m1.1.1.3" xref="algorithm1.12.12.m1.1.1.3.cmml"><mi id="algorithm1.12.12.m1.1.1.3a" xref="algorithm1.12.12.m1.1.1.3.cmml"></mi><mo id="algorithm1.12.12.m1.1.1.3.1" xref="algorithm1.12.12.m1.1.1.3.1.cmml">′</mo></msup></msubsup><annotation-xml encoding="MathML-Content" id="algorithm1.12.12.m1.1b"><apply id="algorithm1.12.12.m1.1.1.cmml" xref="algorithm1.12.12.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.12.12.m1.1.1.1.cmml" xref="algorithm1.12.12.m1.1.1">superscript</csymbol><apply id="algorithm1.12.12.m1.1.1.2.cmml" xref="algorithm1.12.12.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.12.12.m1.1.1.2.1.cmml" xref="algorithm1.12.12.m1.1.1">subscript</csymbol><ci id="algorithm1.12.12.m1.1.1.2.2.cmml" xref="algorithm1.12.12.m1.1.1.2.2">𝑣</ci><ci id="algorithm1.12.12.m1.1.1.2.3.cmml" xref="algorithm1.12.12.m1.1.1.2.3">𝑖</ci></apply><apply id="algorithm1.12.12.m1.1.1.3.cmml" xref="algorithm1.12.12.m1.1.1.3"><ci id="algorithm1.12.12.m1.1.1.3.1.cmml" xref="algorithm1.12.12.m1.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.12.12.m1.1c">v_{i}^{{}^{\prime}}</annotation></semantics></math> = <span id="algorithm1.14.14.1" class="ltx_text ltx_font_smallcaps">mlp</span>([<math id="algorithm1.13.13.m2.1" class="ltx_Math" alttext="\bar{e}_{i}^{{}^{\prime}}" display="inline"><semantics id="algorithm1.13.13.m2.1a"><msubsup id="algorithm1.13.13.m2.1.1" xref="algorithm1.13.13.m2.1.1.cmml"><mover accent="true" id="algorithm1.13.13.m2.1.1.2.2" xref="algorithm1.13.13.m2.1.1.2.2.cmml"><mi id="algorithm1.13.13.m2.1.1.2.2.2" xref="algorithm1.13.13.m2.1.1.2.2.2.cmml">e</mi><mo id="algorithm1.13.13.m2.1.1.2.2.1" xref="algorithm1.13.13.m2.1.1.2.2.1.cmml">¯</mo></mover><mi id="algorithm1.13.13.m2.1.1.2.3" xref="algorithm1.13.13.m2.1.1.2.3.cmml">i</mi><msup id="algorithm1.13.13.m2.1.1.3" xref="algorithm1.13.13.m2.1.1.3.cmml"><mi id="algorithm1.13.13.m2.1.1.3a" xref="algorithm1.13.13.m2.1.1.3.cmml"></mi><mo id="algorithm1.13.13.m2.1.1.3.1" xref="algorithm1.13.13.m2.1.1.3.1.cmml">′</mo></msup></msubsup><annotation-xml encoding="MathML-Content" id="algorithm1.13.13.m2.1b"><apply id="algorithm1.13.13.m2.1.1.cmml" xref="algorithm1.13.13.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.13.13.m2.1.1.1.cmml" xref="algorithm1.13.13.m2.1.1">superscript</csymbol><apply id="algorithm1.13.13.m2.1.1.2.cmml" xref="algorithm1.13.13.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.13.13.m2.1.1.2.1.cmml" xref="algorithm1.13.13.m2.1.1">subscript</csymbol><apply id="algorithm1.13.13.m2.1.1.2.2.cmml" xref="algorithm1.13.13.m2.1.1.2.2"><ci id="algorithm1.13.13.m2.1.1.2.2.1.cmml" xref="algorithm1.13.13.m2.1.1.2.2.1">¯</ci><ci id="algorithm1.13.13.m2.1.1.2.2.2.cmml" xref="algorithm1.13.13.m2.1.1.2.2.2">𝑒</ci></apply><ci id="algorithm1.13.13.m2.1.1.2.3.cmml" xref="algorithm1.13.13.m2.1.1.2.3">𝑖</ci></apply><apply id="algorithm1.13.13.m2.1.1.3.cmml" xref="algorithm1.13.13.m2.1.1.3"><ci id="algorithm1.13.13.m2.1.1.3.1.cmml" xref="algorithm1.13.13.m2.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.13.13.m2.1c">\bar{e}_{i}^{{}^{\prime}}</annotation></semantics></math>, <math id="algorithm1.14.14.m3.2" class="ltx_Math" alttext="v_{i},u" display="inline"><semantics id="algorithm1.14.14.m3.2a"><mrow id="algorithm1.14.14.m3.2.2.1" xref="algorithm1.14.14.m3.2.2.2.cmml"><msub id="algorithm1.14.14.m3.2.2.1.1" xref="algorithm1.14.14.m3.2.2.1.1.cmml"><mi id="algorithm1.14.14.m3.2.2.1.1.2" xref="algorithm1.14.14.m3.2.2.1.1.2.cmml">v</mi><mi id="algorithm1.14.14.m3.2.2.1.1.3" xref="algorithm1.14.14.m3.2.2.1.1.3.cmml">i</mi></msub><mo id="algorithm1.14.14.m3.2.2.1.2" xref="algorithm1.14.14.m3.2.2.2.cmml">,</mo><mi id="algorithm1.14.14.m3.1.1" xref="algorithm1.14.14.m3.1.1.cmml">u</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.14.14.m3.2b"><list id="algorithm1.14.14.m3.2.2.2.cmml" xref="algorithm1.14.14.m3.2.2.1"><apply id="algorithm1.14.14.m3.2.2.1.1.cmml" xref="algorithm1.14.14.m3.2.2.1.1"><csymbol cd="ambiguous" id="algorithm1.14.14.m3.2.2.1.1.1.cmml" xref="algorithm1.14.14.m3.2.2.1.1">subscript</csymbol><ci id="algorithm1.14.14.m3.2.2.1.1.2.cmml" xref="algorithm1.14.14.m3.2.2.1.1.2">𝑣</ci><ci id="algorithm1.14.14.m3.2.2.1.1.3.cmml" xref="algorithm1.14.14.m3.2.2.1.1.3">𝑖</ci></apply><ci id="algorithm1.14.14.m3.1.1.cmml" xref="algorithm1.14.14.m3.1.1">𝑢</ci></list></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.14.14.m3.2c">v_{i},u</annotation></semantics></math>]) 
</div>
<div id="algorithm1.27.29" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">9</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   

</div>
<div id="algorithm1.27.30" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">10</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    end for
</div>
<div id="algorithm1.16.16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">11</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   let <math id="algorithm1.15.15.m1.1" class="ltx_Math" alttext="V^{{}^{\prime}}" display="inline"><semantics id="algorithm1.15.15.m1.1a"><msup id="algorithm1.15.15.m1.1.1" xref="algorithm1.15.15.m1.1.1.cmml"><mi id="algorithm1.15.15.m1.1.1.2" xref="algorithm1.15.15.m1.1.1.2.cmml">V</mi><msup id="algorithm1.15.15.m1.1.1.3" xref="algorithm1.15.15.m1.1.1.3.cmml"><mi id="algorithm1.15.15.m1.1.1.3a" xref="algorithm1.15.15.m1.1.1.3.cmml"></mi><mo id="algorithm1.15.15.m1.1.1.3.1" xref="algorithm1.15.15.m1.1.1.3.1.cmml">′</mo></msup></msup><annotation-xml encoding="MathML-Content" id="algorithm1.15.15.m1.1b"><apply id="algorithm1.15.15.m1.1.1.cmml" xref="algorithm1.15.15.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.15.15.m1.1.1.1.cmml" xref="algorithm1.15.15.m1.1.1">superscript</csymbol><ci id="algorithm1.15.15.m1.1.1.2.cmml" xref="algorithm1.15.15.m1.1.1.2">𝑉</ci><apply id="algorithm1.15.15.m1.1.1.3.cmml" xref="algorithm1.15.15.m1.1.1.3"><ci id="algorithm1.15.15.m1.1.1.3.1.cmml" xref="algorithm1.15.15.m1.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.15.15.m1.1c">V^{{}^{\prime}}</annotation></semantics></math> = <math id="algorithm1.16.16.m2.1" class="ltx_Math" alttext="\{v^{{}^{\prime}}\}_{i=1:N^{v}}" display="inline"><semantics id="algorithm1.16.16.m2.1a"><msub id="algorithm1.16.16.m2.1.1" xref="algorithm1.16.16.m2.1.1.cmml"><mrow id="algorithm1.16.16.m2.1.1.1.1" xref="algorithm1.16.16.m2.1.1.1.2.cmml"><mo stretchy="false" id="algorithm1.16.16.m2.1.1.1.1.2" xref="algorithm1.16.16.m2.1.1.1.2.cmml">{</mo><msup id="algorithm1.16.16.m2.1.1.1.1.1" xref="algorithm1.16.16.m2.1.1.1.1.1.cmml"><mi id="algorithm1.16.16.m2.1.1.1.1.1.2" xref="algorithm1.16.16.m2.1.1.1.1.1.2.cmml">v</mi><msup id="algorithm1.16.16.m2.1.1.1.1.1.3" xref="algorithm1.16.16.m2.1.1.1.1.1.3.cmml"><mi id="algorithm1.16.16.m2.1.1.1.1.1.3a" xref="algorithm1.16.16.m2.1.1.1.1.1.3.cmml"></mi><mo id="algorithm1.16.16.m2.1.1.1.1.1.3.1" xref="algorithm1.16.16.m2.1.1.1.1.1.3.1.cmml">′</mo></msup></msup><mo stretchy="false" id="algorithm1.16.16.m2.1.1.1.1.3" xref="algorithm1.16.16.m2.1.1.1.2.cmml">}</mo></mrow><mrow id="algorithm1.16.16.m2.1.1.3" xref="algorithm1.16.16.m2.1.1.3.cmml"><mrow id="algorithm1.16.16.m2.1.1.3.2" xref="algorithm1.16.16.m2.1.1.3.2.cmml"><mi id="algorithm1.16.16.m2.1.1.3.2.2" xref="algorithm1.16.16.m2.1.1.3.2.2.cmml">i</mi><mo id="algorithm1.16.16.m2.1.1.3.2.1" xref="algorithm1.16.16.m2.1.1.3.2.1.cmml">=</mo><mn id="algorithm1.16.16.m2.1.1.3.2.3" xref="algorithm1.16.16.m2.1.1.3.2.3.cmml">1</mn></mrow><mo lspace="0.278em" rspace="0.278em" id="algorithm1.16.16.m2.1.1.3.1" xref="algorithm1.16.16.m2.1.1.3.1.cmml">:</mo><msup id="algorithm1.16.16.m2.1.1.3.3" xref="algorithm1.16.16.m2.1.1.3.3.cmml"><mi id="algorithm1.16.16.m2.1.1.3.3.2" xref="algorithm1.16.16.m2.1.1.3.3.2.cmml">N</mi><mi id="algorithm1.16.16.m2.1.1.3.3.3" xref="algorithm1.16.16.m2.1.1.3.3.3.cmml">v</mi></msup></mrow></msub><annotation-xml encoding="MathML-Content" id="algorithm1.16.16.m2.1b"><apply id="algorithm1.16.16.m2.1.1.cmml" xref="algorithm1.16.16.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.16.16.m2.1.1.2.cmml" xref="algorithm1.16.16.m2.1.1">subscript</csymbol><set id="algorithm1.16.16.m2.1.1.1.2.cmml" xref="algorithm1.16.16.m2.1.1.1.1"><apply id="algorithm1.16.16.m2.1.1.1.1.1.cmml" xref="algorithm1.16.16.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.16.16.m2.1.1.1.1.1.1.cmml" xref="algorithm1.16.16.m2.1.1.1.1.1">superscript</csymbol><ci id="algorithm1.16.16.m2.1.1.1.1.1.2.cmml" xref="algorithm1.16.16.m2.1.1.1.1.1.2">𝑣</ci><apply id="algorithm1.16.16.m2.1.1.1.1.1.3.cmml" xref="algorithm1.16.16.m2.1.1.1.1.1.3"><ci id="algorithm1.16.16.m2.1.1.1.1.1.3.1.cmml" xref="algorithm1.16.16.m2.1.1.1.1.1.3.1">′</ci></apply></apply></set><apply id="algorithm1.16.16.m2.1.1.3.cmml" xref="algorithm1.16.16.m2.1.1.3"><ci id="algorithm1.16.16.m2.1.1.3.1.cmml" xref="algorithm1.16.16.m2.1.1.3.1">:</ci><apply id="algorithm1.16.16.m2.1.1.3.2.cmml" xref="algorithm1.16.16.m2.1.1.3.2"><eq id="algorithm1.16.16.m2.1.1.3.2.1.cmml" xref="algorithm1.16.16.m2.1.1.3.2.1"></eq><ci id="algorithm1.16.16.m2.1.1.3.2.2.cmml" xref="algorithm1.16.16.m2.1.1.3.2.2">𝑖</ci><cn type="integer" id="algorithm1.16.16.m2.1.1.3.2.3.cmml" xref="algorithm1.16.16.m2.1.1.3.2.3">1</cn></apply><apply id="algorithm1.16.16.m2.1.1.3.3.cmml" xref="algorithm1.16.16.m2.1.1.3.3"><csymbol cd="ambiguous" id="algorithm1.16.16.m2.1.1.3.3.1.cmml" xref="algorithm1.16.16.m2.1.1.3.3">superscript</csymbol><ci id="algorithm1.16.16.m2.1.1.3.3.2.cmml" xref="algorithm1.16.16.m2.1.1.3.3.2">𝑁</ci><ci id="algorithm1.16.16.m2.1.1.3.3.3.cmml" xref="algorithm1.16.16.m2.1.1.3.3.3">𝑣</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.16.16.m2.1c">\{v^{{}^{\prime}}\}_{i=1:N^{v}}</annotation></semantics></math> 
</div>
<div id="algorithm1.18.18" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">12</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
let <math id="algorithm1.17.17.m1.1" class="ltx_Math" alttext="E^{{}^{\prime}}" display="inline"><semantics id="algorithm1.17.17.m1.1a"><msup id="algorithm1.17.17.m1.1.1" xref="algorithm1.17.17.m1.1.1.cmml"><mi id="algorithm1.17.17.m1.1.1.2" xref="algorithm1.17.17.m1.1.1.2.cmml">E</mi><msup id="algorithm1.17.17.m1.1.1.3" xref="algorithm1.17.17.m1.1.1.3.cmml"><mi id="algorithm1.17.17.m1.1.1.3a" xref="algorithm1.17.17.m1.1.1.3.cmml"></mi><mo id="algorithm1.17.17.m1.1.1.3.1" xref="algorithm1.17.17.m1.1.1.3.1.cmml">′</mo></msup></msup><annotation-xml encoding="MathML-Content" id="algorithm1.17.17.m1.1b"><apply id="algorithm1.17.17.m1.1.1.cmml" xref="algorithm1.17.17.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.17.17.m1.1.1.1.cmml" xref="algorithm1.17.17.m1.1.1">superscript</csymbol><ci id="algorithm1.17.17.m1.1.1.2.cmml" xref="algorithm1.17.17.m1.1.1.2">𝐸</ci><apply id="algorithm1.17.17.m1.1.1.3.cmml" xref="algorithm1.17.17.m1.1.1.3"><ci id="algorithm1.17.17.m1.1.1.3.1.cmml" xref="algorithm1.17.17.m1.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.17.17.m1.1c">E^{{}^{\prime}}</annotation></semantics></math> = <math id="algorithm1.18.18.m2.1" class="ltx_Math" alttext="\{(e_{k}^{{}^{\prime}},r_{k},s_{k})\}_{k=1:N^{e}}" display="inline"><semantics id="algorithm1.18.18.m2.1a"><msub id="algorithm1.18.18.m2.1.1" xref="algorithm1.18.18.m2.1.1.cmml"><mrow id="algorithm1.18.18.m2.1.1.1.1" xref="algorithm1.18.18.m2.1.1.1.2.cmml"><mo stretchy="false" id="algorithm1.18.18.m2.1.1.1.1.2" xref="algorithm1.18.18.m2.1.1.1.2.cmml">{</mo><mrow id="algorithm1.18.18.m2.1.1.1.1.1.3" xref="algorithm1.18.18.m2.1.1.1.1.1.4.cmml"><mo stretchy="false" id="algorithm1.18.18.m2.1.1.1.1.1.3.4" xref="algorithm1.18.18.m2.1.1.1.1.1.4.cmml">(</mo><msubsup id="algorithm1.18.18.m2.1.1.1.1.1.1.1" xref="algorithm1.18.18.m2.1.1.1.1.1.1.1.cmml"><mi id="algorithm1.18.18.m2.1.1.1.1.1.1.1.2.2" xref="algorithm1.18.18.m2.1.1.1.1.1.1.1.2.2.cmml">e</mi><mi id="algorithm1.18.18.m2.1.1.1.1.1.1.1.2.3" xref="algorithm1.18.18.m2.1.1.1.1.1.1.1.2.3.cmml">k</mi><msup id="algorithm1.18.18.m2.1.1.1.1.1.1.1.3" xref="algorithm1.18.18.m2.1.1.1.1.1.1.1.3.cmml"><mi id="algorithm1.18.18.m2.1.1.1.1.1.1.1.3a" xref="algorithm1.18.18.m2.1.1.1.1.1.1.1.3.cmml"></mi><mo id="algorithm1.18.18.m2.1.1.1.1.1.1.1.3.1" xref="algorithm1.18.18.m2.1.1.1.1.1.1.1.3.1.cmml">′</mo></msup></msubsup><mo id="algorithm1.18.18.m2.1.1.1.1.1.3.5" xref="algorithm1.18.18.m2.1.1.1.1.1.4.cmml">,</mo><msub id="algorithm1.18.18.m2.1.1.1.1.1.2.2" xref="algorithm1.18.18.m2.1.1.1.1.1.2.2.cmml"><mi id="algorithm1.18.18.m2.1.1.1.1.1.2.2.2" xref="algorithm1.18.18.m2.1.1.1.1.1.2.2.2.cmml">r</mi><mi id="algorithm1.18.18.m2.1.1.1.1.1.2.2.3" xref="algorithm1.18.18.m2.1.1.1.1.1.2.2.3.cmml">k</mi></msub><mo id="algorithm1.18.18.m2.1.1.1.1.1.3.6" xref="algorithm1.18.18.m2.1.1.1.1.1.4.cmml">,</mo><msub id="algorithm1.18.18.m2.1.1.1.1.1.3.3" xref="algorithm1.18.18.m2.1.1.1.1.1.3.3.cmml"><mi id="algorithm1.18.18.m2.1.1.1.1.1.3.3.2" xref="algorithm1.18.18.m2.1.1.1.1.1.3.3.2.cmml">s</mi><mi id="algorithm1.18.18.m2.1.1.1.1.1.3.3.3" xref="algorithm1.18.18.m2.1.1.1.1.1.3.3.3.cmml">k</mi></msub><mo stretchy="false" id="algorithm1.18.18.m2.1.1.1.1.1.3.7" xref="algorithm1.18.18.m2.1.1.1.1.1.4.cmml">)</mo></mrow><mo stretchy="false" id="algorithm1.18.18.m2.1.1.1.1.3" xref="algorithm1.18.18.m2.1.1.1.2.cmml">}</mo></mrow><mrow id="algorithm1.18.18.m2.1.1.3" xref="algorithm1.18.18.m2.1.1.3.cmml"><mrow id="algorithm1.18.18.m2.1.1.3.2" xref="algorithm1.18.18.m2.1.1.3.2.cmml"><mi id="algorithm1.18.18.m2.1.1.3.2.2" xref="algorithm1.18.18.m2.1.1.3.2.2.cmml">k</mi><mo id="algorithm1.18.18.m2.1.1.3.2.1" xref="algorithm1.18.18.m2.1.1.3.2.1.cmml">=</mo><mn id="algorithm1.18.18.m2.1.1.3.2.3" xref="algorithm1.18.18.m2.1.1.3.2.3.cmml">1</mn></mrow><mo lspace="0.278em" rspace="0.278em" id="algorithm1.18.18.m2.1.1.3.1" xref="algorithm1.18.18.m2.1.1.3.1.cmml">:</mo><msup id="algorithm1.18.18.m2.1.1.3.3" xref="algorithm1.18.18.m2.1.1.3.3.cmml"><mi id="algorithm1.18.18.m2.1.1.3.3.2" xref="algorithm1.18.18.m2.1.1.3.3.2.cmml">N</mi><mi id="algorithm1.18.18.m2.1.1.3.3.3" xref="algorithm1.18.18.m2.1.1.3.3.3.cmml">e</mi></msup></mrow></msub><annotation-xml encoding="MathML-Content" id="algorithm1.18.18.m2.1b"><apply id="algorithm1.18.18.m2.1.1.cmml" xref="algorithm1.18.18.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.18.18.m2.1.1.2.cmml" xref="algorithm1.18.18.m2.1.1">subscript</csymbol><set id="algorithm1.18.18.m2.1.1.1.2.cmml" xref="algorithm1.18.18.m2.1.1.1.1"><vector id="algorithm1.18.18.m2.1.1.1.1.1.4.cmml" xref="algorithm1.18.18.m2.1.1.1.1.1.3"><apply id="algorithm1.18.18.m2.1.1.1.1.1.1.1.cmml" xref="algorithm1.18.18.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.18.18.m2.1.1.1.1.1.1.1.1.cmml" xref="algorithm1.18.18.m2.1.1.1.1.1.1.1">superscript</csymbol><apply id="algorithm1.18.18.m2.1.1.1.1.1.1.1.2.cmml" xref="algorithm1.18.18.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.18.18.m2.1.1.1.1.1.1.1.2.1.cmml" xref="algorithm1.18.18.m2.1.1.1.1.1.1.1">subscript</csymbol><ci id="algorithm1.18.18.m2.1.1.1.1.1.1.1.2.2.cmml" xref="algorithm1.18.18.m2.1.1.1.1.1.1.1.2.2">𝑒</ci><ci id="algorithm1.18.18.m2.1.1.1.1.1.1.1.2.3.cmml" xref="algorithm1.18.18.m2.1.1.1.1.1.1.1.2.3">𝑘</ci></apply><apply id="algorithm1.18.18.m2.1.1.1.1.1.1.1.3.cmml" xref="algorithm1.18.18.m2.1.1.1.1.1.1.1.3"><ci id="algorithm1.18.18.m2.1.1.1.1.1.1.1.3.1.cmml" xref="algorithm1.18.18.m2.1.1.1.1.1.1.1.3.1">′</ci></apply></apply><apply id="algorithm1.18.18.m2.1.1.1.1.1.2.2.cmml" xref="algorithm1.18.18.m2.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="algorithm1.18.18.m2.1.1.1.1.1.2.2.1.cmml" xref="algorithm1.18.18.m2.1.1.1.1.1.2.2">subscript</csymbol><ci id="algorithm1.18.18.m2.1.1.1.1.1.2.2.2.cmml" xref="algorithm1.18.18.m2.1.1.1.1.1.2.2.2">𝑟</ci><ci id="algorithm1.18.18.m2.1.1.1.1.1.2.2.3.cmml" xref="algorithm1.18.18.m2.1.1.1.1.1.2.2.3">𝑘</ci></apply><apply id="algorithm1.18.18.m2.1.1.1.1.1.3.3.cmml" xref="algorithm1.18.18.m2.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="algorithm1.18.18.m2.1.1.1.1.1.3.3.1.cmml" xref="algorithm1.18.18.m2.1.1.1.1.1.3.3">subscript</csymbol><ci id="algorithm1.18.18.m2.1.1.1.1.1.3.3.2.cmml" xref="algorithm1.18.18.m2.1.1.1.1.1.3.3.2">𝑠</ci><ci id="algorithm1.18.18.m2.1.1.1.1.1.3.3.3.cmml" xref="algorithm1.18.18.m2.1.1.1.1.1.3.3.3">𝑘</ci></apply></vector></set><apply id="algorithm1.18.18.m2.1.1.3.cmml" xref="algorithm1.18.18.m2.1.1.3"><ci id="algorithm1.18.18.m2.1.1.3.1.cmml" xref="algorithm1.18.18.m2.1.1.3.1">:</ci><apply id="algorithm1.18.18.m2.1.1.3.2.cmml" xref="algorithm1.18.18.m2.1.1.3.2"><eq id="algorithm1.18.18.m2.1.1.3.2.1.cmml" xref="algorithm1.18.18.m2.1.1.3.2.1"></eq><ci id="algorithm1.18.18.m2.1.1.3.2.2.cmml" xref="algorithm1.18.18.m2.1.1.3.2.2">𝑘</ci><cn type="integer" id="algorithm1.18.18.m2.1.1.3.2.3.cmml" xref="algorithm1.18.18.m2.1.1.3.2.3">1</cn></apply><apply id="algorithm1.18.18.m2.1.1.3.3.cmml" xref="algorithm1.18.18.m2.1.1.3.3"><csymbol cd="ambiguous" id="algorithm1.18.18.m2.1.1.3.3.1.cmml" xref="algorithm1.18.18.m2.1.1.3.3">superscript</csymbol><ci id="algorithm1.18.18.m2.1.1.3.3.2.cmml" xref="algorithm1.18.18.m2.1.1.3.3.2">𝑁</ci><ci id="algorithm1.18.18.m2.1.1.3.3.3.cmml" xref="algorithm1.18.18.m2.1.1.3.3.3">𝑒</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.18.18.m2.1c">\{(e_{k}^{{}^{\prime}},r_{k},s_{k})\}_{k=1:N^{e}}</annotation></semantics></math> 
</div>
<div id="algorithm1.20.20" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">13</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math id="algorithm1.19.19.m1.1" class="ltx_Math" alttext="\bar{e}^{{}^{\prime}}" display="inline"><semantics id="algorithm1.19.19.m1.1a"><msup id="algorithm1.19.19.m1.1.1" xref="algorithm1.19.19.m1.1.1.cmml"><mover accent="true" id="algorithm1.19.19.m1.1.1.2" xref="algorithm1.19.19.m1.1.1.2.cmml"><mi id="algorithm1.19.19.m1.1.1.2.2" xref="algorithm1.19.19.m1.1.1.2.2.cmml">e</mi><mo id="algorithm1.19.19.m1.1.1.2.1" xref="algorithm1.19.19.m1.1.1.2.1.cmml">¯</mo></mover><msup id="algorithm1.19.19.m1.1.1.3" xref="algorithm1.19.19.m1.1.1.3.cmml"><mi id="algorithm1.19.19.m1.1.1.3a" xref="algorithm1.19.19.m1.1.1.3.cmml"></mi><mo id="algorithm1.19.19.m1.1.1.3.1" xref="algorithm1.19.19.m1.1.1.3.1.cmml">′</mo></msup></msup><annotation-xml encoding="MathML-Content" id="algorithm1.19.19.m1.1b"><apply id="algorithm1.19.19.m1.1.1.cmml" xref="algorithm1.19.19.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.19.19.m1.1.1.1.cmml" xref="algorithm1.19.19.m1.1.1">superscript</csymbol><apply id="algorithm1.19.19.m1.1.1.2.cmml" xref="algorithm1.19.19.m1.1.1.2"><ci id="algorithm1.19.19.m1.1.1.2.1.cmml" xref="algorithm1.19.19.m1.1.1.2.1">¯</ci><ci id="algorithm1.19.19.m1.1.1.2.2.cmml" xref="algorithm1.19.19.m1.1.1.2.2">𝑒</ci></apply><apply id="algorithm1.19.19.m1.1.1.3.cmml" xref="algorithm1.19.19.m1.1.1.3"><ci id="algorithm1.19.19.m1.1.1.3.1.cmml" xref="algorithm1.19.19.m1.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.19.19.m1.1c">\bar{e}^{{}^{\prime}}</annotation></semantics></math> = <span id="algorithm1.20.20.1" class="ltx_text ltx_font_smallcaps">mean</span>(<math id="algorithm1.20.20.m2.1" class="ltx_Math" alttext="E^{{}^{\prime}}" display="inline"><semantics id="algorithm1.20.20.m2.1a"><msup id="algorithm1.20.20.m2.1.1" xref="algorithm1.20.20.m2.1.1.cmml"><mi id="algorithm1.20.20.m2.1.1.2" xref="algorithm1.20.20.m2.1.1.2.cmml">E</mi><msup id="algorithm1.20.20.m2.1.1.3" xref="algorithm1.20.20.m2.1.1.3.cmml"><mi id="algorithm1.20.20.m2.1.1.3a" xref="algorithm1.20.20.m2.1.1.3.cmml"></mi><mo id="algorithm1.20.20.m2.1.1.3.1" xref="algorithm1.20.20.m2.1.1.3.1.cmml">′</mo></msup></msup><annotation-xml encoding="MathML-Content" id="algorithm1.20.20.m2.1b"><apply id="algorithm1.20.20.m2.1.1.cmml" xref="algorithm1.20.20.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.20.20.m2.1.1.1.cmml" xref="algorithm1.20.20.m2.1.1">superscript</csymbol><ci id="algorithm1.20.20.m2.1.1.2.cmml" xref="algorithm1.20.20.m2.1.1.2">𝐸</ci><apply id="algorithm1.20.20.m2.1.1.3.cmml" xref="algorithm1.20.20.m2.1.1.3"><ci id="algorithm1.20.20.m2.1.1.3.1.cmml" xref="algorithm1.20.20.m2.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.20.20.m2.1c">E^{{}^{\prime}}</annotation></semantics></math>) 
</div>
<div id="algorithm1.22.22" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">14</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math id="algorithm1.21.21.m1.1" class="ltx_Math" alttext="\bar{v}^{{}^{\prime}}" display="inline"><semantics id="algorithm1.21.21.m1.1a"><msup id="algorithm1.21.21.m1.1.1" xref="algorithm1.21.21.m1.1.1.cmml"><mover accent="true" id="algorithm1.21.21.m1.1.1.2" xref="algorithm1.21.21.m1.1.1.2.cmml"><mi id="algorithm1.21.21.m1.1.1.2.2" xref="algorithm1.21.21.m1.1.1.2.2.cmml">v</mi><mo id="algorithm1.21.21.m1.1.1.2.1" xref="algorithm1.21.21.m1.1.1.2.1.cmml">¯</mo></mover><msup id="algorithm1.21.21.m1.1.1.3" xref="algorithm1.21.21.m1.1.1.3.cmml"><mi id="algorithm1.21.21.m1.1.1.3a" xref="algorithm1.21.21.m1.1.1.3.cmml"></mi><mo id="algorithm1.21.21.m1.1.1.3.1" xref="algorithm1.21.21.m1.1.1.3.1.cmml">′</mo></msup></msup><annotation-xml encoding="MathML-Content" id="algorithm1.21.21.m1.1b"><apply id="algorithm1.21.21.m1.1.1.cmml" xref="algorithm1.21.21.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.21.21.m1.1.1.1.cmml" xref="algorithm1.21.21.m1.1.1">superscript</csymbol><apply id="algorithm1.21.21.m1.1.1.2.cmml" xref="algorithm1.21.21.m1.1.1.2"><ci id="algorithm1.21.21.m1.1.1.2.1.cmml" xref="algorithm1.21.21.m1.1.1.2.1">¯</ci><ci id="algorithm1.21.21.m1.1.1.2.2.cmml" xref="algorithm1.21.21.m1.1.1.2.2">𝑣</ci></apply><apply id="algorithm1.21.21.m1.1.1.3.cmml" xref="algorithm1.21.21.m1.1.1.3"><ci id="algorithm1.21.21.m1.1.1.3.1.cmml" xref="algorithm1.21.21.m1.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.21.21.m1.1c">\bar{v}^{{}^{\prime}}</annotation></semantics></math> = <span id="algorithm1.22.22.1" class="ltx_text ltx_font_smallcaps">mean</span>(<math id="algorithm1.22.22.m2.1" class="ltx_Math" alttext="V^{{}^{\prime}}" display="inline"><semantics id="algorithm1.22.22.m2.1a"><msup id="algorithm1.22.22.m2.1.1" xref="algorithm1.22.22.m2.1.1.cmml"><mi id="algorithm1.22.22.m2.1.1.2" xref="algorithm1.22.22.m2.1.1.2.cmml">V</mi><msup id="algorithm1.22.22.m2.1.1.3" xref="algorithm1.22.22.m2.1.1.3.cmml"><mi id="algorithm1.22.22.m2.1.1.3a" xref="algorithm1.22.22.m2.1.1.3.cmml"></mi><mo id="algorithm1.22.22.m2.1.1.3.1" xref="algorithm1.22.22.m2.1.1.3.1.cmml">′</mo></msup></msup><annotation-xml encoding="MathML-Content" id="algorithm1.22.22.m2.1b"><apply id="algorithm1.22.22.m2.1.1.cmml" xref="algorithm1.22.22.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.22.22.m2.1.1.1.cmml" xref="algorithm1.22.22.m2.1.1">superscript</csymbol><ci id="algorithm1.22.22.m2.1.1.2.cmml" xref="algorithm1.22.22.m2.1.1.2">𝑉</ci><apply id="algorithm1.22.22.m2.1.1.3.cmml" xref="algorithm1.22.22.m2.1.1.3"><ci id="algorithm1.22.22.m2.1.1.3.1.cmml" xref="algorithm1.22.22.m2.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.22.22.m2.1c">V^{{}^{\prime}}</annotation></semantics></math>) 
</div>
<div id="algorithm1.24.24" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">15</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math id="algorithm1.23.23.m1.1" class="ltx_Math" alttext="u^{{}^{\prime}}" display="inline"><semantics id="algorithm1.23.23.m1.1a"><msup id="algorithm1.23.23.m1.1.1" xref="algorithm1.23.23.m1.1.1.cmml"><mi id="algorithm1.23.23.m1.1.1.2" xref="algorithm1.23.23.m1.1.1.2.cmml">u</mi><msup id="algorithm1.23.23.m1.1.1.3" xref="algorithm1.23.23.m1.1.1.3.cmml"><mi id="algorithm1.23.23.m1.1.1.3a" xref="algorithm1.23.23.m1.1.1.3.cmml"></mi><mo id="algorithm1.23.23.m1.1.1.3.1" xref="algorithm1.23.23.m1.1.1.3.1.cmml">′</mo></msup></msup><annotation-xml encoding="MathML-Content" id="algorithm1.23.23.m1.1b"><apply id="algorithm1.23.23.m1.1.1.cmml" xref="algorithm1.23.23.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.23.23.m1.1.1.1.cmml" xref="algorithm1.23.23.m1.1.1">superscript</csymbol><ci id="algorithm1.23.23.m1.1.1.2.cmml" xref="algorithm1.23.23.m1.1.1.2">𝑢</ci><apply id="algorithm1.23.23.m1.1.1.3.cmml" xref="algorithm1.23.23.m1.1.1.3"><ci id="algorithm1.23.23.m1.1.1.3.1.cmml" xref="algorithm1.23.23.m1.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.23.23.m1.1c">u^{{}^{\prime}}</annotation></semantics></math> = <span id="algorithm1.24.24.1" class="ltx_text ltx_font_smallcaps">mlp</span>([<math id="algorithm1.24.24.m2.3" class="ltx_Math" alttext="\bar{e}^{{}^{\prime}},\bar{v}^{{}^{\prime}},u" display="inline"><semantics id="algorithm1.24.24.m2.3a"><mrow id="algorithm1.24.24.m2.3.3.2" xref="algorithm1.24.24.m2.3.3.3.cmml"><msup id="algorithm1.24.24.m2.2.2.1.1" xref="algorithm1.24.24.m2.2.2.1.1.cmml"><mover accent="true" id="algorithm1.24.24.m2.2.2.1.1.2" xref="algorithm1.24.24.m2.2.2.1.1.2.cmml"><mi id="algorithm1.24.24.m2.2.2.1.1.2.2" xref="algorithm1.24.24.m2.2.2.1.1.2.2.cmml">e</mi><mo id="algorithm1.24.24.m2.2.2.1.1.2.1" xref="algorithm1.24.24.m2.2.2.1.1.2.1.cmml">¯</mo></mover><msup id="algorithm1.24.24.m2.2.2.1.1.3" xref="algorithm1.24.24.m2.2.2.1.1.3.cmml"><mi id="algorithm1.24.24.m2.2.2.1.1.3a" xref="algorithm1.24.24.m2.2.2.1.1.3.cmml"></mi><mo id="algorithm1.24.24.m2.2.2.1.1.3.1" xref="algorithm1.24.24.m2.2.2.1.1.3.1.cmml">′</mo></msup></msup><mo id="algorithm1.24.24.m2.3.3.2.3" xref="algorithm1.24.24.m2.3.3.3.cmml">,</mo><msup id="algorithm1.24.24.m2.3.3.2.2" xref="algorithm1.24.24.m2.3.3.2.2.cmml"><mover accent="true" id="algorithm1.24.24.m2.3.3.2.2.2" xref="algorithm1.24.24.m2.3.3.2.2.2.cmml"><mi id="algorithm1.24.24.m2.3.3.2.2.2.2" xref="algorithm1.24.24.m2.3.3.2.2.2.2.cmml">v</mi><mo id="algorithm1.24.24.m2.3.3.2.2.2.1" xref="algorithm1.24.24.m2.3.3.2.2.2.1.cmml">¯</mo></mover><msup id="algorithm1.24.24.m2.3.3.2.2.3" xref="algorithm1.24.24.m2.3.3.2.2.3.cmml"><mi id="algorithm1.24.24.m2.3.3.2.2.3a" xref="algorithm1.24.24.m2.3.3.2.2.3.cmml"></mi><mo id="algorithm1.24.24.m2.3.3.2.2.3.1" xref="algorithm1.24.24.m2.3.3.2.2.3.1.cmml">′</mo></msup></msup><mo id="algorithm1.24.24.m2.3.3.2.4" xref="algorithm1.24.24.m2.3.3.3.cmml">,</mo><mi id="algorithm1.24.24.m2.1.1" xref="algorithm1.24.24.m2.1.1.cmml">u</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.24.24.m2.3b"><list id="algorithm1.24.24.m2.3.3.3.cmml" xref="algorithm1.24.24.m2.3.3.2"><apply id="algorithm1.24.24.m2.2.2.1.1.cmml" xref="algorithm1.24.24.m2.2.2.1.1"><csymbol cd="ambiguous" id="algorithm1.24.24.m2.2.2.1.1.1.cmml" xref="algorithm1.24.24.m2.2.2.1.1">superscript</csymbol><apply id="algorithm1.24.24.m2.2.2.1.1.2.cmml" xref="algorithm1.24.24.m2.2.2.1.1.2"><ci id="algorithm1.24.24.m2.2.2.1.1.2.1.cmml" xref="algorithm1.24.24.m2.2.2.1.1.2.1">¯</ci><ci id="algorithm1.24.24.m2.2.2.1.1.2.2.cmml" xref="algorithm1.24.24.m2.2.2.1.1.2.2">𝑒</ci></apply><apply id="algorithm1.24.24.m2.2.2.1.1.3.cmml" xref="algorithm1.24.24.m2.2.2.1.1.3"><ci id="algorithm1.24.24.m2.2.2.1.1.3.1.cmml" xref="algorithm1.24.24.m2.2.2.1.1.3.1">′</ci></apply></apply><apply id="algorithm1.24.24.m2.3.3.2.2.cmml" xref="algorithm1.24.24.m2.3.3.2.2"><csymbol cd="ambiguous" id="algorithm1.24.24.m2.3.3.2.2.1.cmml" xref="algorithm1.24.24.m2.3.3.2.2">superscript</csymbol><apply id="algorithm1.24.24.m2.3.3.2.2.2.cmml" xref="algorithm1.24.24.m2.3.3.2.2.2"><ci id="algorithm1.24.24.m2.3.3.2.2.2.1.cmml" xref="algorithm1.24.24.m2.3.3.2.2.2.1">¯</ci><ci id="algorithm1.24.24.m2.3.3.2.2.2.2.cmml" xref="algorithm1.24.24.m2.3.3.2.2.2.2">𝑣</ci></apply><apply id="algorithm1.24.24.m2.3.3.2.2.3.cmml" xref="algorithm1.24.24.m2.3.3.2.2.3"><ci id="algorithm1.24.24.m2.3.3.2.2.3.1.cmml" xref="algorithm1.24.24.m2.3.3.2.2.3.1">′</ci></apply></apply><ci id="algorithm1.24.24.m2.1.1.cmml" xref="algorithm1.24.24.m2.1.1">𝑢</ci></list></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.24.24.m2.3c">\bar{e}^{{}^{\prime}},\bar{v}^{{}^{\prime}},u</annotation></semantics></math>]) 
</div>
<div id="algorithm1.26.26" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">16</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math id="algorithm1.25.25.m1.3" class="ltx_Math" alttext="(E,V,u)" display="inline"><semantics id="algorithm1.25.25.m1.3a"><mrow id="algorithm1.25.25.m1.3.4.2" xref="algorithm1.25.25.m1.3.4.1.cmml"><mo stretchy="false" id="algorithm1.25.25.m1.3.4.2.1" xref="algorithm1.25.25.m1.3.4.1.cmml">(</mo><mi id="algorithm1.25.25.m1.1.1" xref="algorithm1.25.25.m1.1.1.cmml">E</mi><mo id="algorithm1.25.25.m1.3.4.2.2" xref="algorithm1.25.25.m1.3.4.1.cmml">,</mo><mi id="algorithm1.25.25.m1.2.2" xref="algorithm1.25.25.m1.2.2.cmml">V</mi><mo id="algorithm1.25.25.m1.3.4.2.3" xref="algorithm1.25.25.m1.3.4.1.cmml">,</mo><mi id="algorithm1.25.25.m1.3.3" xref="algorithm1.25.25.m1.3.3.cmml">u</mi><mo stretchy="false" id="algorithm1.25.25.m1.3.4.2.4" xref="algorithm1.25.25.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.25.25.m1.3b"><vector id="algorithm1.25.25.m1.3.4.1.cmml" xref="algorithm1.25.25.m1.3.4.2"><ci id="algorithm1.25.25.m1.1.1.cmml" xref="algorithm1.25.25.m1.1.1">𝐸</ci><ci id="algorithm1.25.25.m1.2.2.cmml" xref="algorithm1.25.25.m1.2.2">𝑉</ci><ci id="algorithm1.25.25.m1.3.3.cmml" xref="algorithm1.25.25.m1.3.3">𝑢</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.25.25.m1.3c">(E,V,u)</annotation></semantics></math> = <math id="algorithm1.26.26.m2.3" class="ltx_Math" alttext="(E^{{}^{\prime}},V^{{}^{\prime}},u^{{}^{\prime}})" display="inline"><semantics id="algorithm1.26.26.m2.3a"><mrow id="algorithm1.26.26.m2.3.3.3" xref="algorithm1.26.26.m2.3.3.4.cmml"><mo stretchy="false" id="algorithm1.26.26.m2.3.3.3.4" xref="algorithm1.26.26.m2.3.3.4.cmml">(</mo><msup id="algorithm1.26.26.m2.1.1.1.1" xref="algorithm1.26.26.m2.1.1.1.1.cmml"><mi id="algorithm1.26.26.m2.1.1.1.1.2" xref="algorithm1.26.26.m2.1.1.1.1.2.cmml">E</mi><msup id="algorithm1.26.26.m2.1.1.1.1.3" xref="algorithm1.26.26.m2.1.1.1.1.3.cmml"><mi id="algorithm1.26.26.m2.1.1.1.1.3a" xref="algorithm1.26.26.m2.1.1.1.1.3.cmml"></mi><mo id="algorithm1.26.26.m2.1.1.1.1.3.1" xref="algorithm1.26.26.m2.1.1.1.1.3.1.cmml">′</mo></msup></msup><mo id="algorithm1.26.26.m2.3.3.3.5" xref="algorithm1.26.26.m2.3.3.4.cmml">,</mo><msup id="algorithm1.26.26.m2.2.2.2.2" xref="algorithm1.26.26.m2.2.2.2.2.cmml"><mi id="algorithm1.26.26.m2.2.2.2.2.2" xref="algorithm1.26.26.m2.2.2.2.2.2.cmml">V</mi><msup id="algorithm1.26.26.m2.2.2.2.2.3" xref="algorithm1.26.26.m2.2.2.2.2.3.cmml"><mi id="algorithm1.26.26.m2.2.2.2.2.3a" xref="algorithm1.26.26.m2.2.2.2.2.3.cmml"></mi><mo id="algorithm1.26.26.m2.2.2.2.2.3.1" xref="algorithm1.26.26.m2.2.2.2.2.3.1.cmml">′</mo></msup></msup><mo id="algorithm1.26.26.m2.3.3.3.6" xref="algorithm1.26.26.m2.3.3.4.cmml">,</mo><msup id="algorithm1.26.26.m2.3.3.3.3" xref="algorithm1.26.26.m2.3.3.3.3.cmml"><mi id="algorithm1.26.26.m2.3.3.3.3.2" xref="algorithm1.26.26.m2.3.3.3.3.2.cmml">u</mi><msup id="algorithm1.26.26.m2.3.3.3.3.3" xref="algorithm1.26.26.m2.3.3.3.3.3.cmml"><mi id="algorithm1.26.26.m2.3.3.3.3.3a" xref="algorithm1.26.26.m2.3.3.3.3.3.cmml"></mi><mo id="algorithm1.26.26.m2.3.3.3.3.3.1" xref="algorithm1.26.26.m2.3.3.3.3.3.1.cmml">′</mo></msup></msup><mo stretchy="false" id="algorithm1.26.26.m2.3.3.3.7" xref="algorithm1.26.26.m2.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.26.26.m2.3b"><vector id="algorithm1.26.26.m2.3.3.4.cmml" xref="algorithm1.26.26.m2.3.3.3"><apply id="algorithm1.26.26.m2.1.1.1.1.cmml" xref="algorithm1.26.26.m2.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.26.26.m2.1.1.1.1.1.cmml" xref="algorithm1.26.26.m2.1.1.1.1">superscript</csymbol><ci id="algorithm1.26.26.m2.1.1.1.1.2.cmml" xref="algorithm1.26.26.m2.1.1.1.1.2">𝐸</ci><apply id="algorithm1.26.26.m2.1.1.1.1.3.cmml" xref="algorithm1.26.26.m2.1.1.1.1.3"><ci id="algorithm1.26.26.m2.1.1.1.1.3.1.cmml" xref="algorithm1.26.26.m2.1.1.1.1.3.1">′</ci></apply></apply><apply id="algorithm1.26.26.m2.2.2.2.2.cmml" xref="algorithm1.26.26.m2.2.2.2.2"><csymbol cd="ambiguous" id="algorithm1.26.26.m2.2.2.2.2.1.cmml" xref="algorithm1.26.26.m2.2.2.2.2">superscript</csymbol><ci id="algorithm1.26.26.m2.2.2.2.2.2.cmml" xref="algorithm1.26.26.m2.2.2.2.2.2">𝑉</ci><apply id="algorithm1.26.26.m2.2.2.2.2.3.cmml" xref="algorithm1.26.26.m2.2.2.2.2.3"><ci id="algorithm1.26.26.m2.2.2.2.2.3.1.cmml" xref="algorithm1.26.26.m2.2.2.2.2.3.1">′</ci></apply></apply><apply id="algorithm1.26.26.m2.3.3.3.3.cmml" xref="algorithm1.26.26.m2.3.3.3.3"><csymbol cd="ambiguous" id="algorithm1.26.26.m2.3.3.3.3.1.cmml" xref="algorithm1.26.26.m2.3.3.3.3">superscript</csymbol><ci id="algorithm1.26.26.m2.3.3.3.3.2.cmml" xref="algorithm1.26.26.m2.3.3.3.3.2">𝑢</ci><apply id="algorithm1.26.26.m2.3.3.3.3.3.cmml" xref="algorithm1.26.26.m2.3.3.3.3.3"><ci id="algorithm1.26.26.m2.3.3.3.3.3.1.cmml" xref="algorithm1.26.26.m2.3.3.3.3.3.1">′</ci></apply></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.26.26.m2.3c">(E^{{}^{\prime}},V^{{}^{\prime}},u^{{}^{\prime}})</annotation></semantics></math>

</div>
<div id="algorithm1.27.31" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">17</span> end for
</div>
<div id="algorithm1.27.27" class="ltx_listingline">
<span id="algorithm1.27.27.2" class="ltx_text ltx_font_bold">return</span> <em id="algorithm1.27.27.1" class="ltx_emph ltx_font_italic"><math id="algorithm1.27.27.1.m1.3" class="ltx_Math" alttext="(E,V,U)" display="inline"><semantics id="algorithm1.27.27.1.m1.3a"><mrow id="algorithm1.27.27.1.m1.3.4.2" xref="algorithm1.27.27.1.m1.3.4.1.cmml"><mo stretchy="false" id="algorithm1.27.27.1.m1.3.4.2.1" xref="algorithm1.27.27.1.m1.3.4.1.cmml">(</mo><mi id="algorithm1.27.27.1.m1.1.1" xref="algorithm1.27.27.1.m1.1.1.cmml">E</mi><mo id="algorithm1.27.27.1.m1.3.4.2.2" xref="algorithm1.27.27.1.m1.3.4.1.cmml">,</mo><mi id="algorithm1.27.27.1.m1.2.2" xref="algorithm1.27.27.1.m1.2.2.cmml">V</mi><mo id="algorithm1.27.27.1.m1.3.4.2.3" xref="algorithm1.27.27.1.m1.3.4.1.cmml">,</mo><mi id="algorithm1.27.27.1.m1.3.3" xref="algorithm1.27.27.1.m1.3.3.cmml">U</mi><mo stretchy="false" id="algorithm1.27.27.1.m1.3.4.2.4" xref="algorithm1.27.27.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.27.27.1.m1.3b"><vector id="algorithm1.27.27.1.m1.3.4.1.cmml" xref="algorithm1.27.27.1.m1.3.4.2"><ci id="algorithm1.27.27.1.m1.1.1.cmml" xref="algorithm1.27.27.1.m1.1.1">𝐸</ci><ci id="algorithm1.27.27.1.m1.2.2.cmml" xref="algorithm1.27.27.1.m1.2.2">𝑉</ci><ci id="algorithm1.27.27.1.m1.3.3.cmml" xref="algorithm1.27.27.1.m1.3.3">𝑈</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.27.27.1.m1.3c">(E,V,U)</annotation></semantics></math></em>


</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="algorithm1.30.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span>Graph Network (<span id="algorithm1.31.2" class="ltx_text ltx_font_smallcaps">GN</span>)</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Question Answering with Scene Graphs</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.2" class="ltx_p">As described in Section <a href="#S4.SS3" title="4.3 Scene Graph Encoding ‣ 4 Proposed Approach ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>, the <span id="S4.SS4.p1.2.1" class="ltx_text ltx_font_smallcaps">gn</span> produces updated representations of the node, edge and global vectors. To convert these to a similar format as the object features <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="O" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mi id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><ci id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">O</annotation></semantics></math>, we stack the node and global vectors to get the scene graph encoding, <span id="S4.SS4.p1.2.2" class="ltx_text ltx_font_smallcaps">sge</span>. The <span id="S4.SS4.p1.2.3" class="ltx_text ltx_font_smallcaps">attn</span> baseline (Section <a href="#S4.SS1" title="4.1 Baselines ‣ 4 Proposed Approach ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>) takes as input, <math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="O" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><mi id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><ci id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">O</annotation></semantics></math>. We replace this with <span id="S4.SS4.p1.2.4" class="ltx_text ltx_font_smallcaps">sge</span> to get a question answering model <span id="S4.SS4.p1.2.5" class="ltx_text ltx_font_smallcaps">(sgatt)</span> that uses scene graphs. Similarly, we pass <span id="S4.SS4.p1.2.6" class="ltx_text ltx_font_smallcaps">sge</span> to <span id="S4.SS4.p1.2.7" class="ltx_text ltx_font_smallcaps">mac</span>, replacing the image input, to get <span id="S4.SS4.p1.2.8" class="ltx_text ltx_font_smallcaps">sgmac</span>. We jointly train the <span id="S4.SS4.p1.2.9" class="ltx_text ltx_font_smallcaps">gn</span> and <span id="S4.SS4.p1.2.10" class="ltx_text ltx_font_smallcaps">mac</span> from scratch, and use <span id="S4.SS4.p1.2.11" class="ltx_text ltx_font_smallcaps">gn</span> weights thus obtained, to train <span id="S4.SS4.p1.2.12" class="ltx_text ltx_font_smallcaps">sgatt</span>.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Scene Graph Generation</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">The problem of generating a scene graph from an image is typically split into 2 phases. The first phase consists of training an object detector such as Faster-RCNN. The second phase uses the features extracted from this model to train a relation detector model responsible for identifying potential edge pairs between a set of objects and predicting the relationship between them. A popular work in the field of scene graph classification is the Neural Motifs <cite class="ltx_cite ltx_citemacro_citep">(Zellers et al., <a href="#bib.bib25" title="" class="ltx_ref">2018</a>)</cite> model, which uses a stacked biLSTM structure to predict edge pairs and relations.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">In this work, we rely on a Faster R-CNN object feature extractor with a ResNet-50 backbone, paired with a Neural Motif model trained in 2 different variants (Section <a href="#S6.SS5.SSS1" title="6.5.1 Impact of Label Loss Smoothing ‣ 6.5 Effect of scene graph quality ‣ 6 Results and Analysis ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.5.1</span></a>), to study the stability and variance associated with generating scene graphs.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Training Curriculum</h3>

<figure id="S4.F2" class="ltx_figure"><img src="/html/2101.05479/assets/figs/noisy_and_gt.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="437" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>(Left) Ground truth scene graph and (Right) Generated noisy scene graph</figcaption>
</figure>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2101.05479/assets/figs/noisy_gt_swap.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="214" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Scene graph generated after injecting noise</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2101.05479/assets/figs/corrupt_gt.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="214" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Scene graph generated after corrupting ground truth</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Noisy/Corrupted Scene graph representations</figcaption>
</figure>
<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">Since test images are evaluated on the noisy scene graphs, we propose a novel training curriculum to ensure the Graph Network representation is able to capture maximum information from the generated scene graphs.
A Graph Network trained on only ground truth scene graphs encodes scene graphs for test images poorly since there is a large difference in the representations of ground truth and generated scene graphs due to differences in quality.
</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.1" class="ltx_p">In order to train a model to have strong low-level features, we first train the Graph Network on ground truth scene graphs for 2 epochs and slowly introduce features of the noisy scene graphs into the ground truth scene graphs by randomly swapping out nodes, edge pairs and relations between the two (ground truth and noisy) graphs. The degree with which noise is introduced is controlled by 2 parameters: the <em id="S4.SS6.p2.1.1" class="ltx_emph ltx_font_italic">probability</em> with which noise is introduced into the scene graph and the <em id="S4.SS6.p2.1.2" class="ltx_emph ltx_font_italic">proportion</em> of the components for which we switch out with noisy counterparts. For example, if probability is 0.5 and proportion is 0.8 and we have 10 edges, then there is a 50% chance that we switch out 8 ground truth edges with noisy edges. This injection of noise is slowly introduced while training the model with an increasing probability and proportion (of components) with each passing epoch.</p>
</div>
<div id="S4.SS6.p3" class="ltx_para">
<p id="S4.SS6.p3.1" class="ltx_p">Maintaining a steady probabilistic approach of injecting noise into the ground truth scene graphs allows the model to have superior lower-level feature extraction as well as high-level semantic extraction in the upper layer more akin to the noisy graph features. Figures <a href="#S4.F2" title="Figure 2 ‣ 4.6 Training Curriculum ‣ 4 Proposed Approach ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S4.F3.sf1" title="Figure 3(a) ‣ Figure 3 ‣ 4.6 Training Curriculum ‣ 4 Proposed Approach ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a> illustrate how a swap happens between components of the two scene graphs. We also experiment with switching out the entire graph (replace ground truth with noisy) at random during training, and with models that are trained on a mixed dataset.</p>
</div>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>UNITER with Scene Graphs</h3>

<div id="S4.SS7.p1" class="ltx_para">
<p id="S4.SS7.p1.1" class="ltx_p">Given the success of pre-trained models across a wide range of tasks, we seek to use scene graphs with <span id="S4.SS7.p1.1.1" class="ltx_text ltx_font_smallcaps">uniter</span>. While the task-specific nature of <span id="S4.SS7.p1.1.2" class="ltx_text ltx_font_smallcaps">mac</span> allowed us to replace the image with the scene graph, doing so with <span id="S4.SS7.p1.1.3" class="ltx_text ltx_font_smallcaps">uniter</span> would not work since only image representations were fed to the model during pre-training. Hence, we adopt the late fusion architecture shown in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.7 UNITER with Scene Graphs ‣ 4 Proposed Approach ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, where the output of an <span id="S4.SS7.p1.1.4" class="ltx_text ltx_font_smallcaps">sgmac</span> model trained on ground truth scene graphs is passed through fully-connected layers and the output of a pre-trained <span id="S4.SS7.p1.1.5" class="ltx_text ltx_font_smallcaps">uniter</span> model is passed through separate fully-connected layers. The outputs of these layers are then combined and passed through a classification layer. The fully-connected layers are trained from scratch, while <span id="S4.SS7.p1.1.6" class="ltx_text ltx_font_smallcaps">sgmac</span> and <span id="S4.SS7.p1.1.7" class="ltx_text ltx_font_smallcaps">uniter</span> are fine-tuned.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<p id="S4.F4.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S4.F4.1.1.1" class="ltx_text"><img src="/html/2101.05479/assets/figs/slide_29.jpg" id="S4.F4.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="347" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S4.F4.3.1" class="ltx_text ltx_font_smallcaps">uniter</span> with Scene Graphs</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Setup</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Throughout the course of this project, we primarily work with the GQA dataset, where we analyze the effects of using scene graphs instead of images to aid our models in the task of VQA.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Dataset</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The GQA dataset consists of 22M+ questions and 113K+ images, with associated scene graphs for 85K+ images.
The dataset is divided into 5 splits: train, validation, testdev, test and challenge, with the option to avail a more balanced subset for most categories. The test and challenge datasets are used for leaderboard submissions and do not come with ground truth scene graphs. The training and validation sets are provided with images which have annotated ground truth scene graphs. Apart from the ground truth scene graphs which we leverage for our experiments, we also use the extracted object features and spatial features, provided with the dataset, in our baseline models.
</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The GQA task uses six metrics to evaluate a model’s visual reasoning ability, namely <em id="S5.SS1.p2.1.1" class="ltx_emph ltx_font_italic">accuracy</em>, <em id="S5.SS1.p2.1.2" class="ltx_emph ltx_font_italic">validity</em>, <em id="S5.SS1.p2.1.3" class="ltx_emph ltx_font_italic">plausibility</em>, <em id="S5.SS1.p2.1.4" class="ltx_emph ltx_font_italic">consistency</em>, <em id="S5.SS1.p2.1.5" class="ltx_emph ltx_font_italic">grounding</em>, and <em id="S5.SS1.p2.1.6" class="ltx_emph ltx_font_italic">distribution</em>. Details about the calculation of each metric can be found in the GQA paper <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib11" title="" class="ltx_ref">2019b</a>)</cite>.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Training Environment</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The experimental setup for this project can be divided into various components based on how the training phase is carried out. The details regarding the various training stages and requirements are outlined in Table <a href="#S5.T1" title="Table 1 ‣ 5.2 Training Environment ‣ 5 Experimental Setup ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<div id="S5.T1.1" class="ltx_inline-block ltx_transformed_outer" style="width:455.2pt;height:167.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(7.9pt,-2.9pt) scale(1.03598499369413,1.03598499369413) ;">
<table id="S5.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.1.1.1.1" class="ltx_tr">
<td id="S5.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Training phase</span></td>
<td id="S5.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Scene Graph Generation</span></td>
<td id="S5.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S5.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">GQA Answering Model</span></td>
</tr>
<tr id="S5.T1.1.1.2.2" class="ltx_tr">
<td id="S5.T1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T1.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Model Architecture</span></td>
<td id="S5.T1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ResNet 50 + Neural Motifs</td>
<td id="S5.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">GN + <span id="S5.T1.1.1.2.2.3.1" class="ltx_text ltx_font_smallcaps">mac</span>
</td>
<td id="S5.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.1.2.2.4.1" class="ltx_text ltx_font_smallcaps">uniter</span></td>
</tr>
<tr id="S5.T1.1.1.3.3" class="ltx_tr">
<td id="S5.T1.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T1.1.1.3.3.1.1" class="ltx_text ltx_font_bold">Optimizer</span></td>
<td id="S5.T1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SGD</td>
<td id="S5.T1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Adam</td>
<td id="S5.T1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">AdamW</td>
</tr>
<tr id="S5.T1.1.1.4.4" class="ltx_tr">
<td id="S5.T1.1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T1.1.1.4.4.1.1" class="ltx_text ltx_font_bold">Learning rate</span></td>
<td id="S5.T1.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.02</td>
<td id="S5.T1.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1e-4</td>
<td id="S5.T1.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8e-5</td>
</tr>
<tr id="S5.T1.1.1.5.5" class="ltx_tr">
<td id="S5.T1.1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T1.1.1.5.5.1.1" class="ltx_text ltx_font_bold">Epochs / Steps</span></td>
<td id="S5.T1.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50000 steps</td>
<td id="S5.T1.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15 epochs</td>
<td id="S5.T1.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6000 steps</td>
</tr>
<tr id="S5.T1.1.1.6.6" class="ltx_tr">
<td id="S5.T1.1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T1.1.1.6.6.1.1" class="ltx_text ltx_font_bold">Batch size</span></td>
<td id="S5.T1.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4</td>
<td id="S5.T1.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">128</td>
<td id="S5.T1.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">500</td>
</tr>
<tr id="S5.T1.1.1.7.7" class="ltx_tr">
<td id="S5.T1.1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T1.1.1.7.7.1.1" class="ltx_text ltx_font_bold">No. of GPUs</span></td>
<td id="S5.T1.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4</td>
<td id="S5.T1.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S5.T1.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
</tr>
<tr id="S5.T1.1.1.8.8" class="ltx_tr">
<td id="S5.T1.1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T1.1.1.8.8.1.1" class="ltx_text ltx_font_bold">GPU Model / RAM</span></td>
<td id="S5.T1.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Tesla V100 / 32 GB</td>
<td id="S5.T1.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Tesla V100 / 16 GB</td>
<td id="S5.T1.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Tesla V100 / 16 GB</td>
</tr>
<tr id="S5.T1.1.1.9.9" class="ltx_tr">
<td id="S5.T1.1.1.9.9.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T1.1.1.9.9.1.1" class="ltx_text ltx_font_bold">Time taken</span></td>
<td id="S5.T1.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1 day 18 hours</td>
<td id="S5.T1.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">22.5 hours</td>
<td id="S5.T1.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">45 minutes</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Experimental settings of various models and stages of training</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results and Analysis</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we present the results of our proposed approach along with an extensive analysis. We use the semantic types in <a href="#A1.SS1" title="A.1 Semantic Types in GQA ‣ Appendix A Appendices ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a> for our analysis.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Results of Baselines</h3>

<figure id="S6.T2" class="ltx_table">
<table id="S6.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T2.1.1.1" class="ltx_tr">
<th id="S6.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="2">Our Models</th>
<th id="S6.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2">Comparable Models</th>
</tr>
<tr id="S6.T2.1.2.2" class="ltx_tr">
<th id="S6.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Model</th>
<th id="S6.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Test</th>
<th id="S6.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Model</th>
<th id="S6.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Test</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T2.1.3.1" class="ltx_tr">
<th id="S6.T2.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Image</th>
<th id="S6.T2.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">0.174</th>
<td id="S6.T2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CNN</td>
<td id="S6.T2.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.178</td>
</tr>
<tr id="S6.T2.1.4.2" class="ltx_tr">
<th id="S6.T2.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Text</th>
<th id="S6.T2.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.398</th>
<td id="S6.T2.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r">LSTM</td>
<td id="S6.T2.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r">0.41</td>
</tr>
<tr id="S6.T2.1.5.3" class="ltx_tr">
<th id="S6.T2.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S6.T2.1.5.3.1.1" class="ltx_text ltx_font_smallcaps">concat</span></th>
<th id="S6.T2.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.435</th>
<td id="S6.T2.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r">CNN+LSTM</td>
<td id="S6.T2.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r">0.465</td>
</tr>
<tr id="S6.T2.1.6.4" class="ltx_tr">
<th id="S6.T2.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"><span id="S6.T2.1.6.4.1.1" class="ltx_text ltx_font_smallcaps">attn</span></th>
<th id="S6.T2.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">0.48</th>
<td id="S6.T2.1.6.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Bottom-up</td>
<td id="S6.T2.1.6.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.497</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of Baselines</figcaption>
</figure>
<figure id="S6.F5" class="ltx_figure">
<p id="S6.F5.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S6.F5.1.1.1" class="ltx_text"><img src="/html/2101.05479/assets/plots/baseline_compare_dotted2.png" id="S6.F5.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="377" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparison of the two baselines</figcaption>
</figure>
<figure id="S6.T3" class="ltx_table">
<table id="S6.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T3.1.1.1" class="ltx_tr">
<th id="S6.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Model</th>
<td id="S6.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Val</td>
<td id="S6.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Testdev</td>
</tr>
<tr id="S6.T3.1.2.2" class="ltx_tr">
<th id="S6.T3.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.1.2.2.1.1" class="ltx_text ltx_font_smallcaps">uniter</span></th>
<td id="S6.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.69</td>
<td id="S6.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.598</td>
</tr>
<tr id="S6.T3.1.3.3" class="ltx_tr">
<th id="S6.T3.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">
<span id="S6.T3.1.3.3.1.1" class="ltx_text ltx_font_smallcaps">sgmac</span> (GT)</th>
<td id="S6.T3.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r">0.94</td>
<td id="S6.T3.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S6.T3.1.4.4" class="ltx_tr">
<th id="S6.T3.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">
<span id="S6.T3.1.4.4.1.1" class="ltx_text ltx_font_smallcaps">sgmac</span> (Noisy)</th>
<td id="S6.T3.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r">0.548</td>
<td id="S6.T3.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r">0.478</td>
</tr>
<tr id="S6.T3.1.5.5" class="ltx_tr">
<th id="S6.T3.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">
<span id="S6.T3.1.5.5.1.1" class="ltx_text ltx_font_smallcaps">sgmac</span> + <span id="S6.T3.1.5.5.1.2" class="ltx_text ltx_font_smallcaps">uniter</span>
</th>
<td id="S6.T3.1.5.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.688</td>
<td id="S6.T3.1.5.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.592</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results of Proposed Approaches</figcaption>
</figure>
<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">From Table <a href="#S6.T2" title="Table 2 ‣ 6.1 Results of Baselines ‣ 6 Results and Analysis ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we see that the text only baseline does much better than the image only one. This is expected, as the question has more useful information to infer the answer than the image alone. The <span id="S6.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">attn</span> model achieves a significantly higher accuracy than <span id="S6.SS1.p1.1.2" class="ltx_text ltx_font_smallcaps">concat</span>. We believe that the attention mechanism helps <span id="S6.SS1.p1.1.3" class="ltx_text ltx_font_smallcaps">attn</span> focus on the most relevant objects in the image, based on the question at hand.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">Figure <a href="#S6.F5" title="Figure 5 ‣ 6.1 Results of Baselines ‣ 6 Results and Analysis ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the breakdown of the performance of both baselines on questions of different semantic types. <span id="S6.SS1.p2.1.1" class="ltx_text ltx_font_smallcaps">attn</span> does better than <span id="S6.SS1.p2.1.2" class="ltx_text ltx_font_smallcaps">concat</span> in every category. We see that there is a large scope for improvement in all the semantic types, especially in the relation type which primarily consists of questions that require knowledge about the relationships between objects to answer. This information can be found in the scene graphs.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Performance of UNITER</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">Figure <a href="#S6.F6" title="Figure 6 ‣ 6.2 Performance of UNITER ‣ 6 Results and Analysis ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the performance of <span id="S6.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">uniter</span> over the different semantic types. The performance across all the types far exceeds that of the baselines, demonstrating the strength of <span id="S6.SS2.p1.1.2" class="ltx_text ltx_font_smallcaps">uniter</span>’s image-text representations and the advantages of pre-training. It is interesting to note that the relative ordering of semantic type accuracy is the same for <span id="S6.SS2.p1.1.3" class="ltx_text ltx_font_smallcaps">uniter</span> and the baselines. The accuracy on the object type is very high, while there is large room for improvement in the relation type.
An analysis of the predictions of the model shows that for many examples, the model predicts an answer that is semantically close to the ground truth answer (e.g. ’bananas’ vs ’banana’, ’desk’ vs ’table’). Here, it is useful to look to metrics other than accuracy which do not enforce an exact match with the ground truth answer. <span id="S6.SS2.p1.1.4" class="ltx_text ltx_font_smallcaps">uniter</span> obtains high plausibility (0.92) and validity (0.95) scores, indicating a good understanding of the image and text.</p>
</div>
<figure id="S6.F6" class="ltx_figure">
<p id="S6.F6.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S6.F6.1.1.1" class="ltx_text"><img src="/html/2101.05479/assets/plots/sem_type_uniter.png" id="S6.F6.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="431" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Performance of <span id="S6.F6.3.1" class="ltx_text ltx_font_smallcaps">uniter</span> over semantic types</figcaption>
</figure>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span><span id="S6.SS3.1.1" class="ltx_text ltx_font_smallcaps">SGMAC</span> on GT Scene Graphs</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">The performance of <span id="S6.SS3.p1.1.1" class="ltx_text ltx_font_smallcaps">sgmac</span> with GT scene graphs on different semantic types is shown in Figure <a href="#S6.F7" title="Figure 7 ‣ 6.3 SGMAC on GT Scene Graphs ‣ 6 Results and Analysis ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. This approach does very well over all types, and most notably the relation semantic type where <span id="S6.SS3.p1.1.2" class="ltx_text ltx_font_smallcaps">uniter</span> struggles. The overall validation accuracy of this approach is 94.6%. This shows the effectiveness of the scene graph representation, and perhaps represents the upper limit of what can be achieved on GQA <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><cite class="ltx_cite ltx_citemacro_citet">Lee et al. (<a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite> report a score of 96.3 on training for 100 epochs, while we train for only 15-20 epochs.</span></span></span>.</p>
</div>
<figure id="S6.F7" class="ltx_figure">
<p id="S6.F7.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S6.F7.1.1.1" class="ltx_text"><img src="/html/2101.05479/assets/plots/gn_sem_dotted2.png" id="S6.F7.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="431" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Performance of <span id="S6.F7.3.1" class="ltx_text ltx_font_smallcaps">sgmac</span> on ground truth scene graphs</figcaption>
</figure>
<section id="S6.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.1 </span><span id="S6.SS3.SSS1.1.1" class="ltx_text ltx_font_smallcaps">SGMAC</span> vs <span id="S6.SS3.SSS1.2.2" class="ltx_text ltx_font_smallcaps">SGATT</span>
</h4>

<div id="S6.SS3.SSS1.p1" class="ltx_para">
<p id="S6.SS3.SSS1.p1.1" class="ltx_p">The attention model described in Section <a href="#S4.SS1" title="4.1 Baselines ‣ 4 Proposed Approach ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> with the Graph Network (66.4% accuracy) does not match the performance of <span id="S6.SS3.SSS1.p1.1.1" class="ltx_text ltx_font_smallcaps">sgmac</span> with the Graph Network (94.5% accuracy), even when trained and evaluated on ground truth scene graphs. This demonstrates the importance of the manner in which the scene graph is parsed and proves that a good scene graph representation alone is not sufficient to achieve high accuracy.</p>
</div>
</section>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>GT vs Generated Scene Graphs</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">Figure <a href="#S6.F8" title="Figure 8 ‣ 6.4 GT vs Generated Scene Graphs ‣ 6 Results and Analysis ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows a comparison of the semantic type accuracy of <span id="S6.SS4.p1.1.1" class="ltx_text ltx_font_smallcaps">sgmac</span> with GT scene graphs and noisy scene graphs.
It is clear that the performance of <span id="S6.SS4.p1.1.2" class="ltx_text ltx_font_smallcaps">sgmac</span> with ground truth scene graphs far exceeds that of <span id="S6.SS4.p1.1.3" class="ltx_text ltx_font_smallcaps">sgmac</span> with noisy scene graphs on attribute, category and relation types and has a comparable performance on global and object types. We believe that this is due to the quality of the generated scene graphs rather than the technique used to encode them.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<table id="S6.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T4.1.1.1" class="ltx_tr">
<th id="S6.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Scene Graphs</th>
<th id="S6.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Val</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T4.1.2.1" class="ltx_tr">
<td id="S6.T4.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">GT</td>
<td id="S6.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.945</td>
</tr>
<tr id="S6.T4.1.3.2" class="ltx_tr">
<td id="S6.T4.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">GT - No Relations</td>
<td id="S6.T4.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.851</td>
</tr>
<tr id="S6.T4.1.4.3" class="ltx_tr">
<td id="S6.T4.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">GT - No Attributes</td>
<td id="S6.T4.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0.851</td>
</tr>
<tr id="S6.T4.1.5.4" class="ltx_tr">
<td id="S6.T4.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Noisy</td>
<td id="S6.T4.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">0.529</td>
</tr>
<tr id="S6.T4.1.6.5" class="ltx_tr">
<td id="S6.T4.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Noisy - No Relations</td>
<td id="S6.T4.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">0.532</td>
</tr>
<tr id="S6.T4.1.7.6" class="ltx_tr">
<td id="S6.T4.1.7.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">Noisy - No Attributes</td>
<td id="S6.T4.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.53</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span><span id="S6.T4.3.1" class="ltx_text ltx_font_smallcaps">sgmac</span> performance on different scene graphs</figcaption>
</figure>
<div id="S6.SS4.p2" class="ltx_para">
<p id="S6.SS4.p2.1" class="ltx_p">To get a better idea of the differences between the generated and ground truth scene graphs, we analyze their corresponding objects, attributes and relations.</p>
</div>
<div id="S6.SS4.p3" class="ltx_para">
<p id="S6.SS4.p3.1" class="ltx_p"><span id="S6.SS4.p3.1.1" class="ltx_text ltx_font_bold">Objects</span>: On average, GT scene graphs have 16 objects, and 3 of these are not present in the generated scene graphs. Since most objects are correctly detected, the model correctly answers the majority of questions about the existence of objects in the image (object semantic type). The performance on this type using GT and generated scene graphs is comparable due to the high overlap in the objects.</p>
</div>
<div id="S6.SS4.p4" class="ltx_para">
<p id="S6.SS4.p4.1" class="ltx_p"><span id="S6.SS4.p4.1.1" class="ltx_text ltx_font_bold">Attributes</span>: We find that most attributes in the GT scene graphs are present in the generated scene graphs. However, the generated scene graphs additionally contain many spurious attributes, which negates the effect of getting the attributes right and explains the poor performance on the attribute semantic type. For example, a phone which is grey in color might also have “blue” and “brown” as additional attributes, and hence a question such as “What color is the phone?” would not be answered correctly. In Sec. <a href="#S6.SS5.SSS3" title="6.5.3 Filtering Predicted Scene Graphs ‣ 6.5 Effect of scene graph quality ‣ 6 Results and Analysis ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.5.3</span></a> we describe the use of thresholds to reduce spurious attributes.</p>
</div>
<div id="S6.SS4.p5" class="ltx_para">
<p id="S6.SS4.p5.1" class="ltx_p"><span id="S6.SS4.p5.1.1" class="ltx_text ltx_font_bold">Relations</span>: We find that most of the edges have extremely low overlap (1-2 edges) between GT and noisy scene graphs, which explains why the gap between the performance of the two on the relation semantic type is high.</p>
</div>
<div id="S6.SS4.p6" class="ltx_para">
<p id="S6.SS4.p6.1" class="ltx_p">The above analysis showcases the need for improvement in the quality of the generated scene graphs. Specifically, correctly detecting relations would go a long way towards improving performance. It also shows how the use of accurate scene graphs would benefit VQA.</p>
</div>
<div id="S6.SS4.p7" class="ltx_para">
<p id="S6.SS4.p7.1" class="ltx_p">In addition to qualitative analysis, we conduct the ablation studies shown in Table <a href="#S6.T4" title="Table 4 ‣ 6.4 GT vs Generated Scene Graphs ‣ 6 Results and Analysis ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> to understand the impact of various components of scene graphs. We run experiments wherein we remove either relation names or attributes from the scene graph. On removing relations/attributes from GT scene graphs, we observe a 9.4% drop in accuracy. The same experiments when run on generated scene graphs yield little difference in accuracy further indicating that the quality of detected relations is poor.</p>
</div>
<figure id="S6.F8" class="ltx_figure">
<p id="S6.F8.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S6.F8.1.1.1" class="ltx_text"><img src="/html/2101.05479/assets/plots/gtvsnoisy_sg_dotted.png" id="S6.F8.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="387" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Comparison of <span id="S6.F8.3.1" class="ltx_text ltx_font_smallcaps">sgmac</span> using ground truth and noisy scene graphs</figcaption>
</figure>
</section>
<section id="S6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Effect of scene graph quality</h3>

<div id="S6.SS5.p1" class="ltx_para">
<p id="S6.SS5.p1.1" class="ltx_p">We further experiment with various scene graphs outputs that were generated by different variations of the Neural Motifs model, to effectively analyze the difference in quality of the scene graphs generated. We experiment with 2 variations of the model, making changes to the loss function to better understand the differences in prediction pertaining to the predicted labels for an edge relation. The variations are as follows:</p>
</div>
<section id="S6.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.5.1 </span>Impact of Label Loss Smoothing</h4>

<div id="S6.SS5.SSS1.p1" class="ltx_para">
<p id="S6.SS5.SSS1.p1.1" class="ltx_p">During a preliminary analysis of the dataset, we noticed that there is a stark imbalance in the training data for the relation labels “to the left of” and “to the right of”. Both labels appear at least 18 times more often than next most frequent relation in the training set. This causes the standard variant of our model to disproportionately predict the above mentioned relations. In an attempt to mitigate this class imbalance, we use an exponential label smoothing loss function.</p>
</div>
<figure id="S6.T5" class="ltx_table">
<table id="S6.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T5.1.1.1" class="ltx_tr">
<th id="S6.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Model</th>
<th id="S6.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Testdev</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T5.1.2.1" class="ltx_tr">
<td id="S6.T5.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Without Smoothing - Noisy</td>
<td id="S6.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.458</td>
</tr>
<tr id="S6.T5.1.3.2" class="ltx_tr">
<td id="S6.T5.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Without Smoothing - GT + Noisy</td>
<td id="S6.T5.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.479</td>
</tr>
<tr id="S6.T5.1.4.3" class="ltx_tr">
<td id="S6.T5.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">With Smoothing - Noisy</td>
<td id="S6.T5.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0.457</td>
</tr>
<tr id="S6.T5.1.5.4" class="ltx_tr">
<td id="S6.T5.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">With Smoothing - GT + Noisy</td>
<td id="S6.T5.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.476</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span><span id="S6.T5.3.1" class="ltx_text ltx_font_smallcaps">sgmac</span> performance with and without label smoothing</figcaption>
</figure>
<div id="S6.SS5.SSS1.p2" class="ltx_para">
<p id="S6.SS5.SSS1.p2.1" class="ltx_p">We can see from Table <a href="#S6.T5" title="Table 5 ‣ 6.5.1 Impact of Label Loss Smoothing ‣ 6.5 Effect of scene graph quality ‣ 6 Results and Analysis ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> that the label smoothing does not help improve the downstream model performance much. Another potential remedy to handle the class imbalance would be to use a two-way softmax that separates the “to the left of ” and “to the right of” labels from the other relations.</p>
</div>
</section>
<section id="S6.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.5.2 </span>Corruption of GT Scene Graphs</h4>

<figure id="S6.T6" class="ltx_table">
<table id="S6.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T6.1.1.1" class="ltx_tr">
<th id="S6.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Level of Corruption</th>
<th id="S6.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Val</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T6.1.2.1" class="ltx_tr">
<td id="S6.T6.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">0.2</td>
<td id="S6.T6.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.861</td>
</tr>
<tr id="S6.T6.1.3.2" class="ltx_tr">
<td id="S6.T6.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">0.4</td>
<td id="S6.T6.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.788</td>
</tr>
<tr id="S6.T6.1.4.3" class="ltx_tr">
<td id="S6.T6.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">0.6</td>
<td id="S6.T6.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0.721</td>
</tr>
<tr id="S6.T6.1.5.4" class="ltx_tr">
<td id="S6.T6.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">0.8</td>
<td id="S6.T6.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">0.662</td>
</tr>
<tr id="S6.T6.1.6.5" class="ltx_tr">
<td id="S6.T6.1.6.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Noisy</td>
<td id="S6.T6.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.461</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span><span id="S6.T6.3.1" class="ltx_text ltx_font_smallcaps">sgmac</span> performance with different levels of scene graph corruption</figcaption>
</figure>
<div id="S6.SS5.SSS2.p1" class="ltx_para">
<p id="S6.SS5.SSS2.p1.1" class="ltx_p">In order to understand how the noisy scene graphs affect downstream performance in our GQA model, we conduct evaluation of our best performing GT (<span id="S6.SS5.SSS2.p1.1.1" class="ltx_text ltx_font_smallcaps">sgmac</span>) model on varying levels of corrupted ground truth scene graphs. This allows us to estimate the amount of noise it would take in a gold standard scene graph dataset to reach the same accuracy we obtain when evaluated on a noisy scene graph dataset. In order to corrupt the ground truth scene graphs, we randomly swap nodes, edge pairs and relations, and also corrupt object attributes and names by replacing them with <em id="S6.SS5.SSS2.p1.1.2" class="ltx_emph ltx_font_italic">UNK</em> tokens. Figure <a href="#S4.F3.sf2" title="Figure 3(b) ‣ Figure 3 ‣ 4.6 Training Curriculum ‣ 4 Proposed Approach ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a> illustrates an example of what a corrupted ground truth scene graph would look like, in contrast to the ground truth scene graph, as depicted on the left side of Figure <a href="#S4.F2" title="Figure 2 ‣ 4.6 Training Curriculum ‣ 4 Proposed Approach ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. These differ from the noisy injected scene graphs (Section <a href="#S4.SS6" title="4.6 Training Curriculum ‣ 4 Proposed Approach ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.6</span></a>) as we do not bring in any external components from the noisy scene graphs into the corrupted GT scene graph. We create 4 validation datasets of different levels of corruption by varying degrees of probability with which we corrupt the scene graph as shown in Table <a href="#S6.T6" title="Table 6 ‣ 6.5.2 Corruption of GT Scene Graphs ‣ 6.5 Effect of scene graph quality ‣ 6 Results and Analysis ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. From the table it is clear that the noisy scene graphs differ much more significantly from the ground truth representations and that this level of noisiness will be very hard to overcome even with the most powerful model capable of reasoning over scene graphs.</p>
</div>
</section>
<section id="S6.SS5.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.5.3 </span>Filtering Predicted Scene Graphs</h4>

<div id="S6.SS5.SSS3.p1" class="ltx_para">
<p id="S6.SS5.SSS3.p1.1" class="ltx_p">In order to remove any redundant or spurious information that is included while generating the noisy scene graphs, we also employ some standard filtering techniques to take the top k components of the noisy scene graph. In an unfiltered noisy scene graph, we would have 80 objects, 10 attributes/object and 100 relations between object pairs. From this set, we employ two filtering strategies based on both the confidence score of the model for each label and also based on the top k labels the model predicts.</p>
</div>
<div id="S6.SS5.SSS3.p2" class="ltx_para">
<p id="S6.SS5.SSS3.p2.1" class="ltx_p">For the top k setting, we take the top 40 objects, the top 3 attributes/object and the top 80 relations from each noisy scene graph. We also alternatively filter them by a threshold value and only include components that are greater than a particular threshold. The threshold values selected are 0.1 for objects, 0.9 for attributes and 0.8 for relations.</p>
</div>
<figure id="S6.T7" class="ltx_table">
<table id="S6.T7.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T7.1.1.1" class="ltx_tr">
<th id="S6.T7.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Scene Graph Filter</th>
<th id="S6.T7.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Testdev</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T7.1.2.1" class="ltx_tr">
<td id="S6.T7.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">No Filter - Noisy</td>
<td id="S6.T7.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.468</td>
</tr>
<tr id="S6.T7.1.3.2" class="ltx_tr">
<td id="S6.T7.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Top k - Noisy</td>
<td id="S6.T7.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.464</td>
</tr>
<tr id="S6.T7.1.4.3" class="ltx_tr">
<td id="S6.T7.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Threshold - Noisy</td>
<td id="S6.T7.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0.458</td>
</tr>
<tr id="S6.T7.1.5.4" class="ltx_tr">
<td id="S6.T7.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">No Filter - GT + Noisy</td>
<td id="S6.T7.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.475</td>
</tr>
<tr id="S6.T7.1.6.5" class="ltx_tr">
<td id="S6.T7.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Top k - GT + Noisy</td>
<td id="S6.T7.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">0.478</td>
</tr>
<tr id="S6.T7.1.7.6" class="ltx_tr">
<td id="S6.T7.1.7.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">Threshold - GT + Noisy</td>
<td id="S6.T7.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.479</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span><span id="S6.T7.3.1" class="ltx_text ltx_font_smallcaps">sgmac</span> performance with different filters on scene graphs</figcaption>
</figure>
<div id="S6.SS5.SSS3.p3" class="ltx_para">
<p id="S6.SS5.SSS3.p3.1" class="ltx_p">When training from scratch, retaining all the model attributes and relations intact seems to let the model learn more information with further training. This indicates that the filtered noisy scene graphs are possibly more akin to the ground truth representations but without a good lower level feature representation (a good starting checkpoint), the Graph Network might not be powerful enough to learn much from the (sparsely) filtered noisy scene graphs.
</p>
</div>
</section>
</section>
<section id="S6.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.6 </span>Training Curriculum</h3>

<figure id="S6.T8" class="ltx_table">
<table id="S6.T8.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T8.1.1.1" class="ltx_tr">
<th id="S6.T8.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Training Curriculum</th>
<th id="S6.T8.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Testdev</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T8.1.2.1" class="ltx_tr">
<td id="S6.T8.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">GT</td>
<td id="S6.T8.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.373</td>
</tr>
<tr id="S6.T8.1.3.2" class="ltx_tr">
<td id="S6.T8.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Noisy</td>
<td id="S6.T8.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.468</td>
</tr>
<tr id="S6.T8.1.4.3" class="ltx_tr">
<td id="S6.T8.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">GT+Noisy</td>
<td id="S6.T8.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0.475</td>
</tr>
<tr id="S6.T8.1.5.4" class="ltx_tr">
<td id="S6.T8.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Probabilistic</td>
<td id="S6.T8.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">0.471</td>
</tr>
<tr id="S6.T8.1.6.5" class="ltx_tr">
<td id="S6.T8.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Probabilistic (Filtered)</td>
<td id="S6.T8.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">0.477</td>
</tr>
<tr id="S6.T8.1.7.6" class="ltx_tr">
<td id="S6.T8.1.7.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">Probabilistic (Complete)</td>
<td id="S6.T8.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.478</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Training Curriculum Results</figcaption>
</figure>
<div id="S6.SS6.p1" class="ltx_para">
<p id="S6.SS6.p1.1" class="ltx_p">As mentioned in Section <a href="#S4.SS6" title="4.6 Training Curriculum ‣ 4 Proposed Approach ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.6</span></a>, we see that training with just the GT and or just noisy scene graphs does not yield very desirable results. However, training the model using the novel training curriculum outlined in Section <a href="#S4.SS6" title="4.6 Training Curriculum ‣ 4 Proposed Approach ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.6</span></a> gives us a gain in performance.</p>
</div>
<div id="S6.SS6.p2" class="ltx_para">
<p id="S6.SS6.p2.1" class="ltx_p">We see that gradually swapping ground truth scene graphs with noisy scene graphs during training (Probabilistic (Complete) in <a href="#S6.T8" title="Table 8 ‣ 6.6 Training Curriculum ‣ 6 Results and Analysis ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>) performs better than completely replacing ground truth scene graphs with noisy scene graphs after a few epochs (GT + Noisy in <a href="#S6.T8" title="Table 8 ‣ 6.6 Training Curriculum ‣ 6 Results and Analysis ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>).
Completely replacing the ground truth scene graphs (for random noisy graphs) performs on par with the probabilistic training mechanism explained in Figure <a href="#S4.F2" title="Figure 2 ‣ 4.6 Training Curriculum ‣ 4 Proposed Approach ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S4.F3.sf1" title="Figure 3(a) ‣ Figure 3 ‣ 4.6 Training Curriculum ‣ 4 Proposed Approach ‣ Understanding the Role of Scene Graphs in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>.</p>
</div>
<div id="S6.SS6.p3" class="ltx_para">
<p id="S6.SS6.p3.1" class="ltx_p">We see that filtering the noisy dataset on the top k objects/attributes/relations helps weed out the redundant/spurious labels and improve accuracy slightly. Overall, these results prove that the probabilistic training curriculum helps the model generalize much better compared to training on one source of data, and hence validate the effectiveness of the proposed novel training curriculum.
</p>
</div>
</section>
<section id="S6.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.7 </span>UNITER with Scene Graphs</h3>

<div id="S6.SS7.p1" class="ltx_para">
<p id="S6.SS7.p1.1" class="ltx_p">The late-fusion ensemble of <span id="S6.SS7.p1.1.1" class="ltx_text ltx_font_smallcaps">uniter</span> with <span id="S6.SS7.p1.1.2" class="ltx_text ltx_font_smallcaps">sgmac</span> does not perform better than <span id="S6.SS7.p1.1.3" class="ltx_text ltx_font_smallcaps">uniter</span> itself. We analyse the accuracies across semantic types to see if it does better on relational queries, but find no significant differences. It could be that <span id="S6.SS7.p1.1.4" class="ltx_text ltx_font_smallcaps">uniter</span> is a very powerful model which overwhelms the much smaller <span id="S6.SS7.p1.1.5" class="ltx_text ltx_font_smallcaps">sgmac</span>.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion and Future Work</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this work, we present novel research that is aimed at understanding various aspects of the use of scene graphs for the task of VQA. We develop modules for scene graph encoding, architectures for question answering using scene graphs, probabilistic training curricula, and late fusion models for question answering. We primarily use validation and test-dev scores for our analysis, but our test submissions can be found on the leaderboard<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://eval.ai/web/challenges/challenge-page/225/leaderboard/733" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://eval.ai/web/challenges/challenge-page/225/leaderboard/733</a></span></span></span> under the team name “bert”.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">The architecture combining a Graph Network with <span id="S7.p2.1.1" class="ltx_text ltx_font_smallcaps">mac</span> surpasses the performance of state of the art models when ground truth scene graphs are available. However, this performance quickly deteriorates as reliance on auto-generated scene graphs increases. We show that a training curriculum that utilizes generated and ground truth scene graphs is more effective than the use of a singular type of scene graph. This approach does not come close to matching pre-trained image based models such as <span id="S7.p2.1.2" class="ltx_text ltx_font_smallcaps">uniter</span>, suggesting that improving scene graph generation will be the most important step for using scene graph based models for VQA. Tackling the class imbalance for relations and improving the object detection could help in this regard. Techniques that allow the model to fall-back to image features based on the level of noise in the scene graph could be quite effective and we plan to explore these further.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We would like to express our gratitude to Dr. Lu Jiang (Google) for providing us invaluable advice and guidance throughout the course of this work. We would also like to thank Dr. Mayu Otani for her continued support.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2018)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. 2018.

</span>
<span class="ltx_bibblock">Don’t just assume; look and answer: Overcoming priors for visual
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 4971–4980.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al. (2018)</span>
<span class="ltx_bibblock">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang. 2018.

</span>
<span class="ltx_bibblock">Bottom-up and top-down attention for image captioning and visual
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 6077–6086.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andreas et al. (2016)</span>
<span class="ltx_bibblock">
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016.

</span>
<span class="ltx_bibblock">Neural module networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 39–48.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 2425–2433.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Battaglia et al. (2018)</span>
<span class="ltx_bibblock">
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez,
Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam
Santoro, Ryan Faulkner, et al. 2018.

</span>
<span class="ltx_bibblock">Relational inductive biases, deep learning, and graph networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.01261</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,
Yu Cheng, and Jingjing Liu. 2020.

</span>
<span class="ltx_bibblock">Uniter: Universal image-text representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 104–120.
Springer.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N19-1423" title="" class="ltx_ref ltx_href">BERT: Pre-training of
deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186,
Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2017)</span>
<span class="ltx_bibblock">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. 2017.

</span>
<span class="ltx_bibblock">Mask r-cnn.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 2961–2969.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning (2019a)</span>
<span class="ltx_bibblock">
Drew Hudson and Christopher D Manning. 2019a.

</span>
<span class="ltx_bibblock">Learning by abstraction: The neural state machine.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, pages
5903–5916.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning (2018)</span>
<span class="ltx_bibblock">
Drew A Hudson and Christopher D Manning. 2018.

</span>
<span class="ltx_bibblock">Compositional attention networks for machine reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.03067</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning (2019b)</span>
<span class="ltx_bibblock">
Drew A Hudson and Christopher D Manning. 2019b.

</span>
<span class="ltx_bibblock">Gqa: A new dataset for real-world visual reasoning and compositional
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 6700–6709.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2018)</span>
<span class="ltx_bibblock">
Justin Johnson, Agrim Gupta, and Li Fei-Fei. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1804.01622" title="" class="ltx_ref ltx_href">Image generation from scene
graphs</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2017)</span>
<span class="ltx_bibblock">
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei,
C Lawrence Zitnick, and Ross Girshick. 2017.

</span>
<span class="ltx_bibblock">Clevr: A diagnostic dataset for compositional language and elementary
visual reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 2901–2910.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. (2017)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.
2017.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em>, 123(1):32–73.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2020)</span>
<span class="ltx_bibblock">
Hsin-Ying Lee, Lu Jiang, Irfan Essa, Phuong B Le, Haifeng Gong, Ming-Hsuan
Yang, and Weilong Yang. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1912.09421" title="" class="ltx_ref ltx_href">Neural design network:
Graphic layout generation with constraints</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2019)</span>
<span class="ltx_bibblock">
S. Lee, J. Kim, Y. Oh, and J. H. Jeon. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/GC46384.2019.00015" title="" class="ltx_ref ltx_href">Visual question
answering over scene graph</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2019 First International Conference on Graph Computing
(GC)</em>, pages 45–50.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2019)</span>
<span class="ltx_bibblock">
Yijun Li, Lu Jiang, and Ming-Hsuan Yang. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1912.11711" title="" class="ltx_ref ltx_href">Controllable and progressive
image extrapolation</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2019)</span>
<span class="ltx_bibblock">
J. Liang, L. Jiang, L. Cao, Y. Kalantidis, L. Li, and A. G.
Hauptmann. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TPAMI.2018.2890628" title="" class="ltx_ref ltx_href">Focal visual-text
attention for memex question answering</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em>, 41(8):1893–1908.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington et al. (2014)</span>
<span class="ltx_bibblock">
Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3115/v1/D14-1162" title="" class="ltx_ref ltx_href">GloVe: Global
vectors for word representation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 1532–1543, Doha, Qatar.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2016)</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2016.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal
networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine
intelligence</em>, 39(6):1137–1149.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Bansal (2019)</span>
<span class="ltx_bibblock">
Hao Tan and Mohit Bansal. 2019.

</span>
<span class="ltx_bibblock">Lxmert: Learning cross-modality encoder representations from
transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.07490</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2020)</span>
<span class="ltx_bibblock">
Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, and Hanwang Zhang. 2020.

</span>
<span class="ltx_bibblock">Unbiased scene graph generation from biased training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 3716–3725.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2019)</span>
<span class="ltx_bibblock">
Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo, and Wei Liu. 2019.

</span>
<span class="ltx_bibblock">Learning to compose dynamic tree structures for visual contexts.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 6619–6628.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2017)</span>
<span class="ltx_bibblock">
Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei. 2017.

</span>
<span class="ltx_bibblock">Scene graph generation by iterative message passing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 5410–5419.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et al. (2018)</span>
<span class="ltx_bibblock">
Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin Choi. 2018.

</span>
<span class="ltx_bibblock">Neural motifs: Scene graph parsing with global context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 5831–5840.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2018)</span>
<span class="ltx_bibblock">
Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang,
Changcheng Li, and Maosong Sun. 2018.

</span>
<span class="ltx_bibblock">Graph neural networks: A review of methods and applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.08434</em>.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendices</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Semantic Types in GQA</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">GQA classifies its questions into 5 semantic types:</p>
<ol id="A1.I1" class="ltx_enumerate">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p"><span id="A1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Relation</span>: subject/object of a described relation, e.g. “What furniture is the beer can on?”</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p"><span id="A1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Attribute</span>: properties/position of an object, e.g. “What is the color of the man’s shirt?”</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p"><span id="A1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Object</span>: existence of objects in the image, e.g. “Are there any wine glasses or cups?”</p>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.1" class="ltx_p"><span id="A1.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Category</span>: object identification within a class, e.g. “Which kind of clothing is maroon?”</p>
</div>
</li>
<li id="A1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="A1.I1.i5.p1" class="ltx_para">
<p id="A1.I1.i5.p1.1" class="ltx_p"><span id="A1.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Global</span>: overall scene properties, e.g. “How’s the weather?”</p>
</div>
</li>
</ol>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2101.05478" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2101.05479" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2101.05479">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2101.05479" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2101.05480" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  8 00:13:58 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
