<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2101.05428] Federated Learning: Opportunities and Challenges</title><meta property="og:description" content="Federated Learning (FL) is a concept first introduced by Google in 2016, in which multiple devices collaboratively learn a machine learning model without sharing their private data under the supervision of a central se…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Learning: Opportunities and Challenges">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Learning: Opportunities and Challenges">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2101.05428">

<!--Generated on Fri Mar  1 14:20:23 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Federated Learning,  Distributed Systems">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Federated Learning: Opportunities and Challenges</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Priyanka Mary Mammen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">University of Massachusetts, Amherst</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_streetaddress">1 Thørväld Circle</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:pmammen@cs.umass.edu">pmammen@cs.umass.edu</a>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id3.id1" class="ltx_p">Federated Learning (FL) is a concept first introduced by Google in 2016, in which multiple devices collaboratively learn a machine learning model without sharing their private data under the supervision of a central server. This offers ample opportunities in critical domains such as healthcare, finance etc, where it is risky to share private user information to other organisations or devices. While FL appears to be a promising Machine Learning (ML) technique to keep the local data private, it is also vulnerable to attacks like other ML models. Given the growing interest in the FL domain, this report discusses the opportunities and challenges in federated learning.</p>
</div>
<div class="ltx_keywords">Federated Learning, Distributed Systems
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Machine learning</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Artificial Intelligence (AI)/Machine Learning (ML) started getting popular in the last few 4-5 years when AI beat humans in a board game named Alpha-Go<cite class="ltx_cite ltx_citemacro_citep">(Silver
et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2017</a>)</cite>. Availability of Big-data and powerful computing units further accelerated the adoption of Machine Learning technologies in domains such as finance, healthcare, transportation, customer services, e-commerce, smart home applications etc. With this widespread adoption of ML techniques, it is therefore important to ensure the security and privacy of the techniques. In most of the machine learning applications, data from various organizations or devices are aggregated in a central server or a cloud platform for training the model. This is a key limitation especially when the training data set contains sensitive information and therefore, poses security threats. For example, to develop a breast cancer detection model from MRI scans, different hospitals can share their data to develop a collaborated ML model. Whereas, sharing private patient information to a central server can reveal sensitive information to the public with several repercussions. In such scenarios, Federated Learning can be better option.Federated Learning is a collaborative learning technique among devices/organizations, where the model parameters from local models are shared and aggregated instead of sharing their local data.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The notion of Federated Learning is introduced by Google in 2016, where they first applied in google keyboard to collaboratively learn from several android phones <cite class="ltx_cite ltx_citemacro_citep">(McMahan and
Ramage, <a href="#bib.bib25" title="" class="ltx_ref">2017</a>)</cite>. Given that FL can be applied to any edge device, it has the potential to revolutionize critical domains such as healthcare, transportation, finance, smart home etc. The most prominent example is when the researchers and medical practitioners from different parts of the world collaboratively developed an AI pandemic engine for COVID-19 diagnosis from chest scans <cite class="ltx_cite ltx_citemacro_citep">(web, <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>. Another interesting application would be in transportation networks, for training the vehicles for autonomous driving and city route planning. Similarly for smart-home applications, edge devices in different homes can collaboratively learn on context aware policies using a federated learning framework <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2020a</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">While the applications are many, there are several challenges associated with federated learning. The challenges can be broadly classified into two: training-related challenges and security challenges. Training related challenges encompass the communication overhead during multiple training iterations, heterogeneity of the devices participating in the learning and heterogeneity of data used for training .Whereas security challenges include the privacy and security threats due to the presence of adversaries ranging from malicious clients in the local device to a malicious user who has only a black-box access to the model. In FL, although the private data does not leave the device, it might be still possible for an adversary or a curious observer to learn the presence of a data point used for training in the local models. In order to overcome this attack, some kind of cryptographic technique is required to keep the information differentially private <cite class="ltx_cite ltx_citemacro_citep">(Geyer
et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2017</a>)</cite>. Whereas security attacks can be mostly induced by the presence of malicious clients in the learning, and they can be either targeted or non-targeted. In targeted attacks, the adversary wants to manipulate the labels on specific tasks. Whereas in non-targeted attacks, the motivation of the adversary is just to compromise the accuracy of the global model. The defense mechanisms require to detect malicious devices and remove them from further learning or nullify the effect on the global model induced by the malicious devices <cite class="ltx_cite ltx_citemacro_citep">(Fang
et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Numerous research efforts have been undertaken in the last few years to fortify the Federated Learning domain and it has some effect on the performance parameters like accuracy, computational costs etc. Being a distributed system,its difficult to identify malicious participants in FL. FL domain has grown so far from being a small application introduced by Google. Researchers have introduced various FL architectures, incentive mechanisms to foster user participation, cloud services for FL etc. Motivated by this growing interest in the Federated Learning domain, we present this survey paper. The recent works <cite class="ltx_cite ltx_citemacro_citep">(Kairouz
et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2019</a>; Yang
et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2019</a>; Mothukuri et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2020</a>; Aledhari
et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite> are focused either on different federated learning architecture or on different challenges in FL domain. Whereas so many interesting advancements are taking place beyond applying it to new application domains and fostering the security aspects Therefore, in this survey we also try to cover the recent developments along with providing give a general overview on Federated Learning applications and security concerns in the domain.</p>
</div>
<figure id="S1.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2101.05428/assets/images/FL1.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="252" height="123" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Overview of Federated Learning across devices.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2101.05428/assets/images/health1.png" id="S1.F2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="240" height="197" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Overview of Federated Learning across organisations</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Overview of Federated Learning</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">A simple representation of Federated Learning is shown in fig. <a href="#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ Federated Learning: Opportunities and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and fig. <a href="#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ Federated Learning: Opportunities and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Federated Learning has primarily four main steps:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Client Selection/Sampling : Server either randomly picks desired participants from a pool of devices or use some algorithm for client selection. <cite class="ltx_cite ltx_citemacro_citep">(Zhan
et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2020</a>; Khan et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2020</a>; Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2020b</a>)</cite> talk about some client selection techniques for FL.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Parameter Broadcasting: Server broadcasts the global model parameters to the selected clients</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">Local Model Training: The clients will parallelly retrain the models using their local data.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p">Model Aggregation: Clients will send back their local model parameters to the server and model parameters will be aggregated towards the global model.</p>
</div>
</li>
</ul>
<p id="S2.p1.2" class="ltx_p">The above steps will be repeated in an iterative manner for n times as desired.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Types of Federated Learning</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we introduce different types of Federated Learning frameworks.</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Vertical Federated Learning -Vertical Federated Learning is used for cases in which each device contains dataset with different features but from sample instances. For instance, two organisations have data about the same group of people with different feature set can use Vertical FL to build a shared ML model.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Horizontal Federated Learning - Horizontal Federated Learning is used for cases in which each device contains dataset with the same feature space but with different sample instances. The first use case of FL- Google keyboard uses this type of learning in which the participating mobile phones have different training data with same features.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Federated Transfer Learning - Federated Transfer learning is similar to the traditional Machine Learning, where we want to add a new feature on a pre-trained model. The best example would be for giving an extension to the vertical federated learning - If we want to extend the ML to more number of sample instances which are not present in all of the collaborating organisations.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Cross-Silo Federated Learning - Cross- Silo Federated Learning is used when the participating devices are less in number and available for all rounds. The training data can be in horizontal or vertical FL format. Mostly cross-silo is used for cases with organisations. Works such as <cite class="ltx_cite ltx_citemacro_citep">(Zhang
et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2020</a>)</cite> use cross-silo FL to develop their model.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">Cross-Device Federated Learning - Scenarios with a large number of participating devices use Cross-device Federated Learning. Client-selection and incentive designs<cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2020b</a>)</cite> are some notable techniques needed to facilitate this type of FL.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Applications</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Healthcare</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Electronic Health Records (EHR) is considered as the main source of healthcare data for machine learning applications <cite class="ltx_cite ltx_citemacro_citep">(Ghassemi et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>. If ML models are trained only using the limited data available in a single hospital, it might introduce some amount of bias in the predictions. Thus, to make the models more generalizable, it requires training with more data, which can be realized by sharing data among organizations. Given the sensitive nature of the healthcare data, it might not be feasible to share the electronic health records of patients among hospitals. In such situations, federated learning can serve as an option for building a collaborative learning model for healthcare data.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Transportation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">With the increase in the ubiquity of sensors in vehicular networks, it is feasible to capture more data and train ML models. Machine Learning based models are generally applied to both vehicle management and traffic management <cite class="ltx_cite ltx_citemacro_citep">(Tan
et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite>. The current autonomous driving decisions are limited by the dynamic nature of the surroundings as the training is carried out offline. FL can rescue such situations by online training vehicles from different geographical locations which can facilitate accurate labelling of the features. Similarly for traffic flow prediction techniques, a large amount of data is required, but most of the data is divided among various organizations and cannot be exchanged to protect the privacy <cite class="ltx_cite ltx_citemacro_citep">(Liu
et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite>. To address such situations also, we can deploy FL methods.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Finance</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">One best use of federated learning in finance is in the banking sector, for loan risk assessment <cite class="ltx_cite ltx_citemacro_citep">(Cheng
et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>. Normally banks use white-listing techniques to rule out the customers using their credit card reports from the central banks. Factors such as taxation, reputation etc can also be utilized for risk management by collaborating with other finance institutions and e-commerce companies. As it is risky to share private information of customers among organizations, they can make use of FL to build a risk assessment ML model.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Natural Language Processing</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Natural Language Processing (NLP) is one of the most common applications which is built on machine learning models. It helps us understand human language semantics in a better way. However, it requires huge amount of data to train highly accurate language models. This data can be easily gathered from mobile phones, tablets etc. Again, privacy comes as a bottleneck here for centralized language learning models, as the textual information from each edge device contains user information. In <cite class="ltx_cite ltx_citemacro_citep">(Garcia Bernal, <a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>, the authors have shown that it is feasible to build NLP models using a FL framework.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Training Bottlenecks</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Being a distributed system, Federated Learning faces several challenges during the training time.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Communication Overheads</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Communication overheads is one of the major bottlenecks in federated learning. Existing works try to solve this by either data compression <cite class="ltx_cite ltx_citemacro_citep">(Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2016</a>)</cite> or by allowing only the relevant outputs by the clients<cite class="ltx_cite ltx_citemacro_citep">(Hsieh et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2017</a>; Luping
et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite> to be sent back to the central server.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Systems and Data Heterogeneity</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The heterogeneity of the systems in the network as well as the non-identically distributed data from the devices affect the performance of the FL model <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2017</a>; Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite>. Although, FedAvg is introduced as a method to tackle the heterogeneity, it is still not robust enough to systems heterogeneity. Works such as <cite class="ltx_cite ltx_citemacro_citep">(Liu
et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2020</a>; Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite> try to address this problem by modifying the model aggregation methods.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Privacy and Security Concerns</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Like any machine learning model, Federated Learning models are also prone to attacks. The attacks can be introduced by a compromised central server or compromised local devices in the learning framework or by any participant in the FL workflow. Attacks in the context of FL will be discussed in this section.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Membership Inference Attacks</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">Although the raw user data does not leave the local device, there are still many ways to infer the training data used in FL. For instance, in some scenarios, it is possible to infer the information about the training data from the model updates during the learning process. The defense measure looks for mechanism offering a differential privacy guarantee. The most common techniques are secure computation, differential privacy schemes and running in a trusted execution environment.</p>
</div>
<section id="S6.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1. </span>Defense Mechanism 1: Secure Computation</h4>

<div id="S6.SS1.SSS1.p1" class="ltx_para">
<p id="S6.SS1.SSS1.p1.1" class="ltx_p">Two main techniques come under Secure Computation: Secure Multiparty Computation (SMC) and Homomorphic Encryption. In SMC, two or more parties agree to perform the inputs provided by the participants and reveal the outputs only to a subset of participants. Whereas in homomorphic encryption, computations are performed on encrypted inputs without decrypting first.</p>
</div>
</section>
<section id="S6.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2. </span>Defense Mechanism 2: Differential Privacy (DP)</h4>

<div id="S6.SS1.SSS2.p1" class="ltx_para">
<p id="S6.SS1.SSS2.p1.1" class="ltx_p">In differential privacy schemes, the contribution of a user is masked by adding noise to the clipped model parameters before model aggregation <cite class="ltx_cite ltx_citemacro_citep">(Geyer
et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2017</a>)</cite>. Some amount of model accuracy will be lost by adding noise to the parameters.</p>
</div>
</section>
<section id="S6.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.3. </span>Defense Mechanism 3:Trusted Execution Environment</h4>

<div id="S6.SS1.SSS3.p1" class="ltx_para">
<p id="S6.SS1.SSS3.p1.1" class="ltx_p">Trusted Execution Environment(TEE) provides a secure platform to run the federated learning process with low computational overhead when compared to secure computation techniques <cite class="ltx_cite ltx_citemacro_citep">(Mo and Haddadi, <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>. The current TEE environment is suitable only to CPU devices.</p>
</div>
</section>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Data Poisoning Attacks</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">Data Poisoning Attacks are the most common attacks against ML models. In order to launch a data poisoning attack in FL model, adversary poison the training data in a certain number of devices participating in the learning process so that the global model accuracy is compromised. The adversary can poison the data either by directly injecting poisoned data to the targeted device or injecting poisoned data through other devices <cite class="ltx_cite ltx_citemacro_citep">(Sun
et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite>. Such attacks can be either targeted or non-targeted. By targeted it means, the adversaries want to influence on the prediction of a subset of classes while deteriorating the global model accuracy<cite class="ltx_cite ltx_citemacro_citep">(Tolpegin
et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<section id="S6.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1. </span>Defense Mechanisms</h4>

<div id="S6.SS2.SSS1.p1" class="ltx_para">
<p id="S6.SS2.SSS1.p1.1" class="ltx_p">The defense mechanism against such attacks is to identify the malicious participants based on their model updates before model averaging in each round of learning.</p>
</div>
</section>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Model Poisoning Attacks</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">Model Poisoning attacks are similar to data poisoning attacks, where the adversary tries to poison the local models instead of the local data. The major motivation behind the model poisoning attack is to introduce errors in the global model. The adversary launches a model poisoning attack by compromising some of the devices and modifying it’s local model parameters so that the accuracy of the global model is affected.</p>
</div>
<section id="S6.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.1. </span>Defense Mechanisms</h4>

<div id="S6.SS3.SSS1.p1" class="ltx_para">
<p id="S6.SS3.SSS1.p1.1" class="ltx_p">The defenses against model poisoning attacks are similar to data poisoning attacks. The most common defense measures are rejections based on Error rate and Loss function <cite class="ltx_cite ltx_citemacro_citep">(Fang
et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>. In error rate based rejections, the models with significant impact on the error rate on the global model will be rejected. Whereas in loss function based rejections, the models will be rejected based on their impact on loss function of the global model. In some cases, rejections can be made by combining both error based rejections and loss function based rejections.</p>
</div>
</section>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4. </span>Backdoor Attacks</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">Secure averaging in federated learning lets the devices to be anonymous during the model updating process. Using the same functionality, a device or a group of devices can introduce a backdoor functionality in the global model of federated learning <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>. Using a backdoor, an adversary can mislabel certain tasks without affecting the accuracy of the global model. For instance, an attacker can choose a specific label for a data instance with specific characteristics. Backdoor attacks are also known as targeted attacks. The Intensity of such attacks depends on the proportion of the compromised devices present and model capacity of federated learning <cite class="ltx_cite ltx_citemacro_citep">(Sun
et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite> .</p>
</div>
<section id="S6.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.4.1. </span>Defense Mechanisms</h4>

<div id="S6.SS4.SSS1.p1" class="ltx_para">
<p id="S6.SS4.SSS1.p1.1" class="ltx_p">: The defense against backdoor attacks is either weak differential privacy or norm thresholding of updates. Participant level differential privacy can serve as a form of defense against such attacks but at the cost of performance of the global model <cite class="ltx_cite ltx_citemacro_citep">(Geyer
et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2017</a>)</cite>. Whereas norm thresholding can be applied to remove models with boosted model parameters. Even with this defense measures, it is hard to find the malicious participants owing to the secure aggregation techniques and capacity of the deep learning model.
Also FL framework being a distributed system, it might be even harder to manage the randomly misbehaving devices.</p>
</div>
</section>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Recent Developments in FL</h2>

<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>One-shot federated Learning</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">In most of the federated learning frameworks, there will be multiple rounds of communication between devices and the central server, which increases the communication overheads. Recently, there is a growing interest in one-shot federated learning which is first introduced by <cite class="ltx_cite ltx_citemacro_citep">(Guha
et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite>, where the global model is learned in a single round of communication. In order to overcome communication overheads of sending bulky gradients, <cite class="ltx_cite ltx_citemacro_citep">(Zhou
et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2020</a>)</cite> proposes a distilled one-shot federated learning, where each device distills their data and send the fabricated data to the central server. The server then learns the global model by training over the combined data from all the devices.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2. </span>Incentive Mechanisms</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">Current FL approaches work under the assumption that devices will cooperate in the learning process whenever required without considering the rewards. Whereas in actual practice, devices or clients must be economically compensated for their participation. To encourage/improve device participation in FL, works such as <cite class="ltx_cite ltx_citemacro_citep">(Kang
et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2019</a>; Zhan
et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2020</a>; Khan et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2020</a>; Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2020b</a>)</cite> propose a reputation based incentive mechanism i.e, devices get rewards based on their model accuracy, data reliability and contribution to the global model. However, these works did not talk about how to model convergence and additional communication overheads induced into the framework.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3. </span>Federated Learning as a Service</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">Machine Learning as a Service is getting popular these days and most of them offer only centralized services. In order to offer Federated Learning as a cloud service, it should consider collaboration among third party applications. A recent work <cite class="ltx_cite ltx_citemacro_citep">(Kourtellis
et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite> tried to develop a FL framework (as a service) which allows applications of third parties to contribute and collaborate on a ML model. The framework is claimed to be suitable for any operation environment as well.</p>
</div>
</section>
<section id="S7.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4. </span>Asynchronous Federated Learning</h3>

<div id="S7.SS4.p1" class="ltx_para">
<p id="S7.SS4.p1.1" class="ltx_p">Most of the current FL aggregation techniques are designed for devices working in a synchronized manner. However, due to systems and data heterogeneity, training and model transfer occur in a asynchronized manner. Therefore it might not be feasible to scale federated optimization in an synchronized manner <cite class="ltx_cite ltx_citemacro_citep">(Xie
et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2019</a>)</cite>. Works such as <cite class="ltx_cite ltx_citemacro_citep">(Sprague et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2018</a>; van Dijk et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2020</a>; Xie
et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2019</a>)</cite> talks about carrying out federated learning in an asynchronous environment. Compared to FedAvg (working in a synchronized manner), asynchronous Federated averaging techniques can handle more devices and allows updates to come at a time.</p>
</div>
</section>
<section id="S7.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5. </span>Blockchain in FL</h3>

<figure id="S7.F3" class="ltx_figure"><img src="/html/2101.05428/assets/images/blockchain1.png" id="S7.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="252" height="225" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>A sample architecture for federated learning over blockchain.</figcaption>
</figure>
<div id="S7.SS5.p1" class="ltx_para">
<p id="S7.SS5.p1.1" class="ltx_p">An aggregator is necessary to update the global model managing the asynchronous arrival of parameters from the devices. This can be a constraint for the widespread adoption for the FL models. As blockchain is a decentralized network, devices can collaboratively learn without the central aggregator. Works such as <cite class="ltx_cite ltx_citemacro_citep">(Ramanan
et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2019</a>; Kumar et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2020</a>; Desai
et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2020</a>; Bao
et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite> propose Federated Learning in a block-chain framework. A sample architecture <cite class="ltx_cite ltx_citemacro_citep">(Desai
et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> is shown in fig <a href="#S7.F3" title="Figure 3 ‣ 7.5. Blockchain in FL ‣ 7. Recent Developments in FL ‣ Federated Learning: Opportunities and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Final Remarks </h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">Federated Learning offers a secure collaborative machine learning framework for different devices without sharing their private data. This attracted a lot of researchers and there is extensive research happening in this domain. Federated Learning has been applied in several domains such as healthcare, transportation etc. Although FL frameworks offer a better privacy guarantee than other ML frameworks, it is still prone to several attacks. The distributed nature of the framework makes it even harder to deploy defense measures. For instance,the gaussian noise added to the local models (for DP) can confuse the aggregation schemes and may result in leaving out the benign participants (while applying model poisoning defense measures). So therefore an interesting research question to pursue will be:
Is it possible to develop a byzantine tolerant FL model while ensuring user privacy using schemes with low computational cost?</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
I would like to thank my course advisor, Dr. Amir Houmansadr for his invaluable guidance and support.

</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">web (2020)</span>
<span class="ltx_bibblock">
2020.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">AI pandemic engine</em>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://hai.stanford.edu/blog/pandemic-ai-engine-without-borders" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://hai.stanford.edu/blog/pandemic-ai-engine-without-borders</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aledhari
et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Mohammed Aledhari, Rehma
Razzak, Reza M Parizi, and Fahad
Saeed. 2020.

</span>
<span class="ltx_bibblock">Federated learning: A survey on enabling
technologies, protocols, and applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">IEEE Access</em> 8
(2020), 140699–140725.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bagdasaryan et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Eugene Bagdasaryan,
Andreas Veit, Yiqing Hua,
Deborah Estrin, and Vitaly Shmatikov.
2020.

</span>
<span class="ltx_bibblock">How to backdoor federated learning. In
<em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence
and Statistics</em>. PMLR, 2938–2948.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao
et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Xianglin Bao, Cheng Su,
Yan Xiong, Wenchao Huang, and
Yifei Hu. 2019.

</span>
<span class="ltx_bibblock">Flchain: A blockchain for auditable federated
learning with trust and incentive. In <em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">2019 5th
International Conference on Big Data Computing and Communications (BIGCOM)</em>.
IEEE, 151–159.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al<span id="bib.bib6.3.3.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Keith Bonawitz, Hubert
Eichner, Wolfgang Grieskamp, Dzmitry
Huba, Alex Ingerman, Vladimir Ivanov,
Chloe Kiddon, Jakub Konečnỳ,
Stefano Mazzocchi, H Brendan McMahan,
et al<span id="bib.bib6.4.1" class="ltx_text">.</span> 2019.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: System
design.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.01046</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng
et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yong Cheng, Yang Liu,
Tianjian Chen, and Qiang Yang.
2020.

</span>
<span class="ltx_bibblock">Federated learning for privacy-preserving AI.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">Commun. ACM</em> 63,
12 (2020), 33–36.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Desai
et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Harsh Bimal Desai,
Mustafa Safa Ozdayi, and Murat
Kantarcioglu. 2020.

</span>
<span class="ltx_bibblock">BlockFLA: Accountable Federated Learning via Hybrid
Blockchain Architecture.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.07427</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang
et al<span id="bib.bib9.6.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Minghong Fang, Xiaoyu
Cao, Jinyuan Jia, and Neil Gong.
2020.

</span>
<span class="ltx_bibblock">Local model poisoning attacks to Byzantine-robust
federated learning. In <em id="bib.bib9.4.4" class="ltx_emph ltx_font_italic">29th <math id="bib.bib9.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib9.1.1.m1.1a"><mo stretchy="false" id="bib.bib9.1.1.m1.1.1" xref="bib.bib9.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib9.1.1.m1.1b"><ci id="bib.bib9.1.1.m1.1.1.cmml" xref="bib.bib9.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib9.1.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib9.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib9.2.2.m2.1a"><mo stretchy="false" id="bib.bib9.2.2.m2.1.1" xref="bib.bib9.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib9.2.2.m2.1b"><ci id="bib.bib9.2.2.m2.1.1.cmml" xref="bib.bib9.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib9.2.2.m2.1c">\}</annotation></semantics></math>
Security Symposium (<math id="bib.bib9.3.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib9.3.3.m3.1a"><mo stretchy="false" id="bib.bib9.3.3.m3.1.1" xref="bib.bib9.3.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib9.3.3.m3.1b"><ci id="bib.bib9.3.3.m3.1.1.cmml" xref="bib.bib9.3.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib9.3.3.m3.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib9.4.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib9.4.4.m4.1a"><mo stretchy="false" id="bib.bib9.4.4.m4.1.1" xref="bib.bib9.4.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib9.4.4.m4.1b"><ci id="bib.bib9.4.4.m4.1.1.cmml" xref="bib.bib9.4.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib9.4.4.m4.1c">\}</annotation></semantics></math> Security 20)</em>.
1605–1622.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garcia Bernal (2020)</span>
<span class="ltx_bibblock">
Daniel Garcia Bernal.
2020.

</span>
<span class="ltx_bibblock">Decentralizing Large-Scale Natural Language
Processing with Federated Learning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geyer
et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Robin C Geyer, Tassilo
Klein, and Moin Nabi. 2017.

</span>
<span class="ltx_bibblock">Differentially private federated learning: A client
level perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1712.07557</em>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghassemi et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Marzyeh Ghassemi, Tristan
Naumann, Peter Schulam, Andrew L Beam,
Irene Y Chen, and Rajesh Ranganath.
2020.

</span>
<span class="ltx_bibblock">A Review of Challenges and Opportunities in Machine
Learning for Health.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">AMIA Summits on Translational Science
Proceedings</em> 2020 (2020),
191.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guha
et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Neel Guha, Ameet
Talwalkar, and Virginia Smith.
2019.

</span>
<span class="ltx_bibblock">One-shot federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.11175</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsieh et al<span id="bib.bib14.8.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Kevin Hsieh, Aaron
Harlap, Nandita Vijaykumar, Dimitris
Konomis, Gregory R Ganger, Phillip B
Gibbons, and Onur Mutlu.
2017.

</span>
<span class="ltx_bibblock">Gaia: Geo-distributed machine learning approaching
<math id="bib.bib14.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib14.1.m1.1a"><mo stretchy="false" id="bib.bib14.1.m1.1.1" xref="bib.bib14.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib14.1.m1.1b"><ci id="bib.bib14.1.m1.1.1.cmml" xref="bib.bib14.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib14.1.m1.1c">\{</annotation></semantics></math>LAN<math id="bib.bib14.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib14.2.m2.1a"><mo stretchy="false" id="bib.bib14.2.m2.1.1" xref="bib.bib14.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib14.2.m2.1b"><ci id="bib.bib14.2.m2.1.1.cmml" xref="bib.bib14.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib14.2.m2.1c">\}</annotation></semantics></math> speeds. In <em id="bib.bib14.6.4" class="ltx_emph ltx_font_italic">14th <math id="bib.bib14.3.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib14.3.1.m1.1a"><mo stretchy="false" id="bib.bib14.3.1.m1.1.1" xref="bib.bib14.3.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib14.3.1.m1.1b"><ci id="bib.bib14.3.1.m1.1.1.cmml" xref="bib.bib14.3.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib14.3.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib14.4.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib14.4.2.m2.1a"><mo stretchy="false" id="bib.bib14.4.2.m2.1.1" xref="bib.bib14.4.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib14.4.2.m2.1b"><ci id="bib.bib14.4.2.m2.1.1.cmml" xref="bib.bib14.4.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib14.4.2.m2.1c">\}</annotation></semantics></math>
Symposium on Networked Systems Design and Implementation (<math id="bib.bib14.5.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib14.5.3.m3.1a"><mo stretchy="false" id="bib.bib14.5.3.m3.1.1" xref="bib.bib14.5.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib14.5.3.m3.1b"><ci id="bib.bib14.5.3.m3.1.1.cmml" xref="bib.bib14.5.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib14.5.3.m3.1c">\{</annotation></semantics></math>NSDI<math id="bib.bib14.6.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib14.6.4.m4.1a"><mo stretchy="false" id="bib.bib14.6.4.m4.1.1" xref="bib.bib14.6.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib14.6.4.m4.1b"><ci id="bib.bib14.6.4.m4.1.1.cmml" xref="bib.bib14.6.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib14.6.4.m4.1c">\}</annotation></semantics></math> 17)</em>.
629–647.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz
et al<span id="bib.bib15.3.3.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan
McMahan, Brendan Avent, Aurélien
Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary
Charles, Graham Cormode, Rachel
Cummings, et al<span id="bib.bib15.4.1" class="ltx_text">.</span> 2019.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.04977</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang
et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Jiawen Kang, Zehui Xiong,
Dusit Niyato, Han Yu,
Ying-Chang Liang, and Dong In Kim.
2019.

</span>
<span class="ltx_bibblock">Incentive design for efficient federated learning
in mobile networks: A contract theory approach. In
<em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">2019 IEEE VTS Asia Pacific Wireless Communications
Symposium (APWCS)</em>. IEEE, 1–5.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Latif U Khan, Shashi Raj
Pandey, Nguyen H Tran, Walid Saad,
Zhu Han, Minh NH Nguyen, and
Choong Seon Hong. 2020.

</span>
<span class="ltx_bibblock">Federated learning for edge networks: Resource
optimization and incentive mechanism.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">IEEE Communications Magazine</em>
58, 10 (2020),
88–93.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečnỳ et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Jakub Konečnỳ,
H Brendan McMahan, Felix X Yu,
Peter Richtárik, Ananda Theertha
Suresh, and Dave Bacon.
2016.

</span>
<span class="ltx_bibblock">Federated learning: Strategies for improving
communication efficiency.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.05492</em>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kourtellis
et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Nicolas Kourtellis,
Kleomenis Katevas, and Diego Perino.
2020.

</span>
<span class="ltx_bibblock">FLaaS: Federated Learning as a Service.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.09359</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Rajesh Kumar,
Abdullah Aman Khan, Sinmin Zhang,
WenYong Wang, Yousif Abuidris,
Waqas Amin, and Jay Kumar.
2020.

</span>
<span class="ltx_bibblock">Blockchain-federated-learning and deep learning
models for covid-19 detection using ct imaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.06537</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu,
Ameet Talwalkar, and Virginia Smith.
2020.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future
directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>
37, 3 (2020),
50–60.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu
et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yi Liu, JQ James,
Jiawen Kang, Dusit Niyato, and
Shuyu Zhang. 2020.

</span>
<span class="ltx_bibblock">Privacy-preserving Traffic Flow Prediction: A
Federated Learning Approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luping
et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
WANG Luping, WANG Wei,
and LI Bo. 2019.

</span>
<span class="ltx_bibblock">Cmfl: Mitigating communication overhead for
federated learning. In <em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">2019 IEEE 39th
International Conference on Distributed Computing Systems (ICDCS)</em>. IEEE,
954–964.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider
Moore, Daniel Ramage, Seth Hampson,
and Blaise Aguera y Arcas.
2017.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks
from decentralized data. In <em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">Artificial
Intelligence and Statistics</em>. PMLR, 1273–1282.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan and
Ramage (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan and
Daniel Ramage. 2017.

</span>
<span class="ltx_bibblock">Federated learning: Collaborative machine learning
without centralized training data.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Google Research Blog</em> 3
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mo and Haddadi (2019)</span>
<span class="ltx_bibblock">
Fan Mo and Hamed
Haddadi. 2019.

</span>
<span class="ltx_bibblock">Efficient and private federated learning using
tee. In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">EuroSys</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mothukuri et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Viraaji Mothukuri, Reza M
Parizi, Seyedamin Pouriyeh, Yan Huang,
Ali Dehghantanha, and Gautam
Srivastava. 2020.

</span>
<span class="ltx_bibblock">A survey on security and privacy of federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">Future Generation Computer Systems</em>
115 (2020), 619–640.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramanan
et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Paritosh Ramanan, Kiyoshi
Nakayama, and Ratnesh Sharma.
2019.

</span>
<span class="ltx_bibblock">BAFFLE: Blockchain based aggregator free federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.07452</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silver
et al<span id="bib.bib29.3.3.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
David Silver, Julian
Schrittwieser, Karen Simonyan, Ioannis
Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker,
Matthew Lai, Adrian Bolton,
et al<span id="bib.bib29.4.1" class="ltx_text">.</span> 2017.

</span>
<span class="ltx_bibblock">Mastering the game of go without human knowledge.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.5.1" class="ltx_emph ltx_font_italic">nature</em> 550,
7676 (2017), 354–359.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sprague et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Michael R Sprague, Amir
Jalalirad, Marco Scavuzzo, Catalin
Capota, Moritz Neun, Lyman Do, and
Michael Kopp. 2018.

</span>
<span class="ltx_bibblock">Asynchronous federated learning for geospatial
applications. In <em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">Joint European Conference on
Machine Learning and Knowledge Discovery in Databases</em>. Springer,
21–28.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun
et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Gan Sun, Yang Cong,
Jiahua Dong, Qiang Wang, and
Ji Liu. 2020.

</span>
<span class="ltx_bibblock">Data Poisoning Attacks on Federated Machine
Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.10020</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun
et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Ziteng Sun, Peter
Kairouz, Ananda Theertha Suresh, and
H Brendan McMahan. 2019.

</span>
<span class="ltx_bibblock">Can you really backdoor federated learning?

</span>
<span class="ltx_bibblock"><em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.07963</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan
et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Kang Tan, Duncan Bremner,
Julien Le Kernec, and Muhammad Imran.
2020.

</span>
<span class="ltx_bibblock">Federated Machine Learning in Vehicular Networks: A
summary of Recent Applications. In <em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">2020
International Conference on UK-China Emerging Technologies (UCET)</em>. IEEE,
1–4.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tolpegin
et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Vale Tolpegin, Stacey
Truex, Mehmet Emre Gursoy, and Ling
Liu. 2020.

</span>
<span class="ltx_bibblock">Data Poisoning Attacks Against Federated Learning
Systems. In <em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">European Symposium on Research in
Computer Security</em>. Springer, 480–501.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van Dijk et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Marten van Dijk, Nhuong V
Nguyen, Toan N Nguyen, Lam M Nguyen,
Quoc Tran-Dinh, and Phuong Ha Nguyen.
2020.

</span>
<span class="ltx_bibblock">Asynchronous Federated Learning with Reduced Number
of Rounds and with Differential Privacy from Less Aggregated Gaussian Noise.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.09208</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie
et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Cong Xie, Sanmi Koyejo,
and Indranil Gupta. 2019.

</span>
<span class="ltx_bibblock">Asynchronous federated optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1903.03934</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang
et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu,
Tianjian Chen, and Yongxin Tong.
2019.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and
applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and
Technology (TIST)</em> 10, 2
(2019), 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Han Yu, Zelei Liu,
Yang Liu, Tianjian Chen,
Mingshu Cong, Xi Weng,
Dusit Niyato, and Qiang Yang.
2020b.

</span>
<span class="ltx_bibblock">A Sustainable Incentive Scheme for Federated
Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.3.1" class="ltx_emph ltx_font_italic">IEEE Intelligent Systems</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Tianlong Yu, Tian Li,
Yuqiong Sun, Susanta Nanda,
Virginia Smith, Vyas Sekar, and
Srinivasan Seshan. 2020a.

</span>
<span class="ltx_bibblock">Learning Context-Aware Policies from Multiple Smart
Homes via Federated Multi-Task Learning. In <em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">2020
IEEE/ACM Fifth International Conference on Internet-of-Things Design and
Implementation (IoTDI)</em>. IEEE, 104–115.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhan
et al<span id="bib.bib40.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yufeng Zhan, Peng Li,
Zhihao Qu, Deze Zeng, and
Song Guo. 2020.

</span>
<span class="ltx_bibblock">A learning-based incentive mechanism for federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.3.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
et al<span id="bib.bib41.8.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Chengliang Zhang, Suyi
Li, Junzhe Xia, Wei Wang,
Feng Yan, and Yang Liu.
2020.

</span>
<span class="ltx_bibblock">Batchcrypt: Efficient homomorphic encryption for
cross-silo federated learning. In <em id="bib.bib41.6.6" class="ltx_emph ltx_font_italic">2020
<math id="bib.bib41.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib41.1.1.m1.1a"><mo stretchy="false" id="bib.bib41.1.1.m1.1.1" xref="bib.bib41.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib41.1.1.m1.1b"><ci id="bib.bib41.1.1.m1.1.1.cmml" xref="bib.bib41.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib41.1.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib41.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib41.2.2.m2.1a"><mo stretchy="false" id="bib.bib41.2.2.m2.1.1" xref="bib.bib41.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib41.2.2.m2.1b"><ci id="bib.bib41.2.2.m2.1.1.cmml" xref="bib.bib41.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib41.2.2.m2.1c">\}</annotation></semantics></math> Annual Technical Conference (<math id="bib.bib41.3.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib41.3.3.m3.1a"><mo stretchy="false" id="bib.bib41.3.3.m3.1.1" xref="bib.bib41.3.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib41.3.3.m3.1b"><ci id="bib.bib41.3.3.m3.1.1.cmml" xref="bib.bib41.3.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib41.3.3.m3.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib41.4.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib41.4.4.m4.1a"><mo stretchy="false" id="bib.bib41.4.4.m4.1.1" xref="bib.bib41.4.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib41.4.4.m4.1b"><ci id="bib.bib41.4.4.m4.1.1.cmml" xref="bib.bib41.4.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib41.4.4.m4.1c">\}</annotation></semantics></math><math id="bib.bib41.5.5.m5.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib41.5.5.m5.1a"><mo stretchy="false" id="bib.bib41.5.5.m5.1.1" xref="bib.bib41.5.5.m5.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib41.5.5.m5.1b"><ci id="bib.bib41.5.5.m5.1.1.cmml" xref="bib.bib41.5.5.m5.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib41.5.5.m5.1c">\{</annotation></semantics></math>ATC<math id="bib.bib41.6.6.m6.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib41.6.6.m6.1a"><mo stretchy="false" id="bib.bib41.6.6.m6.1.1" xref="bib.bib41.6.6.m6.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib41.6.6.m6.1b"><ci id="bib.bib41.6.6.m6.1.1.cmml" xref="bib.bib41.6.6.m6.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib41.6.6.m6.1c">\}</annotation></semantics></math> 20)</em>.
493–506.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou
et al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yanlin Zhou, George Pu,
Xiyao Ma, Xiaolin Li, and
Dapeng Wu. 2020.

</span>
<span class="ltx_bibblock">Distilled One-Shot Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.07999</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2101.05427" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2101.05428" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2101.05428">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2101.05428" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2101.05429" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 14:20:23 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
