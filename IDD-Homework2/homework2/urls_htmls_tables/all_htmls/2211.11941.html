<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2211.11941] Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft</title><meta property="og:description" content="Images of spacecraft photographed from other spacecraft operating in outer space are difficult to come by, especially at a scale typically required for deep learning tasks. Semantic image segmentation, object detection‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2211.11941">

<!--Generated on Thu Mar 14 09:19:02 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">William S.¬†Armstrong
<br class="ltx_break">Stanford Center for Professional Development
<br class="ltx_break">Stanford University
<br class="ltx_break">Stanford
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> CA
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> 94305
<br class="ltx_break">wsa@nx-2.com
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Spencer Drakontaidis
<br class="ltx_break">Department of Computer Science
<br class="ltx_break">Stanford University
<br class="ltx_break">Stanford
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> CA
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> 94305
<br class="ltx_break">spencer@drakontaidis.com
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nicholas Lui
<br class="ltx_break">Department of Statistics
<br class="ltx_break">Stanford University
<br class="ltx_break">Stanford
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> CA
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> 94305
<br class="ltx_break">niclui@stanford.edu
</span><span class="ltx_author_notes"><span id="id1.1.1" class="ltx_text" style="font-size:80%;">978-1-6654-9032-0/23/<math id="id1.1.1.m1.1" class="ltx_Math" alttext="\$31.00" display="inline"><semantics id="id1.1.1.m1.1a"><mrow id="id1.1.1.m1.1.1" xref="id1.1.1.m1.1.1.cmml"><mo rspace="0.167em" id="id1.1.1.m1.1.1.1" xref="id1.1.1.m1.1.1.1.cmml">$</mo><mn id="id1.1.1.m1.1.1.2" xref="id1.1.1.m1.1.1.2.cmml">31.00</mn></mrow><annotation-xml encoding="MathML-Content" id="id1.1.1.m1.1b"><apply id="id1.1.1.m1.1.1.cmml" xref="id1.1.1.m1.1.1"><csymbol cd="latexml" id="id1.1.1.m1.1.1.1.cmml" xref="id1.1.1.m1.1.1.1">currency-dollar</csymbol><cn type="float" id="id1.1.1.m1.1.1.2.cmml" xref="id1.1.1.m1.1.1.2">31.00</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.m1.1c">\$31.00</annotation></semantics></math> ¬©2023 IEEE</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Images of spacecraft photographed from other spacecraft operating in outer space are difficult to come by, especially at a scale typically required for deep learning tasks. Semantic image segmentation, object detection and localization, and pose estimation are well researched areas with powerful results for many applications, and would be very useful in autonomous spacecraft operation and rendezvous. However, recent studies show that these strong results in broad and common domains may generalize poorly even to specific industrial applications on earth. To address this, we propose a method for generating synthetic image data that are labelled for semantic segmentation, generalizable to other tasks, and provide a prototype synthetic image dataset consisting of 2D monocular images of unmanned spacecraft, in order to enable further research in the area of autonomous spacecraft rendezvous. We also present a strong benchmark result (S√∏rensen-Dice coefficient 0.8723) on these synthetic data, suggesting that it is feasible to train well-performing image segmentation models for this task, especially if the target spacecraft and its configuration are known.</p>
</div>
<nav class="ltx_TOC ltx_list_toc ltx_toc_toc"><h6 class="ltx_title ltx_title_contents">Contents</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S1" title="In Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S1.SS1" title="In 1 Introduction ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Related work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S2" title="In Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS1" title="In 2 Dataset ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>3D models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS2" title="In 2 Dataset ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Class labels</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS3" title="In 2 Dataset ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Procedural image generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS4" title="In 2 Dataset ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Ground truth representation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS5" title="In 2 Dataset ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Unknown target test set</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S3" title="In Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Baseline methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS1" title="In 3 Baseline methods ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>U-Net</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS2" title="In 3 Baseline methods ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>HRNet</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS3" title="In 3 Baseline methods ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>DeepLab</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S3.SS4" title="In 3 Baseline methods ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Loss functions</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S3.SS4.SSS1" title="In 3.4 Loss functions ‚Ä£ 3 Baseline methods ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Categorical cross-entropy loss</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S3.SS4.SSS2" title="In 3.4 Loss functions ‚Ä£ 3 Baseline methods ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Dice loss</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S3.SS4.SSS3" title="In 3.4 Loss functions ‚Ä£ 3 Baseline methods ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.3 </span>Dice + focal loss</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S4" title="In Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S4.SS1" title="In 4 Experiments ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Model selection criterion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S4.SS2" title="In 4 Experiments ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Unknown target test</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S4.SS3" title="In 4 Experiments ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Practical test</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S5" title="In Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S5.SS1" title="In 5 Results ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Practical test results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S6" title="In Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusions</span></a></li>
</ol></nav>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Images of spacecraft photographed from other spacecraft operating in outer space are difficult to come by, especially at a scale typically required for deep learning tasks. Semantic image segmentation, object detection and localization, and pose estimation are well researched areas with powerful results for many applications, and would be very useful in autonomous spacecraft operation and rendezvous. However, Wong et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> note that these strong results in broad and common domains may generalize poorly even to specific industrial applications on earth.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To address this, we generated a prototype synthetic image dataset labelled for semantic segmentation of 2D images of unmanned spacecraft, and are endeavouring to train a performant deep learning image segmentation model using the same, with the ultimate goal of enabling further research in the area of autonomous spacecraft rendezvous.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2211.11941/assets/imgs/image_2741_1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="236" height="177" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.2.1" class="ltx_text ltx_font_bold">Chandra spacecraft 3D Model</span></figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2211.11941/assets/imgs/mask_2741_1.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="236" height="177" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S1.F2.2.1" class="ltx_text ltx_font_bold">Corresponding segmentation mask</span></figcaption>
</figure>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Related work</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Minaee et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and Ghosh et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> provide recent surveys of deep learning approaches for semantic image segmentation. Treml et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> discuss semantic segmentation using deep learning for autonomous terrestrial vehicles (i.e.¬†self-driving cars).</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">Arantes et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> present a discussion of machine vision pose estimation for on-orbit autonomous satellite intercept and rendezvous with uncooperative spacecraft using a monocular optical sensor. Kisantal et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> present a synthetic dataset of images of unmanned spacecraft labelled for pose estimation, including images generated from 3D computer models, and hardware-in-the-loop simulation using a physical mock-up.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">Wong et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> express concerns about the availability of training data at scale for domain-specific object recognition tasks, and propose a method of procedurally generating large image datasets from 3D models based on a small number of physical examples, labelled for image classification.</p>
</div>
<div id="S1.SS1.p4" class="ltx_para">
<p id="S1.SS1.p4.1" class="ltx_p">Yang et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> present an analysis of the impact of atmospheric distortion on performance of a semantic segmentation task on ground-based images of satellites on low earth orbit; the formulation of the segmentation task discussed herein largely follows that posed in this prior work, however, our aim is to present a method for generating pairs of synthetic images and ground truth labels suitable for semantic segmentation tasks in this domain, as well as provide some benchmark results using these data. Our major hypothesis is that since this task is fundamentally about object or component recognition using data from an optical sensor, images with a degree of verisimilitude to the human eye would be effective for training deep learning models that would be useful in a practical application.</p>
</div>
<div id="S1.SS1.p5" class="ltx_para">
<p id="S1.SS1.p5.1" class="ltx_p">To that end, we will describe the method by which the data were generated, discuss the baseline methods that were attempted and the evaluation criteria selected, and finally present the results of these along with a practical test of the best-performing method using three non-synthetic images of satellites taken from the vantage point of an on-orbit camera.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Dataset</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The data generated consist of 60,000 renderings of one of four open-source 3D models of unmanned spacecraft published by NASA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, with accompanying ground truth labels, created using the open source 3D modeling software Blender <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>3D models</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">To provide our dataset with a variety of spacecraft configurations, we chose the Chandra X-Ray Observatory, Near Earth Asteroid Rendezvous ‚Äì NEAR Shoemaker, Cluster II, and the IBEX Interstellar Boundary Explorer, as 3D models from which to generate training, validation, and test images in this domain.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">We made some artistic modifications to the published 3D models in order to be able to generate more realistic looking images where possible, and attempted to simulate the lighting conditions of low-earth orbit by illuminating the 3D model with two light sources: one light source at infinity simulating the intensity, color, and parallel light rays of the sun, and one planar light source to simulate earthshine. As such, our simulated environment assumes that rendezvous is taking place on the day side of the earth, which is perhaps not unreasonable since we are assuming an optical sensor.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">We then duplicated the 3D model used for image generation and manually colored each polygon of the duplicate model from a discrete set of colors uniquely associated with one of the class labels. Generation of the semantic masks was then accomplished by removing all light sources from the Blender scene and modifying the shaders, material properties, and renderer settings such that the ray-tracer projected only the appropriate color value onto each pixel of the rendered image based on the camera‚Äôs perspective.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">An example simulated image and ground truth mask can be seen in figures <a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S1.F2" title="Figure 2 ‚Ä£ 1 Introduction ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, respectively.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Class labels</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">We worked with an industry expert to define eleven class labels for the segmentation task, shown in table <a href="#S2.T1" title="Table 1 ‚Ä£ 2.2 Class labels ‚Ä£ 2 Dataset ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, along with pixel- and case-level distributions of class prevalence among images generated from the four in-distribution spacecraft.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S2.T1.2.1" class="ltx_text ltx_font_bold">Semantic space and distribution</span></figcaption>
<div id="S2.T1.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:239.0pt;height:172.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-29.9pt,21.6pt) scale(0.8,0.8) ;">
<table id="S2.T1.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.3.1.1.1" class="ltx_tr">
<th id="S2.T1.3.1.1.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S2.T1.3.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th id="S2.T1.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S2.T1.3.1.1.1.3.1" class="ltx_text ltx_font_bold">% pixels</span></th>
<th id="S2.T1.3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S2.T1.3.1.1.1.4.1" class="ltx_text ltx_font_bold">% cases</span></th>
<th id="S2.T1.3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S2.T1.3.1.1.1.5.1" class="ltx_text ltx_font_bold">Mask color</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.3.1.2.1" class="ltx_tr">
<th id="S2.T1.3.1.2.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">0.</th>
<th id="S2.T1.3.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">None / background</th>
<td id="S2.T1.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">93.95%</td>
<td id="S2.T1.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">100%</td>
<td id="S2.T1.3.1.2.1.5" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S2.T1.3.1.3.2" class="ltx_tr">
<th id="S2.T1.3.1.3.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">1.</th>
<th id="S2.T1.3.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Solar panels</th>
<td id="S2.T1.3.1.3.2.3" class="ltx_td ltx_align_center">1.75%</td>
<td id="S2.T1.3.1.3.2.4" class="ltx_td ltx_align_center">100%</td>
<td id="S2.T1.3.1.3.2.5" class="ltx_td"></td>
</tr>
<tr id="S2.T1.3.1.4.3" class="ltx_tr">
<th id="S2.T1.3.1.4.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">2.</th>
<th id="S2.T1.3.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Solar panel drive shaft</th>
<td id="S2.T1.3.1.4.3.3" class="ltx_td ltx_align_center">0.03%</td>
<td id="S2.T1.3.1.4.3.4" class="ltx_td ltx_align_center">49.99%</td>
<td id="S2.T1.3.1.4.3.5" class="ltx_td"></td>
</tr>
<tr id="S2.T1.3.1.5.4" class="ltx_tr">
<th id="S2.T1.3.1.5.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">3.</th>
<th id="S2.T1.3.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Antenna</th>
<td id="S2.T1.3.1.5.4.3" class="ltx_td ltx_align_center">0.05%</td>
<td id="S2.T1.3.1.5.4.4" class="ltx_td ltx_align_center">73.30%</td>
<td id="S2.T1.3.1.5.4.5" class="ltx_td"></td>
</tr>
<tr id="S2.T1.3.1.6.5" class="ltx_tr">
<th id="S2.T1.3.1.6.5.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">4.</th>
<th id="S2.T1.3.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Parabolic reflector</th>
<td id="S2.T1.3.1.6.5.3" class="ltx_td ltx_align_center">0.08%</td>
<td id="S2.T1.3.1.6.5.4" class="ltx_td ltx_align_center">25.00%</td>
<td id="S2.T1.3.1.6.5.5" class="ltx_td"></td>
</tr>
<tr id="S2.T1.3.1.7.6" class="ltx_tr">
<th id="S2.T1.3.1.7.6.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">5.</th>
<th id="S2.T1.3.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Main module</th>
<td id="S2.T1.3.1.7.6.3" class="ltx_td ltx_align_center">2.90%</td>
<td id="S2.T1.3.1.7.6.4" class="ltx_td ltx_align_center">100%</td>
<td id="S2.T1.3.1.7.6.5" class="ltx_td"></td>
</tr>
<tr id="S2.T1.3.1.8.7" class="ltx_tr">
<th id="S2.T1.3.1.8.7.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">6.</th>
<th id="S2.T1.3.1.8.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Telescope</th>
<td id="S2.T1.3.1.8.7.3" class="ltx_td ltx_align_center">0.58%</td>
<td id="S2.T1.3.1.8.7.4" class="ltx_td ltx_align_center">25.00%</td>
<td id="S2.T1.3.1.8.7.5" class="ltx_td"></td>
</tr>
<tr id="S2.T1.3.1.9.8" class="ltx_tr">
<th id="S2.T1.3.1.9.8.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">7.</th>
<th id="S2.T1.3.1.9.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Main thrusters</th>
<td id="S2.T1.3.1.9.8.3" class="ltx_td ltx_align_center">0.05%</td>
<td id="S2.T1.3.1.9.8.4" class="ltx_td ltx_align_center">77.03%</td>
<td id="S2.T1.3.1.9.8.5" class="ltx_td"></td>
</tr>
<tr id="S2.T1.3.1.10.9" class="ltx_tr">
<th id="S2.T1.3.1.10.9.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">8.</th>
<th id="S2.T1.3.1.10.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Rotational thrusters</th>
<td id="S2.T1.3.1.10.9.3" class="ltx_td ltx_align_center">0.02%</td>
<td id="S2.T1.3.1.10.9.4" class="ltx_td ltx_align_center">99.99%</td>
<td id="S2.T1.3.1.10.9.5" class="ltx_td"></td>
</tr>
<tr id="S2.T1.3.1.11.10" class="ltx_tr">
<th id="S2.T1.3.1.11.10.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">9.</th>
<th id="S2.T1.3.1.11.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Sensors</th>
<td id="S2.T1.3.1.11.10.3" class="ltx_td ltx_align_center">0.53%</td>
<td id="S2.T1.3.1.11.10.4" class="ltx_td ltx_align_center">98.27%</td>
<td id="S2.T1.3.1.11.10.5" class="ltx_td"></td>
</tr>
<tr id="S2.T1.3.1.12.11" class="ltx_tr">
<th id="S2.T1.3.1.12.11.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b">10.</th>
<th id="S2.T1.3.1.12.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">Launch vehicle adapter</th>
<td id="S2.T1.3.1.12.11.3" class="ltx_td ltx_align_center ltx_border_b">0.08%</td>
<td id="S2.T1.3.1.12.11.4" class="ltx_td ltx_align_center ltx_border_b">49.44%</td>
<td id="S2.T1.3.1.12.11.5" class="ltx_td ltx_border_b"></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Our objective was both to have a single meaningful semantic space that covered a variety of spacecrafts and configurations, and also for the class labels to provide a clear delineation between components to fixate on (e.g., the launch vehicle adapter) and components to avoid (e.g., thrusters) during rendezvous.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Procedural image generation</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">We used the Blender API for Python to automate our dataset generation in a scalable way. A Python script moves the camera in a spherical pattern around the spacecraft to one of 5,000 positions. For each position, three rendered images were generated with the same aspect, but with different ranges; one at the first position, and one each from approximately twice and three times the distance from the spacecraft, thus creating a total of 15,000 training images from each of the 3D models. The process was then repeated for the color-coded ground truth 3D model, taking corresponding renders from identical positions in the scene.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Ground truth representation</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">The procedurally generated ground truth images were then post-processed using Python‚Äôs Pillow library to represent a pixel-wise categorical encoding in the semantic space; we programatically assigned RGB color values in the generated images to their corresponding class labels, resulting in single-channel images with pixel values equal to the cardinal number associated with each category <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. These single-channel PNG format images were generated with embedded color palettes for human and machine readability.</p>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Unknown target test set</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">In addition to the 60,000 images used for training, validation, and testing, we also generated an additional 1,500 images from a 3D model of a fifth spacecraft ‚Äì the Deep Space Program Science Experiment (DSPSE, a.k.a.¬†Clementine) ‚Äì to evaluate the ability of a semantic segmentation model trained on these data to identify specific components independently and generalize to an unknown target.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Baseline methods</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We trained U-Net, HRNet, and DeepLab deep image segmentation models to determine which architecture performed the best for this task. All models were trained using Python‚Äôs FastAI and SemTorch libraries <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, to enable rapid testing of a variety of benchmark models. In each case, a backbone pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> was incorporated to leverage transfer learning in extracting features from the input image. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>U-Net</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Ronneberger et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> describe U-Net, which aims to provide precise localization even when using a smaller dataset than is typically used for image segmentation tasks. SemTorch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> uses PyTorch‚Äôs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> implementation of U-Net.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">For the U-Net model we trained, we selected a ResNet34 backbone. A batch size of 8 was chosen as the maximum feasible batch size for this model, given the GPU that was used.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>HRNet</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">HRNet (High-Resolution Net) is a CNN developed specifically to retain and use high-resolution inputs throughout the network, resulting in better performing for pixel labelling and segmenation tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. HRNet aims to provide high spatial precision, which is desirable in this task due to the variety of classes and class imbalance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We selected a pre-trained HRNet30 backbone to perform feature extraction. A batch size of 16 was chosen using the same criteria as before.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>DeepLab</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">DeepLab is a CNN developed and open-sourced by Google that relies on atrous convolutions to perform image segmentation tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. More specifically, we used the latest iteration of the DeepLab model at time of writing, DeepLabv3+, as implemented by FastAI. DeepLabv3+ aims to incorporate the best aspects of spatial pyramid pooling and encoder-decoder models that leads to a faster and more performant model overall <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">In this case, a ResNet50 backbone was selected over ResNet34, due to limitations of the SemTorch library. A batch size of 16 was chosen by the same criteria as before.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Loss functions</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">We experimented with three different loss functions: categorical cross-entropy loss, Dice loss, and a mixture of focal and Dice losses. These choices were motivated by the considerable class imbalance as shown in table <a href="#S2.T1" title="Table 1 ‚Ä£ 2.2 Class labels ‚Ä£ 2 Dataset ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Categorical cross-entropy loss</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">For each pixel, this function computes the log loss summed over all possible classes.</p>
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.2" class="ltx_Math" alttext="\textrm{CCE}_{i}=-\sum_{\textrm{classes}}y\log(\hat{y})" display="block"><semantics id="S3.Ex1.m1.2a"><mrow id="S3.Ex1.m1.2.3" xref="S3.Ex1.m1.2.3.cmml"><msub id="S3.Ex1.m1.2.3.2" xref="S3.Ex1.m1.2.3.2.cmml"><mtext id="S3.Ex1.m1.2.3.2.2" xref="S3.Ex1.m1.2.3.2.2a.cmml">CCE</mtext><mi id="S3.Ex1.m1.2.3.2.3" xref="S3.Ex1.m1.2.3.2.3.cmml">i</mi></msub><mo id="S3.Ex1.m1.2.3.1" xref="S3.Ex1.m1.2.3.1.cmml">=</mo><mrow id="S3.Ex1.m1.2.3.3" xref="S3.Ex1.m1.2.3.3.cmml"><mo id="S3.Ex1.m1.2.3.3a" xref="S3.Ex1.m1.2.3.3.cmml">‚àí</mo><mrow id="S3.Ex1.m1.2.3.3.2" xref="S3.Ex1.m1.2.3.3.2.cmml"><munder id="S3.Ex1.m1.2.3.3.2.1" xref="S3.Ex1.m1.2.3.3.2.1.cmml"><mo movablelimits="false" id="S3.Ex1.m1.2.3.3.2.1.2" xref="S3.Ex1.m1.2.3.3.2.1.2.cmml">‚àë</mo><mtext id="S3.Ex1.m1.2.3.3.2.1.3" xref="S3.Ex1.m1.2.3.3.2.1.3a.cmml">classes</mtext></munder><mrow id="S3.Ex1.m1.2.3.3.2.2" xref="S3.Ex1.m1.2.3.3.2.2.cmml"><mi id="S3.Ex1.m1.2.3.3.2.2.2" xref="S3.Ex1.m1.2.3.3.2.2.2.cmml">y</mi><mo lspace="0.167em" rspace="0em" id="S3.Ex1.m1.2.3.3.2.2.1" xref="S3.Ex1.m1.2.3.3.2.2.1.cmml">‚Äã</mo><mrow id="S3.Ex1.m1.2.3.3.2.2.3.2" xref="S3.Ex1.m1.2.3.3.2.2.3.1.cmml"><mi id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml">log</mi><mo id="S3.Ex1.m1.2.3.3.2.2.3.2a" xref="S3.Ex1.m1.2.3.3.2.2.3.1.cmml">‚Å°</mo><mrow id="S3.Ex1.m1.2.3.3.2.2.3.2.1" xref="S3.Ex1.m1.2.3.3.2.2.3.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.2.3.3.2.2.3.2.1.1" xref="S3.Ex1.m1.2.3.3.2.2.3.1.cmml">(</mo><mover accent="true" id="S3.Ex1.m1.2.2" xref="S3.Ex1.m1.2.2.cmml"><mi id="S3.Ex1.m1.2.2.2" xref="S3.Ex1.m1.2.2.2.cmml">y</mi><mo id="S3.Ex1.m1.2.2.1" xref="S3.Ex1.m1.2.2.1.cmml">^</mo></mover><mo stretchy="false" id="S3.Ex1.m1.2.3.3.2.2.3.2.1.2" xref="S3.Ex1.m1.2.3.3.2.2.3.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.2b"><apply id="S3.Ex1.m1.2.3.cmml" xref="S3.Ex1.m1.2.3"><eq id="S3.Ex1.m1.2.3.1.cmml" xref="S3.Ex1.m1.2.3.1"></eq><apply id="S3.Ex1.m1.2.3.2.cmml" xref="S3.Ex1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.3.2.1.cmml" xref="S3.Ex1.m1.2.3.2">subscript</csymbol><ci id="S3.Ex1.m1.2.3.2.2a.cmml" xref="S3.Ex1.m1.2.3.2.2"><mtext id="S3.Ex1.m1.2.3.2.2.cmml" xref="S3.Ex1.m1.2.3.2.2">CCE</mtext></ci><ci id="S3.Ex1.m1.2.3.2.3.cmml" xref="S3.Ex1.m1.2.3.2.3">ùëñ</ci></apply><apply id="S3.Ex1.m1.2.3.3.cmml" xref="S3.Ex1.m1.2.3.3"><minus id="S3.Ex1.m1.2.3.3.1.cmml" xref="S3.Ex1.m1.2.3.3"></minus><apply id="S3.Ex1.m1.2.3.3.2.cmml" xref="S3.Ex1.m1.2.3.3.2"><apply id="S3.Ex1.m1.2.3.3.2.1.cmml" xref="S3.Ex1.m1.2.3.3.2.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.3.3.2.1.1.cmml" xref="S3.Ex1.m1.2.3.3.2.1">subscript</csymbol><sum id="S3.Ex1.m1.2.3.3.2.1.2.cmml" xref="S3.Ex1.m1.2.3.3.2.1.2"></sum><ci id="S3.Ex1.m1.2.3.3.2.1.3a.cmml" xref="S3.Ex1.m1.2.3.3.2.1.3"><mtext mathsize="70%" id="S3.Ex1.m1.2.3.3.2.1.3.cmml" xref="S3.Ex1.m1.2.3.3.2.1.3">classes</mtext></ci></apply><apply id="S3.Ex1.m1.2.3.3.2.2.cmml" xref="S3.Ex1.m1.2.3.3.2.2"><times id="S3.Ex1.m1.2.3.3.2.2.1.cmml" xref="S3.Ex1.m1.2.3.3.2.2.1"></times><ci id="S3.Ex1.m1.2.3.3.2.2.2.cmml" xref="S3.Ex1.m1.2.3.3.2.2.2">ùë¶</ci><apply id="S3.Ex1.m1.2.3.3.2.2.3.1.cmml" xref="S3.Ex1.m1.2.3.3.2.2.3.2"><log id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1"></log><apply id="S3.Ex1.m1.2.2.cmml" xref="S3.Ex1.m1.2.2"><ci id="S3.Ex1.m1.2.2.1.cmml" xref="S3.Ex1.m1.2.2.1">^</ci><ci id="S3.Ex1.m1.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2">ùë¶</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.2c">\textrm{CCE}_{i}=-\sum_{\textrm{classes}}y\log(\hat{y})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS4.SSS1.p1.2" class="ltx_p">This scoring is computed over all pixels and the average taken. However, this loss function is susceptible to class imbalance. For unbalanced data, training might be dominated by the most prevalent class.</p>
</div>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Dice loss</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p">The Dice loss function is derived from the S√∏rensen-Dice Coefficient (see ¬ß<a href="#S4.SS1" title="4.1 Model selection criterion ‚Ä£ 4 Experiments ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>), which is robust to class imbalance as it balances between precision and recall <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S3.SS4.SSS2.p2" class="ltx_para">
<table id="S3.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex2.m1.2" class="ltx_Math" alttext="(\textrm{Dice loss})_{i}=1-\frac{\sum_{\textrm{classes}}(\textrm{Dice coef.})_{\textrm{class}}}{\textrm{\# classes}}" display="block"><semantics id="S3.Ex2.m1.2a"><mrow id="S3.Ex2.m1.2.3" xref="S3.Ex2.m1.2.3.cmml"><msub id="S3.Ex2.m1.2.3.2" xref="S3.Ex2.m1.2.3.2.cmml"><mrow id="S3.Ex2.m1.2.3.2.2.2" xref="S3.Ex2.m1.2.2a.cmml"><mo stretchy="false" id="S3.Ex2.m1.2.3.2.2.2.1" xref="S3.Ex2.m1.2.2a.cmml">(</mo><mtext id="S3.Ex2.m1.2.2" xref="S3.Ex2.m1.2.2.cmml">Dice loss</mtext><mo stretchy="false" id="S3.Ex2.m1.2.3.2.2.2.2" xref="S3.Ex2.m1.2.2a.cmml">)</mo></mrow><mi id="S3.Ex2.m1.2.3.2.3" xref="S3.Ex2.m1.2.3.2.3.cmml">i</mi></msub><mo id="S3.Ex2.m1.2.3.1" xref="S3.Ex2.m1.2.3.1.cmml">=</mo><mrow id="S3.Ex2.m1.2.3.3" xref="S3.Ex2.m1.2.3.3.cmml"><mn id="S3.Ex2.m1.2.3.3.2" xref="S3.Ex2.m1.2.3.3.2.cmml">1</mn><mo id="S3.Ex2.m1.2.3.3.1" xref="S3.Ex2.m1.2.3.3.1.cmml">‚àí</mo><mfrac id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml"><mrow id="S3.Ex2.m1.1.1.1" xref="S3.Ex2.m1.1.1.1.cmml"><msub id="S3.Ex2.m1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.2.cmml"><mo id="S3.Ex2.m1.1.1.1.2.2" xref="S3.Ex2.m1.1.1.1.2.2.cmml">‚àë</mo><mtext id="S3.Ex2.m1.1.1.1.2.3" xref="S3.Ex2.m1.1.1.1.2.3a.cmml">classes</mtext></msub><msub id="S3.Ex2.m1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.3.cmml"><mrow id="S3.Ex2.m1.1.1.1.3.2.2" xref="S3.Ex2.m1.1.1.1.1a.cmml"><mo lspace="0em" stretchy="false" id="S3.Ex2.m1.1.1.1.3.2.2.1" xref="S3.Ex2.m1.1.1.1.1a.cmml">(</mo><mtext id="S3.Ex2.m1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.cmml">Dice coef.</mtext><mo stretchy="false" id="S3.Ex2.m1.1.1.1.3.2.2.2" xref="S3.Ex2.m1.1.1.1.1a.cmml">)</mo></mrow><mtext id="S3.Ex2.m1.1.1.1.3.3" xref="S3.Ex2.m1.1.1.1.3.3a.cmml">class</mtext></msub></mrow><mtext id="S3.Ex2.m1.1.1.3" xref="S3.Ex2.m1.1.1.3a.cmml"># classes</mtext></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.2b"><apply id="S3.Ex2.m1.2.3.cmml" xref="S3.Ex2.m1.2.3"><eq id="S3.Ex2.m1.2.3.1.cmml" xref="S3.Ex2.m1.2.3.1"></eq><apply id="S3.Ex2.m1.2.3.2.cmml" xref="S3.Ex2.m1.2.3.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.3.2.1.cmml" xref="S3.Ex2.m1.2.3.2">subscript</csymbol><ci id="S3.Ex2.m1.2.2a.cmml" xref="S3.Ex2.m1.2.3.2.2.2"><mtext id="S3.Ex2.m1.2.2.cmml" xref="S3.Ex2.m1.2.2">Dice loss</mtext></ci><ci id="S3.Ex2.m1.2.3.2.3.cmml" xref="S3.Ex2.m1.2.3.2.3">ùëñ</ci></apply><apply id="S3.Ex2.m1.2.3.3.cmml" xref="S3.Ex2.m1.2.3.3"><minus id="S3.Ex2.m1.2.3.3.1.cmml" xref="S3.Ex2.m1.2.3.3.1"></minus><cn type="integer" id="S3.Ex2.m1.2.3.3.2.cmml" xref="S3.Ex2.m1.2.3.3.2">1</cn><apply id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1"><divide id="S3.Ex2.m1.1.1.2.cmml" xref="S3.Ex2.m1.1.1"></divide><apply id="S3.Ex2.m1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1"><apply id="S3.Ex2.m1.1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.2.1.cmml" xref="S3.Ex2.m1.1.1.1.2">subscript</csymbol><sum id="S3.Ex2.m1.1.1.1.2.2.cmml" xref="S3.Ex2.m1.1.1.1.2.2"></sum><ci id="S3.Ex2.m1.1.1.1.2.3a.cmml" xref="S3.Ex2.m1.1.1.1.2.3"><mtext mathsize="70%" id="S3.Ex2.m1.1.1.1.2.3.cmml" xref="S3.Ex2.m1.1.1.1.2.3">classes</mtext></ci></apply><apply id="S3.Ex2.m1.1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.3.1.cmml" xref="S3.Ex2.m1.1.1.1.3">subscript</csymbol><ci id="S3.Ex2.m1.1.1.1.1a.cmml" xref="S3.Ex2.m1.1.1.1.3.2.2"><mtext id="S3.Ex2.m1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1">Dice coef.</mtext></ci><ci id="S3.Ex2.m1.1.1.1.3.3a.cmml" xref="S3.Ex2.m1.1.1.1.3.3"><mtext mathsize="70%" id="S3.Ex2.m1.1.1.1.3.3.cmml" xref="S3.Ex2.m1.1.1.1.3.3">class</mtext></ci></apply></apply><ci id="S3.Ex2.m1.1.1.3a.cmml" xref="S3.Ex2.m1.1.1.3"><mtext id="S3.Ex2.m1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.3"># classes</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.2c">(\textrm{Dice loss})_{i}=1-\frac{\sum_{\textrm{classes}}(\textrm{Dice coef.})_{\textrm{class}}}{\textrm{\# classes}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>Dice + focal loss</h4>

<div id="S3.SS4.SSS3.p1" class="ltx_para">
<p id="S3.SS4.SSS3.p1.1" class="ltx_p">Focal loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> modifies the pixel-wise cross-entropy loss by down-weighting the loss of easy-to-classify pixels based on a hyperparamter <math id="S3.SS4.SSS3.p1.1.m1.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.SS4.SSS3.p1.1.m1.1a"><mi id="S3.SS4.SSS3.p1.1.m1.1.1" xref="S3.SS4.SSS3.p1.1.m1.1.1.cmml">Œ≥</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS3.p1.1.m1.1b"><ci id="S3.SS4.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS3.p1.1.m1.1.1">ùõæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS3.p1.1.m1.1c">\gamma</annotation></semantics></math>, focusing training on more difficult examples. Focal loss is defined as:</p>
<table id="S3.Ex3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex3.m1.4" class="ltx_Math" alttext="(\textrm{Focal loss})_{i}=-\sum_{\textrm{classes}}(1-\hat{y})^{\gamma}y\log(\hat{y})." display="block"><semantics id="S3.Ex3.m1.4a"><mrow id="S3.Ex3.m1.4.4.1" xref="S3.Ex3.m1.4.4.1.1.cmml"><mrow id="S3.Ex3.m1.4.4.1.1" xref="S3.Ex3.m1.4.4.1.1.cmml"><msub id="S3.Ex3.m1.4.4.1.1.3" xref="S3.Ex3.m1.4.4.1.1.3.cmml"><mrow id="S3.Ex3.m1.4.4.1.1.3.2.2" xref="S3.Ex3.m1.1.1a.cmml"><mo stretchy="false" id="S3.Ex3.m1.4.4.1.1.3.2.2.1" xref="S3.Ex3.m1.1.1a.cmml">(</mo><mtext id="S3.Ex3.m1.1.1" xref="S3.Ex3.m1.1.1.cmml">Focal loss</mtext><mo stretchy="false" id="S3.Ex3.m1.4.4.1.1.3.2.2.2" xref="S3.Ex3.m1.1.1a.cmml">)</mo></mrow><mi id="S3.Ex3.m1.4.4.1.1.3.3" xref="S3.Ex3.m1.4.4.1.1.3.3.cmml">i</mi></msub><mo id="S3.Ex3.m1.4.4.1.1.2" xref="S3.Ex3.m1.4.4.1.1.2.cmml">=</mo><mrow id="S3.Ex3.m1.4.4.1.1.1" xref="S3.Ex3.m1.4.4.1.1.1.cmml"><mo id="S3.Ex3.m1.4.4.1.1.1a" xref="S3.Ex3.m1.4.4.1.1.1.cmml">‚àí</mo><mrow id="S3.Ex3.m1.4.4.1.1.1.1" xref="S3.Ex3.m1.4.4.1.1.1.1.cmml"><munder id="S3.Ex3.m1.4.4.1.1.1.1.2" xref="S3.Ex3.m1.4.4.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.Ex3.m1.4.4.1.1.1.1.2.2" xref="S3.Ex3.m1.4.4.1.1.1.1.2.2.cmml">‚àë</mo><mtext id="S3.Ex3.m1.4.4.1.1.1.1.2.3" xref="S3.Ex3.m1.4.4.1.1.1.1.2.3a.cmml">classes</mtext></munder><mrow id="S3.Ex3.m1.4.4.1.1.1.1.1" xref="S3.Ex3.m1.4.4.1.1.1.1.1.cmml"><msup id="S3.Ex3.m1.4.4.1.1.1.1.1.1" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.cmml"><mrow id="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mn id="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml">‚àí</mo><mover accent="true" id="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.3.2" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.3.2.cmml">y</mi><mo id="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.3.1" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo stretchy="false" id="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S3.Ex3.m1.4.4.1.1.1.1.1.1.3" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.cmml">Œ≥</mi></msup><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.4.4.1.1.1.1.1.2" xref="S3.Ex3.m1.4.4.1.1.1.1.1.2.cmml">‚Äã</mo><mi id="S3.Ex3.m1.4.4.1.1.1.1.1.3" xref="S3.Ex3.m1.4.4.1.1.1.1.1.3.cmml">y</mi><mo lspace="0.167em" rspace="0em" id="S3.Ex3.m1.4.4.1.1.1.1.1.2a" xref="S3.Ex3.m1.4.4.1.1.1.1.1.2.cmml">‚Äã</mo><mrow id="S3.Ex3.m1.4.4.1.1.1.1.1.4.2" xref="S3.Ex3.m1.4.4.1.1.1.1.1.4.1.cmml"><mi id="S3.Ex3.m1.2.2" xref="S3.Ex3.m1.2.2.cmml">log</mi><mo id="S3.Ex3.m1.4.4.1.1.1.1.1.4.2a" xref="S3.Ex3.m1.4.4.1.1.1.1.1.4.1.cmml">‚Å°</mo><mrow id="S3.Ex3.m1.4.4.1.1.1.1.1.4.2.1" xref="S3.Ex3.m1.4.4.1.1.1.1.1.4.1.cmml"><mo stretchy="false" id="S3.Ex3.m1.4.4.1.1.1.1.1.4.2.1.1" xref="S3.Ex3.m1.4.4.1.1.1.1.1.4.1.cmml">(</mo><mover accent="true" id="S3.Ex3.m1.3.3" xref="S3.Ex3.m1.3.3.cmml"><mi id="S3.Ex3.m1.3.3.2" xref="S3.Ex3.m1.3.3.2.cmml">y</mi><mo id="S3.Ex3.m1.3.3.1" xref="S3.Ex3.m1.3.3.1.cmml">^</mo></mover><mo stretchy="false" id="S3.Ex3.m1.4.4.1.1.1.1.1.4.2.1.2" xref="S3.Ex3.m1.4.4.1.1.1.1.1.4.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.Ex3.m1.4.4.1.2" xref="S3.Ex3.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex3.m1.4b"><apply id="S3.Ex3.m1.4.4.1.1.cmml" xref="S3.Ex3.m1.4.4.1"><eq id="S3.Ex3.m1.4.4.1.1.2.cmml" xref="S3.Ex3.m1.4.4.1.1.2"></eq><apply id="S3.Ex3.m1.4.4.1.1.3.cmml" xref="S3.Ex3.m1.4.4.1.1.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.4.4.1.1.3.1.cmml" xref="S3.Ex3.m1.4.4.1.1.3">subscript</csymbol><ci id="S3.Ex3.m1.1.1a.cmml" xref="S3.Ex3.m1.4.4.1.1.3.2.2"><mtext id="S3.Ex3.m1.1.1.cmml" xref="S3.Ex3.m1.1.1">Focal loss</mtext></ci><ci id="S3.Ex3.m1.4.4.1.1.3.3.cmml" xref="S3.Ex3.m1.4.4.1.1.3.3">ùëñ</ci></apply><apply id="S3.Ex3.m1.4.4.1.1.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1"><minus id="S3.Ex3.m1.4.4.1.1.1.2.cmml" xref="S3.Ex3.m1.4.4.1.1.1"></minus><apply id="S3.Ex3.m1.4.4.1.1.1.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1"><apply id="S3.Ex3.m1.4.4.1.1.1.1.2.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.4.4.1.1.1.1.2.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.2">subscript</csymbol><sum id="S3.Ex3.m1.4.4.1.1.1.1.2.2.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.2.2"></sum><ci id="S3.Ex3.m1.4.4.1.1.1.1.2.3a.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.2.3"><mtext mathsize="70%" id="S3.Ex3.m1.4.4.1.1.1.1.2.3.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.2.3">classes</mtext></ci></apply><apply id="S3.Ex3.m1.4.4.1.1.1.1.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1"><times id="S3.Ex3.m1.4.4.1.1.1.1.1.2.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.2"></times><apply id="S3.Ex3.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1">superscript</csymbol><apply id="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1"><minus id="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.2">1</cn><apply id="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.3"><ci id="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.3.1">^</ci><ci id="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.1.1.3.2">ùë¶</ci></apply></apply><ci id="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.3">ùõæ</ci></apply><ci id="S3.Ex3.m1.4.4.1.1.1.1.1.3.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.3">ùë¶</ci><apply id="S3.Ex3.m1.4.4.1.1.1.1.1.4.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.4.2"><log id="S3.Ex3.m1.2.2.cmml" xref="S3.Ex3.m1.2.2"></log><apply id="S3.Ex3.m1.3.3.cmml" xref="S3.Ex3.m1.3.3"><ci id="S3.Ex3.m1.3.3.1.cmml" xref="S3.Ex3.m1.3.3.1">^</ci><ci id="S3.Ex3.m1.3.3.2.cmml" xref="S3.Ex3.m1.3.3.2">ùë¶</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex3.m1.4c">(\textrm{Focal loss})_{i}=-\sum_{\textrm{classes}}(1-\hat{y})^{\gamma}y\log(\hat{y}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.SSS3.p2" class="ltx_para">
<p id="S3.SS4.SSS3.p2.3" class="ltx_p">Dice + focal loss combines the two objectives with a mixing parameter <math id="S3.SS4.SSS3.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS4.SSS3.p2.1.m1.1a"><mi id="S3.SS4.SSS3.p2.1.m1.1.1" xref="S3.SS4.SSS3.p2.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS3.p2.1.m1.1b"><ci id="S3.SS4.SSS3.p2.1.m1.1.1.cmml" xref="S3.SS4.SSS3.p2.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS3.p2.1.m1.1c">\alpha</annotation></semantics></math>, balancing global (Dice) and local (focal) features of the target mask. Default values of <math id="S3.SS4.SSS3.p2.2.m2.1" class="ltx_Math" alttext="\gamma=2" display="inline"><semantics id="S3.SS4.SSS3.p2.2.m2.1a"><mrow id="S3.SS4.SSS3.p2.2.m2.1.1" xref="S3.SS4.SSS3.p2.2.m2.1.1.cmml"><mi id="S3.SS4.SSS3.p2.2.m2.1.1.2" xref="S3.SS4.SSS3.p2.2.m2.1.1.2.cmml">Œ≥</mi><mo id="S3.SS4.SSS3.p2.2.m2.1.1.1" xref="S3.SS4.SSS3.p2.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS4.SSS3.p2.2.m2.1.1.3" xref="S3.SS4.SSS3.p2.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS3.p2.2.m2.1b"><apply id="S3.SS4.SSS3.p2.2.m2.1.1.cmml" xref="S3.SS4.SSS3.p2.2.m2.1.1"><eq id="S3.SS4.SSS3.p2.2.m2.1.1.1.cmml" xref="S3.SS4.SSS3.p2.2.m2.1.1.1"></eq><ci id="S3.SS4.SSS3.p2.2.m2.1.1.2.cmml" xref="S3.SS4.SSS3.p2.2.m2.1.1.2">ùõæ</ci><cn type="integer" id="S3.SS4.SSS3.p2.2.m2.1.1.3.cmml" xref="S3.SS4.SSS3.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS3.p2.2.m2.1c">\gamma=2</annotation></semantics></math> and <math id="S3.SS4.SSS3.p2.3.m3.1" class="ltx_Math" alttext="\alpha=1" display="inline"><semantics id="S3.SS4.SSS3.p2.3.m3.1a"><mrow id="S3.SS4.SSS3.p2.3.m3.1.1" xref="S3.SS4.SSS3.p2.3.m3.1.1.cmml"><mi id="S3.SS4.SSS3.p2.3.m3.1.1.2" xref="S3.SS4.SSS3.p2.3.m3.1.1.2.cmml">Œ±</mi><mo id="S3.SS4.SSS3.p2.3.m3.1.1.1" xref="S3.SS4.SSS3.p2.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS4.SSS3.p2.3.m3.1.1.3" xref="S3.SS4.SSS3.p2.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS3.p2.3.m3.1b"><apply id="S3.SS4.SSS3.p2.3.m3.1.1.cmml" xref="S3.SS4.SSS3.p2.3.m3.1.1"><eq id="S3.SS4.SSS3.p2.3.m3.1.1.1.cmml" xref="S3.SS4.SSS3.p2.3.m3.1.1.1"></eq><ci id="S3.SS4.SSS3.p2.3.m3.1.1.2.cmml" xref="S3.SS4.SSS3.p2.3.m3.1.1.2">ùõº</ci><cn type="integer" id="S3.SS4.SSS3.p2.3.m3.1.1.3.cmml" xref="S3.SS4.SSS3.p2.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS3.p2.3.m3.1c">\alpha=1</annotation></semantics></math> were used during training.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2211.11941/assets/imgs/real_test/img_0.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_img_square" width="186" height="186" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Test image</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2211.11941/assets/imgs/real_test/true_mask_0.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_img_square" width="186" height="186" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>True mask</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2211.11941/assets/imgs/real_test/pred_mask_0.png" id="S4.F3.sf3.g1" class="ltx_graphics ltx_img_square" width="186" height="186" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Predicted mask</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2211.11941/assets/imgs/real_test/img_1.png" id="S4.F3.sf4.g1" class="ltx_graphics ltx_img_square" width="186" height="186" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Test image</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2211.11941/assets/imgs/real_test/true_mask_1.png" id="S4.F3.sf5.g1" class="ltx_graphics ltx_img_square" width="186" height="186" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>True mask</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2211.11941/assets/imgs/real_test/pred_mask_1.png" id="S4.F3.sf6.g1" class="ltx_graphics ltx_img_square" width="186" height="186" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>Predicted mask</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf7" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2211.11941/assets/imgs/real_test/img_2.png" id="S4.F3.sf7.g1" class="ltx_graphics ltx_img_square" width="186" height="186" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(g) </span>Test image</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf8" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2211.11941/assets/imgs/real_test/true_mask_2.png" id="S4.F3.sf8.g1" class="ltx_graphics ltx_img_square" width="186" height="186" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(h) </span>True mask</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf9" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2211.11941/assets/imgs/real_test/pred_mask_2.png" id="S4.F3.sf9.g1" class="ltx_graphics ltx_img_square" width="186" height="186" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(i) </span>Predicted mask</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S4.F3.2.1" class="ltx_text ltx_font_bold">True and predicted masks for three test images from the NASA Image and Video Library, with other spacecraft or planetary bodies removed from the original photo. From top to bottom: the Space Flyer Unit (macro-average S√∏rensen‚ÄìDice coefficient: 0.2098), Diwata-1 (macro-average S√∏rensen‚ÄìDice coefficient: 0.2311), and Satcom K-2 (macro-average S√∏rensen‚ÄìDice coefficient: 0.2759)</span></figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We trained nine image segmentation models for this task, one with each combination of architecture and loss function described above, each on the same randomly selected 49,864 images sampled from our 60,000 image dataset, with a validation set of 5,306 similarly selected images, and a test set of 6,000. The input and output layers were fixed at a size of 256 <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.p1.1.m1.1a"><mo id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><times id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\times</annotation></semantics></math> 256, and input images were normalized using image mean and variance from ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">A narrow range of learning rates was selected for each experiment by varying the learning rate over a small number of training batches using FastAI‚Äôs <span id="S4.p2.1.1" class="ltx_text ltx_font_typewriter">lr_find()</span> method and selecting a region of greatest descent for the loss. Once this region was selected, learning rate annealing was used during Adam optimization with otherwise default parameters. Each model was trained for five epochs, with early stopping at a patience of two; though the loss appeared to plateau in all cases, the early stopping criterion was met in none.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.4" class="ltx_p">Data augmentation (flip with <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="p=0.5" display="inline"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mi id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">p</mi><mo id="S4.p3.1.m1.1.1.1" xref="S4.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><eq id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1"></eq><ci id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2">ùëù</ci><cn type="float" id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">p=0.5</annotation></semantics></math>, transpose with <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="p=0.5" display="inline"><semantics id="S4.p3.2.m2.1a"><mrow id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml"><mi id="S4.p3.2.m2.1.1.2" xref="S4.p3.2.m2.1.1.2.cmml">p</mi><mo id="S4.p3.2.m2.1.1.1" xref="S4.p3.2.m2.1.1.1.cmml">=</mo><mn id="S4.p3.2.m2.1.1.3" xref="S4.p3.2.m2.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><apply id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1"><eq id="S4.p3.2.m2.1.1.1.cmml" xref="S4.p3.2.m2.1.1.1"></eq><ci id="S4.p3.2.m2.1.1.2.cmml" xref="S4.p3.2.m2.1.1.2">ùëù</ci><cn type="float" id="S4.p3.2.m2.1.1.3.cmml" xref="S4.p3.2.m2.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">p=0.5</annotation></semantics></math>, rotate with <math id="S4.p3.3.m3.1" class="ltx_Math" alttext="p=0.4" display="inline"><semantics id="S4.p3.3.m3.1a"><mrow id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml"><mi id="S4.p3.3.m3.1.1.2" xref="S4.p3.3.m3.1.1.2.cmml">p</mi><mo id="S4.p3.3.m3.1.1.1" xref="S4.p3.3.m3.1.1.1.cmml">=</mo><mn id="S4.p3.3.m3.1.1.3" xref="S4.p3.3.m3.1.1.3.cmml">0.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><apply id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1"><eq id="S4.p3.3.m3.1.1.1.cmml" xref="S4.p3.3.m3.1.1.1"></eq><ci id="S4.p3.3.m3.1.1.2.cmml" xref="S4.p3.3.m3.1.1.2">ùëù</ci><cn type="float" id="S4.p3.3.m3.1.1.3.cmml" xref="S4.p3.3.m3.1.1.3">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">p=0.4</annotation></semantics></math>) was applied to each batch during training.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>
Augmentation was performed using Python‚Äôs <span id="footnote1.1" class="ltx_text ltx_font_typewriter">albumentation</span> library <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.
</span></span></span> Weight decay was set to <math id="S4.p3.4.m4.1" class="ltx_Math" alttext="1\mathrm{e}{-2}" display="inline"><semantics id="S4.p3.4.m4.1a"><mrow id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml"><mrow id="S4.p3.4.m4.1.1.2" xref="S4.p3.4.m4.1.1.2.cmml"><mn id="S4.p3.4.m4.1.1.2.2" xref="S4.p3.4.m4.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.p3.4.m4.1.1.2.1" xref="S4.p3.4.m4.1.1.2.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S4.p3.4.m4.1.1.2.3" xref="S4.p3.4.m4.1.1.2.3.cmml">e</mi></mrow><mo id="S4.p3.4.m4.1.1.1" xref="S4.p3.4.m4.1.1.1.cmml">‚àí</mo><mn id="S4.p3.4.m4.1.1.3" xref="S4.p3.4.m4.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.1b"><apply id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1"><minus id="S4.p3.4.m4.1.1.1.cmml" xref="S4.p3.4.m4.1.1.1"></minus><apply id="S4.p3.4.m4.1.1.2.cmml" xref="S4.p3.4.m4.1.1.2"><times id="S4.p3.4.m4.1.1.2.1.cmml" xref="S4.p3.4.m4.1.1.2.1"></times><cn type="integer" id="S4.p3.4.m4.1.1.2.2.cmml" xref="S4.p3.4.m4.1.1.2.2">1</cn><ci id="S4.p3.4.m4.1.1.2.3.cmml" xref="S4.p3.4.m4.1.1.2.3">e</ci></apply><cn type="integer" id="S4.p3.4.m4.1.1.3.cmml" xref="S4.p3.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.1c">1\mathrm{e}{-2}</annotation></semantics></math> and batch normalization was used.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Model selection criterion</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We chose the S√∏rensen-Dice coefficient as a model selection criterion. The Dice coefficient, equivalent to F1 score, is computed pixel-wise between the predicted and target mask where</p>
<table id="S4.Ex4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.Ex4.m1.1" class="ltx_Math" alttext="\textrm{Dice coef.}=\frac{2TP}{2TP+FP+FN}." display="block"><semantics id="S4.Ex4.m1.1a"><mrow id="S4.Ex4.m1.1.1.1" xref="S4.Ex4.m1.1.1.1.1.cmml"><mrow id="S4.Ex4.m1.1.1.1.1" xref="S4.Ex4.m1.1.1.1.1.cmml"><mtext id="S4.Ex4.m1.1.1.1.1.2" xref="S4.Ex4.m1.1.1.1.1.2a.cmml">Dice coef.</mtext><mo id="S4.Ex4.m1.1.1.1.1.1" xref="S4.Ex4.m1.1.1.1.1.1.cmml">=</mo><mfrac id="S4.Ex4.m1.1.1.1.1.3" xref="S4.Ex4.m1.1.1.1.1.3.cmml"><mrow id="S4.Ex4.m1.1.1.1.1.3.2" xref="S4.Ex4.m1.1.1.1.1.3.2.cmml"><mn id="S4.Ex4.m1.1.1.1.1.3.2.2" xref="S4.Ex4.m1.1.1.1.1.3.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.Ex4.m1.1.1.1.1.3.2.1" xref="S4.Ex4.m1.1.1.1.1.3.2.1.cmml">‚Äã</mo><mi id="S4.Ex4.m1.1.1.1.1.3.2.3" xref="S4.Ex4.m1.1.1.1.1.3.2.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.Ex4.m1.1.1.1.1.3.2.1a" xref="S4.Ex4.m1.1.1.1.1.3.2.1.cmml">‚Äã</mo><mi id="S4.Ex4.m1.1.1.1.1.3.2.4" xref="S4.Ex4.m1.1.1.1.1.3.2.4.cmml">P</mi></mrow><mrow id="S4.Ex4.m1.1.1.1.1.3.3" xref="S4.Ex4.m1.1.1.1.1.3.3.cmml"><mrow id="S4.Ex4.m1.1.1.1.1.3.3.2" xref="S4.Ex4.m1.1.1.1.1.3.3.2.cmml"><mn id="S4.Ex4.m1.1.1.1.1.3.3.2.2" xref="S4.Ex4.m1.1.1.1.1.3.3.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.Ex4.m1.1.1.1.1.3.3.2.1" xref="S4.Ex4.m1.1.1.1.1.3.3.2.1.cmml">‚Äã</mo><mi id="S4.Ex4.m1.1.1.1.1.3.3.2.3" xref="S4.Ex4.m1.1.1.1.1.3.3.2.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.Ex4.m1.1.1.1.1.3.3.2.1a" xref="S4.Ex4.m1.1.1.1.1.3.3.2.1.cmml">‚Äã</mo><mi id="S4.Ex4.m1.1.1.1.1.3.3.2.4" xref="S4.Ex4.m1.1.1.1.1.3.3.2.4.cmml">P</mi></mrow><mo id="S4.Ex4.m1.1.1.1.1.3.3.1" xref="S4.Ex4.m1.1.1.1.1.3.3.1.cmml">+</mo><mrow id="S4.Ex4.m1.1.1.1.1.3.3.3" xref="S4.Ex4.m1.1.1.1.1.3.3.3.cmml"><mi id="S4.Ex4.m1.1.1.1.1.3.3.3.2" xref="S4.Ex4.m1.1.1.1.1.3.3.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.Ex4.m1.1.1.1.1.3.3.3.1" xref="S4.Ex4.m1.1.1.1.1.3.3.3.1.cmml">‚Äã</mo><mi id="S4.Ex4.m1.1.1.1.1.3.3.3.3" xref="S4.Ex4.m1.1.1.1.1.3.3.3.3.cmml">P</mi></mrow><mo id="S4.Ex4.m1.1.1.1.1.3.3.1a" xref="S4.Ex4.m1.1.1.1.1.3.3.1.cmml">+</mo><mrow id="S4.Ex4.m1.1.1.1.1.3.3.4" xref="S4.Ex4.m1.1.1.1.1.3.3.4.cmml"><mi id="S4.Ex4.m1.1.1.1.1.3.3.4.2" xref="S4.Ex4.m1.1.1.1.1.3.3.4.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.Ex4.m1.1.1.1.1.3.3.4.1" xref="S4.Ex4.m1.1.1.1.1.3.3.4.1.cmml">‚Äã</mo><mi id="S4.Ex4.m1.1.1.1.1.3.3.4.3" xref="S4.Ex4.m1.1.1.1.1.3.3.4.3.cmml">N</mi></mrow></mrow></mfrac></mrow><mo lspace="0em" id="S4.Ex4.m1.1.1.1.2" xref="S4.Ex4.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex4.m1.1b"><apply id="S4.Ex4.m1.1.1.1.1.cmml" xref="S4.Ex4.m1.1.1.1"><eq id="S4.Ex4.m1.1.1.1.1.1.cmml" xref="S4.Ex4.m1.1.1.1.1.1"></eq><ci id="S4.Ex4.m1.1.1.1.1.2a.cmml" xref="S4.Ex4.m1.1.1.1.1.2"><mtext id="S4.Ex4.m1.1.1.1.1.2.cmml" xref="S4.Ex4.m1.1.1.1.1.2">Dice coef.</mtext></ci><apply id="S4.Ex4.m1.1.1.1.1.3.cmml" xref="S4.Ex4.m1.1.1.1.1.3"><divide id="S4.Ex4.m1.1.1.1.1.3.1.cmml" xref="S4.Ex4.m1.1.1.1.1.3"></divide><apply id="S4.Ex4.m1.1.1.1.1.3.2.cmml" xref="S4.Ex4.m1.1.1.1.1.3.2"><times id="S4.Ex4.m1.1.1.1.1.3.2.1.cmml" xref="S4.Ex4.m1.1.1.1.1.3.2.1"></times><cn type="integer" id="S4.Ex4.m1.1.1.1.1.3.2.2.cmml" xref="S4.Ex4.m1.1.1.1.1.3.2.2">2</cn><ci id="S4.Ex4.m1.1.1.1.1.3.2.3.cmml" xref="S4.Ex4.m1.1.1.1.1.3.2.3">ùëá</ci><ci id="S4.Ex4.m1.1.1.1.1.3.2.4.cmml" xref="S4.Ex4.m1.1.1.1.1.3.2.4">ùëÉ</ci></apply><apply id="S4.Ex4.m1.1.1.1.1.3.3.cmml" xref="S4.Ex4.m1.1.1.1.1.3.3"><plus id="S4.Ex4.m1.1.1.1.1.3.3.1.cmml" xref="S4.Ex4.m1.1.1.1.1.3.3.1"></plus><apply id="S4.Ex4.m1.1.1.1.1.3.3.2.cmml" xref="S4.Ex4.m1.1.1.1.1.3.3.2"><times id="S4.Ex4.m1.1.1.1.1.3.3.2.1.cmml" xref="S4.Ex4.m1.1.1.1.1.3.3.2.1"></times><cn type="integer" id="S4.Ex4.m1.1.1.1.1.3.3.2.2.cmml" xref="S4.Ex4.m1.1.1.1.1.3.3.2.2">2</cn><ci id="S4.Ex4.m1.1.1.1.1.3.3.2.3.cmml" xref="S4.Ex4.m1.1.1.1.1.3.3.2.3">ùëá</ci><ci id="S4.Ex4.m1.1.1.1.1.3.3.2.4.cmml" xref="S4.Ex4.m1.1.1.1.1.3.3.2.4">ùëÉ</ci></apply><apply id="S4.Ex4.m1.1.1.1.1.3.3.3.cmml" xref="S4.Ex4.m1.1.1.1.1.3.3.3"><times id="S4.Ex4.m1.1.1.1.1.3.3.3.1.cmml" xref="S4.Ex4.m1.1.1.1.1.3.3.3.1"></times><ci id="S4.Ex4.m1.1.1.1.1.3.3.3.2.cmml" xref="S4.Ex4.m1.1.1.1.1.3.3.3.2">ùêπ</ci><ci id="S4.Ex4.m1.1.1.1.1.3.3.3.3.cmml" xref="S4.Ex4.m1.1.1.1.1.3.3.3.3">ùëÉ</ci></apply><apply id="S4.Ex4.m1.1.1.1.1.3.3.4.cmml" xref="S4.Ex4.m1.1.1.1.1.3.3.4"><times id="S4.Ex4.m1.1.1.1.1.3.3.4.1.cmml" xref="S4.Ex4.m1.1.1.1.1.3.3.4.1"></times><ci id="S4.Ex4.m1.1.1.1.1.3.3.4.2.cmml" xref="S4.Ex4.m1.1.1.1.1.3.3.4.2">ùêπ</ci><ci id="S4.Ex4.m1.1.1.1.1.3.3.4.3.cmml" xref="S4.Ex4.m1.1.1.1.1.3.3.4.3">ùëÅ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex4.m1.1c">\textrm{Dice coef.}=\frac{2TP}{2TP+FP+FN}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS1.p1.2" class="ltx_p">For an aggregate measure of model performance, the macro-average Dice coefficient was considered.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Unknown target test</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">After selecting a model based on this criterion, we evaluated its performance on the 1500 images from the set of images of the ‚Äòunknown‚Äô target, as described in ¬ß<a href="#S2.SS5" title="2.5 Unknown target test set ‚Ä£ 2 Dataset ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.5</span></a>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Practical test</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">To evaluate how well a model trained on these data might perform in a practical application, we obtained three photographs of unmanned spacecraft in low Earth orbit taken from the vantage point of another on-orbit spacecraft and annotated them by hand, evaluating the best-performing segmentation model on each.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<figure id="S5.T2" class="ltx_table">
<div id="S5.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:443.6pt;height:179.1pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-24.6pt,9.9pt) scale(0.9,0.9) ;">
<table id="S5.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T2.1.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th id="S5.T2.1.1.1.1.3" class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th id="S5.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="3"><span id="S5.T2.1.1.1.1.4.1" class="ltx_text ltx_font_bold">S√∏rensen‚ÄìDice coefficient</span></th>
</tr>
<tr id="S5.T2.1.1.2.2" class="ltx_tr">
<th id="S5.T2.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row"><span id="S5.T2.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Model architecture</span></th>
<th id="S5.T2.1.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row"><span id="S5.T2.1.1.2.2.2.1" class="ltx_text ltx_font_bold">Backbone</span></th>
<th id="S5.T2.1.1.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row"><span id="S5.T2.1.1.2.2.3.1" class="ltx_text ltx_font_bold">Loss function</span></th>
<th id="S5.T2.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.1.1.2.2.4.1" class="ltx_text ltx_font_bold">Validation</span></th>
<th id="S5.T2.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.1.1.2.2.5.1" class="ltx_text ltx_font_bold">Test; known target</span></th>
<th id="S5.T2.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.1.1.2.2.6.1" class="ltx_text ltx_font_bold">Test; unknown target</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.3.1" class="ltx_tr">
<th id="S5.T2.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">DeepLab</th>
<th id="S5.T2.1.1.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ResNet50</th>
<th id="S5.T2.1.1.3.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">CCE</th>
<td id="S5.T2.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">0.7703</td>
<td id="S5.T2.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">0.7689</td>
<td id="S5.T2.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.1.1.3.1.6.1" class="ltx_text ltx_font_bold">0.2519</span></td>
</tr>
<tr id="S5.T2.1.1.4.2" class="ltx_tr">
<th id="S5.T2.1.1.4.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T2.1.1.4.2.2" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T2.1.1.4.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">Dice</th>
<td id="S5.T2.1.1.4.2.4" class="ltx_td ltx_align_center">0.7597</td>
<td id="S5.T2.1.1.4.2.5" class="ltx_td ltx_align_center">0.7593</td>
<td id="S5.T2.1.1.4.2.6" class="ltx_td ltx_align_center">0.2449</td>
</tr>
<tr id="S5.T2.1.1.5.3" class="ltx_tr">
<th id="S5.T2.1.1.5.3.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T2.1.1.5.3.2" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T2.1.1.5.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">Dice + focal</th>
<td id="S5.T2.1.1.5.3.4" class="ltx_td ltx_align_center">0.7869</td>
<td id="S5.T2.1.1.5.3.5" class="ltx_td ltx_align_center">0.7871</td>
<td id="S5.T2.1.1.5.3.6" class="ltx_td ltx_align_center">0.2502</td>
</tr>
<tr id="S5.T2.1.1.6.4" class="ltx_tr">
<th id="S5.T2.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">HRNet</th>
<th id="S5.T2.1.1.6.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">HRNet_w30</th>
<th id="S5.T2.1.1.6.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">CCE</th>
<td id="S5.T2.1.1.6.4.4" class="ltx_td ltx_align_center">0.7618</td>
<td id="S5.T2.1.1.6.4.5" class="ltx_td ltx_align_center">0.7618</td>
<td id="S5.T2.1.1.6.4.6" class="ltx_td ltx_align_center">0.2342</td>
</tr>
<tr id="S5.T2.1.1.7.5" class="ltx_tr">
<th id="S5.T2.1.1.7.5.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T2.1.1.7.5.2" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T2.1.1.7.5.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">Dice</th>
<td id="S5.T2.1.1.7.5.4" class="ltx_td ltx_align_center">0.7712</td>
<td id="S5.T2.1.1.7.5.5" class="ltx_td ltx_align_center">0.8043</td>
<td id="S5.T2.1.1.7.5.6" class="ltx_td ltx_align_center">0.2404</td>
</tr>
<tr id="S5.T2.1.1.8.6" class="ltx_tr">
<th id="S5.T2.1.1.8.6.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T2.1.1.8.6.2" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T2.1.1.8.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">Dice + focal</th>
<td id="S5.T2.1.1.8.6.4" class="ltx_td ltx_align_center">0.7878</td>
<td id="S5.T2.1.1.8.6.5" class="ltx_td ltx_align_center">0.7886</td>
<td id="S5.T2.1.1.8.6.6" class="ltx_td ltx_align_center">0.2371</td>
</tr>
<tr id="S5.T2.1.1.9.7" class="ltx_tr">
<th id="S5.T2.1.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">U-Net</th>
<th id="S5.T2.1.1.9.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">ResNet34</th>
<th id="S5.T2.1.1.9.7.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">CCE</th>
<td id="S5.T2.1.1.9.7.4" class="ltx_td ltx_align_center">0.8707</td>
<td id="S5.T2.1.1.9.7.5" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.9.7.5.1" class="ltx_text ltx_font_bold">0.8723</span></td>
<td id="S5.T2.1.1.9.7.6" class="ltx_td ltx_align_center">0.2282</td>
</tr>
<tr id="S5.T2.1.1.10.8" class="ltx_tr">
<th id="S5.T2.1.1.10.8.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T2.1.1.10.8.2" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T2.1.1.10.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">Dice</th>
<td id="S5.T2.1.1.10.8.4" class="ltx_td ltx_align_center">0.4423</td>
<td id="S5.T2.1.1.10.8.5" class="ltx_td ltx_align_center">0.4422</td>
<td id="S5.T2.1.1.10.8.6" class="ltx_td ltx_align_center">0.2284</td>
</tr>
<tr id="S5.T2.1.1.11.9" class="ltx_tr">
<th id="S5.T2.1.1.11.9.1" class="ltx_td ltx_th ltx_th_row ltx_border_b"></th>
<th id="S5.T2.1.1.11.9.2" class="ltx_td ltx_th ltx_th_row ltx_border_b"></th>
<th id="S5.T2.1.1.11.9.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">Dice + focal</th>
<td id="S5.T2.1.1.11.9.4" class="ltx_td ltx_align_center ltx_border_b">0.8389</td>
<td id="S5.T2.1.1.11.9.5" class="ltx_td ltx_align_center ltx_border_b">0.8395</td>
<td id="S5.T2.1.1.11.9.6" class="ltx_td ltx_align_center ltx_border_b">0.2357</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S5.T2.3.1" class="ltx_text ltx_font_bold">Test statistic (S√∏rensen‚ÄìDice coefficient) evaluated across experiments. The consistent gap in evaluation criterion between known and unknown target tests suggest that memorization of components and configurations of the spacecraft present in both the training and known target test sets is occurring.</span></figcaption>
</figure>
<figure id="S5.T3" class="ltx_table">
<div id="S5.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:325.6pt;height:210.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-18.1pt,11.7pt) scale(0.9,0.9) ;">
<table id="S5.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T3.1.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th id="S5.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2"><span id="S5.T3.1.1.1.1.3.1" class="ltx_text ltx_font_bold">S√∏rensen‚ÄìDice coefficient</span></th>
</tr>
<tr id="S5.T3.1.1.2.2" class="ltx_tr">
<th id="S5.T3.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" colspan="2"><span id="S5.T3.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Class</span></th>
<th id="S5.T3.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T3.1.1.2.2.2.1" class="ltx_text ltx_font_bold">Test; in-distribution</span></th>
<th id="S5.T3.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T3.1.1.2.2.3.1" class="ltx_text ltx_font_bold">Test; out-of-distribution</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.1.3.1" class="ltx_tr">
<th id="S5.T3.1.1.3.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">0.</th>
<th id="S5.T3.1.1.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">None / background</th>
<td id="S5.T3.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0.9991</td>
<td id="S5.T3.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">0.9972</td>
</tr>
<tr id="S5.T3.1.1.4.2" class="ltx_tr">
<th id="S5.T3.1.1.4.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">1.</th>
<th id="S5.T3.1.1.4.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Solar panels</th>
<td id="S5.T3.1.1.4.2.3" class="ltx_td ltx_align_center">0.9836</td>
<td id="S5.T3.1.1.4.2.4" class="ltx_td ltx_align_center">0.7673</td>
</tr>
<tr id="S5.T3.1.1.5.3" class="ltx_tr">
<th id="S5.T3.1.1.5.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">2.</th>
<th id="S5.T3.1.1.5.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Solar panel drive shaft</th>
<td id="S5.T3.1.1.5.3.3" class="ltx_td ltx_align_center">0.7142</td>
<td id="S5.T3.1.1.5.3.4" class="ltx_td ltx_align_center">0.0259</td>
</tr>
<tr id="S5.T3.1.1.6.4" class="ltx_tr">
<th id="S5.T3.1.1.6.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">3.</th>
<th id="S5.T3.1.1.6.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Antenna</th>
<td id="S5.T3.1.1.6.4.3" class="ltx_td ltx_align_center">0.6481</td>
<td id="S5.T3.1.1.6.4.4" class="ltx_td ltx_align_center">0.0003</td>
</tr>
<tr id="S5.T3.1.1.7.5" class="ltx_tr">
<th id="S5.T3.1.1.7.5.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">4.</th>
<th id="S5.T3.1.1.7.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Parabolic reflector</th>
<td id="S5.T3.1.1.7.5.3" class="ltx_td ltx_align_center">0.9493</td>
<td id="S5.T3.1.1.7.5.4" class="ltx_td ltx_align_center">0.0009</td>
</tr>
<tr id="S5.T3.1.1.8.6" class="ltx_tr">
<th id="S5.T3.1.1.8.6.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">5.</th>
<th id="S5.T3.1.1.8.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Main module</th>
<td id="S5.T3.1.1.8.6.3" class="ltx_td ltx_align_center">0.9838</td>
<td id="S5.T3.1.1.8.6.4" class="ltx_td ltx_align_center">0.6678</td>
</tr>
<tr id="S5.T3.1.1.9.7" class="ltx_tr">
<th id="S5.T3.1.1.9.7.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">6.</th>
<th id="S5.T3.1.1.9.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Telescope</th>
<td id="S5.T3.1.1.9.7.3" class="ltx_td ltx_align_center">0.9875</td>
<td id="S5.T3.1.1.9.7.4" class="ltx_td ltx_align_center">NA</td>
</tr>
<tr id="S5.T3.1.1.10.8" class="ltx_tr">
<th id="S5.T3.1.1.10.8.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">7.</th>
<th id="S5.T3.1.1.10.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Main thrusters</th>
<td id="S5.T3.1.1.10.8.3" class="ltx_td ltx_align_center">0.8707</td>
<td id="S5.T3.1.1.10.8.4" class="ltx_td ltx_align_center">0.0100</td>
</tr>
<tr id="S5.T3.1.1.11.9" class="ltx_tr">
<th id="S5.T3.1.1.11.9.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">8.</th>
<th id="S5.T3.1.1.11.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Rotational thrusters</th>
<td id="S5.T3.1.1.11.9.3" class="ltx_td ltx_align_center">0.6404</td>
<td id="S5.T3.1.1.11.9.4" class="ltx_td ltx_align_center">0.0046</td>
</tr>
<tr id="S5.T3.1.1.12.10" class="ltx_tr">
<th id="S5.T3.1.1.12.10.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">9.</th>
<th id="S5.T3.1.1.12.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Sensors</th>
<td id="S5.T3.1.1.12.10.3" class="ltx_td ltx_align_center">0.9486</td>
<td id="S5.T3.1.1.12.10.4" class="ltx_td ltx_align_center">0.0131</td>
</tr>
<tr id="S5.T3.1.1.13.11" class="ltx_tr">
<th id="S5.T3.1.1.13.11.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b">10.</th>
<th id="S5.T3.1.1.13.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">Launch vehicle adapter</th>
<td id="S5.T3.1.1.13.11.3" class="ltx_td ltx_align_center ltx_border_b">0.8699</td>
<td id="S5.T3.1.1.13.11.4" class="ltx_td ltx_align_center ltx_border_b">NA</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="S5.T3.3.1" class="ltx_text ltx_font_bold">S√∏renson-Dice coefficient by class for the best performing segmentation model at a test with a known target; a U-Net model with categorical cross-entropy loss. The unknown target spacecraft, Clementine, lacks a telescope (category 6) and a launch vehicle adapter similar to the spacecraft in the known target training set, instead having an inter-stage adapter that was not given its own class label.</span></figcaption>
</figure>
<figure id="S5.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2211.11941/assets/imgs/y-8-pretty.png" id="S5.F4.sf1.g1" class="ltx_graphics ltx_img_square" width="135" height="135" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>True mask, known target</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2211.11941/assets/imgs/pred-8-pretty.png" id="S5.F4.sf2.g1" class="ltx_graphics ltx_img_square" width="135" height="135" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Predicted mask, known target</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F4.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2211.11941/assets/imgs/unseen-true-mask.png" id="S5.F4.sf3.g1" class="ltx_graphics ltx_img_square" width="135" height="135" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>True mask, unknown target</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F4.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2211.11941/assets/imgs/unseen-predicted-mask.png" id="S5.F4.sf4.g1" class="ltx_graphics ltx_img_square" width="135" height="135" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Predicted mask, unknown target</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S5.F4.2.1" class="ltx_text ltx_font_bold">Example out-of-distribution true and predicted masks for a known (Chandra) and unknown (Clementine) target.</span></figcaption>
</figure>
<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Table <a href="#S5.T2" title="Table 2 ‚Ä£ 5 Results ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows S√∏rensen-Dice coefficients for each trained model on validation, known target test, and unknown target test sets. Performance on spacecraft present in the training and validation sets is consistently high, however, all of the trained models struggle on the unknown target spacecraft. The U-Net model with categorical cross-entropy loss showed highest performance on the known target test, with a S√∏renson-Dice coefficient of 0.8723; for an unknown target, the DeepLab model with CCE loss was highest at 0.2519. The gap in performance between known and unknown targets was relatively consistent across methods.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Figure <a href="#S5.F4" title="Figure 4 ‚Ä£ 5 Results ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows example ground truth and model-predicted masks for the selected U-Net model for a known and unknown target spacecraft.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Table 3 shows per-class S√∏rensen-Dice coefficients from the selected model on the in-distribution and out-of-distribution test data.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Practical test results</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Figure <a href="#S4.F3" title="Figure 3 ‚Ä£ 4 Experiments ‚Ä£ Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the results of the practical test of the highest performing segmentation model on three photographs, taken from the NASA Image and Video Library, of on-orbit unmanned spacecraft. The results are more or less consistent with synthetic images of unknown target spacecraft; notably, for the image of the Space Flyer Unit, some sensor or image artefacts, imperceptible to the human eye, resulted in false positive classification of the background and a ¬†0.05 degradation in S√∏renson-Dice coefficient; this could be mitigated in practise with classical object detection methods.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Benchmark results on these synthetic data show promise for semantic image segmentation in this domain, and show deep architectures for semantic segmentation can learn to recognize many different spacecraft components and categorize them appropriately by type.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Even with a high degree of class imbalance, Dice loss and Dice + focal loss did not always lead to an improvement in model performance, perhaps owing to CCE loss having a smoother gradient than that of Dice loss, resulting in a less noisy descent path during optimization. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.2" class="ltx_p">Our unknown target test shows that the selected model struggles to generalize beyond the four main spacecraft in the dataset for nearly every type of spacecraft component; the selected model is able to identify those larger components (main module, solar panels) of the Clementine spacecraft which have a variety of configurations in the training data, but did overfit to smaller components (sensors, thrusters, etc.), possibly by recognizing a limited number of examples of each. Notably, the selected model mis-classified the parabolic reflector for the Clementine spacecraft almost entirely (Dice <math id="S6.p3.1.m1.1" class="ltx_Math" alttext="&lt;0.001" display="inline"><semantics id="S6.p3.1.m1.1a"><mrow id="S6.p3.1.m1.1.1" xref="S6.p3.1.m1.1.1.cmml"><mi id="S6.p3.1.m1.1.1.2" xref="S6.p3.1.m1.1.1.2.cmml"></mi><mo id="S6.p3.1.m1.1.1.1" xref="S6.p3.1.m1.1.1.1.cmml">&lt;</mo><mn id="S6.p3.1.m1.1.1.3" xref="S6.p3.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p3.1.m1.1b"><apply id="S6.p3.1.m1.1.1.cmml" xref="S6.p3.1.m1.1.1"><lt id="S6.p3.1.m1.1.1.1.cmml" xref="S6.p3.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S6.p3.1.m1.1.1.2.cmml" xref="S6.p3.1.m1.1.1.2">absent</csymbol><cn type="float" id="S6.p3.1.m1.1.1.3.cmml" xref="S6.p3.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p3.1.m1.1c">&lt;0.001</annotation></semantics></math>); our synthetic data contained only one example of this type of component, on the NEAR spacecraft, which the selected model was able to classify quite well on test data (Dice <math id="S6.p3.2.m2.1" class="ltx_Math" alttext="\approx.95" display="inline"><semantics id="S6.p3.2.m2.1a"><mrow id="S6.p3.2.m2.1.1" xref="S6.p3.2.m2.1.1.cmml"><mi id="S6.p3.2.m2.1.1.2" xref="S6.p3.2.m2.1.1.2.cmml"></mi><mo id="S6.p3.2.m2.1.1.1" xref="S6.p3.2.m2.1.1.1.cmml">‚âà</mo><mn id="S6.p3.2.m2.1.1.3" xref="S6.p3.2.m2.1.1.3.cmml">.95</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p3.2.m2.1b"><apply id="S6.p3.2.m2.1.1.cmml" xref="S6.p3.2.m2.1.1"><approx id="S6.p3.2.m2.1.1.1.cmml" xref="S6.p3.2.m2.1.1.1"></approx><csymbol cd="latexml" id="S6.p3.2.m2.1.1.2.cmml" xref="S6.p3.2.m2.1.1.2">absent</csymbol><cn type="float" id="S6.p3.2.m2.1.1.3.cmml" xref="S6.p3.2.m2.1.1.3">.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p3.2.m2.1c">\approx.95</annotation></semantics></math>).</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">Semantic image segmentation for autonomous rendezvous may be achievable with these methods <em id="S6.p4.1.1" class="ltx_emph ltx_font_italic">if the target spacecraft is known</em>, pending further tests with physical simulation and on-orbit testing. Image segmentation for arbitrary targets may still be achievable with more representative training data or different methods. Shallower architectures may merit exploration, since this domain likely has a much smaller collection of salient features, and a smaller semantic space than more general segmentation tasks. Pre-training on large datasets of unrelated images such as ImageNet may have hindered, rather than helped, the benchmark models to learn the appropriate features.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">For the practical test, the observed evaluation metric seemed consistent with that of the unknown target test, suggesting a need to experiment with synthetic images of a target for which real sensor data are available. Additionally, our data synthesis pipeline did not simulate camera or sensor effects; this is one way in which the data synthesis pipeline presented here could be improved.</p>
</div>
<div id="S6.p6" class="ltx_para">
<p id="S6.p6.1" class="ltx_p">If these limitations can be addressed, a data synthesis pipeline such as this would allow for scaling the number of training data to the complexity of a machine learning task, enabling development of sophisticated perceptual systems <em id="S6.p6.1.1" class="ltx_emph ltx_font_italic">in silico</em> for applications where acquiring real sensor data at scale is prohibitively expensive.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>The authors would like to thank Jesse Trutna of Stanford‚Äôs Space Rendezvous Lab, for inspiring us to think about applications of machine learning to research in autonomous spacecraft operations; Kevin Okseniuk, a program manager at SpaceWorks Enterprises, for generously assisting the authors by helping define the semantic space for our class labels and teaching us how to recognize the components they correspond to; and Laura Rakiewicz, an architectural designer at Alliiance, who assisted in hand-annotation of the practical test photographs.


</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
M.¬†Z. Wong, K.¬†Kunii, M.¬†Baylis, W.¬†H. Ong, P.¬†Kroupa, and S.¬†Koller,
‚ÄúSynthetic dataset generation for object-to-model deep learning in
industrial applications,‚Äù <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">PeerJ Computer Science</em>, vol.¬†5, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S.¬†Minaee, Y.¬†Boykov, F.¬†Porikli, A.¬†Plaza, N.¬†Kehtarnavaz, and D.¬†Terzopoulos,
‚ÄúImage segmentation using deep learning: A survey,‚Äù <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions
on Pattern Analysis and Machine Intelligence</em>, vol.¬†44, no.¬†7, pp.
3523‚Äì3542, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S.¬†Ghosh, N.¬†Das, I.¬†Das, and U.¬†Maulik, ‚ÄúUnderstanding deep learning
techniques for image segmentation,‚Äù <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">ACM Comput. Surv.</em>, vol.¬†52,
no.¬†4, aug 2019. [Online]. Available: <a target="_blank" href="https://doi.org/10.1145/3329784" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3329784</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M.¬†Treml, J.¬†Arjona-Medina, T.¬†Unterthiner, R.¬†Durgesh, F.¬†Friedmann,
P.¬†Schuberth, A.¬†Mayr, M.¬†Heusel, M.¬†Hofmarcher, M.¬†Widrich, B.¬†Nessler, and
S.¬†Hochreiter, ‚ÄúSpeeding up semantic segmentation for autonomous driving,‚Äù
12 2016.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
G.¬†Arantes, E.¬†M. Rocco, I.¬†M. da Fonseca, and S.¬†Theil, ‚ÄúFar and proximity
maneuvers of a constellation of service satellites and autonomous pose
estimation of customer satellite using machine vision,‚Äù <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Acta
Astronautica</em>, vol.¬†66, no.¬†9, pp. 1493‚Äì1505, 2010. [Online]. Available:
<a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0094576509005724" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/S0094576509005724</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
M.¬†Kisantal, S.¬†Sharma, T.¬†H. Park, D.¬†Izzo, M.¬†M√§rtens, and S.¬†D‚ÄôAmico,
‚ÄúSatellite Pose Estimation Challenge: Dataset, Competition
Design, and Results,‚Äù <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Aerospace and
Electronic Systems</em>, vol.¬†56, no.¬†5, pp. 4083‚Äì4098, Oct. 2020, conference
Name: IEEE Transactions on Aerospace and Electronic Systems.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J.¬†Yang, J.¬†Lucas, T.¬†Kyono, M.¬†Abercrombie, A.¬†V. Berg, and J.¬†Fletcher,
‚ÄúSemantic segmentation of low earth orbit satellites using convolutional
neural networks,‚Äù in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">2022 IEEE Aerospace Conference (AERO)</em>, 2022, pp.
1‚Äì13.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
NASA, ‚ÄúModels | 3D Resources.‚Äù [Online]. Available:
<a target="_blank" href="https://nasa3d.arc.nasa.gov/models" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://nasa3d.arc.nasa.gov/models</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Blender Online Community, <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Blender - a 3D modelling and rendering
package</em>, Blender Foundation, Blender Institute, Amsterdam. [Online].
Available: <a target="_blank" href="http://www.blender.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.blender.org</a>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
‚ÄúPillow python library v8.4.0,‚Äù Nov. 2021. [Online].
Available: <a target="_blank" href="https://pillow.readthedocs.io/en/stable/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pillow.readthedocs.io/en/stable/</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
‚ÄúFastai python library v2.5.3,‚Äù Nov. 2021. [Online].
Available: <a target="_blank" href="https://docs.fast.ai/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://docs.fast.ai/</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
‚ÄúSemtorch python library v0.1.1,‚Äù Nov. 2021.
[Online]. Available: <a target="_blank" href="https://pypi.org/project/SemTorch/#description" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pypi.org/project/SemTorch/#description</a>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
‚ÄúPytorch python library v1.10.0,‚Äù Nov. 2021.
[Online]. Available: <a target="_blank" href="https://pytorch.org/docs/stable/index.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pytorch.org/docs/stable/index.html</a>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J.¬†Deng, W.¬†Dong, R.¬†Socher, L.-J. Li, K.¬†Li, and L.¬†Fei-Fei, ‚ÄúImagenet: A
large-scale hierarchical image database,‚Äù in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2009 IEEE conference on
computer vision and pattern recognition</em>.¬†¬†¬†Ieee, 2009, pp. 248‚Äì255.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
O.¬†Ronneberger, P.¬†Fischer, and T.¬†Brox, ‚ÄúU-net: Convolutional networks for
biomedical image segmentation,‚Äù in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Medical Image Computing and
Computer-Assisted Intervention ‚Äì MICCAI 2015</em>, N.¬†Navab, J.¬†Hornegger, W.¬†M.
Wells, and A.¬†F. Frangi, Eds.¬†¬†¬†Cham:
Springer International Publishing, 2015, pp. 234‚Äì241.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J.¬†Wang, K.¬†Sun, T.¬†Cheng, B.¬†Jiang, C.¬†Deng, Y.¬†Zhao, D.¬†Liu, Y.¬†Mu, M.¬†Tan,
X.¬†Wang, W.¬†Liu, and B.¬†Xiao, ‚ÄúDeep high-resolution representation learning
for visual recognition,‚Äù <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and
Machine Intelligence</em>, vol.¬†43, no.¬†10, pp. 3349‚Äì3364, 2021.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
L.-C. Chen, Y.¬†Zhu, G.¬†Papandreou, F.¬†Schroff, and H.¬†Adam, ‚ÄúEncoder-decoder
with atrous separable convolution for semantic image segmentation,‚Äù in
<em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Computer Vision ‚Äì ECCV 2018</em>, V.¬†Ferrari, M.¬†Hebert, C.¬†Sminchisescu,
and Y.¬†Weiss, Eds.¬†¬†¬†Cham: Springer
International Publishing, 2018, pp. 833‚Äì851.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
C.¬†H. Sudre, W.¬†Li, T.¬†K.¬†M. Vercauteren, S.¬†Ourselin, and M.¬†J. Cardoso,
‚ÄúGeneralised dice overlap as a deep learning loss function for highly
unbalanced segmentations,‚Äù <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Deep learning in medical image analysis and
multimodal learning for clinical decision support</em>, vol. 2017, pp. 240‚Äì248,
2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
T.¬†Lin, P.¬†Goyal, R.¬†Girshick, K.¬†He, and P.¬†Dollar, ‚ÄúFocal loss for dense
object detection,‚Äù <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em>, vol.¬†42, no.¬†02, pp. 318‚Äì327, feb 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
‚ÄúAlbumentations python library v8.4.0,‚Äù Nov. 2021.
[Online]. Available: <a target="_blank" href="https://albumentations.ai/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://albumentations.ai/</a>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
N.¬†Anantrasirichai and D.¬†Bull, ‚ÄúDefectnet: Multi-class fault detection on
highly-imbalanced datasets,‚Äù in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on
Image Processing (ICIP)</em>, 2019, pp. 2481‚Äì2485.

</span>
</li>
</ul>
</section>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\thebiography</span><span id="p1.2" class="ltx_ERROR undefined">{biographywithpic}</span>
<p id="p1.3" class="ltx_p">William S.¬†Armstrongimgs/heads/wsa.png
recieved a B.A.¬†in Mathematics from University of Minnesota in 2005. After a decade-long career in mathematical risk modeling for the insurance and financial services sector, he worked as a consultant in data science and machine learning, and now heads the machine learning research and development group at a Fortune 500 retailer. He is a part-time student with the Stanford Center for Professional Development, and has contributed to research in natural language processing. He is deeply passionate about astronomy and space exploration.</p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">{biographywithpic}</span>
<p id="p2.2" class="ltx_p">Spencer Drakontaidisimgs/heads/spencer.png
is a Software Engineer working on tools to help other engineers secure their code more easily and efficiently. He received his B.S.¬†in Computer Science from the United States Military Academy and a M.S.¬†in Computer Science from Stanford University. He is passionate about computer security and novel techniques to improve the way society interacts with computing.</p>
</div>
<div id="p3" class="ltx_para">
<span id="p3.1" class="ltx_ERROR undefined">{biographywithpic}</span>
<p id="p3.2" class="ltx_p">Nicholas Luiimgs/heads/nic.png
is a M.S.¬†in Statistics student at Stanford funded by the Knight-Hennessy Scholars program. He is passionate about the application of tech to social good and has worked on housing equity, criminal justice reform, and climate change. Nicholas received his B.A. degree in Economics from the University of Cambridge.</p>
</div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2211.11940" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2211.11941" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2211.11941">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2211.11941" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2211.11942" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 09:19:02 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
