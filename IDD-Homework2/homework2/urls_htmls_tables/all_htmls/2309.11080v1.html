<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2309.11080] Visual Question Answering in the Medical Domain</title><meta property="og:description" content="Medical visual question answering (Med-VQA) is a machine learning task that aims to create a system that can answer natural language questions based on given medical images. Although there has been rapid progress on th…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Visual Question Answering in the Medical Domain">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Visual Question Answering in the Medical Domain">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2309.11080">

<!--Generated on Wed Feb 28 04:58:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Computer Vision,  Natural Language Processing,  Medical Visual Question Answering,  Convolutional Neural Network,  Recurrent Neural Network,  Transformers,  Computed Tomography,  Magnetic Resonance Imaging
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Visual Question Answering in the Medical Domain</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Louisa Canepa1,
Sonit Singh1,
Arcot Sowmya1

<br class="ltx_break">
{l.canepa, sonit.singh, a.sowmya}@unsw.edu.au
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">1 School of Computer Science and Engineering, University of New South Wales, Sydney, Australia

</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Medical visual question answering (Med-VQA) is a machine learning task that aims to create a system that can answer natural language questions based on given medical images. Although there has been rapid progress on the general VQA task, less progress has been made on Med-VQA due to the lack of large-scale annotated datasets. In this paper, we present domain-specific pre-training strategies, including a novel contrastive learning pretraining method, to mitigate the problem of small datasets for the Med-VQA task. We find that the model benefits from components that use fewer parameters. We also evaluate and discuss the model’s visual reasoning using evidence verification techniques. Our proposed model obtained an accuracy of 60% on the VQA-Med 2019 test set, giving comparable results to other state-of-the-art Med-VQA models.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Computer Vision, Natural Language Processing, Medical Visual Question Answering, Convolutional Neural Network, Recurrent Neural Network, Transformers, Computed Tomography, Magnetic Resonance Imaging

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With recent advancements in the field of Computer Vision (CV) and Natural Language Processing (NLP), researchers have started looking at <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">cross-modal</em> problems that require deeper understanding of both images and text. Of the various tasks at the intersection of CV and NLP, Visual Question Answering (VQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> involves taking as input a natural language question and an image, and producing the correct natural language answer. The goal is to design Artificial Intelligence (AI) systems that can form a holistic understanding of images, and are able to effectively express that understanding in natural language. Inspired by the general domain VQA, Medical Visual Question Answering (Med-VQA) takes as input a natural language question and a medical image, and produces a plausible correct natural language answer as the output. The Med-VQA task has gained popularity more recently after the introduction of the ImageCLEF Med-VQA 2018 challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. However, the field is still at a nascent stage and there is still much progress to be made before Med-VQA systems are ready to be deployed in real clinical settings.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The field of medicine has seen great technological advancements with increases in the amount and accessibility of data. With the federal regulations around electronic health records (EHR), patients have now more access to their medical data than ever before. Given that patients can independently check their health records outside of their official consultations, there is an increased need for an accessible way to have their questions answered correctly. Patients can book a consultation with a doctor to obtain answers to their questions, but may be hesitant due to time and money constraints. On the other hand, patients have the option to rely on search engines and conversational agents such as Chat-GPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. However, there is an increased risk of getting misleading or incorrect information. To overcome these challenges, there is a need for a system that helps patients to better manage and understand their medical data without oversight from a healthcare professional. A Med-VQA system could fulfill this need, particularly as its inclusion of natural language question input makes it suited for answering unguided natural language questions, like those that patients may have about their medical images.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Most existing studies on Med-VQA rely on Convolutional Neural Networks (CNNs) for images and Recurrent Neural Networks (RNNs) for questions and answers as their building blocks. However, less attention has been paid to why a particular component was chosen over another, and why the base model was built in a particular way. There exists a trend in component choice towards more advanced modules without much clarity on why. Hence, it is important to investigate and understand the benefits (or lack) of advanced modules that are being adopted. Datasets for Med-VQA are small, presenting a challenge for a model attempting to learn patterns from them. Therefore, pretraining plays a crucial role in improving model performance. However, medical images and text can be very different in nature from general domain images and text. We hypothesise that pretraining on the medical domain instead could provide performance benefits, since the model is fine-tuned with more domain-specific knowledge that is more directly applicable to the task. Apart from Med-VQA model performance comparison, it is important to have evidence verification, involving visualisation techniques to understand why a particular model gives a particular output for a particular input, and it is crucial to unbox deep learning models in the medical field. Evidence verification is particularly important for a verified Med-VQA system, as it could be making diagnostic judgements about patient’s medical images. In this paper, we make the following contributions:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We systematically compare various components forming the image encoder, question encoder and answer encoder. This helps us to obtain a well-optimised Med-VQA model.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We evaluate the importance of domain-specific knowledge for the Med-VQA task. We not only used pretraining for images but also used pretraining for questions and answers, thereby forcing the Med-VQA model to utilise medical domain knowledge compared to pretrained components in the general domain.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We use evidence verification techniques to evaluate results. Specifically, we use Gradient Weighted Class Activation Mapping (GradCAM), highlighting regions of the image that are important for predicting a particular answer.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The rest of the paper is organised as follows: in section 2 related work is briefly reviewed; in section 3 details about the methodology are provided; in section 4 experimental results are presented. In section 5 the results and ablation studies are discussed. Finally, section 6 concludes this paper and recommends future directions.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><em id="S2.p1.1.1" class="ltx_emph ltx_font_italic">Deep Learning</em> (DL), a sub-field of Machine Learning (ML), makes use of neural networks, which are complex models consisting of interconnected units (“neurons”) that aim to mimic the human brain in order to learn complex tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. DL gives systems the ability to extract relevant features automatically from the raw data and to be trained in an end-to-end manner. Over the past few decades, researchers have sought to push boundaries within individual fields such as CV and NLP. However, with the rise of DL, researchers found that DL has the advantage of being generalisable to a variety of tasks within a variety of fields. Therefore, researchers started focussing on problems that lie at the intersection of various fields such as CV and NLP, such as <em id="S2.p1.1.2" class="ltx_emph ltx_font_italic">Med-VQA</em>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The most common approach for a Med-VQA system is the <em id="S2.p2.1.1" class="ltx_emph ltx_font_italic">joint-embedding framework</em>. This framework consists of four components: an <em id="S2.p2.1.2" class="ltx_emph ltx_font_italic">image encoder</em> to extract visual features from an image, a <em id="S2.p2.1.3" class="ltx_emph ltx_font_italic">question encoder</em> to extract textual features from the question, a <em id="S2.p2.1.4" class="ltx_emph ltx_font_italic">feature fusion algorithm</em> to combine visual and textual features in a meaningful way and an <em id="S2.p2.1.5" class="ltx_emph ltx_font_italic">answer generation module</em> to predict an answer in the form of natural language. CNNs are specialised neural networks suitable for processing images or videos, and are typically used as an image encoder. Of the various CNNs, VGG-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> have been widely used in Med-VQA systems. Other options include Inception-Net or even an ensemble of different CNNs. For e.g., Gong <em id="S2.p2.1.6" class="ltx_emph ltx_font_italic">et al.</em>, noticed that the VQA-RAD dataset consists of CT, MRI and X-rays. They trained three separate ResNet models, one on each modality, and then selected the best network for a given input. However, deeper networks showed an overfitting problem due to the increase in model complexity and lack of training data. To compensate for the relatively smaller dataset sizes in Med-VQA, the use of <em id="S2.p2.1.7" class="ltx_emph ltx_font_italic">transfer learning</em> or <em id="S2.p2.1.8" class="ltx_emph ltx_font_italic">pretrained CNNs</em> has been widely adopted for Med-VQA systems. However, general domain images are very different to medical images in terms of their features.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">On question answering, Recurrent Neural Networks (RNNs), which are specialised neural networks suitable for processing sequential data such as text and speech, have been widely used. Although vanilla RNNs can successfully remember information with short-term dependencies, they struggle to remember information from further in the past (long-range dependency). To overcome this issue, Long Short-Term Memory (LSTM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and Gated Recurrent Unit (GRU) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> networks have been proposed. To further improve the long-range dependency problem, the attention mechanism was introduced and is widely used in conjunction with LSTM or GRU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. The <em id="S2.p3.1.1" class="ltx_emph ltx_font_italic">Transformer</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, an encoder-decoder architecture that is entirely based on the attention mechanism, has recently been the preferred choice for text modelling. Some studies (e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>) chose to discard complex language encoding and make use of light language encoding, such as template matching. This strategy became a popular choice for the VQA-Med 2020 and VQA-Med 2021 challenges <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Researchers found that questions in these datasets have a repetitive format and belong to only a single category (abnormality), and therefore require only a simple language encoder. For e.g., Liao <em id="S2.p3.1.2" class="ltx_emph ltx_font_italic">et al.</em> used Skeleton-based Sentence Mapping <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, creating a limited number of templates based on similar questions. However, this method has a clear limitation in that it requires a limited number of question types in order to work well. Bidirectional Encoder Representations for Transformers (BERT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> based on the Transformer model has also been widely applied for question encoding in Med-VQA.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">The choice of the fusion algorithm can extend from a simple pooling mechanism to complex attention mechanisms. Common fusion algorithms include simple concatenation, element-wise multiplication or element-wise sum of image and question features. However, studies showed that simple fusion algorithms are not expressive enough to capture complex associations between image and text, and the outer product of vectors should be used instead. Calculating the outer product is computationally very expensive, therefore methods such as Multimodal Compact Bilinear Pooling have been proposed. Attention mechanisms are much more complex than simple fusion, but can provide better performance as they aim to more meaningfully relate the image and question vectors. One commonly used attention mechanism is the Stacked Attention Network (SAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. SAN has multiple attention layers that interact with the image features multiple times. Each time, the network generates an attention distribution over the image, and adds this to the query vector to generate a “refined” query vector. This allows the network to infer the answer progressively by gradually filtering out irrelevant regions.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">The answer generation component aims to output the correct answer, given the fused image and question features. This can be implemented via classification or generation. In the classification method, models use a softmax layer that outputs one of a finite number of possible answers, whereas the generation method involves using a language decoder to output the correct answer, such as an RNN or Transformer decoder. The classification method is much simpler than the generation method, and works particularly well when the questions and answers are closed-ended, repetitive and therefore limited in number. However, it is clearly more rigid than the generation method, which can become an issue when questions and answers are more complex.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Dataset</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We use the VQA-Med 2019 challenge dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> as it is the largest dataset currently available for the Med-VQA task and has diversity in terms of question categories and image modalities. The dataset consists of 4,200 images from the MedPix database, with 15,992 corresponding question and answer pairs. In Figure <a href="#S3.F1" title="Figure 1 ‣ III-A Dataset ‣ III Methodology ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> a random set of examples from the dataset is shown. There are four possible question categories—modality, plane, abnormality and organ system.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.11080/assets/figures/fig_data_ex_modality.png" id="S3.F1.sf1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="79" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Modality category example</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.11080/assets/figures/fig_data_ex_plane.png" id="S3.F1.sf2.g1" class="ltx_graphics ltx_img_landscape" width="157" height="79" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Plane category example</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.11080/assets/figures/fig_data_ex_abnormality.png" id="S3.F1.sf3.g1" class="ltx_graphics ltx_img_landscape" width="157" height="79" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Abnormality category example</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F1.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.11080/assets/figures/fig_data_ex_organ.png" id="S3.F1.sf4.g1" class="ltx_graphics ltx_img_landscape" width="157" height="79" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Organ system category example</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example data from the VQA-Med 2019 dataset in each of the four categories.</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The questions in the VQA-Med dataset are generated artificially and then verified by humans. This allows data to be created faster and more cost-effectively, however it also introduces limitations as questions are likely to have less variation in structure. In Figure <a href="#S3.F2" title="Figure 2 ‣ III-A Dataset ‣ III Methodology ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> a graphical representation of the frequency of each possible word for the first four words of all questions in the dataset is provided. The innermost ring shows the distribution of the first word, the second ring splits this further by the next word and so on. Sections of the chart in white indicate the next words that make up less than 2% of questions. This chart shows that questions are rigid in structure, with many questions beginning the same way and appearing predominantly close-ended. Most questions begin with “is this…” or “which plane…”, which implies only one correct answer. This is further evident by examining the number of words in answers. More than 50% of answers consist of only one word, and more than 82% of answers have between one and three words. The shortness of answers indicates that there is not much opportunity for them to be worded differently. Based on this analysis, we conclude that the best answer generation strategy is likely to be classification rather than generation, since generation is more suited for open-ended questions.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2309.11080/assets/figures/fig_q_structure.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="560" height="555" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Distribution of the first four words in the VQA-Med 2019 dataset.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Model development</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We use a joint-embedding framework as the structure of our model, and test the performance of various components. An overview of the model structure as well as components tested can be seen in Figure <a href="#S3.F3" title="Figure 3 ‣ III-B Model development ‣ III Methodology ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2309.11080/assets/figures/fig_model.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1668" height="446" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An overview of the structure of our model and the various components tested.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The baseline model was constructed with simpler modules that are commonly used for Med-VQA, even though they may not be the most modern or advanced techniques available. This was done for the preliminary model in order to use methods that are well-tested, to provide a benchmark against which we can measure future experiments. The image is passed through a VGG-16 network that is pretrained on ImageNet to generate a 1-dimensional encoding of the image. As discussed in Section II, VGG-16 is a CNN that is quite straightforward in structure, with thirteen convolutional layers, five max-pooling layers and three dense layers. The question is first tokenised using pretrained word embeddings (specifically, we use BioWordVec embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>), which encodes the question as a numerical vector. This is then passed through an LSTM network to generate a 1-dimensional encoding of the questions. These vectors are then concatenated, and this final feature vector is passed through two fully connected layers to generate the final output class.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">To improve performance over the baseline, we considered other components that could be used for the image encoder, question encoder and fusion algorithms in the model. These substitutions are summarised in Table <a href="#S3.T1" title="TABLE I ‣ III-B Model development ‣ III Methodology ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Component substitutions to be tested</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Module</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Baseline</span></th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Alternative Component</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt"><span id="S3.T1.1.2.1.1.1" class="ltx_text ltx_font_bold">Image Encoder</span></th>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">VGG-16</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">ResNet-50, ResNet-152</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"><span id="S3.T1.1.3.2.1.1" class="ltx_text ltx_font_bold">Question Encoder</span></th>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LSTM</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">BERT Transformer</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t"><span id="S3.T1.1.4.3.1.1" class="ltx_text ltx_font_bold">Feature Fusion</span></th>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Concatenation</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Stacked Attention Network</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">ResNet is a natural substitution to make for the image encoder component, as it is a newer, more advanced network than VGG-Net, that seeks to solve the vanishing gradient problem and allow for much deeper networks. ResNet models achieve a higher accuracy than VGG-Net on the ImageNet classification task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">For the question encoder, we tested the performance of a BERT transformer compared to the LSTM network that was used for the baseline model. The transformer discards the complex recurrent structure that was used by the LSTM and other similar NLP models, instead using only attention mechanisms to process text. BERT is pretrained on English Wikipedia and BookCorpus for the language modelling task.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">In the baseline model, the image and question feature vectors were fused by concatenating the two vectors. This is one of the simplest methods, relying on the fully connected layers in the final answer generation component to form meaningful connections between the question and image feature vectors. However, as discussed before, using an attention mechanism could be a better way to fuse image and question feature vectors. We applied Stacked Attention Network (SAN), which uses multiple attention layers to progressively refine attention distributions over the image, in order to focus on parts of the image that are more relevant to the question. This could provide benefits to the model’s understanding of the image as it relates to the given question. To implement SAN, we discarded the fully connected layers from the image encoder to maintain positional information in the encoding, and pass computed image and question features to the SAN, rather than just concatenating them. We tested SAN with 2, 3, or 4 attention layers, achieving best results using 3 attention layers.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Incorporating medical domain knowledge</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">One issue that has been integral to applying deep learning models on the Med-VQA task is the lack of large-scale annotated datasets. Therefore, it is important to consider techniques that could help mitigate this issue. Both the image encoder (VGG-Net or ResNet) and language encoder (BERT) can be pretrained on general domain images or text, respectively. However, although this pretraining is invaluable, medical images and medical text in Med-VQA are undoubtedly different from the general domain. Incorporating medical domain knowledge as part of pretraining could help the model to learn representations that are more directly applicable to downstream tasks, leading to improved performance. We implemented pretraining for both the question encoder and the image encoder to evaluate the benefits of using medical-specific pretraining for the Med-VQA task.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">For the image encoder, given that there is no large-scale annotated dataset available, we used <em id="S3.SS3.p2.1.1" class="ltx_emph ltx_font_italic">self-supervised pretraining</em> which involves training the network on unlabelled data through tasks that allow it to learn a generalised representation of images. Of the various methods available for self-supervised pretraining, we applied the <em id="S3.SS3.p2.1.2" class="ltx_emph ltx_font_italic">contrastive learning</em> method, similar to the method implemented by SimCLR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, although modified for application to the VGG-Net encoder and using data augmentations that are more applicable to the medical image dataset. We used the Radiology Objects in COntext (ROCO) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, which consists of over 81,000 radiology images in a wide variety of imaging modalities. ROCO was chosen as it is large, diverse and has images similar to the Med-VQA task. The image encoder was pretrained for 80 epochs, with a batch size of 128.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">The original BERT transformer uses general domain BERT pretraining, which pretrains the model on two NLP tasks (language modelling and next-sentence prediction). The pretraining corpus consists of BooksCorpus (an approximately 800 million word collection of freely available novels) and English Wikipedia (approximately 2.5 billion words). However, since medical language can be very different from general domain language, using a significant amount of specialised terminology, it is thought that giving the transformer a better understanding of medical language could improve its performance. To investigate this, we used a BERT model called <em id="S3.SS3.p3.1.1" class="ltx_emph ltx_font_italic">BioBERT</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, pretrained on the same tasks as BERT, but using the PubMed corpus, which comprises more than 35 million citations for biomedical literature from MEDLINE, life science journals, and online books (approximately 4.5 billion words). This makes BioBERT particularly suited to biomedical NLP applications.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">Evidence Verification</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">In our experiments, we used classification accuracy to evaluate the performance of the model. However, quantitative evaluation cannot evaluate the quality of model reasoning. Evidence verification involves generating output that gives insight into why the model generated a particular answer, and it is crucial for deep learning models in the medical field, particularly when they may be making diagnostic judgements. We used Gradient Weighted Class Activation Mapping (GradCAM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> for evidence verification. GradCAM uses the gradients of a target output class flowing into the final convolutional layer of a network to produce a localisation map that highlights regions of the image that were important to predicting that class. In this way, GradCAM can produce a heat map over the input image showing the areas the model paid the most attention to in order to produce its answer. To implement GradCAM, we used a Python package<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/jacobgil/pytorch-grad-cam" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/jacobgil/pytorch-grad-cam</a></span></span></span>, with some modification in order to handle the two multi-modal inputs that are required for our network. We then qualitatively examined the heat map outputs.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Results</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">All experiments were implemented in Python using the PyTorch library<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. To ensure robustness of results, we performed five-fold cross-validation on the dataset. This was done by randomly splitting the dataset into training set (80%) and test set (20%), and repeating the process five times. The split was seeded with the same number for all versions of the model to enable fair comparison. The model was trained for 50 epochs, optimising loss with the Adam optimiser <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> with a learning rate of <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="1e-4" display="inline"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mrow id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml"><mn id="S4.p1.1.m1.1.1.2.2" xref="S4.p1.1.m1.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.p1.1.m1.1.1.2.1" xref="S4.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.p1.1.m1.1.1.2.3" xref="S4.p1.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">−</mo><mn id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><minus id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1"></minus><apply id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2"><times id="S4.p1.1.m1.1.1.2.1.cmml" xref="S4.p1.1.m1.1.1.2.1"></times><cn type="integer" id="S4.p1.1.m1.1.1.2.2.cmml" xref="S4.p1.1.m1.1.1.2.2">1</cn><ci id="S4.p1.1.m1.1.1.2.3.cmml" xref="S4.p1.1.m1.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">1e-4</annotation></semantics></math> and a batch size of 64. The input images were normalised and data augmentation was performed during training to increase dataset size and minimise model overfitting. We changed image brightness and contrast by 5% with a probability of 0.4, translation and rotation by 5 units with probability of 0.5, adding Gaussian blur with probability of 0.5 and adding Gaussian noise with a probability of 0.4.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Quantitative results</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The baseline model achieved an accuracy of <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="0.56\pm 0.01" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">0.56</mn><mo id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">±</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">0.56</cn><cn type="float" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">0.56\pm 0.01</annotation></semantics></math>. We can see that there is still some overfitting happening, despite the data augmentation. Qualitatively examining the model’s outputs, there is a marked difference in the model’s ability for different question categories. The model has reasonable accuracy on plane and organ system questions (78% and 74% respectively), and a lower accuracy on modality questions (64%). However the model’s performance on abnormality questions is extremely poor, at only 6%. This is as expected, since in this dataset most abnormality classes have very few examples, making it very difficult for the model to learn to recognise them. A limitation of the accuracy metric is that it cannot account for cases where the model was technically correct but gave the answer in different wording. For example, in a question asking “What is abnormal in the CT scan?” the model answers “Pulmonary embolism”, where the ground truth answer is “PE”, an acronym that stands for pulmonary embolism. These types of issues can only be fixed by someone with professional medical knowledge updating the dataset to make these answers consistent. However, even accounting for these cases, the model’s performance on abnormality questions is still extremely low compared to other question categories.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Test accuracy achieved by each model variation.</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Model Variation</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Test Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">VGG-16 + LSTM + Concatenation</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.56</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_left">ResNet-50 + LSTM + Concatenation</td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center">0.54</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_left">ResNet-152 + LSTM + Concatenation</td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center">0.53</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<td id="S4.T2.1.5.4.1" class="ltx_td ltx_align_left"><span id="S4.T2.1.5.4.1.1" class="ltx_text ltx_font_bold">VGG-16 + BERT + Concatenation</span></td>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.4.2.1" class="ltx_text ltx_font_bold">0.60</span></td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<td id="S4.T2.1.6.5.1" class="ltx_td ltx_align_left">VGG-16 + BERT + SAN</td>
<td id="S4.T2.1.6.5.2" class="ltx_td ltx_align_center">0.58</td>
</tr>
<tr id="S4.T2.1.7.6" class="ltx_tr">
<td id="S4.T2.1.7.6.1" class="ltx_td ltx_align_left"><span id="S4.T2.1.7.6.1.1" class="ltx_text ltx_font_bold">VGG-16 + BioBERT + Concatenation</span></td>
<td id="S4.T2.1.7.6.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.7.6.2.1" class="ltx_text ltx_font_bold">0.60</span></td>
</tr>
<tr id="S4.T2.1.8.7" class="ltx_tr">
<td id="S4.T2.1.8.7.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T2.1.8.7.1.1" class="ltx_text ltx_font_bold">Pretrained VGG-16 + BERT + Concatenation</span></td>
<td id="S4.T2.1.8.7.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.8.7.2.1" class="ltx_text ltx_font_bold">0.60</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">In Table <a href="#S4.T2" title="TABLE II ‣ IV-A Quantitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> the overall test accuracy achieved by each of the model variations as detailed in Section III are shown. Overall best performance is achieved using a VGG-16 image encoder, BERT Transformer question encoder and concatenation as the feature fusion strategy, achieving a test accuracy of <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="0.60\pm 0.01" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mn id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">0.60</mn><mo id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">±</mo><mn id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">0.60</cn><cn type="float" id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">0.60\pm 0.01</annotation></semantics></math>. For the image encoder, we find that deeper and more complex networks such as ResNet-50 or ResNet-152 do not provide better results and in fact demonstrate a higher degree of overfitting. This clearly highlights the issue of the small dataset size of Med-VQA. For question encoding, the BERT transformer gave higher test performance compared to the LSTM network.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">In Table <a href="#S4.T3" title="TABLE III ‣ IV-A Quantitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> the test accuracy is shown by question category for the baseline model compared to the model with highest test accuracy, showing in more detail exactly what aspects the latter model improves on. There is a significant increase in accuracy in the modality category (+12%) and a modest increase in the abnormality category (+2%), which allows the +BERT model to achieve an increased overall accuracy.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Accuracy of the baseline versus BERT model per category type.</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Model Variation</span></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Baseline</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">+BERT</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<th id="S4.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Modality</th>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.64</td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.76</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<th id="S4.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Plane</th>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_center">0.78</td>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_center">0.77</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<th id="S4.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Organ</th>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_center">0.74</td>
<td id="S4.T3.1.4.3.3" class="ltx_td ltx_align_center">0.74</td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<th id="S4.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Abnormality</th>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_align_center">0.06</td>
<td id="S4.T3.1.5.4.3" class="ltx_td ltx_align_center">0.08</td>
</tr>
<tr id="S4.T3.1.6.5" class="ltx_tr">
<th id="S4.T3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Overall</th>
<td id="S4.T3.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.56</td>
<td id="S4.T3.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.6.5.3.1" class="ltx_text ltx_font_bold">0.60</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.11080/assets/figures/fig_bert_ex_2.png" id="S4.F4.sf1.g1" class="ltx_graphics ltx_img_square" width="138" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Category misclassification</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.11080/assets/figures/fig_bert_ex_1.png" id="S4.F4.sf2.g1" class="ltx_graphics ltx_img_square" width="138" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Wrong answer type</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F4.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.11080/assets/figures/fig_bert_ex_3.png" id="S4.F4.sf3.g1" class="ltx_graphics ltx_img_square" width="138" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Not choosing from options</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Example responses from LSTM and BERT variations of model.</figcaption>
</figure>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">By examining the model’s answers, we find that this improvement is due to the model’s improved understanding of the question. For example, the model no longer misclassifies question categories. In the baseline model, approximately 8% of abnormality questions were misclassified as other categories by the model, whereas the BERT model now correctly identifies all abnormality questions (e.g. Figure  <a href="#S4.F4" title="Figure 4 ‣ IV-A Quantitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>a), providing a performance benefit. Secondly, the BERT model is better able to understand the required answer type, and always answers questions in a way that makes sense. For example, in Figure <a href="#S4.F4" title="Figure 4 ‣ IV-A Quantitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>b, the baseline model incorrectly identifies this question as requiring a yes/no answer, whereas the BERT model is able to give a reasonable answer to the question. Similarly, as shown in Figure <a href="#S4.F4" title="Figure 4 ‣ IV-A Quantitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>c, the baseline model would sometimes incorrectly handle questions requiring the model to select from the given options, whereas the BERT model always chooses from the provided options for these questions. These improvements show that a better understanding of the question can lead to higher accuracy overall.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">Our results in Table <a href="#S4.T2" title="TABLE II ‣ IV-A Quantitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> show that SAN as a feature fusion method did not improve results compared to the concatenation method. This is because there is simply not enough data for the model to learn a useful refined attention distribution, and the added complexity of generating an attention distribution can actually cause the model to obscure some useful information. By qualitatively examining the model’s generated attention distributions (examples of which are shown in Figure <a href="#S4.F5" title="Figure 5 ‣ IV-A Quantitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, with original images on the left, and attention distributions on the right), we find that the issue is that the questions in the dataset need either a holistic view of the image (for example most questions in the modality, plane and organ system categories), or a very strong focus on a small part of the image (for example most abnormality questions). For the former type, we find that the model either attends to all parts of the image more or less equally (e.g. Figure <a href="#S4.F5" title="Figure 5 ‣ IV-A Quantitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>a), making attention redundant, or the attention distribution obscures relevant parts of the image (e.g. Figure <a href="#S4.F5" title="Figure 5 ‣ IV-A Quantitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>b). For questions requiring good localisation, the model instead tends to produce attention distributions of the type shown in Figure <a href="#S4.F5" title="Figure 5 ‣ IV-A Quantitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>c, where the model does not attend to any part of the image in particular. This is due to insufficient examples of particular abnormalities in the dataset, therefore the model is not able to learn the small areas of interest in these images, and instead does not focus on any part, leading to performance loss.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.11080/assets/figures/fig_attn_ex_1.png" id="S4.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="177" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.11080/assets/figures/fig_attn_ex_2.png" id="S4.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="177" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.11080/assets/figures/fig_attn_ex_3.png" id="S4.F5.sf3.g1" class="ltx_graphics ltx_img_landscape" width="177" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Examples of attention distribution output by SAN fusion method.</figcaption>
</figure>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p">The results in Table <a href="#S4.T2" title="TABLE II ‣ IV-A Quantitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> show that neither domain-specific pretraining method that we tested provided a performance benefit. Our testing of BioBERT as the question encoder showed that medical domain-specific pretraining does not appear to be important to the question encoder for the VQA-Med 2019 dataset. This is likely because of the lack of medical language used in the questions of this dataset. As previously noted, the questions in this dataset are rigid in structure, likely generated from a limited number of templates. Almost all medical language in the text comes from the answers, rather than the questions. Although there are some exceptions (e.g. “angiogram”, “gastrointestinal”, “ultrasound”), these medical terms are never crucial to understanding the question as a whole. Therefore, using a question encoder that has been pretrained on the medical domain does not appear to help the model better understand the questions in this dataset.</p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p">Self-supervised pretraining for the image encoder also did not appear to improve the final performance of the model. This is likely due to the distinct types of visual reasoning that are required for different questions. Some questions, such as organ system identification, require the model to be able to distinguish between large-scale differences in images. However, other questions, such as abnormality questions, require the model to be able to recognise very small-scale differences between images. Our results indicate that contrastive learning, at least in the generalised form that was tested here, is not beneficial for improving performance on the Med-VQA task, likely because it is not able to aid the model in performing both types of image differentiation.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Qualitative results</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In order to evaluate the quality of the model’s reasoning when producing answers, we used GradCAM to produce visualisations of the model’s attention over the input image, and Figure <a href="#S4.F6" title="Figure 6 ‣ IV-B Qualitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows examples of these outputs.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.11080/assets/figures/fig_good_gradcam_1.png" id="S4.F6.sf1.g1" class="ltx_graphics ltx_img_square" width="118" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.11080/assets/figures/fig_good_gradcam_2.png" id="S4.F6.sf2.g1" class="ltx_graphics ltx_img_square" width="118" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F6.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.11080/assets/figures/fig_bad_gradcam_1.png" id="S4.F6.sf3.g1" class="ltx_graphics ltx_img_square" width="118" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F6.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.11080/assets/figures/fig_bad_gradcam_2.png" id="S4.F6.sf4.g1" class="ltx_graphics ltx_img_square" width="118" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F6.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.11080/assets/figures/fig_ok_gradcam.png" id="S4.F6.sf5.g1" class="ltx_graphics ltx_img_square" width="118" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Example output from GradCAM.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">We found that for the most part, the model does well at predicting the answer from reasonable parts of the image. In Figure <a href="#S4.F6" title="Figure 6 ‣ IV-B Qualitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>a, we see that when asked to determine whether the image is T1 weighted, the model focusses on the light band in the centre of the image. Since this band is dark in T1 images, this is a good reason for the model to make its prediction. In Figure <a href="#S4.F6" title="Figure 6 ‣ IV-B Qualitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>b, the model is asked to determine the plane of the MRI, and focusses in particular on the back of the neck and the eye area to produce the correct “sagittal” answer. These two features would only occur in skull images taken from the sagittal plane, so this is again good reasoning.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">There are however some cases where the model’s visual reasoning could be improved. This can either mean that it gives the correct answer for the wrong reason, or it gives the wrong answer because it focusses on irrelevant areas of the image. GradCAM allows us to identify both of these cases, examples of which are shown in Figure <a href="#S4.F6" title="Figure 6 ‣ IV-B Qualitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>c and Figure <a href="#S4.F6" title="Figure 6 ‣ IV-B Qualitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>d. In Figure <a href="#S4.F6" title="Figure 6 ‣ IV-B Qualitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>c, the model correctly identifies the image to be a gastrointestinal image, however the GradCAM output shows that this decision was based predominantly on the presence of the text in the bottom left corner of the image. This indicates that the model has identified this as a shortcut in the data, and has not really learned the correct features. In Figure <a href="#S4.F6" title="Figure 6 ‣ IV-B Qualitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>d the model provides the wrong answer due to wrong reasoning. The model does give an abnormality as the answer, showing that it has identified the correct question category, but has not selected the correct abnormality. The heat map reveals this to be likely a guess, based on the fact that the model was not focussing on the bone at all.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Finally, analysis of GradCAM outputs can also identify answers that were almost correct. For example, in Figure <a href="#S4.F6" title="Figure 6 ‣ IV-B Qualitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>e, the model is asked for the abnormality, and answers with a disorder that is similar but not the same as the ground truth. Comparing the heat map with the diagram in Figure <a href="#S4.F7" title="Figure 7 ‣ IV-B Qualitative results ‣ IV Results ‣ Visual Question Answering in the Medical Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows that the model was focussing on the correct parts of the image to be able to distinguish between the two disorders. Its failure to predict the correct answer indicates that there was not enough training data for the model to accurately distinguish between the two disorders, but also shows that the model was doing the correct reasoning on the image. With more data, the model would certainly be able to improve its accuracy on abnormality images.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2309.11080/assets/figures/fig_spondylolysis.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="484" height="410" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Spondylolysis vs. spondylolisthesis<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Discussion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we quantitatively compared the performance of various components that are commonly used for the Med-VQA task, allowing us to evaluate their appropriateness for this task against each other. Our results show that, in general, less complex models with fewer parameters tend to perform better on this dataset. We find that models that achieve better performance in the general domain do not necessarily achieve better performance on the Med-VQA task. The Med-VQA field deals with a low-data regime, and therefore benefits from simpler models that are less likely to overfit.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">We conducted domain-specific pretraining in both the question and image encoders. For the question encoder, we found that domain knowledge is not useful for the VQA-Med 2019 dataset, as the questions use limited medical terminology. However, it is possible that a more complex dataset could benefit from question encoder pretraining. For the image encoder, we developed and tested a contrastive learning pretraining method. Our results indicate that this pretraining did not benefit our model, and we propose that Med-VQA is not suited to such a generalised contrastive learning method, owing to the different kinds of visual reasoning required for different questions–some requiring an understanding of large-scale differences, and others requiring an understanding of very small-scale differences.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">We used GradCAM to output heat maps corresponding to the visual attention of our model, allowing us to better evaluate our results by gaining insights into the model’s visual reasoning. We find that this provides great benefits to our model’s interpretability, allowing us to gain a deeper understanding of the strengths and shortcomings of the model, beyond the quantitative accuracy metric.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">One of the biggest challenges inherent to the Med-VQA field is the lack of data. Datasets are very small, but consist of a very wide variety of different possible answers, presenting a barrier to achieving very high accuracy on the task. In this work we investigated self-supervised contrastive learning as a domain-specific pretraining method to mitigate this issue, but found that it did not aid model performance. However, other domain-specific pretraining methods could still be investigated that may help this problem. Further, pretraining alone will not be able to fully overcome this issue. In future, focus should be on generating significantly larger datasets. Future work could involve investigation into automated dataset generation methods for this task, in order to generate bigger and more diverse datasets for training. Better evidence verification techniques and evaluation are also crucial to further development in this area. For medical diagnosis, it is essential to be able to verify why the system answered the way it did, to ensure that the answer is supported by the right evidence. Although we have found GradCAM to give good insight into the visual reasoning of the model, future work could, for example, explore the model’s attention on the question features to better evaluate how the model interprets the question.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusions</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work, we conducted an investigation of some existing and new techniques used in the Med-VQA task. We systematically evaluated some common Med-VQA components, and found that generally, simpler and shallower models benefit this task due to the small dataset size. We investigated the importance of domain knowledge in the question encoder and found that textual domain knowledge is not beneficial for this dataset. We developed and tested a contrastive learning pretraining method in the image encoder, and found that the contrastive learning method is not suited to the varied types of visual reasoning required for this task. Our final model achieved a 60% accuracy on the VQA-Med 2019 dataset. This matches current state of the art models in non-ensembled versions (e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>), suggesting that ensembling our model could provide further performance benefit. We also evaluated our model’s results and reasoning qualitatively using GradCAM, finding that as a whole our model is able to show good judgement in making decisions.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgement</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The authors would like to thank organisers of the ImageCLEF VQA-Med challenge for their time and effort in curation of the MED-VQA dataset, and sharing it for research purposes.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Declaration of competing interest</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">The authors declare no potential conflicts of interest.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and
D. Parikh, “VQA: Visual question answering,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE ICCV</em>, 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S. A. Hasan, Y. Ling, O. Farri, J. Liu, H. Müller, and M. Lungren,
“Overview of ImageCLEF 2018 medical domain visual question answering
task,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">CLEF (Working Notes)</em>, 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language
understanding by generative pre-training,” 2018.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
I. Goodfellow, Y. Bengio, and A. Courville, <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Deep Learning</em>.   MIT Press, 2016,
<a target="_blank" href="http://www.deeplearningbook.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.deeplearningbook.org</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2015.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE CVPR</em>, 2016, pp. 770–778.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Neural
computation</em>, vol. 9, no. 8, pp. 1735–1780, 1997.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
K. Cho, B. van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio, “Learning phrase representations using RNN
encoder–decoder for statistical machine translation,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Conference
on EMNLP</em>, 2014, pp. 1724–1734.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
learning to align and translate,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2015.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u.
Kaiser, and I. Polosukhin, “Attention is all you need,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Advances
in Neural Information Processing Systems</em>, 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Z. Liao, Q. Wu, C. Shen, A. Van Den Hengel, and J. Verjans, “AIML at
VQA-Med 2020: Knowledge inference via a skeleton-based sentence mapping
approach for medical domain visual question answering,” 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Z. Lin, D. Zhang, Q. Tac, D. Shi, G. Haffari, Q. Wu, M. He, and Z. Ge,
“Medical visual question answering: A survey,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2111.10056</em>, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of
deep bidirectional transformers for language understanding,” in
<em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">NAACL</em>, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Z. Yang, X. He, J. Gao, L. Deng, and A. Smola, “Stacked attention networks for
image question answering,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE CVPR</em>, 2016.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. B. Abacha, S. A. Hasan, V. V. Datla, J. Liu, D. Demner-Fushman, and
H. Müller, “VQA-Med: Overview of the medical visual question answering
task at ImageCLEF 2019.” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">CLEF (Working Notes)</em>, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Y. Zhang, Q. Chen, Z. Yang, H. Lin, and Z. Lu, “BioWordVec, improving
biomedical word embeddings with subword information and MeSH,”
<em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Scientific data</em>, vol. 6, no. 1, pp. 1–9, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for
contrastive learning of visual representations,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
O. Pelka, S. Koitka, J. Rückert, F. Nensa, and C. M. Friedrich, “Radiology
objects in context (ROCO): a multimodal image dataset,” in
<em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Intravascular Imaging and Computer Assisted Stenting and Large-Scale
Annotation of Biomedical Data and Expert Label Synthesis</em>.   Springer, 2018, pp. 180–189.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang, “BioBERT: a
pre-trained biomedical language representation model for biomedical text
mining,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Bioinformatics</em>, vol. 36, no. 4, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra,
“Grad-CAM: Visual explanations from deep networks via gradient-based
localization,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE ICCV</em>, 2017, pp. 618–626.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito,
M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and
S. Chintala, “PyTorch: An imperative style, high-performance deep learning
library,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems
32</em>.   Curran Associates, Inc., 2019,
pp. 8024–8035.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in
<em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2015.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
American Academy of Orthopaedic Surgeons, “Spondylolysis and
spondylolisthesis,” 2020. [Online]. Available:
<a target="_blank" href="https://orthoinfo.aaos.org/en/diseases--conditions/spondylolysis-and-spondylolisthesis" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://orthoinfo.aaos.org/en/diseases--conditions/spondylolysis-and-spondylolisthesis</a>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Y. Khare, V. Bagal, M. Mathew, A. Devi, U. D. Priyakumar, and C. Jawahar,
“MMBERT: Multimodal BERT pretraining for improved medical VQA,” in
<em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ISBI)</em>.   IEEE, 2021, pp.
1033–1036.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
F. Ren and Y. Zhou, “CGMVQA: A new classification and generative model for
medical visual question answering,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 8, pp.
50 626–50 636, 2020.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2309.11079" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2309.11080" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2309.11080">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2309.11080" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2309.11081" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 04:58:38 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
