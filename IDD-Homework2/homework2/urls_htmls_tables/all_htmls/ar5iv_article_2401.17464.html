<article class="ltx_document">
 <h1 class="ltx_title ltx_title_document">
  Efficient Tool Use with Chain-of-Abstraction Reasoning
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <span class="ltx_text ltx_font_bold" id="id4.4.4">
     Silin Gao
     <sup class="ltx_sup" id="id4.4.4.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id4.4.4.1.1">
       1,2∗
      </span>
     </sup>
     , Jane Dwivedi-Yu
     <sup class="ltx_sup" id="id4.4.4.2">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id4.4.4.2.1">
       2
      </span>
     </sup>
     , Ping Yu
     <sup class="ltx_sup" id="id4.4.4.3">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id4.4.4.3.1">
       2
      </span>
     </sup>
     , Xiaoqing Ellen Tan
     <sup class="ltx_sup" id="id4.4.4.4">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id4.4.4.4.1">
       2
      </span>
     </sup>
     ,
    </span>
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_bold" id="id7.7.7">
     Ramakanth Pasunuru
     <sup class="ltx_sup" id="id7.7.7.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id7.7.7.1.1">
       2
      </span>
     </sup>
     , Olga Golovneva
     <sup class="ltx_sup" id="id7.7.7.2">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id7.7.7.2.1">
       2
      </span>
     </sup>
     , Koustuv Sinha
     <sup class="ltx_sup" id="id7.7.7.3">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id7.7.7.3.1">
       2
      </span>
     </sup>
    </span>
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_bold" id="id10.10.10">
     Asli Celikyilmaz
     <sup class="ltx_sup" id="id10.10.10.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id10.10.10.1.1">
       2
      </span>
     </sup>
     , Antoine Bosselut
     <sup class="ltx_sup" id="id10.10.10.2">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id10.10.10.2.1">
       1
      </span>
     </sup>
     , Tianlu Wang
     <sup class="ltx_sup" id="id10.10.10.3">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id10.10.10.3.1">
       2
      </span>
     </sup>
    </span>
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id19.16.id1">
     1
    </sup>
    EPFL,
    <sup class="ltx_sup" id="id20.17.id2">
     2
    </sup>
    FAIR @ Meta
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id21.18.id3">
     1
    </sup>
    <span class="ltx_text ltx_font_typewriter" id="id22.19.id4">
     {silin.gao,antoine.bosselut}@epfl.ch
    </span>
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id23.20.id5">
     2
    </sup>
    <span class="ltx_text ltx_font_typewriter" id="id24.21.id6">
     {silingao,janeyu,pingyu,ellenxtan}@meta.com
    </span>
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id25.22.id7">
     2
    </sup>
    <span class="ltx_text ltx_font_typewriter" id="id26.23.id8">
     {rpasunuru,olggol,koustuvs,aslic,tianluwang}@meta.com
    </span>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id27.id1">
   To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (
   <span class="ltx_text ltx_font_italic" id="id27.id1.1">
    e.g.
   </span>
   , web facts, math and physical rules).
Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (
   <span class="ltx_text ltx_font_italic" id="id27.id1.2">
    e.g.
   </span>
   , Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning.
  </p>
  <p class="ltx_p" id="id18.3">
   In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning.
Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge.
This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (
   <span class="ltx_text ltx_font_italic" id="id18.3.1">
    e.g.
   </span>
   , math results) relevant to different reasoning questions.
It also allows LLMs to perform decoding and calling of external tools in parallel, which avoids the inference delay caused by waiting for tool responses.
In mathematical reasoning and Wiki QA domains, we show that our method consistently outperforms previous chain-of-thought and tool-augmented baselines on both in-distribution and out-of-distribution test sets, with an average
   <math alttext="\sim 6\%" class="ltx_Math" display="inline" id="id16.1.m1.1">
    <semantics id="id16.1.m1.1a">
     <mrow id="id16.1.m1.1.1" xref="id16.1.m1.1.1.cmml">
      <mi id="id16.1.m1.1.1.2" xref="id16.1.m1.1.1.2.cmml">
      </mi>
      <mo id="id16.1.m1.1.1.1" xref="id16.1.m1.1.1.1.cmml">
       ∼
      </mo>
      <mrow id="id16.1.m1.1.1.3" xref="id16.1.m1.1.1.3.cmml">
       <mn id="id16.1.m1.1.1.3.2" xref="id16.1.m1.1.1.3.2.cmml">
        6
       </mn>
       <mo id="id16.1.m1.1.1.3.1" xref="id16.1.m1.1.1.3.1.cmml">
        %
       </mo>
      </mrow>
     </mrow>
     <annotation-xml encoding="MathML-Content" id="id16.1.m1.1b">
      <apply id="id16.1.m1.1.1.cmml" xref="id16.1.m1.1.1">
       <csymbol cd="latexml" id="id16.1.m1.1.1.1.cmml" xref="id16.1.m1.1.1.1">
        similar-to
       </csymbol>
       <csymbol cd="latexml" id="id16.1.m1.1.1.2.cmml" xref="id16.1.m1.1.1.2">
        absent
       </csymbol>
       <apply id="id16.1.m1.1.1.3.cmml" xref="id16.1.m1.1.1.3">
        <csymbol cd="latexml" id="id16.1.m1.1.1.3.1.cmml" xref="id16.1.m1.1.1.3.1">
         percent
        </csymbol>
        <cn id="id16.1.m1.1.1.3.2.cmml" type="integer" xref="id16.1.m1.1.1.3.2">
         6
        </cn>
       </apply>
      </apply>
     </annotation-xml>
     <annotation encoding="application/x-tex" id="id16.1.m1.1c">
      \sim 6\%
     </annotation>
    </semantics>
   </math>
   absolute QA accuracy improvement.
LLM agents trained with our method also show more efficient tool use, with inference speed being on average
   <math alttext="\sim" class="ltx_Math" display="inline" id="id17.2.m2.1">
    <semantics id="id17.2.m2.1a">
     <mo id="id17.2.m2.1.1" xref="id17.2.m2.1.1.cmml">
      ∼
     </mo>
     <annotation-xml encoding="MathML-Content" id="id17.2.m2.1b">
      <csymbol cd="latexml" id="id17.2.m2.1.1.cmml" xref="id17.2.m2.1.1">
       similar-to
      </csymbol>
     </annotation-xml>
     <annotation encoding="application/x-tex" id="id17.2.m2.1c">
      \sim
     </annotation>
    </semantics>
   </math>
   <math alttext="1.4\times" class="ltx_math_unparsed" display="inline" id="id18.3.m3.1">
    <semantics id="id18.3.m3.1a">
     <mrow id="id18.3.m3.1b">
      <mn id="id18.3.m3.1.1">
       1.4
      </mn>
      <mo id="id18.3.m3.1.2" lspace="0.222em">
       ×
      </mo>
     </mrow>
     <annotation encoding="application/x-tex" id="id18.3.m3.1c">
      1.4\times
     </annotation>
    </semantics>
   </math>
   faster than baseline tool-augmented LLMs.
  </p>
 </div>
 <div class="ltx_para ltx_noindent" id="p1">
  <div class="ltx_block ltx_align_bottom" id="p1.15">
   <p class="ltx_p" id="p1.15.16">
    <span class="ltx_text ltx_font_bold" id="p1.15.16.1">
     Efficient Tool Use with Chain-of-Abstraction Reasoning
    </span>
   </p>
   <br class="ltx_break ltx_centering"/>
   <p class="ltx_p ltx_align_center" id="p1.15.15" style="width:433.6pt;">
    <span class="ltx_text ltx_inline-block" id="p1.15.15.15" style="width:0.0pt;">
     <span class="ltx_tabular ltx_guessed_headers ltx_align_top" id="p1.15.15.15.15">
      <span class="ltx_thead">
       <span class="ltx_tr" id="p1.4.4.4.4.4">
        <span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="p1.4.4.4.4.4.4">
         <span class="ltx_text ltx_font_bold" id="p1.4.4.4.4.4.4.4">
          Silin Gao
          <sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.1">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.4.4.4.4.4.4.4.1.1">
            1,2∗
           </span>
          </sup>
          , Jane Dwivedi-Yu
          <sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.2">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.4.4.4.4.4.4.4.2.1">
            2
           </span>
          </sup>
          , Ping Yu
          <sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.3">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.4.4.4.4.4.4.4.3.1">
            2
           </span>
          </sup>
          , Xiaoqing Ellen Tan
          <sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.4">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.4.4.4.4.4.4.4.4.1">
            2
           </span>
          </sup>
          ,
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="p1.7.7.7.7.7">
        <span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="p1.7.7.7.7.7.3">
         <span class="ltx_text ltx_font_bold" id="p1.7.7.7.7.7.3.3">
          Ramakanth Pasunuru
          <sup class="ltx_sup" id="p1.7.7.7.7.7.3.3.1">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.7.7.7.7.7.3.3.1.1">
            2
           </span>
          </sup>
          , Olga Golovneva
          <sup class="ltx_sup" id="p1.7.7.7.7.7.3.3.2">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.7.7.7.7.7.3.3.2.1">
            2
           </span>
          </sup>
          , Koustuv Sinha
          <sup class="ltx_sup" id="p1.7.7.7.7.7.3.3.3">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.7.7.7.7.7.3.3.3.1">
            2
           </span>
          </sup>
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="p1.10.10.10.10.10">
        <span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="p1.10.10.10.10.10.3">
         <span class="ltx_text ltx_font_bold" id="p1.10.10.10.10.10.3.3">
          Asli Celikyilmaz
          <sup class="ltx_sup" id="p1.10.10.10.10.10.3.3.1">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.10.10.10.10.10.3.3.1.1">
            2
           </span>
          </sup>
          , Antoine Bosselut
          <sup class="ltx_sup" id="p1.10.10.10.10.10.3.3.2">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.10.10.10.10.10.3.3.2.1">
            1
           </span>
          </sup>
          , Tianlu Wang
          <sup class="ltx_sup" id="p1.10.10.10.10.10.3.3.3">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.10.10.10.10.10.3.3.3.1">
            2
           </span>
          </sup>
         </span>
        </span>
       </span>
      </span>
      <span class="ltx_tbody">
       <span class="ltx_tr" id="p1.12.12.12.12.12">
        <span class="ltx_td ltx_align_center" id="p1.12.12.12.12.12.2">
         <sup class="ltx_sup" id="p1.12.12.12.12.12.2.1">
          1
         </sup>
         EPFL,
         <sup class="ltx_sup" id="p1.12.12.12.12.12.2.2">
          2
         </sup>
         FAIR @ Meta
        </span>
       </span>
       <span class="ltx_tr" id="p1.13.13.13.13.13">
        <span class="ltx_td ltx_align_center" id="p1.13.13.13.13.13.1">
         <sup class="ltx_sup" id="p1.13.13.13.13.13.1.1">
          1
         </sup>
         <span class="ltx_text ltx_font_typewriter" id="p1.13.13.13.13.13.1.2">
          {silin.gao,antoine.bosselut}@epfl.ch
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="p1.14.14.14.14.14">
        <span class="ltx_td ltx_align_center" id="p1.14.14.14.14.14.1">
         <sup class="ltx_sup" id="p1.14.14.14.14.14.1.1">
          2
         </sup>
         <span class="ltx_text ltx_font_typewriter" id="p1.14.14.14.14.14.1.2">
          {silingao,janeyu,pingyu,ellenxtan}@meta.com
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="p1.15.15.15.15.15">
        <span class="ltx_td ltx_align_center" id="p1.15.15.15.15.15.1">
         <sup class="ltx_sup" id="p1.15.15.15.15.15.1.1">
          2
         </sup>
         <span class="ltx_text ltx_font_typewriter" id="p1.15.15.15.15.15.1.2">
          {rpasunuru,olggol,koustuvs,aslic,tianluwang}@meta.com
         </span>
        </span>
       </span>
      </span>
     </span>
    </span>
   </p>
   <br class="ltx_break ltx_centering"/>
  </div>
 </div>
 <span class="ltx_note ltx_role_footnotetext" id="footnotex1">
  <sup class="ltx_note_mark">
   1
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     1
    </sup>
    <span class="ltx_note_type">
     footnotetext:
    </span>
    Work done during Silin Gao’s internship at FAIR.
   </span>
  </span>
 </span>
 <figure class="ltx_figure" id="S0.F1">
  <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="363" id="S0.F1.g1" src="/html/2401.17464/assets/x1.png" width="461"/>
  <figcaption class="ltx_caption ltx_centering">
   <span class="ltx_tag ltx_tag_figure">
    Figure 1:
   </span>
   Overview of chain-of-abstraction reasoning with tools. Given a domain question (green scroll), a LLM is fine-tuned to first generate an abstract multi-step reasoning chain (blue bubble), and then call external tools to reify the chain with domain-specific knowledge (orange label). The final answer (yellow bubble) is obtained based on the reified chain of reasoning.
  </figcaption>
 </figure>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Recent large language models (LLMs;
    <cite class="ltx_cite ltx_citemacro_citep">
     Touvron et al.,
     <a class="ltx_ref" href="#bib.bib37" title="">
      2023b
     </a>
     ; Anil et al.,
     <a class="ltx_ref" href="#bib.bib1" title="">
      2023
     </a>
     ; OpenAI,
     <a class="ltx_ref" href="#bib.bib27" title="">
      2023
     </a>
    </cite>
    ), have made progress at interpreting and executing instructions
    <cite class="ltx_cite ltx_citemacro_citep">
     (Wei et al.,
     <a class="ltx_ref" href="#bib.bib39" title="">
      2021
     </a>
     ; Chung et al.,
     <a class="ltx_ref" href="#bib.bib8" title="">
      2022
     </a>
     )
    </cite>
    , but still make errors when recalling and composing world knowledge for their responses,
    <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">
     e.g.
    </span>
    , making unfactual statements
    <cite class="ltx_cite ltx_citemacro_citep">
     (Maynez et al.,
     <a class="ltx_ref" href="#bib.bib25" title="">
      2020
     </a>
     ; Ji et al.,
     <a class="ltx_ref" href="#bib.bib17" title="">
      2023
     </a>
     )
    </cite>
    , incorrect calculations
    <cite class="ltx_cite ltx_citemacro_citep">
     (Patel et al.,
     <a class="ltx_ref" href="#bib.bib29" title="">
      2021
     </a>
     )
    </cite>
    , etc. Using auxiliary tools (
    <span class="ltx_text ltx_font_italic" id="S1.p1.1.2">
     e.g.
    </span>
    , a search engine to provide credible facts, a calculator for accurate math operations, etc.) at inference time can mitigate some of these errors, motivating tool-augmented language models that integrate external API calls into their output generations
    <cite class="ltx_cite ltx_citemacro_citep">
     (Parisi et al.,
     <a class="ltx_ref" href="#bib.bib28" title="">
      2022
     </a>
     ; Schick et al.,
     <a class="ltx_ref" href="#bib.bib34" title="">
      2023
     </a>
     ; Hao et al.,
     <a class="ltx_ref" href="#bib.bib14" title="">
      2023b
     </a>
     )
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    However, we show that current tool-augmented LLMs,
    <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">
     e.g.
    </span>
    , Toolformer
    <cite class="ltx_cite ltx_citemacro_citep">
     (Schick et al.,
     <a class="ltx_ref" href="#bib.bib34" title="">
      2023
     </a>
     )
    </cite>
    , struggle to reliably and efficiently leverage tools in multi-step reasoning.
In particular, tool calls in multi-step reasoning tasks are often interleaved (
    <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">
     i.e.
    </span>
    , the response of an API call is often part of the query of a subsequent call; as shown in Figure
    <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    ).
Without explicitly modeling these interconnections in reasoning chains, LLMs do not learn effective planning for tool use, which leads to less accurate reasoning with tools.
    <span class="ltx_note ltx_role_footnote" id="footnote1">
     <sup class="ltx_note_mark">
      1
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        1
       </sup>
       <span class="ltx_tag ltx_tag_note">
        1
       </span>
       as verified by our analysis in §
       <a class="ltx_ref" href="#S5" title="5 Results and Analysis ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
        <span class="ltx_text ltx_ref_tag">
         5
        </span>
       </a>
      </span>
     </span>
    </span>
    Meanwhile, interleaving text generation with API calls also introduces inefficient inference “waiting times,” where the model must wait for the response from the API call before resuming the decoding process. This inefficiency becomes more significant in multi-step reasoning scenarios, when multiple rounds of API calls are typically required for each reasoning process.
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    In this work, we propose
    <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">
     C
    </span>
    hain-
    <span class="ltx_text ltx_font_bold" id="S1.p3.1.2">
     o
    </span>
    f-
    <span class="ltx_text ltx_font_bold" id="S1.p3.1.3">
     A
    </span>
    bstraction (
    <span class="ltx_text ltx_font_bold" id="S1.p3.1.4">
     CoA
    </span>
    ) reasoning, a robust and efficient method for LLMs to perform multi-step reasoning with tools.
As shown in Figure
    <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    , LLMs are fine-tuned with a goal of making reasoning chains with abstract placeholders.
The placeholders do not affect LLMs’ reasoning flow, and are subsequently infilled with specific knowledge retrieved from specialized tools, to ground the final answer generations.
Planning abstract chain of reasoning encourages LLMs to inter-connect multiple tool calls and adopt more feasible reasoning strategies, which are robust to the variation of domain knowledge involved in each reasoning process,
    <span class="ltx_text ltx_font_italic" id="S1.p3.1.5">
     e.g.
    </span>
    , specific calculation results.
Unlike previous methods where LLM decoding and API calls are executed in an interleaved manner, our method leverages tools to infill knowledge
    <span class="ltx_text ltx_font_bold" id="S1.p3.1.6">
     once
    </span>
    after the whole chain of reasoning is generated.
This enables more efficient decoding across multiple examples (
    <span class="ltx_text ltx_font_italic" id="S1.p3.1.7">
     e.g.
    </span>
    , as in a stream) because CoA traces for subsequent examples can be decoded while tool calls are made for the preceding ones, amortizing overall inference time.
We develop a simple pipeline to build fine-tuning data for models to learn CoA, where we first prompt LLMs to re-write existing responses to instructions as abstract chains, and then use domain tools to check the validity of re-writing, as shown in Figure
    <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3 Method ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.7">
    After training LLMs to learn CoA reasoning, we evaluate the finetuned models on two representative multi-step reasoning domains, including mathematical reasoning
    <cite class="ltx_cite ltx_citemacro_citep">
     (Cobbe et al.,
     <a class="ltx_ref" href="#bib.bib9" title="">
      2021
     </a>
     ; Miao et al.,
     <a class="ltx_ref" href="#bib.bib26" title="">
      2020
     </a>
     ; Patel et al.,
     <a class="ltx_ref" href="#bib.bib29" title="">
      2021
     </a>
     ; Koncel-Kedziorski et al.,
     <a class="ltx_ref" href="#bib.bib20" title="">
      2016
     </a>
     )
    </cite>
    , and Wikipedia (Wiki) QA
    <cite class="ltx_cite ltx_citemacro_citep">
     (Yang et al.,
     <a class="ltx_ref" href="#bib.bib43" title="">
      2018
     </a>
     ; Berant et al.,
     <a class="ltx_ref" href="#bib.bib3" title="">
      2013
     </a>
     ; Kwiatkowski et al.,
     <a class="ltx_ref" href="#bib.bib21" title="">
      2019
     </a>
     ; Joshi et al.,
     <a class="ltx_ref" href="#bib.bib19" title="">
      2017
     </a>
     )
    </cite>
    that involves reasoning on factual descriptive knowledge.
We show that our method boosts LLMs’ performances, with average
    <math alttext="\sim" class="ltx_Math" display="inline" id="S1.p4.1.m1.1">
     <semantics id="S1.p4.1.m1.1a">
      <mo id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">
       ∼
      </mo>
      <annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b">
       <csymbol cd="latexml" id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1">
        similar-to
       </csymbol>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">
       \sim
      </annotation>
     </semantics>
    </math>
    <math alttext="7.5\%" class="ltx_Math" display="inline" id="S1.p4.2.m2.1">
     <semantics id="S1.p4.2.m2.1a">
      <mrow id="S1.p4.2.m2.1.1" xref="S1.p4.2.m2.1.1.cmml">
       <mn id="S1.p4.2.m2.1.1.2" xref="S1.p4.2.m2.1.1.2.cmml">
        7.5
       </mn>
       <mo id="S1.p4.2.m2.1.1.1" xref="S1.p4.2.m2.1.1.1.cmml">
        %
       </mo>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="S1.p4.2.m2.1b">
       <apply id="S1.p4.2.m2.1.1.cmml" xref="S1.p4.2.m2.1.1">
        <csymbol cd="latexml" id="S1.p4.2.m2.1.1.1.cmml" xref="S1.p4.2.m2.1.1.1">
         percent
        </csymbol>
        <cn id="S1.p4.2.m2.1.1.2.cmml" type="float" xref="S1.p4.2.m2.1.1.2">
         7.5
        </cn>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S1.p4.2.m2.1c">
       7.5\%
      </annotation>
     </semantics>
    </math>
    and
    <math alttext="4.5\%" class="ltx_Math" display="inline" id="S1.p4.3.m3.1">
     <semantics id="S1.p4.3.m3.1a">
      <mrow id="S1.p4.3.m3.1.1" xref="S1.p4.3.m3.1.1.cmml">
       <mn id="S1.p4.3.m3.1.1.2" xref="S1.p4.3.m3.1.1.2.cmml">
        4.5
       </mn>
       <mo id="S1.p4.3.m3.1.1.1" xref="S1.p4.3.m3.1.1.1.cmml">
        %
       </mo>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="S1.p4.3.m3.1b">
       <apply id="S1.p4.3.m3.1.1.cmml" xref="S1.p4.3.m3.1.1">
        <csymbol cd="latexml" id="S1.p4.3.m3.1.1.1.cmml" xref="S1.p4.3.m3.1.1.1">
         percent
        </csymbol>
        <cn id="S1.p4.3.m3.1.1.2.cmml" type="float" xref="S1.p4.3.m3.1.1.2">
         4.5
        </cn>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S1.p4.3.m3.1c">
       4.5\%
      </annotation>
     </semantics>
    </math>
    absolute accuracy improvements on math and Wiki QA, respectively.
These improvements are consistent across both in-distribution
and (zero-shot) out-of-distribution test sets, and are especially pronounced on questions that require complex chain-of-thought reasoning.
    <span class="ltx_note ltx_role_footnote" id="footnote2">
     <sup class="ltx_note_mark">
      2
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        2
       </sup>
       <span class="ltx_tag ltx_tag_note">
        2
       </span>
       <span class="ltx_text ltx_font_italic" id="footnote2.1">
        e.g.
       </span>
       , more than 3 steps of math derivations
      </span>
     </span>
    </span>
    Meanwhile, our method also uses tools more efficiently than previous augmentation methods, with average
    <math alttext="\sim" class="ltx_Math" display="inline" id="S1.p4.4.m4.1">
     <semantics id="S1.p4.4.m4.1a">
      <mo id="S1.p4.4.m4.1.1" xref="S1.p4.4.m4.1.1.cmml">
       ∼
      </mo>
      <annotation-xml encoding="MathML-Content" id="S1.p4.4.m4.1b">
       <csymbol cd="latexml" id="S1.p4.4.m4.1.1.cmml" xref="S1.p4.4.m4.1.1">
        similar-to
       </csymbol>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S1.p4.4.m4.1c">
       \sim
      </annotation>
     </semantics>
    </math>
    <math alttext="1.47\times" class="ltx_math_unparsed" display="inline" id="S1.p4.5.m5.1">
     <semantics id="S1.p4.5.m5.1a">
      <mrow id="S1.p4.5.m5.1b">
       <mn id="S1.p4.5.m5.1.1">
        1.47
       </mn>
       <mo id="S1.p4.5.m5.1.2" lspace="0.222em">
        ×
       </mo>
      </mrow>
      <annotation encoding="application/x-tex" id="S1.p4.5.m5.1c">
       1.47\times
      </annotation>
     </semantics>
    </math>
    and
    <math alttext="1.33\times" class="ltx_math_unparsed" display="inline" id="S1.p4.6.m6.1">
     <semantics id="S1.p4.6.m6.1a">
      <mrow id="S1.p4.6.m6.1b">
       <mn id="S1.p4.6.m6.1.1">
        1.33
       </mn>
       <mo id="S1.p4.6.m6.1.2" lspace="0.222em">
        ×
       </mo>
      </mrow>
      <annotation encoding="application/x-tex" id="S1.p4.6.m6.1c">
       1.33\times
      </annotation>
     </semantics>
    </math>
    faster inference speeds on math and Wiki QA tasks, respectively.
Finally, extensive human evaluation demonstrates that our method guides LLMs to learn more accurate reasoning, which leads to
    <math alttext="\sim 8\%" class="ltx_Math" display="inline" id="S1.p4.7.m7.1">
     <semantics id="S1.p4.7.m7.1a">
      <mrow id="S1.p4.7.m7.1.1" xref="S1.p4.7.m7.1.1.cmml">
       <mi id="S1.p4.7.m7.1.1.2" xref="S1.p4.7.m7.1.1.2.cmml">
       </mi>
       <mo id="S1.p4.7.m7.1.1.1" xref="S1.p4.7.m7.1.1.1.cmml">
        ∼
       </mo>
       <mrow id="S1.p4.7.m7.1.1.3" xref="S1.p4.7.m7.1.1.3.cmml">
        <mn id="S1.p4.7.m7.1.1.3.2" xref="S1.p4.7.m7.1.1.3.2.cmml">
         8
        </mn>
        <mo id="S1.p4.7.m7.1.1.3.1" xref="S1.p4.7.m7.1.1.3.1.cmml">
         %
        </mo>
       </mrow>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="S1.p4.7.m7.1b">
       <apply id="S1.p4.7.m7.1.1.cmml" xref="S1.p4.7.m7.1.1">
        <csymbol cd="latexml" id="S1.p4.7.m7.1.1.1.cmml" xref="S1.p4.7.m7.1.1.1">
         similar-to
        </csymbol>
        <csymbol cd="latexml" id="S1.p4.7.m7.1.1.2.cmml" xref="S1.p4.7.m7.1.1.2">
         absent
        </csymbol>
        <apply id="S1.p4.7.m7.1.1.3.cmml" xref="S1.p4.7.m7.1.1.3">
         <csymbol cd="latexml" id="S1.p4.7.m7.1.1.3.1.cmml" xref="S1.p4.7.m7.1.1.3.1">
          percent
         </csymbol>
         <cn id="S1.p4.7.m7.1.1.3.2.cmml" type="integer" xref="S1.p4.7.m7.1.1.3.2">
          8
         </cn>
        </apply>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S1.p4.7.m7.1c">
       \sim 8\%
      </annotation>
     </semantics>
    </math>
    fewer reasoning errors.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Related Work
  </h2>
  <section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
   <h4 class="ltx_title ltx_title_paragraph">
    Tool-Augmented LLMs
   </h4>
   <div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
    <p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">
     There has been a growing interest in augmenting LLMs using external tools.
Considerable work has tried to adapt LLMs as tool-using reasoners through in-context learning, demonstrating promising performance improvements in various applications,
     <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px1.p1.1.1">
      e.g.
     </span>
     , math problem solving
     <cite class="ltx_cite ltx_citemacro_citep">
      (Gao et al.,
      <a class="ltx_ref" href="#bib.bib10" title="">
       2023
      </a>
      ; Chen et al.,
      <a class="ltx_ref" href="#bib.bib7" title="">
       2022
      </a>
      )
     </cite>
     , biomedical question answering
     <cite class="ltx_cite ltx_citemacro_citep">
      (Jin et al.,
      <a class="ltx_ref" href="#bib.bib18" title="">
       2023
      </a>
      )
     </cite>
     and self-critiquing
     <cite class="ltx_cite ltx_citemacro_citep">
      (Gou et al.,
      <a class="ltx_ref" href="#bib.bib12" title="">
       2023
      </a>
      )
     </cite>
     .
Nevertheless, guiding LLMs to effectively use tools using in-context demonstrations is challenging, which requires elaborate task-specific prompt engineering and is restricted by the model’s instruction following ability
     <cite class="ltx_cite ltx_citemacro_citep">
      (Jacovi et al.,
      <a class="ltx_ref" href="#bib.bib16" title="">
       2023
      </a>
      )
     </cite>
     . Noticing the limitations of in-context learning, several works teach LLMs to learn the usage of tools by fine-tuning
     <cite class="ltx_cite ltx_citemacro_citep">
      (Parisi et al.,
      <a class="ltx_ref" href="#bib.bib28" title="">
       2022
      </a>
      ; Schick et al.,
      <a class="ltx_ref" href="#bib.bib34" title="">
       2023
      </a>
      ; Hao et al.,
      <a class="ltx_ref" href="#bib.bib14" title="">
       2023b
      </a>
      )
     </cite>
     , which more robustly improves LLMs’ performance.
However, all above approaches adopt sequential interactions with tools throughout reasoning, slowing the inference speed as a function of the latency of the tool (or API) and the number of API calls that are made.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS0.SSS0.Px1.p2">
    <p class="ltx_p" id="S2.SS0.SSS0.Px1.p2.1">
     Some other prior works focus on using LLMs for multi-step reasoning with other modules.
In particular, ReAct
     <cite class="ltx_cite ltx_citemacro_citep">
      (Yao et al.,
      <a class="ltx_ref" href="#bib.bib46" title="">
       2023b
      </a>
      )
     </cite>
     and FireAct
     <cite class="ltx_cite ltx_citemacro_citep">
      (Chen et al.,
      <a class="ltx_ref" href="#bib.bib5" title="">
       2023
      </a>
      )
     </cite>
     integrate LLMs with tools into a closed loop of thought, action and observation steps.
This verbose reasoning loop slows down the LLM decoding, and still incorporates tools via sequential interactions, resulting in inefficient inference.
Another line of work, PAL
     <cite class="ltx_cite ltx_citemacro_citep">
      (Gao et al.,
      <a class="ltx_ref" href="#bib.bib10" title="">
       2023
      </a>
      )
     </cite>
     and Program of Thoughts
     <cite class="ltx_cite ltx_citemacro_citep">
      (Chen et al.,
      <a class="ltx_ref" href="#bib.bib7" title="">
       2022
      </a>
      )
     </cite>
     prompt LLMs to generate program-based reasoning and interact with code executors, which however, heavily rely on closed source coding models,
     <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px1.p2.1.1">
      i.e.
     </span>
     , Codex
     <cite class="ltx_cite ltx_citemacro_citep">
      (Chen et al.,
      <a class="ltx_ref" href="#bib.bib6" title="">
       2021
      </a>
      )
     </cite>
     , and are restricted to procedural arithmetic reasoning.
In our work, we aim to design a more general and efficient strategy for LLMs to leverage tools, especially on multi-step reasoning scenarios.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
   <h4 class="ltx_title ltx_title_paragraph">
    Tool Usage Planning
   </h4>
   <div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
    <p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">
     Several previous work research the planning of tool usage in LLMs.
Specifically, HuggingGPT
     <cite class="ltx_cite ltx_citemacro_citep">
      (Shen et al.,
      <a class="ltx_ref" href="#bib.bib35" title="">
       2023
      </a>
      )
     </cite>
     , Chameleon
     <cite class="ltx_cite ltx_citemacro_citep">
      (Lu et al.,
      <a class="ltx_ref" href="#bib.bib24" title="">
       2023
      </a>
      )
     </cite>
     , OpenAGI
     <cite class="ltx_cite ltx_citemacro_citep">
      (Ge et al.,
      <a class="ltx_ref" href="#bib.bib11" title="">
       2023
      </a>
      )
     </cite>
     and MetaTool
     <cite class="ltx_cite ltx_citemacro_citep">
      (Huang et al.,
      <a class="ltx_ref" href="#bib.bib15" title="">
       2023
      </a>
      )
     </cite>
     focus on planning the high-level sequence of using multiple tools to address multi-domain mixed tasks.
Similarly, LATM
     <cite class="ltx_cite ltx_citemacro_citep">
      (Cai et al.,
      <a class="ltx_ref" href="#bib.bib4" title="">
       2023
      </a>
      )
     </cite>
     , ML-BENCH
     <cite class="ltx_cite ltx_citemacro_citep">
      (Liu et al.,
      <a class="ltx_ref" href="#bib.bib22" title="">
       2023
      </a>
      )
     </cite>
     and Gorilla
     <cite class="ltx_cite ltx_citemacro_citep">
      (Patil et al.,
      <a class="ltx_ref" href="#bib.bib30" title="">
       2023
      </a>
      )
     </cite>
     aim at planning program-level integration of multiple APIs for designing scripts of procedural tasks,
     <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px2.p1.1.1">
      e.g.
     </span>
     , a script for training a model described by a GitHub repository.
ToolChain*
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zhuang et al.,
      <a class="ltx_ref" href="#bib.bib47" title="">
       2023
      </a>
      )
     </cite>
     combines the planning of tool usage with tree-search-based reasoning
     <cite class="ltx_cite ltx_citemacro_citep">
      (Yao et al.,
      <a class="ltx_ref" href="#bib.bib44" title="">
       2023a
      </a>
      ; Hao et al.,
      <a class="ltx_ref" href="#bib.bib13" title="">
       2023a
      </a>
      )
     </cite>
     , which is especially useful for procedural tasks
     <cite class="ltx_cite ltx_citemacro_citep">
      (Xu et al.,
      <a class="ltx_ref" href="#bib.bib42" title="">
       2023
      </a>
      ; Cobbe et al.,
      <a class="ltx_ref" href="#bib.bib9" title="">
       2021
      </a>
      )
     </cite>
     .
Different from above work, we focus on the planning of general chain-of-thought
     <cite class="ltx_cite ltx_citemacro_citep">
      (Wei et al.,
      <a class="ltx_ref" href="#bib.bib40" title="">
       2022
      </a>
      )
     </cite>
     reasoning with awareness of domain specialized tools.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Method
  </h2>
  <figure class="ltx_figure" id="S3.F2">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="443" id="S3.F2.g1" src="/html/2401.17464/assets/x2.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 2:
    </span>
    Illustration of gold data re-writing for fine-tuning data construction. Given a pair of domain question (green scroll) and gold answer (yellow scroll), an LLM is prompted to re-write the gold answer as a reasoning chain with abstract variables (purple bubble). Then, domain specialized tools validate the correctness of the re-writing by checking whether the abstract chain can be reified to get the final answer (orange label).
   </figcaption>
  </figure>
  <section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
   <h4 class="ltx_title ltx_title_paragraph">
    Chain-of-Abstraction (CoA) Reasoning
   </h4>
   <div class="ltx_para" id="S3.SS0.SSS0.Px1.p1">
    <p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.3">
     Our method decouples the general reasoning of LLMs from domain-specific knowledge obtained from external tools.
Figure
     <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     shows an overview of our method.
In particular, we first fine-tune LLMs to generate reasoning chains with abstract placeholders,
     <span class="ltx_text ltx_font_italic" id="S3.SS0.SSS0.Px1.p1.3.1">
      e.g.
     </span>
     ,
     <math alttext="y1" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p1.1.m1.1">
      <semantics id="S3.SS0.SSS0.Px1.p1.1.m1.1a">
       <mrow id="S3.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">
        <mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">
         y
        </mi>
        <mo id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.1" lspace="0em" rspace="0em" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml">
         ​
        </mo>
        <mn id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml">
         1
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.1.m1.1b">
        <apply id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1">
         <times id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.1">
         </times>
         <ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2">
          𝑦
         </ci>
         <cn id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3">
          1
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.1.m1.1c">
        y1
       </annotation>
      </semantics>
     </math>
     ,
     <math alttext="y2" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p1.2.m2.1">
      <semantics id="S3.SS0.SSS0.Px1.p1.2.m2.1a">
       <mrow id="S3.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">
        <mi id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">
         y
        </mi>
        <mo id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1" lspace="0em" rspace="0em" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml">
         ​
        </mo>
        <mn id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml">
         2
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.2.m2.1b">
        <apply id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1">
         <times id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1">
         </times>
         <ci id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.2">
          𝑦
         </ci>
         <cn id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.3">
          2
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.2.m2.1c">
        y2
       </annotation>
      </semantics>
     </math>
     and
     <math alttext="y3" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p1.3.m3.1">
      <semantics id="S3.SS0.SSS0.Px1.p1.3.m3.1a">
       <mrow id="S3.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">
        <mi id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.2" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml">
         y
        </mi>
        <mo id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.1" lspace="0em" rspace="0em" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.1.cmml">
         ​
        </mo>
        <mn id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.cmml">
         3
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.3.m3.1b">
        <apply id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1">
         <times id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.1">
         </times>
         <ci id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.2">
          𝑦
         </ci>
         <cn id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3.cmml" type="integer" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.3">
          3
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.3.m3.1c">
        y3
       </annotation>
      </semantics>
     </math>
     ,
     <span class="ltx_note ltx_role_footnote" id="footnote3">
      <sup class="ltx_note_mark">
       3
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         3
        </sup>
        <span class="ltx_tag ltx_tag_note">
         3
        </span>
        We also test placeholders in single-character format,
        <span class="ltx_text ltx_font_italic" id="footnote3.1">
         e.g.
        </span>
        ,
        <math alttext="x" class="ltx_Math" display="inline" id="footnote3.m1.1">
         <semantics id="footnote3.m1.1b">
          <mi id="footnote3.m1.1.1" xref="footnote3.m1.1.1.cmml">
           x
          </mi>
          <annotation-xml encoding="MathML-Content" id="footnote3.m1.1c">
           <ci id="footnote3.m1.1.1.cmml" xref="footnote3.m1.1.1">
            𝑥
           </ci>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="footnote3.m1.1d">
           x
          </annotation>
         </semantics>
        </math>
        ,
        <math alttext="y" class="ltx_Math" display="inline" id="footnote3.m2.1">
         <semantics id="footnote3.m2.1b">
          <mi id="footnote3.m2.1.1" xref="footnote3.m2.1.1.cmml">
           y
          </mi>
          <annotation-xml encoding="MathML-Content" id="footnote3.m2.1c">
           <ci id="footnote3.m2.1.1.cmml" xref="footnote3.m2.1.1">
            𝑦
           </ci>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="footnote3.m2.1d">
           y
          </annotation>
         </semantics>
        </math>
        and
        <math alttext="z" class="ltx_Math" display="inline" id="footnote3.m3.1">
         <semantics id="footnote3.m3.1b">
          <mi id="footnote3.m3.1.1" xref="footnote3.m3.1.1.cmml">
           z
          </mi>
          <annotation-xml encoding="MathML-Content" id="footnote3.m3.1c">
           <ci id="footnote3.m3.1.1.cmml" xref="footnote3.m3.1.1">
            𝑧
           </ci>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="footnote3.m3.1d">
           z
          </annotation>
         </semantics>
        </math>
        , which however leads to sub-optimal results.
       </span>
      </span>
     </span>
     as shown in Figure
     <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     .
In the second stage, we reify each reasoning chain by replacing placeholders with domain-specific knowledge obtained from external tools,
     <span class="ltx_text ltx_font_italic" id="S3.SS0.SSS0.Px1.p1.3.2">
      e.g.
     </span>
     , calculation results from a calculator, relevant articles retrieved from web search engine, etc.
Finally, the question is answered based on the reified reasoning chain.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS0.SSS0.Px1.p2">
    <p class="ltx_p" id="S3.SS0.SSS0.Px1.p2.1">
     Note that since the LLMs are trained to generate abstract chain of reasoning instead of regular chain-of-thought (CoT) reasoning with explicit values, this enables LLMs to focus on learning general and holistic reasoning strategies without needing to generate instance-specific knowledge for the model’s parameters.
Moreover, decoupling general reasoning and domain-specific knowledge enables LLM decoding to proceed and switch between different samples in parallel with API calling (via a pipeline),
     <span class="ltx_text ltx_font_italic" id="S3.SS0.SSS0.Px1.p2.1.1">
      i.e.
     </span>
     , LLM can start generating the next abstract chain while the tool fills the current chain, which speeds up the overall inference process.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S3.SS0.SSS0.Px2">
   <h4 class="ltx_title ltx_title_paragraph">
    Fine-tuning Data Construction
   </h4>
   <div class="ltx_para" id="S3.SS0.SSS0.Px2.p1">
    <p class="ltx_p" id="S3.SS0.SSS0.Px2.p1.2">
     To construct chain-of-abstraction (CoA) data for fine-tuning LLMs, we collect question answering (QA) samples from existing open-source QA datasets
     <cite class="ltx_cite ltx_citemacro_citep">
      (Cobbe et al.,
      <a class="ltx_ref" href="#bib.bib9" title="">
       2021
      </a>
      ; Miao et al.,
      <a class="ltx_ref" href="#bib.bib26" title="">
       2020
      </a>
      ; Yang et al.,
      <a class="ltx_ref" href="#bib.bib43" title="">
       2018
      </a>
      )
     </cite>
     , and prompt LLaMa-70B
     <cite class="ltx_cite ltx_citemacro_citep">
      (Touvron et al.,
      <a class="ltx_ref" href="#bib.bib36" title="">
       2023a
      </a>
      )
     </cite>
     to re-write the answer of each sampled question, as shown in Figure
     <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3 Method ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     .
Specifically, we prompt LLaMa-70B to label the spans in gold answers that correspond to knowledge operations (
     <span class="ltx_text ltx_font_italic" id="S3.SS0.SSS0.Px2.p1.2.1">
      e.g.
     </span>
     , math derivations, statements based on Wikipedia references) and then to re-write the sentences with labeled spans as fillable CoA traces, where the operation results are replaced with abstract placeholders.
     <span class="ltx_note ltx_role_footnote" id="footnote4">
      <sup class="ltx_note_mark">
       4
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         4
        </sup>
        <span class="ltx_tag ltx_tag_note">
         4
        </span>
        We provide our few-shot prompting examples for CoA data re-writing in Appendix
        <a class="ltx_ref" href="#A3" title="Appendix C Fine-Tuning Data Re-writing Details ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
         <span class="ltx_text ltx_ref_tag">
          C
         </span>
        </a>
        .
       </span>
      </span>
     </span>
     For example, the two derivations in the example in Figure
     <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3 Method ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     are re-written as “[
     <math alttext="20+35=y1" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px2.p1.1.m1.1">
      <semantics id="S3.SS0.SSS0.Px2.p1.1.m1.1a">
       <mrow id="S3.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">
        <mrow id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml">
         <mn id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2.2" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2.2.cmml">
          20
         </mn>
         <mo id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2.1" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2.1.cmml">
          +
         </mo>
         <mn id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2.3" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2.3.cmml">
          35
         </mn>
        </mrow>
        <mo id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.1" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml">
         =
        </mo>
        <mrow id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml">
         <mi id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.2" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.2.cmml">
          y
         </mi>
         <mo id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.1" lspace="0em" rspace="0em" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.1.cmml">
          ​
         </mo>
         <mn id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.cmml">
          1
         </mn>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.1.m1.1b">
        <apply id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1">
         <eq id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.1">
         </eq>
         <apply id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2">
          <plus id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2.1">
          </plus>
          <cn id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2.2.cmml" type="integer" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2.2">
           20
          </cn>
          <cn id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2.3.cmml" type="integer" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2.3">
           35
          </cn>
         </apply>
         <apply id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3">
          <times id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.1">
          </times>
          <ci id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.2">
           𝑦
          </ci>
          <cn id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.cmml" type="integer" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3">
           1
          </cn>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.1.m1.1c">
        20+35=y1
       </annotation>
      </semantics>
     </math>
     ]" and “[
     <math alttext="90-y1=y2" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px2.p1.2.m2.1">
      <semantics id="S3.SS0.SSS0.Px2.p1.2.m2.1a">
       <mrow id="S3.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.cmml">
        <mrow id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml">
         <mn id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.2" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.2.cmml">
          90
         </mn>
         <mo id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.1" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.1.cmml">
          −
         </mo>
         <mrow id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.cmml">
          <mi id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.2" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.2.cmml">
           y
          </mi>
          <mo id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.1" lspace="0em" rspace="0em" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.1.cmml">
           ​
          </mo>
          <mn id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.3" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.3.cmml">
           1
          </mn>
         </mrow>
        </mrow>
        <mo id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.1" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml">
         =
        </mo>
        <mrow id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml">
         <mi id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.2" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.2.cmml">
          y
         </mi>
         <mo id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.1" lspace="0em" rspace="0em" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.1.cmml">
          ​
         </mo>
         <mn id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.3" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.3.cmml">
          2
         </mn>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.2.m2.1b">
        <apply id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1">
         <eq id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.1">
         </eq>
         <apply id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2">
          <minus id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.1">
          </minus>
          <cn id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.2.cmml" type="integer" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.2">
           90
          </cn>
          <apply id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3">
           <times id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.1.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.1">
           </times>
           <ci id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.2.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.2">
            𝑦
           </ci>
           <cn id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.3.cmml" type="integer" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.3">
            1
           </cn>
          </apply>
         </apply>
         <apply id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3">
          <times id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.1">
          </times>
          <ci id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.2">
           𝑦
          </ci>
          <cn id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.3">
           2
          </cn>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.2.m2.1c">
        90-y1=y2
       </annotation>
      </semantics>
     </math>
     ]", respectively.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS0.SSS0.Px2.p2">
    <p class="ltx_p" id="S3.SS0.SSS0.Px2.p2.1">
     Note that an intermediate result may appear multiple times in a re-written answer,
     <span class="ltx_text ltx_font_italic" id="S3.SS0.SSS0.Px2.p2.1.1">
      e.g.
     </span>
     , the math calculation result
     <math alttext="55" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px2.p2.1.m1.1">
      <semantics id="S3.SS0.SSS0.Px2.p2.1.m1.1a">
       <mn id="S3.SS0.SSS0.Px2.p2.1.m1.1.1" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.cmml">
        55
       </mn>
       <annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.1.m1.1b">
        <cn id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.cmml" type="integer" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1">
         55
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.1.m1.1c">
        55
       </annotation>
      </semantics>
     </math>
     in Figure
     <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3 Method ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     .
We prompt LLaMa-70B to replace all occurrences of the same intermediate result with the same placeholder, thereby explicitly connecting the multiple reasoning steps.
To ensure that the re-written data is accurate, we use domain-specialized tools to verify the correctness of each CoA reasoning trace.
     <span class="ltx_note ltx_role_footnote" id="footnote5">
      <sup class="ltx_note_mark">
       5
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         5
        </sup>
        <span class="ltx_tag ltx_tag_note">
         5
        </span>
        Detailed implementations of reasoning chain verification are described in Sec.
        <a class="ltx_ref" href="#S4.SS1" title="4.1 Mathematical Reasoning ‣ 4 Experimental Settings ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
         <span class="ltx_text ltx_ref_tag">
          4.1
         </span>
        </a>
        and
        <a class="ltx_ref" href="#S4.SS2" title="4.2 Wikipedia QA ‣ 4 Experimental Settings ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
         <span class="ltx_text ltx_ref_tag">
          4.2
         </span>
        </a>
        .
       </span>
      </span>
     </span>
     Specifically, we use the tools to execute the labeled operations in each CoA, and only keep questions whose CoA can be infilled with valid results by the tools.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Experimental Settings
  </h2>
  <div class="ltx_para" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    We conduct our experiments on two representative domains: mathematical reasoning and Wikipedia (Wiki) QA, which involves commonsense and logical reasoning on factual descriptive knowledge.
   </p>
  </div>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    Mathematical Reasoning
   </h3>
   <div class="ltx_para" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     Given a math question, the QA system needs to generate a natural language solution to the problem with step-by-step arithmetic derivations (as demonstrated in the left column of Figure
     <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     ).
We assume that the derivations involved in the solution are the specialized knowledge operations required in this domain, which are labeled in square brackets with derivation results being replaced by abstract placeholders, e.g., “[
     <math alttext="20+35=y1" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1">
      <semantics id="S4.SS1.p1.1.m1.1a">
       <mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">
        <mrow id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">
         <mn id="S4.SS1.p1.1.m1.1.1.2.2" xref="S4.SS1.p1.1.m1.1.1.2.2.cmml">
          20
         </mn>
         <mo id="S4.SS1.p1.1.m1.1.1.2.1" xref="S4.SS1.p1.1.m1.1.1.2.1.cmml">
          +
         </mo>
         <mn id="S4.SS1.p1.1.m1.1.1.2.3" xref="S4.SS1.p1.1.m1.1.1.2.3.cmml">
          35
         </mn>
        </mrow>
        <mo id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">
         =
        </mo>
        <mrow id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">
         <mi id="S4.SS1.p1.1.m1.1.1.3.2" xref="S4.SS1.p1.1.m1.1.1.3.2.cmml">
          y
         </mi>
         <mo id="S4.SS1.p1.1.m1.1.1.3.1" lspace="0em" rspace="0em" xref="S4.SS1.p1.1.m1.1.1.3.1.cmml">
          ​
         </mo>
         <mn id="S4.SS1.p1.1.m1.1.1.3.3" xref="S4.SS1.p1.1.m1.1.1.3.3.cmml">
          1
         </mn>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b">
        <apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">
         <eq id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1">
         </eq>
         <apply id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">
          <plus id="S4.SS1.p1.1.m1.1.1.2.1.cmml" xref="S4.SS1.p1.1.m1.1.1.2.1">
          </plus>
          <cn id="S4.SS1.p1.1.m1.1.1.2.2.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1.2.2">
           20
          </cn>
          <cn id="S4.SS1.p1.1.m1.1.1.2.3.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1.2.3">
           35
          </cn>
         </apply>
         <apply id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">
          <times id="S4.SS1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.p1.1.m1.1.1.3.1">
          </times>
          <ci id="S4.SS1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS1.p1.1.m1.1.1.3.2">
           𝑦
          </ci>
          <cn id="S4.SS1.p1.1.m1.1.1.3.3.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1.3.3">
           1
          </cn>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">
        20+35=y1
       </annotation>
      </semantics>
     </math>
     ]".
    </p>
   </div>
   <section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Datasets
    </h4>
    <div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
     <p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">
      We construct most of our fine-tuning CoA data by re-writing the GSM8K
      <cite class="ltx_cite ltx_citemacro_citep">
       (Cobbe et al.,
       <a class="ltx_ref" href="#bib.bib9" title="">
        2021
       </a>
       )
      </cite>
      training set, which contains 7473 linguistically diverse grade school math problems.
As GSM8K dataset focuses on multi-step reasoning, it lacks coverage of single-step arithmetic problems, so we also re-write an additional set of 691 single-step math problems from the ASDiv
      <cite class="ltx_cite ltx_citemacro_citep">
       (Miao et al.,
       <a class="ltx_ref" href="#bib.bib26" title="">
        2020
       </a>
       )
      </cite>
      dataset.
Across these re-written datasets, we find that
      <math alttext="\sim 76.6\%" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.1.m1.1">
       <semantics id="S4.SS1.SSS0.Px1.p1.1.m1.1a">
        <mrow id="S4.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">
         <mi id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">
         </mi>
         <mo id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">
          ∼
         </mo>
         <mrow id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">
          <mn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.cmml">
           76.6
          </mn>
          <mo id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">
           %
          </mo>
         </mrow>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.1.m1.1b">
         <apply id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1">
          <csymbol cd="latexml" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1">
           similar-to
          </csymbol>
          <csymbol cd="latexml" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2">
           absent
          </csymbol>
          <apply id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3">
           <csymbol cd="latexml" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.1">
            percent
           </csymbol>
           <cn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" type="float" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.2">
            76.6
           </cn>
          </apply>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.1.m1.1c">
         \sim 76.6\%
        </annotation>
       </semantics>
      </math>
      of the CoA reasoning traces generated by LLaMa-70B are verified by our equation solver (described below).
Table
      <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ Datasets ‣ 4.1 Mathematical Reasoning ‣ 4 Experimental Settings ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
       <span class="ltx_text ltx_ref_tag">
        1
       </span>
      </a>
      shows the reasoning step distribution (
      <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px1.p1.1.1">
       i.e.
      </span>
      , number of derivations) of our constructed fine-tuning data.
     </p>
    </div>
    <figure class="ltx_table" id="S4.T1">
     <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.1" style="width:433.6pt;height:140.3pt;vertical-align:-135.3pt;">
      <span class="ltx_transformed_inner" style="transform:translate(87.0pt,-1.0pt) scale(1.66981785361832,1.66981785361832) ;">
       <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.1.1">
        <thead class="ltx_thead">
         <tr class="ltx_tr" id="S4.T1.1.1.2.1">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T1.1.1.2.1.1" rowspan="2">
           <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.2.1.1.1">
            Source
           </span>
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="7" id="S4.T1.1.1.2.1.2">
           <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.2.1.2.1">
            Reasoning Step
           </span>
          </th>
         </tr>
         <tr class="ltx_tr" id="S4.T1.1.1.1">
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.2">
           1
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.3">
           2
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.4">
           3
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.5">
           4
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.6">
           5
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.1">
           <math alttext="&gt;" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.m1.1">
            <semantics id="S4.T1.1.1.1.1.m1.1a">
             <mo id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml">
              &gt;
             </mo>
             <annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b">
              <gt id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">
              </gt>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">
              &gt;
             </annotation>
            </semantics>
           </math>
           5
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.7">
           All
          </th>
         </tr>
        </thead>
        <tbody class="ltx_tbody">
         <tr class="ltx_tr" id="S4.T1.1.1.3.1">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.1.3.1.1">
           GSM8K
          </th>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.2">
           8
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.3">
           1540
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.4">
           1648
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.5">
           1164
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.6">
           666
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.7">
           553
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.8">
           5579
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T1.1.1.4.2">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T1.1.1.4.2.1">
           ASDiv
          </th>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.4.2.2">
           677
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.4.2.3">
           0
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.4.2.4">
           0
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.4.2.5">
           0
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.4.2.6">
           0
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.4.2.7">
           0
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.4.2.8">
           677
          </td>
         </tr>
        </tbody>
       </table>
      </span>
     </div>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_table">
       Table 1:
      </span>
      Reasoning step distribution of correctly re-written reasoning chains in math domain.
     </figcaption>
    </figure>
    <div class="ltx_para" id="S4.SS1.SSS0.Px1.p2">
     <p class="ltx_p" id="S4.SS1.SSS0.Px1.p2.1">
      For an in-distribution evaluation, we test models on GSM8K and ASDiv, containing 1319 and 2305 testing problems.
To further test the models’ generalization ability, we also conduct zero-shot evaluation on other representative math datasets, including SVAMP
      <cite class="ltx_cite ltx_citemacro_citep">
       (Patel et al.,
       <a class="ltx_ref" href="#bib.bib29" title="">
        2021
       </a>
       )
      </cite>
      and MAWPS
      <cite class="ltx_cite ltx_citemacro_citep">
       (Koncel-Kedziorski et al.,
       <a class="ltx_ref" href="#bib.bib20" title="">
        2016
       </a>
       )
      </cite>
      , which contain 1000 and 2065 testing samples, respectively.
      <span class="ltx_note ltx_role_footnote" id="footnote6">
       <sup class="ltx_note_mark">
        6
       </sup>
       <span class="ltx_note_outer">
        <span class="ltx_note_content">
         <sup class="ltx_note_mark">
          6
         </sup>
         <span class="ltx_tag ltx_tag_note">
          6
         </span>
         For the MAWPS benchmark, we test on the 395, 508, 562 and 600 math problems from AddSub, SingleEq, SingleOp and MultiArith portions, respectively.
        </span>
       </span>
      </span>
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Domain Tool
    </h4>
    <div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
     <p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.2">
      We use an equation solver to perform the arithmetic derivations required in the math domain.
Our equation solver first extracts the derivations labeled in the CoA reasoning, e.g., “[
      <math alttext="20+35=y1" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.1.m1.1">
       <semantics id="S4.SS1.SSS0.Px2.p1.1.m1.1a">
        <mrow id="S4.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">
         <mrow id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml">
          <mn id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.2" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.2.cmml">
           20
          </mn>
          <mo id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.1" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.1.cmml">
           +
          </mo>
          <mn id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.3" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.3.cmml">
           35
          </mn>
         </mrow>
         <mo id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml">
          =
         </mo>
         <mrow id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml">
          <mi id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.2" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.cmml">
           y
          </mi>
          <mo id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.1" lspace="0em" rspace="0em" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.cmml">
           ​
          </mo>
          <mn id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.3" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.cmml">
           1
          </mn>
         </mrow>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.1.m1.1b">
         <apply id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1">
          <eq id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1">
          </eq>
          <apply id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2">
           <plus id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.1">
           </plus>
           <cn id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.2.cmml" type="integer" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.2">
            20
           </cn>
           <cn id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.3.cmml" type="integer" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.3">
            35
           </cn>
          </apply>
          <apply id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3">
           <times id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.1">
           </times>
           <ci id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.2">
            𝑦
           </ci>
           <cn id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.cmml" type="integer" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.3">
            1
           </cn>
          </apply>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.1.m1.1c">
         20+35=y1
        </annotation>
       </semantics>
      </math>
      ]" and “[
      <math alttext="90-y1=y2" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.2.m2.1">
       <semantics id="S4.SS1.SSS0.Px2.p1.2.m2.1a">
        <mrow id="S4.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml">
         <mrow id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml">
          <mn id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.2" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.cmml">
           90
          </mn>
          <mo id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.1" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.1.cmml">
           −
          </mo>
          <mrow id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.3" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.cmml">
           <mi id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.2" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.2.cmml">
            y
           </mi>
           <mo id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.1" lspace="0em" rspace="0em" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.1.cmml">
            ​
           </mo>
           <mn id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.3" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.3.cmml">
            1
           </mn>
          </mrow>
         </mrow>
         <mo id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.1" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml">
          =
         </mo>
         <mrow id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml">
          <mi id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.2" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.cmml">
           y
          </mi>
          <mo id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.1" lspace="0em" rspace="0em" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.1.cmml">
           ​
          </mo>
          <mn id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.3" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.3.cmml">
           2
          </mn>
         </mrow>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.2.m2.1b">
         <apply id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1">
          <eq id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.1">
          </eq>
          <apply id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2">
           <minus id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.1.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.1">
           </minus>
           <cn id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.cmml" type="integer" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.2">
            90
           </cn>
           <apply id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.3">
            <times id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.1.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.1">
            </times>
            <ci id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.2.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.2">
             𝑦
            </ci>
            <cn id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.3.cmml" type="integer" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.3">
             1
            </cn>
           </apply>
          </apply>
          <apply id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3">
           <times id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.1.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.1">
           </times>
           <ci id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.2">
            𝑦
           </ci>
           <cn id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.3">
            2
           </cn>
          </apply>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.2.m2.1c">
         90-y1=y2
        </annotation>
       </semantics>
      </math>
      ]", and combines all derivations into a system of equations.
Then the system of equations is solved by the SymPy toolkit,
      <span class="ltx_note ltx_role_footnote" id="footnote7">
       <sup class="ltx_note_mark">
        7
       </sup>
       <span class="ltx_note_outer">
        <span class="ltx_note_content">
         <sup class="ltx_note_mark">
          7
         </sup>
         <span class="ltx_tag ltx_tag_note">
          7
         </span>
         <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sympy.org/en/index.html" target="_blank" title="">
          https://www.sympy.org/en/index.html
         </a>
        </span>
       </span>
      </span>
      to get the true value of each variable (
      <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px2.p1.2.1">
       i.e.
      </span>
      , the value of the abstract placeholder).
Finally, our equation solver returns the reified chain of reasoning by replacing all the variables with their solved true values (including the answer).
     </p>
    </div>
    <figure class="ltx_table" id="S4.T2">
     <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.1" style="width:433.6pt;height:219.7pt;vertical-align:-216.3pt;">
      <span class="ltx_transformed_inner" style="transform:translate(27.4pt,-0.2pt) scale(1.1444076421958,1.1444076421958) ;">
       <table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1">
        <tbody class="ltx_tbody">
         <tr class="ltx_tr" id="S4.T2.1.1.1.1">
          <td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.1.1.1.1.1" rowspan="2">
           <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1.1">
            Question
           </span>
          </td>
          <td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.1.1.1.1.2">
           The director of the romantic comedy “Big Stone Gap” is based in
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T2.1.1.2.2">
          <td class="ltx_td ltx_align_left" id="S4.T2.1.1.2.2.1">
           what New York city?
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T2.1.1.3.3">
          <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.3.3.1">
           <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.3.3.1.1">
            Answer
           </span>
          </td>
          <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.3.3.2">
           Greenwich Village
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T2.1.1.4.4">
          <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.4.4.1">
           <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.4.1.1">
            Wikipedia
           </span>
          </td>
          <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.4.4.2">
           Big Stone Gap (film) &gt; Big Stone Gap is a 2014 American romantic
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T2.1.1.5.5">
          <td class="ltx_td ltx_align_left" id="S4.T2.1.1.5.5.1" rowspan="3">
           <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.5.5.1.1">
            References
           </span>
          </td>
          <td class="ltx_td ltx_align_left" id="S4.T2.1.1.5.5.2">
           comedy film directed by Adriana Trigiani.
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T2.1.1.6.6">
          <td class="ltx_td ltx_align_left" id="S4.T2.1.1.6.6.1">
           Adriana Trigiani &gt; Adriana Trigiani is an Italian American film
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T2.1.1.7.7">
          <td class="ltx_td ltx_align_left" id="S4.T2.1.1.7.7.1">
           director based in Greenwich Village.
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T2.1.1.8.8">
          <td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.1.1.8.8.1" rowspan="3">
           <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.8.8.1.1">
            CoA Trace
           </span>
          </td>
          <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.8.8.2">
           Find the [director of romantic comedy “Big Stone Gap” -Wiki-&gt; y1].
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T2.1.1.9.9">
          <td class="ltx_td ltx_align_left" id="S4.T2.1.1.9.9.1">
           The name of this film’s director is [y1 -NER(person)-&gt; y2].
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T2.1.1.10.10">
          <td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.1.1.10.10.1">
           Then determine [y2 in what New York city -Wiki-&gt; y3].
          </td>
         </tr>
        </tbody>
       </table>
      </span>
     </div>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_table">
       Table 2:
      </span>
      An example of CoA fine-tuning data construction in Wiki QA domain.
     </figcaption>
    </figure>
   </section>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    Wikipedia QA
   </h3>
   <div class="ltx_para" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     Given a question based on Wikipedia knowledge, the model needs to first identify Wikipedia articles as references related to the question, and then reason on key knowledge in the reference articles to answer the question (as shown in the right column of Figure
     <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     ).
We assume that the specialized knowledge operation in this domain is the retrieval of relevant Wikipedia articles and important named-entities, which are re-written as Wikipedia searching (WikiSearch) and named-entity recognition (NER)
     <span class="ltx_note ltx_role_footnote" id="footnote8">
      <sup class="ltx_note_mark">
       8
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         8
        </sup>
        <span class="ltx_tag ltx_tag_note">
         8
        </span>
        We use NER to extract entities from the article that bridge the former Wikipedia search results to the latter Wikipedia search queries.
       </span>
      </span>
     </span>
     queries.
Table
     <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ Domain Tool ‣ 4.1 Mathematical Reasoning ‣ 4 Experimental Settings ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     shows an example of a re-written CoA trace for Wiki QA.
     <span class="ltx_note ltx_role_footnote" id="footnote9">
      <sup class="ltx_note_mark">
       9
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         9
        </sup>
        <span class="ltx_tag ltx_tag_note">
         9
        </span>
        We include more prompting examples of Wiki QA answer re-writing in Appendix
        <a class="ltx_ref" href="#A3" title="Appendix C Fine-Tuning Data Re-writing Details ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
         <span class="ltx_text ltx_ref_tag">
          C
         </span>
        </a>
        .
       </span>
      </span>
     </span>
    </p>
   </div>
   <section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Datasets
    </h4>
    <div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
     <p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">
      We use the HotpotQA
      <cite class="ltx_cite ltx_citemacro_citep">
       (Yang et al.,
       <a class="ltx_ref" href="#bib.bib43" title="">
        2018
       </a>
       )
      </cite>
      dataset to construct our fine-tuning CoA data in the Wiki QA domain.
HotpotQA contains 113K multi-hop QA examples, each labeled with two Wikipedia articles that provide supporting knowledge.
Among the 90447 training QA pairs, we identify 72991 as
      <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.1">
       Bridge
      </span>
      QA pairs, where an intermediate entity must be identified to link the answer to the question, as shown in Table
      <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ Domain Tool ‣ 4.1 Mathematical Reasoning ‣ 4 Experimental Settings ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
       <span class="ltx_text ltx_ref_tag">
        2
       </span>
      </a>
      .
The remaining 17456 are
      <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.2">
       Comparison
      </span>
      QA pairs, where the attributes of two entities are compared,
      <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS0.Px1.p1.1.3">
       e.g.
      </span>
      , “Are Randal Kleiser and Kyle Schickner of the same nationality?”.
We prompt LLaMa-70B to re-write these training QAs into CoAs with WikiSearch and NER queries, and verify each CoA with our domain tools (described below), by checking whether all the articles returned by the WikiSearch queries match one of the titles in the gold articles.
Finally, 8956 Bridge QAs and 5405 Comparison QAs are selected as fine-tuning data.
      <span class="ltx_note ltx_role_footnote" id="footnote10">
       <sup class="ltx_note_mark">
        10
       </sup>
       <span class="ltx_note_outer">
        <span class="ltx_note_content">
         <sup class="ltx_note_mark">
          10
         </sup>
         <span class="ltx_tag ltx_tag_note">
          10
         </span>
         Compared to mathematical reasoning, generating CoA data for Wiki QA requires more complex tool use combining WikiSearch and NER models, leading to a lower re-writing success rate (
         <math alttext="\sim 15.9\%" class="ltx_Math" display="inline" id="footnote10.m1.1">
          <semantics id="footnote10.m1.1b">
           <mrow id="footnote10.m1.1.1" xref="footnote10.m1.1.1.cmml">
            <mi id="footnote10.m1.1.1.2" xref="footnote10.m1.1.1.2.cmml">
            </mi>
            <mo id="footnote10.m1.1.1.1" xref="footnote10.m1.1.1.1.cmml">
             ∼
            </mo>
            <mrow id="footnote10.m1.1.1.3" xref="footnote10.m1.1.1.3.cmml">
             <mn id="footnote10.m1.1.1.3.2" xref="footnote10.m1.1.1.3.2.cmml">
              15.9
             </mn>
             <mo id="footnote10.m1.1.1.3.1" xref="footnote10.m1.1.1.3.1.cmml">
              %
             </mo>
            </mrow>
           </mrow>
           <annotation-xml encoding="MathML-Content" id="footnote10.m1.1c">
            <apply id="footnote10.m1.1.1.cmml" xref="footnote10.m1.1.1">
             <csymbol cd="latexml" id="footnote10.m1.1.1.1.cmml" xref="footnote10.m1.1.1.1">
              similar-to
             </csymbol>
             <csymbol cd="latexml" id="footnote10.m1.1.1.2.cmml" xref="footnote10.m1.1.1.2">
              absent
             </csymbol>
             <apply id="footnote10.m1.1.1.3.cmml" xref="footnote10.m1.1.1.3">
              <csymbol cd="latexml" id="footnote10.m1.1.1.3.1.cmml" xref="footnote10.m1.1.1.3.1">
               percent
              </csymbol>
              <cn id="footnote10.m1.1.1.3.2.cmml" type="float" xref="footnote10.m1.1.1.3.2">
               15.9
              </cn>
             </apply>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="footnote10.m1.1d">
            \sim 15.9\%
           </annotation>
          </semantics>
         </math>
         ).
        </span>
       </span>
      </span>
      For Wiki QA, we note that besides training a LLM to produce CoA data using WikiSearch, we also fine-tune a second LLM to learn to generate the final gold answer based on a correctly reified CoA reasoning trace.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS2.SSS0.Px1.p2">
     <p class="ltx_p" id="S4.SS2.SSS0.Px1.p2.1">
      We evaluate models on the HotpotQA development set, which contains 5918 Bridge QA pairs and 1487 Comparison QA pairs. Similar to the mathematical reasoning domain, we also conduct zero-shot evaluation on other open-domain QA datasets: WebQuestions (WQ;
      <cite class="ltx_cite ltx_citemacro_citep">
       Berant et al.,
       <a class="ltx_ref" href="#bib.bib3" title="">
        2013
       </a>
      </cite>
      ), NaturalQuestions (NQ;
      <cite class="ltx_cite ltx_citemacro_citep">
       Kwiatkowski et al.,
       <a class="ltx_ref" href="#bib.bib21" title="">
        2019
       </a>
      </cite>
      ) and TriviaQA
      <cite class="ltx_cite ltx_citemacro_citep">
       (Joshi et al.,
       <a class="ltx_ref" href="#bib.bib19" title="">
        2017
       </a>
       )
      </cite>
      , which contain 2032, 3610 and 17944 test questions, respectively.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Domain Tools
    </h4>
    <div class="ltx_para" id="S4.SS2.SSS0.Px2.p1">
     <p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1">
      The specialized tools required for Wiki QA include a Wikipedia search engine to retrieve reference articles, and a NER toolkit to extract entities that bridge multi-step searching queries.
We follow Toolformer
      <cite class="ltx_cite ltx_citemacro_citep">
       (Schick et al.,
       <a class="ltx_ref" href="#bib.bib34" title="">
        2023
       </a>
       )
      </cite>
      and implement a Wikipedia search engine as a BM25 retriever
      <cite class="ltx_cite ltx_citemacro_citep">
       (Robertson et al.,
       <a class="ltx_ref" href="#bib.bib33" title="">
        1995
       </a>
       ; Baeza-Yates et al.,
       <a class="ltx_ref" href="#bib.bib2" title="">
        1999
       </a>
       )
      </cite>
      that indexes the Wikipedia dump from the KILT benchmark
      <cite class="ltx_cite ltx_citemacro_citep">
       (Petroni et al.,
       <a class="ltx_ref" href="#bib.bib31" title="">
        2021
       </a>
       )
      </cite>
      .
We use the BM25 retriever to search the top-10 articles relevant to the input query, and then re-rank the articles based on their Sentence-BERT
      <cite class="ltx_cite ltx_citemacro_citep">
       (Reimers and Gurevych,
       <a class="ltx_ref" href="#bib.bib32" title="">
        2019
       </a>
       )
      </cite>
      embedding cosine similarity with the question.
After re-ranking, the top-
      <math alttext="1" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px2.p1.1.m1.1">
       <semantics id="S4.SS2.SSS0.Px2.p1.1.m1.1a">
        <mn id="S4.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">
         1
        </mn>
        <annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p1.1.m1.1b">
         <cn id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1">
          1
         </cn>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p1.1.m1.1c">
         1
        </annotation>
       </semantics>
      </math>
      article is selected to be the final search result.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS2.SSS0.Px2.p2">
     <p class="ltx_p" id="S4.SS2.SSS0.Px2.p2.1">
      We use SpaCy
      <span class="ltx_note ltx_role_footnote" id="footnote11">
       <sup class="ltx_note_mark">
        11
       </sup>
       <span class="ltx_note_outer">
        <span class="ltx_note_content">
         <sup class="ltx_note_mark">
          11
         </sup>
         <span class="ltx_tag ltx_tag_note">
          11
         </span>
         <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://spacy.io/models/en" target="_blank" title="">
          https://spacy.io/models/en
         </a>
        </span>
       </span>
      </span>
      (en_core_web_sm) as the NER toolkit to extract named entities.
To simplify NER, we aggregate the numerous SpaCy NER types into 6 general classes, as shown in Table
      <a class="ltx_ref" href="#S4.T3" title="Table 3 ‣ Domain Tools ‣ 4.2 Wikipedia QA ‣ 4 Experimental Settings ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
       <span class="ltx_text ltx_ref_tag">
        3
       </span>
      </a>
      .
If multiple named entities are recognized, we input each recognized entity to the subsequent WikiSearch query, and select the entity whose subsequent search result has the highest Sentence-BERT embedding cosine similarity with the question.
     </p>
    </div>
    <figure class="ltx_table" id="S4.T3">
     <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.1" style="width:433.6pt;height:202.9pt;vertical-align:-199.0pt;">
      <span class="ltx_transformed_inner" style="transform:translate(50.1pt,-0.5pt) scale(1.30085792100183,1.30085792100183) ;">
       <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.1.1">
        <tbody class="ltx_tbody">
         <tr class="ltx_tr" id="S4.T3.1.1.1.1">
          <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.1.1.1.1">
           <span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1.1">
            General
           </span>
          </th>
          <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.1.1.2" rowspan="2">
           <span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.2.1">
            SpaCy NER Types included in each General Class
           </span>
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T3.1.1.2.2">
          <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T3.1.1.2.2.1">
           <span class="ltx_text ltx_font_bold" id="S4.T3.1.1.2.2.1.1">
            Class
           </span>
          </th>
         </tr>
         <tr class="ltx_tr" id="S4.T3.1.1.3.3">
          <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.1.3.3.1">
           person
          </th>
          <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.3.3.2">
           PERSON
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T3.1.1.4.4">
          <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T3.1.1.4.4.1">
           group
          </th>
          <td class="ltx_td ltx_align_left" id="S4.T3.1.1.4.4.2">
           NORP, ORG, LANGUAGE
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T3.1.1.5.5">
          <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T3.1.1.5.5.1">
           location
          </th>
          <td class="ltx_td ltx_align_left" id="S4.T3.1.1.5.5.2">
           GPE, FAC, LOC
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T3.1.1.6.6">
          <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T3.1.1.6.6.1">
           culture
          </th>
          <td class="ltx_td ltx_align_left" id="S4.T3.1.1.6.6.2">
           EVENT, WORK_OF_ART, LAW, PRODUCT
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T3.1.1.7.7">
          <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T3.1.1.7.7.1">
           date
          </th>
          <td class="ltx_td ltx_align_left" id="S4.T3.1.1.7.7.2">
           DATE, TIME
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T3.1.1.8.8">
          <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S4.T3.1.1.8.8.1">
           numeral
          </th>
          <td class="ltx_td ltx_align_left ltx_border_b" id="S4.T3.1.1.8.8.2">
           CARDINAL, PERCENT, MONEY, QUANTITY, ORDINAL
          </td>
         </tr>
        </tbody>
       </table>
      </span>
     </div>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_table">
       Table 3:
      </span>
      Aggregation of SpaCy NER types.
     </figcaption>
    </figure>
    <figure class="ltx_table" id="S4.T4">
     <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.1" style="width:433.6pt;height:264pt;vertical-align:-261.5pt;">
      <span class="ltx_transformed_inner" style="transform:translate(-45.2pt,0.3pt) scale(0.827544811273777,0.827544811273777) ;">
       <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.1.1">
        <tbody class="ltx_tbody">
         <tr class="ltx_tr" id="S4.T4.1.1.1.1">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.1.1.1.1" rowspan="2">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.1.1">
            Model
           </span>
          </th>
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.1.1.1.2" rowspan="2">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.2.1">
            Method
           </span>
          </th>
          <td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T4.1.1.1.1.3" rowspan="2">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.3.1">
            GSM8K
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.1.4" rowspan="2">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.4.1">
            ASDiv
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.1.5" rowspan="2">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.5.1">
            SVAMP
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" colspan="5" id="S4.T4.1.1.1.1.6">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.6.1">
            MAWPS
           </span>
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T4.1.1.2.2">
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.2.2.1">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.1.1">
            AddSub
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.2.2.2">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.2.1">
            SingleEQ
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.2.2.3">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.3.1">
            SingleOp
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.2.2.4">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.4.1">
            MultiArith
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.2.2.5">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.5.1">
            All
           </span>
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T4.1.1.3.3">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.1.3.3.1">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.3.3.1.1">
            LLaMa-2
           </span>
          </th>
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.1.3.3.2">
           CoT-FSP
          </th>
          <td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.3">
           16.38
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.4">
           47.85
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.5">
           38.40
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.6">
           52.41
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.7">
           63.39
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.8">
           82.03
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.9">
           43.33
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.10">
           60.53
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T4.1.1.4.4">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.1.4.4.1" rowspan="3">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.4.4.1.1">
            -7B
           </span>
          </th>
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.1.4.4.2">
           CoT-FT
          </th>
          <td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T4.1.1.4.4.3">
           35.33
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.4.4.4">
           57.18
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.4.4.5">
           48.20
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.4.4.6">
           66.08
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.4.4.7">
           74.41
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.4.4.8">
           85.23
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.4.4.9">
           65.00
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.4.4.10">
           73.03
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T4.1.1.5.5">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.1.5.5.1">
           Toolformer
          </th>
          <td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T4.1.1.5.5.2">
           17.59
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.3">
           48.55
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.4">
           37.10
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.5">
           47.34
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.6">
           58.46
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.7">
           79.54
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.8">
           50.67
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.9">
           59.81
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T4.1.1.6.6">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.1.6.6.1">
           CoA
          </th>
          <td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T4.1.1.6.6.2">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.6.6.2.1">
            37.83
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.3">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.6.6.3.1">
            57.61
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.4">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.6.6.4.1">
            51.70
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.5">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.6.6.5.1">
            72.15
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.6">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.6.6.6.1">
            82.48
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.7">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.6.6.7.1">
            86.48
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.8">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.6.6.8.1">
            73.17
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.9">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.6.6.9.1">
            78.89
           </span>
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T4.1.1.7.7">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.1.7.7.1">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.7.7.1.1">
            LLaMa-2
           </span>
          </th>
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.1.7.7.2">
           CoT-FSP
          </th>
          <td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T4.1.1.7.7.3">
           24.03
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.7.7.4">
           54.14
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.7.7.5">
           51.30
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.7.7.6">
           71.90
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.7.7.7">
           72.44
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.7.7.8">
           85.41
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.7.7.9">
           74.00
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.7.7.10">
           76.32
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T4.1.1.8.8">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.1.8.8.1" rowspan="5">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.8.8.1.1">
            -Chat-7B
           </span>
          </th>
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.1.8.8.2">
           CoT-FT
          </th>
          <td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T4.1.1.8.8.3">
           35.41
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.8.8.4">
           59.00
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.8.8.5">
           46.90
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.8.8.6">
           58.23
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.8.8.7">
           72.24
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.8.8.8">
           85.41
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.8.8.9">
           73.00
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.8.8.10">
           73.37
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T4.1.1.9.9">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.1.9.9.1">
           Toolformer
          </th>
          <td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T4.1.1.9.9.2">
           23.65
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.9.9.3">
           50.85
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.9.9.4">
           48.80
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.9.9.5">
           61.01
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.9.9.6">
           69.09
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.9.9.7">
           81.85
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.9.9.8">
           68.50
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.9.9.9">
           70.85
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T4.1.1.10.10">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.1.10.10.1">
           Toolformer - Math
          </th>
          <td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T4.1.1.10.10.2">
           36.01
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.3">
           59.18
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.4">
           47.60
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.5">
           58.99
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.6">
           72.44
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.7">
           85.94
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.8">
           75.50
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.9">
           74.43
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T4.1.1.11.11">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.1.11.11.1">
           CoA
          </th>
          <td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T4.1.1.11.11.2">
           38.29
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.11.11.3">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.11.11.3.1">
            59.57
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.11.11.4">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.11.11.4.1">
            54.20
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.11.11.5">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.11.11.5.1">
            72.41
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.11.11.6">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.11.11.6.1">
            81.89
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.11.11.7">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.11.11.7.1">
            88.26
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.11.11.8">
           83.00
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.11.11.9">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.11.11.9.1">
            82.13
           </span>
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T4.1.1.12.12">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.1.12.12.1">
           CoA (no Tool)
          </th>
          <td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T4.1.1.12.12.2">
           35.03
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.12.12.3">
           58.79
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.12.12.4">
           51.50
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.12.12.5">
           68.10
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.12.12.6">
           74.21
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.12.12.7">
           86.48
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.12.12.8">
           77.67
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.12.12.9">
           77.38
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T4.1.1.13.13">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.1.13.13.1">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.13.13.1.1">
            LLaMa-2
           </span>
          </th>
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.1.13.13.2">
           CoT-FSP
          </th>
          <td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T4.1.1.13.13.3">
           56.18
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.13.13.4">
           65.94
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.13.13.5">
           70.60
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.13.13.6">
           86.08
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.13.13.7">
           89.17
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.13.13.8">
           92.88
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.13.13.9">
           84.50
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.13.13.10">
           88.23
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T4.1.1.14.14">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T4.1.1.14.14.1" rowspan="4">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.14.14.1.1">
            -Chat-70B
           </span>
          </th>
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.1.14.14.2">
           CoT-FT
          </th>
          <td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T4.1.1.14.14.3">
           60.50
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.14.14.4">
           70.24
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.14.14.5">
           70.40
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.14.14.6">
           81.52
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.14.14.7">
           87.60
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.14.14.8">
           92.35
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.14.14.9">
           89.17
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.14.14.10">
           88.18
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T4.1.1.15.15">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.1.15.15.1">
           Toolformer
          </th>
          <td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T4.1.1.15.15.2">
           52.54
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.15.15.3">
           69.07
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.15.15.4">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.15.15.4.1">
            73.60
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.15.15.5">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.15.15.5.1">
            86.84
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.15.15.6">
           89.76
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.15.15.7">
           91.46
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.15.15.8">
           81.50
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.15.15.9">
           87.26
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T4.1.1.16.16">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.1.16.16.1">
           Toolformer - Math
          </th>
          <td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T4.1.1.16.16.2">
           61.03
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.16.16.3">
           70.59
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.16.16.4">
           73.20
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.16.16.5">
           85.57
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.16.16.6">
           91.34
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.16.16.7">
           91.99
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.16.16.8">
           92.00
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T4.1.1.16.16.9">
           90.60
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T4.1.1.17.17">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T4.1.1.17.17.1">
           CoA
          </th>
          <td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b" id="S4.T4.1.1.17.17.2">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.17.17.2.1">
            62.32
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.1.17.17.3">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.17.17.3.1">
            71.89
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.1.17.17.4">
           73.40
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.1.17.17.5">
           86.33
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.1.17.17.6">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.17.17.6.1">
            94.49
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.1.17.17.7">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.17.17.7.1">
            93.06
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.1.17.17.8">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.17.17.8.1">
            92.33
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.1.17.17.9">
           <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.17.17.9.1">
            91.91
           </span>
          </td>
         </tr>
        </tbody>
       </table>
      </span>
     </div>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_table">
       Table 4:
      </span>
      Evaluation results on LLaMa-2 and LLaMa-2-Chat for mathematical reasoning. “All” denotes the averaged results on four MAWPS portions. Exact match rate to the final gold answer (
      <span class="ltx_text ltx_font_italic" id="S4.T4.4.1">
       i.e.
      </span>
      , accuracy) is reported. Best performing augmentation approach for each base model is
      <span class="ltx_text ltx_font_bold" id="S4.T4.5.2">
       bolded
      </span>
      .
     </figcaption>
    </figure>
   </section>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3
    </span>
    Baselines
   </h3>
   <div class="ltx_para" id="S4.SS3.p1">
    <p class="ltx_p" id="S4.SS3.p1.1">
     We apply our
     <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">
      CoA
     </span>
     reasoning method to both 7B and 70B LLaMa models, and test various model versions including the first version of LLaMa
     <cite class="ltx_cite ltx_citemacro_citep">
      (Touvron et al.,
      <a class="ltx_ref" href="#bib.bib36" title="">
       2023a
      </a>
      )
     </cite>
     and more advanced LLaMa-2 and LLaMa-2-Chat
     <cite class="ltx_cite ltx_citemacro_citep">
      (Touvron et al.,
      <a class="ltx_ref" href="#bib.bib37" title="">
       2023b
      </a>
      )
     </cite>
     .
We compare our method to several baselines, including: a) few-shot prompting using 8 randomly sampled QA exemplars from the original (
     <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.2">
      i.e.
     </span>
     , not re-written) chain-of-thought data (
     <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.3">
      CoT-FSP
     </span>
     ), b) fine-tuning with original chain-of-thought data (
     <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.4">
      CoT-FT
     </span>
     )
     <span class="ltx_note ltx_role_footnote" id="footnote12">
      <sup class="ltx_note_mark">
       12
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         12
        </sup>
        <span class="ltx_tag ltx_tag_note">
         12
        </span>
        Note that in Wiki QA domain, the HotpotQA data used for prompting or fine-tuning baselines is pre-processed to contain both gold Wikipedia articles (serving as chain-of-thought explanations) and the final answer.
       </span>
      </span>
     </span>
     , and c)
     <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.5">
      Toolformer
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      Schick et al. (
      <a class="ltx_ref" href="#bib.bib34" title="">
       2023
      </a>
      )
     </cite>
     which fine-tunes LLMs on CCNet
     <cite class="ltx_cite ltx_citemacro_citep">
      (Wenzek et al.,
      <a class="ltx_ref" href="#bib.bib41" title="">
       2020
      </a>
      )
     </cite>
     texts augmented with API calls.
For evaluation on Wiki QA, we also compared our method with
     <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.6">
      FireAct
     </span>
     <cite class="ltx_cite ltx_citemacro_citep">
      (Chen et al.,
      <a class="ltx_ref" href="#bib.bib5" title="">
       2023
      </a>
      )
     </cite>
     , which fine-tunes LLMs on HotpotQA ReAct
     <cite class="ltx_cite ltx_citemacro_citep">
      (Yao et al.,
      <a class="ltx_ref" href="#bib.bib45" title="">
       2022
      </a>
      )
     </cite>
     trajectories distilled from GPT-4
     <cite class="ltx_cite ltx_citemacro_citep">
      (OpenAI,
      <a class="ltx_ref" href="#bib.bib27" title="">
       2023
      </a>
      )
     </cite>
     .
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Results and Analysis
  </h2>
  <section class="ltx_subsection" id="S5.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.1
    </span>
    Mathematical Reasoning
   </h3>
   <div class="ltx_para" id="S5.SS1.p1">
    <p class="ltx_p" id="S5.SS1.p1.1">
     Table
     <a class="ltx_ref" href="#S4.T4" title="Table 4 ‣ Domain Tools ‣ 4.2 Wikipedia QA ‣ 4 Experimental Settings ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     shows the evaluation results for the LLaMa-2 and LLaMa-2-Chat models.
     <span class="ltx_note ltx_role_footnote" id="footnote13">
      <sup class="ltx_note_mark">
       13
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         13
        </sup>
        <span class="ltx_tag ltx_tag_note">
         13
        </span>
        We include similar evaluation results for the original LLaMa model (7B) in Appendix
        <a class="ltx_ref" href="#A2" title="Appendix B Full Experimental Results ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
         <span class="ltx_text ltx_ref_tag">
          B
         </span>
        </a>
        .
       </span>
      </span>
     </span>
     On the GSM8K and ASDiv datasets, our chain-of-abstraction (CoA) method outperforms the few-shot baseline method CoT-FSP and the regular fine-tuning baseline CoT-FT, demonstrating that our CoA fine-tuning with tool augmentation is more effective in adapting LLMs to multi-step reasoning tasks.
Similarly, when evaluated on SVAMP and MAWPS, CoA also consistently outperforms CoT-FSP.
Interestingly, for these out-of-distribution datasets, CoT-FT lags further behind CoA, particularly for 7B models, showing that CoA reasoning yields more distributionally robust reasoning performance.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS1.p2">
    <p class="ltx_p" id="S5.SS1.p2.1">
     Our CoA method also surpasses the tool-augmented baseline Toolformer, which implies that planning the abstract variables in CoA can improve the accuracy of reasoning with tools.
However, as Toolformer is not originally trained with in-domain fine-tuning data,
     <span class="ltx_note ltx_role_footnote" id="footnote14">
      <sup class="ltx_note_mark">
       14
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         14
        </sup>
        <span class="ltx_tag ltx_tag_note">
         14
        </span>
        Toolformer is fine-tuned on CCNet data, which may not contain rich mathematical reasoning samples.
       </span>
      </span>
     </span>
     we also fine-tune a new version of Toolformer with the chain-of-thought data from GSM8K and ASDiv, denoted as
     <span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">
      Toolformer - Math
     </span>
     in Table
     <a class="ltx_ref" href="#S4.T4" title="Table 4 ‣ Domain Tools ‣ 4.2 Wikipedia QA ‣ 4 Experimental Settings ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     .
We also observe that CoA performs better than Toolformer - Math, confirming that the introduction of abstract variables enables more robust tool use compared to direct integration of API calls within chain-of-thought reasoning.
    </p>
   </div>
   <section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Ablation Study
    </h4>
    <div class="ltx_para" id="S5.SS1.SSS0.Px1.p1">
     <p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.1">
      We verify that the robustness of CoA reasoning does not merely benefit from using additional tools, by fine-tuning another LLM (from the same model backbone) to perform the equation solving instead of calling the equation solver, denoted as
      <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px1.p1.1.1">
       CoA (no Tool)
      </span>
      in Table
      <a class="ltx_ref" href="#S4.T4" title="Table 4 ‣ Domain Tools ‣ 4.2 Wikipedia QA ‣ 4 Experimental Settings ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
       <span class="ltx_text ltx_ref_tag">
        4
       </span>
      </a>
      .
We find that CoA (no Tool) performs consistently worse than CoA across all datasets, confirming that using specialized tools enables LLM agents to conduct more precise operations, rather than directly solving the same operations.
However, we find that CoA (no Tool) still outperforms all baseline methods on zero-shot generalization to SVAMP and MAWPS datasets, implying that chain-of-abstraction reasoning also contributes to better robustness of CoA, perhaps due to better planning of multiple reasoning steps indexed by abstract variables.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Reasoning Steps
    </h4>
    <div class="ltx_para" id="S5.SS1.SSS0.Px2.p1">
     <p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.1">
      Our findings suggest that the benefits of chain-of-abstraction reasoning are most pronounced when problems require long reasoning chains to be solved. Figure
      <a class="ltx_ref" href="#S5.F3" title="Figure 3 ‣ Reasoning Steps ‣ 5.1 Mathematical Reasoning ‣ 5 Results and Analysis ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
       <span class="ltx_text ltx_ref_tag">
        3
       </span>
      </a>
      shows the stratified performance of three models on GSM8K QA, relative to the number of reasoning steps in the predicted and gold reasoning chains.
Compared to the few-shot CoT-FSP, CoA produces reasoning chains that more often match the length of the gold reasoning chains, as reflected by the heat-map statistics (left column) being more aggregated around the diagonal (comparable to CoT-FT).
At the same time, we observe that models achieve better QA accuracy when the number of reasoning steps in their generated answers are aligned with the gold references (
      <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px2.p1.1.1">
       i.e.
      </span>
      , the diagonal of heat-maps in right column).
All above results show that fine-tuned models are better at learning to produce reasoning chains that match the true reasoning chain for the problem.
     </p>
    </div>
    <div class="ltx_para" id="S5.SS1.SSS0.Px2.p2">
     <p class="ltx_p" id="S5.SS1.SSS0.Px2.p2.1">
      Interestingly, we find that CoA, compared to CoT-FT, achieves higher performance especially on questions that require more reasoning steps. In the right column of Figure
      <a class="ltx_ref" href="#S5.F3" title="Figure 3 ‣ Reasoning Steps ‣ 5.1 Mathematical Reasoning ‣ 5 Results and Analysis ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
       <span class="ltx_text ltx_ref_tag">
        3
       </span>
      </a>
      , CoA’s improvement over CoT-FT is more pronounced on questions with more than
      <math alttext="3" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p2.1.m1.1">
       <semantics id="S5.SS1.SSS0.Px2.p2.1.m1.1a">
        <mn id="S5.SS1.SSS0.Px2.p2.1.m1.1.1" xref="S5.SS1.SSS0.Px2.p2.1.m1.1.1.cmml">
         3
        </mn>
        <annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p2.1.m1.1b">
         <cn id="S5.SS1.SSS0.Px2.p2.1.m1.1.1.cmml" type="integer" xref="S5.SS1.SSS0.Px2.p2.1.m1.1.1">
          3
         </cn>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p2.1.m1.1c">
         3
        </annotation>
       </semantics>
      </math>
      steps in the gold reasoning chain (highlighted with red squares).
We also present overall accuracy scores on GSM8K subsets according to varying numbers of gold reasoning steps in Table
      <a class="ltx_ref" href="#S5.T5" title="Table 5 ‣ Reasoning Steps ‣ 5.1 Mathematical Reasoning ‣ 5 Results and Analysis ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
       <span class="ltx_text ltx_ref_tag">
        5
       </span>
      </a>
      , where we confirm this result, indicating that the model trained with CoA has more robust long chain-of-thought reasoning capability, which is enabled from learning to plan using abstractions.
     </p>
    </div>
    <figure class="ltx_figure" id="S5.F3">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="519" id="S5.F3.g1" src="/html/2401.17464/assets/x3.png" width="461"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       Figure 3:
      </span>
      Fine-grained GSM8K evaluation results on LLaMa-2-Chat-7B
      <span class="ltx_text ltx_font_italic" id="S5.F3.2.1">
       w.r.t.
      </span>
      the number of reasoning steps in the predicted and gold reasoning chain. (Left) The total number of test examples that belong to each stratum. (Right) The corresponding model accuracy (%) for those examples. Non-diagonal cells with fewer than 15 examples are ignored.
     </figcaption>
    </figure>
    <figure class="ltx_table" id="S5.T5">
     <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.5" style="width:433.6pt;height:236.7pt;vertical-align:-230.8pt;">
      <span class="ltx_transformed_inner" style="transform:translate(106.9pt,-1.5pt) scale(1.97298939929525,1.97298939929525) ;">
       <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.5.5">
        <thead class="ltx_thead">
         <tr class="ltx_tr" id="S5.T5.5.5.6.1">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T5.5.5.6.1.1" rowspan="2">
           <span class="ltx_text ltx_font_bold" id="S5.T5.5.5.6.1.1.1">
            Method
           </span>
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="5" id="S5.T5.5.5.6.1.2">
           <span class="ltx_text ltx_font_bold" id="S5.T5.5.5.6.1.2.1">
            Gold Reasoning Step
           </span>
          </th>
         </tr>
         <tr class="ltx_tr" id="S5.T5.5.5.5">
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.1">
           <math alttext="\leq 2" class="ltx_Math" display="inline" id="S5.T5.1.1.1.1.m1.1">
            <semantics id="S5.T5.1.1.1.1.m1.1a">
             <mrow id="S5.T5.1.1.1.1.m1.1.1" xref="S5.T5.1.1.1.1.m1.1.1.cmml">
              <mi id="S5.T5.1.1.1.1.m1.1.1.2" xref="S5.T5.1.1.1.1.m1.1.1.2.cmml">
              </mi>
              <mo id="S5.T5.1.1.1.1.m1.1.1.1" xref="S5.T5.1.1.1.1.m1.1.1.1.cmml">
               ≤
              </mo>
              <mn id="S5.T5.1.1.1.1.m1.1.1.3" xref="S5.T5.1.1.1.1.m1.1.1.3.cmml">
               2
              </mn>
             </mrow>
             <annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.1.m1.1b">
              <apply id="S5.T5.1.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.1.m1.1.1">
               <leq id="S5.T5.1.1.1.1.m1.1.1.1.cmml" xref="S5.T5.1.1.1.1.m1.1.1.1">
               </leq>
               <csymbol cd="latexml" id="S5.T5.1.1.1.1.m1.1.1.2.cmml" xref="S5.T5.1.1.1.1.m1.1.1.2">
                absent
               </csymbol>
               <cn id="S5.T5.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S5.T5.1.1.1.1.m1.1.1.3">
                2
               </cn>
              </apply>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="S5.T5.1.1.1.1.m1.1c">
              \leq 2
             </annotation>
            </semantics>
           </math>
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.2.2.2.2">
           <math alttext="3" class="ltx_Math" display="inline" id="S5.T5.2.2.2.2.m1.1">
            <semantics id="S5.T5.2.2.2.2.m1.1a">
             <mn id="S5.T5.2.2.2.2.m1.1.1" xref="S5.T5.2.2.2.2.m1.1.1.cmml">
              3
             </mn>
             <annotation-xml encoding="MathML-Content" id="S5.T5.2.2.2.2.m1.1b">
              <cn id="S5.T5.2.2.2.2.m1.1.1.cmml" type="integer" xref="S5.T5.2.2.2.2.m1.1.1">
               3
              </cn>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="S5.T5.2.2.2.2.m1.1c">
              3
             </annotation>
            </semantics>
           </math>
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.3.3.3.3">
           <math alttext="4" class="ltx_Math" display="inline" id="S5.T5.3.3.3.3.m1.1">
            <semantics id="S5.T5.3.3.3.3.m1.1a">
             <mn id="S5.T5.3.3.3.3.m1.1.1" xref="S5.T5.3.3.3.3.m1.1.1.cmml">
              4
             </mn>
             <annotation-xml encoding="MathML-Content" id="S5.T5.3.3.3.3.m1.1b">
              <cn id="S5.T5.3.3.3.3.m1.1.1.cmml" type="integer" xref="S5.T5.3.3.3.3.m1.1.1">
               4
              </cn>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="S5.T5.3.3.3.3.m1.1c">
              4
             </annotation>
            </semantics>
           </math>
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.4.4.4.4">
           <math alttext="5" class="ltx_Math" display="inline" id="S5.T5.4.4.4.4.m1.1">
            <semantics id="S5.T5.4.4.4.4.m1.1a">
             <mn id="S5.T5.4.4.4.4.m1.1.1" xref="S5.T5.4.4.4.4.m1.1.1.cmml">
              5
             </mn>
             <annotation-xml encoding="MathML-Content" id="S5.T5.4.4.4.4.m1.1b">
              <cn id="S5.T5.4.4.4.4.m1.1.1.cmml" type="integer" xref="S5.T5.4.4.4.4.m1.1.1">
               5
              </cn>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="S5.T5.4.4.4.4.m1.1c">
              5
             </annotation>
            </semantics>
           </math>
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.5.5.5.5">
           <math alttext="&gt;5" class="ltx_Math" display="inline" id="S5.T5.5.5.5.5.m1.1">
            <semantics id="S5.T5.5.5.5.5.m1.1a">
             <mrow id="S5.T5.5.5.5.5.m1.1.1" xref="S5.T5.5.5.5.5.m1.1.1.cmml">
              <mi id="S5.T5.5.5.5.5.m1.1.1.2" xref="S5.T5.5.5.5.5.m1.1.1.2.cmml">
              </mi>
              <mo id="S5.T5.5.5.5.5.m1.1.1.1" xref="S5.T5.5.5.5.5.m1.1.1.1.cmml">
               &gt;
              </mo>
              <mn id="S5.T5.5.5.5.5.m1.1.1.3" xref="S5.T5.5.5.5.5.m1.1.1.3.cmml">
               5
              </mn>
             </mrow>
             <annotation-xml encoding="MathML-Content" id="S5.T5.5.5.5.5.m1.1b">
              <apply id="S5.T5.5.5.5.5.m1.1.1.cmml" xref="S5.T5.5.5.5.5.m1.1.1">
               <gt id="S5.T5.5.5.5.5.m1.1.1.1.cmml" xref="S5.T5.5.5.5.5.m1.1.1.1">
               </gt>
               <csymbol cd="latexml" id="S5.T5.5.5.5.5.m1.1.1.2.cmml" xref="S5.T5.5.5.5.5.m1.1.1.2">
                absent
               </csymbol>
               <cn id="S5.T5.5.5.5.5.m1.1.1.3.cmml" type="integer" xref="S5.T5.5.5.5.5.m1.1.1.3">
                5
               </cn>
              </apply>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="S5.T5.5.5.5.5.m1.1c">
              &gt;5
             </annotation>
            </semantics>
           </math>
          </th>
         </tr>
        </thead>
        <tbody class="ltx_tbody">
         <tr class="ltx_tr" id="S5.T5.5.5.7.1">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.5.5.7.1.1">
           CoT-FSP
          </th>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.5.5.7.1.2">
           42.9
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.5.5.7.1.3">
           26.3
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.5.5.7.1.4">
           18.0
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.5.5.7.1.5">
           10.9
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.5.5.7.1.6">
           3.6
          </td>
         </tr>
         <tr class="ltx_tr" id="S5.T5.5.5.8.2">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.5.5.8.2.1">
           CoT-FT
          </th>
          <td class="ltx_td ltx_align_center" id="S5.T5.5.5.8.2.2">
           55.5
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T5.5.5.8.2.3">
           42.6
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T5.5.5.8.2.4">
           25.8
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T5.5.5.8.2.5">
           19.0
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T5.5.5.8.2.6">
           10.8
          </td>
         </tr>
         <tr class="ltx_tr" id="S5.T5.5.5.9.3">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S5.T5.5.5.9.3.1" rowspan="2">
           <span class="ltx_text" id="S5.T5.5.5.9.3.1.1">
            CoA
           </span>
          </th>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.5.5.9.3.2">
           <span class="ltx_text ltx_font_bold" id="S5.T5.5.5.9.3.2.1">
            55.8
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.5.5.9.3.3">
           <span class="ltx_text ltx_font_bold" id="S5.T5.5.5.9.3.3.1">
            44.4
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.5.5.9.3.4">
           <span class="ltx_text ltx_font_bold" id="S5.T5.5.5.9.3.4.1">
            32.5
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.5.5.9.3.5">
           <span class="ltx_text ltx_font_bold" id="S5.T5.5.5.9.3.5.1">
            25.3
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.5.5.9.3.6">
           <span class="ltx_text ltx_font_bold" id="S5.T5.5.5.9.3.6.1">
            15.1
           </span>
          </td>
         </tr>
         <tr class="ltx_tr" id="S5.T5.5.5.10.4">
          <td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.5.5.10.4.1">
           +0.3
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.5.5.10.4.2">
           +1.8
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.5.5.10.4.3">
           +6.7
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.5.5.10.4.4">
           +6.3
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.5.5.10.4.5">
           +4.3
          </td>
         </tr>
        </tbody>
       </table>
      </span>
     </div>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_table">
       Table 5:
      </span>
      Stratified LLaMa-2-Chat-7B evaluation results on GSM8K with different gold reasoning steps. The last row reports absolute accuracy improvement of our CoA method compared to fine-tuning baseline CoT-FT.
     </figcaption>
    </figure>
    <figure class="ltx_table" id="S5.T6">
     <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T6.1">
      <thead class="ltx_thead">
       <tr class="ltx_tr" id="S5.T6.1.1.1">
        <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T6.1.1.1.1" rowspan="2">
         <span class="ltx_text ltx_font_bold" id="S5.T6.1.1.1.1.1">
          Method
         </span>
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S5.T6.1.1.1.2">
         <span class="ltx_text ltx_font_bold" id="S5.T6.1.1.1.2.1">
          Error Rate
         </span>
        </th>
       </tr>
       <tr class="ltx_tr" id="S5.T6.1.2.2">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.1.2.2.1">
         Arithmetic
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.1.2.2.2">
         Reasoning
        </th>
       </tr>
      </thead>
      <tbody class="ltx_tbody">
       <tr class="ltx_tr" id="S5.T6.1.3.1">
        <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T6.1.3.1.1">
         CoT-FSP
        </th>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.3.1.2">
         17.3
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.3.1.3">
         70.3
        </td>
       </tr>
       <tr class="ltx_tr" id="S5.T6.1.4.2">
        <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.1.4.2.1">
         CoT-FT
        </th>
        <td class="ltx_td ltx_align_center" id="S5.T6.1.4.2.2">
         25.2
        </td>
        <td class="ltx_td ltx_align_center" id="S5.T6.1.4.2.3">
         67.8
        </td>
       </tr>
       <tr class="ltx_tr" id="S5.T6.1.5.3">
        <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S5.T6.1.5.3.1">
         CoA
        </th>
        <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T6.1.5.3.2">
         <span class="ltx_text ltx_font_bold" id="S5.T6.1.5.3.2.1">
          0.0
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T6.1.5.3.3">
         <span class="ltx_text ltx_font_bold" id="S5.T6.1.5.3.3.1">
          60.4
         </span>
        </td>
       </tr>
      </tbody>
     </table>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_table">
       Table 6:
      </span>
      Human evaluation results of arithmetic and reasoning error rates on 200 GSM8K test samples. Models developed based on LLaMa-2-Chat-7B are presented.
     </figcaption>
    </figure>
    <figure class="ltx_figure" id="S5.F4">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="347" id="S5.F4.g1" src="/html/2401.17464/assets/x4.png" width="461"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       Figure 4:
      </span>
      Wall-clock inference time on GSM8K (seeded with LLaMa-2-Chat-7B). Average time of answering a question is measured (in seconds)
      <span class="ltx_text ltx_font_italic" id="S5.F4.2.1">
       w.r.t.
      </span>
      the number of gold reasoning steps required for the question.
     </figcaption>
    </figure>
    <figure class="ltx_table" id="S5.T7">
     <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T7.1" style="width:433.6pt;height:240pt;vertical-align:-237.1pt;">
      <span class="ltx_transformed_inner" style="transform:translate(-6.3pt,0.0pt) scale(0.971695673870306,0.971695673870306) ;">
       <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T7.1.1">
        <tbody class="ltx_tbody">
         <tr class="ltx_tr" id="S5.T7.1.1.1.1">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T7.1.1.1.1.1" rowspan="2">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.1.1.1.1">
            Model
           </span>
          </th>
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T7.1.1.1.1.2" rowspan="2">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.1.1.2.1">
            Method
           </span>
          </th>
          <td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="S5.T7.1.1.1.1.3">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.1.1.3.1">
            HotpotQA
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.1.1.4" rowspan="2">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.1.1.4.1">
            WQ
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.1.1.5" rowspan="2">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.1.1.5.1">
            NQ
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.1.1.6" rowspan="2">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.1.1.6.1">
            TriviaQA
           </span>
          </td>
         </tr>
         <tr class="ltx_tr" id="S5.T7.1.1.2.2">
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.2.2.1">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.2.2.1.1">
            Bridge
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.2.2.2">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.2.2.2.1">
            Comparison
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.2.2.3">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.2.2.3.1">
            Both
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.2.2.4">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.2.2.4.1">
            Time
           </span>
          </td>
         </tr>
         <tr class="ltx_tr" id="S5.T7.1.1.3.3">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T7.1.1.3.3.1">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.3.3.1.1">
            LLaMa-2
           </span>
          </th>
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T7.1.1.3.3.2">
           CoT-FSP
          </th>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.3.3.3">
           11.69
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.3.3.4">
           45.46
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.3.3.5">
           18.47
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.3.3.6">
           2.074
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.3.3.7">
           34.65
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.3.3.8">
           30.91
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.3.3.9">
           53.48
          </td>
         </tr>
         <tr class="ltx_tr" id="S5.T7.1.1.4.4">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.1.4.4.1" rowspan="5">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.4.4.1.1">
            -Chat-7B
           </span>
          </th>
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.1.4.4.2">
           CoT-FT
          </th>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.4.4.3">
           14.24
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.4.4.4">
           56.69
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.4.4.5">
           22.77
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.4.4.6">
           1.937
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.4.4.7">
           33.51
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.4.4.8">
           25.40
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.4.4.9">
           51.05
          </td>
         </tr>
         <tr class="ltx_tr" id="S5.T7.1.1.5.5">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.1.5.5.1">
           Toolformer
          </th>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.5.5.2">
           12.99
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.5.5.3">
           44.59
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.5.5.4">
           20.00
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.5.5.5">
           2.350
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.5.5.6">
           36.22
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.5.5.7">
           30.22
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.5.5.8">
           54.15
          </td>
         </tr>
         <tr class="ltx_tr" id="S5.T7.1.1.6.6">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.1.6.6.1">
           Toolformer - Wiki
          </th>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.6.6.2">
           15.68
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.6.6.3">
           56.42
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.6.6.4">
           23.86
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.6.6.5">
           2.301
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.6.6.6">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.6.6.6.1">
            36.61
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.6.6.7">
           32.96
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.6.6.8">
           55.08
          </td>
         </tr>
         <tr class="ltx_tr" id="S5.T7.1.1.7.7">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.1.7.7.1">
           FireAct
          </th>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.7.7.2">
           19.18
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.7.7.3">
           54.14
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.7.7.4">
           26.20
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.7.7.5">
           2.706
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.7.7.6">
           36.02
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.7.7.7">
           35.87
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.7.7.8">
           52.96
          </td>
         </tr>
         <tr class="ltx_tr" id="S5.T7.1.1.8.8">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.1.8.8.1">
           CoA
          </th>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.8.8.2">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.8.8.2.1">
            21.00
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.8.8.3">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.8.8.3.1">
            56.96
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.8.8.4">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.8.8.4.1">
            28.22
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.8.8.5">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.8.8.5.1">
            1.896
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.8.8.6">
           35.97
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.8.8.7">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.8.8.7.1">
            38.67
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.8.8.8">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.8.8.8.1">
            57.90
           </span>
          </td>
         </tr>
         <tr class="ltx_tr" id="S5.T7.1.1.9.9">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T7.1.1.9.9.1">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.9.9.1.1">
            LLaMa-2
           </span>
          </th>
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T7.1.1.9.9.2">
           CoT-FSP
          </th>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.9.9.3">
           21.39
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.9.9.4">
           56.62
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.9.9.5">
           28.47
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.9.9.6">
           6.668
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.9.9.7">
           34.89
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.9.9.8">
           37.42
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.9.9.9">
           63.61
          </td>
         </tr>
         <tr class="ltx_tr" id="S5.T7.1.1.10.10">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.1.10.10.1" rowspan="3">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.10.10.1.1">
            -Chat-70B
           </span>
          </th>
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.1.10.10.2">
           CoT-FT
          </th>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.10.10.3">
           23.84
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.10.10.4">
           63.95
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.10.10.5">
           31.90
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.10.10.6">
           6.401
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.10.10.7">
           34.15
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.10.10.8">
           39.75
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.10.10.9">
           62.28
          </td>
         </tr>
         <tr class="ltx_tr" id="S5.T7.1.1.11.11">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.1.11.11.1">
           Toolformer
          </th>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.11.11.2">
           22.24
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.11.11.3">
           56.09
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.11.11.4">
           29.04
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.11.11.5">
           6.888
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.11.11.6">
           37.16
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.11.11.7">
           40.42
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.11.11.8">
           64.31
          </td>
         </tr>
         <tr class="ltx_tr" id="S5.T7.1.1.12.12">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.1.12.12.1">
           Toolformer - Wiki
          </th>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.12.12.2">
           26.38
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.12.12.3">
           63.82
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.12.12.4">
           33.90
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.12.12.5">
           6.855
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.12.12.6">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.12.12.6.1">
            37.70
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.12.12.7">
           41.25
          </td>
          <td class="ltx_td ltx_align_center" id="S5.T7.1.1.12.12.8">
           66.64
          </td>
         </tr>
         <tr class="ltx_tr" id="S5.T7.1.1.13.13">
          <th class="ltx_td ltx_th ltx_th_row ltx_border_b" id="S5.T7.1.1.13.13.1">
          </th>
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T7.1.1.13.13.2">
           CoA
          </th>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S5.T7.1.1.13.13.3">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.13.13.3.1">
            27.61
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S5.T7.1.1.13.13.4">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.13.13.4.1">
            64.09
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S5.T7.1.1.13.13.5">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.13.13.5.1">
            34.94
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S5.T7.1.1.13.13.6">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.13.13.6.1">
            6.369
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S5.T7.1.1.13.13.7">
           36.37
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S5.T7.1.1.13.13.8">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.13.13.8.1">
            43.57
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S5.T7.1.1.13.13.9">
           <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.13.13.9.1">
            69.08
           </span>
          </td>
         </tr>
        </tbody>
       </table>
      </span>
     </div>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_table">
       Table 7:
      </span>
      Wiki QA evaluation results on LLaMa-2-Chat-based models. “Both” denotes the overall evaluation results on both bridge and comparison portions of HotpotQA. “Time” denotes the average seconds that each agent needs to answer a question in HotpotQA. Exact match rate to the final gold answer (
      <span class="ltx_text ltx_font_italic" id="S5.T7.3.1">
       i.e.
      </span>
      , accuracy) is reported.
     </figcaption>
    </figure>
   </section>
   <section class="ltx_paragraph" id="S5.SS1.SSS0.Px3">
    <h4 class="ltx_title ltx_title_paragraph">
     Human Evaluation
    </h4>
    <div class="ltx_para" id="S5.SS1.SSS0.Px3.p1">
     <p class="ltx_p" id="S5.SS1.SSS0.Px3.p1.1">
      To more comprehensively verify that CoA improves both knowledge operation (
      <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px3.p1.1.1">
       i.e.
      </span>
      , arithmetic by using tools) and reasoning accuracy, we conduct a human evaluation on different model answers to 200 randomly sampled GSM8K test questions.
Specifically, given a GSM8K question and a model’s answer to the question, we ask human workers to judge whether the answer contains any arithmetic errors (
      <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px3.p1.1.2">
       e.g.
      </span>
      , wrong calculations, invalid equations) or reasoning errors unrelated to math derivations (
      <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px3.p1.1.3">
       e.g.
      </span>
      , misunderstanding of the question, improper strategy for solving the question), and report how often the model makes these two kinds of errors.
In Table
      <a class="ltx_ref" href="#S5.T6" title="Table 6 ‣ Reasoning Steps ‣ 5.1 Mathematical Reasoning ‣ 5 Results and Analysis ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
       <span class="ltx_text ltx_ref_tag">
        6
       </span>
      </a>
      , we find that CoA effectively reduces arithmetic errors to zero, due to the use of equation solver to perform accurate calculations.
More importantly, our method also makes fewer reasoning errors compared to the baselines, verifying that CoA fine-tuning guides the model to learn more accurate reasoning through the holistic planning of abstract reasoning chains.
By contrast, ordinary fine-tuning (
      <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px3.p1.1.4">
       i.e.
      </span>
      , CoT-FT) produces a more limited reasoning improvement compared to the few-shot CoT-FSP, while also failing to suppress arithmetic errors.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S5.SS1.SSS0.Px4">
    <h4 class="ltx_title ltx_title_paragraph">
     Inference Efficiency
    </h4>
    <div class="ltx_para" id="S5.SS1.SSS0.Px4.p1">
     <p class="ltx_p" id="S5.SS1.SSS0.Px4.p1.1">
      Importantly, we find that the performance benefits of CoA reasoning do not come with increased computational costs.
In Figure
      <a class="ltx_ref" href="#S5.F4" title="Figure 4 ‣ Reasoning Steps ‣ 5.1 Mathematical Reasoning ‣ 5 Results and Analysis ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
       <span class="ltx_text ltx_ref_tag">
        4
       </span>
      </a>
      , we show the average time (seconds) that CoA and baseline agents (seeded with LLaMa-2-Chat-7B) needs to answer a question
      <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px4.p1.1.1">
       w.r.t.
      </span>
      a varying number of gold reasoning steps.
Compared to the CoT baselines, CoA requires less time than the few-shot baseline CoT-FSP, whose generation needs to be conditioned on additional examples.
However, CoA is slightly less inference-efficient compared to CoT-FT, likely due to the decoding of additional tokens (
      <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px4.p1.1.2">
       e.g.
      </span>
      , “[” and “]”) for the abstract statements.
     </p>
    </div>
    <div class="ltx_para" id="S5.SS1.SSS0.Px4.p2">
     <p class="ltx_p" id="S5.SS1.SSS0.Px4.p2.1">
      Compared to Toolformer, CoA has a lower and flatter inference time curve, indicating better scaling as the number of reasoning steps increases.
This difference arises because CoA decouples the generation of (abstract) reasoning chains from the retrieval of knowledge (
      <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px4.p2.1.1">
       i.e.
      </span>
      , tool use), allowing full reasoning chains to be decoded before any tool is called.
This procedure amortizes inference costs in two ways.
First, tool calls are made after the CoA trace has been decoded, enabling parallel tool calls for the same trace (
      <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px4.p2.1.2">
       e.g.
      </span>
      , using an equation solver once rather than multiple calls to a calculator), and avoiding the time delay caused by waiting for external API responses. Consequently, the model fine-tuned with CoA is more efficient at multi-step reasoning, especially when the number of reasoning steps (
      <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px4.p2.1.3">
       i.e.
      </span>
      , tool calls) increases.
Second, across multiple examples, the model can generate the CoA trace of the next example while tool calls are made for the preceding one, parallelizing CoA decoding and tools calls across examples.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S5.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.2
    </span>
    Wiki QA
   </h3>
   <div class="ltx_para" id="S5.SS2.p1">
    <p class="ltx_p" id="S5.SS2.p1.1">
     Table
     <a class="ltx_ref" href="#S5.T7" title="Table 7 ‣ Reasoning Steps ‣ 5.1 Mathematical Reasoning ‣ 5 Results and Analysis ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
      <span class="ltx_text ltx_ref_tag">
       7
      </span>
     </a>
     shows our Wiki QA evaluation results using LLaMa-2-Chat models.
     <span class="ltx_note ltx_role_footnote" id="footnote15">
      <sup class="ltx_note_mark">
       15
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         15
        </sup>
        <span class="ltx_tag ltx_tag_note">
         15
        </span>
        We include similar evaluation results on LLaMa-2-7B in Appendix
        <a class="ltx_ref" href="#A2" title="Appendix B Full Experimental Results ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
         <span class="ltx_text ltx_ref_tag">
          B
         </span>
        </a>
        .
       </span>
      </span>
     </span>
     Similar to mathematical reasoning, we fine-tune a new version of Toolformer with in-domain chain-of-thought data from HotpotQA, denoted as
     <span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.1">
      Toolformer - Wiki
     </span>
     .
On HotpotQA, CoA achieves higher exact match rates with the gold reference compared to the few-shot or fine-tuning baselines.
In particular, CoA outperforms CoT-FSP, CoT-FT, Toolformer and Toolformer - Wiki on the more challenging bridge-type QAs, where two steps of reasoning over Wikipedia knowledge are
     <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.2">
      consecutively
     </span>
     entangled,
     <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.3">
      i.e.
     </span>
     , cannot be performed independently in parallel as in comparison-type QAs.
Compared to FireAct fine-tuning, CoA also achieves better performance on both bridge and comparison QAs, without requiring data distilled from closed source GPT-4.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS2.p2">
    <p class="ltx_p" id="S5.SS2.p2.1">
     As with mathematical reasoning, CoA agents also perform more efficient inference than Toolformer and FireAct agents when answering the HotpotQA questions.
We also find that CoA is more efficient (
     <span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">
      Time
     </span>
     column) than both CoT-FSP and CoT-FT, as CoA does not require few-shot examples as additional inputs and does not need to generate long Wiki articles, which are instead provided by the Wikipedia search engine.
Finally, CoA improves over the baseline methods in zero-shot generalization experiments on other Wiki QA datasets, outperforming all baselines on NaturalQuestions and TriviaQA, and matching the best baselines on WebQuestions.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    In this work, we propose to decouple the general reasoning ability of LLM agents from executing specialized knowledge via external tools.
Our method, chain-of-abstraction (CoA), encourages LLMs to learn the planning of abstract multi-step reasoning, which are more robust to out-of-distribution knowledge shifts.
CoA also achieves a more efficient pipeline for tool usage that significantly improves the speed of tool-augmented multi-step reasoning.
The simple, yet effective, implementations of our method on two diverse tasks (
    <span class="ltx_text ltx_font_italic" id="S6.p1.1.1">
     i.e.
    </span>
    , mathematical reasoning and open-domain QA) demonstrate its potential for being adapted to new reasoning scenarios.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Anil et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Palm 2 technical report.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">
      arXiv preprint arXiv:2305.10403
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Baeza-Yates et al. (1999)
    </span>
    <span class="ltx_bibblock">
     Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et al. 1999.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">
      Modern information retrieval
     </em>
     , volume 463.
    </span>
    <span class="ltx_bibblock">
     ACM press New York.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Berant et al. (2013)
    </span>
    <span class="ltx_bibblock">
     Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013.
    </span>
    <span class="ltx_bibblock">
     Semantic parsing on freebase from question-answer pairs.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">
      Proceedings of the 2013 conference on empirical methods in natural language processing
     </em>
     , pages 1533–1544.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cai et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. 2023.
    </span>
    <span class="ltx_bibblock">
     Large language models as tool makers.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      arXiv preprint arXiv:2305.17126
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. 2023.
    </span>
    <span class="ltx_bibblock">
     Fireact: Toward language agent fine-tuning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">
      arXiv preprint arXiv:2310.05915
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021.
    </span>
    <span class="ltx_bibblock">
     Evaluating large language models trained on code.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">
      arXiv preprint arXiv:2107.03374
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022.
    </span>
    <span class="ltx_bibblock">
     Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">
      arXiv preprint arXiv:2211.12588
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chung et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022.
    </span>
    <span class="ltx_bibblock">
     Scaling instruction-finetuned language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">
      arXiv preprint arXiv:2210.11416
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cobbe et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021.
    </span>
    <span class="ltx_bibblock">
     Training verifiers to solve math word problems.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">
      arXiv preprint arXiv:2110.14168
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gao et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.
    </span>
    <span class="ltx_bibblock">
     Pal: Program-aided language models.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      International Conference on Machine Learning
     </em>
     , pages 10764–10799. PMLR.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ge et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan, Shuyuan Xu, and Yongfeng Zhang. 2023.
    </span>
    <span class="ltx_bibblock">
     Openagi: When llm meets domain experts.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">
      arXiv preprint arXiv:2304.04370
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gou et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2023.
    </span>
    <span class="ltx_bibblock">
     Critic: Large language models can self-correct with tool-interactive critiquing.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      arXiv preprint arXiv:2305.11738
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hao et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023a.
    </span>
    <span class="ltx_bibblock">
     Reasoning with language model is planning with world model.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">
      arXiv preprint arXiv:2305.14992
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hao et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. 2023b.
    </span>
    <span class="ltx_bibblock">
     Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">
      arXiv preprint arXiv:2305.11554
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Metatool benchmark for large language models: Deciding whether to use tools and which to use.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      arXiv preprint arXiv:2310.03128
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jacovi et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Alon Jacovi, Avi Caciularu, Jonathan Herzig, Roee Aharoni, Bernd Bohnet, and Mor Geva. 2023.
    </span>
    <span class="ltx_bibblock">
     A comprehensive evaluation of tool-assisted generation strategies.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">
      Findings of the Association for Computational Linguistics: EMNLP 2023
     </em>
     , pages 13856–13878.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ji et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023.
    </span>
    <span class="ltx_bibblock">
     Survey of hallucination in natural language generation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">
      ACM Computing Surveys
     </em>
     , 55:1–38.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jin et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Qiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.09667" target="_blank" title="">
      Genegpt: Augmenting large language models with domain tools for improved access to biomedical information
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Joshi et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017.
    </span>
    <span class="ltx_bibblock">
     Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">
      Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
     </em>
     , pages 1601–1611.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Koncel-Kedziorski et al. (2016)
    </span>
    <span class="ltx_bibblock">
     Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016.
    </span>
    <span class="ltx_bibblock">
     Mawps: A math word problem repository.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies
     </em>
     , pages 1152–1157.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kwiatkowski et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019.
    </span>
    <span class="ltx_bibblock">
     Natural questions: a benchmark for question answering research.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">
      Transactions of the Association for Computational Linguistics
     </em>
     , 7:452–466.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yuliang Liu, Xiangru Tang, Zefan Cai, Junjie Lu, Yichi Zhang, Yanjun Shao, Zexuan Deng, Helan Hu, Zengxian Yang, Kaikai An, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Ml-bench: Large language models leverage open-source libraries for machine learning tasks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">
      arXiv preprint arXiv:2311.09835
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Loshchilov and Hutter (2018)
    </span>
    <span class="ltx_bibblock">
     Ilya Loshchilov and Frank Hutter. 2018.
    </span>
    <span class="ltx_bibblock">
     Decoupled weight decay regularization.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      International Conference on Learning Representations
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023.
    </span>
    <span class="ltx_bibblock">
     Chameleon: Plug-and-play compositional reasoning with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">
      arXiv preprint arXiv:2304.09842
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Maynez et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020.
    </span>
    <span class="ltx_bibblock">
     On faithfulness and factuality in abstractive summarization.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">
      Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics
     </em>
     , pages 1906–1919.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Miao et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020.
    </span>
    <span class="ltx_bibblock">
     A diverse corpus for evaluating and developing english math word problem solvers.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">
      Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics
     </em>
     , pages 975–984.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     OpenAI (2023)
    </span>
    <span class="ltx_bibblock">
     OpenAI. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2303.08774" target="_blank" title="">
      Gpt-4 technical report
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Parisi et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022.
    </span>
    <span class="ltx_bibblock">
     Talm: Tool augmented language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">
      arXiv preprint arXiv:2205.12255
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Patel et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021.
    </span>
    <span class="ltx_bibblock">
     Are nlp models really able to solve simple math word problems?
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">
      Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
     </em>
     , pages 2080–2094.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Patil et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. 2023.
    </span>
    <span class="ltx_bibblock">
     Gorilla: Large language model connected with massive apis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">
      arXiv preprint arXiv:2305.15334
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Petroni et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. 2021.
    </span>
    <span class="ltx_bibblock">
     Kilt: a benchmark for knowledge intensive language tasks.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">
      Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
     </em>
     , pages 2523–2544.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Reimers and Gurevych (2019)
    </span>
    <span class="ltx_bibblock">
     Nils Reimers and Iryna Gurevych. 2019.
    </span>
    <span class="ltx_bibblock">
     Sentence-bert: Sentence embeddings using siamese bert-networks.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">
      Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)
     </em>
     , pages 3982–3992.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Robertson et al. (1995)
    </span>
    <span class="ltx_bibblock">
     Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. 1995.
    </span>
    <span class="ltx_bibblock">
     Okapi at trec-3.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">
      Nist Special Publication Sp
     </em>
     , 109:109.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schick et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.
    </span>
    <span class="ltx_bibblock">
     Toolformer: Language models can teach themselves to use tools.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">
      arXiv preprint arXiv:2302.04761
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shen et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023.
    </span>
    <span class="ltx_bibblock">
     Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">
      arXiv preprint arXiv:2303.17580
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Touvron et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a.
    </span>
    <span class="ltx_bibblock">
     Llama: Open and efficient foundation language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">
      arXiv preprint arXiv:2302.13971
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Touvron et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b.
    </span>
    <span class="ltx_bibblock">
     Llama 2: Open foundation and fine-tuned chat models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">
      arXiv preprint arXiv:2307.09288
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022.
    </span>
    <span class="ltx_bibblock">
     Self-consistency improves chain of thought reasoning in language models.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">
      The Eleventh International Conference on Learning Representations
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021.
    </span>
    <span class="ltx_bibblock">
     Finetuned language models are zero-shot learners.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">
      arXiv preprint arXiv:2109.01652
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022.
    </span>
    <span class="ltx_bibblock">
     Chain-of-thought prompting elicits reasoning in large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 35:24824–24837.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wenzek et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Édouard Grave. 2020.
    </span>
    <span class="ltx_bibblock">
     Ccnet: Extracting high quality monolingual datasets from web crawl data.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">
      Proceedings of the Twelfth Language Resources and Evaluation Conference
     </em>
     , pages 4003–4012.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. 2023.
    </span>
    <span class="ltx_bibblock">
     On the tool manipulation capability of open-source large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">
      arXiv preprint arXiv:2305.16504
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al. (2018)
    </span>
    <span class="ltx_bibblock">
     Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018.
    </span>
    <span class="ltx_bibblock">
     Hotpotqa: A dataset for diverse, explainable multi-hop question answering.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">
      Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing
     </em>
     . Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a.
    </span>
    <span class="ltx_bibblock">
     Tree of thoughts: Deliberate problem solving with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">
      arXiv preprint arXiv:2305.10601
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib45">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022.
    </span>
    <span class="ltx_bibblock">
     React: Synergizing reasoning and acting in language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">
      arXiv preprint arXiv:2210.03629
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib46">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023b.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2210.03629" target="_blank" title="">
      React: Synergizing reasoning and acting in language models
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib47">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhuang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan A Rossi, Somdeb Sarkhel, and Chao Zhang. 2023.
    </span>
    <span class="ltx_bibblock">
     Toolchain*: Efficient action space navigation in large language models with a* search.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">
      arXiv preprint arXiv:2310.13227
     </em>
     .
    </span>
   </li>
  </ul>
 </section>
 <section class="ltx_appendix" id="A1">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix A
   </span>
   Implementation Details
  </h2>
  <section class="ltx_paragraph" id="A1.SS0.SSS0.Px1">
   <h4 class="ltx_title ltx_title_paragraph">
    Evaluation Details
   </h4>
   <div class="ltx_para" id="A1.SS0.SSS0.Px1.p1">
    <p class="ltx_p" id="A1.SS0.SSS0.Px1.p1.1">
     For mathematical reasoning evaluation, we extract the last number appeared in each model’s answer, and check whether the number exactly match the gold reference. The accuracy is reported as the rate of such exact match across all QAs in a test set.
For Wiki QA evaluation, similar to mathematical reasoning, we extract the final answer of each model and calculate its exact match rate to the gold reference.
Specifically, the final answer is supposed to be the words after “Action: finish[” for FireAct baseline, and words after “The answer is ” for other models.
Our 8-shot in-domain examples used for the CoT-FSP baseline are shown in Table
     <a class="ltx_ref" href="#A3.T13" title="Table 13 ‣ Appendix C Fine-Tuning Data Re-writing Details ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
      <span class="ltx_text ltx_ref_tag">
       13
      </span>
     </a>
     and
     <a class="ltx_ref" href="#A3.T14" title="Table 14 ‣ Appendix C Fine-Tuning Data Re-writing Details ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
      <span class="ltx_text ltx_ref_tag">
       14
      </span>
     </a>
     , which enables the model to provide answer with our required format for evaluation,
     <span class="ltx_text ltx_font_italic" id="A1.SS0.SSS0.Px1.p1.1.1">
      i.e.
     </span>
     , stating its final answer after “The answer is ”.
Our human evaluation on GSM8K is conducted by 5 internal domain experts from our research group.
For each math question, we provide the experts with the gold answer as reference, and ask them to evaluate each model answer in anonymous manner,
     <span class="ltx_text ltx_font_italic" id="A1.SS0.SSS0.Px1.p1.1.2">
      i.e.
     </span>
     , experts do not know which model each answer comes from.
Two yes-or-no questions are asked for evaluating each model answer, including: a) whether the answer has any arithmetic error, and b) whether the answer has any reasoning error, and binary choices from the experts are collected to calculate the error rates of each model’s generation.
We present our detailed instructions for human evaluation in Figure
     <a class="ltx_ref" href="#A3.F5" title="Figure 5 ‣ Appendix C Fine-Tuning Data Re-writing Details ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     .
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="A1.SS0.SSS0.Px2">
   <h4 class="ltx_title ltx_title_paragraph">
    Model Training
   </h4>
   <div class="ltx_para" id="A1.SS0.SSS0.Px2.p1">
    <p class="ltx_p" id="A1.SS0.SSS0.Px2.p1.19">
     We fine-tune our models with batch size
     <math alttext="8" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px2.p1.1.m1.1">
      <semantics id="A1.SS0.SSS0.Px2.p1.1.m1.1a">
       <mn id="A1.SS0.SSS0.Px2.p1.1.m1.1.1" xref="A1.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">
        8
       </mn>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.1.m1.1b">
        <cn id="A1.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS0.SSS0.Px2.p1.1.m1.1.1">
         8
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.1.m1.1c">
        8
       </annotation>
      </semantics>
     </math>
     and learning rate
     <math alttext="2e^{-5}" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px2.p1.2.m2.1">
      <semantics id="A1.SS0.SSS0.Px2.p1.2.m2.1a">
       <mrow id="A1.SS0.SSS0.Px2.p1.2.m2.1.1" xref="A1.SS0.SSS0.Px2.p1.2.m2.1.1.cmml">
        <mn id="A1.SS0.SSS0.Px2.p1.2.m2.1.1.2" xref="A1.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml">
         2
        </mn>
        <mo id="A1.SS0.SSS0.Px2.p1.2.m2.1.1.1" lspace="0em" rspace="0em" xref="A1.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml">
         ​
        </mo>
        <msup id="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3" xref="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml">
         <mi id="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3.2" xref="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3.2.cmml">
          e
         </mi>
         <mrow id="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3.3" xref="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3.3.cmml">
          <mo id="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3.3a" xref="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3.3.cmml">
           −
          </mo>
          <mn id="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3.3.2" xref="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3.3.2.cmml">
           5
          </mn>
         </mrow>
        </msup>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.2.m2.1b">
        <apply id="A1.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="A1.SS0.SSS0.Px2.p1.2.m2.1.1">
         <times id="A1.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="A1.SS0.SSS0.Px2.p1.2.m2.1.1.1">
         </times>
         <cn id="A1.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml" type="integer" xref="A1.SS0.SSS0.Px2.p1.2.m2.1.1.2">
          2
         </cn>
         <apply id="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3">
          <csymbol cd="ambiguous" id="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3.1.cmml" xref="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3">
           superscript
          </csymbol>
          <ci id="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3.2.cmml" xref="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3.2">
           𝑒
          </ci>
          <apply id="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3.3.cmml" xref="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3.3">
           <minus id="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3.3.1.cmml" xref="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3.3">
           </minus>
           <cn id="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3.3.2.cmml" type="integer" xref="A1.SS0.SSS0.Px2.p1.2.m2.1.1.3.3.2">
            5
           </cn>
          </apply>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.2.m2.1c">
        2e^{-5}
       </annotation>
      </semantics>
     </math>
     and
     <math alttext="1e^{-5}" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px2.p1.3.m3.1">
      <semantics id="A1.SS0.SSS0.Px2.p1.3.m3.1a">
       <mrow id="A1.SS0.SSS0.Px2.p1.3.m3.1.1" xref="A1.SS0.SSS0.Px2.p1.3.m3.1.1.cmml">
        <mn id="A1.SS0.SSS0.Px2.p1.3.m3.1.1.2" xref="A1.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml">
         1
        </mn>
        <mo id="A1.SS0.SSS0.Px2.p1.3.m3.1.1.1" lspace="0em" rspace="0em" xref="A1.SS0.SSS0.Px2.p1.3.m3.1.1.1.cmml">
         ​
        </mo>
        <msup id="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3" xref="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3.cmml">
         <mi id="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3.2" xref="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3.2.cmml">
          e
         </mi>
         <mrow id="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3.3" xref="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.cmml">
          <mo id="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3.3a" xref="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.cmml">
           −
          </mo>
          <mn id="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.2" xref="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.2.cmml">
           5
          </mn>
         </mrow>
        </msup>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.3.m3.1b">
        <apply id="A1.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="A1.SS0.SSS0.Px2.p1.3.m3.1.1">
         <times id="A1.SS0.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="A1.SS0.SSS0.Px2.p1.3.m3.1.1.1">
         </times>
         <cn id="A1.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml" type="integer" xref="A1.SS0.SSS0.Px2.p1.3.m3.1.1.2">
          1
         </cn>
         <apply id="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3">
          <csymbol cd="ambiguous" id="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3.1.cmml" xref="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3">
           superscript
          </csymbol>
          <ci id="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3.2.cmml" xref="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3.2">
           𝑒
          </ci>
          <apply id="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.cmml" xref="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3.3">
           <minus id="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.1.cmml" xref="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3.3">
           </minus>
           <cn id="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.2.cmml" type="integer" xref="A1.SS0.SSS0.Px2.p1.3.m3.1.1.3.3.2">
            5
           </cn>
          </apply>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.3.m3.1c">
        1e^{-5}
       </annotation>
      </semantics>
     </math>
     for 7B and 70B model sizes, respectively, using cosine learning rate scheduler with warm-up step
     <math alttext="10" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px2.p1.4.m4.1">
      <semantics id="A1.SS0.SSS0.Px2.p1.4.m4.1a">
       <mn id="A1.SS0.SSS0.Px2.p1.4.m4.1.1" xref="A1.SS0.SSS0.Px2.p1.4.m4.1.1.cmml">
        10
       </mn>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.4.m4.1b">
        <cn id="A1.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" type="integer" xref="A1.SS0.SSS0.Px2.p1.4.m4.1.1">
         10
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.4.m4.1c">
        10
       </annotation>
      </semantics>
     </math>
     .
We use AdamW
     <cite class="ltx_cite ltx_citemacro_citep">
      (Loshchilov and Hutter,
      <a class="ltx_ref" href="#bib.bib23" title="">
       2018
      </a>
      )
     </cite>
     optimizer for all our fine-tuning experiments, with
     <math alttext="\beta_{1}" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px2.p1.5.m5.1">
      <semantics id="A1.SS0.SSS0.Px2.p1.5.m5.1a">
       <msub id="A1.SS0.SSS0.Px2.p1.5.m5.1.1" xref="A1.SS0.SSS0.Px2.p1.5.m5.1.1.cmml">
        <mi id="A1.SS0.SSS0.Px2.p1.5.m5.1.1.2" xref="A1.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml">
         β
        </mi>
        <mn id="A1.SS0.SSS0.Px2.p1.5.m5.1.1.3" xref="A1.SS0.SSS0.Px2.p1.5.m5.1.1.3.cmml">
         1
        </mn>
       </msub>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.5.m5.1b">
        <apply id="A1.SS0.SSS0.Px2.p1.5.m5.1.1.cmml" xref="A1.SS0.SSS0.Px2.p1.5.m5.1.1">
         <csymbol cd="ambiguous" id="A1.SS0.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="A1.SS0.SSS0.Px2.p1.5.m5.1.1">
          subscript
         </csymbol>
         <ci id="A1.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="A1.SS0.SSS0.Px2.p1.5.m5.1.1.2">
          𝛽
         </ci>
         <cn id="A1.SS0.SSS0.Px2.p1.5.m5.1.1.3.cmml" type="integer" xref="A1.SS0.SSS0.Px2.p1.5.m5.1.1.3">
          1
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.5.m5.1c">
        \beta_{1}
       </annotation>
      </semantics>
     </math>
     ,
     <math alttext="\beta_{2}" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px2.p1.6.m6.1">
      <semantics id="A1.SS0.SSS0.Px2.p1.6.m6.1a">
       <msub id="A1.SS0.SSS0.Px2.p1.6.m6.1.1" xref="A1.SS0.SSS0.Px2.p1.6.m6.1.1.cmml">
        <mi id="A1.SS0.SSS0.Px2.p1.6.m6.1.1.2" xref="A1.SS0.SSS0.Px2.p1.6.m6.1.1.2.cmml">
         β
        </mi>
        <mn id="A1.SS0.SSS0.Px2.p1.6.m6.1.1.3" xref="A1.SS0.SSS0.Px2.p1.6.m6.1.1.3.cmml">
         2
        </mn>
       </msub>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.6.m6.1b">
        <apply id="A1.SS0.SSS0.Px2.p1.6.m6.1.1.cmml" xref="A1.SS0.SSS0.Px2.p1.6.m6.1.1">
         <csymbol cd="ambiguous" id="A1.SS0.SSS0.Px2.p1.6.m6.1.1.1.cmml" xref="A1.SS0.SSS0.Px2.p1.6.m6.1.1">
          subscript
         </csymbol>
         <ci id="A1.SS0.SSS0.Px2.p1.6.m6.1.1.2.cmml" xref="A1.SS0.SSS0.Px2.p1.6.m6.1.1.2">
          𝛽
         </ci>
         <cn id="A1.SS0.SSS0.Px2.p1.6.m6.1.1.3.cmml" type="integer" xref="A1.SS0.SSS0.Px2.p1.6.m6.1.1.3">
          2
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.6.m6.1c">
        \beta_{2}
       </annotation>
      </semantics>
     </math>
     and
     <math alttext="\epsilon" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px2.p1.7.m7.1">
      <semantics id="A1.SS0.SSS0.Px2.p1.7.m7.1a">
       <mi id="A1.SS0.SSS0.Px2.p1.7.m7.1.1" xref="A1.SS0.SSS0.Px2.p1.7.m7.1.1.cmml">
        ϵ
       </mi>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.7.m7.1b">
        <ci id="A1.SS0.SSS0.Px2.p1.7.m7.1.1.cmml" xref="A1.SS0.SSS0.Px2.p1.7.m7.1.1">
         italic-ϵ
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.7.m7.1c">
        \epsilon
       </annotation>
      </semantics>
     </math>
     set to
     <math alttext="0.9" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px2.p1.8.m8.1">
      <semantics id="A1.SS0.SSS0.Px2.p1.8.m8.1a">
       <mn id="A1.SS0.SSS0.Px2.p1.8.m8.1.1" xref="A1.SS0.SSS0.Px2.p1.8.m8.1.1.cmml">
        0.9
       </mn>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.8.m8.1b">
        <cn id="A1.SS0.SSS0.Px2.p1.8.m8.1.1.cmml" type="float" xref="A1.SS0.SSS0.Px2.p1.8.m8.1.1">
         0.9
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.8.m8.1c">
        0.9
       </annotation>
      </semantics>
     </math>
     ,
     <math alttext="0.95" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px2.p1.9.m9.1">
      <semantics id="A1.SS0.SSS0.Px2.p1.9.m9.1a">
       <mn id="A1.SS0.SSS0.Px2.p1.9.m9.1.1" xref="A1.SS0.SSS0.Px2.p1.9.m9.1.1.cmml">
        0.95
       </mn>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.9.m9.1b">
        <cn id="A1.SS0.SSS0.Px2.p1.9.m9.1.1.cmml" type="float" xref="A1.SS0.SSS0.Px2.p1.9.m9.1.1">
         0.95
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.9.m9.1c">
        0.95
       </annotation>
      </semantics>
     </math>
     and
     <math alttext="1e^{-8}" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px2.p1.10.m10.1">
      <semantics id="A1.SS0.SSS0.Px2.p1.10.m10.1a">
       <mrow id="A1.SS0.SSS0.Px2.p1.10.m10.1.1" xref="A1.SS0.SSS0.Px2.p1.10.m10.1.1.cmml">
        <mn id="A1.SS0.SSS0.Px2.p1.10.m10.1.1.2" xref="A1.SS0.SSS0.Px2.p1.10.m10.1.1.2.cmml">
         1
        </mn>
        <mo id="A1.SS0.SSS0.Px2.p1.10.m10.1.1.1" lspace="0em" rspace="0em" xref="A1.SS0.SSS0.Px2.p1.10.m10.1.1.1.cmml">
         ​
        </mo>
        <msup id="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3" xref="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3.cmml">
         <mi id="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3.2" xref="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3.2.cmml">
          e
         </mi>
         <mrow id="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3.3" xref="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3.3.cmml">
          <mo id="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3.3a" xref="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3.3.cmml">
           −
          </mo>
          <mn id="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3.3.2" xref="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3.3.2.cmml">
           8
          </mn>
         </mrow>
        </msup>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.10.m10.1b">
        <apply id="A1.SS0.SSS0.Px2.p1.10.m10.1.1.cmml" xref="A1.SS0.SSS0.Px2.p1.10.m10.1.1">
         <times id="A1.SS0.SSS0.Px2.p1.10.m10.1.1.1.cmml" xref="A1.SS0.SSS0.Px2.p1.10.m10.1.1.1">
         </times>
         <cn id="A1.SS0.SSS0.Px2.p1.10.m10.1.1.2.cmml" type="integer" xref="A1.SS0.SSS0.Px2.p1.10.m10.1.1.2">
          1
         </cn>
         <apply id="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3.cmml" xref="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3">
          <csymbol cd="ambiguous" id="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3.1.cmml" xref="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3">
           superscript
          </csymbol>
          <ci id="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3.2.cmml" xref="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3.2">
           𝑒
          </ci>
          <apply id="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3.3.cmml" xref="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3.3">
           <minus id="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3.3.1.cmml" xref="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3.3">
           </minus>
           <cn id="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3.3.2.cmml" type="integer" xref="A1.SS0.SSS0.Px2.p1.10.m10.1.1.3.3.2">
            8
           </cn>
          </apply>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.10.m10.1c">
        1e^{-8}
       </annotation>
      </semantics>
     </math>
     , respectively.
Training weight decay is set to
     <math alttext="0.1" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px2.p1.11.m11.1">
      <semantics id="A1.SS0.SSS0.Px2.p1.11.m11.1a">
       <mn id="A1.SS0.SSS0.Px2.p1.11.m11.1.1" xref="A1.SS0.SSS0.Px2.p1.11.m11.1.1.cmml">
        0.1
       </mn>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.11.m11.1b">
        <cn id="A1.SS0.SSS0.Px2.p1.11.m11.1.1.cmml" type="float" xref="A1.SS0.SSS0.Px2.p1.11.m11.1.1">
         0.1
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.11.m11.1c">
        0.1
       </annotation>
      </semantics>
     </math>
     .
For mathematical reasoning, we use a total of
     <math alttext="400" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px2.p1.12.m12.1">
      <semantics id="A1.SS0.SSS0.Px2.p1.12.m12.1a">
       <mn id="A1.SS0.SSS0.Px2.p1.12.m12.1.1" xref="A1.SS0.SSS0.Px2.p1.12.m12.1.1.cmml">
        400
       </mn>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.12.m12.1b">
        <cn id="A1.SS0.SSS0.Px2.p1.12.m12.1.1.cmml" type="integer" xref="A1.SS0.SSS0.Px2.p1.12.m12.1.1">
         400
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.12.m12.1c">
        400
       </annotation>
      </semantics>
     </math>
     training steps, and get the best model checkpoints (with highest validation scores) at step
     <math alttext="240" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px2.p1.13.m13.1">
      <semantics id="A1.SS0.SSS0.Px2.p1.13.m13.1a">
       <mn id="A1.SS0.SSS0.Px2.p1.13.m13.1.1" xref="A1.SS0.SSS0.Px2.p1.13.m13.1.1.cmml">
        240
       </mn>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.13.m13.1b">
        <cn id="A1.SS0.SSS0.Px2.p1.13.m13.1.1.cmml" type="integer" xref="A1.SS0.SSS0.Px2.p1.13.m13.1.1">
         240
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.13.m13.1c">
        240
       </annotation>
      </semantics>
     </math>
     and
     <math alttext="200" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px2.p1.14.m14.1">
      <semantics id="A1.SS0.SSS0.Px2.p1.14.m14.1a">
       <mn id="A1.SS0.SSS0.Px2.p1.14.m14.1.1" xref="A1.SS0.SSS0.Px2.p1.14.m14.1.1.cmml">
        200
       </mn>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.14.m14.1b">
        <cn id="A1.SS0.SSS0.Px2.p1.14.m14.1.1.cmml" type="integer" xref="A1.SS0.SSS0.Px2.p1.14.m14.1.1">
         200
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.14.m14.1c">
        200
       </annotation>
      </semantics>
     </math>
     for 7B and 70B model sizes.
For Wiki QA domain, we adjust the total training steps to
     <math alttext="500" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px2.p1.15.m15.1">
      <semantics id="A1.SS0.SSS0.Px2.p1.15.m15.1a">
       <mn id="A1.SS0.SSS0.Px2.p1.15.m15.1.1" xref="A1.SS0.SSS0.Px2.p1.15.m15.1.1.cmml">
        500
       </mn>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.15.m15.1b">
        <cn id="A1.SS0.SSS0.Px2.p1.15.m15.1.1.cmml" type="integer" xref="A1.SS0.SSS0.Px2.p1.15.m15.1.1">
         500
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.15.m15.1c">
        500
       </annotation>
      </semantics>
     </math>
     , and get the best checkpoints at step
     <math alttext="450" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px2.p1.16.m16.1">
      <semantics id="A1.SS0.SSS0.Px2.p1.16.m16.1a">
       <mn id="A1.SS0.SSS0.Px2.p1.16.m16.1.1" xref="A1.SS0.SSS0.Px2.p1.16.m16.1.1.cmml">
        450
       </mn>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.16.m16.1b">
        <cn id="A1.SS0.SSS0.Px2.p1.16.m16.1.1.cmml" type="integer" xref="A1.SS0.SSS0.Px2.p1.16.m16.1.1">
         450
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.16.m16.1c">
        450
       </annotation>
      </semantics>
     </math>
     and
     <math alttext="300" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px2.p1.17.m17.1">
      <semantics id="A1.SS0.SSS0.Px2.p1.17.m17.1a">
       <mn id="A1.SS0.SSS0.Px2.p1.17.m17.1.1" xref="A1.SS0.SSS0.Px2.p1.17.m17.1.1.cmml">
        300
       </mn>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.17.m17.1b">
        <cn id="A1.SS0.SSS0.Px2.p1.17.m17.1.1.cmml" type="integer" xref="A1.SS0.SSS0.Px2.p1.17.m17.1.1">
         300
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.17.m17.1c">
        300
       </annotation>
      </semantics>
     </math>
     for 7B and 70B models.
Therefore, only
     <math alttext="\sim" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px2.p1.18.m18.1">
      <semantics id="A1.SS0.SSS0.Px2.p1.18.m18.1a">
       <mo id="A1.SS0.SSS0.Px2.p1.18.m18.1.1" xref="A1.SS0.SSS0.Px2.p1.18.m18.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.18.m18.1b">
        <csymbol cd="latexml" id="A1.SS0.SSS0.Px2.p1.18.m18.1.1.cmml" xref="A1.SS0.SSS0.Px2.p1.18.m18.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.18.m18.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     2K and
     <math alttext="\sim" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px2.p1.19.m19.1">
      <semantics id="A1.SS0.SSS0.Px2.p1.19.m19.1a">
       <mo id="A1.SS0.SSS0.Px2.p1.19.m19.1.1" xref="A1.SS0.SSS0.Px2.p1.19.m19.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px2.p1.19.m19.1b">
        <csymbol cd="latexml" id="A1.SS0.SSS0.Px2.p1.19.m19.1.1.cmml" xref="A1.SS0.SSS0.Px2.p1.19.m19.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px2.p1.19.m19.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     3K QAs are required in practice for fine-tuning our models in math and Wiki QA domains.
The training of our 7B and 70B models is based on 8 and 64 NVIDIA A100-SXM4 (80GB) GPUs, respectively.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_appendix" id="A2">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix B
   </span>
   Full Experimental Results
  </h2>
  <div class="ltx_para" id="A2.p1">
   <p class="ltx_p" id="A2.p1.1">
    Table
    <a class="ltx_ref" href="#A2.T9" title="Table 9 ‣ Self-Consistency Decoding ‣ Appendix B Full Experimental Results ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
     <span class="ltx_text ltx_ref_tag">
      9
     </span>
    </a>
    and
    <a class="ltx_ref" href="#A2.T10" title="Table 10 ‣ Self-Consistency Decoding ‣ Appendix B Full Experimental Results ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
     <span class="ltx_text ltx_ref_tag">
      10
     </span>
    </a>
    show the full results of our experiments on math and Wiki QA domains.
Our method of CoA achieves consistent improvements over baselines across various LLaMa model versions (LLaMa, LLaMa-2 and LLaMa-2-Chat), model sizes (7B and 70B), and domain benchmarks.
This shows great potential of our method being generalized to new model backbones and reasoning tasks.
   </p>
  </div>
  <section class="ltx_paragraph" id="A2.SS0.SSS0.Px1">
   <h4 class="ltx_title ltx_title_paragraph">
    Fine-Tuning Data Balance
   </h4>
   <div class="ltx_para" id="A2.SS0.SSS0.Px1.p1">
    <p class="ltx_p" id="A2.SS0.SSS0.Px1.p1.1">
     In the mathematical reasoning domain, we also validate the importance of using fine-tuning data that is balanced across different reasoning steps.
Specifically, we conduct an ablation study on CoT-FT and CoA seeded with LLaMa-2-Chat-7B model, by removing the single-step QA samples of ASDiv from the fine-tuning data (
     <span class="ltx_text ltx_font_bold" id="A2.SS0.SSS0.Px1.p1.1.1">
      no ASDiv
     </span>
     ).
We find that CoT-FT (no ASDiv) and CoA (no ASDiv) turn out to be biased towards multi-step reasoning, where they achieve better performance on GSM8K and MultiArith that contain mainly multi-step QAs, but suffer from severe performance degradation on other datasets that contain many single-step math problems.
This demonstrates that maintaining a good balance of single-step and multi-step reasoning data is important for adapting LLMs to be robust reasoners.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="A2.SS0.SSS0.Px2">
   <h4 class="ltx_title ltx_title_paragraph">
    Self-Consistency Decoding
   </h4>
   <div class="ltx_para" id="A2.SS0.SSS0.Px2.p1">
    <p class="ltx_p" id="A2.SS0.SSS0.Px2.p1.1">
     Besides of greedy decoding, we also test more advanced inference strategy,
     <span class="ltx_text ltx_font_italic" id="A2.SS0.SSS0.Px2.p1.1.1">
      i.e.
     </span>
     , self-consistency
     <cite class="ltx_cite ltx_citemacro_citep">
      (Wang et al.,
      <a class="ltx_ref" href="#bib.bib38" title="">
       2022
      </a>
      )
     </cite>
     decoding, on our CoA reasoning method, compared to chain-of-thought baselines CoT-FSP and CoT-FT, and tool-augmented baselines Toolformer and Toolformer - Math.
We test all methods on the GSM8K dataset seeded with LLaMa-2-Chat-7B.
Each method samples 16 reasoning chains and uses majority voting to aggregate the 16 answers derived by the reasoning chains, to get the final answer.
For the hyperparameters of sampling, we set the temperature, top-k and top-p as 1.0, 40 and 0.5, respectively.
Table
     <a class="ltx_ref" href="#A2.T8" title="Table 8 ‣ Self-Consistency Decoding ‣ Appendix B Full Experimental Results ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
      <span class="ltx_text ltx_ref_tag">
       8
      </span>
     </a>
     shows our evaluation results.
We find that our CoA method consistently outperforms all baseline methods when shifting from greedy decoding to self-consistency decoding.
This shows that our method also has better potential to be generalized to different LLM decoding schemes.
    </p>
   </div>
   <figure class="ltx_table" id="A2.T8">
    <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T8.1">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="A2.T8.1.1.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A2.T8.1.1.1.1">
        <span class="ltx_text ltx_font_bold" id="A2.T8.1.1.1.1.1">
         Method
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T8.1.1.1.2">
        <span class="ltx_text ltx_font_bold" id="A2.T8.1.1.1.2.1">
         Accuracy
        </span>
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="A2.T8.1.2.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T8.1.2.1.1">
        CoT-FSP
       </th>
       <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.2.1.2">
        27.90
       </td>
      </tr>
      <tr class="ltx_tr" id="A2.T8.1.3.2">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.1.3.2.1">
        CoT-FT
       </th>
       <td class="ltx_td ltx_align_center" id="A2.T8.1.3.2.2">
        39.12
       </td>
      </tr>
      <tr class="ltx_tr" id="A2.T8.1.4.3">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.1.4.3.1">
        Toolformer
       </th>
       <td class="ltx_td ltx_align_center" id="A2.T8.1.4.3.2">
        24.56
       </td>
      </tr>
      <tr class="ltx_tr" id="A2.T8.1.5.4">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T8.1.5.4.1">
        Toolformer - Math
       </th>
       <td class="ltx_td ltx_align_center" id="A2.T8.1.5.4.2">
        35.25
       </td>
      </tr>
      <tr class="ltx_tr" id="A2.T8.1.6.5">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="A2.T8.1.6.5.1">
        CoA
       </th>
       <td class="ltx_td ltx_align_center ltx_border_b" id="A2.T8.1.6.5.2">
        <span class="ltx_text ltx_font_bold" id="A2.T8.1.6.5.2.1">
         40.79
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 8:
     </span>
     Evaluation results on GSM8K with self-consistency decoding. Each model samples 16 reasoning chains with temperature 1.0, top-k 40 and top-p 0.5. All models are developed based on LLaMa-2-Chat-7B.
    </figcaption>
   </figure>
   <figure class="ltx_table" id="A2.T9">
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T9.1" style="width:433.6pt;height:318.5pt;vertical-align:-316.3pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-73.8pt,0.4pt) scale(0.746064952973922,0.746064952973922) ;">
      <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T9.1.1">
       <tbody class="ltx_tbody">
        <tr class="ltx_tr" id="A2.T9.1.1.1.1">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T9.1.1.1.1.1" rowspan="2">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.1.1.1.1">
           Model
          </span>
         </th>
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T9.1.1.1.1.2" rowspan="2">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.1.1.2.1">
           Method
          </span>
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3" rowspan="2">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.1.1.3.1">
           GSM8K
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.4" rowspan="2">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.1.1.4.1">
           ASDiv
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.5" rowspan="2">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.1.1.5.1">
           SVAMP
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" colspan="5" id="A2.T9.1.1.1.1.6">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.1.1.6.1">
           MAWPS
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.2.2">
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.2.2.1">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.2.2.1.1">
           AddSub
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.2.2.2">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.2.2.2.1">
           SingleEQ
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.2.2.3">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.2.2.3.1">
           SingleOp
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.2.2.4">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.2.2.4.1">
           MultiArith
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.2.2.5">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.2.2.5.1">
           All
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.3.3">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T9.1.1.3.3.1" rowspan="3">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.3.3.1.1">
           LLaMa-7B
          </span>
         </th>
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T9.1.1.3.3.2">
          CoT-FSP
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.3.3.3">
          11.90
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.3.3.4">
          44.69
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.3.3.5">
          31.80
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.3.3.6">
          56.20
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.3.3.7">
          59.65
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.3.3.8">
          70.28
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.3.3.9">
          43.00
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.3.3.10">
          57.05
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.4.4">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.1.1.4.4.1">
          CoT-FT
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.4.4.2">
          30.71
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.4.4.3">
          53.19
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.4.4.4">
          42.30
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.4.4.5">
          55.70
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.4.4.6">
          69.09
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.4.4.7">
          77.05
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.4.4.8">
          54.17
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.4.4.9">
          64.36
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.5.5">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.1.1.5.5.1">
          CoA
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.5.5.2">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.5.5.2.1">
           35.71
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.5.5.3">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.5.5.3.1">
           56.36
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.5.5.4">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.5.5.4.1">
           51.10
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.5.5.5">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.5.5.5.1">
           67.59
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.5.5.6">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.5.5.6.1">
           80.51
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.5.5.7">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.5.5.7.1">
           85.94
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.5.5.8">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.5.5.8.1">
           68.33
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.5.5.9">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.5.5.9.1">
           75.98
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.6.6">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T9.1.1.6.6.1" rowspan="4">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.6.6.1.1">
           LLaMa-2-7B
          </span>
         </th>
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T9.1.1.6.6.2">
          CoT-FSP
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.6.6.3">
          16.38
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.6.6.4">
          47.85
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.6.6.5">
          38.40
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.6.6.6">
          52.41
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.6.6.7">
          63.39
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.6.6.8">
          82.03
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.6.6.9">
          43.33
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.6.6.10">
          60.53
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.7.7">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.1.1.7.7.1">
          CoT-FT
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.7.7.2">
          35.33
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.7.7.3">
          57.18
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.7.7.4">
          48.20
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.7.7.5">
          66.08
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.7.7.6">
          74.41
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.7.7.7">
          85.23
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.7.7.8">
          65.00
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.7.7.9">
          73.03
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.8.8">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.1.1.8.8.1">
          Toolformer
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.8.8.2">
          17.59
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.8.8.3">
          48.55
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.8.8.4">
          37.10
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.8.8.5">
          47.34
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.8.8.6">
          58.46
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.8.8.7">
          79.54
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.8.8.8">
          50.67
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.8.8.9">
          59.81
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.9.9">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.1.1.9.9.1">
          CoA
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.9.9.2">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.9.9.2.1">
           37.83
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.9.9.3">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.9.9.3.1">
           57.61
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.9.9.4">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.9.9.4.1">
           51.70
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.9.9.5">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.9.9.5.1">
           72.15
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.9.9.6">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.9.9.6.1">
           82.48
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.9.9.7">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.9.9.7.1">
           86.48
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.9.9.8">
          73.17
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.9.9.9">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.9.9.9.1">
           78.89
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.10.10">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T9.1.1.10.10.1" rowspan="8">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.10.10.1.1">
           LLaMa-2-Chat-7B
          </span>
         </th>
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T9.1.1.10.10.2">
          CoT-FSP
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.10.10.3">
          24.03
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.10.10.4">
          54.14
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.10.10.5">
          51.30
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.10.10.6">
          71.90
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.10.10.7">
          72.44
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.10.10.8">
          85.41
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.10.10.9">
          74.00
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.10.10.10">
          76.32
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.11.11">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.1.1.11.11.1">
          CoT-FT
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.11.11.2">
          35.41
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.11.11.3">
          59.00
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.11.11.4">
          46.90
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.11.11.5">
          58.23
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.11.11.6">
          72.24
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.11.11.7">
          85.41
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.11.11.8">
          73.00
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.11.11.9">
          73.37
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.12.12">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.1.1.12.12.1">
          CoT-FT (no ASDiv)
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.12.12.2">
          36.19
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.12.12.3">
          44.93
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.12.12.4">
          35.30
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.12.12.5">
          38.48
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.12.12.6">
          52.95
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.12.12.7">
          61.21
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.12.12.8">
          77.67
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.12.12.9">
          59.61
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.13.13">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.1.1.13.13.1">
          Toolformer
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.13.13.2">
          23.65
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.13.13.3">
          50.85
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.13.13.4">
          48.80
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.13.13.5">
          61.01
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.13.13.6">
          69.09
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.13.13.7">
          81.85
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.13.13.8">
          68.50
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.13.13.9">
          70.85
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.14.14">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.1.1.14.14.1">
          Toolformer - Math
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.14.14.2">
          36.01
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.14.14.3">
          59.18
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.14.14.4">
          47.60
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.14.14.5">
          58.99
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.14.14.6">
          72.44
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.14.14.7">
          85.94
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.14.14.8">
          75.50
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.14.14.9">
          74.43
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.15.15">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.1.1.15.15.1">
          CoA
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.15.15.2">
          38.29
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.15.15.3">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.15.15.3.1">
           59.57
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.15.15.4">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.15.15.4.1">
           54.20
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.15.15.5">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.15.15.5.1">
           72.41
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.15.15.6">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.15.15.6.1">
           81.89
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.15.15.7">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.15.15.7.1">
           88.26
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.15.15.8">
          83.00
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.15.15.9">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.15.15.9.1">
           82.13
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.16.16">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.1.1.16.16.1">
          CoA (no ASDiv)
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.16.16.2">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.16.16.2.1">
           39.73
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.16.16.3">
          54.19
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.16.16.4">
          44.40
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.16.16.5">
          54.18
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.16.16.6">
          73.62
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.16.16.7">
          73.49
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.16.16.8">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.16.16.8.1">
           85.33
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.16.16.9">
          73.27
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.17.17">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.1.1.17.17.1">
          CoA (no Tool)
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.17.17.2">
          35.03
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.17.17.3">
          58.79
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.17.17.4">
          51.50
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.17.17.5">
          68.10
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.17.17.6">
          74.21
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.17.17.7">
          86.48
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.17.17.8">
          77.67
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.17.17.9">
          77.38
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.18.18">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T9.1.1.18.18.1" rowspan="5">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.18.18.1.1">
           LLaMa-2-Chat-70B
          </span>
         </th>
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T9.1.1.18.18.2">
          CoT-FSP
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.18.18.3">
          56.18
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.18.18.4">
          65.94
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.18.18.5">
          70.60
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.18.18.6">
          86.08
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.18.18.7">
          89.17
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.18.18.8">
          92.88
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.18.18.9">
          84.50
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.18.18.10">
          88.23
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.19.19">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.1.1.19.19.1">
          CoT-FT
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.19.19.2">
          60.50
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.19.19.3">
          70.24
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.19.19.4">
          70.40
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.19.19.5">
          81.52
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.19.19.6">
          87.60
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.19.19.7">
          92.35
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.19.19.8">
          89.17
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.19.19.9">
          88.18
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.20.20">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.1.1.20.20.1">
          Toolformer
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.20.20.2">
          52.54
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.20.20.3">
          69.07
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.20.20.4">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.20.20.4.1">
           73.60
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.20.20.5">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.20.20.5.1">
           86.84
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.20.20.6">
          89.76
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.20.20.7">
          91.46
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.20.20.8">
          81.50
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.20.20.9">
          87.26
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.21.21">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.1.1.21.21.1">
          Toolformer - Math
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.21.21.2">
          61.03
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.21.21.3">
          70.59
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.21.21.4">
          73.20
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.21.21.5">
          85.57
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.21.21.6">
          91.34
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.21.21.7">
          91.99
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.21.21.8">
          92.00
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.21.21.9">
          90.60
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.22.22">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T9.1.1.22.22.1">
          CoA
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.22.22.2">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.22.22.2.1">
           62.32
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.22.22.3">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.22.22.3.1">
           71.89
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.22.22.4">
          73.40
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.22.22.5">
          86.33
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.22.22.6">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.22.22.6.1">
           94.49
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.22.22.7">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.22.22.7.1">
           93.06
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.22.22.8">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.22.22.8.1">
           92.33
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T9.1.1.22.22.9">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.22.22.9.1">
           91.91
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T9.1.1.23.23">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="A2.T9.1.1.23.23.1">
          <span class="ltx_text ltx_font_bold" id="A2.T9.1.1.23.23.1.1">
           GPT-J
          </span>
         </th>
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="A2.T9.1.1.23.23.2">
          Toolformer
         </th>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A2.T9.1.1.23.23.3">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A2.T9.1.1.23.23.4">
          40.4
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A2.T9.1.1.23.23.5">
          29.4
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A2.T9.1.1.23.23.6">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A2.T9.1.1.23.23.7">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A2.T9.1.1.23.23.8">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A2.T9.1.1.23.23.9">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A2.T9.1.1.23.23.10">
          44.0
         </td>
        </tr>
       </tbody>
      </table>
     </span>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 9:
     </span>
     Mathematical reasoning evaluation results.
    </figcaption>
   </figure>
   <figure class="ltx_table" id="A2.T10">
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T10.1" style="width:390.3pt;height:273.9pt;vertical-align:-271.5pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-44.9pt,0.3pt) scale(0.812842297157887,0.812842297157887) ;">
      <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T10.1.1">
       <tbody class="ltx_tbody">
        <tr class="ltx_tr" id="A2.T10.1.1.1.1">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T10.1.1.1.1.1" rowspan="2">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.1.1.1.1">
           Model
          </span>
         </th>
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T10.1.1.1.1.2" rowspan="2">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.1.1.2.1">
           Method
          </span>
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="A2.T10.1.1.1.1.3">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.1.1.3.1">
           HotpotQA
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.1.1.4" rowspan="2">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.1.1.4.1">
           WebQ.
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.1.1.5" rowspan="2">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.1.1.5.1">
           NaturalQ.
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.1.1.6" rowspan="2">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.1.1.6.1">
           TriviaQA
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T10.1.1.2.2">
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.2.2.1">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.2.2.1.1">
           Bridge
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.2.2.2">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.2.2.2.1">
           Comparison
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.2.2.3">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.2.2.3.1">
           All
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T10.1.1.3.3">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T10.1.1.3.3.1" rowspan="4">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.3.3.1.1">
           LLaMa-2-7B
          </span>
         </th>
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T10.1.1.3.3.2">
          CoT-FSP
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.3.3.3">
          14.43
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.3.3.4">
          45.26
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.3.3.5">
          20.62
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.3.3.6">
          33.96
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.3.3.7">
          33.35
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.3.3.8">
          56.95
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T10.1.1.4.4">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T10.1.1.4.4.1">
          CoT-FT
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.4.4.2">
          14.85
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.4.4.3">
          57.36
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.4.4.4">
          23.39
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.4.4.5">
          31.50
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.4.4.6">
          26.93
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.4.4.7">
          52.32
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T10.1.1.5.5">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T10.1.1.5.5.1">
          Toolformer
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.5.5.2">
          14.12
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.5.5.3">
          42.76
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.5.5.4">
          20.35
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.5.5.5">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.5.5.5.1">
           37.11
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.5.5.6">
          34.49
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.5.5.7">
          57.79
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T10.1.1.6.6">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T10.1.1.6.6.1">
          CoA
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.6.6.2">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.6.6.2.1">
           22.00
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.6.6.3">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.6.6.3.1">
           57.43
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.6.6.4">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.6.6.4.1">
           29.12
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.6.6.5">
          34.60
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.6.6.6">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.6.6.6.1">
           38.28
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.6.6.7">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.6.6.7.1">
           58.28
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T10.1.1.7.7">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T10.1.1.7.7.1" rowspan="6">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.7.7.1.1">
           LLaMa-2-Chat-7B
          </span>
         </th>
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T10.1.1.7.7.2">
          CoT-FSP
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.7.7.3">
          11.69
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.7.7.4">
          45.46
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.7.7.5">
          18.47
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.7.7.6">
          34.65
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.7.7.7">
          30.91
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.7.7.8">
          53.48
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T10.1.1.8.8">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T10.1.1.8.8.1">
          CoT-FT
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.8.8.2">
          14.24
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.8.8.3">
          56.69
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.8.8.4">
          22.77
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.8.8.5">
          33.51
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.8.8.6">
          25.40
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.8.8.7">
          51.05
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T10.1.1.9.9">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T10.1.1.9.9.1">
          Toolformer
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.9.9.2">
          12.99
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.9.9.3">
          44.59
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.9.9.4">
          20.00
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.9.9.5">
          36.22
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.9.9.6">
          30.22
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.9.9.7">
          54.15
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T10.1.1.10.10">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T10.1.1.10.10.1">
          Toolformer - Wiki
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.10.10.2">
          15.68
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.10.10.3">
          56.42
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.10.10.4">
          23.86
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.10.10.5">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.10.10.5.1">
           36.61
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.10.10.6">
          32.96
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.10.10.7">
          55.08
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T10.1.1.11.11">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T10.1.1.11.11.1">
          FireAct
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.11.11.2">
          19.18
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.11.11.3">
          54.14
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.11.11.4">
          26.20
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.11.11.5">
          36.02
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.11.11.6">
          35.87
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.11.11.7">
          52.96
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T10.1.1.12.12">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T10.1.1.12.12.1">
          CoA
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.12.12.2">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.12.12.2.1">
           21.00
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.12.12.3">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.12.12.3.1">
           56.96
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.12.12.4">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.12.12.4.1">
           28.22
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.12.12.5">
          35.97
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.12.12.6">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.12.12.6.1">
           38.67
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.12.12.7">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.12.12.7.1">
           57.90
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T10.1.1.13.13">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T10.1.1.13.13.1" rowspan="5">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.13.13.1.1">
           LLaMa-2-Chat-70B
          </span>
         </th>
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T10.1.1.13.13.2">
          CoT-FSP
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.13.13.3">
          21.39
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.13.13.4">
          56.62
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.13.13.5">
          28.47
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.13.13.6">
          34.89
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.13.13.7">
          37.42
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.13.13.8">
          63.61
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T10.1.1.14.14">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T10.1.1.14.14.1">
          CoT-FT
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.14.14.2">
          23.84
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.14.14.3">
          63.95
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.14.14.4">
          31.90
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.14.14.5">
          34.15
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.14.14.6">
          39.75
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.14.14.7">
          62.28
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T10.1.1.15.15">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T10.1.1.15.15.1">
          Toolformer
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.15.15.2">
          22.24
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.15.15.3">
          56.09
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.15.15.4">
          29.04
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.15.15.5">
          37.16
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.15.15.6">
          40.42
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.15.15.7">
          64.31
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T10.1.1.16.16">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T10.1.1.16.16.1">
          Toolformer - Wiki
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.16.16.2">
          26.38
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.16.16.3">
          63.82
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.16.16.4">
          33.90
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.16.16.5">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.16.16.5.1">
           37.70
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.16.16.6">
          41.25
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.16.16.7">
          66.64
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T10.1.1.17.17">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T10.1.1.17.17.1">
          CoA
         </th>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.17.17.2">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.17.17.2.1">
           27.61
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.17.17.3">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.17.17.3.1">
           64.09
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.17.17.4">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.17.17.4.1">
           34.94
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.17.17.5">
          36.37
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.17.17.6">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.17.17.6.1">
           43.57
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="A2.T10.1.1.17.17.7">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.17.17.7.1">
           69.08
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="A2.T10.1.1.18.18">
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="A2.T10.1.1.18.18.1">
          <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.18.18.1.1">
           GPT-J
          </span>
         </th>
         <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="A2.T10.1.1.18.18.2">
          Toolformer
         </th>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A2.T10.1.1.18.18.3">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A2.T10.1.1.18.18.4">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A2.T10.1.1.18.18.5">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A2.T10.1.1.18.18.6">
          26.3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A2.T10.1.1.18.18.7">
          17.7
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A2.T10.1.1.18.18.8">
          48.8
         </td>
        </tr>
       </tbody>
      </table>
     </span>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 10:
     </span>
     Wiki QA evaluation results.
    </figcaption>
   </figure>
  </section>
 </section>
 <section class="ltx_appendix" id="A3">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix C
   </span>
   Fine-Tuning Data Re-writing Details
  </h2>
  <div class="ltx_para" id="A3.p1">
   <p class="ltx_p" id="A3.p1.1">
    Table
    <a class="ltx_ref" href="#A3.T11" title="Table 11 ‣ Appendix C Fine-Tuning Data Re-writing Details ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
     <span class="ltx_text ltx_ref_tag">
      11
     </span>
    </a>
    and
    <a class="ltx_ref" href="#A3.T12" title="Table 12 ‣ Appendix C Fine-Tuning Data Re-writing Details ‣ Efficient Tool Use with Chain-of-Abstraction Reasoning">
     <span class="ltx_text ltx_ref_tag">
      12
     </span>
    </a>
    show the prompting examples for fine-tuning data construction of our method.
We simply prompt LLaMa-70B to re-write existing math and Wiki QAs as abstract reasoning chains, which gets rid of data distillation from close-sourced LLMs, yet obtains data resources that enable more effective learning of multi-step reasoning.
   </p>
  </div>
  <figure class="ltx_table" id="A3.T11">
   <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T11.1" style="width:433.6pt;height:675.4pt;vertical-align:-672.5pt;">
    <span class="ltx_transformed_inner" style="transform:translate(-9.9pt,0.1pt) scale(0.956454697348506,0.956454697348506) ;">
     <table class="ltx_tabular ltx_align_middle" id="A3.T11.1.1">
      <tbody class="ltx_tbody">
       <tr class="ltx_tr" id="A3.T11.1.1.1.1">
        <td class="ltx_td ltx_align_left ltx_border_tt" id="A3.T11.1.1.1.1.1">
         <span class="ltx_text ltx_font_bold" id="A3.T11.1.1.1.1.1.1">
          Q
         </span>
         : There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees will the grove
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.2.2">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.2.2.1">
         workers plant today?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.3.3">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.3.3.1">
         <span class="ltx_text ltx_font_bold" id="A3.T11.1.1.3.3.1.1">
          A
         </span>
         : There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21-15=6. The answer is 6.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.4.4">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.4.4.1">
         <span class="ltx_text ltx_font_bold" id="A3.T11.1.1.4.4.1.1">
          C
         </span>
         : There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been [21 - 15 = y1]. The answer is y1.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.5.5">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.5.5.1">
         <span class="ltx_text ltx_font_bold" id="A3.T11.1.1.5.5.1.1">
          Q
         </span>
         : The flowers cost $9, the clay pot costs $20 more than the flower, and the bag of soil costs $2 less than the flower. How much does it cost to plant the flowers?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.6.6">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.6.6.1">
         <span class="ltx_text ltx_font_bold" id="A3.T11.1.1.6.6.1.1">
          A
         </span>
         : The clay pot costs $20 + $9 = $29. The bag of soil costs $9 - $2 = $7. The cost to plant the flowers is $9 + $29 + $7 = $45. The answer is 45.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.7.7">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.7.7.1">
         <span class="ltx_text ltx_font_bold" id="A3.T11.1.1.7.7.1.1">
          C
         </span>
         : The clay pot costs [20 + 9 = y1]. The bag of soil costs [9 - 2 = y2]. The cost to plant the flowers is [9 + y1 + y2 = y3]. The answer is y3.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.8.8">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.8.8.1">
         <span class="ltx_text ltx_font_bold" id="A3.T11.1.1.8.8.1.1">
          Q
         </span>
         : From March to August, Sam made $460 doing 23 hours of yard work. However, from September to February, Sam was only able to work for 8 hours. If Sam
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.9.9">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.9.9.1">
         is saving up to buy a video game console that costs $600 and has already spent $340 to fix his car, how many more hours does he need to work before he can buy
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.10.10">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.10.10.1">
         the video game console?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.11.11">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.11.11.1">
         <span class="ltx_text ltx_font_bold" id="A3.T11.1.1.11.11.1.1">
          A
         </span>
         : Sam makes $460 / 23 hrs = $20/hr. From September to February, Sam made 8hrs x $20/hr = $160. From March to February, Sam made a total of $460 + $160
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.12.12">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.12.12.1">
         = $620. After fixing his car, he was left with $620 - $340 = $280. Sam needs another $600 - $280 = $320. Sam needs to work another $320 / $20/hr = 16 hours.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.13.13">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.13.13.1">
         The answer is 16.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.14.14">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.14.14.1">
         <span class="ltx_text ltx_font_bold" id="A3.T11.1.1.14.14.1.1">
          C
         </span>
         : Sam makes [460 / 23 = y1] dollars per hour. From September to February, Sam made [8 * y1 = y2] dollars. From March to February, Sam made a
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.15.15">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.15.15.1">
         total of [460 + y2 = y3] dollars. After fixing his car, he was left with [y3 - 340 = y4]. Sam needs another [600 - y4 = y5] dollars. Sam needs to work
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.16.16">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.16.16.1">
         another [y5 / y1 = y6] hours. The answer is y6.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.17.17">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.17.17.1">
         <span class="ltx_text ltx_font_bold" id="A3.T11.1.1.17.17.1.1">
          Q
         </span>
         : There were nine computers in the server room. Five more computers were installed each day, from Monday to Thursday. How many computers are now in
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.18.18">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.18.18.1">
         the server room?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.19.19">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.19.19.1">
         <span class="ltx_text ltx_font_bold" id="A3.T11.1.1.19.19.1.1">
          A
         </span>
         : There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.20.20">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.20.20.1">
         <span class="ltx_text ltx_font_bold" id="A3.T11.1.1.20.20.1.1">
          C
         </span>
         : There were originally 9 computers. For each of 4 days, 5 more computers were added. So [5 * 4 = y1] computers were added. [9 + y1 = y2].
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.21.21">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.21.21.1">
         The answer is y2.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.22.22">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.22.22.1">
         <span class="ltx_text ltx_font_bold" id="A3.T11.1.1.22.22.1.1">
          Q
         </span>
         : Of the 90 people on William’s bus, 3/5 were Dutch. Of the 1/2 of the Dutch who were also American, 1/3 got window seats. What’s the number of Dutch
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.23.23">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.23.23.1">
         Americans who sat at the windows?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.24.24">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.24.24.1">
         <span class="ltx_text ltx_font_bold" id="A3.T11.1.1.24.24.1.1">
          A
         </span>
         : On the bus, the number of Dutch people was 3/5 of the total number, a total of 3/5 x 90 = 54 people. Out of the 54 people who were Dutch, 1/2 were Dutch
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.25.25">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.25.25.1">
         Americans, a total of 1/2 x 54 = 27 people. If 1/3 of the passengers on the bus identifying as Dutch Americans sat at the windows, their number is 1/3 x 27 = 9.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.26.26">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.26.26.1">
         The answer is 9.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.27.27">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.27.27.1">
         <span class="ltx_text ltx_font_bold" id="A3.T11.1.1.27.27.1.1">
          C
         </span>
         : On the bus, the number of Dutch people was 3/5 of the total number, a total of [3/5 * 90 = y1] people. Out of the Dutch people, 1/2 were Dutch
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.28.28">
        <td class="ltx_td ltx_align_left" id="A3.T11.1.1.28.28.1">
         Americans, a total of [1/2 * y1 = y2] people. If 1/3 of the passengers on the bus identifying as Dutch Americans sat at the windows, their number
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T11.1.1.29.29">
        <td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T11.1.1.29.29.1">
         is [1/3 * y2 = y3]. The answer is y3.
        </td>
       </tr>
      </tbody>
     </table>
    </span>
   </div>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_table">
     Table 11:
    </span>
    Prompting examples for fine-tuning data construction in mathematical reasoning domain. Given a question (Q) and a gold answer (A), LLaMa-70B is prompted to generate the re-writing of answer as abstract reasoning chain (C). Based on that, our method trains a LLM to generate the abstract chain based on the question, and the final answer is derived by reify the chain of reasoning with the domain tool (
    <span class="ltx_text ltx_font_italic" id="A3.T11.3.1">
     i.e.
    </span>
    , equation solver).
   </figcaption>
  </figure>
  <figure class="ltx_table" id="A3.T12">
   <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T12.1" style="width:433.6pt;height:810.4pt;vertical-align:-807.5pt;">
    <span class="ltx_transformed_inner" style="transform:translate(-7.9pt,0.1pt) scale(0.964879015277243,0.964879015277243) ;">
     <table class="ltx_tabular ltx_align_middle" id="A3.T12.1.1">
      <tbody class="ltx_tbody">
       <tr class="ltx_tr" id="A3.T12.1.1.1.1">
        <td class="ltx_td ltx_align_left ltx_border_tt" id="A3.T12.1.1.1.1.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.1.1.1.1">
          Q
         </span>
         : Fritz von Brodowski was killed during what global war that lasted from 1939 to 1945?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.2.2">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.2.2.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.2.2.1.1">
          A
         </span>
         : The answer is World War II.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.3.3">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.3.3.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.3.3.1.1">
          W
         </span>
         : Fritz von Brodowski &gt; Friedrich Wilhelm Konrad von Brodowski was controversially killed while in French custody during World War II.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.4.4">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.4.4.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.4.4.1.1">
          C
         </span>
         : Find the [war in which Fritz von Brodowski was killed -Wiki-&gt; y1].
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.5.5">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.5.5.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.5.5.1.1">
          Q
         </span>
         : Which tennis player won more Grand Slam titles, Henri Leconte or Jonathan Stark?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.6.6">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.6.6.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.6.6.1.1">
          A
         </span>
         : The answer is Jonathan Stark.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.7.7">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.7.7.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.7.7.1.1">
          W
         </span>
         : Henri Leconte &gt; He won the French Open men’s doubles title in 1984. Jonathan Stark (tennis) &gt; During his career he won two Grand Slam doubles titles.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.8.8">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.8.8.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.8.8.1.1">
          C
         </span>
         : First identify the [number of Grand Slam titles Henri Leconte won -Wiki-&gt; y1]. Then find out the [number of Grand Slam titles Jonathan Stark won -Wiki-&gt; y2].
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.9.9">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.9.9.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.9.9.1.1">
          Q
         </span>
         : The director of the romantic comedy “Big Stone Gap” is based in what New York city?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.10.10">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.10.10.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.10.10.1.1">
          A
         </span>
         : The answer is Greenwich Village.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.11.11">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.11.11.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.11.11.1.1">
          W
         </span>
         : Big Stone Gap (film) &gt; Big Stone Gap is a 2014 American romantic comedy film directed by Adriana Trigiani. Adriana Trigiani &gt; Adriana Trigiani is an
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.12.12">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.12.12.1">
         Italian American film director based in Greenwich Village.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.13.13">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.13.13.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.13.13.1.1">
          C
         </span>
         : First search the [director of romantic comedy “Big Stone Gap” -Wiki-&gt; y1]. The name of this film’s director is [y1 -NER(person)-&gt; y2]. Then determine [y2 in
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.14.14">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.14.14.1">
         what New York city -Wiki-&gt; y3].
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.15.15">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.15.15.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.15.15.1.1">
          Q
         </span>
         : Are Randal Kleiser and Kyle Schickner of the same nationality?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.16.16">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.16.16.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.16.16.1.1">
          A
         </span>
         : The answer is yes.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.17.17">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.17.17.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.17.17.1.1">
          W
         </span>
         : Randal Kleiser &gt; John Randal Kleiser (born July 20, 1946) is an American film director and producer. Kyle Schickner &gt; Kyle Schickner is an American film
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.18.18">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.18.18.1">
         producer, writer, director, actor.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.19.19">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.19.19.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.19.19.1.1">
          C
         </span>
         : First find out the [nationality of Randal Kleiser -Wiki-&gt; y1]. Then figure out the [nationality of Kyle Schickner -Wiki-&gt; y2].
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.20.20">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.20.20.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.20.20.1.1">
          Q
         </span>
         : Extras was created, written, and directed by Ricky Dene Gervais, an English comedian, actor, writer, producer, director, singer, and musician, born on which date?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.21.21">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.21.21.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.21.21.1.1">
          A
         </span>
         : The answer is 25 June 1961.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.22.22">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.22.22.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.22.22.1.1">
          W
         </span>
         : Ricky Gervais &gt; Ricky Dene Gervais (born 25 June 1961) is an English comedian, actor, writer, producer, director, singer, and musician.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.23.23">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.23.23.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.23.23.1.1">
          C
         </span>
         : Search [when Ricky Dene Gervais was born -Wiki-&gt; y1].
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.24.24">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.24.24.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.24.24.1.1">
          Q
         </span>
         : Sameera Perera is a cricketer from what island country located southeast of the Republic of India and northeast of the Maldives?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.25.25">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.25.25.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.25.25.1.1">
          A
         </span>
         : The answer is Sri Lanka.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.26.26">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.26.26.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.26.26.1.1">
          W
         </span>
         : Sameera Perera &gt; Sameera Perera (born 20 August 1988) is a Sri Lankan cricketer.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.27.27">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.27.27.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.27.27.1.1">
          C
         </span>
         : Identify the [country that cricketer Sameera Perera is from -Wiki-&gt; y1].
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.28.28">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.28.28.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.28.28.1.1">
          Q
         </span>
         : What screenwriter with credits for “Evolution” co-wrote a film starring Nicolas Cage and Téa Leoni?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.29.29">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.29.29.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.29.29.1.1">
          A
         </span>
         : The answer is David Weissman.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.30.30">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.30.30.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.30.30.1.1">
          W
         </span>
         : The Family Man &gt; The Family Man is a 2000 American romantic comedy-drama film starring Nicolas Cage and Téa Leoni. David Weissman &gt; His film credits
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.31.31">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.31.31.1">
         include “The Family Man” (2000), “Evolution” (2001), and “When in Rome” (2010).
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.32.32">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.32.32.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.32.32.1.1">
          C
         </span>
         : First figure out the [film of Nicolas Cage and Téa Leoni -Wiki-&gt; y1]. The name of this film is [y1 -NER(culture)-&gt; y2]. Then find out [who wrote y2 with
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.33.33">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.33.33.1">
         credits for “Evolution” -Wiki-&gt; y3].
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.34.34">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.34.34.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.34.34.1.1">
          Q
         </span>
         : Ralph Hefferline was a psychology professor at a university that is located in what city?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.35.35">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.35.35.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.35.35.1.1">
          A
         </span>
         : The answer is New York City.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.36.36">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.36.36.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.36.36.1.1">
          W
         </span>
         : Ralph Hefferline &gt; Ralph Franklin Hefferline was a psychology professor at Columbia University. Columbia University &gt; Columbia University is a private Ivy
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.37.37">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.37.37.1">
         League research university in Upper Manhattan, New York City.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.38.38">
        <td class="ltx_td ltx_align_left" id="A3.T12.1.1.38.38.1">
         <span class="ltx_text ltx_font_bold" id="A3.T12.1.1.38.38.1.1">
          C
         </span>
         : First identify the [university of psychology professor Ralph Hefferline -Wiki-&gt; y1]. The university of this professor is [y1 -NER(group)-&gt; y2]. Then figure
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T12.1.1.39.39">
        <td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T12.1.1.39.39.1">
         out [y2 is in what city -Wiki-&gt; y3].
        </td>
       </tr>
      </tbody>
     </table>
    </span>
   </div>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_table">
     Table 12:
    </span>
    Prompting examples for fine-tuning data construction in Wiki QA domain. Given a question (Q), a gold answer (A) and its supporting Wikipedia articles (W), LLaMa-70B is prompted to generate an abstract reasoning chain (C) with Wikipedia searching and NER queries. Based on that, our method first trains a LLM to generate the abstract chain of queries based on the question, and then execute the queries by domain tools (
    <span class="ltx_text ltx_font_italic" id="A3.T12.3.1">
     i.e.
    </span>
    , Wikipedia search engine and NER toolkit). Finally, a second LLM is trained to generate the final answer based on the Wikipedia searching results (excluding intermediate NER results) in the reified chain of reasoning.
   </figcaption>
  </figure>
  <figure class="ltx_table" id="A3.T13">
   <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T13.1" style="width:433.6pt;height:627pt;vertical-align:-624.1pt;">
    <span class="ltx_transformed_inner" style="transform:translate(-9.4pt,0.1pt) scale(0.958627874183119,0.958627874183119) ;">
     <table class="ltx_tabular ltx_align_middle" id="A3.T13.1.1">
      <tbody class="ltx_tbody">
       <tr class="ltx_tr" id="A3.T13.1.1.1.1">
        <td class="ltx_td ltx_align_left ltx_border_tt" id="A3.T13.1.1.1.1.1">
         <span class="ltx_text ltx_font_bold" id="A3.T13.1.1.1.1.1.1">
          Q
         </span>
         : There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees will the grove
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.2.2">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.2.2.1">
         workers plant today?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.3.3">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.3.3.1">
         <span class="ltx_text ltx_font_bold" id="A3.T13.1.1.3.3.1.1">
          A
         </span>
         : There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21-15=6. The answer is 6.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.4.4">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.4.4.1">
         <span class="ltx_text ltx_font_bold" id="A3.T13.1.1.4.4.1.1">
          Q
         </span>
         : If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.5.5">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.5.5.1">
         <span class="ltx_text ltx_font_bold" id="A3.T13.1.1.5.5.1.1">
          A
         </span>
         : There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.6.6">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.6.6.1">
         <span class="ltx_text ltx_font_bold" id="A3.T13.1.1.6.6.1.1">
          Q
         </span>
         : The flowers cost $9, the clay pot costs $20 more than the flower, and the bag of soil costs $2 less than the flower. How much does it cost to plant the flowers?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.7.7">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.7.7.1">
         <span class="ltx_text ltx_font_bold" id="A3.T13.1.1.7.7.1.1">
          A
         </span>
         : The clay pot costs $20 + $9 = $29. The bag of soil costs $9 - $2 = $7. The cost to plant the flowers is $9 + $29 + $7 = $45. The answer is 45.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.8.8">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.8.8.1">
         <span class="ltx_text ltx_font_bold" id="A3.T13.1.1.8.8.1.1">
          Q
         </span>
         : Maddie wants to see how much her mom spends on coffee each week. She makes herself 2 cups of coffee per day. Each cup has 1.5 ounces of coffee beans.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.9.9">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.9.9.1">
         A bag of coffee costs $8 and contains 10.5 ounces of beans. How much does she spend on her coffee per week?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.10.10">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.10.10.1">
         <span class="ltx_text ltx_font_bold" id="A3.T13.1.1.10.10.1.1">
          A
         </span>
         : She uses 3 ounces of beans per day because 2 x 1.5 = 3. She uses 21 ounces of beans per week because 7 x 3 = 21. She buys 2 bags of beans per week
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.11.11">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.11.11.1">
         because 21 / 10.5 = 2. She spends $16 on the beans per week because 2 x 8 = 16. The answer is 16.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.12.12">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.12.12.1">
         <span class="ltx_text ltx_font_bold" id="A3.T13.1.1.12.12.1.1">
          Q
         </span>
         : There were nine computers in the server room. Five more computers were installed each day, from Monday to Thursday. How many computers are now in
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.13.13">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.13.13.1">
         the server room?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.14.14">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.14.14.1">
         <span class="ltx_text ltx_font_bold" id="A3.T13.1.1.14.14.1.1">
          A
         </span>
         : There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.15.15">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.15.15.1">
         <span class="ltx_text ltx_font_bold" id="A3.T13.1.1.15.15.1.1">
          Q
         </span>
         : From March to August, Sam made $460 doing 23 hours of yard work. However, from September to February, Sam was only able to work for 8 hours. If Sam
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.16.16">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.16.16.1">
         is saving up to buy a video game console that costs $600 and has already spent $340 to fix his car, how many more hours does he need to work before he can buy
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.17.17">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.17.17.1">
         the video game console?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.18.18">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.18.18.1">
         <span class="ltx_text ltx_font_bold" id="A3.T13.1.1.18.18.1.1">
          A
         </span>
         : Sam makes $460 / 23 hrs = $20/hr. From September to February, Sam made 8hrs x $20/hr = $160. From March to February, Sam made a total of $460 + $160
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.19.19">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.19.19.1">
         = $620. After fixing his car, he was left with $620 - $340 = $280. Sam needs another $600 - $280 = $320. Sam needs to work another $320 / $20/hr = 16 hours.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.20.20">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.20.20.1">
         The answer is 16.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.21.21">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.21.21.1">
         <span class="ltx_text ltx_font_bold" id="A3.T13.1.1.21.21.1.1">
          Q
         </span>
         : Of the 90 people on William’s bus, 3/5 were Dutch. Of the 1/2 of the Dutch who were also American, 1/3 got window seats. What’s the number of Dutch
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.22.22">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.22.22.1">
         Americans who sat at the windows?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.23.23">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.23.23.1">
         <span class="ltx_text ltx_font_bold" id="A3.T13.1.1.23.23.1.1">
          A
         </span>
         : On the bus, the number of Dutch people was 3/5 of the total number, a total of 3/5 x 90 = 54 people. Out of the 54 people who were Dutch, 1/2 were Dutch
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.24.24">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.24.24.1">
         Americans, a total of 1/2 x 54 = 27 people. If 1/3 of the passengers on the bus identifying as Dutch Americans sat at the windows, their number is 1/3 x 27 = 9.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.25.25">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.25.25.1">
         The answer is 9.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.26.26">
        <td class="ltx_td ltx_align_left" id="A3.T13.1.1.26.26.1">
         <span class="ltx_text ltx_font_bold" id="A3.T13.1.1.26.26.1.1">
          Q
         </span>
         : Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T13.1.1.27.27">
        <td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T13.1.1.27.27.1">
         <span class="ltx_text ltx_font_bold" id="A3.T13.1.1.27.27.1.1">
          A
         </span>
         : Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74-35=39. The answer is 39.
        </td>
       </tr>
      </tbody>
     </table>
    </span>
   </div>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_table">
     Table 13:
    </span>
    Few-shot examples used for CoT-FSP baseline model in mathematical reasoning domain.
   </figcaption>
  </figure>
  <figure class="ltx_table" id="A3.T14">
   <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T14.1" style="width:433.6pt;height:462.9pt;vertical-align:-460.0pt;">
    <span class="ltx_transformed_inner" style="transform:translate(-9.2pt,0.1pt) scale(0.959393371666908,0.959393371666908) ;">
     <table class="ltx_tabular ltx_align_middle" id="A3.T14.1.1">
      <tbody class="ltx_tbody">
       <tr class="ltx_tr" id="A3.T14.1.1.1.1">
        <td class="ltx_td ltx_align_left ltx_border_tt" id="A3.T14.1.1.1.1.1">
         <span class="ltx_text ltx_font_bold" id="A3.T14.1.1.1.1.1.1">
          Q
         </span>
         : Fritz von Brodowski was killed during what global war that lasted from 1939 to 1945?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.2.2">
        <td class="ltx_td ltx_align_left" id="A3.T14.1.1.2.2.1">
         <span class="ltx_text ltx_font_bold" id="A3.T14.1.1.2.2.1.1">
          A
         </span>
         : Fritz von Brodowski &gt; Friedrich Wilhelm Konrad von Brodowski was controversially killed while in French custody during World War II. The answer is World War II.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.3.3">
        <td class="ltx_td ltx_align_left" id="A3.T14.1.1.3.3.1">
         <span class="ltx_text ltx_font_bold" id="A3.T14.1.1.3.3.1.1">
          Q
         </span>
         : Which tennis player won more Grand Slam titles, Henri Leconte or Jonathan Stark?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.4.4">
        <td class="ltx_td ltx_align_left" id="A3.T14.1.1.4.4.1">
         <span class="ltx_text ltx_font_bold" id="A3.T14.1.1.4.4.1.1">
          A
         </span>
         : Henri Leconte &gt; He won the French Open men’s doubles title in 1984. Jonathan Stark (tennis) &gt; During his career he won two Grand Slam doubles titles.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.5.5">
        <td class="ltx_td ltx_align_left" id="A3.T14.1.1.5.5.1">
         The answer is Jonathan Stark.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.6.6">
        <td class="ltx_td ltx_align_left" id="A3.T14.1.1.6.6.1">
         <span class="ltx_text ltx_font_bold" id="A3.T14.1.1.6.6.1.1">
          Q
         </span>
         : The director of the romantic comedy “Big Stone Gap” is based in what New York city?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.7.7">
        <td class="ltx_td ltx_align_left" id="A3.T14.1.1.7.7.1">
         <span class="ltx_text ltx_font_bold" id="A3.T14.1.1.7.7.1.1">
          A
         </span>
         : Big Stone Gap (film) &gt; Big Stone Gap is a 2014 American romantic comedy film directed by Adriana Trigiani. Adriana Trigiani &gt; Adriana Trigiani is an
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.8.8">
        <td class="ltx_td ltx_align_left" id="A3.T14.1.1.8.8.1">
         Italian American film director based in Greenwich Village. The answer is Greenwich Village.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.9.9">
        <td class="ltx_td ltx_align_left" id="A3.T14.1.1.9.9.1">
         <span class="ltx_text ltx_font_bold" id="A3.T14.1.1.9.9.1.1">
          Q
         </span>
         : Are Randal Kleiser and Kyle Schickner of the same nationality?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.10.10">
        <td class="ltx_td ltx_align_left" id="A3.T14.1.1.10.10.1">
         <span class="ltx_text ltx_font_bold" id="A3.T14.1.1.10.10.1.1">
          A
         </span>
         : Randal Kleiser &gt; John Randal Kleiser (born July 20, 1946) is an American film director and producer. Kyle Schickner &gt; Kyle Schickner is an American film
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.11.11">
        <td class="ltx_td ltx_align_left" id="A3.T14.1.1.11.11.1">
         producer, writer, director, actor. The answer is yes.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.12.12">
        <td class="ltx_td ltx_align_left" id="A3.T14.1.1.12.12.1">
         <span class="ltx_text ltx_font_bold" id="A3.T14.1.1.12.12.1.1">
          Q
         </span>
         : Extras was created, written, and directed by Ricky Dene Gervais, an English comedian, actor, writer, producer, director, singer, and musician, born on which date?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.13.13">
        <td class="ltx_td ltx_align_left" id="A3.T14.1.1.13.13.1">
         <span class="ltx_text ltx_font_bold" id="A3.T14.1.1.13.13.1.1">
          A
         </span>
         : Ricky Gervais &gt; Ricky Dene Gervais (born 25 June 1961) is an English comedian, actor, writer, producer, director, singer, and musician. The answer is 25 June 1961.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.14.14">
        <td class="ltx_td ltx_align_left" id="A3.T14.1.1.14.14.1">
         <span class="ltx_text ltx_font_bold" id="A3.T14.1.1.14.14.1.1">
          Q
         </span>
         : Sameera Perera is a cricketer from what island country located southeast of the Republic of India and northeast of the Maldives?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.15.15">
        <td class="ltx_td ltx_align_left" id="A3.T14.1.1.15.15.1">
         <span class="ltx_text ltx_font_bold" id="A3.T14.1.1.15.15.1.1">
          A
         </span>
         : Sameera Perera &gt; Sameera Perera (born 20 August 1988) is a Sri Lankan cricketer. The answer is Sri Lanka.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.16.16">
        <td class="ltx_td ltx_align_left" id="A3.T14.1.1.16.16.1">
         <span class="ltx_text ltx_font_bold" id="A3.T14.1.1.16.16.1.1">
          Q
         </span>
         : What screenwriter with credits for “Evolution” co-wrote a film starring Nicolas Cage and Téa Leoni?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.17.17">
        <td class="ltx_td ltx_align_left" id="A3.T14.1.1.17.17.1">
         <span class="ltx_text ltx_font_bold" id="A3.T14.1.1.17.17.1.1">
          A
         </span>
         : The Family Man &gt; The Family Man is a 2000 American romantic comedy-drama film starring Nicolas Cage and Téa Leoni. David Weissman &gt; His film credits
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.18.18">
        <td class="ltx_td ltx_align_left" id="A3.T14.1.1.18.18.1">
         include “The Family Man” (2000), “Evolution” (2001), and “When in Rome” (2010). The answer is David Weissman.
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.19.19">
        <td class="ltx_td ltx_align_left" id="A3.T14.1.1.19.19.1">
         <span class="ltx_text ltx_font_bold" id="A3.T14.1.1.19.19.1.1">
          Q
         </span>
         : Ralph Hefferline was a psychology professor at a university that is located in what city?
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.20.20">
        <td class="ltx_td ltx_align_left" id="A3.T14.1.1.20.20.1">
         <span class="ltx_text ltx_font_bold" id="A3.T14.1.1.20.20.1.1">
          A
         </span>
         : Ralph Hefferline &gt; Ralph Franklin Hefferline was a psychology professor at Columbia University. Columbia University &gt; Columbia University is a private Ivy
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T14.1.1.21.21">
        <td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T14.1.1.21.21.1">
         League research university in Upper Manhattan, New York City. The answer is New York City.
        </td>
       </tr>
      </tbody>
     </table>
    </span>
   </div>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_table">
     Table 14:
    </span>
    Few-shot examples used for CoT-FSP baseline model in Wiki QA domain.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A3.F5">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="495" id="A3.F5.g1" src="/html/2401.17464/assets/x5.png" width="368"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 5:
    </span>
    Guideline for human evaluation on GSM8K mathematical reasoning.
   </figcaption>
  </figure>
 </section>
</article>
