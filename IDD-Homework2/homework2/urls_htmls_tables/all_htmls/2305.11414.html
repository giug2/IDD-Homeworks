<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2305.11414] Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models</title><meta property="og:description" content="Foundation Models (FMs), such as LLaMA, BERT, GPT, ViT, and CLIP, have demonstrated remarkable success in a wide range of applications, driven by their ability to leverage vast amounts of data for pre-training. However…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2305.11414">

<!--Generated on Thu Feb 29 06:34:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">Foundation Models (FMs), such as LLaMA, BERT, GPT, ViT, and CLIP, have demonstrated remarkable success in a wide range of applications, driven by their ability to leverage vast amounts of data for pre-training. However, optimizing FMs often requires access to sensitive data, raising privacy concerns and limiting their applicability in many domains.
In this paper, we propose the Federated Foundation Models (FFMs) paradigm, which combines the benefits of FMs and Federated Learning (FL) to enable privacy-preserving and collaborative learning across multiple end-users.
We discuss the potential benefits and challenges of integrating FL into the lifespan of FMs, covering pre-training, fine-tuning, and application.
We further outline potential future research avenues in FFM, including FFM pre-training, FFM fine-tuning, and federated prompt tuning, which allow the development of more personalized and context-aware models while ensuring data privacy.
Moreover, we explore the possibility of continual/lifelong learning in FFMs, as increased computational power at the edge may unlock the potential for optimizing FMs using newly generated private data close to the data source.
The proposed FFM concepts offer a flexible and scalable framework for training large language models in a privacy-preserving manner, setting the stage for subsequent advancements in both FM training and federated learning. 
<br class="ltx_break">
<br class="ltx_break">
<span id="id7.id1.1" class="ltx_text ltx_font_bold">Keywords: </span>Federated Learning, Foundation Models, Machine Learning, Data Privacy</p>
</div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\NAT@set@cites</span>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text"></span></p>
</div>
<div id="id6" class="ltx_logical-block">
<div id="id6.p1" class="ltx_para">
<p id="id6.p1.1" class="ltx_p ltx_align_center"><span id="id6.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models</span></p>
<br class="ltx_break ltx_centering">
<table id="id5.5" class="ltx_tabular ltx_centering ltx_align_top">
<tbody class="ltx_tbody">
<tr id="id3.3.3" class="ltx_tr">
<td id="id3.3.3.3" class="ltx_td ltx_align_center"><span id="id3.3.3.3.3" class="ltx_text ltx_font_bold" style="font-size:120%;">Sixing Yu<sup id="id3.3.3.3.3.1" class="ltx_sup"><span id="id3.3.3.3.3.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>,  J. Pablo Muñoz<sup id="id3.3.3.3.3.2" class="ltx_sup"><span id="id3.3.3.3.3.2.1" class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup>,  Ali Jannesari<sup id="id3.3.3.3.3.3" class="ltx_sup"><span id="id3.3.3.3.3.3.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span></td>
</tr>
<tr id="id4.4.4" class="ltx_tr">
<td id="id4.4.4.1" class="ltx_td ltx_align_center"><sup id="id4.4.4.1.1" class="ltx_sup"><span id="id4.4.4.1.1.1" class="ltx_text ltx_font_italic">1</span></sup>Iowa State University</td>
</tr>
<tr id="id5.5.5" class="ltx_tr">
<td id="id5.5.5.1" class="ltx_td ltx_align_center">
<sup id="id5.5.5.1.1" class="ltx_sup"><span id="id5.5.5.1.1.1" class="ltx_text ltx_font_italic">2</span></sup>Intel Labs</td>
</tr>
<tr id="id5.5.6.1" class="ltx_tr">
<td id="id5.5.6.1.1" class="ltx_td ltx_align_center"><span id="id5.5.6.1.1.1" class="ltx_text ltx_font_typewriter">{yusx, jannesar}@iastate.edu, {pablo.munoz}@intel.com</span></td>
</tr>
</tbody>
</table>
<p id="id6.p1.2" class="ltx_p ltx_align_center"><span id="id6.p1.2.1" class="ltx_text ltx_font_italic">Abstract content</span></p>
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.   Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, Foundation Models (FMs) such as BERT <cite class="ltx_cite ltx_citemacro_cite">Kenton and Toutanova (<a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>, GPT <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>); Radford et al. (<a href="#bib.bib29" title="" class="ltx_ref">2019</a>)</cite>, Llama <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib33" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib34" title="" class="ltx_ref">b</a>)</cite>, ViT <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>, and CLIP <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite> have significantly advanced the field of artificial intelligence, showcasing impressive performance across a wide range of tasks and domains. However, the optimization of increasingly complex FMs heavily depends on the collections of massive datasets, which introduces concerns regarding training data scarcity, computational resources, privacy, and ethical considerations.
Simultaneously, the prevalent trend of advancement in edge technologies generates a vast amount of decentralized data, creating potential resources for further optimizing and specializing FMs. Nevertheless, due to privacy concerns, this private data is rarely leveraged for FM optimizations. In light of this, Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite> has emerged as a pioneering approach for decentralized and privacy-preserving machine learning, allowing models to learn from distributed private data sources without directly accessing the raw data.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2305.11414/assets/content/figures/ffm_benefits.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="189" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Federated Foundation Model: Integrating federated learning into the lifespan of foundation models, facilitating privacy-preserving, scalable, lifelong learning, robustness, and decentralized FMs.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The intersection of these two domains presents a unique opportunity to unlock new possibilities in AI research and to address critical challenges in AI model development and real-world applications.
Hence, we propose the concept of Federated Foundation Models (FFMs), a novel paradigm that integrates FL into the lifespan of FMs. This integration addresses the challenges mentioned above related to data scarcity, computational resources, privacy, and ethical considerations while facilitating privacy-preserving and collaborative learning across multiple end-users. As advancements in edge computing enable the optimization of FMs using FL, we further explore the possibility of continual/lifelong learning for FMs in FFMs.
We also discuss the potential benefits and challenges of integrating FL into different stages of the FMs’ lifespan, including pre-training, fine-tuning, and application, and provide potential research directions for FFM tasks such as FFM Pre-training, FFM Fine-tuning, and Federated Prompt Tuning. These tasks promote the development of personalized and context-aware models while maintaining data privacy.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In summary, this paper offers a comprehensive examination of the prospective of FFMs, proposing a flexible and scalable framework for training large models in a privacy-preserving manner. We believe our work contributes to paving the way for future advancements in both FMs and FL, fostering the development of more secure and adaptable large models and FL algorithms that cater to a wide range of applications.

</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.   Background</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">2.1.   Federated Learning</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">As concerns about user data privacy grow, there is an increasing need for AI models to be trained on decentralized data without sharing private information between clients. Federated Learning (FL) has emerged as a solution to this problem, offering a distributed and privacy-preserving machine learning approach that enables training on decentralized data without compromising data privacy <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">In FL, raw data remains on local clients, ensuring data privacy and security while also enabling collaborative learning across multiple clients. The FL process involves local model training, model aggregation algorithm, and global model updates. Throughout this process, clients only share model updates, such as weights and gradients, asynchronously, reducing bandwidth requirements and minimizing the risk of data leaks and breaches.
A typical FL algorithm is FedAvg <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite>, which demonstrates the FL process (see Algorithm <a href="#alg1" title="Algorithm 1 ‣ 2.1. Federated Learning ‣ 2. Background ‣ Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). The privacy-preserving nature of FL has led to its widespread adoption in various applications, particularly in privacy-sensitive domains like healthcare.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.2.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Federated Learning Process (FedAvg)</figcaption>
<div id="alg1.3" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l1.1.1.1" class="ltx_text" style="font-size:90%;">1:</span></span><span id="alg1.l1.2" class="ltx_text ltx_font_bold">Input:</span> Global AI model <math id="alg1.l1.m1.1" class="ltx_Math" alttext="w_{0}" display="inline"><semantics id="alg1.l1.m1.1a"><msub id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml"><mi id="alg1.l1.m1.1.1.2" xref="alg1.l1.m1.1.1.2.cmml">w</mi><mn id="alg1.l1.m1.1.1.3" xref="alg1.l1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><apply id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1"><csymbol cd="ambiguous" id="alg1.l1.m1.1.1.1.cmml" xref="alg1.l1.m1.1.1">subscript</csymbol><ci id="alg1.l1.m1.1.1.2.cmml" xref="alg1.l1.m1.1.1.2">𝑤</ci><cn type="integer" id="alg1.l1.m1.1.1.3.cmml" xref="alg1.l1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">w_{0}</annotation></semantics></math>, clients <math id="alg1.l1.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="alg1.l1.m2.1a"><mi id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><ci id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">S</annotation></semantics></math>, communication rounds <math id="alg1.l1.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="alg1.l1.m3.1a"><mi id="alg1.l1.m3.1.1" xref="alg1.l1.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m3.1b"><ci id="alg1.l1.m3.1.1.cmml" xref="alg1.l1.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m3.1c">T</annotation></semantics></math>

</div>
<div id="alg1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l2.1.1.1" class="ltx_text" style="font-size:90%;">2:</span></span><span id="alg1.l2.2" class="ltx_text ltx_font_bold">for</span> <math id="alg1.l2.m1.4" class="ltx_Math" alttext="t=1,2,\ldots,T" display="inline"><semantics id="alg1.l2.m1.4a"><mrow id="alg1.l2.m1.4.5" xref="alg1.l2.m1.4.5.cmml"><mi id="alg1.l2.m1.4.5.2" xref="alg1.l2.m1.4.5.2.cmml">t</mi><mo id="alg1.l2.m1.4.5.1" xref="alg1.l2.m1.4.5.1.cmml">=</mo><mrow id="alg1.l2.m1.4.5.3.2" xref="alg1.l2.m1.4.5.3.1.cmml"><mn id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml">1</mn><mo id="alg1.l2.m1.4.5.3.2.1" xref="alg1.l2.m1.4.5.3.1.cmml">,</mo><mn id="alg1.l2.m1.2.2" xref="alg1.l2.m1.2.2.cmml">2</mn><mo id="alg1.l2.m1.4.5.3.2.2" xref="alg1.l2.m1.4.5.3.1.cmml">,</mo><mi mathvariant="normal" id="alg1.l2.m1.3.3" xref="alg1.l2.m1.3.3.cmml">…</mi><mo id="alg1.l2.m1.4.5.3.2.3" xref="alg1.l2.m1.4.5.3.1.cmml">,</mo><mi id="alg1.l2.m1.4.4" xref="alg1.l2.m1.4.4.cmml">T</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.4b"><apply id="alg1.l2.m1.4.5.cmml" xref="alg1.l2.m1.4.5"><eq id="alg1.l2.m1.4.5.1.cmml" xref="alg1.l2.m1.4.5.1"></eq><ci id="alg1.l2.m1.4.5.2.cmml" xref="alg1.l2.m1.4.5.2">𝑡</ci><list id="alg1.l2.m1.4.5.3.1.cmml" xref="alg1.l2.m1.4.5.3.2"><cn type="integer" id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1">1</cn><cn type="integer" id="alg1.l2.m1.2.2.cmml" xref="alg1.l2.m1.2.2">2</cn><ci id="alg1.l2.m1.3.3.cmml" xref="alg1.l2.m1.3.3">…</ci><ci id="alg1.l2.m1.4.4.cmml" xref="alg1.l2.m1.4.4">𝑇</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.4c">t=1,2,\ldots,T</annotation></semantics></math> <span id="alg1.l2.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l3.1.1.1" class="ltx_text" style="font-size:90%;">3:</span></span>     Server deploys global model <math id="alg1.l3.m1.1" class="ltx_Math" alttext="w_{t-1}" display="inline"><semantics id="alg1.l3.m1.1a"><msub id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><mi id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2.cmml">w</mi><mrow id="alg1.l3.m1.1.1.3" xref="alg1.l3.m1.1.1.3.cmml"><mi id="alg1.l3.m1.1.1.3.2" xref="alg1.l3.m1.1.1.3.2.cmml">t</mi><mo id="alg1.l3.m1.1.1.3.1" xref="alg1.l3.m1.1.1.3.1.cmml">−</mo><mn id="alg1.l3.m1.1.1.3.3" xref="alg1.l3.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><csymbol cd="ambiguous" id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1">subscript</csymbol><ci id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2">𝑤</ci><apply id="alg1.l3.m1.1.1.3.cmml" xref="alg1.l3.m1.1.1.3"><minus id="alg1.l3.m1.1.1.3.1.cmml" xref="alg1.l3.m1.1.1.3.1"></minus><ci id="alg1.l3.m1.1.1.3.2.cmml" xref="alg1.l3.m1.1.1.3.2">𝑡</ci><cn type="integer" id="alg1.l3.m1.1.1.3.3.cmml" xref="alg1.l3.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">w_{t-1}</annotation></semantics></math> to clients <math id="alg1.l3.m2.1" class="ltx_Math" alttext="\in S" display="inline"><semantics id="alg1.l3.m2.1a"><mrow id="alg1.l3.m2.1.1" xref="alg1.l3.m2.1.1.cmml"><mi id="alg1.l3.m2.1.1.2" xref="alg1.l3.m2.1.1.2.cmml"></mi><mo id="alg1.l3.m2.1.1.1" xref="alg1.l3.m2.1.1.1.cmml">∈</mo><mi id="alg1.l3.m2.1.1.3" xref="alg1.l3.m2.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3.m2.1b"><apply id="alg1.l3.m2.1.1.cmml" xref="alg1.l3.m2.1.1"><in id="alg1.l3.m2.1.1.1.cmml" xref="alg1.l3.m2.1.1.1"></in><csymbol cd="latexml" id="alg1.l3.m2.1.1.2.cmml" xref="alg1.l3.m2.1.1.2">absent</csymbol><ci id="alg1.l3.m2.1.1.3.cmml" xref="alg1.l3.m2.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m2.1c">\in S</annotation></semantics></math>

</div>
<div id="alg1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l4.1.1.1" class="ltx_text" style="font-size:90%;">4:</span></span>     <span id="alg1.l4.2" class="ltx_text ltx_font_bold">for</span> each client <math id="alg1.l4.m1.1" class="ltx_Math" alttext="k\in S" display="inline"><semantics id="alg1.l4.m1.1a"><mrow id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml"><mi id="alg1.l4.m1.1.1.2" xref="alg1.l4.m1.1.1.2.cmml">k</mi><mo id="alg1.l4.m1.1.1.1" xref="alg1.l4.m1.1.1.1.cmml">∈</mo><mi id="alg1.l4.m1.1.1.3" xref="alg1.l4.m1.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><apply id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1"><in id="alg1.l4.m1.1.1.1.cmml" xref="alg1.l4.m1.1.1.1"></in><ci id="alg1.l4.m1.1.1.2.cmml" xref="alg1.l4.m1.1.1.2">𝑘</ci><ci id="alg1.l4.m1.1.1.3.cmml" xref="alg1.l4.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">k\in S</annotation></semantics></math> <span id="alg1.l4.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l5.1.1.1" class="ltx_text" style="font-size:90%;">5:</span></span>         Client <math id="alg1.l5.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="alg1.l5.m1.1a"><mi id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><ci id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">k</annotation></semantics></math> optimizes <math id="alg1.l5.m2.1" class="ltx_Math" alttext="w_{t-1}" display="inline"><semantics id="alg1.l5.m2.1a"><msub id="alg1.l5.m2.1.1" xref="alg1.l5.m2.1.1.cmml"><mi id="alg1.l5.m2.1.1.2" xref="alg1.l5.m2.1.1.2.cmml">w</mi><mrow id="alg1.l5.m2.1.1.3" xref="alg1.l5.m2.1.1.3.cmml"><mi id="alg1.l5.m2.1.1.3.2" xref="alg1.l5.m2.1.1.3.2.cmml">t</mi><mo id="alg1.l5.m2.1.1.3.1" xref="alg1.l5.m2.1.1.3.1.cmml">−</mo><mn id="alg1.l5.m2.1.1.3.3" xref="alg1.l5.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="alg1.l5.m2.1b"><apply id="alg1.l5.m2.1.1.cmml" xref="alg1.l5.m2.1.1"><csymbol cd="ambiguous" id="alg1.l5.m2.1.1.1.cmml" xref="alg1.l5.m2.1.1">subscript</csymbol><ci id="alg1.l5.m2.1.1.2.cmml" xref="alg1.l5.m2.1.1.2">𝑤</ci><apply id="alg1.l5.m2.1.1.3.cmml" xref="alg1.l5.m2.1.1.3"><minus id="alg1.l5.m2.1.1.3.1.cmml" xref="alg1.l5.m2.1.1.3.1"></minus><ci id="alg1.l5.m2.1.1.3.2.cmml" xref="alg1.l5.m2.1.1.3.2">𝑡</ci><cn type="integer" id="alg1.l5.m2.1.1.3.3.cmml" xref="alg1.l5.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m2.1c">w_{t-1}</annotation></semantics></math> on local data, producing <math id="alg1.l5.m3.1" class="ltx_Math" alttext="w_{t}^{k}" display="inline"><semantics id="alg1.l5.m3.1a"><msubsup id="alg1.l5.m3.1.1" xref="alg1.l5.m3.1.1.cmml"><mi id="alg1.l5.m3.1.1.2.2" xref="alg1.l5.m3.1.1.2.2.cmml">w</mi><mi id="alg1.l5.m3.1.1.2.3" xref="alg1.l5.m3.1.1.2.3.cmml">t</mi><mi id="alg1.l5.m3.1.1.3" xref="alg1.l5.m3.1.1.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="alg1.l5.m3.1b"><apply id="alg1.l5.m3.1.1.cmml" xref="alg1.l5.m3.1.1"><csymbol cd="ambiguous" id="alg1.l5.m3.1.1.1.cmml" xref="alg1.l5.m3.1.1">superscript</csymbol><apply id="alg1.l5.m3.1.1.2.cmml" xref="alg1.l5.m3.1.1"><csymbol cd="ambiguous" id="alg1.l5.m3.1.1.2.1.cmml" xref="alg1.l5.m3.1.1">subscript</csymbol><ci id="alg1.l5.m3.1.1.2.2.cmml" xref="alg1.l5.m3.1.1.2.2">𝑤</ci><ci id="alg1.l5.m3.1.1.2.3.cmml" xref="alg1.l5.m3.1.1.2.3">𝑡</ci></apply><ci id="alg1.l5.m3.1.1.3.cmml" xref="alg1.l5.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m3.1c">w_{t}^{k}</annotation></semantics></math>

</div>
<div id="alg1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l6.1.1.1" class="ltx_text" style="font-size:90%;">6:</span></span>     <span id="alg1.l6.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l6.3" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l7.1.1.1" class="ltx_text" style="font-size:90%;">7:</span></span>     Select a subset of clients <math id="alg1.l7.m1.1" class="ltx_Math" alttext="S_{t}" display="inline"><semantics id="alg1.l7.m1.1a"><msub id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml"><mi id="alg1.l7.m1.1.1.2" xref="alg1.l7.m1.1.1.2.cmml">S</mi><mi id="alg1.l7.m1.1.1.3" xref="alg1.l7.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><apply id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1"><csymbol cd="ambiguous" id="alg1.l7.m1.1.1.1.cmml" xref="alg1.l7.m1.1.1">subscript</csymbol><ci id="alg1.l7.m1.1.1.2.cmml" xref="alg1.l7.m1.1.1.2">𝑆</ci><ci id="alg1.l7.m1.1.1.3.cmml" xref="alg1.l7.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">S_{t}</annotation></semantics></math> to communicate with the server

</div>
<div id="alg1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l8.1.1.1" class="ltx_text" style="font-size:90%;">8:</span></span>     <span id="alg1.l8.2" class="ltx_text ltx_font_bold">for</span> each client <math id="alg1.l8.m1.1" class="ltx_Math" alttext="k\in S_{t}" display="inline"><semantics id="alg1.l8.m1.1a"><mrow id="alg1.l8.m1.1.1" xref="alg1.l8.m1.1.1.cmml"><mi id="alg1.l8.m1.1.1.2" xref="alg1.l8.m1.1.1.2.cmml">k</mi><mo id="alg1.l8.m1.1.1.1" xref="alg1.l8.m1.1.1.1.cmml">∈</mo><msub id="alg1.l8.m1.1.1.3" xref="alg1.l8.m1.1.1.3.cmml"><mi id="alg1.l8.m1.1.1.3.2" xref="alg1.l8.m1.1.1.3.2.cmml">S</mi><mi id="alg1.l8.m1.1.1.3.3" xref="alg1.l8.m1.1.1.3.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="alg1.l8.m1.1b"><apply id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1"><in id="alg1.l8.m1.1.1.1.cmml" xref="alg1.l8.m1.1.1.1"></in><ci id="alg1.l8.m1.1.1.2.cmml" xref="alg1.l8.m1.1.1.2">𝑘</ci><apply id="alg1.l8.m1.1.1.3.cmml" xref="alg1.l8.m1.1.1.3"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.1.cmml" xref="alg1.l8.m1.1.1.3">subscript</csymbol><ci id="alg1.l8.m1.1.1.3.2.cmml" xref="alg1.l8.m1.1.1.3.2">𝑆</ci><ci id="alg1.l8.m1.1.1.3.3.cmml" xref="alg1.l8.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m1.1c">k\in S_{t}</annotation></semantics></math> <span id="alg1.l8.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l9.1.1.1" class="ltx_text" style="font-size:90%;">9:</span></span>         Client <math id="alg1.l9.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="alg1.l9.m1.1a"><mi id="alg1.l9.m1.1.1" xref="alg1.l9.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.1b"><ci id="alg1.l9.m1.1.1.cmml" xref="alg1.l9.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.1c">k</annotation></semantics></math> sends local model update <math id="alg1.l9.m2.1" class="ltx_Math" alttext="\Delta w_{t}^{k}=w_{t}^{k}-w_{t-1}" display="inline"><semantics id="alg1.l9.m2.1a"><mrow id="alg1.l9.m2.1.1" xref="alg1.l9.m2.1.1.cmml"><mrow id="alg1.l9.m2.1.1.2" xref="alg1.l9.m2.1.1.2.cmml"><mi mathvariant="normal" id="alg1.l9.m2.1.1.2.2" xref="alg1.l9.m2.1.1.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="alg1.l9.m2.1.1.2.1" xref="alg1.l9.m2.1.1.2.1.cmml">​</mo><msubsup id="alg1.l9.m2.1.1.2.3" xref="alg1.l9.m2.1.1.2.3.cmml"><mi id="alg1.l9.m2.1.1.2.3.2.2" xref="alg1.l9.m2.1.1.2.3.2.2.cmml">w</mi><mi id="alg1.l9.m2.1.1.2.3.2.3" xref="alg1.l9.m2.1.1.2.3.2.3.cmml">t</mi><mi id="alg1.l9.m2.1.1.2.3.3" xref="alg1.l9.m2.1.1.2.3.3.cmml">k</mi></msubsup></mrow><mo id="alg1.l9.m2.1.1.1" xref="alg1.l9.m2.1.1.1.cmml">=</mo><mrow id="alg1.l9.m2.1.1.3" xref="alg1.l9.m2.1.1.3.cmml"><msubsup id="alg1.l9.m2.1.1.3.2" xref="alg1.l9.m2.1.1.3.2.cmml"><mi id="alg1.l9.m2.1.1.3.2.2.2" xref="alg1.l9.m2.1.1.3.2.2.2.cmml">w</mi><mi id="alg1.l9.m2.1.1.3.2.2.3" xref="alg1.l9.m2.1.1.3.2.2.3.cmml">t</mi><mi id="alg1.l9.m2.1.1.3.2.3" xref="alg1.l9.m2.1.1.3.2.3.cmml">k</mi></msubsup><mo id="alg1.l9.m2.1.1.3.1" xref="alg1.l9.m2.1.1.3.1.cmml">−</mo><msub id="alg1.l9.m2.1.1.3.3" xref="alg1.l9.m2.1.1.3.3.cmml"><mi id="alg1.l9.m2.1.1.3.3.2" xref="alg1.l9.m2.1.1.3.3.2.cmml">w</mi><mrow id="alg1.l9.m2.1.1.3.3.3" xref="alg1.l9.m2.1.1.3.3.3.cmml"><mi id="alg1.l9.m2.1.1.3.3.3.2" xref="alg1.l9.m2.1.1.3.3.3.2.cmml">t</mi><mo id="alg1.l9.m2.1.1.3.3.3.1" xref="alg1.l9.m2.1.1.3.3.3.1.cmml">−</mo><mn id="alg1.l9.m2.1.1.3.3.3.3" xref="alg1.l9.m2.1.1.3.3.3.3.cmml">1</mn></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l9.m2.1b"><apply id="alg1.l9.m2.1.1.cmml" xref="alg1.l9.m2.1.1"><eq id="alg1.l9.m2.1.1.1.cmml" xref="alg1.l9.m2.1.1.1"></eq><apply id="alg1.l9.m2.1.1.2.cmml" xref="alg1.l9.m2.1.1.2"><times id="alg1.l9.m2.1.1.2.1.cmml" xref="alg1.l9.m2.1.1.2.1"></times><ci id="alg1.l9.m2.1.1.2.2.cmml" xref="alg1.l9.m2.1.1.2.2">Δ</ci><apply id="alg1.l9.m2.1.1.2.3.cmml" xref="alg1.l9.m2.1.1.2.3"><csymbol cd="ambiguous" id="alg1.l9.m2.1.1.2.3.1.cmml" xref="alg1.l9.m2.1.1.2.3">superscript</csymbol><apply id="alg1.l9.m2.1.1.2.3.2.cmml" xref="alg1.l9.m2.1.1.2.3"><csymbol cd="ambiguous" id="alg1.l9.m2.1.1.2.3.2.1.cmml" xref="alg1.l9.m2.1.1.2.3">subscript</csymbol><ci id="alg1.l9.m2.1.1.2.3.2.2.cmml" xref="alg1.l9.m2.1.1.2.3.2.2">𝑤</ci><ci id="alg1.l9.m2.1.1.2.3.2.3.cmml" xref="alg1.l9.m2.1.1.2.3.2.3">𝑡</ci></apply><ci id="alg1.l9.m2.1.1.2.3.3.cmml" xref="alg1.l9.m2.1.1.2.3.3">𝑘</ci></apply></apply><apply id="alg1.l9.m2.1.1.3.cmml" xref="alg1.l9.m2.1.1.3"><minus id="alg1.l9.m2.1.1.3.1.cmml" xref="alg1.l9.m2.1.1.3.1"></minus><apply id="alg1.l9.m2.1.1.3.2.cmml" xref="alg1.l9.m2.1.1.3.2"><csymbol cd="ambiguous" id="alg1.l9.m2.1.1.3.2.1.cmml" xref="alg1.l9.m2.1.1.3.2">superscript</csymbol><apply id="alg1.l9.m2.1.1.3.2.2.cmml" xref="alg1.l9.m2.1.1.3.2"><csymbol cd="ambiguous" id="alg1.l9.m2.1.1.3.2.2.1.cmml" xref="alg1.l9.m2.1.1.3.2">subscript</csymbol><ci id="alg1.l9.m2.1.1.3.2.2.2.cmml" xref="alg1.l9.m2.1.1.3.2.2.2">𝑤</ci><ci id="alg1.l9.m2.1.1.3.2.2.3.cmml" xref="alg1.l9.m2.1.1.3.2.2.3">𝑡</ci></apply><ci id="alg1.l9.m2.1.1.3.2.3.cmml" xref="alg1.l9.m2.1.1.3.2.3">𝑘</ci></apply><apply id="alg1.l9.m2.1.1.3.3.cmml" xref="alg1.l9.m2.1.1.3.3"><csymbol cd="ambiguous" id="alg1.l9.m2.1.1.3.3.1.cmml" xref="alg1.l9.m2.1.1.3.3">subscript</csymbol><ci id="alg1.l9.m2.1.1.3.3.2.cmml" xref="alg1.l9.m2.1.1.3.3.2">𝑤</ci><apply id="alg1.l9.m2.1.1.3.3.3.cmml" xref="alg1.l9.m2.1.1.3.3.3"><minus id="alg1.l9.m2.1.1.3.3.3.1.cmml" xref="alg1.l9.m2.1.1.3.3.3.1"></minus><ci id="alg1.l9.m2.1.1.3.3.3.2.cmml" xref="alg1.l9.m2.1.1.3.3.3.2">𝑡</ci><cn type="integer" id="alg1.l9.m2.1.1.3.3.3.3.cmml" xref="alg1.l9.m2.1.1.3.3.3.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m2.1c">\Delta w_{t}^{k}=w_{t}^{k}-w_{t-1}</annotation></semantics></math> to the server

</div>
<div id="alg1.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l10.1.1.1" class="ltx_text" style="font-size:90%;">10:</span></span>     <span id="alg1.l10.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l10.3" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l11.1.1.1" class="ltx_text" style="font-size:90%;">11:</span></span>     Server aggregates local updates and computes the new global model:

<table id="S2.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex1.m1.1" class="ltx_Math" alttext="w_{t}=w_{t-1}+\eta_{t}\sum_{k\in S_{t}}n_{k}\Delta w_{t}^{k}" display="block"><semantics id="S2.Ex1.m1.1a"><mrow id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml"><msub id="S2.Ex1.m1.1.1.2" xref="S2.Ex1.m1.1.1.2.cmml"><mi id="S2.Ex1.m1.1.1.2.2" xref="S2.Ex1.m1.1.1.2.2.cmml">w</mi><mi id="S2.Ex1.m1.1.1.2.3" xref="S2.Ex1.m1.1.1.2.3.cmml">t</mi></msub><mo id="S2.Ex1.m1.1.1.1" xref="S2.Ex1.m1.1.1.1.cmml">=</mo><mrow id="S2.Ex1.m1.1.1.3" xref="S2.Ex1.m1.1.1.3.cmml"><msub id="S2.Ex1.m1.1.1.3.2" xref="S2.Ex1.m1.1.1.3.2.cmml"><mi id="S2.Ex1.m1.1.1.3.2.2" xref="S2.Ex1.m1.1.1.3.2.2.cmml">w</mi><mrow id="S2.Ex1.m1.1.1.3.2.3" xref="S2.Ex1.m1.1.1.3.2.3.cmml"><mi id="S2.Ex1.m1.1.1.3.2.3.2" xref="S2.Ex1.m1.1.1.3.2.3.2.cmml">t</mi><mo id="S2.Ex1.m1.1.1.3.2.3.1" xref="S2.Ex1.m1.1.1.3.2.3.1.cmml">−</mo><mn id="S2.Ex1.m1.1.1.3.2.3.3" xref="S2.Ex1.m1.1.1.3.2.3.3.cmml">1</mn></mrow></msub><mo id="S2.Ex1.m1.1.1.3.1" xref="S2.Ex1.m1.1.1.3.1.cmml">+</mo><mrow id="S2.Ex1.m1.1.1.3.3" xref="S2.Ex1.m1.1.1.3.3.cmml"><msub id="S2.Ex1.m1.1.1.3.3.2" xref="S2.Ex1.m1.1.1.3.3.2.cmml"><mi id="S2.Ex1.m1.1.1.3.3.2.2" xref="S2.Ex1.m1.1.1.3.3.2.2.cmml">η</mi><mi id="S2.Ex1.m1.1.1.3.3.2.3" xref="S2.Ex1.m1.1.1.3.3.2.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.1.1.3.3.1" xref="S2.Ex1.m1.1.1.3.3.1.cmml">​</mo><mrow id="S2.Ex1.m1.1.1.3.3.3" xref="S2.Ex1.m1.1.1.3.3.3.cmml"><munder id="S2.Ex1.m1.1.1.3.3.3.1" xref="S2.Ex1.m1.1.1.3.3.3.1.cmml"><mo movablelimits="false" id="S2.Ex1.m1.1.1.3.3.3.1.2" xref="S2.Ex1.m1.1.1.3.3.3.1.2.cmml">∑</mo><mrow id="S2.Ex1.m1.1.1.3.3.3.1.3" xref="S2.Ex1.m1.1.1.3.3.3.1.3.cmml"><mi id="S2.Ex1.m1.1.1.3.3.3.1.3.2" xref="S2.Ex1.m1.1.1.3.3.3.1.3.2.cmml">k</mi><mo id="S2.Ex1.m1.1.1.3.3.3.1.3.1" xref="S2.Ex1.m1.1.1.3.3.3.1.3.1.cmml">∈</mo><msub id="S2.Ex1.m1.1.1.3.3.3.1.3.3" xref="S2.Ex1.m1.1.1.3.3.3.1.3.3.cmml"><mi id="S2.Ex1.m1.1.1.3.3.3.1.3.3.2" xref="S2.Ex1.m1.1.1.3.3.3.1.3.3.2.cmml">S</mi><mi id="S2.Ex1.m1.1.1.3.3.3.1.3.3.3" xref="S2.Ex1.m1.1.1.3.3.3.1.3.3.3.cmml">t</mi></msub></mrow></munder><mrow id="S2.Ex1.m1.1.1.3.3.3.2" xref="S2.Ex1.m1.1.1.3.3.3.2.cmml"><msub id="S2.Ex1.m1.1.1.3.3.3.2.2" xref="S2.Ex1.m1.1.1.3.3.3.2.2.cmml"><mi id="S2.Ex1.m1.1.1.3.3.3.2.2.2" xref="S2.Ex1.m1.1.1.3.3.3.2.2.2.cmml">n</mi><mi id="S2.Ex1.m1.1.1.3.3.3.2.2.3" xref="S2.Ex1.m1.1.1.3.3.3.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.1.1.3.3.3.2.1" xref="S2.Ex1.m1.1.1.3.3.3.2.1.cmml">​</mo><mi mathvariant="normal" id="S2.Ex1.m1.1.1.3.3.3.2.3" xref="S2.Ex1.m1.1.1.3.3.3.2.3.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.1.1.3.3.3.2.1a" xref="S2.Ex1.m1.1.1.3.3.3.2.1.cmml">​</mo><msubsup id="S2.Ex1.m1.1.1.3.3.3.2.4" xref="S2.Ex1.m1.1.1.3.3.3.2.4.cmml"><mi id="S2.Ex1.m1.1.1.3.3.3.2.4.2.2" xref="S2.Ex1.m1.1.1.3.3.3.2.4.2.2.cmml">w</mi><mi id="S2.Ex1.m1.1.1.3.3.3.2.4.2.3" xref="S2.Ex1.m1.1.1.3.3.3.2.4.2.3.cmml">t</mi><mi id="S2.Ex1.m1.1.1.3.3.3.2.4.3" xref="S2.Ex1.m1.1.1.3.3.3.2.4.3.cmml">k</mi></msubsup></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.1b"><apply id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.1.1"><eq id="S2.Ex1.m1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1"></eq><apply id="S2.Ex1.m1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.2.1.cmml" xref="S2.Ex1.m1.1.1.2">subscript</csymbol><ci id="S2.Ex1.m1.1.1.2.2.cmml" xref="S2.Ex1.m1.1.1.2.2">𝑤</ci><ci id="S2.Ex1.m1.1.1.2.3.cmml" xref="S2.Ex1.m1.1.1.2.3">𝑡</ci></apply><apply id="S2.Ex1.m1.1.1.3.cmml" xref="S2.Ex1.m1.1.1.3"><plus id="S2.Ex1.m1.1.1.3.1.cmml" xref="S2.Ex1.m1.1.1.3.1"></plus><apply id="S2.Ex1.m1.1.1.3.2.cmml" xref="S2.Ex1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.3.2.1.cmml" xref="S2.Ex1.m1.1.1.3.2">subscript</csymbol><ci id="S2.Ex1.m1.1.1.3.2.2.cmml" xref="S2.Ex1.m1.1.1.3.2.2">𝑤</ci><apply id="S2.Ex1.m1.1.1.3.2.3.cmml" xref="S2.Ex1.m1.1.1.3.2.3"><minus id="S2.Ex1.m1.1.1.3.2.3.1.cmml" xref="S2.Ex1.m1.1.1.3.2.3.1"></minus><ci id="S2.Ex1.m1.1.1.3.2.3.2.cmml" xref="S2.Ex1.m1.1.1.3.2.3.2">𝑡</ci><cn type="integer" id="S2.Ex1.m1.1.1.3.2.3.3.cmml" xref="S2.Ex1.m1.1.1.3.2.3.3">1</cn></apply></apply><apply id="S2.Ex1.m1.1.1.3.3.cmml" xref="S2.Ex1.m1.1.1.3.3"><times id="S2.Ex1.m1.1.1.3.3.1.cmml" xref="S2.Ex1.m1.1.1.3.3.1"></times><apply id="S2.Ex1.m1.1.1.3.3.2.cmml" xref="S2.Ex1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.3.3.2.1.cmml" xref="S2.Ex1.m1.1.1.3.3.2">subscript</csymbol><ci id="S2.Ex1.m1.1.1.3.3.2.2.cmml" xref="S2.Ex1.m1.1.1.3.3.2.2">𝜂</ci><ci id="S2.Ex1.m1.1.1.3.3.2.3.cmml" xref="S2.Ex1.m1.1.1.3.3.2.3">𝑡</ci></apply><apply id="S2.Ex1.m1.1.1.3.3.3.cmml" xref="S2.Ex1.m1.1.1.3.3.3"><apply id="S2.Ex1.m1.1.1.3.3.3.1.cmml" xref="S2.Ex1.m1.1.1.3.3.3.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.3.3.3.1.1.cmml" xref="S2.Ex1.m1.1.1.3.3.3.1">subscript</csymbol><sum id="S2.Ex1.m1.1.1.3.3.3.1.2.cmml" xref="S2.Ex1.m1.1.1.3.3.3.1.2"></sum><apply id="S2.Ex1.m1.1.1.3.3.3.1.3.cmml" xref="S2.Ex1.m1.1.1.3.3.3.1.3"><in id="S2.Ex1.m1.1.1.3.3.3.1.3.1.cmml" xref="S2.Ex1.m1.1.1.3.3.3.1.3.1"></in><ci id="S2.Ex1.m1.1.1.3.3.3.1.3.2.cmml" xref="S2.Ex1.m1.1.1.3.3.3.1.3.2">𝑘</ci><apply id="S2.Ex1.m1.1.1.3.3.3.1.3.3.cmml" xref="S2.Ex1.m1.1.1.3.3.3.1.3.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.3.3.3.1.3.3.1.cmml" xref="S2.Ex1.m1.1.1.3.3.3.1.3.3">subscript</csymbol><ci id="S2.Ex1.m1.1.1.3.3.3.1.3.3.2.cmml" xref="S2.Ex1.m1.1.1.3.3.3.1.3.3.2">𝑆</ci><ci id="S2.Ex1.m1.1.1.3.3.3.1.3.3.3.cmml" xref="S2.Ex1.m1.1.1.3.3.3.1.3.3.3">𝑡</ci></apply></apply></apply><apply id="S2.Ex1.m1.1.1.3.3.3.2.cmml" xref="S2.Ex1.m1.1.1.3.3.3.2"><times id="S2.Ex1.m1.1.1.3.3.3.2.1.cmml" xref="S2.Ex1.m1.1.1.3.3.3.2.1"></times><apply id="S2.Ex1.m1.1.1.3.3.3.2.2.cmml" xref="S2.Ex1.m1.1.1.3.3.3.2.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.3.3.3.2.2.1.cmml" xref="S2.Ex1.m1.1.1.3.3.3.2.2">subscript</csymbol><ci id="S2.Ex1.m1.1.1.3.3.3.2.2.2.cmml" xref="S2.Ex1.m1.1.1.3.3.3.2.2.2">𝑛</ci><ci id="S2.Ex1.m1.1.1.3.3.3.2.2.3.cmml" xref="S2.Ex1.m1.1.1.3.3.3.2.2.3">𝑘</ci></apply><ci id="S2.Ex1.m1.1.1.3.3.3.2.3.cmml" xref="S2.Ex1.m1.1.1.3.3.3.2.3">Δ</ci><apply id="S2.Ex1.m1.1.1.3.3.3.2.4.cmml" xref="S2.Ex1.m1.1.1.3.3.3.2.4"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.3.3.3.2.4.1.cmml" xref="S2.Ex1.m1.1.1.3.3.3.2.4">superscript</csymbol><apply id="S2.Ex1.m1.1.1.3.3.3.2.4.2.cmml" xref="S2.Ex1.m1.1.1.3.3.3.2.4"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.3.3.3.2.4.2.1.cmml" xref="S2.Ex1.m1.1.1.3.3.3.2.4">subscript</csymbol><ci id="S2.Ex1.m1.1.1.3.3.3.2.4.2.2.cmml" xref="S2.Ex1.m1.1.1.3.3.3.2.4.2.2">𝑤</ci><ci id="S2.Ex1.m1.1.1.3.3.3.2.4.2.3.cmml" xref="S2.Ex1.m1.1.1.3.3.3.2.4.2.3">𝑡</ci></apply><ci id="S2.Ex1.m1.1.1.3.3.3.2.4.3.cmml" xref="S2.Ex1.m1.1.1.3.3.3.2.4.3">𝑘</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.1c">w_{t}=w_{t-1}+\eta_{t}\sum_{k\in S_{t}}n_{k}\Delta w_{t}^{k}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>

</div>
<div id="alg1.l12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l12.1.1.1" class="ltx_text" style="font-size:90%;">12:</span></span><span id="alg1.l12.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l12.3" class="ltx_text ltx_font_bold">for</span>
</div>
</div>
</figure>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">However, FL still faces challenges related to heterogeneous data distribution. Data may be non-independent and identically distributed (non-IID) across clients, leading to poor model convergence and performance. Recent work in FL has focused on improving gradient descent to stabilize training <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>); Karimireddy et al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>); Yu et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite>; personalizing model weights to enhance performance on downstream tasks <cite class="ltx_cite ltx_citemacro_cite">Deng et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>); Tan et al. (<a href="#bib.bib32" title="" class="ltx_ref">2022</a>); Yu et al. (<a href="#bib.bib39" title="" class="ltx_ref">2022b</a>, <a href="#bib.bib38" title="" class="ltx_ref">a</a>)</cite>; and employing model compression techniques like knowledge distillation, dynamic dropout, and adaptive pruning to reduce overfitting on non-IID datasets and improve communication efficiency  <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>); Yu et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>); Lin et al. (<a href="#bib.bib15" title="" class="ltx_ref">2020</a>); Yu et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>); Lin et al. (<a href="#bib.bib15" title="" class="ltx_ref">2020</a>); Yu et al. (<a href="#bib.bib38" title="" class="ltx_ref">2022a</a>, <a href="#bib.bib41" title="" class="ltx_ref">c</a>)</cite>.
Despite these advances, there remains a gap between traditional model training and FL, particularly in terms of performance when dealing with heterogeneous data distributions.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">2.2.   Foundation Models</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Foundation Models (FMs), such as the GPT family <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>); Radford et al. (<a href="#bib.bib29" title="" class="ltx_ref">2019</a>)</cite>, ViT <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>, CLIP <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite>, and BERT <cite class="ltx_cite ltx_citemacro_cite">Kenton and Toutanova (<a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>, have become a driving force in AI, serving as the basis for various downstream tasks. These models are trained on massive datasets and demonstrate remarkable capabilities across multiple domains.
The lifespan of FMs typically includes pre-training, fine-tuning, and application. Pre-training involves unsupervised or self-supervised learning on large-scale datasets, while fine-tuning adapts the models to specialized tasks. For example, GPT <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>); Radford et al. (<a href="#bib.bib29" title="" class="ltx_ref">2019</a>); OpenAI (<a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite> models learn grammar, syntax, and semantics during pre-training, enabling them to be easily fine-tuned for tasks such as text classification, sentiment analysis, translation, and summarization.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">In the application stage, FMs show extraordinary adaptability to downstream tasks using zero-shot learning. Prompt Engineering, an emerging research area, explores this potential by optimizing the interaction between users and FMs through carefully crafted prompts, thereby improving performance on downstream tasks. Various methods for prompt engineering have been proposed, including prompt templates <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib37" title="" class="ltx_ref">2021</a>)</cite>, prompt tuning and instruction tuning <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib37" title="" class="ltx_ref">2021</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Lester et al. (<a href="#bib.bib13" title="" class="ltx_ref">2021</a>); Han et al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite>, automated prompt generating <cite class="ltx_cite ltx_citemacro_cite">Zhou et al. (<a href="#bib.bib46" title="" class="ltx_ref">2022</a>); Sanh et al. (<a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>, and in-context learning <cite class="ltx_cite ltx_citemacro_cite">Min et al. (<a href="#bib.bib24" title="" class="ltx_ref">2021</a>, <a href="#bib.bib25" title="" class="ltx_ref">2022</a>); Rubin et al. (<a href="#bib.bib30" title="" class="ltx_ref">2021</a>); Liu et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>. These approaches enable FMs to learn from examples or instructions supplied as part of the input without the need for explicit fine-tuning or labeled examples.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">In summary, the combination of Federated Learning and Foundation Models offers great opportunities to revolutionize the AI landscape by leveraging the strengths of both paradigms. This intersection opens up numerous research directions and applications in areas such as personalized recommendations, natural language understanding, healthcare, finance, and more. As AI researchers continue to explore Federated Foundation Models, we expect to see innovative solutions and breakthroughs that lead to more robust, efficient, and ethical AI systems serving the needs of individuals and society.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of the Federated Foundation Model with Traditional FM Optimization</figcaption>
<div id="S2.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:364.2pt;height:288.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-40.9pt,32.4pt) scale(0.816582969845777,0.816582969845777) ;">
<table id="S2.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1.1" class="ltx_td ltx_border_t"></td>
<th id="S2.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">
<table id="S2.T1.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.1.1.2.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.1.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Federated</span></td>
</tr>
<tr id="S2.T1.1.1.1.1.2.1.2" class="ltx_tr">
<td id="S2.T1.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.1.1.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">Foundation Model</span></td>
</tr>
</table>
</th>
<th id="S2.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">
<table id="S2.T1.1.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.1.1.3.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.1.1.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">Traditional</span></td>
</tr>
<tr id="S2.T1.1.1.1.1.3.1.2" class="ltx_tr">
<td id="S2.T1.1.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.1.1.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">FM Optimization</span></td>
</tr>
</table>
</th>
</tr>
<tr id="S2.T1.1.1.2.2" class="ltx_tr">
<td id="S2.T1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.2.2.1.1" class="ltx_text">Data Privacy</span></td>
<td id="S2.T1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.2.2.2.1" class="ltx_text">Privacy-preserve</span></td>
<td id="S2.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.2.2.3.1" class="ltx_text">✓</span></td>
<td id="S2.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.2.2.4.1" class="ltx_text">Centralized Data Collection</span></td>
<td id="S2.T1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.2.2.5.1" class="ltx_text">✗</span></td>
</tr>
<tr id="S2.T1.1.1.3.3" class="ltx_tr">
<td id="S2.T1.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t">
<table id="S2.T1.1.1.3.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.3.3.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.3.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Communication</td>
</tr>
<tr id="S2.T1.1.1.3.3.1.1.2" class="ltx_tr">
<td id="S2.T1.1.1.3.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Overhead</td>
</tr>
</table>
</td>
<td id="S2.T1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">Communicate Model Updates</td>
<td id="S2.T1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S2.T1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">Communicate Data to Central Server</td>
<td id="S2.T1.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">✗</td>
</tr>
<tr id="S2.T1.1.1.4.4" class="ltx_tr">
<td id="S2.T1.1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_t">
<table id="S2.T1.1.1.4.4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.4.4.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.4.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Model</td>
</tr>
<tr id="S2.T1.1.1.4.4.1.1.2" class="ltx_tr">
<td id="S2.T1.1.1.4.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Performance</td>
</tr>
</table>
</td>
<td id="S2.T1.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t">Diverse Data Improvement</td>
<td id="S2.T1.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S2.T1.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">Lacks Diversity</td>
<td id="S2.T1.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">✗</td>
</tr>
<tr id="S2.T1.1.1.5.5" class="ltx_tr">
<td id="S2.T1.1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_t">
<table id="S2.T1.1.1.5.5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.5.5.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.5.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Resource</td>
</tr>
<tr id="S2.T1.1.1.5.5.1.1.2" class="ltx_tr">
<td id="S2.T1.1.1.5.5.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Distribution</td>
</tr>
</table>
</td>
<td id="S2.T1.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">Distributed Across Devices</td>
<td id="S2.T1.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S2.T1.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">Centralized</td>
<td id="S2.T1.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">✗</td>
</tr>
<tr id="S2.T1.1.1.6.6" class="ltx_tr">
<td id="S2.T1.1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_t">
<table id="S2.T1.1.1.6.6.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.6.6.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.6.6.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Data</td>
</tr>
<tr id="S2.T1.1.1.6.6.1.1.2" class="ltx_tr">
<td id="S2.T1.1.1.6.6.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Efficiency</td>
</tr>
</table>
</td>
<td id="S2.T1.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_t">Better with data diversity</td>
<td id="S2.T1.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S2.T1.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t">
<table id="S2.T1.1.1.6.6.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.6.6.4.1.1" class="ltx_tr">
<td id="S2.T1.1.1.6.6.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Requires more data for</td>
</tr>
<tr id="S2.T1.1.1.6.6.4.1.2" class="ltx_tr">
<td id="S2.T1.1.1.6.6.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">similar performance</td>
</tr>
</table>
</td>
<td id="S2.T1.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_t">✗</td>
</tr>
<tr id="S2.T1.1.1.7.7" class="ltx_tr">
<td id="S2.T1.1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.7.7.1.1" class="ltx_text">Latency</span></td>
<td id="S2.T1.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.7.7.2.1" class="ltx_text">Distributed Computation</span></td>
<td id="S2.T1.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.7.7.3.1" class="ltx_text">✗</span></td>
<td id="S2.T1.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.7.7.4.1" class="ltx_text">Lower with Centralized Computation</span></td>
<td id="S2.T1.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.7.7.5.1" class="ltx_text">✓</span></td>
</tr>
<tr id="S2.T1.1.1.8.8" class="ltx_tr">
<td id="S2.T1.1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_t">
<table id="S2.T1.1.1.8.8.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.8.8.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.8.8.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">System</td>
</tr>
<tr id="S2.T1.1.1.8.8.1.1.2" class="ltx_tr">
<td id="S2.T1.1.1.8.8.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Complexity</td>
</tr>
</table>
</td>
<td id="S2.T1.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_t">Distributed Coordination</td>
<td id="S2.T1.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_t">✗</td>
<td id="S2.T1.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_t">Centrally Managed</td>
<td id="S2.T1.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_t">✓</td>
</tr>
<tr id="S2.T1.1.1.9.9" class="ltx_tr">
<td id="S2.T1.1.1.9.9.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.9.9.1.1" class="ltx_text">Scalability</span></td>
<td id="S2.T1.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.9.9.2.1" class="ltx_text">Scalable to Many Clients</span></td>
<td id="S2.T1.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.9.9.3.1" class="ltx_text">✓</span></td>
<td id="S2.T1.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.9.9.4.1" class="ltx_text">Unscalable with Large Datasets</span></td>
<td id="S2.T1.1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.9.9.5.1" class="ltx_text">✗</span></td>
</tr>
<tr id="S2.T1.1.1.10.10" class="ltx_tr">
<td id="S2.T1.1.1.10.10.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.10.10.1.1" class="ltx_text">Consistency</span></td>
<td id="S2.T1.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.10.10.2.1" class="ltx_text">
<span id="S2.T1.1.1.10.10.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.1.1.10.10.2.1.1.1" class="ltx_tr">
<span id="S2.T1.1.1.10.10.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Weakly Connected</span></span>
<span id="S2.T1.1.1.10.10.2.1.1.2" class="ltx_tr">
<span id="S2.T1.1.1.10.10.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Collaborative Learning</span></span>
</span></span></td>
<td id="S2.T1.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.10.10.3.1" class="ltx_text">✗</span></td>
<td id="S2.T1.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.10.10.4.1" class="ltx_text">
<span id="S2.T1.1.1.10.10.4.1.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.1.1.10.10.4.1.1.1" class="ltx_tr">
<span id="S2.T1.1.1.10.10.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Consistent Updates in</span></span>
<span id="S2.T1.1.1.10.10.4.1.1.2" class="ltx_tr">
<span id="S2.T1.1.1.10.10.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Controlled Environment</span></span>
</span></span></td>
<td id="S2.T1.1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.10.10.5.1" class="ltx_text">✓</span></td>
</tr>
<tr id="S2.T1.1.1.11.11" class="ltx_tr">
<td id="S2.T1.1.1.11.11.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">
<table id="S2.T1.1.1.11.11.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.11.11.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.11.11.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Ease of</td>
</tr>
<tr id="S2.T1.1.1.11.11.1.1.2" class="ltx_tr">
<td id="S2.T1.1.1.11.11.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Deployment</td>
</tr>
</table>
</td>
<td id="S2.T1.1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">Challenging</td>
<td id="S2.T1.1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">✗</td>
<td id="S2.T1.1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">Easier</td>
<td id="S2.T1.1.1.11.11.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">✓</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.   Motivation for Federated Foundation Models</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we discuss the various challenges that motivate the development of Federated Foundation Models (FFMs), covering aspects such as data privacy, model performance, communication cost, scalability, deployment, personalization and real-time adaptation, and bias reduction. As shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, These existing challenges highlight the potential advantages of combining Foundation Models (FMs) and Federated Learning (FL) for a wide range of applications and scenarios.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Data privacy.</span>
The widespread deployment of AI in society generates vast amounts of data (e.g., images collected by cameras in smartphone applications, prompt dialog produced by users), presenting potential resources for optimizing and specializing FMs.
However, privacy concerns have limited the use of private data for FM optimization.
FFMs offer significant improvements in data privacy by incorporating FL, enabling FM optimization on private data.
By optimizing FM tasks (e.g., pre-training, fine-tuning, and prompt tuning) on local data without sharing raw information, FFMs comply with data protection regulations and preserve user privacy. This approach is particularly beneficial when sensitive data, such as medical records or personal communications, must be used to improve model performance without compromising confidentiality.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Model performance.</span>
Combining FMs and FL provides benefits to FMs, boosting their performance. FMs gain access to a broader range of data for optimization tasks such as fine-tuning, prompt tuning, and pre-training. This expanded data access enables the development of more accurate and efficient AI systems better suited for users in diverse scenarios. This combination benefits FL, as well. FL can overcome challenges associated with Non-IID (Non-Identical Independent Distributed) and biased data <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib45" title="" class="ltx_ref">2018</a>)</cite> by leveraging the advanced capabilities of FMs, leading to improved performance across different tasks and domains.</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">Cost.</span>
FFMs reduce communication costs by sharing only model updates between devices and the central server, significantly saving bandwidth and communication costs for transmitting raw data. Additionally, FFMs can potentially reduce the labor cost associated with collecting and managing data in a central location, as data is generated and used locally at edge devices. This efficiency makes FFMs a more practical and cost-effective solution for training and deploying FMs.</p>
</div>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p id="S3.p5.1" class="ltx_p"><span id="S3.p5.1.1" class="ltx_text ltx_font_bold">Scalability.</span>
Current FMs, especially large language models, often face scalability limitations due to limited computational power at the edge. Many FMs are run centrally and provide API access for users, which can lead to capacity constraints and API congestion. In the near future, advancements in computational power may enable FMs to run locally on edge devices. FL’s scalable nature makes it an ideal framework for combining with FMs, accommodating numerous devices with varying computational capabilities. By integrating FL principles, FMs can leverage advancements in computational power, becoming more scalable and enabling broader deployment and improved performance across various tasks and domains.</p>
</div>
<div id="S3.p6" class="ltx_para ltx_noindent">
<p id="S3.p6.1" class="ltx_p"><span id="S3.p6.1.1" class="ltx_text ltx_font_bold">Deployment.</span>
FFMs offer potential advantages in deployment, particularly in reducing latency and enhancing user experience. Running FMs centrally with API access for users can result in latency issues due to network communication between the user’s device and the central server hosting the model. In contrast, FFMs can be deployed and run locally on edge devices, potentially reducing latency by eliminating network communication. This allows for faster response times and a more seamless user experience when interacting with the model. However, available computational resources on edge devices must be considered when deploying FMs locally. As discussed in the Scalability section, advancements in computational power will be crucial for enabling local deployment on a wide range of devices, ensuring efficient and effective performance across various tasks and domains.</p>
</div>
<div id="S3.p7" class="ltx_para ltx_noindent">
<p id="S3.p7.1" class="ltx_p"><span id="S3.p7.1.1" class="ltx_text ltx_font_bold">Personalization and real-time adaptation.</span>
FFMs facilitate a high degree of personalization by leveraging the decentralized nature of FL. By training on diverse, user-generated data, FMs can be tailored to individual preferences and requirements, offering more personalized and context-aware solutions across various tasks and domains.
A key advantage of FFMs is their ability to adapt in real-time as new personalized data becomes available from edge devices. This continuous learning capability ensures that the models remain up-to-date with users’ evolving needs and preferences, further enhancing their personalization.
The focus on personalization in FFMs leads to improved performance and greater user satisfaction. By providing AI solutions that dynamically adapt to user-specific needs, FFMs enable more effective and engaging user experiences across a wide range of applications and domains.</p>
</div>
<div id="S3.p8" class="ltx_para ltx_noindent">
<p id="S3.p8.1" class="ltx_p"><span id="S3.p8.1.1" class="ltx_text ltx_font_bold">Bias reduction.</span>
FFMs contribute to bias reduction in AI systems by incorporating diverse data from decentralized sources, resulting in more inclusive and fair AI solutions. The models learn from various users, increasing their awareness of the nuances and complexities of real-world scenarios, and leading to more informed and less biased decisions across tasks and domains.
Additionally, the privacy-preserving nature of FL encourages more users to participate in the training process, further diversifying the data and knowledge incorporated into FMs. This results in models better equipped to handle and minimize biases, providing fairer and more equitable AI solutions for all users.</p>
</div>
<div id="S3.p9" class="ltx_para ltx_noindent">
<p id="S3.p9.1" class="ltx_p"><span id="S3.p9.1.1" class="ltx_text ltx_font_bold">Continual/Lifelong learning.</span>
FMs combined with FL provide an ideal platform for continual lifelong learning. This combination facilitates the continuous adaptation and improvement of models by harnessing decentralized and diverse data sources, leading to more versatile and effective AI systems. As advancements in edge computing power become more prevalent, the realization of continual lifelong learning in FMs will soon be within reach. This progress will enable AI models to learn and grow throughout their lifespan, unlocking new possibilities for AI research and practical applications in various domains. By embracing continual lifelong learning, FFMs can help create more adaptive, efficient, and personalized AI systems that can dynamically adjust to user-specific needs and preferences, ultimately benefiting users from all walks of life.</p>
</div>
<div id="S3.p10" class="ltx_para">
<p id="S3.p10.1" class="ltx_p">In summary, FFMs offer a promising approach to address many challenges and limitations associated with traditional, centralized machine learning. By integrating FL into FM optimization, we can create more efficient, personalized, privacy-preserving, and inclusive AI systems. This opens up new possibilities for AI research and practical applications, making AI more accessible and beneficial to users from all walks of life.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2305.11414/assets/x1.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="135" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Federated Foundation Model tasks: The FFM centralized optimization process aggregates local models and updates them using public data. Private clients download up-to-date global model parameters from the server, optimize the FM locally on their tasks, and send the optimized model back to the server. </figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.   Federated Foundation Model: Prospective and Future Research</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we discuss potential future research directions and general challenges related to FFMs, covering but not limited:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Federated foundation model pre-training</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Federated foundation model fine-tuning</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Federated prompt tuning</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Federated continual (lifelong) learning</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p">Federated retrieval augmented generation</p>
</div>
</li>
<li id="S4.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i6.p1" class="ltx_para">
<p id="S4.I1.i6.p1.1" class="ltx_p">General challenges</p>
</div>
</li>
<li id="S4.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i7.p1" class="ltx_para">
<p id="S4.I1.i7.p1.1" class="ltx_p">Other future research directions</p>
</div>
</li>
</ul>
<p id="S4.p1.2" class="ltx_p">We scrutinize the distinct characteristics and prerequisites of these tasks, spotlighting the opportunities and hurdles encountered when employing FFMs to address real-world issues. Our aim is to build a robust foundation for comprehending the breadth and potential of this emerging paradigm, thereby fostering further research and development.
As mentioned in Section <a href="#S3" title="3. Motivation for Federated Foundation Models ‣ Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, some tasks may not be feasible until computational power at the edge advances further.</p>
</div>
<figure id="alg2" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg2.2.1.1" class="ltx_text ltx_font_bold">Algorithm 2</span> </span> General FFM Optimization process</figcaption>
<div id="alg2.3" class="ltx_listing ltx_listing">
<div id="alg2.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg2.l1.1.1.1" class="ltx_text" style="font-size:90%;">1:</span></span><span id="alg2.l1.2" class="ltx_text ltx_font_bold">Input:</span> Global AI model <math id="alg2.l1.m1.1" class="ltx_Math" alttext="w_{0}" display="inline"><semantics id="alg2.l1.m1.1a"><msub id="alg2.l1.m1.1.1" xref="alg2.l1.m1.1.1.cmml"><mi id="alg2.l1.m1.1.1.2" xref="alg2.l1.m1.1.1.2.cmml">w</mi><mn id="alg2.l1.m1.1.1.3" xref="alg2.l1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="alg2.l1.m1.1b"><apply id="alg2.l1.m1.1.1.cmml" xref="alg2.l1.m1.1.1"><csymbol cd="ambiguous" id="alg2.l1.m1.1.1.1.cmml" xref="alg2.l1.m1.1.1">subscript</csymbol><ci id="alg2.l1.m1.1.1.2.cmml" xref="alg2.l1.m1.1.1.2">𝑤</ci><cn type="integer" id="alg2.l1.m1.1.1.3.cmml" xref="alg2.l1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l1.m1.1c">w_{0}</annotation></semantics></math>, clients <math id="alg2.l1.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="alg2.l1.m2.1a"><mi id="alg2.l1.m2.1.1" xref="alg2.l1.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="alg2.l1.m2.1b"><ci id="alg2.l1.m2.1.1.cmml" xref="alg2.l1.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.l1.m2.1c">S</annotation></semantics></math>, communication rounds <math id="alg2.l1.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="alg2.l1.m3.1a"><mi id="alg2.l1.m3.1.1" xref="alg2.l1.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="alg2.l1.m3.1b"><ci id="alg2.l1.m3.1.1.cmml" xref="alg2.l1.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.l1.m3.1c">T</annotation></semantics></math>

</div>
<div id="alg2.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg2.l2.1.1.1" class="ltx_text" style="font-size:90%;">2:</span></span>Server initialize global model <math id="alg2.l2.m1.1" class="ltx_Math" alttext="w_{0}" display="inline"><semantics id="alg2.l2.m1.1a"><msub id="alg2.l2.m1.1.1" xref="alg2.l2.m1.1.1.cmml"><mi id="alg2.l2.m1.1.1.2" xref="alg2.l2.m1.1.1.2.cmml">w</mi><mn id="alg2.l2.m1.1.1.3" xref="alg2.l2.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="alg2.l2.m1.1b"><apply id="alg2.l2.m1.1.1.cmml" xref="alg2.l2.m1.1.1"><csymbol cd="ambiguous" id="alg2.l2.m1.1.1.1.cmml" xref="alg2.l2.m1.1.1">subscript</csymbol><ci id="alg2.l2.m1.1.1.2.cmml" xref="alg2.l2.m1.1.1.2">𝑤</ci><cn type="integer" id="alg2.l2.m1.1.1.3.cmml" xref="alg2.l2.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l2.m1.1c">w_{0}</annotation></semantics></math>

</div>
<div id="alg2.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg2.l3.1.1.1" class="ltx_text" style="font-size:90%;">3:</span></span><span id="alg2.l3.2" class="ltx_text ltx_font_bold">for</span> <math id="alg2.l3.m1.4" class="ltx_Math" alttext="t=1,2,\ldots,T" display="inline"><semantics id="alg2.l3.m1.4a"><mrow id="alg2.l3.m1.4.5" xref="alg2.l3.m1.4.5.cmml"><mi id="alg2.l3.m1.4.5.2" xref="alg2.l3.m1.4.5.2.cmml">t</mi><mo id="alg2.l3.m1.4.5.1" xref="alg2.l3.m1.4.5.1.cmml">=</mo><mrow id="alg2.l3.m1.4.5.3.2" xref="alg2.l3.m1.4.5.3.1.cmml"><mn id="alg2.l3.m1.1.1" xref="alg2.l3.m1.1.1.cmml">1</mn><mo id="alg2.l3.m1.4.5.3.2.1" xref="alg2.l3.m1.4.5.3.1.cmml">,</mo><mn id="alg2.l3.m1.2.2" xref="alg2.l3.m1.2.2.cmml">2</mn><mo id="alg2.l3.m1.4.5.3.2.2" xref="alg2.l3.m1.4.5.3.1.cmml">,</mo><mi mathvariant="normal" id="alg2.l3.m1.3.3" xref="alg2.l3.m1.3.3.cmml">…</mi><mo id="alg2.l3.m1.4.5.3.2.3" xref="alg2.l3.m1.4.5.3.1.cmml">,</mo><mi id="alg2.l3.m1.4.4" xref="alg2.l3.m1.4.4.cmml">T</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg2.l3.m1.4b"><apply id="alg2.l3.m1.4.5.cmml" xref="alg2.l3.m1.4.5"><eq id="alg2.l3.m1.4.5.1.cmml" xref="alg2.l3.m1.4.5.1"></eq><ci id="alg2.l3.m1.4.5.2.cmml" xref="alg2.l3.m1.4.5.2">𝑡</ci><list id="alg2.l3.m1.4.5.3.1.cmml" xref="alg2.l3.m1.4.5.3.2"><cn type="integer" id="alg2.l3.m1.1.1.cmml" xref="alg2.l3.m1.1.1">1</cn><cn type="integer" id="alg2.l3.m1.2.2.cmml" xref="alg2.l3.m1.2.2">2</cn><ci id="alg2.l3.m1.3.3.cmml" xref="alg2.l3.m1.3.3">…</ci><ci id="alg2.l3.m1.4.4.cmml" xref="alg2.l3.m1.4.4">𝑇</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l3.m1.4c">t=1,2,\ldots,T</annotation></semantics></math> <span id="alg2.l3.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg2.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg2.l4.1.1.1" class="ltx_text" style="font-size:90%;">4:</span></span>     <span id="alg2.l4.2" class="ltx_text ltx_font_bold">if</span> Public data available <span id="alg2.l4.3" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg2.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg2.l5.1.1.1" class="ltx_text" style="font-size:90%;">5:</span></span>         Server optimize <math id="alg2.l5.m1.1" class="ltx_Math" alttext="w_{t-1}" display="inline"><semantics id="alg2.l5.m1.1a"><msub id="alg2.l5.m1.1.1" xref="alg2.l5.m1.1.1.cmml"><mi id="alg2.l5.m1.1.1.2" xref="alg2.l5.m1.1.1.2.cmml">w</mi><mrow id="alg2.l5.m1.1.1.3" xref="alg2.l5.m1.1.1.3.cmml"><mi id="alg2.l5.m1.1.1.3.2" xref="alg2.l5.m1.1.1.3.2.cmml">t</mi><mo id="alg2.l5.m1.1.1.3.1" xref="alg2.l5.m1.1.1.3.1.cmml">−</mo><mn id="alg2.l5.m1.1.1.3.3" xref="alg2.l5.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="alg2.l5.m1.1b"><apply id="alg2.l5.m1.1.1.cmml" xref="alg2.l5.m1.1.1"><csymbol cd="ambiguous" id="alg2.l5.m1.1.1.1.cmml" xref="alg2.l5.m1.1.1">subscript</csymbol><ci id="alg2.l5.m1.1.1.2.cmml" xref="alg2.l5.m1.1.1.2">𝑤</ci><apply id="alg2.l5.m1.1.1.3.cmml" xref="alg2.l5.m1.1.1.3"><minus id="alg2.l5.m1.1.1.3.1.cmml" xref="alg2.l5.m1.1.1.3.1"></minus><ci id="alg2.l5.m1.1.1.3.2.cmml" xref="alg2.l5.m1.1.1.3.2">𝑡</ci><cn type="integer" id="alg2.l5.m1.1.1.3.3.cmml" xref="alg2.l5.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l5.m1.1c">w_{t-1}</annotation></semantics></math> on public data 

</div>
<div id="alg2.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg2.l6.1.1.1" class="ltx_text" style="font-size:90%;">6:</span></span>     <span id="alg2.l6.2" class="ltx_text ltx_font_bold">end</span> <span id="alg2.l6.3" class="ltx_text ltx_font_bold">if</span>
</div>
<div id="alg2.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg2.l7.1.1.1" class="ltx_text" style="font-size:90%;">7:</span></span>     Server send global model <math id="alg2.l7.m1.1" class="ltx_Math" alttext="w_{t-1}" display="inline"><semantics id="alg2.l7.m1.1a"><msub id="alg2.l7.m1.1.1" xref="alg2.l7.m1.1.1.cmml"><mi id="alg2.l7.m1.1.1.2" xref="alg2.l7.m1.1.1.2.cmml">w</mi><mrow id="alg2.l7.m1.1.1.3" xref="alg2.l7.m1.1.1.3.cmml"><mi id="alg2.l7.m1.1.1.3.2" xref="alg2.l7.m1.1.1.3.2.cmml">t</mi><mo id="alg2.l7.m1.1.1.3.1" xref="alg2.l7.m1.1.1.3.1.cmml">−</mo><mn id="alg2.l7.m1.1.1.3.3" xref="alg2.l7.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="alg2.l7.m1.1b"><apply id="alg2.l7.m1.1.1.cmml" xref="alg2.l7.m1.1.1"><csymbol cd="ambiguous" id="alg2.l7.m1.1.1.1.cmml" xref="alg2.l7.m1.1.1">subscript</csymbol><ci id="alg2.l7.m1.1.1.2.cmml" xref="alg2.l7.m1.1.1.2">𝑤</ci><apply id="alg2.l7.m1.1.1.3.cmml" xref="alg2.l7.m1.1.1.3"><minus id="alg2.l7.m1.1.1.3.1.cmml" xref="alg2.l7.m1.1.1.3.1"></minus><ci id="alg2.l7.m1.1.1.3.2.cmml" xref="alg2.l7.m1.1.1.3.2">𝑡</ci><cn type="integer" id="alg2.l7.m1.1.1.3.3.cmml" xref="alg2.l7.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l7.m1.1c">w_{t-1}</annotation></semantics></math> to participate clients <math id="alg2.l7.m2.1" class="ltx_Math" alttext="\in S" display="inline"><semantics id="alg2.l7.m2.1a"><mrow id="alg2.l7.m2.1.1" xref="alg2.l7.m2.1.1.cmml"><mi id="alg2.l7.m2.1.1.2" xref="alg2.l7.m2.1.1.2.cmml"></mi><mo id="alg2.l7.m2.1.1.1" xref="alg2.l7.m2.1.1.1.cmml">∈</mo><mi id="alg2.l7.m2.1.1.3" xref="alg2.l7.m2.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="alg2.l7.m2.1b"><apply id="alg2.l7.m2.1.1.cmml" xref="alg2.l7.m2.1.1"><in id="alg2.l7.m2.1.1.1.cmml" xref="alg2.l7.m2.1.1.1"></in><csymbol cd="latexml" id="alg2.l7.m2.1.1.2.cmml" xref="alg2.l7.m2.1.1.2">absent</csymbol><ci id="alg2.l7.m2.1.1.3.cmml" xref="alg2.l7.m2.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l7.m2.1c">\in S</annotation></semantics></math>

</div>
<div id="alg2.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg2.l8.1.1.1" class="ltx_text" style="font-size:90%;">8:</span></span>     <span id="alg2.l8.2" class="ltx_text ltx_font_bold">for</span> each client <math id="alg2.l8.m1.1" class="ltx_Math" alttext="k\in S" display="inline"><semantics id="alg2.l8.m1.1a"><mrow id="alg2.l8.m1.1.1" xref="alg2.l8.m1.1.1.cmml"><mi id="alg2.l8.m1.1.1.2" xref="alg2.l8.m1.1.1.2.cmml">k</mi><mo id="alg2.l8.m1.1.1.1" xref="alg2.l8.m1.1.1.1.cmml">∈</mo><mi id="alg2.l8.m1.1.1.3" xref="alg2.l8.m1.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="alg2.l8.m1.1b"><apply id="alg2.l8.m1.1.1.cmml" xref="alg2.l8.m1.1.1"><in id="alg2.l8.m1.1.1.1.cmml" xref="alg2.l8.m1.1.1.1"></in><ci id="alg2.l8.m1.1.1.2.cmml" xref="alg2.l8.m1.1.1.2">𝑘</ci><ci id="alg2.l8.m1.1.1.3.cmml" xref="alg2.l8.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l8.m1.1c">k\in S</annotation></semantics></math> <span id="alg2.l8.3" class="ltx_text ltx_font_bold">do</span> <span id="alg2.l8.4" class="ltx_text ltx_font_bold"> in parallel</span>

</div>
<div id="alg2.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg2.l9.1.1.1" class="ltx_text" style="font-size:90%;">9:</span></span>         Client <math id="alg2.l9.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="alg2.l9.m1.1a"><mi id="alg2.l9.m1.1.1" xref="alg2.l9.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="alg2.l9.m1.1b"><ci id="alg2.l9.m1.1.1.cmml" xref="alg2.l9.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.l9.m1.1c">k</annotation></semantics></math> optimizes <math id="alg2.l9.m2.1" class="ltx_Math" alttext="w_{t-1}" display="inline"><semantics id="alg2.l9.m2.1a"><msub id="alg2.l9.m2.1.1" xref="alg2.l9.m2.1.1.cmml"><mi id="alg2.l9.m2.1.1.2" xref="alg2.l9.m2.1.1.2.cmml">w</mi><mrow id="alg2.l9.m2.1.1.3" xref="alg2.l9.m2.1.1.3.cmml"><mi id="alg2.l9.m2.1.1.3.2" xref="alg2.l9.m2.1.1.3.2.cmml">t</mi><mo id="alg2.l9.m2.1.1.3.1" xref="alg2.l9.m2.1.1.3.1.cmml">−</mo><mn id="alg2.l9.m2.1.1.3.3" xref="alg2.l9.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="alg2.l9.m2.1b"><apply id="alg2.l9.m2.1.1.cmml" xref="alg2.l9.m2.1.1"><csymbol cd="ambiguous" id="alg2.l9.m2.1.1.1.cmml" xref="alg2.l9.m2.1.1">subscript</csymbol><ci id="alg2.l9.m2.1.1.2.cmml" xref="alg2.l9.m2.1.1.2">𝑤</ci><apply id="alg2.l9.m2.1.1.3.cmml" xref="alg2.l9.m2.1.1.3"><minus id="alg2.l9.m2.1.1.3.1.cmml" xref="alg2.l9.m2.1.1.3.1"></minus><ci id="alg2.l9.m2.1.1.3.2.cmml" xref="alg2.l9.m2.1.1.3.2">𝑡</ci><cn type="integer" id="alg2.l9.m2.1.1.3.3.cmml" xref="alg2.l9.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l9.m2.1c">w_{t-1}</annotation></semantics></math> on local data, producing <math id="alg2.l9.m3.1" class="ltx_Math" alttext="w_{t}^{k}" display="inline"><semantics id="alg2.l9.m3.1a"><msubsup id="alg2.l9.m3.1.1" xref="alg2.l9.m3.1.1.cmml"><mi id="alg2.l9.m3.1.1.2.2" xref="alg2.l9.m3.1.1.2.2.cmml">w</mi><mi id="alg2.l9.m3.1.1.2.3" xref="alg2.l9.m3.1.1.2.3.cmml">t</mi><mi id="alg2.l9.m3.1.1.3" xref="alg2.l9.m3.1.1.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="alg2.l9.m3.1b"><apply id="alg2.l9.m3.1.1.cmml" xref="alg2.l9.m3.1.1"><csymbol cd="ambiguous" id="alg2.l9.m3.1.1.1.cmml" xref="alg2.l9.m3.1.1">superscript</csymbol><apply id="alg2.l9.m3.1.1.2.cmml" xref="alg2.l9.m3.1.1"><csymbol cd="ambiguous" id="alg2.l9.m3.1.1.2.1.cmml" xref="alg2.l9.m3.1.1">subscript</csymbol><ci id="alg2.l9.m3.1.1.2.2.cmml" xref="alg2.l9.m3.1.1.2.2">𝑤</ci><ci id="alg2.l9.m3.1.1.2.3.cmml" xref="alg2.l9.m3.1.1.2.3">𝑡</ci></apply><ci id="alg2.l9.m3.1.1.3.cmml" xref="alg2.l9.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l9.m3.1c">w_{t}^{k}</annotation></semantics></math>

</div>
<div id="alg2.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg2.l10.1.1.1" class="ltx_text" style="font-size:90%;">10:</span></span>     <span id="alg2.l10.2" class="ltx_text ltx_font_bold">end</span> <span id="alg2.l10.3" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg2.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg2.l11.1.1.1" class="ltx_text" style="font-size:90%;">11:</span></span>     Select a subset of clients <math id="alg2.l11.m1.1" class="ltx_Math" alttext="S_{t}" display="inline"><semantics id="alg2.l11.m1.1a"><msub id="alg2.l11.m1.1.1" xref="alg2.l11.m1.1.1.cmml"><mi id="alg2.l11.m1.1.1.2" xref="alg2.l11.m1.1.1.2.cmml">S</mi><mi id="alg2.l11.m1.1.1.3" xref="alg2.l11.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="alg2.l11.m1.1b"><apply id="alg2.l11.m1.1.1.cmml" xref="alg2.l11.m1.1.1"><csymbol cd="ambiguous" id="alg2.l11.m1.1.1.1.cmml" xref="alg2.l11.m1.1.1">subscript</csymbol><ci id="alg2.l11.m1.1.1.2.cmml" xref="alg2.l11.m1.1.1.2">𝑆</ci><ci id="alg2.l11.m1.1.1.3.cmml" xref="alg2.l11.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l11.m1.1c">S_{t}</annotation></semantics></math> to communicate with the server

</div>
<div id="alg2.l12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg2.l12.1.1.1" class="ltx_text" style="font-size:90%;">12:</span></span>     <span id="alg2.l12.2" class="ltx_text ltx_font_bold">for</span> each client <math id="alg2.l12.m1.1" class="ltx_Math" alttext="k\in S_{t}" display="inline"><semantics id="alg2.l12.m1.1a"><mrow id="alg2.l12.m1.1.1" xref="alg2.l12.m1.1.1.cmml"><mi id="alg2.l12.m1.1.1.2" xref="alg2.l12.m1.1.1.2.cmml">k</mi><mo id="alg2.l12.m1.1.1.1" xref="alg2.l12.m1.1.1.1.cmml">∈</mo><msub id="alg2.l12.m1.1.1.3" xref="alg2.l12.m1.1.1.3.cmml"><mi id="alg2.l12.m1.1.1.3.2" xref="alg2.l12.m1.1.1.3.2.cmml">S</mi><mi id="alg2.l12.m1.1.1.3.3" xref="alg2.l12.m1.1.1.3.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="alg2.l12.m1.1b"><apply id="alg2.l12.m1.1.1.cmml" xref="alg2.l12.m1.1.1"><in id="alg2.l12.m1.1.1.1.cmml" xref="alg2.l12.m1.1.1.1"></in><ci id="alg2.l12.m1.1.1.2.cmml" xref="alg2.l12.m1.1.1.2">𝑘</ci><apply id="alg2.l12.m1.1.1.3.cmml" xref="alg2.l12.m1.1.1.3"><csymbol cd="ambiguous" id="alg2.l12.m1.1.1.3.1.cmml" xref="alg2.l12.m1.1.1.3">subscript</csymbol><ci id="alg2.l12.m1.1.1.3.2.cmml" xref="alg2.l12.m1.1.1.3.2">𝑆</ci><ci id="alg2.l12.m1.1.1.3.3.cmml" xref="alg2.l12.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l12.m1.1c">k\in S_{t}</annotation></semantics></math> <span id="alg2.l12.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg2.l13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg2.l13.1.1.1" class="ltx_text" style="font-size:90%;">13:</span></span>         Client <math id="alg2.l13.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="alg2.l13.m1.1a"><mi id="alg2.l13.m1.1.1" xref="alg2.l13.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="alg2.l13.m1.1b"><ci id="alg2.l13.m1.1.1.cmml" xref="alg2.l13.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.l13.m1.1c">k</annotation></semantics></math> sends local model update <math id="alg2.l13.m2.1" class="ltx_Math" alttext="\Delta w_{t}^{k}=w_{t}^{k}-w_{t-1}" display="inline"><semantics id="alg2.l13.m2.1a"><mrow id="alg2.l13.m2.1.1" xref="alg2.l13.m2.1.1.cmml"><mrow id="alg2.l13.m2.1.1.2" xref="alg2.l13.m2.1.1.2.cmml"><mi mathvariant="normal" id="alg2.l13.m2.1.1.2.2" xref="alg2.l13.m2.1.1.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="alg2.l13.m2.1.1.2.1" xref="alg2.l13.m2.1.1.2.1.cmml">​</mo><msubsup id="alg2.l13.m2.1.1.2.3" xref="alg2.l13.m2.1.1.2.3.cmml"><mi id="alg2.l13.m2.1.1.2.3.2.2" xref="alg2.l13.m2.1.1.2.3.2.2.cmml">w</mi><mi id="alg2.l13.m2.1.1.2.3.2.3" xref="alg2.l13.m2.1.1.2.3.2.3.cmml">t</mi><mi id="alg2.l13.m2.1.1.2.3.3" xref="alg2.l13.m2.1.1.2.3.3.cmml">k</mi></msubsup></mrow><mo id="alg2.l13.m2.1.1.1" xref="alg2.l13.m2.1.1.1.cmml">=</mo><mrow id="alg2.l13.m2.1.1.3" xref="alg2.l13.m2.1.1.3.cmml"><msubsup id="alg2.l13.m2.1.1.3.2" xref="alg2.l13.m2.1.1.3.2.cmml"><mi id="alg2.l13.m2.1.1.3.2.2.2" xref="alg2.l13.m2.1.1.3.2.2.2.cmml">w</mi><mi id="alg2.l13.m2.1.1.3.2.2.3" xref="alg2.l13.m2.1.1.3.2.2.3.cmml">t</mi><mi id="alg2.l13.m2.1.1.3.2.3" xref="alg2.l13.m2.1.1.3.2.3.cmml">k</mi></msubsup><mo id="alg2.l13.m2.1.1.3.1" xref="alg2.l13.m2.1.1.3.1.cmml">−</mo><msub id="alg2.l13.m2.1.1.3.3" xref="alg2.l13.m2.1.1.3.3.cmml"><mi id="alg2.l13.m2.1.1.3.3.2" xref="alg2.l13.m2.1.1.3.3.2.cmml">w</mi><mrow id="alg2.l13.m2.1.1.3.3.3" xref="alg2.l13.m2.1.1.3.3.3.cmml"><mi id="alg2.l13.m2.1.1.3.3.3.2" xref="alg2.l13.m2.1.1.3.3.3.2.cmml">t</mi><mo id="alg2.l13.m2.1.1.3.3.3.1" xref="alg2.l13.m2.1.1.3.3.3.1.cmml">−</mo><mn id="alg2.l13.m2.1.1.3.3.3.3" xref="alg2.l13.m2.1.1.3.3.3.3.cmml">1</mn></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg2.l13.m2.1b"><apply id="alg2.l13.m2.1.1.cmml" xref="alg2.l13.m2.1.1"><eq id="alg2.l13.m2.1.1.1.cmml" xref="alg2.l13.m2.1.1.1"></eq><apply id="alg2.l13.m2.1.1.2.cmml" xref="alg2.l13.m2.1.1.2"><times id="alg2.l13.m2.1.1.2.1.cmml" xref="alg2.l13.m2.1.1.2.1"></times><ci id="alg2.l13.m2.1.1.2.2.cmml" xref="alg2.l13.m2.1.1.2.2">Δ</ci><apply id="alg2.l13.m2.1.1.2.3.cmml" xref="alg2.l13.m2.1.1.2.3"><csymbol cd="ambiguous" id="alg2.l13.m2.1.1.2.3.1.cmml" xref="alg2.l13.m2.1.1.2.3">superscript</csymbol><apply id="alg2.l13.m2.1.1.2.3.2.cmml" xref="alg2.l13.m2.1.1.2.3"><csymbol cd="ambiguous" id="alg2.l13.m2.1.1.2.3.2.1.cmml" xref="alg2.l13.m2.1.1.2.3">subscript</csymbol><ci id="alg2.l13.m2.1.1.2.3.2.2.cmml" xref="alg2.l13.m2.1.1.2.3.2.2">𝑤</ci><ci id="alg2.l13.m2.1.1.2.3.2.3.cmml" xref="alg2.l13.m2.1.1.2.3.2.3">𝑡</ci></apply><ci id="alg2.l13.m2.1.1.2.3.3.cmml" xref="alg2.l13.m2.1.1.2.3.3">𝑘</ci></apply></apply><apply id="alg2.l13.m2.1.1.3.cmml" xref="alg2.l13.m2.1.1.3"><minus id="alg2.l13.m2.1.1.3.1.cmml" xref="alg2.l13.m2.1.1.3.1"></minus><apply id="alg2.l13.m2.1.1.3.2.cmml" xref="alg2.l13.m2.1.1.3.2"><csymbol cd="ambiguous" id="alg2.l13.m2.1.1.3.2.1.cmml" xref="alg2.l13.m2.1.1.3.2">superscript</csymbol><apply id="alg2.l13.m2.1.1.3.2.2.cmml" xref="alg2.l13.m2.1.1.3.2"><csymbol cd="ambiguous" id="alg2.l13.m2.1.1.3.2.2.1.cmml" xref="alg2.l13.m2.1.1.3.2">subscript</csymbol><ci id="alg2.l13.m2.1.1.3.2.2.2.cmml" xref="alg2.l13.m2.1.1.3.2.2.2">𝑤</ci><ci id="alg2.l13.m2.1.1.3.2.2.3.cmml" xref="alg2.l13.m2.1.1.3.2.2.3">𝑡</ci></apply><ci id="alg2.l13.m2.1.1.3.2.3.cmml" xref="alg2.l13.m2.1.1.3.2.3">𝑘</ci></apply><apply id="alg2.l13.m2.1.1.3.3.cmml" xref="alg2.l13.m2.1.1.3.3"><csymbol cd="ambiguous" id="alg2.l13.m2.1.1.3.3.1.cmml" xref="alg2.l13.m2.1.1.3.3">subscript</csymbol><ci id="alg2.l13.m2.1.1.3.3.2.cmml" xref="alg2.l13.m2.1.1.3.3.2">𝑤</ci><apply id="alg2.l13.m2.1.1.3.3.3.cmml" xref="alg2.l13.m2.1.1.3.3.3"><minus id="alg2.l13.m2.1.1.3.3.3.1.cmml" xref="alg2.l13.m2.1.1.3.3.3.1"></minus><ci id="alg2.l13.m2.1.1.3.3.3.2.cmml" xref="alg2.l13.m2.1.1.3.3.3.2">𝑡</ci><cn type="integer" id="alg2.l13.m2.1.1.3.3.3.3.cmml" xref="alg2.l13.m2.1.1.3.3.3.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l13.m2.1c">\Delta w_{t}^{k}=w_{t}^{k}-w_{t-1}</annotation></semantics></math> to the server

</div>
<div id="alg2.l14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg2.l14.1.1.1" class="ltx_text" style="font-size:90%;">14:</span></span>     <span id="alg2.l14.2" class="ltx_text ltx_font_bold">end</span> <span id="alg2.l14.3" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg2.l15" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg2.l15.1.1.1" class="ltx_text" style="font-size:90%;">15:</span></span>     Server aggregates local updates and computes the new global model:

<table id="S4.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.Ex2.m1.1" class="ltx_Math" alttext="w_{t}=w_{t-1}+\eta_{t}\sum_{k\in S_{t}}n_{k}\Delta w_{t}^{k}" display="block"><semantics id="S4.Ex2.m1.1a"><mrow id="S4.Ex2.m1.1.1" xref="S4.Ex2.m1.1.1.cmml"><msub id="S4.Ex2.m1.1.1.2" xref="S4.Ex2.m1.1.1.2.cmml"><mi id="S4.Ex2.m1.1.1.2.2" xref="S4.Ex2.m1.1.1.2.2.cmml">w</mi><mi id="S4.Ex2.m1.1.1.2.3" xref="S4.Ex2.m1.1.1.2.3.cmml">t</mi></msub><mo id="S4.Ex2.m1.1.1.1" xref="S4.Ex2.m1.1.1.1.cmml">=</mo><mrow id="S4.Ex2.m1.1.1.3" xref="S4.Ex2.m1.1.1.3.cmml"><msub id="S4.Ex2.m1.1.1.3.2" xref="S4.Ex2.m1.1.1.3.2.cmml"><mi id="S4.Ex2.m1.1.1.3.2.2" xref="S4.Ex2.m1.1.1.3.2.2.cmml">w</mi><mrow id="S4.Ex2.m1.1.1.3.2.3" xref="S4.Ex2.m1.1.1.3.2.3.cmml"><mi id="S4.Ex2.m1.1.1.3.2.3.2" xref="S4.Ex2.m1.1.1.3.2.3.2.cmml">t</mi><mo id="S4.Ex2.m1.1.1.3.2.3.1" xref="S4.Ex2.m1.1.1.3.2.3.1.cmml">−</mo><mn id="S4.Ex2.m1.1.1.3.2.3.3" xref="S4.Ex2.m1.1.1.3.2.3.3.cmml">1</mn></mrow></msub><mo id="S4.Ex2.m1.1.1.3.1" xref="S4.Ex2.m1.1.1.3.1.cmml">+</mo><mrow id="S4.Ex2.m1.1.1.3.3" xref="S4.Ex2.m1.1.1.3.3.cmml"><msub id="S4.Ex2.m1.1.1.3.3.2" xref="S4.Ex2.m1.1.1.3.3.2.cmml"><mi id="S4.Ex2.m1.1.1.3.3.2.2" xref="S4.Ex2.m1.1.1.3.3.2.2.cmml">η</mi><mi id="S4.Ex2.m1.1.1.3.3.2.3" xref="S4.Ex2.m1.1.1.3.3.2.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S4.Ex2.m1.1.1.3.3.1" xref="S4.Ex2.m1.1.1.3.3.1.cmml">​</mo><mrow id="S4.Ex2.m1.1.1.3.3.3" xref="S4.Ex2.m1.1.1.3.3.3.cmml"><munder id="S4.Ex2.m1.1.1.3.3.3.1" xref="S4.Ex2.m1.1.1.3.3.3.1.cmml"><mo movablelimits="false" id="S4.Ex2.m1.1.1.3.3.3.1.2" xref="S4.Ex2.m1.1.1.3.3.3.1.2.cmml">∑</mo><mrow id="S4.Ex2.m1.1.1.3.3.3.1.3" xref="S4.Ex2.m1.1.1.3.3.3.1.3.cmml"><mi id="S4.Ex2.m1.1.1.3.3.3.1.3.2" xref="S4.Ex2.m1.1.1.3.3.3.1.3.2.cmml">k</mi><mo id="S4.Ex2.m1.1.1.3.3.3.1.3.1" xref="S4.Ex2.m1.1.1.3.3.3.1.3.1.cmml">∈</mo><msub id="S4.Ex2.m1.1.1.3.3.3.1.3.3" xref="S4.Ex2.m1.1.1.3.3.3.1.3.3.cmml"><mi id="S4.Ex2.m1.1.1.3.3.3.1.3.3.2" xref="S4.Ex2.m1.1.1.3.3.3.1.3.3.2.cmml">S</mi><mi id="S4.Ex2.m1.1.1.3.3.3.1.3.3.3" xref="S4.Ex2.m1.1.1.3.3.3.1.3.3.3.cmml">t</mi></msub></mrow></munder><mrow id="S4.Ex2.m1.1.1.3.3.3.2" xref="S4.Ex2.m1.1.1.3.3.3.2.cmml"><msub id="S4.Ex2.m1.1.1.3.3.3.2.2" xref="S4.Ex2.m1.1.1.3.3.3.2.2.cmml"><mi id="S4.Ex2.m1.1.1.3.3.3.2.2.2" xref="S4.Ex2.m1.1.1.3.3.3.2.2.2.cmml">n</mi><mi id="S4.Ex2.m1.1.1.3.3.3.2.2.3" xref="S4.Ex2.m1.1.1.3.3.3.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S4.Ex2.m1.1.1.3.3.3.2.1" xref="S4.Ex2.m1.1.1.3.3.3.2.1.cmml">​</mo><mi mathvariant="normal" id="S4.Ex2.m1.1.1.3.3.3.2.3" xref="S4.Ex2.m1.1.1.3.3.3.2.3.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.Ex2.m1.1.1.3.3.3.2.1a" xref="S4.Ex2.m1.1.1.3.3.3.2.1.cmml">​</mo><msubsup id="S4.Ex2.m1.1.1.3.3.3.2.4" xref="S4.Ex2.m1.1.1.3.3.3.2.4.cmml"><mi id="S4.Ex2.m1.1.1.3.3.3.2.4.2.2" xref="S4.Ex2.m1.1.1.3.3.3.2.4.2.2.cmml">w</mi><mi id="S4.Ex2.m1.1.1.3.3.3.2.4.2.3" xref="S4.Ex2.m1.1.1.3.3.3.2.4.2.3.cmml">t</mi><mi id="S4.Ex2.m1.1.1.3.3.3.2.4.3" xref="S4.Ex2.m1.1.1.3.3.3.2.4.3.cmml">k</mi></msubsup></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex2.m1.1b"><apply id="S4.Ex2.m1.1.1.cmml" xref="S4.Ex2.m1.1.1"><eq id="S4.Ex2.m1.1.1.1.cmml" xref="S4.Ex2.m1.1.1.1"></eq><apply id="S4.Ex2.m1.1.1.2.cmml" xref="S4.Ex2.m1.1.1.2"><csymbol cd="ambiguous" id="S4.Ex2.m1.1.1.2.1.cmml" xref="S4.Ex2.m1.1.1.2">subscript</csymbol><ci id="S4.Ex2.m1.1.1.2.2.cmml" xref="S4.Ex2.m1.1.1.2.2">𝑤</ci><ci id="S4.Ex2.m1.1.1.2.3.cmml" xref="S4.Ex2.m1.1.1.2.3">𝑡</ci></apply><apply id="S4.Ex2.m1.1.1.3.cmml" xref="S4.Ex2.m1.1.1.3"><plus id="S4.Ex2.m1.1.1.3.1.cmml" xref="S4.Ex2.m1.1.1.3.1"></plus><apply id="S4.Ex2.m1.1.1.3.2.cmml" xref="S4.Ex2.m1.1.1.3.2"><csymbol cd="ambiguous" id="S4.Ex2.m1.1.1.3.2.1.cmml" xref="S4.Ex2.m1.1.1.3.2">subscript</csymbol><ci id="S4.Ex2.m1.1.1.3.2.2.cmml" xref="S4.Ex2.m1.1.1.3.2.2">𝑤</ci><apply id="S4.Ex2.m1.1.1.3.2.3.cmml" xref="S4.Ex2.m1.1.1.3.2.3"><minus id="S4.Ex2.m1.1.1.3.2.3.1.cmml" xref="S4.Ex2.m1.1.1.3.2.3.1"></minus><ci id="S4.Ex2.m1.1.1.3.2.3.2.cmml" xref="S4.Ex2.m1.1.1.3.2.3.2">𝑡</ci><cn type="integer" id="S4.Ex2.m1.1.1.3.2.3.3.cmml" xref="S4.Ex2.m1.1.1.3.2.3.3">1</cn></apply></apply><apply id="S4.Ex2.m1.1.1.3.3.cmml" xref="S4.Ex2.m1.1.1.3.3"><times id="S4.Ex2.m1.1.1.3.3.1.cmml" xref="S4.Ex2.m1.1.1.3.3.1"></times><apply id="S4.Ex2.m1.1.1.3.3.2.cmml" xref="S4.Ex2.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S4.Ex2.m1.1.1.3.3.2.1.cmml" xref="S4.Ex2.m1.1.1.3.3.2">subscript</csymbol><ci id="S4.Ex2.m1.1.1.3.3.2.2.cmml" xref="S4.Ex2.m1.1.1.3.3.2.2">𝜂</ci><ci id="S4.Ex2.m1.1.1.3.3.2.3.cmml" xref="S4.Ex2.m1.1.1.3.3.2.3">𝑡</ci></apply><apply id="S4.Ex2.m1.1.1.3.3.3.cmml" xref="S4.Ex2.m1.1.1.3.3.3"><apply id="S4.Ex2.m1.1.1.3.3.3.1.cmml" xref="S4.Ex2.m1.1.1.3.3.3.1"><csymbol cd="ambiguous" id="S4.Ex2.m1.1.1.3.3.3.1.1.cmml" xref="S4.Ex2.m1.1.1.3.3.3.1">subscript</csymbol><sum id="S4.Ex2.m1.1.1.3.3.3.1.2.cmml" xref="S4.Ex2.m1.1.1.3.3.3.1.2"></sum><apply id="S4.Ex2.m1.1.1.3.3.3.1.3.cmml" xref="S4.Ex2.m1.1.1.3.3.3.1.3"><in id="S4.Ex2.m1.1.1.3.3.3.1.3.1.cmml" xref="S4.Ex2.m1.1.1.3.3.3.1.3.1"></in><ci id="S4.Ex2.m1.1.1.3.3.3.1.3.2.cmml" xref="S4.Ex2.m1.1.1.3.3.3.1.3.2">𝑘</ci><apply id="S4.Ex2.m1.1.1.3.3.3.1.3.3.cmml" xref="S4.Ex2.m1.1.1.3.3.3.1.3.3"><csymbol cd="ambiguous" id="S4.Ex2.m1.1.1.3.3.3.1.3.3.1.cmml" xref="S4.Ex2.m1.1.1.3.3.3.1.3.3">subscript</csymbol><ci id="S4.Ex2.m1.1.1.3.3.3.1.3.3.2.cmml" xref="S4.Ex2.m1.1.1.3.3.3.1.3.3.2">𝑆</ci><ci id="S4.Ex2.m1.1.1.3.3.3.1.3.3.3.cmml" xref="S4.Ex2.m1.1.1.3.3.3.1.3.3.3">𝑡</ci></apply></apply></apply><apply id="S4.Ex2.m1.1.1.3.3.3.2.cmml" xref="S4.Ex2.m1.1.1.3.3.3.2"><times id="S4.Ex2.m1.1.1.3.3.3.2.1.cmml" xref="S4.Ex2.m1.1.1.3.3.3.2.1"></times><apply id="S4.Ex2.m1.1.1.3.3.3.2.2.cmml" xref="S4.Ex2.m1.1.1.3.3.3.2.2"><csymbol cd="ambiguous" id="S4.Ex2.m1.1.1.3.3.3.2.2.1.cmml" xref="S4.Ex2.m1.1.1.3.3.3.2.2">subscript</csymbol><ci id="S4.Ex2.m1.1.1.3.3.3.2.2.2.cmml" xref="S4.Ex2.m1.1.1.3.3.3.2.2.2">𝑛</ci><ci id="S4.Ex2.m1.1.1.3.3.3.2.2.3.cmml" xref="S4.Ex2.m1.1.1.3.3.3.2.2.3">𝑘</ci></apply><ci id="S4.Ex2.m1.1.1.3.3.3.2.3.cmml" xref="S4.Ex2.m1.1.1.3.3.3.2.3">Δ</ci><apply id="S4.Ex2.m1.1.1.3.3.3.2.4.cmml" xref="S4.Ex2.m1.1.1.3.3.3.2.4"><csymbol cd="ambiguous" id="S4.Ex2.m1.1.1.3.3.3.2.4.1.cmml" xref="S4.Ex2.m1.1.1.3.3.3.2.4">superscript</csymbol><apply id="S4.Ex2.m1.1.1.3.3.3.2.4.2.cmml" xref="S4.Ex2.m1.1.1.3.3.3.2.4"><csymbol cd="ambiguous" id="S4.Ex2.m1.1.1.3.3.3.2.4.2.1.cmml" xref="S4.Ex2.m1.1.1.3.3.3.2.4">subscript</csymbol><ci id="S4.Ex2.m1.1.1.3.3.3.2.4.2.2.cmml" xref="S4.Ex2.m1.1.1.3.3.3.2.4.2.2">𝑤</ci><ci id="S4.Ex2.m1.1.1.3.3.3.2.4.2.3.cmml" xref="S4.Ex2.m1.1.1.3.3.3.2.4.2.3">𝑡</ci></apply><ci id="S4.Ex2.m1.1.1.3.3.3.2.4.3.cmml" xref="S4.Ex2.m1.1.1.3.3.3.2.4.3">𝑘</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex2.m1.1c">w_{t}=w_{t-1}+\eta_{t}\sum_{k\in S_{t}}n_{k}\Delta w_{t}^{k}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>

</div>
<div id="alg2.l16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg2.l16.1.1.1" class="ltx_text" style="font-size:90%;">16:</span></span><span id="alg2.l16.2" class="ltx_text ltx_font_bold">end</span> <span id="alg2.l16.3" class="ltx_text ltx_font_bold">for</span>
</div>
</div>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.1.   Pre-training of Federated Foundation Models</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Motivation:</span>
The motivation behind Federated Foundation Model (FFM) pre-training is to enhance traditional Foundation Model (FM) pre-training methodologies, harnessing Federated Learning’s (FL) capability to utilize private data to improve model generalization while preserving data privacy. Introducing FL to FM lifespan allows for the FM to access a broader range of knowledge spectrum from private parties, mitigating overfitting on public data, and potentially enabling more generalized and context-aware FMs, while still benefiting from centralized data.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Goal:</span>
Enhance FM pre-train methodologies via FL, and allow FMs to foster a deeper understanding of data representations from private data, thereby enhancing the model’s capability to generalize across various tasks and domains.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Procedure Overview:</span>
As shown in Algorithm <a href="#alg2" title="Algorithm 2 ‣ 4. Federated Foundation Model: Prospective and Future Research ‣ Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and Figure <a href="#S3.F2" title="Figure 2 ‣ 3. Motivation for Federated Foundation Models ‣ Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, FFM pre-training is structured in two phases: centralized pre-training on public data, and federated pre-training on private data.
these phases interact via an adaptive switching mechanism, enabling the model to alternate between centralized pre-training (if the centralized public data is available) and federated pre-training.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.2.   Federated Foundation Model Fine-tuning</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Motivation:</span>
Traditional FM fine-tuning typically involves an offline deployment where the model is fine-tuned on private data, and subsequently isolated. This isolation precludes collaboration among end-users, potentially limiting the FM’s efficacy, especially when the local private data is limited and biased.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Goal:</span>
Leverage the collaborative learning feature of FL, enabling end-users with similar downstream tasks to collaboratively fine-tune FMs while preserving data privacy, thus potentially achieving enhanced performance on downstream tasks.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Procedure Overview:</span>
Similar to FFM pre-training, FFM fine-tuning follows the same procedure in Algorithm <a href="#alg2" title="Algorithm 2 ‣ 4. Federated Foundation Model: Prospective and Future Research ‣ Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, FFM fine-tuning builds upon FFM pre-training phase. It employs an adaptive switching mechanism to alternate between centralized fine-tuning on public datasets for benchmark tasks and federated fine-tuning on private data for local tasks.
As depicted in Figure <a href="#S3.F2" title="Figure 2 ‣ 3. Motivation for Federated Foundation Models ‣ Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, various fine-tuning strategies can be adopted with FFM. These include, but are not limited to, (1) direct fine-tuning of the FM backbone, and (2) Parameter Efficient Fine-tuning (PEFT) of a lightweight adapter head, while keeping the FM backbone frozen.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.3.   Federated Prompt Tuning</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Motivation:</span>
Incorporating FL into prompt engineering presents a promising avenue for enhancing the performance of FMs while maintaining data privacy. Specifically, FFMs can assist in utilizing sensitive data for crafting prompt templates and soft prompt tuning, which in turn, enables more accurate and personalized prompt conditioning for tasks.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Goal:</span>
Collaboratively develop more effective and adaptable prompts without compromising the privacy of sensitive data.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Procedure Overview:</span>
This subsection primarily explores automated prompt (soft prompt) methods like prompt tuning <cite class="ltx_cite ltx_citemacro_cite">Lester et al. (<a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite>, which refines the input prompt to better the model’s output. As illustrated in Figure <a href="#S3.F2" title="Figure 2 ‣ 3. Motivation for Federated Foundation Models ‣ Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and the general FFM optimization process in Algorithm <a href="#alg2" title="Algorithm 2 ‣ 4. Federated Foundation Model: Prospective and Future Research ‣ Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, within federated prompt engineering settings, end-users can collaboratively train auto-prompt models (prompt generator components in Figure <a href="#S3.F2" title="Figure 2 ‣ 3. Motivation for Federated Foundation Models ‣ Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) on their local private data and tasks, sharing the learned auto prompt models without disclosing the sensitive data. This collaborative endeavor facilitates the creation of more effective and adaptable prompts, thereby enhancing the overall performance of FMs on downstream tasks.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.4.   Federated Continual (Lifelong) Learning</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p"><span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_bold">Motivation:</span>
FMs exhibit a significant limitation due to their dependency on pre-trained offline knowledge. For example, ChatGPT’s knowledge is up-to-date only until 2021. With the anticipated increase in computational power, FM optimization at the edge may become feasible. FFMs can unlock the possibility of continual and lifelong learning from newly generated private edge data. With its scalability and privacy-preserving nature, FL can harness decentralized power to optimize FMs using emerging private data at the edge, which can serve as a valuable resource for model optimization.
Furthermore, federated continual and lifelong learning could lead to a more efficient utilization of resources. Institutions would no longer necessitate retraining models from scratch with the availability of new data. Through FL, incremental model improvements can be attained, thus diminishing the time and computational resources requisite for model training and refinement.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.1" class="ltx_p"><span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_bold">Goal:</span>
Employ FL to harness the computational power at the edge, unlocking the potential for continual and lifelong learning of FMs on newly generated private data at the edge. This approach also aims to keep FMs updated with contemporary knowledge while preserving data privacy.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para ltx_noindent">
<p id="S4.SS4.p3.1" class="ltx_p"><span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_bold">Procedure Overview:</span>
As delineated in Sections <a href="#S4.SS1" title="4.1. Pre-training of Federated Foundation Models ‣ 4. Federated Foundation Model: Prospective and Future Research ‣ Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> and <a href="#S4.SS2" title="4.2. Federated Foundation Model Fine-tuning ‣ 4. Federated Foundation Model: Prospective and Future Research ‣ Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, establishing an online federated server is essential to facilitate the continuous communication between the server and edge end-users. The FM is updated at the edge based on the newly generated private data and regularly synchronizes with the online server.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.5.   Federated Retrieval Augmented Generation</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p"><span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_bold">Motivation:</span>
Federated Retrieval Augmented Generation (FRAG) seeks to extend the advantages of Retrieval Augmented Generation (RAG) by leveraging decentralized data across various clients while ensuring privacy preservation. This amalgamation aims to furnish more current and precise responses in a privacy-conducive manner.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para ltx_noindent">
<p id="S4.SS5.p2.1" class="ltx_p"><span id="S4.SS5.p2.1.1" class="ltx_text ltx_font_bold">Goal:</span>
Integrate FL with the RAG framework to bolster the performance of Language Model Generators (LMGs) in crafting responses, utilizing both centralized and decentralized data sources.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para ltx_noindent">
<p id="S4.SS5.p3.1" class="ltx_p"><span id="S4.SS5.p3.1.1" class="ltx_text ltx_font_bold">Procedure Overview:</span>
In the FRAG framework, the procedure unfolds in several distinct phases to ensure both effective data retrieval and privacy preservation. During the retrieval phase, a query is initiated from a user end, which triggers data retrieval from both a centralized server and local databases of clients within a federated network. This query is shared among clients in a privacy-preserving manner, enabling local clients to fetch relevant private data at the edge.
Following the data retrieval, the generation phase commences where each client independently generates a response based on the retrieved data and the initial query. The responses from all clients are then aggregated in a privacy-preserving manner, ensuring no sensitive information is exposed during the process. Finally, an aggregated response, which encapsulates the collective intelligence of the federated network while preserving user privacy, is relayed back to the user. This structure allows for a more informed and accurate response generation in a decentralized and privacy-preserving environment.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.6.   Challenges</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">Despite the benefits associated with FFM, several substantial challenges persist. This subsection enumerates and discusses these general challenges.</p>
</div>
<div id="S4.SS6.p2" class="ltx_para ltx_noindent">
<p id="S4.SS6.p2.1" class="ltx_p"><span id="S4.SS6.p2.1.1" class="ltx_text ltx_font_bold">Model Size:</span>
The substantial size of FMs, such as GPT <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite> and Llama <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib34" title="" class="ltx_ref">2023b</a>)</cite>, presents a significant challenge for optimization FMs at the edge, especially when considering the resource-constraint edge devices in FL settings.</p>
</div>
<div id="S4.SS6.p3" class="ltx_para ltx_noindent">
<p id="S4.SS6.p3.1" class="ltx_p"><span id="S4.SS6.p3.1.1" class="ltx_text ltx_font_bold">Data Quality:</span>
The effectiveness of FM pre-training and fine-tuning, including self-supervised pre-training, is heavily contingent on data quality as highlighted in <cite class="ltx_cite ltx_citemacro_cite">Gunasekar et al. (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>. Ensuring high-quality data in private federated settings, where data sharing is restricted, presents a notable challenge in filtering out toxic and redundant data.</p>
</div>
<div id="S4.SS6.p4" class="ltx_para ltx_noindent">
<p id="S4.SS6.p4.1" class="ltx_p"><span id="S4.SS6.p4.1.1" class="ltx_text ltx_font_bold">Computational Cost:</span>
Optimizing FMs entails substantial computational cost <cite class="ltx_cite ltx_citemacro_cite">Meng et al. (<a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite>. In FL environments, collaborative optimization of FMs at the edge necessitates high hardware specifications for edge clients <cite class="ltx_cite ltx_citemacro_cite">Meindl and Moser (<a href="#bib.bib22" title="" class="ltx_ref">2023</a>); Malandrino and Chiasserini (<a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S4.SS6.p5" class="ltx_para ltx_noindent">
<p id="S4.SS6.p5.1" class="ltx_p"><span id="S4.SS6.p5.1.1" class="ltx_text ltx_font_bold">Communication Cost:</span>
The routine sharing of model updates, encompassing model weights and gradients, incurs significant communication overhead <cite class="ltx_cite ltx_citemacro_cite">Ángel Morell et al. (<a href="#bib.bib47" title="" class="ltx_ref">2022</a>); Almanifi et al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>); Mohammadi et al. (<a href="#bib.bib26" title="" class="ltx_ref">2021</a>); WANG et al. (<a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite> between clients and the server in FL environments.</p>
</div>
<div id="S4.SS6.p6" class="ltx_para ltx_noindent">
<p id="S4.SS6.p6.1" class="ltx_p"><span id="S4.SS6.p6.1.1" class="ltx_text ltx_font_bold">Data Heterogeneity:</span>
In FL, data is often non-identically distributed (non-IID) across clients <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib45" title="" class="ltx_ref">2018</a>); McMahan et al. (<a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite>, which could adversely affect the convergence and performance of the optimization process.</p>
</div>
<div id="S4.SS6.p7" class="ltx_para ltx_noindent">
<p id="S4.SS6.p7.1" class="ltx_p"><span id="S4.SS6.p7.1.1" class="ltx_text ltx_font_bold">Security Attacks:</span>
Although FL inherently preserves privacy, ensuring robust privacy guarantees in FFM, especially against sophisticated security attacks, remains vital <cite class="ltx_cite ltx_citemacro_cite">Lyu et al. (<a href="#bib.bib19" title="" class="ltx_ref">2022</a>); Zhang et al. (<a href="#bib.bib44" title="" class="ltx_ref">2022b</a>); Liu et al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S4.SS6.p8" class="ltx_para ltx_noindent">
<p id="S4.SS6.p8.1" class="ltx_p"><span id="S4.SS6.p8.1.1" class="ltx_text ltx_font_bold">Scalability:</span>
With the escalating scale of deployment, efficiently managing collaborative training and sharing model updates becomes increasingly challenging <cite class="ltx_cite ltx_citemacro_cite">Díaz and García (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>); Zawad et al. (<a href="#bib.bib42" title="" class="ltx_ref">2022</a>); Kołodziej and Rościszewski (<a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S4.SS6.p9" class="ltx_para ltx_noindent">
<p id="S4.SS6.p9.1" class="ltx_p"><span id="S4.SS6.p9.1.1" class="ltx_text ltx_font_bold">Asynchronous Training:</span>
As the number of clients increases, efficiently aggregating updates from a large number of asynchronous clients and ensuring consistent performance scaling is challenging <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib36" title="" class="ltx_ref">2022</a>); Chen et al. (<a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S4.SS6.p10" class="ltx_para ltx_noindent">
<p id="S4.SS6.p10.1" class="ltx_p"><span id="S4.SS6.p10.1.1" class="ltx_text ltx_font_bold">Non-Stationary Data Distributions:</span> The perpetually evolving nature of the user data suggests that data distributions may shift over time <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib43" title="" class="ltx_ref">2022a</a>)</cite>. Ensuring robust model performance amidst such changes is a significant challenge.</p>
</div>
<div id="S4.SS6.p11" class="ltx_para ltx_noindent">
<p id="S4.SS6.p11.1" class="ltx_p"><span id="S4.SS6.p11.1.1" class="ltx_text ltx_font_bold">Resource Constraints:</span> The resource-constrained edge devices could impede the optimization process of FMs at the edge.</p>
</div>
<div id="S4.SS6.p12" class="ltx_para ltx_noindent">
<p id="S4.SS6.p12.1" class="ltx_p"><span id="S4.SS6.p12.1.1" class="ltx_text ltx_font_bold">Global Model Synchronization:</span> Achieving global model synchronization across all participants while accommodating local updates and ensuring model stability is a nuanced challenge.</p>
</div>
<div id="S4.SS6.p13" class="ltx_para ltx_noindent">
<p id="S4.SS6.p13.1" class="ltx_p"><span id="S4.SS6.p13.1.1" class="ltx_text ltx_font_bold">Evaluation Metrics:</span> Establishing robust metrics to evaluate the performance, privacy, and other crucial aspects of the FFM process is pivotal.</p>
</div>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.7.   Other Future Research Directions</h3>

<div id="S4.SS7.p1" class="ltx_para">
<p id="S4.SS7.p1.1" class="ltx_p">In addition to the potential FFM tasks and general challenges discussed earlier, we outline several potential future research directions below.</p>
</div>
<div id="S4.SS7.p2" class="ltx_para ltx_noindent">
<p id="S4.SS7.p2.1" class="ltx_p"><span id="S4.SS7.p2.1.1" class="ltx_text ltx_font_bold">Advancement in Edge Hardware:</span>
Supporting the substantial computational and resource requirements of FM optimization in FL-edge scenarios necessitates significant advancements in edge hardware.</p>
</div>
<div id="S4.SS7.p3" class="ltx_para ltx_noindent">
<p id="S4.SS7.p3.1" class="ltx_p"><span id="S4.SS7.p3.1.1" class="ltx_text ltx_font_bold">Private-preserve Training Data Process:</span>
The success of self-supervised pre-training largely hinges on data quality. In the context of FFM, where private data at FL-edge clients remains inaccessible, and only the data owner can access it, devising private-preserving training data processing methods is crucial. This is to ensure data quality at the edge, where preprocessing is challenging. Recent works, such as <cite class="ltx_cite ltx_citemacro_cite">Gunasekar et al. (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>); Li et al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite>, propose automatic training data filters to evaluate and enhance data quality, addressing a critical aspect of data processing in FFM.</p>
</div>
<div id="S4.SS7.p4" class="ltx_para ltx_noindent">
<p id="S4.SS7.p4.1" class="ltx_p"><span id="S4.SS7.p4.1.1" class="ltx_text ltx_font_bold">Collaborative Model Compression:</span>
Designing specialized model compression methods, like network pruning and quantization, for heterogeneous-resource edge clients is essential to efficiently utilize the resources at edge clients. It also helps reduce the size of FMs without sacrificing performance. This is particularly critical for environments with limited computational resources.
</p>
</div>
<div id="S4.SS7.p5" class="ltx_para ltx_noindent">
<p id="S4.SS7.p5.1" class="ltx_p"><span id="S4.SS7.p5.1.1" class="ltx_text ltx_font_bold">Neural Architecture Design:</span>
The design of computational and hardware-efficient neural network architectures is a promising direction to explore, aiming to address the resource constraints and performance requirements in FFM deployment.</p>
</div>
<div id="S4.SS7.p6" class="ltx_para ltx_noindent">
<p id="S4.SS7.p6.1" class="ltx_p"><span id="S4.SS7.p6.1.1" class="ltx_text ltx_font_bold">Collaborative Self-supervised Learning:</span>
Self-supervised learning has been a dominant approach for FM pre-training. Developing specialized collaborative self-supervised learning methods can effectively harness decentralized computational power in FL-edge environments.</p>
</div>
<div id="S4.SS7.p7" class="ltx_para ltx_noindent">
<p id="S4.SS7.p7.1" class="ltx_p"><span id="S4.SS7.p7.1.1" class="ltx_text ltx_font_bold">Collaborative Parameter Efficient Fine-tuning:</span>
Designing collaborative parameter-efficient fine-tuning (PEFT) methods is crucial for fine-tuning FMs in FL scenarios, especially given the limited and heterogeneous resource capacities of edge clients.</p>
</div>
<div id="S4.SS7.p8" class="ltx_para ltx_noindent">
<p id="S4.SS7.p8.1" class="ltx_p"><span id="S4.SS7.p8.1.1" class="ltx_text ltx_font_bold">Robust Model Fusion Algorithms:</span>
Creating robust algorithms for model fusion is vital to ensure the effective aggregation of model updates from different clients while preserving data privacy and model performance.</p>
</div>
<div id="S4.SS7.p9" class="ltx_para ltx_noindent">
<p id="S4.SS7.p9.1" class="ltx_p"><span id="S4.SS7.p9.1.1" class="ltx_text ltx_font_bold">Federated Multi-task Learning:</span>
Exploring federated multi-task learning can facilitate the simultaneous optimization of multiple learning tasks across a federated network, leveraging the collective data and computational resources to improve model performance across various domains.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.   Conclusion and discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we introduced the concept of Federated Foundation Models (FFMs), which integrate Federated Learning (FL) into the lifespan of Foundation Models (FMs). We discussed FFM tasks, general challenges and potential future research directions.
It is important to note that the advancement of computation at edge users is crucial for the widespread adoption of FFMs, and we believe that such advancements will be realized in the near future. As the field of FFM continues to grow, we anticipate the emergence of numerous related research areas, including improved privacy-preserving techniques, the integration of FFM with emerging technologies like IoT and edge computing, and the exploration of FFM in various application domains such as healthcare, finance, and manufacturing.
Additionally, we foresee advancements in adaptive model compression methods for FFM local institutions, communication efficiency research, specialized FL algorithms for efficient updates and aggregation of FFM models, and security attack research. Overall, FFM represents a promising research area in the age of FMs, with the potential to address various challenges in privacy, scalability, and robustness across diverse domains.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.   Bibliographical References</h2>

<div id="S6.p1" class="ltx_para">
<span id="S6.p1.1" class="ltx_ERROR undefined">\c@NAT@ctr</span>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography"></h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Almanifi et al. (2023)</span>
<span class="ltx_bibblock">
Omair Rashed Abdulwareth Almanifi, Chee-Onn Chow, Mau-Luen Tham, Joon Huang Chuah, and Jeevan Kanesan. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1016/j.iot.2023.100742" title="" class="ltx_ref ltx_href">Communication and computation efficiency in federated learning: A survey</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Internet of Things</em>, 22:100742.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:1877–1901.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Z Chen, W Liao, K Hua, C Lu, and W Yu. 2021.

</span>
<span class="ltx_bibblock">Towards asynchronous federated learning for heterogeneous edge-powered internet of things. digit commun netw 7 (3): 317–326.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2020)</span>
<span class="ltx_bibblock">
Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. 2020.

</span>
<span class="ltx_bibblock">Adaptive personalized federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.13461</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Díaz and García (2023)</span>
<span class="ltx_bibblock">
Judith Sáinz-Pardo Díaz and Álvaro López García. 2023.

</span>
<span class="ltx_bibblock">Study of the performance and scalability of federated learning for medical imaging with intermittent clients.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, 518:142–154.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2020)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.11929</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gunasekar et al. (2023)</span>
<span class="ltx_bibblock">
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. 2023.

</span>
<span class="ltx_bibblock">Textbooks are all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.11644</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. (2022)</span>
<span class="ltx_bibblock">
Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. 2022.

</span>
<span class="ltx_bibblock">Ptr: Prompt tuning with rules for text classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">AI Open</em>, 3:182–192.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2022)</span>
<span class="ltx_bibblock">
Yuang Jiang, Shiqiang Wang, Victor Valls, Bong Jun Ko, Wei-Han Lee, Kin K Leung, and Leandros Tassiulas. 2022.

</span>
<span class="ltx_bibblock">Model pruning enables efficient federated learning on edge devices.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimireddy et al. (2020)</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. 2020.

</span>
<span class="ltx_bibblock">Scaffold: Stochastic controlled averaging for federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 5132–5143. PMLR.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kenton and Toutanova (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of naacL-HLT</em>, volume 1, page 2.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kołodziej and Rościszewski (2021)</span>
<span class="ltx_bibblock">
Tomasz Kołodziej and Paweł Rościszewski. 2021.

</span>
<span class="ltx_bibblock">Towards scalable simulation of federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Neural Information Processing: 28th International Conference, ICONIP 2021, Sanur, Bali, Indonesia, December 8–12, 2021, Proceedings, Part V 28</em>, pages 248–256. Springer.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lester et al. (2021)</span>
<span class="ltx_bibblock">
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.

</span>
<span class="ltx_bibblock">The power of scale for parameter-efficient prompt tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.08691</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023.

</span>
<span class="ltx_bibblock">Textbooks are all you need ii: phi-1.5 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.05463</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2020)</span>
<span class="ltx_bibblock">
Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. 2020.

</span>
<span class="ltx_bibblock">Ensemble distillation for robust model fusion in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:2351–2363.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021.

</span>
<span class="ltx_bibblock">What makes good in-context examples for gpt-<math id="bib.bib16.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="bib.bib16.1.m1.1a"><mn id="bib.bib16.1.m1.1.1" xref="bib.bib16.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="bib.bib16.1.m1.1b"><cn type="integer" id="bib.bib16.1.m1.1.1.cmml" xref="bib.bib16.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="bib.bib16.1.m1.1c">3</annotation></semantics></math>?

</span>
<span class="ltx_bibblock"><em id="bib.bib16.2.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.06804</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022)</span>
<span class="ltx_bibblock">
Pengrui Liu, Xiangrui Xu, and Wei Wang. 2022.

</span>
<span class="ltx_bibblock">Threats, attacks and defenses to federated learning: issues, taxonomy and perspectives.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Cybersecurity</em>, 5(1):1–19.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2020)</span>
<span class="ltx_bibblock">
Wei Liu, Li Chen, Yunfei Chen, and Wenyi Zhang. 2020.

</span>
<span class="ltx_bibblock">Accelerating federated learning via momentum gradient descent.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Parallel and Distributed Systems</em>, 31(8):1754–1766.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. (2022)</span>
<span class="ltx_bibblock">
Lingjuan Lyu, Han Yu, Xingjun Ma, Chen Chen, Lichao Sun, Jun Zhao, Qiang Yang, and Philip S. Yu. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TNNLS.2022.3216981" title="" class="ltx_ref ltx_href">Privacy and robustness in federated learning: Attacks and defenses</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, pages 1–21.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malandrino and Chiasserini (2021)</span>
<span class="ltx_bibblock">
Francesco Malandrino and Carla Fabiana Chiasserini. 2021.

</span>
<span class="ltx_bibblock">Toward node liability in federated learning: Computational cost and network overhead.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Magazine</em>, 59(9):72–77.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. 2017.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence and statistics</em>, pages 1273–1282. PMLR.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meindl and Moser (2023)</span>
<span class="ltx_bibblock">
Rainer Meindl and Bernhard A Moser. 2023.

</span>
<span class="ltx_bibblock">Measuring overhead costs of federated learning systems by eavesdropping.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">International Conference on Database and Expert Systems Applications</em>, pages 33–42. Springer.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et al. (2023)</span>
<span class="ltx_bibblock">
Fanqing Meng, Wenqi Shao, Zhanglin Peng, Chonghe Jiang, Kaipeng Zhang, Yu Qiao, and Ping Luo. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.06262" title="" class="ltx_ref ltx_href">Foundation model is efficient multimodal multitask model selector</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min et al. (2021)</span>
<span class="ltx_bibblock">
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2021.

</span>
<span class="ltx_bibblock">Metaicl: Learning to learn in context.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.15943</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min et al. (2022)</span>
<span class="ltx_bibblock">
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022.

</span>
<span class="ltx_bibblock">Rethinking the role of demonstrations: What makes in-context learning work?

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.12837</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohammadi et al. (2021)</span>
<span class="ltx_bibblock">
Nima Mohammadi, Jianan Bai, Qiang Fan, Yifei Song, Yang Yi, and Lingjia Liu. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2101.12240" title="" class="ltx_ref ltx_href">Differential privacy meets federated learning under communication constraints</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.08774" title="" class="ltx_ref ltx_href">Gpt-4 technical report</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages 8748–8763. PMLR.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">OpenAI blog</em>, 1(8):9.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rubin et al. (2021)</span>
<span class="ltx_bibblock">
Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2021.

</span>
<span class="ltx_bibblock">Learning to retrieve prompts for in-context learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.08633</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et al. (2021)</span>
<span class="ltx_bibblock">
Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021.

</span>
<span class="ltx_bibblock">Multitask prompted training enables zero-shot task generalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.08207</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al. (2022)</span>
<span class="ltx_bibblock">
Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. 2022.

</span>
<span class="ltx_bibblock">Towards personalized federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WANG et al. (2019)</span>
<span class="ltx_bibblock">
Luping WANG, Wei WANG, and Bo LI. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICDCS.2019.00099" title="" class="ltx_ref ltx_href">Cmfl: Mitigating communication overhead for federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)</em>, pages 954–964.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Qiyuan Wang, Qianqian Yang, Shibo He, Zhiguo Shi, and Jiming Chen. 2022.

</span>
<span class="ltx_bibblock">Asyncfeded: Asynchronous federated learning with euclidean distance based adaptive weight aggregation.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.13797</em>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2021)</span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.01652</em>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2022a)</span>
<span class="ltx_bibblock">
Sixing Yu, Phuong Nguyen, Waqwoya Abebe, Wei Qian, Ali Anwar, and Ali Jannesari. 2022a.

</span>
<span class="ltx_bibblock">Spatl: salient parameter aggregation and transfer learning for heterogeneous federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">2022 SC22: International Conference for High Performance Computing, Networking, Storage and Analysis (SC)</em>, pages 495–508. IEEE Computer Society.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2022b)</span>
<span class="ltx_bibblock">
Sixing Yu, Phuong Nguyen, Waqwoya Abebe, Justin Stanley, Pablo Munoz, and Ali Jannesari. 2022b.

</span>
<span class="ltx_bibblock">Resource-aware heterogeneous federated learning using neural architecture search.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.05716</em>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2021)</span>
<span class="ltx_bibblock">
Sixing Yu, Phuong Nguyen, Ali Anwar, and Ali Jannesari. 2021.

</span>
<span class="ltx_bibblock">Adaptive dynamic pruning for non-iid federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.06921</em>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2022c)</span>
<span class="ltx_bibblock">
Sixing Yu, Wei Qian, and Ali Jannesari. 2022c.

</span>
<span class="ltx_bibblock">Resource-aware federated learning using knowledge extraction and multi-model fusion.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.07978</em>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zawad et al. (2022)</span>
<span class="ltx_bibblock">
Syed Zawad, Feng Yan, and Ali Anwar. 2022.

</span>
<span class="ltx_bibblock">Local training and scalability of federated learning systems.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Federated Learning: A Comprehensive Overview of Methods and Applications</em>, pages 213–233. Springer.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022a)</span>
<span class="ltx_bibblock">
Hongwei Zhang, Meixia Tao, Yuanming Shi, and Xiaoyan Bi. 2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICC45855.2022.9838703" title="" class="ltx_ref ltx_href">Federated multi-task learning with non-stationary heterogeneous data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">ICC 2022 - IEEE International Conference on Communications</em>, pages 4950–4955.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022b)</span>
<span class="ltx_bibblock">
Junpeng Zhang, Hui Zhu, Fengwei Wang, Jiaqi Zhao, Qi Xu, Hui Li, et al. 2022b.

</span>
<span class="ltx_bibblock">Security and privacy threats to federated learning: Issues, methods, and challenges.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Security and Communication Networks</em>, 2022.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2018)</span>
<span class="ltx_bibblock">
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. 2018.

</span>
<span class="ltx_bibblock">Federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.00582</em>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2022)</span>
<span class="ltx_bibblock">
Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2022.

</span>
<span class="ltx_bibblock">Learning to prompt for vision-language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 130(9):2337–2348.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ángel Morell et al. (2022)</span>
<span class="ltx_bibblock">
José Ángel Morell, Zakaria Abdelmoiz Dahi, Francisco Chicano, Gabriel Luque, and Enrique Alba. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2204.02183" title="" class="ltx_ref ltx_href">Optimising communication overhead in federated learning using nsga-ii</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2305.11413" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2305.11414" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2305.11414">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2305.11414" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2305.11415" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 06:34:05 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
