<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.00300] Towards Flexible Evaluation for Generative Visual Question Answering</title><meta property="og:description" content="Throughout rapid development of multimodal large language models, a crucial ingredient is a fair and accurate evaluation of their multimodal comprehension abilities. Although Visual Question Answering (VQA) could serve…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards Flexible Evaluation for Generative Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards Flexible Evaluation for Generative Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.00300">

<!--Generated on Thu Sep  5 16:31:39 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Visual Question Answering,  Semantic Textual Similarity,  Contrastive Learning,  Evaluation Method">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Towards Flexible Evaluation for Generative 
<br class="ltx_break">Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Huishan Ji, Qingyi Si, Zheng Lin<sup id="id1.1.id1" class="ltx_sup">*</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:jihuishan,%20siqingyi,%20linzheng@iie.ac.cn">jihuishan, siqingyi, linzheng@iie.ac.cn</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0009-0002-7815-9508,%200000-0001-8433-0215,%200000-0002-8432-1658" title="ORCID identifier" class="ltx_ref">0009-0002-7815-9508, 0000-0001-8433-0215, 0000-0002-8432-1658</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id2.2.id1" class="ltx_text ltx_affiliation_institution">Institute of Information Engineering, 
<br class="ltx_break">Chinese Academy of Sciences</span><span id="id3.3.id2" class="ltx_text ltx_affiliation_institution">School of Cyber Security, 
<br class="ltx_break">University of Chinese Academy of Sciences</span><span id="id4.4.id3" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id5.5.id4" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Weiping Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:wangweiping@iie.ac.cn">wangweiping@iie.ac.cn</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-8618-4992" title="ORCID identifier" class="ltx_ref">0000-0002-8618-4992</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id6.1.id1" class="ltx_text ltx_affiliation_institution">Institute of Information Engineering, 
<br class="ltx_break">Chinese Academy of Science</span><span id="id7.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id8.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024; 13 April 2024; 21 July 2024)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id9.id1" class="ltx_p">Throughout rapid development of multimodal large language models, a crucial ingredient is a fair and accurate evaluation of their multimodal comprehension abilities. Although Visual Question Answering (VQA) could serve as a developed test field, limitations of VQA evaluation, like the inflexible pattern of Exact Match, have hindered MLLMs from demonstrating their real capability and discourage rich responses.
Therefore, this paper proposes the use of semantics-based evaluators for assessing unconstrained open-ended responses on VQA datasets. As characteristics of VQA have made such evaluation significantly different than the traditional Semantic Textual Similarity (STS) task, to systematically analyze the behaviour and compare the performance of various evaluators including LLM-based ones, we proposes three key properties, i.e., Alignment, Consistency and Generalization, and a corresponding dataset Assessing VQA Evaluators (AVE) to facilitate analysis.
In addition, this paper proposes a Semantically Flexible VQA Evaluator (SFVE) with meticulous design based on the unique features of VQA evaluation.
Experimental results verify the feasibility of model-based VQA evaluation and effectiveness of the proposed evaluator that surpasses existing semantic evaluators by a large margin. The proposed training scheme generalizes to both the BERT-like encoders and decoder-only LLM. Relaed codes and data available at <a target="_blank" href="https://github.com/jihuishan/flexible_evaluation_for_vqa_mm24" title="" class="ltx_ref ltx_href">Our Repository</a>.</p>
</div>
<div class="ltx_keywords">Visual Question Answering, Semantic Textual Similarity, Contrastive Learning, Evaluation Method
</div>
<div class="ltx_acknowledgements">
<sup id="id10.id1" class="ltx_sup">*</sup>Corresponding author: Zheng Lin, linzheng@iie.ac.cn
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>rightsretained</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Proceedings of the 32nd ACM International Conference
on Multimedia; October 28-November 1, 2024; Melbourne, VIC, Australia</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_booktitle"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Proceedings of the 32nd ACM International Conference on
Multimedia (MM ’24), October 28-November 1, 2024, Melbourne, VIC, Australia</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3664647.3681400</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0686-8/24/10</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Scene understanding</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Activity recognition and understanding</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Information extraction</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering (VQA) evaluates the multimodal comprehension abilities by posing questions about given images and comparing the model’s responses with annotated answers<cite class="ltx_cite ltx_citemacro_citep">(Malinowski and Fritz, <a href="#bib.bib40" title="" class="ltx_ref">2014</a>; Ren et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2015</a>; Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2017</a>; Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2019</a>; Hudson and Manning, <a href="#bib.bib26" title="" class="ltx_ref">2019</a>; Gurari et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2018</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2017</a>)</cite>. However, current VQA evaluation metrics have made it tough for evaluating the rich responses of Multimodal Large Language Models (MLLMs).</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Most VQA datasets comply with a triplet format and each sample consists of a question, image and annotation. Annotations are often a single word or phrase <cite class="ltx_cite ltx_citemacro_citep">(Malinowski and Fritz, <a href="#bib.bib40" title="" class="ltx_ref">2014</a>; Ren et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2015</a>; Hudson and Manning, <a href="#bib.bib26" title="" class="ltx_ref">2019</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2017</a>, <a href="#bib.bib55" title="" class="ltx_ref">2015</a>)</cite> or a set of ten candidate answers <cite class="ltx_cite ltx_citemacro_citep">(Schwenk et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2022</a>; Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2019</a>; Gurari et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2018</a>; Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2017</a>)</cite>. Current evaluation metrics, Exact Match <cite class="ltx_cite ltx_citemacro_citep">(Malinowski and Fritz, <a href="#bib.bib40" title="" class="ltx_ref">2014</a>)</cite> (for samples without candidate answers) and VQA Score <cite class="ltx_cite ltx_citemacro_citep">(Antol et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2015</a>)</cite> (for samples with ten candidate answers), both require the responses to be identical in morphology with the annotation to be considered correct. Variations in tense, singular or plural forms and synonyms are not allowed, let alone sentence-style responses from MLLMs.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Traditional vison-language models treat VQA as a classification problem <cite class="ltx_cite ltx_citemacro_citep">(Noh et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2016</a>; Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2017</a>; Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2016</a>; Ren et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2015</a>; Tan and Bansal, <a href="#bib.bib52" title="" class="ltx_ref">2019</a>)</cite>, where answers collected from the training set are used to establish pre-defined classes, and the possible responses are constrained to these classes. Thus the problem of evaluating multifarious responses does not exist. However, MLLMs treat VQA as a generative problem <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2023</a>; Dai et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2023</a>; Bai et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2023b</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite> and generates assorted responses. Meanwhile, the growing trend that the MLLM community prefers zero-shot test, has made it even tougher for models to generate responses that are identical to the ground-truth answers. As shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, semantically equivalent but morphologically distinct responses are not accepted.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2408.00300/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="323" height="131" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Responses from four MLLMs on a simple visual question. The responses are different in length, styles and complexity, which can all be considered correct but none of them exactly matches the annotated answer.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Although it is possible to force the model to output a single word with a harsh prompt, such remediation may potentially damage the performance and make it unfair for different MLLMs, especially for those with poor instruction-following ability and those that tend to response with long sentences.
As MLLMs inherit the in-context learning capability of LLMs, it is feasible to introduce in-context examples to force short responses. However, since different MLLMs contain different in-context learning capability, such practice interferes a fair evaluation of MLLMs’ multimodal comprehension performance.
There thus is an urgent need for a metric that aligns well with human judgment and accommodates various response types while ensuring consistent evaluation despite variations in response morphology.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To compare different evaluators, traditional Semantic Textual Similarity (STS) task measures the difference between predicted scores and human annotation results from a single aspect of semantic relevance. However, both intuition and our experiments (refer to Section <a href="#S3.SS1" title="3.1. Characteristics of VQA Evaluation ‣ 3. Semantic Evaluation of VQA ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> and <a href="#S5.SS3" title="5.3. Main Experiments ‣ 5. Experiments ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>) suggest that there is a significant difference between the evaluation of correctness in VQA responses and the traditional assessment of media-text relevance in STS.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Therefore, to systematically evaluate the performance of an evaluator, with the unique characteristics in the task of VQA response evaluation taken into consideration, we propose three quantitative key properties, i.e., <span id="S1.p6.1.1" class="ltx_text ltx_font_bold">Alignment</span>, <span id="S1.p6.1.2" class="ltx_text ltx_font_bold">Consistency</span> and <span id="S1.p6.1.3" class="ltx_text ltx_font_bold">Generalization</span>. Alignment stands for the overall correspondence of predicted scores with human annotation. Consistency measures how well an evaluator accommodates semantically equivalent responses of different morphology and length. Generalization indicates the variance of performance on different sources of data. Further, to facilitate comprehensive analysis of the performance and behaviour of evaluators, we provide a human-annotated dataset Assessing VQA Evaluators (AVE) that grades the correctness of model responses towards ground-truth labels on VQA datasets. AVE is further augmented by ChatGPT and WordNet <cite class="ltx_cite ltx_citemacro_citep">(Miller et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">1990</a>)</cite> to increase the diversity.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">As pilot experiment shows (refer to Section <a href="#S5.SS3" title="5.3. Main Experiments ‣ 5. Experiments ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>), formulaic metrics (BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2002</a>)</cite>, ROUGH <cite class="ltx_cite ltx_citemacro_citep">(Lin, <a href="#bib.bib34" title="" class="ltx_ref">2004</a>)</cite>, METEOR <cite class="ltx_cite ltx_citemacro_citep">(Banerjee and Lavie, <a href="#bib.bib11" title="" class="ltx_ref">2005</a>)</cite>) and model-based metrics <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021</a>; Li and Li, <a href="#bib.bib33" title="" class="ltx_ref">2023</a>; Reimers and Gurevych, <a href="#bib.bib47" title="" class="ltx_ref">2019</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite> perform poorly on the VQA response evaluation task. Therefore, we propose a novel evaluator that is trained with meticulously designed pretraining tasks. The tasks are designed for improving the embedding representation of VQA text, which utilizes contrastive learning to guide the evaluator to capture the fine-grained difference within a text pair and ignore the noise in morphology and length.
Experiments demonstrate that the proposed pretraining tasks significantly improve the performance of our evaluator on the AVE dataset, making the evaluator’s prediction aligns much better with human judgement.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">The contribution of this paper can be concluded as follows:</p>
</div>
<div id="S1.p9" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">This paper addresses the dilemma, where rich responses of MLLMs hinder fair evaluation under current metric, by proposing semantic-similarity-based evaluation that applies to various VQA responses.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">This paper proposes three quantitative key properties in VQA response evaluation based on its characteristics, and a high-quality human-annotated dataset, AVE, for assessing different evaluators comprehensively. In addition, we evaluate the performance of various types of existing semantic similarity evaluators on the proposed AVE dataset.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Experimental results demonstrate the feasibility of applying model-based methods to the flexible evaluation of VQA responses as well as the effectiveness of our proposed evaluator. Our evaluator significantly surpasses existing methods, including ChatGPT and the SOTA embedding model Voyage-lite-02-Instruct <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>As of the time of submission, Voyage-lite-02-Instruct achieves the best performance on the task of Semantic Textual Similarity (STS).</span></span></span> by a large margin. Our training scheme generalizes to both encoder-only and decoder-only models.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Visual Question Answering</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">As the answer space of most open-ended VQA datasets is limited and the same answer applies for multiple questions (the most common 2,000 answers in the training set of VQA v2<cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2017</a>)</cite> is able to cover about 94% questions in its validation set), early methods <cite class="ltx_cite ltx_citemacro_citep">(Noh et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2016</a>; Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2017</a>; Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2016</a>; Ren et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2015</a>; Tan and Bansal, <a href="#bib.bib52" title="" class="ltx_ref">2019</a>)</cite> treat VQA as a classification task, which adopt answers in the training set as class labels, unable to predict unseen classes. Generative methods on VQA <cite class="ltx_cite ltx_citemacro_citep">(Dai et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2023</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2024a</a>; Bai et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2023b</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2023</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite> treat VQA as a generation task and facilitates much more variant responses.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Semantic Textual Similarity</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Current semantic evaluation tasks include Semantic Textual Similarity (STS) <cite class="ltx_cite ltx_citemacro_citep">(Agirre et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2012</a>, <a href="#bib.bib6" title="" class="ltx_ref">2013</a>)</cite> that assesses to what extent the two sentences are related, Paraphrase Identification <cite class="ltx_cite ltx_citemacro_citep">(Tan et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2018</a>; Yin et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2016</a>)</cite> that decides whether two texts express the same meaning, and Natural Language Inference <cite class="ltx_cite ltx_citemacro_citep">(Williams et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2017</a>; Conneau et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2017</a>)</cite> that determines the logical relationship between texts. The essence of these tasks lies in quantifying the degree of semantic equivalence between sentences, which is a fundamental challenge due to the complexity and variability of natural language. Methods in STS include formulaic methods like BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2002</a>)</cite> and model-based ones <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021</a>; Li and Li, <a href="#bib.bib33" title="" class="ltx_ref">2023</a>; Niklas et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2024</a>; Reimers and Gurevych, <a href="#bib.bib47" title="" class="ltx_ref">2019</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2019</a>)</cite>. The former mainly relies on n-gram or other statistic features between the candidate and reference to calculate the overlap and import penalty for noise. The latter utilizes models as encoders to extract the information and compare between the candidate and reference. Early model-based evaluator <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2019</a>)</cite> compares the similarity of each pair each time, which is computation-consuming. Later works <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021</a>; Li and Li, <a href="#bib.bib33" title="" class="ltx_ref">2023</a>; Niklas et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2024</a>; Reimers and Gurevych, <a href="#bib.bib47" title="" class="ltx_ref">2019</a>)</cite> first generate embeddings separately for the candidate and reference, then simply calculate the cosine similarity between them as the similarity score. Due to the style of STS, either formulaic or model-based methods pay more attention to the overall similarity and are less capable of detecting fine-grained semantic difference, as shown in our experiments.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Multimodal Comprehension Evaluation of MLLMs</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">As a developed realm and valuable resource of high-quality data, VQA has been applied in the evaluation of MLLMs. Several works <cite class="ltx_cite ltx_citemacro_citep">(Chaoyou et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2023</a>; Ying et al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2024</a>; Schwenk et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite> propose multiple-choice datasets to make evaluation simple and straight. For example, MME <cite class="ltx_cite ltx_citemacro_citep">(Chaoyou et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite> proposes a smart quantitative analysis of MLLMs with manually designed instruction-answer pairs that strictly limit responses to be <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">yes</span> or <span id="S2.SS3.p1.1.2" class="ltx_text ltx_font_italic">no</span>. Therefore, all MLLMs are evaluated relatively fairly. However, although MME is insightful and effective, such detour avoids the problem of evaluating open-ended response directly. It ignores previous huge amount of VQA data and costs additional human annotation, limiting the scale of the dataset and making it tough for expanding. MM-vet <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2023</a>)</cite>, LVLM-eHub <cite class="ltx_cite ltx_citemacro_citep">(Shao et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2023</a>)</cite> and ConvBench <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2024b</a>)</cite> classify VQA into the integration of multiple key abilities, and manually annotates corresponding VQA samples of their required abilities. Then, they use ChatGPT for evaluation (which we show to be less capable in evaluating the correctness of open-ended responses, refer to Section <a href="#S5.SS3" title="5.3. Main Experiments ‣ 5. Experiments ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>). The classification of VQA abilities is insightful and aids to the probing of specific abilities of MLLMs. Yet MM-vet requires high-quality annotation to identify the VQA abilities each question requires and are thus limited to a small amount too.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Semantic Evaluation of VQA</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">With the rapid development of MLLMs, current metrics in VQA response evaluation are too stubborn to assess the rich generation and hinder evaluating MLLMs’ performance with existing VQA datasets. Meanwhile, as mentioned in Section <a href="#S2.SS2" title="2.2. Semantic Textual Similarity ‣ 2. Related Work ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>, current semantic evaluation models and tasks are inconsistent with the goal of flexible VQA response evaluation.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Therefore, this paper proposes the task of semantic evaluation of VQA, aiming at introducing flexible similarity-based soft evaluation with continuous scores into the assessment of VQA responses, contrary to inflexible metrics like Exact Match or VQA Score that require identical morphology of responses towards ground-truth labels. Such flexible evaluation enables to assess the rich generation from MLLMs and thus enables to use existing VQA datasets for probing MLLMs’ multimodal comprehension ability.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Characteristics of VQA Evaluation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The proposed task of semantic VQA response evaluation shares significant difference with existing semantic evaluation tasks like STS and contains its own characteristics.</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Discrimination Granularity</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">As mentioned in Section <a href="#S1" title="1. Introduction ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, traditional semantic evaluation tasks typically focus on the overall meaning in texts, rather than capturing the fine-grained detailed difference. However, the core of semantic VQA evaluation is comparing the response with annotated answer under the same question<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Considering the polysemy and ambiguity of words and phrases, the question text is indispensable for evaluating the semantic correctness.</span></span></span>, where both texts share large overlap in meaning as the questions are same, demanding fine-grained semantic discrimination.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Text Length</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">As VQA answers are generally much shorter than texts in STS <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>About 97.9% answers in VQA v2 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2017</a>)</cite>, 97.7% in OKVQA <cite class="ltx_cite ltx_citemacro_citep">(Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite>, 99.9% in GQA <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite> are shorter than three words. The average length of text in STS-12<cite class="ltx_cite ltx_citemacro_citep">(Agirre et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2012</a>)</cite> is 12.5, which is much longer.</span></span></span>, n-gram based formulaic metrics like BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2002</a>)</cite> will be more easily affected by the context in response. Model-based metrics are also vulnerable to such length shift, as their training data barely cover similar pattern.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Distribution Shift</h5>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">The texts in STS datasets <cite class="ltx_cite ltx_citemacro_citep">(Agirre et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2012</a>, <a href="#bib.bib6" title="" class="ltx_ref">2013</a>, <a href="#bib.bib3" title="" class="ltx_ref">2014</a>)</cite> come from general domains, like news and social media, while different VQA datasets comply to different sub-tasks, like knowledge <cite class="ltx_cite ltx_citemacro_citep">(Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite> or reasoning <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>. Such distribution shift causes inconsistent evaluation on responses from different VQA datasets.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Three Key Properties in VQA Evaluation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To systematically evaluate the performance of a VQA evaluator, we propose three quantitative key properties, i.e., <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">Alignment</span>, <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_bold">Consistency</span> and <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_bold">Generalization</span>.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Alignment</h5>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">Alignment assesses the overall performance of similarity scores predicted by evaluators with that of human annotation, in the metric of Spearman’s Rank Correlation following similar setting in previous works<cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021</a>; Li and Li, <a href="#bib.bib33" title="" class="ltx_ref">2023</a>; Agirre et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2012</a>, <a href="#bib.bib6" title="" class="ltx_ref">2013</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Consistency</h5>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">A smart evaluator shall catch the key information in responses and ignore the noise text, e.g., the response of <span id="S3.SS2.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">elephants</span> shall be scored equally with <span id="S3.SS2.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">Theses are elephants</span> under the question of <span id="S3.SS2.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_italic">What are the animals?</span>. Therefore, Consistency measures how close the different responses sharing the same meaning are scored.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Generalization</h5>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.1" class="ltx_p">Considering various VQA datasets focus on various sub-tasks and come from various sources, Generalization depicts how well an evaluator is able to handle text from different domains. Refer to Section <a href="#S3.SS4" title="3.4. The Proposed Evaluation Indicators ‣ 3. Semantic Evaluation of VQA ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a> for quantitative definitions.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>A Dataset Assessing VQA Evaluators</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To comprehensively compare and analyze the behaviour of different evaluators on VQA responses, taking the proposed three key properties into consideration, we propose a dataset Assessing VQA Evaluators (AVE). By collecting multiple MLLMs’ responses on multiple datasets, the proposed dataset simulates a real scene of applying evaluators to evaluate the quality of various VQA responses. In order to compare the evaluators’ scoring results with human judgement, we provide human annotation of the semantic correctness of responses towards ground-truth answers. The construction process of AVE is shown as follows:</p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Response Collection</h5>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p1.1" class="ltx_p">First, we collect responses of five models, LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2024a</a>)</cite>, BLIP2 <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2023</a>)</cite>, mPLUG-Owl <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2023</a>)</cite>, OFA-large <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2022</a>)</cite>, Qwen-VL <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2023b</a>)</cite> on the validation set of four datasets, OKVQA<cite class="ltx_cite ltx_citemacro_citep">(Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite>, A-OKVQA<cite class="ltx_cite ltx_citemacro_citep">(Schwenk et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>, VQA v2<cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2017</a>)</cite> and GQA<cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite> (balanced testdev set).</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Sampling Results</h5>

<div id="S3.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px2.p1.1" class="ltx_p">Second, we sample in the responses while controlling the sampling amount of each dataset to be the same. In addition, samples that are answered correctly, i.e., the response is identical with the ground-truth answer, are excluded.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Human Annotation</h5>

<div id="S3.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px3.p1.1" class="ltx_p">Third, three annotators are asked to measure the semantic similarity<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>We considered multiple aspects of measuring the correctness of a response towards the ground-truth answer, yet at last we come to the single aspect of semantic similarity for annotation. Refer to Appendix for more explanation.</span></span></span> of each sampled response towards the ground-truth label and annotate an integral similarity score from 0 to 10, under certain rules (refer to Appendix for more details). Then the scores are averaged over the three annotators.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Description Generation</h5>

<div id="S3.SS3.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px4.p1.1" class="ltx_p">Fourth, in order to simulate MLLM responses with sentences instead of words or phrases, we select responses that are shorter than three words for augmentation <span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>For longer responses, we manually condense it.</span></span></span>. The augmentation is conducted in two ways. The first way comes from using ChatGPT (refer to Appendix for prompts) to convert each pair of question and response into three descriptions and asking ChatGPT to select two descriptions that are closest to the original question-answer pair as augmented responses. For example, the question of <span id="S3.SS3.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_italic">What are the animals? </span> and the response of <span id="S3.SS3.SSS0.Px4.p1.1.2" class="ltx_text ltx_font_italic">elephants</span> are fed into ChatGPT, and it generates descriptions like <span id="S3.SS3.SSS0.Px4.p1.1.3" class="ltx_text ltx_font_italic">The animals are elephants.</span> The second way comes from applying manually designed answer templates (refer to Appendix) to increase the diversity of descriptions rather than fully relying on ChatGPT. Now each sample contains three descriptions, i.e., the two ChatGPT-augmented descriptions and a manual-template-fitted one.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Synonym Generation</h5>

<div id="S3.SS3.SSS0.Px5.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px5.p1.1" class="ltx_p">Fifth, we use WordNet <cite class="ltx_cite ltx_citemacro_citep">(Miller et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">1990</a>)</cite> to locate a synonym for each answer. For cases where multiple synonyms exist, we choose the most common synonym by countering frequencies of words in Brown Corpus <cite class="ltx_cite ltx_citemacro_citep">(Francis and Kucera, <a href="#bib.bib21" title="" class="ltx_ref">1979</a>)</cite>. Meanwhile, we ask ChatGPT to introduce a shift in morphology to simulate cases that the outputs are merely different in tenses or singular or plural forms. The augmented answer is deemed to contain the same meaning with small disturbance on the style.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Manual Filter</h5>

<div id="S3.SS3.SSS0.Px6.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px6.p1.1" class="ltx_p">At last, to ensure high-quality of the dataset, the three annotators also conduct manual filter (refer to Appendix for rules) to eliminate ambiguous samples, especially those generated from the fifth stage.</p>
</div>
<div id="S3.SS3.SSS0.Px6.p2" class="ltx_para">
<p id="S3.SS3.SSS0.Px6.p2.1" class="ltx_p">Generally speaking, the whole AVE dataset consists of three parts generated from above: Part 1 contains original answers and original responses (without augmentation). Part 2 contains original answers and the generated three descriptions of responses. Part 3 contains tense-shifted answers and original responses. The whole procedure is depicted in Figure <a href="#S3.F2" title="Figure 2 ‣ Manual Filter ‣ 3.3. A Dataset Assessing VQA Evaluators ‣ 3. Semantic Evaluation of VQA ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S3.SS3.SSS0.Px6.p3" class="ltx_para">
<p id="S3.SS3.SSS0.Px6.p3.1" class="ltx_p">Meanwhile, the AVE dataset can also be clustered by the involved four datasets that each sample belongs to, OKVQA, A-OKVQA, VQA v2 and GQA, merging the Part 1 to Part 3 together and classifies by the sources of data only. Refer to Section <a href="#S3.SS4" title="3.4. The Proposed Evaluation Indicators ‣ 3. Semantic Evaluation of VQA ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a> for how they are used.</p>
</div>
<div id="S3.SS3.SSS0.Px6.p4" class="ltx_para">
<p id="S3.SS3.SSS0.Px6.p4.1" class="ltx_p">The total sample amount of the final AVE dataset is 3,592, with each sample containing four types of augmentation results, as described above. The dataset is then split into a validation set and a test set with the ratio of 3:7.
The distribution of annotated scores is shown in Appendix, which is relatively smooth.
To evaluate the inter-annotator agreement, following previous works <cite class="ltx_cite ltx_citemacro_citep">(Grusky et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2018</a>; Fabbri et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2021</a>; Bhandari et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>, we apply Krippendorff’s alpha <cite class="ltx_cite ltx_citemacro_citep">(Krippendorff, <a href="#bib.bib29" title="" class="ltx_ref">2011</a>)</cite> and obtain a result of 0.713.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2408.00300/assets/x2.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="184" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>The construction procedure of AVE. After randomly sampled from the outputs of models, each sample is manually annotated with a score and automatically augmented by generated descriptions and a variation on the answer word while remaining almost the same correctness as a VQA response. Part 1 to 3 denote different augmentation methods. </figcaption>
</figure>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>The Proposed Evaluation Indicators</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.8" class="ltx_p">In AVE dataset, a sample consists of a question <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="q_{i}" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><msub id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">q</mi><mi id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">𝑞</ci><ci id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">q_{i}</annotation></semantics></math>, a ground-truth label <math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="a_{i}" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><msub id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml">a</mi><mi id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">𝑎</ci><ci id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">a_{i}</annotation></semantics></math>, a source dataset label <math id="S3.SS4.p1.3.m3.1" class="ltx_Math" alttext="d_{i}" display="inline"><semantics id="S3.SS4.p1.3.m3.1a"><msub id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml"><mi id="S3.SS4.p1.3.m3.1.1.2" xref="S3.SS4.p1.3.m3.1.1.2.cmml">d</mi><mi id="S3.SS4.p1.3.m3.1.1.3" xref="S3.SS4.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><apply id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.1.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p1.3.m3.1.1.2.cmml" xref="S3.SS4.p1.3.m3.1.1.2">𝑑</ci><ci id="S3.SS4.p1.3.m3.1.1.3.cmml" xref="S3.SS4.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">d_{i}</annotation></semantics></math>, a response <math id="S3.SS4.p1.4.m4.1" class="ltx_Math" alttext="r_{i}" display="inline"><semantics id="S3.SS4.p1.4.m4.1a"><msub id="S3.SS4.p1.4.m4.1.1" xref="S3.SS4.p1.4.m4.1.1.cmml"><mi id="S3.SS4.p1.4.m4.1.1.2" xref="S3.SS4.p1.4.m4.1.1.2.cmml">r</mi><mi id="S3.SS4.p1.4.m4.1.1.3" xref="S3.SS4.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><apply id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.1.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p1.4.m4.1.1.2.cmml" xref="S3.SS4.p1.4.m4.1.1.2">𝑟</ci><ci id="S3.SS4.p1.4.m4.1.1.3.cmml" xref="S3.SS4.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">r_{i}</annotation></semantics></math> and a human-annotated score <math id="S3.SS4.p1.5.m5.1" class="ltx_Math" alttext="s_{i}" display="inline"><semantics id="S3.SS4.p1.5.m5.1a"><msub id="S3.SS4.p1.5.m5.1.1" xref="S3.SS4.p1.5.m5.1.1.cmml"><mi id="S3.SS4.p1.5.m5.1.1.2" xref="S3.SS4.p1.5.m5.1.1.2.cmml">s</mi><mi id="S3.SS4.p1.5.m5.1.1.3" xref="S3.SS4.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m5.1b"><apply id="S3.SS4.p1.5.m5.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.5.m5.1.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS4.p1.5.m5.1.1.2.cmml" xref="S3.SS4.p1.5.m5.1.1.2">𝑠</ci><ci id="S3.SS4.p1.5.m5.1.1.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m5.1c">s_{i}</annotation></semantics></math>. The task of VQA response evaluation can be defined as: given a question-answer pair, an evaluator <math id="S3.SS4.p1.6.m6.3" class="ltx_Math" alttext="f(q_{i},a_{i},r_{i})" display="inline"><semantics id="S3.SS4.p1.6.m6.3a"><mrow id="S3.SS4.p1.6.m6.3.3" xref="S3.SS4.p1.6.m6.3.3.cmml"><mi id="S3.SS4.p1.6.m6.3.3.5" xref="S3.SS4.p1.6.m6.3.3.5.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.6.m6.3.3.4" xref="S3.SS4.p1.6.m6.3.3.4.cmml">​</mo><mrow id="S3.SS4.p1.6.m6.3.3.3.3" xref="S3.SS4.p1.6.m6.3.3.3.4.cmml"><mo stretchy="false" id="S3.SS4.p1.6.m6.3.3.3.3.4" xref="S3.SS4.p1.6.m6.3.3.3.4.cmml">(</mo><msub id="S3.SS4.p1.6.m6.1.1.1.1.1" xref="S3.SS4.p1.6.m6.1.1.1.1.1.cmml"><mi id="S3.SS4.p1.6.m6.1.1.1.1.1.2" xref="S3.SS4.p1.6.m6.1.1.1.1.1.2.cmml">q</mi><mi id="S3.SS4.p1.6.m6.1.1.1.1.1.3" xref="S3.SS4.p1.6.m6.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS4.p1.6.m6.3.3.3.3.5" xref="S3.SS4.p1.6.m6.3.3.3.4.cmml">,</mo><msub id="S3.SS4.p1.6.m6.2.2.2.2.2" xref="S3.SS4.p1.6.m6.2.2.2.2.2.cmml"><mi id="S3.SS4.p1.6.m6.2.2.2.2.2.2" xref="S3.SS4.p1.6.m6.2.2.2.2.2.2.cmml">a</mi><mi id="S3.SS4.p1.6.m6.2.2.2.2.2.3" xref="S3.SS4.p1.6.m6.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.SS4.p1.6.m6.3.3.3.3.6" xref="S3.SS4.p1.6.m6.3.3.3.4.cmml">,</mo><msub id="S3.SS4.p1.6.m6.3.3.3.3.3" xref="S3.SS4.p1.6.m6.3.3.3.3.3.cmml"><mi id="S3.SS4.p1.6.m6.3.3.3.3.3.2" xref="S3.SS4.p1.6.m6.3.3.3.3.3.2.cmml">r</mi><mi id="S3.SS4.p1.6.m6.3.3.3.3.3.3" xref="S3.SS4.p1.6.m6.3.3.3.3.3.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS4.p1.6.m6.3.3.3.3.7" xref="S3.SS4.p1.6.m6.3.3.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m6.3b"><apply id="S3.SS4.p1.6.m6.3.3.cmml" xref="S3.SS4.p1.6.m6.3.3"><times id="S3.SS4.p1.6.m6.3.3.4.cmml" xref="S3.SS4.p1.6.m6.3.3.4"></times><ci id="S3.SS4.p1.6.m6.3.3.5.cmml" xref="S3.SS4.p1.6.m6.3.3.5">𝑓</ci><vector id="S3.SS4.p1.6.m6.3.3.3.4.cmml" xref="S3.SS4.p1.6.m6.3.3.3.3"><apply id="S3.SS4.p1.6.m6.1.1.1.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p1.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS4.p1.6.m6.1.1.1.1.1.2">𝑞</ci><ci id="S3.SS4.p1.6.m6.1.1.1.1.1.3.cmml" xref="S3.SS4.p1.6.m6.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS4.p1.6.m6.2.2.2.2.2.cmml" xref="S3.SS4.p1.6.m6.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p1.6.m6.2.2.2.2.2.1.cmml" xref="S3.SS4.p1.6.m6.2.2.2.2.2">subscript</csymbol><ci id="S3.SS4.p1.6.m6.2.2.2.2.2.2.cmml" xref="S3.SS4.p1.6.m6.2.2.2.2.2.2">𝑎</ci><ci id="S3.SS4.p1.6.m6.2.2.2.2.2.3.cmml" xref="S3.SS4.p1.6.m6.2.2.2.2.2.3">𝑖</ci></apply><apply id="S3.SS4.p1.6.m6.3.3.3.3.3.cmml" xref="S3.SS4.p1.6.m6.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS4.p1.6.m6.3.3.3.3.3.1.cmml" xref="S3.SS4.p1.6.m6.3.3.3.3.3">subscript</csymbol><ci id="S3.SS4.p1.6.m6.3.3.3.3.3.2.cmml" xref="S3.SS4.p1.6.m6.3.3.3.3.3.2">𝑟</ci><ci id="S3.SS4.p1.6.m6.3.3.3.3.3.3.cmml" xref="S3.SS4.p1.6.m6.3.3.3.3.3.3">𝑖</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m6.3c">f(q_{i},a_{i},r_{i})</annotation></semantics></math> is expected to predict the annotated similarity score <math id="S3.SS4.p1.7.m7.1" class="ltx_Math" alttext="s_{i}" display="inline"><semantics id="S3.SS4.p1.7.m7.1a"><msub id="S3.SS4.p1.7.m7.1.1" xref="S3.SS4.p1.7.m7.1.1.cmml"><mi id="S3.SS4.p1.7.m7.1.1.2" xref="S3.SS4.p1.7.m7.1.1.2.cmml">s</mi><mi id="S3.SS4.p1.7.m7.1.1.3" xref="S3.SS4.p1.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.7.m7.1b"><apply id="S3.SS4.p1.7.m7.1.1.cmml" xref="S3.SS4.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.7.m7.1.1.1.cmml" xref="S3.SS4.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS4.p1.7.m7.1.1.2.cmml" xref="S3.SS4.p1.7.m7.1.1.2">𝑠</ci><ci id="S3.SS4.p1.7.m7.1.1.3.cmml" xref="S3.SS4.p1.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.7.m7.1c">s_{i}</annotation></semantics></math> with the output <math id="S3.SS4.p1.8.m8.1" class="ltx_Math" alttext="o_{i}" display="inline"><semantics id="S3.SS4.p1.8.m8.1a"><msub id="S3.SS4.p1.8.m8.1.1" xref="S3.SS4.p1.8.m8.1.1.cmml"><mi id="S3.SS4.p1.8.m8.1.1.2" xref="S3.SS4.p1.8.m8.1.1.2.cmml">o</mi><mi id="S3.SS4.p1.8.m8.1.1.3" xref="S3.SS4.p1.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.8.m8.1b"><apply id="S3.SS4.p1.8.m8.1.1.cmml" xref="S3.SS4.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.8.m8.1.1.1.cmml" xref="S3.SS4.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS4.p1.8.m8.1.1.2.cmml" xref="S3.SS4.p1.8.m8.1.1.2">𝑜</ci><ci id="S3.SS4.p1.8.m8.1.1.3.cmml" xref="S3.SS4.p1.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.8.m8.1c">o_{i}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="o_{i}=cos(f(q_{i},r_{i}),f(q_{i},a_{i}))" display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><msub id="S3.E1.m1.2.2.4" xref="S3.E1.m1.2.2.4.cmml"><mi id="S3.E1.m1.2.2.4.2" xref="S3.E1.m1.2.2.4.2.cmml">o</mi><mi id="S3.E1.m1.2.2.4.3" xref="S3.E1.m1.2.2.4.3.cmml">i</mi></msub><mo id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.3.cmml">=</mo><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">​</mo><mi id="S3.E1.m1.2.2.2.5" xref="S3.E1.m1.2.2.2.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.3a" xref="S3.E1.m1.2.2.2.3.cmml">​</mo><mi id="S3.E1.m1.2.2.2.6" xref="S3.E1.m1.2.2.2.6.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.3b" xref="S3.E1.m1.2.2.2.3.cmml">​</mo><mrow id="S3.E1.m1.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.3.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.1.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.2.3.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml">q</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.2.2.4" xref="S3.E1.m1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E1.m1.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2.2.cmml">r</mi><mi id="S3.E1.m1.1.1.1.1.1.1.2.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.2.2.5" xref="S3.E1.m1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.2.2.2.4" xref="S3.E1.m1.2.2.2.2.3.cmml">,</mo><mrow id="S3.E1.m1.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.4" xref="S3.E1.m1.2.2.2.2.2.2.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.2.2.3.cmml">​</mo><mrow id="S3.E1.m1.2.2.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.2.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.2.2.2.3.cmml">(</mo><msub id="S3.E1.m1.2.2.2.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.2.2.2.1.1.1.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.1.1.1.2" xref="S3.E1.m1.2.2.2.2.2.2.1.1.1.2.cmml">q</mi><mi id="S3.E1.m1.2.2.2.2.2.2.1.1.1.3" xref="S3.E1.m1.2.2.2.2.2.2.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.2.2.2.2.2.2.2.2.4" xref="S3.E1.m1.2.2.2.2.2.2.2.3.cmml">,</mo><msub id="S3.E1.m1.2.2.2.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.2.2.2.2.cmml">a</mi><mi id="S3.E1.m1.2.2.2.2.2.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E1.m1.2.2.2.2.2.2.2.2.5" xref="S3.E1.m1.2.2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.2.2.2.2.2.5" xref="S3.E1.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><eq id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.3"></eq><apply id="S3.E1.m1.2.2.4.cmml" xref="S3.E1.m1.2.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.4.1.cmml" xref="S3.E1.m1.2.2.4">subscript</csymbol><ci id="S3.E1.m1.2.2.4.2.cmml" xref="S3.E1.m1.2.2.4.2">𝑜</ci><ci id="S3.E1.m1.2.2.4.3.cmml" xref="S3.E1.m1.2.2.4.3">𝑖</ci></apply><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><times id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></times><ci id="S3.E1.m1.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.4">𝑐</ci><ci id="S3.E1.m1.2.2.2.5.cmml" xref="S3.E1.m1.2.2.2.5">𝑜</ci><ci id="S3.E1.m1.2.2.2.6.cmml" xref="S3.E1.m1.2.2.2.6">𝑠</ci><interval closure="open" id="S3.E1.m1.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2"><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3"></times><ci id="S3.E1.m1.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.4">𝑓</ci><interval closure="open" id="S3.E1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2"><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2">𝑞</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2.2">𝑟</ci><ci id="S3.E1.m1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2.3">𝑖</ci></apply></interval></apply><apply id="S3.E1.m1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2"><times id="S3.E1.m1.2.2.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.3"></times><ci id="S3.E1.m1.2.2.2.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.2.2.2.4">𝑓</ci><interval closure="open" id="S3.E1.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2"><apply id="S3.E1.m1.2.2.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.1.1.2">𝑞</ci><ci id="S3.E1.m1.2.2.2.2.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.1.1.3">𝑖</ci></apply><apply id="S3.E1.m1.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.2.2">𝑎</ci><ci id="S3.E1.m1.2.2.2.2.2.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.2.3">𝑖</ci></apply></interval></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">o_{i}=cos(f(q_{i},r_{i}),f(q_{i},a_{i}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.2" class="ltx_Math" alttext="score_{f}=Spearman(O,S)" display="block"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.3" xref="S3.E2.m1.2.3.cmml"><mrow id="S3.E2.m1.2.3.2" xref="S3.E2.m1.2.3.2.cmml"><mi id="S3.E2.m1.2.3.2.2" xref="S3.E2.m1.2.3.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.3.2.1" xref="S3.E2.m1.2.3.2.1.cmml">​</mo><mi id="S3.E2.m1.2.3.2.3" xref="S3.E2.m1.2.3.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.3.2.1a" xref="S3.E2.m1.2.3.2.1.cmml">​</mo><mi id="S3.E2.m1.2.3.2.4" xref="S3.E2.m1.2.3.2.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.3.2.1b" xref="S3.E2.m1.2.3.2.1.cmml">​</mo><mi id="S3.E2.m1.2.3.2.5" xref="S3.E2.m1.2.3.2.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.3.2.1c" xref="S3.E2.m1.2.3.2.1.cmml">​</mo><msub id="S3.E2.m1.2.3.2.6" xref="S3.E2.m1.2.3.2.6.cmml"><mi id="S3.E2.m1.2.3.2.6.2" xref="S3.E2.m1.2.3.2.6.2.cmml">e</mi><mi id="S3.E2.m1.2.3.2.6.3" xref="S3.E2.m1.2.3.2.6.3.cmml">f</mi></msub></mrow><mo id="S3.E2.m1.2.3.1" xref="S3.E2.m1.2.3.1.cmml">=</mo><mrow id="S3.E2.m1.2.3.3" xref="S3.E2.m1.2.3.3.cmml"><mi id="S3.E2.m1.2.3.3.2" xref="S3.E2.m1.2.3.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.3.3.1" xref="S3.E2.m1.2.3.3.1.cmml">​</mo><mi id="S3.E2.m1.2.3.3.3" xref="S3.E2.m1.2.3.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.3.3.1a" xref="S3.E2.m1.2.3.3.1.cmml">​</mo><mi id="S3.E2.m1.2.3.3.4" xref="S3.E2.m1.2.3.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.3.3.1b" xref="S3.E2.m1.2.3.3.1.cmml">​</mo><mi id="S3.E2.m1.2.3.3.5" xref="S3.E2.m1.2.3.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.3.3.1c" xref="S3.E2.m1.2.3.3.1.cmml">​</mo><mi id="S3.E2.m1.2.3.3.6" xref="S3.E2.m1.2.3.3.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.3.3.1d" xref="S3.E2.m1.2.3.3.1.cmml">​</mo><mi id="S3.E2.m1.2.3.3.7" xref="S3.E2.m1.2.3.3.7.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.3.3.1e" xref="S3.E2.m1.2.3.3.1.cmml">​</mo><mi id="S3.E2.m1.2.3.3.8" xref="S3.E2.m1.2.3.3.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.3.3.1f" xref="S3.E2.m1.2.3.3.1.cmml">​</mo><mi id="S3.E2.m1.2.3.3.9" xref="S3.E2.m1.2.3.3.9.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.3.3.1g" xref="S3.E2.m1.2.3.3.1.cmml">​</mo><mrow id="S3.E2.m1.2.3.3.10.2" xref="S3.E2.m1.2.3.3.10.1.cmml"><mo stretchy="false" id="S3.E2.m1.2.3.3.10.2.1" xref="S3.E2.m1.2.3.3.10.1.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">O</mi><mo id="S3.E2.m1.2.3.3.10.2.2" xref="S3.E2.m1.2.3.3.10.1.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">S</mi><mo stretchy="false" id="S3.E2.m1.2.3.3.10.2.3" xref="S3.E2.m1.2.3.3.10.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.3.cmml" xref="S3.E2.m1.2.3"><eq id="S3.E2.m1.2.3.1.cmml" xref="S3.E2.m1.2.3.1"></eq><apply id="S3.E2.m1.2.3.2.cmml" xref="S3.E2.m1.2.3.2"><times id="S3.E2.m1.2.3.2.1.cmml" xref="S3.E2.m1.2.3.2.1"></times><ci id="S3.E2.m1.2.3.2.2.cmml" xref="S3.E2.m1.2.3.2.2">𝑠</ci><ci id="S3.E2.m1.2.3.2.3.cmml" xref="S3.E2.m1.2.3.2.3">𝑐</ci><ci id="S3.E2.m1.2.3.2.4.cmml" xref="S3.E2.m1.2.3.2.4">𝑜</ci><ci id="S3.E2.m1.2.3.2.5.cmml" xref="S3.E2.m1.2.3.2.5">𝑟</ci><apply id="S3.E2.m1.2.3.2.6.cmml" xref="S3.E2.m1.2.3.2.6"><csymbol cd="ambiguous" id="S3.E2.m1.2.3.2.6.1.cmml" xref="S3.E2.m1.2.3.2.6">subscript</csymbol><ci id="S3.E2.m1.2.3.2.6.2.cmml" xref="S3.E2.m1.2.3.2.6.2">𝑒</ci><ci id="S3.E2.m1.2.3.2.6.3.cmml" xref="S3.E2.m1.2.3.2.6.3">𝑓</ci></apply></apply><apply id="S3.E2.m1.2.3.3.cmml" xref="S3.E2.m1.2.3.3"><times id="S3.E2.m1.2.3.3.1.cmml" xref="S3.E2.m1.2.3.3.1"></times><ci id="S3.E2.m1.2.3.3.2.cmml" xref="S3.E2.m1.2.3.3.2">𝑆</ci><ci id="S3.E2.m1.2.3.3.3.cmml" xref="S3.E2.m1.2.3.3.3">𝑝</ci><ci id="S3.E2.m1.2.3.3.4.cmml" xref="S3.E2.m1.2.3.3.4">𝑒</ci><ci id="S3.E2.m1.2.3.3.5.cmml" xref="S3.E2.m1.2.3.3.5">𝑎</ci><ci id="S3.E2.m1.2.3.3.6.cmml" xref="S3.E2.m1.2.3.3.6">𝑟</ci><ci id="S3.E2.m1.2.3.3.7.cmml" xref="S3.E2.m1.2.3.3.7">𝑚</ci><ci id="S3.E2.m1.2.3.3.8.cmml" xref="S3.E2.m1.2.3.3.8">𝑎</ci><ci id="S3.E2.m1.2.3.3.9.cmml" xref="S3.E2.m1.2.3.3.9">𝑛</ci><interval closure="open" id="S3.E2.m1.2.3.3.10.1.cmml" xref="S3.E2.m1.2.3.3.10.2"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑂</ci><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝑆</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">score_{f}=Spearman(O,S)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.5" class="ltx_p">where <math id="S3.SS4.p4.1.m1.1" class="ltx_Math" alttext="cos" display="inline"><semantics id="S3.SS4.p4.1.m1.1a"><mrow id="S3.SS4.p4.1.m1.1.1" xref="S3.SS4.p4.1.m1.1.1.cmml"><mi id="S3.SS4.p4.1.m1.1.1.2" xref="S3.SS4.p4.1.m1.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p4.1.m1.1.1.1" xref="S3.SS4.p4.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS4.p4.1.m1.1.1.3" xref="S3.SS4.p4.1.m1.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p4.1.m1.1.1.1a" xref="S3.SS4.p4.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS4.p4.1.m1.1.1.4" xref="S3.SS4.p4.1.m1.1.1.4.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.1.m1.1b"><apply id="S3.SS4.p4.1.m1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1"><times id="S3.SS4.p4.1.m1.1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1.1"></times><ci id="S3.SS4.p4.1.m1.1.1.2.cmml" xref="S3.SS4.p4.1.m1.1.1.2">𝑐</ci><ci id="S3.SS4.p4.1.m1.1.1.3.cmml" xref="S3.SS4.p4.1.m1.1.1.3">𝑜</ci><ci id="S3.SS4.p4.1.m1.1.1.4.cmml" xref="S3.SS4.p4.1.m1.1.1.4">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.1.m1.1c">cos</annotation></semantics></math> stands for cosine similarity, <math id="S3.SS4.p4.2.m2.1" class="ltx_Math" alttext="score_{f}" display="inline"><semantics id="S3.SS4.p4.2.m2.1a"><mrow id="S3.SS4.p4.2.m2.1.1" xref="S3.SS4.p4.2.m2.1.1.cmml"><mi id="S3.SS4.p4.2.m2.1.1.2" xref="S3.SS4.p4.2.m2.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p4.2.m2.1.1.1" xref="S3.SS4.p4.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS4.p4.2.m2.1.1.3" xref="S3.SS4.p4.2.m2.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p4.2.m2.1.1.1a" xref="S3.SS4.p4.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS4.p4.2.m2.1.1.4" xref="S3.SS4.p4.2.m2.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p4.2.m2.1.1.1b" xref="S3.SS4.p4.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS4.p4.2.m2.1.1.5" xref="S3.SS4.p4.2.m2.1.1.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p4.2.m2.1.1.1c" xref="S3.SS4.p4.2.m2.1.1.1.cmml">​</mo><msub id="S3.SS4.p4.2.m2.1.1.6" xref="S3.SS4.p4.2.m2.1.1.6.cmml"><mi id="S3.SS4.p4.2.m2.1.1.6.2" xref="S3.SS4.p4.2.m2.1.1.6.2.cmml">e</mi><mi id="S3.SS4.p4.2.m2.1.1.6.3" xref="S3.SS4.p4.2.m2.1.1.6.3.cmml">f</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.2.m2.1b"><apply id="S3.SS4.p4.2.m2.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1"><times id="S3.SS4.p4.2.m2.1.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1.1"></times><ci id="S3.SS4.p4.2.m2.1.1.2.cmml" xref="S3.SS4.p4.2.m2.1.1.2">𝑠</ci><ci id="S3.SS4.p4.2.m2.1.1.3.cmml" xref="S3.SS4.p4.2.m2.1.1.3">𝑐</ci><ci id="S3.SS4.p4.2.m2.1.1.4.cmml" xref="S3.SS4.p4.2.m2.1.1.4">𝑜</ci><ci id="S3.SS4.p4.2.m2.1.1.5.cmml" xref="S3.SS4.p4.2.m2.1.1.5">𝑟</ci><apply id="S3.SS4.p4.2.m2.1.1.6.cmml" xref="S3.SS4.p4.2.m2.1.1.6"><csymbol cd="ambiguous" id="S3.SS4.p4.2.m2.1.1.6.1.cmml" xref="S3.SS4.p4.2.m2.1.1.6">subscript</csymbol><ci id="S3.SS4.p4.2.m2.1.1.6.2.cmml" xref="S3.SS4.p4.2.m2.1.1.6.2">𝑒</ci><ci id="S3.SS4.p4.2.m2.1.1.6.3.cmml" xref="S3.SS4.p4.2.m2.1.1.6.3">𝑓</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.2.m2.1c">score_{f}</annotation></semantics></math> is the performance score of the evaluator <math id="S3.SS4.p4.3.m3.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.SS4.p4.3.m3.1a"><mi id="S3.SS4.p4.3.m3.1.1" xref="S3.SS4.p4.3.m3.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.3.m3.1b"><ci id="S3.SS4.p4.3.m3.1.1.cmml" xref="S3.SS4.p4.3.m3.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.3.m3.1c">f</annotation></semantics></math>, with <math id="S3.SS4.p4.4.m4.1" class="ltx_Math" alttext="O" display="inline"><semantics id="S3.SS4.p4.4.m4.1a"><mi id="S3.SS4.p4.4.m4.1.1" xref="S3.SS4.p4.4.m4.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.4.m4.1b"><ci id="S3.SS4.p4.4.m4.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.4.m4.1c">O</annotation></semantics></math> and <math id="S3.SS4.p4.5.m5.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS4.p4.5.m5.1a"><mi id="S3.SS4.p4.5.m5.1.1" xref="S3.SS4.p4.5.m5.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.5.m5.1b"><ci id="S3.SS4.p4.5.m5.1.1.cmml" xref="S3.SS4.p4.5.m5.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.5.m5.1c">S</annotation></semantics></math> indicating the lists of all predicted scores and annotated scores respectively. Note that the metrics used for evaluating evaluators is Spearman’s rank coefficient of correlation (Spearman).</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<p id="S3.SS4.p5.1" class="ltx_p">The key properties of Alignment, Consistency and Generalization introduced in Section <a href="#S3.SS2" title="3.2. Three Key Properties in VQA Evaluation ‣ 3. Semantic Evaluation of VQA ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> are computed as follows:</p>
</div>
<section id="S3.SS4.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Alignment</h5>

<div id="S3.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS4.SSS0.Px1.p1.1" class="ltx_p">We use the average result of an evaluator on all parts of the proposed AVE dataset as alignment:</p>
</div>
<div id="S3.SS4.SSS0.Px1.p2" class="ltx_para">
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="Alignment=\frac{1}{N_{Parts}}\sum^{N_{Parts}}_{i=1}score_{f_{i}}" display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mrow id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.2.2" xref="S3.E3.m1.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.2.1" xref="S3.E3.m1.1.1.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.2.3" xref="S3.E3.m1.1.1.2.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.2.1a" xref="S3.E3.m1.1.1.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.2.4" xref="S3.E3.m1.1.1.2.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.2.1b" xref="S3.E3.m1.1.1.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.2.5" xref="S3.E3.m1.1.1.2.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.2.1c" xref="S3.E3.m1.1.1.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.2.6" xref="S3.E3.m1.1.1.2.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.2.1d" xref="S3.E3.m1.1.1.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.2.7" xref="S3.E3.m1.1.1.2.7.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.2.1e" xref="S3.E3.m1.1.1.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.2.8" xref="S3.E3.m1.1.1.2.8.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.2.1f" xref="S3.E3.m1.1.1.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.2.9" xref="S3.E3.m1.1.1.2.9.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.2.1g" xref="S3.E3.m1.1.1.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.2.10" xref="S3.E3.m1.1.1.2.10.cmml">t</mi></mrow><mo id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mfrac id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml"><mn id="S3.E3.m1.1.1.3.2.2" xref="S3.E3.m1.1.1.3.2.2.cmml">1</mn><msub id="S3.E3.m1.1.1.3.2.3" xref="S3.E3.m1.1.1.3.2.3.cmml"><mi id="S3.E3.m1.1.1.3.2.3.2" xref="S3.E3.m1.1.1.3.2.3.2.cmml">N</mi><mrow id="S3.E3.m1.1.1.3.2.3.3" xref="S3.E3.m1.1.1.3.2.3.3.cmml"><mi id="S3.E3.m1.1.1.3.2.3.3.2" xref="S3.E3.m1.1.1.3.2.3.3.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.2.3.3.1" xref="S3.E3.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.2.3.3.3" xref="S3.E3.m1.1.1.3.2.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.2.3.3.1a" xref="S3.E3.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.2.3.3.4" xref="S3.E3.m1.1.1.3.2.3.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.2.3.3.1b" xref="S3.E3.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.2.3.3.5" xref="S3.E3.m1.1.1.3.2.3.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.2.3.3.1c" xref="S3.E3.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.2.3.3.6" xref="S3.E3.m1.1.1.3.2.3.3.6.cmml">s</mi></mrow></msub></mfrac><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.1" xref="S3.E3.m1.1.1.3.1.cmml">​</mo><mrow id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml"><munderover id="S3.E3.m1.1.1.3.3.1" xref="S3.E3.m1.1.1.3.3.1.cmml"><mo movablelimits="false" id="S3.E3.m1.1.1.3.3.1.2.2" xref="S3.E3.m1.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S3.E3.m1.1.1.3.3.1.3" xref="S3.E3.m1.1.1.3.3.1.3.cmml"><mi id="S3.E3.m1.1.1.3.3.1.3.2" xref="S3.E3.m1.1.1.3.3.1.3.2.cmml">i</mi><mo id="S3.E3.m1.1.1.3.3.1.3.1" xref="S3.E3.m1.1.1.3.3.1.3.1.cmml">=</mo><mn id="S3.E3.m1.1.1.3.3.1.3.3" xref="S3.E3.m1.1.1.3.3.1.3.3.cmml">1</mn></mrow><msub id="S3.E3.m1.1.1.3.3.1.2.3" xref="S3.E3.m1.1.1.3.3.1.2.3.cmml"><mi id="S3.E3.m1.1.1.3.3.1.2.3.2" xref="S3.E3.m1.1.1.3.3.1.2.3.2.cmml">N</mi><mrow id="S3.E3.m1.1.1.3.3.1.2.3.3" xref="S3.E3.m1.1.1.3.3.1.2.3.3.cmml"><mi id="S3.E3.m1.1.1.3.3.1.2.3.3.2" xref="S3.E3.m1.1.1.3.3.1.2.3.3.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.1.2.3.3.1" xref="S3.E3.m1.1.1.3.3.1.2.3.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.3.1.2.3.3.3" xref="S3.E3.m1.1.1.3.3.1.2.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.1.2.3.3.1a" xref="S3.E3.m1.1.1.3.3.1.2.3.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.3.1.2.3.3.4" xref="S3.E3.m1.1.1.3.3.1.2.3.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.1.2.3.3.1b" xref="S3.E3.m1.1.1.3.3.1.2.3.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.3.1.2.3.3.5" xref="S3.E3.m1.1.1.3.3.1.2.3.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.1.2.3.3.1c" xref="S3.E3.m1.1.1.3.3.1.2.3.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.3.1.2.3.3.6" xref="S3.E3.m1.1.1.3.3.1.2.3.3.6.cmml">s</mi></mrow></msub></munderover><mrow id="S3.E3.m1.1.1.3.3.2" xref="S3.E3.m1.1.1.3.3.2.cmml"><mi id="S3.E3.m1.1.1.3.3.2.2" xref="S3.E3.m1.1.1.3.3.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.2.1" xref="S3.E3.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.3.2.3" xref="S3.E3.m1.1.1.3.3.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.2.1a" xref="S3.E3.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.3.2.4" xref="S3.E3.m1.1.1.3.3.2.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.2.1b" xref="S3.E3.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.3.2.5" xref="S3.E3.m1.1.1.3.3.2.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.2.1c" xref="S3.E3.m1.1.1.3.3.2.1.cmml">​</mo><msub id="S3.E3.m1.1.1.3.3.2.6" xref="S3.E3.m1.1.1.3.3.2.6.cmml"><mi id="S3.E3.m1.1.1.3.3.2.6.2" xref="S3.E3.m1.1.1.3.3.2.6.2.cmml">e</mi><msub id="S3.E3.m1.1.1.3.3.2.6.3" xref="S3.E3.m1.1.1.3.3.2.6.3.cmml"><mi id="S3.E3.m1.1.1.3.3.2.6.3.2" xref="S3.E3.m1.1.1.3.3.2.6.3.2.cmml">f</mi><mi id="S3.E3.m1.1.1.3.3.2.6.3.3" xref="S3.E3.m1.1.1.3.3.2.6.3.3.cmml">i</mi></msub></msub></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"></eq><apply id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"><times id="S3.E3.m1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.2.1"></times><ci id="S3.E3.m1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.2.2">𝐴</ci><ci id="S3.E3.m1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.2.3">𝑙</ci><ci id="S3.E3.m1.1.1.2.4.cmml" xref="S3.E3.m1.1.1.2.4">𝑖</ci><ci id="S3.E3.m1.1.1.2.5.cmml" xref="S3.E3.m1.1.1.2.5">𝑔</ci><ci id="S3.E3.m1.1.1.2.6.cmml" xref="S3.E3.m1.1.1.2.6">𝑛</ci><ci id="S3.E3.m1.1.1.2.7.cmml" xref="S3.E3.m1.1.1.2.7">𝑚</ci><ci id="S3.E3.m1.1.1.2.8.cmml" xref="S3.E3.m1.1.1.2.8">𝑒</ci><ci id="S3.E3.m1.1.1.2.9.cmml" xref="S3.E3.m1.1.1.2.9">𝑛</ci><ci id="S3.E3.m1.1.1.2.10.cmml" xref="S3.E3.m1.1.1.2.10">𝑡</ci></apply><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><times id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3.1"></times><apply id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2"><divide id="S3.E3.m1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.3.2"></divide><cn type="integer" id="S3.E3.m1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2">1</cn><apply id="S3.E3.m1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.2.3.1.cmml" xref="S3.E3.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.3.2.cmml" xref="S3.E3.m1.1.1.3.2.3.2">𝑁</ci><apply id="S3.E3.m1.1.1.3.2.3.3.cmml" xref="S3.E3.m1.1.1.3.2.3.3"><times id="S3.E3.m1.1.1.3.2.3.3.1.cmml" xref="S3.E3.m1.1.1.3.2.3.3.1"></times><ci id="S3.E3.m1.1.1.3.2.3.3.2.cmml" xref="S3.E3.m1.1.1.3.2.3.3.2">𝑃</ci><ci id="S3.E3.m1.1.1.3.2.3.3.3.cmml" xref="S3.E3.m1.1.1.3.2.3.3.3">𝑎</ci><ci id="S3.E3.m1.1.1.3.2.3.3.4.cmml" xref="S3.E3.m1.1.1.3.2.3.3.4">𝑟</ci><ci id="S3.E3.m1.1.1.3.2.3.3.5.cmml" xref="S3.E3.m1.1.1.3.2.3.3.5">𝑡</ci><ci id="S3.E3.m1.1.1.3.2.3.3.6.cmml" xref="S3.E3.m1.1.1.3.2.3.3.6">𝑠</ci></apply></apply></apply><apply id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3"><apply id="S3.E3.m1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.1.1.cmml" xref="S3.E3.m1.1.1.3.3.1">subscript</csymbol><apply id="S3.E3.m1.1.1.3.3.1.2.cmml" xref="S3.E3.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.1.2.1.cmml" xref="S3.E3.m1.1.1.3.3.1">superscript</csymbol><sum id="S3.E3.m1.1.1.3.3.1.2.2.cmml" xref="S3.E3.m1.1.1.3.3.1.2.2"></sum><apply id="S3.E3.m1.1.1.3.3.1.2.3.cmml" xref="S3.E3.m1.1.1.3.3.1.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.1.2.3.1.cmml" xref="S3.E3.m1.1.1.3.3.1.2.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.3.1.2.3.2.cmml" xref="S3.E3.m1.1.1.3.3.1.2.3.2">𝑁</ci><apply id="S3.E3.m1.1.1.3.3.1.2.3.3.cmml" xref="S3.E3.m1.1.1.3.3.1.2.3.3"><times id="S3.E3.m1.1.1.3.3.1.2.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3.1.2.3.3.1"></times><ci id="S3.E3.m1.1.1.3.3.1.2.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.1.2.3.3.2">𝑃</ci><ci id="S3.E3.m1.1.1.3.3.1.2.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.1.2.3.3.3">𝑎</ci><ci id="S3.E3.m1.1.1.3.3.1.2.3.3.4.cmml" xref="S3.E3.m1.1.1.3.3.1.2.3.3.4">𝑟</ci><ci id="S3.E3.m1.1.1.3.3.1.2.3.3.5.cmml" xref="S3.E3.m1.1.1.3.3.1.2.3.3.5">𝑡</ci><ci id="S3.E3.m1.1.1.3.3.1.2.3.3.6.cmml" xref="S3.E3.m1.1.1.3.3.1.2.3.3.6">𝑠</ci></apply></apply></apply><apply id="S3.E3.m1.1.1.3.3.1.3.cmml" xref="S3.E3.m1.1.1.3.3.1.3"><eq id="S3.E3.m1.1.1.3.3.1.3.1.cmml" xref="S3.E3.m1.1.1.3.3.1.3.1"></eq><ci id="S3.E3.m1.1.1.3.3.1.3.2.cmml" xref="S3.E3.m1.1.1.3.3.1.3.2">𝑖</ci><cn type="integer" id="S3.E3.m1.1.1.3.3.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3.1.3.3">1</cn></apply></apply><apply id="S3.E3.m1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2"><times id="S3.E3.m1.1.1.3.3.2.1.cmml" xref="S3.E3.m1.1.1.3.3.2.1"></times><ci id="S3.E3.m1.1.1.3.3.2.2.cmml" xref="S3.E3.m1.1.1.3.3.2.2">𝑠</ci><ci id="S3.E3.m1.1.1.3.3.2.3.cmml" xref="S3.E3.m1.1.1.3.3.2.3">𝑐</ci><ci id="S3.E3.m1.1.1.3.3.2.4.cmml" xref="S3.E3.m1.1.1.3.3.2.4">𝑜</ci><ci id="S3.E3.m1.1.1.3.3.2.5.cmml" xref="S3.E3.m1.1.1.3.3.2.5">𝑟</ci><apply id="S3.E3.m1.1.1.3.3.2.6.cmml" xref="S3.E3.m1.1.1.3.3.2.6"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.2.6.1.cmml" xref="S3.E3.m1.1.1.3.3.2.6">subscript</csymbol><ci id="S3.E3.m1.1.1.3.3.2.6.2.cmml" xref="S3.E3.m1.1.1.3.3.2.6.2">𝑒</ci><apply id="S3.E3.m1.1.1.3.3.2.6.3.cmml" xref="S3.E3.m1.1.1.3.3.2.6.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.2.6.3.1.cmml" xref="S3.E3.m1.1.1.3.3.2.6.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.3.2.6.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2.6.3.2">𝑓</ci><ci id="S3.E3.m1.1.1.3.3.2.6.3.3.cmml" xref="S3.E3.m1.1.1.3.3.2.6.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">Alignment=\frac{1}{N_{Parts}}\sum^{N_{Parts}}_{i=1}score_{f_{i}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.SSS0.Px1.p3" class="ltx_para">
<p id="S3.SS4.SSS0.Px1.p3.5" class="ltx_p">where <math id="S3.SS4.SSS0.Px1.p3.1.m1.1" class="ltx_Math" alttext="N_{Parts}" display="inline"><semantics id="S3.SS4.SSS0.Px1.p3.1.m1.1a"><msub id="S3.SS4.SSS0.Px1.p3.1.m1.1.1" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.cmml"><mi id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.2" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.2.cmml">N</mi><mrow id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.cmml"><mi id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.2" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.1" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.3" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.1a" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.4" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.1b" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.5" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.1c" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.6" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.6.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px1.p3.1.m1.1b"><apply id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.cmml" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.1.cmml" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.2.cmml" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.2">𝑁</ci><apply id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.cmml" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3"><times id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.1.cmml" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.1"></times><ci id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.2.cmml" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.2">𝑃</ci><ci id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.3.cmml" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.3">𝑎</ci><ci id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.4.cmml" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.4">𝑟</ci><ci id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.5.cmml" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.5">𝑡</ci><ci id="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.6.cmml" xref="S3.SS4.SSS0.Px1.p3.1.m1.1.1.3.6">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px1.p3.1.m1.1c">N_{Parts}</annotation></semantics></math> is the number of Parts in the AVE dataset and <math id="S3.SS4.SSS0.Px1.p3.2.m2.1" class="ltx_Math" alttext="N_{sets}" display="inline"><semantics id="S3.SS4.SSS0.Px1.p3.2.m2.1a"><msub id="S3.SS4.SSS0.Px1.p3.2.m2.1.1" xref="S3.SS4.SSS0.Px1.p3.2.m2.1.1.cmml"><mi id="S3.SS4.SSS0.Px1.p3.2.m2.1.1.2" xref="S3.SS4.SSS0.Px1.p3.2.m2.1.1.2.cmml">N</mi><mrow id="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3" xref="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.cmml"><mi id="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.2" xref="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.1" xref="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.3" xref="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.1a" xref="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.4" xref="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.1b" xref="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.5" xref="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.5.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px1.p3.2.m2.1b"><apply id="S3.SS4.SSS0.Px1.p3.2.m2.1.1.cmml" xref="S3.SS4.SSS0.Px1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS0.Px1.p3.2.m2.1.1.1.cmml" xref="S3.SS4.SSS0.Px1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.SSS0.Px1.p3.2.m2.1.1.2.cmml" xref="S3.SS4.SSS0.Px1.p3.2.m2.1.1.2">𝑁</ci><apply id="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.cmml" xref="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3"><times id="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.1.cmml" xref="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.1"></times><ci id="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.2.cmml" xref="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.2">𝑠</ci><ci id="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.3.cmml" xref="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.3">𝑒</ci><ci id="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.4.cmml" xref="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.4">𝑡</ci><ci id="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.5.cmml" xref="S3.SS4.SSS0.Px1.p3.2.m2.1.1.3.5">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px1.p3.2.m2.1c">N_{sets}</annotation></semantics></math> is the number of involved VQA datasets in AVE, according to different type of division. The <math id="S3.SS4.SSS0.Px1.p3.3.m3.1" class="ltx_Math" alttext="score_{f_{i}}" display="inline"><semantics id="S3.SS4.SSS0.Px1.p3.3.m3.1a"><mrow id="S3.SS4.SSS0.Px1.p3.3.m3.1.1" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.cmml"><mi id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.2" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.1" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.3" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.1a" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.4" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.1b" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.5" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.1c" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.1.cmml">​</mo><msub id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.cmml"><mi id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.2" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.2.cmml">e</mi><msub id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.3" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.3.cmml"><mi id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.3.2" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.3.2.cmml">f</mi><mi id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.3.3" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.3.3.cmml">i</mi></msub></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px1.p3.3.m3.1b"><apply id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.cmml" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1"><times id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.1.cmml" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.1"></times><ci id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.2.cmml" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.2">𝑠</ci><ci id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.3.cmml" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.3">𝑐</ci><ci id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.4.cmml" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.4">𝑜</ci><ci id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.5.cmml" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.5">𝑟</ci><apply id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.cmml" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6"><csymbol cd="ambiguous" id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.1.cmml" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6">subscript</csymbol><ci id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.2.cmml" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.2">𝑒</ci><apply id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.3.cmml" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.3"><csymbol cd="ambiguous" id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.3.1.cmml" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.3">subscript</csymbol><ci id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.3.2.cmml" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.3.2">𝑓</ci><ci id="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.3.3.cmml" xref="S3.SS4.SSS0.Px1.p3.3.m3.1.1.6.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px1.p3.3.m3.1c">score_{f_{i}}</annotation></semantics></math> is the spearman score of evaluator <math id="S3.SS4.SSS0.Px1.p3.4.m4.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.SS4.SSS0.Px1.p3.4.m4.1a"><mi id="S3.SS4.SSS0.Px1.p3.4.m4.1.1" xref="S3.SS4.SSS0.Px1.p3.4.m4.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px1.p3.4.m4.1b"><ci id="S3.SS4.SSS0.Px1.p3.4.m4.1.1.cmml" xref="S3.SS4.SSS0.Px1.p3.4.m4.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px1.p3.4.m4.1c">f</annotation></semantics></math> on the <math id="S3.SS4.SSS0.Px1.p3.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS4.SSS0.Px1.p3.5.m5.1a"><mi id="S3.SS4.SSS0.Px1.p3.5.m5.1.1" xref="S3.SS4.SSS0.Px1.p3.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px1.p3.5.m5.1b"><ci id="S3.SS4.SSS0.Px1.p3.5.m5.1.1.cmml" xref="S3.SS4.SSS0.Px1.p3.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px1.p3.5.m5.1c">i</annotation></semantics></math> th part of AVE.</p>
</div>
</section>
<section id="S3.SS4.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Consistency</h5>

<div id="S3.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS4.SSS0.Px2.p1.1" class="ltx_p">Consistency measures how close the responses of the same meaning with different morphology are evaluated. We regard the variance of the same sample among the three parts of AVE as consistency:</p>
</div>
<div id="S3.SS4.SSS0.Px2.p2" class="ltx_para">
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.2" class="ltx_Math" alttext="Consistency=\log{(1/(\frac{1}{N_{samples}}\sum^{N_{samples}}_{j=1}var(o_{j_{1}},o_{j_{2}},o_{j_{3}})))}" display="block"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml"><mrow id="S3.E4.m1.2.2.3" xref="S3.E4.m1.2.2.3.cmml"><mi id="S3.E4.m1.2.2.3.2" xref="S3.E4.m1.2.2.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.3.1" xref="S3.E4.m1.2.2.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.3.3" xref="S3.E4.m1.2.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.3.1a" xref="S3.E4.m1.2.2.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.3.4" xref="S3.E4.m1.2.2.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.3.1b" xref="S3.E4.m1.2.2.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.3.5" xref="S3.E4.m1.2.2.3.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.3.1c" xref="S3.E4.m1.2.2.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.3.6" xref="S3.E4.m1.2.2.3.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.3.1d" xref="S3.E4.m1.2.2.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.3.7" xref="S3.E4.m1.2.2.3.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.3.1e" xref="S3.E4.m1.2.2.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.3.8" xref="S3.E4.m1.2.2.3.8.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.3.1f" xref="S3.E4.m1.2.2.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.3.9" xref="S3.E4.m1.2.2.3.9.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.3.1g" xref="S3.E4.m1.2.2.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.3.10" xref="S3.E4.m1.2.2.3.10.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.3.1h" xref="S3.E4.m1.2.2.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.3.11" xref="S3.E4.m1.2.2.3.11.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.3.1i" xref="S3.E4.m1.2.2.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.3.12" xref="S3.E4.m1.2.2.3.12.cmml">y</mi></mrow><mo id="S3.E4.m1.2.2.2" xref="S3.E4.m1.2.2.2.cmml">=</mo><mrow id="S3.E4.m1.2.2.1.1" xref="S3.E4.m1.2.2.1.2.cmml"><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">log</mi><mo id="S3.E4.m1.2.2.1.1a" xref="S3.E4.m1.2.2.1.2.cmml">⁡</mo><mrow id="S3.E4.m1.2.2.1.1.1" xref="S3.E4.m1.2.2.1.2.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.2" xref="S3.E4.m1.2.2.1.2.cmml">(</mo><mrow id="S3.E4.m1.2.2.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.cmml"><mn id="S3.E4.m1.2.2.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.3.cmml">1</mn><mo id="S3.E4.m1.2.2.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.2.cmml">/</mo><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml"><mfrac id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.cmml"><mn id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.2.cmml">1</mn><msub id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.2.cmml">N</mi><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.1a" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.4" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.1b" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.5" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.1c" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.6" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.1d" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.7" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.1e" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.8" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.8.cmml">s</mi></mrow></msub></mfrac><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.4" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.4.cmml">​</mo><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.cmml"><munderover id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.cmml"><mo movablelimits="false" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.2.cmml">∑</mo><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.3.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.3.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.3.2.cmml">j</mi><mo id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.3.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.3.1.cmml">=</mo><mn id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.3.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.3.3.cmml">1</mn></mrow><msub id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.2.cmml">N</mi><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.1a" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.4" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.1b" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.5" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.1c" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.6" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.1d" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.7" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.1e" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.8" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.8.cmml">s</mi></mrow></msub></munderover><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.5" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.5.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.4" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.4.cmml">​</mo><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.6" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.4a" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.4.cmml">​</mo><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.7" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.7.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.4b" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.4.cmml">​</mo><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.4.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.4" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.4.cmml">(</mo><msub id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">o</mi><msub id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">j</mi><mn id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></msub></msub><mo id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.5" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.4.cmml">,</mo><msub id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.2.cmml">o</mi><msub id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.3.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.3.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.3.2.cmml">j</mi><mn id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.3.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.3.3.cmml">2</mn></msub></msub><mo id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.6" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.4.cmml">,</mo><msub id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.2.cmml">o</mi><msub id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.3.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.3.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.3.2.cmml">j</mi><mn id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.3.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.3.3.cmml">3</mn></msub></msub><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.7" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.4.cmml">)</mo></mrow></mrow></mrow></mrow><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.3" xref="S3.E4.m1.2.2.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2"><eq id="S3.E4.m1.2.2.2.cmml" xref="S3.E4.m1.2.2.2"></eq><apply id="S3.E4.m1.2.2.3.cmml" xref="S3.E4.m1.2.2.3"><times id="S3.E4.m1.2.2.3.1.cmml" xref="S3.E4.m1.2.2.3.1"></times><ci id="S3.E4.m1.2.2.3.2.cmml" xref="S3.E4.m1.2.2.3.2">𝐶</ci><ci id="S3.E4.m1.2.2.3.3.cmml" xref="S3.E4.m1.2.2.3.3">𝑜</ci><ci id="S3.E4.m1.2.2.3.4.cmml" xref="S3.E4.m1.2.2.3.4">𝑛</ci><ci id="S3.E4.m1.2.2.3.5.cmml" xref="S3.E4.m1.2.2.3.5">𝑠</ci><ci id="S3.E4.m1.2.2.3.6.cmml" xref="S3.E4.m1.2.2.3.6">𝑖</ci><ci id="S3.E4.m1.2.2.3.7.cmml" xref="S3.E4.m1.2.2.3.7">𝑠</ci><ci id="S3.E4.m1.2.2.3.8.cmml" xref="S3.E4.m1.2.2.3.8">𝑡</ci><ci id="S3.E4.m1.2.2.3.9.cmml" xref="S3.E4.m1.2.2.3.9">𝑒</ci><ci id="S3.E4.m1.2.2.3.10.cmml" xref="S3.E4.m1.2.2.3.10">𝑛</ci><ci id="S3.E4.m1.2.2.3.11.cmml" xref="S3.E4.m1.2.2.3.11">𝑐</ci><ci id="S3.E4.m1.2.2.3.12.cmml" xref="S3.E4.m1.2.2.3.12">𝑦</ci></apply><apply id="S3.E4.m1.2.2.1.2.cmml" xref="S3.E4.m1.2.2.1.1"><log id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"></log><apply id="S3.E4.m1.2.2.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1"><divide id="S3.E4.m1.2.2.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.2"></divide><cn type="integer" id="S3.E4.m1.2.2.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.3">1</cn><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1"><times id="S3.E4.m1.2.2.1.1.1.1.1.1.1.4.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.4"></times><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5"><divide id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5"></divide><cn type="integer" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.2">1</cn><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.2">𝑁</ci><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3"><times id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.1"></times><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.2">𝑠</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.3">𝑎</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.4.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.4">𝑚</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.5.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.5">𝑝</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.6.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.6">𝑙</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.7.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.7">𝑒</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.8.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.5.3.3.8">𝑠</ci></apply></apply></apply><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3"><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4">subscript</csymbol><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4">superscript</csymbol><sum id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.2"></sum><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.2">𝑁</ci><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3"><times id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.1"></times><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.2">𝑠</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.3">𝑎</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.4.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.4">𝑚</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.5.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.5">𝑝</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.6.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.6">𝑙</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.7.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.7">𝑒</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.8.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.2.3.3.8">𝑠</ci></apply></apply></apply><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.3"><eq id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.3.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.3.1"></eq><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.3.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.3.2">𝑗</ci><cn type="integer" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.3.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.4.3.3">1</cn></apply></apply><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3"><times id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.4.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.4"></times><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.5.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.5">𝑣</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.6.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.6">𝑎</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.7.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.7">𝑟</ci><vector id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.4.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3"><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑜</ci><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑗</ci><cn type="integer" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.2">𝑜</ci><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.3.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.3">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.3.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.3.2">𝑗</ci><cn type="integer" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.3.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.3.3">2</cn></apply></apply><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.2">𝑜</ci><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.3.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.3.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.3.2">𝑗</ci><cn type="integer" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.3.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.3.3.3.3.3">3</cn></apply></apply></vector></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">Consistency=\log{(1/(\frac{1}{N_{samples}}\sum^{N_{samples}}_{j=1}var(o_{j_{1}},o_{j_{2}},o_{j_{3}})))}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.SSS0.Px2.p3" class="ltx_para">
<p id="S3.SS4.SSS0.Px2.p3.5" class="ltx_p">where <math id="S3.SS4.SSS0.Px2.p3.1.m1.1" class="ltx_Math" alttext="N_{samples}" display="inline"><semantics id="S3.SS4.SSS0.Px2.p3.1.m1.1a"><msub id="S3.SS4.SSS0.Px2.p3.1.m1.1.1" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.cmml"><mi id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.2" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.2.cmml">N</mi><mrow id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.cmml"><mi id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.2" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.1" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.3" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.1a" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.4" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.1b" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.5" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.1c" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.6" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.1d" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.7" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.1e" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.8" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.8.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px2.p3.1.m1.1b"><apply id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.1.cmml" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.2.cmml" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.2">𝑁</ci><apply id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.cmml" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3"><times id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.1.cmml" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.1"></times><ci id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.2.cmml" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.2">𝑠</ci><ci id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.3.cmml" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.3">𝑎</ci><ci id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.4.cmml" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.4">𝑚</ci><ci id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.5.cmml" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.5">𝑝</ci><ci id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.6.cmml" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.6">𝑙</ci><ci id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.7.cmml" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.7">𝑒</ci><ci id="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.8.cmml" xref="S3.SS4.SSS0.Px2.p3.1.m1.1.1.3.8">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px2.p3.1.m1.1c">N_{samples}</annotation></semantics></math> is the amount of samples and var denotes calculating the variance. Then, <math id="S3.SS4.SSS0.Px2.p3.2.m2.1" class="ltx_Math" alttext="o_{j_{1}}" display="inline"><semantics id="S3.SS4.SSS0.Px2.p3.2.m2.1a"><msub id="S3.SS4.SSS0.Px2.p3.2.m2.1.1" xref="S3.SS4.SSS0.Px2.p3.2.m2.1.1.cmml"><mi id="S3.SS4.SSS0.Px2.p3.2.m2.1.1.2" xref="S3.SS4.SSS0.Px2.p3.2.m2.1.1.2.cmml">o</mi><msub id="S3.SS4.SSS0.Px2.p3.2.m2.1.1.3" xref="S3.SS4.SSS0.Px2.p3.2.m2.1.1.3.cmml"><mi id="S3.SS4.SSS0.Px2.p3.2.m2.1.1.3.2" xref="S3.SS4.SSS0.Px2.p3.2.m2.1.1.3.2.cmml">j</mi><mn id="S3.SS4.SSS0.Px2.p3.2.m2.1.1.3.3" xref="S3.SS4.SSS0.Px2.p3.2.m2.1.1.3.3.cmml">1</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px2.p3.2.m2.1b"><apply id="S3.SS4.SSS0.Px2.p3.2.m2.1.1.cmml" xref="S3.SS4.SSS0.Px2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS0.Px2.p3.2.m2.1.1.1.cmml" xref="S3.SS4.SSS0.Px2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.SSS0.Px2.p3.2.m2.1.1.2.cmml" xref="S3.SS4.SSS0.Px2.p3.2.m2.1.1.2">𝑜</ci><apply id="S3.SS4.SSS0.Px2.p3.2.m2.1.1.3.cmml" xref="S3.SS4.SSS0.Px2.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS0.Px2.p3.2.m2.1.1.3.1.cmml" xref="S3.SS4.SSS0.Px2.p3.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS4.SSS0.Px2.p3.2.m2.1.1.3.2.cmml" xref="S3.SS4.SSS0.Px2.p3.2.m2.1.1.3.2">𝑗</ci><cn type="integer" id="S3.SS4.SSS0.Px2.p3.2.m2.1.1.3.3.cmml" xref="S3.SS4.SSS0.Px2.p3.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px2.p3.2.m2.1c">o_{j_{1}}</annotation></semantics></math>, <math id="S3.SS4.SSS0.Px2.p3.3.m3.1" class="ltx_Math" alttext="o_{j_{2}}" display="inline"><semantics id="S3.SS4.SSS0.Px2.p3.3.m3.1a"><msub id="S3.SS4.SSS0.Px2.p3.3.m3.1.1" xref="S3.SS4.SSS0.Px2.p3.3.m3.1.1.cmml"><mi id="S3.SS4.SSS0.Px2.p3.3.m3.1.1.2" xref="S3.SS4.SSS0.Px2.p3.3.m3.1.1.2.cmml">o</mi><msub id="S3.SS4.SSS0.Px2.p3.3.m3.1.1.3" xref="S3.SS4.SSS0.Px2.p3.3.m3.1.1.3.cmml"><mi id="S3.SS4.SSS0.Px2.p3.3.m3.1.1.3.2" xref="S3.SS4.SSS0.Px2.p3.3.m3.1.1.3.2.cmml">j</mi><mn id="S3.SS4.SSS0.Px2.p3.3.m3.1.1.3.3" xref="S3.SS4.SSS0.Px2.p3.3.m3.1.1.3.3.cmml">2</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px2.p3.3.m3.1b"><apply id="S3.SS4.SSS0.Px2.p3.3.m3.1.1.cmml" xref="S3.SS4.SSS0.Px2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS0.Px2.p3.3.m3.1.1.1.cmml" xref="S3.SS4.SSS0.Px2.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.SSS0.Px2.p3.3.m3.1.1.2.cmml" xref="S3.SS4.SSS0.Px2.p3.3.m3.1.1.2">𝑜</ci><apply id="S3.SS4.SSS0.Px2.p3.3.m3.1.1.3.cmml" xref="S3.SS4.SSS0.Px2.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS0.Px2.p3.3.m3.1.1.3.1.cmml" xref="S3.SS4.SSS0.Px2.p3.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS4.SSS0.Px2.p3.3.m3.1.1.3.2.cmml" xref="S3.SS4.SSS0.Px2.p3.3.m3.1.1.3.2">𝑗</ci><cn type="integer" id="S3.SS4.SSS0.Px2.p3.3.m3.1.1.3.3.cmml" xref="S3.SS4.SSS0.Px2.p3.3.m3.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px2.p3.3.m3.1c">o_{j_{2}}</annotation></semantics></math>, <math id="S3.SS4.SSS0.Px2.p3.4.m4.1" class="ltx_Math" alttext="o_{j_{3}}" display="inline"><semantics id="S3.SS4.SSS0.Px2.p3.4.m4.1a"><msub id="S3.SS4.SSS0.Px2.p3.4.m4.1.1" xref="S3.SS4.SSS0.Px2.p3.4.m4.1.1.cmml"><mi id="S3.SS4.SSS0.Px2.p3.4.m4.1.1.2" xref="S3.SS4.SSS0.Px2.p3.4.m4.1.1.2.cmml">o</mi><msub id="S3.SS4.SSS0.Px2.p3.4.m4.1.1.3" xref="S3.SS4.SSS0.Px2.p3.4.m4.1.1.3.cmml"><mi id="S3.SS4.SSS0.Px2.p3.4.m4.1.1.3.2" xref="S3.SS4.SSS0.Px2.p3.4.m4.1.1.3.2.cmml">j</mi><mn id="S3.SS4.SSS0.Px2.p3.4.m4.1.1.3.3" xref="S3.SS4.SSS0.Px2.p3.4.m4.1.1.3.3.cmml">3</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px2.p3.4.m4.1b"><apply id="S3.SS4.SSS0.Px2.p3.4.m4.1.1.cmml" xref="S3.SS4.SSS0.Px2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS0.Px2.p3.4.m4.1.1.1.cmml" xref="S3.SS4.SSS0.Px2.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.SSS0.Px2.p3.4.m4.1.1.2.cmml" xref="S3.SS4.SSS0.Px2.p3.4.m4.1.1.2">𝑜</ci><apply id="S3.SS4.SSS0.Px2.p3.4.m4.1.1.3.cmml" xref="S3.SS4.SSS0.Px2.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS0.Px2.p3.4.m4.1.1.3.1.cmml" xref="S3.SS4.SSS0.Px2.p3.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS4.SSS0.Px2.p3.4.m4.1.1.3.2.cmml" xref="S3.SS4.SSS0.Px2.p3.4.m4.1.1.3.2">𝑗</ci><cn type="integer" id="S3.SS4.SSS0.Px2.p3.4.m4.1.1.3.3.cmml" xref="S3.SS4.SSS0.Px2.p3.4.m4.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px2.p3.4.m4.1c">o_{j_{3}}</annotation></semantics></math> are the predicted scores of the evaluator on Part 1, 2, 3 for the same sample <math id="S3.SS4.SSS0.Px2.p3.5.m5.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS4.SSS0.Px2.p3.5.m5.1a"><mi id="S3.SS4.SSS0.Px2.p3.5.m5.1.1" xref="S3.SS4.SSS0.Px2.p3.5.m5.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px2.p3.5.m5.1b"><ci id="S3.SS4.SSS0.Px2.p3.5.m5.1.1.cmml" xref="S3.SS4.SSS0.Px2.p3.5.m5.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px2.p3.5.m5.1c">j</annotation></semantics></math>, respectively.</p>
</div>
</section>
<section id="S3.SS4.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Generalization</h5>

<div id="S3.SS4.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS4.SSS0.Px3.p1.1" class="ltx_p">Generalization measures the difference of performance on various datasets, and we define it as the variance of the performance on each involved VQA dataset:</p>
</div>
<div id="S3.SS4.SSS0.Px3.p2" class="ltx_para">
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.41" class="ltx_Math" alttext="\begin{split}\text{Generalization}=\log 1/\ var(align_{OKVQA},\\
align_{A-OKVQA},align_{VQAv2},align_{GQA})\end{split}" display="block"><semantics id="S3.E5.m1.41a"><mtable displaystyle="true" rowspacing="0pt" id="S3.E5.m1.37.37" xref="S3.E5.m1.41.41.4.cmml"><mtr id="S3.E5.m1.37.37a" xref="S3.E5.m1.41.41.4.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E5.m1.37.37b" xref="S3.E5.m1.41.41.4.cmml"><mrow id="S3.E5.m1.16.16.16.16.16" xref="S3.E5.m1.41.41.4.cmml"><mtext id="S3.E5.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1a.cmml">Generalization</mtext><mo id="S3.E5.m1.2.2.2.2.2.2" xref="S3.E5.m1.2.2.2.2.2.2.cmml">=</mo><mi id="S3.E5.m1.3.3.3.3.3.3" xref="S3.E5.m1.3.3.3.3.3.3.cmml">log</mi><mn id="S3.E5.m1.4.4.4.4.4.4" xref="S3.E5.m1.4.4.4.4.4.4.cmml">1</mn><mo rspace="0.722em" id="S3.E5.m1.5.5.5.5.5.5" xref="S3.E5.m1.5.5.5.5.5.5.cmml">/</mo><mi id="S3.E5.m1.6.6.6.6.6.6" xref="S3.E5.m1.6.6.6.6.6.6.cmml">v</mi><mi id="S3.E5.m1.7.7.7.7.7.7" xref="S3.E5.m1.7.7.7.7.7.7.cmml">a</mi><mi id="S3.E5.m1.8.8.8.8.8.8" xref="S3.E5.m1.8.8.8.8.8.8.cmml">r</mi><mrow id="S3.E5.m1.16.16.16.16.16.17" xref="S3.E5.m1.41.41.4.cmml"><mo stretchy="false" id="S3.E5.m1.9.9.9.9.9.9" xref="S3.E5.m1.41.41.4.cmml">(</mo><mi id="S3.E5.m1.10.10.10.10.10.10" xref="S3.E5.m1.10.10.10.10.10.10.cmml">a</mi><mi id="S3.E5.m1.11.11.11.11.11.11" xref="S3.E5.m1.11.11.11.11.11.11.cmml">l</mi><mi id="S3.E5.m1.12.12.12.12.12.12" xref="S3.E5.m1.12.12.12.12.12.12.cmml">i</mi><mi id="S3.E5.m1.13.13.13.13.13.13" xref="S3.E5.m1.13.13.13.13.13.13.cmml">g</mi><msub id="S3.E5.m1.16.16.16.16.16.17.1" xref="S3.E5.m1.41.41.4.cmml"><mi id="S3.E5.m1.14.14.14.14.14.14" xref="S3.E5.m1.14.14.14.14.14.14.cmml">n</mi><mrow id="S3.E5.m1.15.15.15.15.15.15.1" xref="S3.E5.m1.15.15.15.15.15.15.1.cmml"><mi id="S3.E5.m1.15.15.15.15.15.15.1.2" xref="S3.E5.m1.15.15.15.15.15.15.1.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.15.15.15.15.15.15.1.1" xref="S3.E5.m1.15.15.15.15.15.15.1.1.cmml">​</mo><mi id="S3.E5.m1.15.15.15.15.15.15.1.3" xref="S3.E5.m1.15.15.15.15.15.15.1.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.15.15.15.15.15.15.1.1a" xref="S3.E5.m1.15.15.15.15.15.15.1.1.cmml">​</mo><mi id="S3.E5.m1.15.15.15.15.15.15.1.4" xref="S3.E5.m1.15.15.15.15.15.15.1.4.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.15.15.15.15.15.15.1.1b" xref="S3.E5.m1.15.15.15.15.15.15.1.1.cmml">​</mo><mi id="S3.E5.m1.15.15.15.15.15.15.1.5" xref="S3.E5.m1.15.15.15.15.15.15.1.5.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.15.15.15.15.15.15.1.1c" xref="S3.E5.m1.15.15.15.15.15.15.1.1.cmml">​</mo><mi id="S3.E5.m1.15.15.15.15.15.15.1.6" xref="S3.E5.m1.15.15.15.15.15.15.1.6.cmml">A</mi></mrow></msub><mo id="S3.E5.m1.16.16.16.16.16.16" xref="S3.E5.m1.41.41.4.cmml">,</mo></mrow></mrow></mtd></mtr><mtr id="S3.E5.m1.37.37c" xref="S3.E5.m1.41.41.4.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E5.m1.37.37d" xref="S3.E5.m1.41.41.4.cmml"><mrow id="S3.E5.m1.37.37.37.21.21" xref="S3.E5.m1.41.41.4.cmml"><mi id="S3.E5.m1.17.17.17.1.1.1" xref="S3.E5.m1.17.17.17.1.1.1.cmml">a</mi><mi id="S3.E5.m1.18.18.18.2.2.2" xref="S3.E5.m1.18.18.18.2.2.2.cmml">l</mi><mi id="S3.E5.m1.19.19.19.3.3.3" xref="S3.E5.m1.19.19.19.3.3.3.cmml">i</mi><mi id="S3.E5.m1.20.20.20.4.4.4" xref="S3.E5.m1.20.20.20.4.4.4.cmml">g</mi><msub id="S3.E5.m1.37.37.37.21.21.22" xref="S3.E5.m1.41.41.4.cmml"><mi id="S3.E5.m1.21.21.21.5.5.5" xref="S3.E5.m1.21.21.21.5.5.5.cmml">n</mi><mrow id="S3.E5.m1.22.22.22.6.6.6.1" xref="S3.E5.m1.22.22.22.6.6.6.1.cmml"><mi id="S3.E5.m1.22.22.22.6.6.6.1.2" xref="S3.E5.m1.22.22.22.6.6.6.1.2.cmml">A</mi><mo id="S3.E5.m1.22.22.22.6.6.6.1.1" xref="S3.E5.m1.22.22.22.6.6.6.1.1.cmml">−</mo><mrow id="S3.E5.m1.22.22.22.6.6.6.1.3" xref="S3.E5.m1.22.22.22.6.6.6.1.3.cmml"><mi id="S3.E5.m1.22.22.22.6.6.6.1.3.2" xref="S3.E5.m1.22.22.22.6.6.6.1.3.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.22.22.22.6.6.6.1.3.1" xref="S3.E5.m1.22.22.22.6.6.6.1.3.1.cmml">​</mo><mi id="S3.E5.m1.22.22.22.6.6.6.1.3.3" xref="S3.E5.m1.22.22.22.6.6.6.1.3.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.22.22.22.6.6.6.1.3.1a" xref="S3.E5.m1.22.22.22.6.6.6.1.3.1.cmml">​</mo><mi id="S3.E5.m1.22.22.22.6.6.6.1.3.4" xref="S3.E5.m1.22.22.22.6.6.6.1.3.4.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.22.22.22.6.6.6.1.3.1b" xref="S3.E5.m1.22.22.22.6.6.6.1.3.1.cmml">​</mo><mi id="S3.E5.m1.22.22.22.6.6.6.1.3.5" xref="S3.E5.m1.22.22.22.6.6.6.1.3.5.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.22.22.22.6.6.6.1.3.1c" xref="S3.E5.m1.22.22.22.6.6.6.1.3.1.cmml">​</mo><mi id="S3.E5.m1.22.22.22.6.6.6.1.3.6" xref="S3.E5.m1.22.22.22.6.6.6.1.3.6.cmml">A</mi></mrow></mrow></msub><mo id="S3.E5.m1.23.23.23.7.7.7" xref="S3.E5.m1.41.41.4.cmml">,</mo><mi id="S3.E5.m1.24.24.24.8.8.8" xref="S3.E5.m1.24.24.24.8.8.8.cmml">a</mi><mi id="S3.E5.m1.25.25.25.9.9.9" xref="S3.E5.m1.25.25.25.9.9.9.cmml">l</mi><mi id="S3.E5.m1.26.26.26.10.10.10" xref="S3.E5.m1.26.26.26.10.10.10.cmml">i</mi><mi id="S3.E5.m1.27.27.27.11.11.11" xref="S3.E5.m1.27.27.27.11.11.11.cmml">g</mi><msub id="S3.E5.m1.37.37.37.21.21.23" xref="S3.E5.m1.41.41.4.cmml"><mi id="S3.E5.m1.28.28.28.12.12.12" xref="S3.E5.m1.28.28.28.12.12.12.cmml">n</mi><mrow id="S3.E5.m1.29.29.29.13.13.13.1" xref="S3.E5.m1.29.29.29.13.13.13.1.cmml"><mi id="S3.E5.m1.29.29.29.13.13.13.1.2" xref="S3.E5.m1.29.29.29.13.13.13.1.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.29.29.29.13.13.13.1.1" xref="S3.E5.m1.29.29.29.13.13.13.1.1.cmml">​</mo><mi id="S3.E5.m1.29.29.29.13.13.13.1.3" xref="S3.E5.m1.29.29.29.13.13.13.1.3.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.29.29.29.13.13.13.1.1a" xref="S3.E5.m1.29.29.29.13.13.13.1.1.cmml">​</mo><mi id="S3.E5.m1.29.29.29.13.13.13.1.4" xref="S3.E5.m1.29.29.29.13.13.13.1.4.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.29.29.29.13.13.13.1.1b" xref="S3.E5.m1.29.29.29.13.13.13.1.1.cmml">​</mo><mi id="S3.E5.m1.29.29.29.13.13.13.1.5" xref="S3.E5.m1.29.29.29.13.13.13.1.5.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.29.29.29.13.13.13.1.1c" xref="S3.E5.m1.29.29.29.13.13.13.1.1.cmml">​</mo><mn id="S3.E5.m1.29.29.29.13.13.13.1.6" xref="S3.E5.m1.29.29.29.13.13.13.1.6.cmml">2</mn></mrow></msub><mo id="S3.E5.m1.30.30.30.14.14.14" xref="S3.E5.m1.41.41.4.cmml">,</mo><mi id="S3.E5.m1.31.31.31.15.15.15" xref="S3.E5.m1.31.31.31.15.15.15.cmml">a</mi><mi id="S3.E5.m1.32.32.32.16.16.16" xref="S3.E5.m1.32.32.32.16.16.16.cmml">l</mi><mi id="S3.E5.m1.33.33.33.17.17.17" xref="S3.E5.m1.33.33.33.17.17.17.cmml">i</mi><mi id="S3.E5.m1.34.34.34.18.18.18" xref="S3.E5.m1.34.34.34.18.18.18.cmml">g</mi><msub id="S3.E5.m1.37.37.37.21.21.24" xref="S3.E5.m1.41.41.4.cmml"><mi id="S3.E5.m1.35.35.35.19.19.19" xref="S3.E5.m1.35.35.35.19.19.19.cmml">n</mi><mrow id="S3.E5.m1.36.36.36.20.20.20.1" xref="S3.E5.m1.36.36.36.20.20.20.1.cmml"><mi id="S3.E5.m1.36.36.36.20.20.20.1.2" xref="S3.E5.m1.36.36.36.20.20.20.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.36.36.36.20.20.20.1.1" xref="S3.E5.m1.36.36.36.20.20.20.1.1.cmml">​</mo><mi id="S3.E5.m1.36.36.36.20.20.20.1.3" xref="S3.E5.m1.36.36.36.20.20.20.1.3.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.36.36.36.20.20.20.1.1a" xref="S3.E5.m1.36.36.36.20.20.20.1.1.cmml">​</mo><mi id="S3.E5.m1.36.36.36.20.20.20.1.4" xref="S3.E5.m1.36.36.36.20.20.20.1.4.cmml">A</mi></mrow></msub><mo stretchy="false" id="S3.E5.m1.37.37.37.21.21.21" xref="S3.E5.m1.41.41.4.cmml">)</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E5.m1.41b"><apply id="S3.E5.m1.41.41.4.cmml" xref="S3.E5.m1.37.37"><eq id="S3.E5.m1.2.2.2.2.2.2.cmml" xref="S3.E5.m1.2.2.2.2.2.2"></eq><ci id="S3.E5.m1.1.1.1.1.1.1a.cmml" xref="S3.E5.m1.1.1.1.1.1.1"><mtext id="S3.E5.m1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1">Generalization</mtext></ci><apply id="S3.E5.m1.41.41.4.4.cmml" xref="S3.E5.m1.37.37"><times id="S3.E5.m1.41.41.4.4.5.cmml" xref="S3.E5.m1.37.37"></times><apply id="S3.E5.m1.41.41.4.4.6.cmml" xref="S3.E5.m1.37.37"><log id="S3.E5.m1.3.3.3.3.3.3.cmml" xref="S3.E5.m1.3.3.3.3.3.3"></log><apply id="S3.E5.m1.41.41.4.4.6.2.cmml" xref="S3.E5.m1.37.37"><times id="S3.E5.m1.41.41.4.4.6.2.1.cmml" xref="S3.E5.m1.37.37"></times><apply id="S3.E5.m1.41.41.4.4.6.2.2.cmml" xref="S3.E5.m1.37.37"><divide id="S3.E5.m1.5.5.5.5.5.5.cmml" xref="S3.E5.m1.5.5.5.5.5.5"></divide><cn type="integer" id="S3.E5.m1.4.4.4.4.4.4.cmml" xref="S3.E5.m1.4.4.4.4.4.4">1</cn><ci id="S3.E5.m1.6.6.6.6.6.6.cmml" xref="S3.E5.m1.6.6.6.6.6.6">𝑣</ci></apply><ci id="S3.E5.m1.7.7.7.7.7.7.cmml" xref="S3.E5.m1.7.7.7.7.7.7">𝑎</ci><ci id="S3.E5.m1.8.8.8.8.8.8.cmml" xref="S3.E5.m1.8.8.8.8.8.8">𝑟</ci></apply></apply><vector id="S3.E5.m1.41.41.4.4.4.5.cmml" xref="S3.E5.m1.37.37"><apply id="S3.E5.m1.38.38.1.1.1.1.1.cmml" xref="S3.E5.m1.37.37"><times id="S3.E5.m1.38.38.1.1.1.1.1.1.cmml" xref="S3.E5.m1.37.37"></times><ci id="S3.E5.m1.10.10.10.10.10.10.cmml" xref="S3.E5.m1.10.10.10.10.10.10">𝑎</ci><ci id="S3.E5.m1.11.11.11.11.11.11.cmml" xref="S3.E5.m1.11.11.11.11.11.11">𝑙</ci><ci id="S3.E5.m1.12.12.12.12.12.12.cmml" xref="S3.E5.m1.12.12.12.12.12.12">𝑖</ci><ci id="S3.E5.m1.13.13.13.13.13.13.cmml" xref="S3.E5.m1.13.13.13.13.13.13">𝑔</ci><apply id="S3.E5.m1.38.38.1.1.1.1.1.6.cmml" xref="S3.E5.m1.37.37"><csymbol cd="ambiguous" id="S3.E5.m1.38.38.1.1.1.1.1.6.1.cmml" xref="S3.E5.m1.37.37">subscript</csymbol><ci id="S3.E5.m1.14.14.14.14.14.14.cmml" xref="S3.E5.m1.14.14.14.14.14.14">𝑛</ci><apply id="S3.E5.m1.15.15.15.15.15.15.1.cmml" xref="S3.E5.m1.15.15.15.15.15.15.1"><times id="S3.E5.m1.15.15.15.15.15.15.1.1.cmml" xref="S3.E5.m1.15.15.15.15.15.15.1.1"></times><ci id="S3.E5.m1.15.15.15.15.15.15.1.2.cmml" xref="S3.E5.m1.15.15.15.15.15.15.1.2">𝑂</ci><ci id="S3.E5.m1.15.15.15.15.15.15.1.3.cmml" xref="S3.E5.m1.15.15.15.15.15.15.1.3">𝐾</ci><ci id="S3.E5.m1.15.15.15.15.15.15.1.4.cmml" xref="S3.E5.m1.15.15.15.15.15.15.1.4">𝑉</ci><ci id="S3.E5.m1.15.15.15.15.15.15.1.5.cmml" xref="S3.E5.m1.15.15.15.15.15.15.1.5">𝑄</ci><ci id="S3.E5.m1.15.15.15.15.15.15.1.6.cmml" xref="S3.E5.m1.15.15.15.15.15.15.1.6">𝐴</ci></apply></apply></apply><apply id="S3.E5.m1.39.39.2.2.2.2.2.cmml" xref="S3.E5.m1.37.37"><times id="S3.E5.m1.39.39.2.2.2.2.2.1.cmml" xref="S3.E5.m1.37.37"></times><ci id="S3.E5.m1.17.17.17.1.1.1.cmml" xref="S3.E5.m1.17.17.17.1.1.1">𝑎</ci><ci id="S3.E5.m1.18.18.18.2.2.2.cmml" xref="S3.E5.m1.18.18.18.2.2.2">𝑙</ci><ci id="S3.E5.m1.19.19.19.3.3.3.cmml" xref="S3.E5.m1.19.19.19.3.3.3">𝑖</ci><ci id="S3.E5.m1.20.20.20.4.4.4.cmml" xref="S3.E5.m1.20.20.20.4.4.4">𝑔</ci><apply id="S3.E5.m1.39.39.2.2.2.2.2.6.cmml" xref="S3.E5.m1.37.37"><csymbol cd="ambiguous" id="S3.E5.m1.39.39.2.2.2.2.2.6.1.cmml" xref="S3.E5.m1.37.37">subscript</csymbol><ci id="S3.E5.m1.21.21.21.5.5.5.cmml" xref="S3.E5.m1.21.21.21.5.5.5">𝑛</ci><apply id="S3.E5.m1.22.22.22.6.6.6.1.cmml" xref="S3.E5.m1.22.22.22.6.6.6.1"><minus id="S3.E5.m1.22.22.22.6.6.6.1.1.cmml" xref="S3.E5.m1.22.22.22.6.6.6.1.1"></minus><ci id="S3.E5.m1.22.22.22.6.6.6.1.2.cmml" xref="S3.E5.m1.22.22.22.6.6.6.1.2">𝐴</ci><apply id="S3.E5.m1.22.22.22.6.6.6.1.3.cmml" xref="S3.E5.m1.22.22.22.6.6.6.1.3"><times id="S3.E5.m1.22.22.22.6.6.6.1.3.1.cmml" xref="S3.E5.m1.22.22.22.6.6.6.1.3.1"></times><ci id="S3.E5.m1.22.22.22.6.6.6.1.3.2.cmml" xref="S3.E5.m1.22.22.22.6.6.6.1.3.2">𝑂</ci><ci id="S3.E5.m1.22.22.22.6.6.6.1.3.3.cmml" xref="S3.E5.m1.22.22.22.6.6.6.1.3.3">𝐾</ci><ci id="S3.E5.m1.22.22.22.6.6.6.1.3.4.cmml" xref="S3.E5.m1.22.22.22.6.6.6.1.3.4">𝑉</ci><ci id="S3.E5.m1.22.22.22.6.6.6.1.3.5.cmml" xref="S3.E5.m1.22.22.22.6.6.6.1.3.5">𝑄</ci><ci id="S3.E5.m1.22.22.22.6.6.6.1.3.6.cmml" xref="S3.E5.m1.22.22.22.6.6.6.1.3.6">𝐴</ci></apply></apply></apply></apply><apply id="S3.E5.m1.40.40.3.3.3.3.3.cmml" xref="S3.E5.m1.37.37"><times id="S3.E5.m1.40.40.3.3.3.3.3.1.cmml" xref="S3.E5.m1.37.37"></times><ci id="S3.E5.m1.24.24.24.8.8.8.cmml" xref="S3.E5.m1.24.24.24.8.8.8">𝑎</ci><ci id="S3.E5.m1.25.25.25.9.9.9.cmml" xref="S3.E5.m1.25.25.25.9.9.9">𝑙</ci><ci id="S3.E5.m1.26.26.26.10.10.10.cmml" xref="S3.E5.m1.26.26.26.10.10.10">𝑖</ci><ci id="S3.E5.m1.27.27.27.11.11.11.cmml" xref="S3.E5.m1.27.27.27.11.11.11">𝑔</ci><apply id="S3.E5.m1.40.40.3.3.3.3.3.6.cmml" xref="S3.E5.m1.37.37"><csymbol cd="ambiguous" id="S3.E5.m1.40.40.3.3.3.3.3.6.1.cmml" xref="S3.E5.m1.37.37">subscript</csymbol><ci id="S3.E5.m1.28.28.28.12.12.12.cmml" xref="S3.E5.m1.28.28.28.12.12.12">𝑛</ci><apply id="S3.E5.m1.29.29.29.13.13.13.1.cmml" xref="S3.E5.m1.29.29.29.13.13.13.1"><times id="S3.E5.m1.29.29.29.13.13.13.1.1.cmml" xref="S3.E5.m1.29.29.29.13.13.13.1.1"></times><ci id="S3.E5.m1.29.29.29.13.13.13.1.2.cmml" xref="S3.E5.m1.29.29.29.13.13.13.1.2">𝑉</ci><ci id="S3.E5.m1.29.29.29.13.13.13.1.3.cmml" xref="S3.E5.m1.29.29.29.13.13.13.1.3">𝑄</ci><ci id="S3.E5.m1.29.29.29.13.13.13.1.4.cmml" xref="S3.E5.m1.29.29.29.13.13.13.1.4">𝐴</ci><ci id="S3.E5.m1.29.29.29.13.13.13.1.5.cmml" xref="S3.E5.m1.29.29.29.13.13.13.1.5">𝑣</ci><cn type="integer" id="S3.E5.m1.29.29.29.13.13.13.1.6.cmml" xref="S3.E5.m1.29.29.29.13.13.13.1.6">2</cn></apply></apply></apply><apply id="S3.E5.m1.41.41.4.4.4.4.4.cmml" xref="S3.E5.m1.37.37"><times id="S3.E5.m1.41.41.4.4.4.4.4.1.cmml" xref="S3.E5.m1.37.37"></times><ci id="S3.E5.m1.31.31.31.15.15.15.cmml" xref="S3.E5.m1.31.31.31.15.15.15">𝑎</ci><ci id="S3.E5.m1.32.32.32.16.16.16.cmml" xref="S3.E5.m1.32.32.32.16.16.16">𝑙</ci><ci id="S3.E5.m1.33.33.33.17.17.17.cmml" xref="S3.E5.m1.33.33.33.17.17.17">𝑖</ci><ci id="S3.E5.m1.34.34.34.18.18.18.cmml" xref="S3.E5.m1.34.34.34.18.18.18">𝑔</ci><apply id="S3.E5.m1.41.41.4.4.4.4.4.6.cmml" xref="S3.E5.m1.37.37"><csymbol cd="ambiguous" id="S3.E5.m1.41.41.4.4.4.4.4.6.1.cmml" xref="S3.E5.m1.37.37">subscript</csymbol><ci id="S3.E5.m1.35.35.35.19.19.19.cmml" xref="S3.E5.m1.35.35.35.19.19.19">𝑛</ci><apply id="S3.E5.m1.36.36.36.20.20.20.1.cmml" xref="S3.E5.m1.36.36.36.20.20.20.1"><times id="S3.E5.m1.36.36.36.20.20.20.1.1.cmml" xref="S3.E5.m1.36.36.36.20.20.20.1.1"></times><ci id="S3.E5.m1.36.36.36.20.20.20.1.2.cmml" xref="S3.E5.m1.36.36.36.20.20.20.1.2">𝐺</ci><ci id="S3.E5.m1.36.36.36.20.20.20.1.3.cmml" xref="S3.E5.m1.36.36.36.20.20.20.1.3">𝑄</ci><ci id="S3.E5.m1.36.36.36.20.20.20.1.4.cmml" xref="S3.E5.m1.36.36.36.20.20.20.1.4">𝐴</ci></apply></apply></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.41c">\begin{split}\text{Generalization}=\log 1/\ var(align_{OKVQA},\\
align_{A-OKVQA},align_{VQAv2},align_{GQA})\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.SSS0.Px3.p3" class="ltx_para">
<p id="S3.SS4.SSS0.Px3.p3.1" class="ltx_p">where <math id="S3.SS4.SSS0.Px3.p3.1.m1.1" class="ltx_Math" alttext="align_{dataset}" display="inline"><semantics id="S3.SS4.SSS0.Px3.p3.1.m1.1a"><mrow id="S3.SS4.SSS0.Px3.p3.1.m1.1.1" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.cmml"><mi id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.2" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.1" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.3" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.1a" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.4" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.1b" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.5" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.1c" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.1.cmml">​</mo><msub id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.cmml"><mi id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.2" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.2.cmml">n</mi><mrow id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.cmml"><mi id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.2" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.1" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.3" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.1a" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.4" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.1b" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.5" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.1c" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.6" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.6.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.1d" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.7" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.1e" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.1.cmml">​</mo><mi id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.8" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.8.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px3.p3.1.m1.1b"><apply id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.cmml" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1"><times id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.1.cmml" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.1"></times><ci id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.2.cmml" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.2">𝑎</ci><ci id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.3.cmml" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.3">𝑙</ci><ci id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.4.cmml" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.4">𝑖</ci><ci id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.5.cmml" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.5">𝑔</ci><apply id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.cmml" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6"><csymbol cd="ambiguous" id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.1.cmml" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6">subscript</csymbol><ci id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.2.cmml" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.2">𝑛</ci><apply id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.cmml" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3"><times id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.1.cmml" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.1"></times><ci id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.2.cmml" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.2">𝑑</ci><ci id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.3.cmml" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.3">𝑎</ci><ci id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.4.cmml" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.4">𝑡</ci><ci id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.5.cmml" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.5">𝑎</ci><ci id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.6.cmml" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.6">𝑠</ci><ci id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.7.cmml" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.7">𝑒</ci><ci id="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.8.cmml" xref="S3.SS4.SSS0.Px3.p3.1.m1.1.1.6.3.8">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px3.p3.1.m1.1c">align_{dataset}</annotation></semantics></math> is the mean Alignment score on the AVE data belonging to the corresponding VQA dataset.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Semantically Flexible VQA Evaluator</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">With the three key properties of an ideal evaluator taken into consideration, we propose a novel evaluator based on meticulously designed pretraining tasks.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Pretraining Tasks</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To guide the model to be sensitive to the key information between answer and response, this paper introduces several pretraining tasks to enhance the embedding. Data for augmentation come from a random sampling of VQA data in the training sets of OKVQA<cite class="ltx_cite ltx_citemacro_citep">(Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite>, A-OKVQA<cite class="ltx_cite ltx_citemacro_citep">(Schwenk et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>, TDIUC<cite class="ltx_cite ltx_citemacro_citep">(Kafle and Kanan, <a href="#bib.bib28" title="" class="ltx_ref">2017</a>)</cite>, VG-QA<cite class="ltx_cite ltx_citemacro_citep">(Krishna et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2017</a>)</cite>, GQA<cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite> and VQA v2 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2017</a>)</cite>, which end to a total amount of 105, 311 samples. All augmented samples are mixed for training.</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">NLI data</h5>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">In previous works <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021</a>; Li and Li, <a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite>, models perform well with the natural language inference datasets SNLI <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2017</a>)</cite> and MNLI <cite class="ltx_cite ltx_citemacro_citep">(Williams et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2017</a>)</cite>, where each sample includes a premise, an entailment and a contradiction.
In addition, these NLI datasets are all manually constructed, ensuring the high quality of their data, and the premise shares limited overlap with entailment compared with the sentence pairs in back-translation datasets.
Therefore, to ensure the fundamental discriminating ability of models to capture overall meaning of sentences, we adopt NLI data and regard the premise-entailment pairs as postive pairs and premise-contradiction pairs as negative pairs.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Candidate answers</h5>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">To make the best of available VQA datasets, for datasets with ten candidate answers, OKVQA, A-OKVQA and VQA v2, we consider candidate answers as correct answers as well. Then, for each sample, the most common candidate answer and a less common one are used to form a positive pair, with a random answer sampled from the answer space as negative.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Synonym and Antonym</h5>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p">In VQA response evaluation, semantically similar answers shall receive similar scores. We replace the answer with a synonym by WordNet <cite class="ltx_cite ltx_citemacro_citep">(Miller et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">1990</a>)</cite>, and if the antonym of an answer exists, we then pair up the answer and antonym as a negative pair, else we pair up the answer and a randomly sampled answer from the answer space as a negative pair. In addition,
we use ChatGPT to produce synonyms as well, as ChatGPT is able to capture contextual information in the question and thus generates more accurate synonyms.
</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Generated descriptions</h5>

<div id="S4.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px4.p1.1" class="ltx_p">To simulate the output of MLLMs, for each sample, we provide ChatGPT with its question and answer to generate three descriptions with small disturbance of the same meaning. Then, we construct positive samples by pairing up the original answer and each generated description. For negative samples, we replace the answer in the generated description with a randomly sampled answer. The goal is to pull the embedding representation of a natural language description close to its simple form of a single answer, so that responses with different length but carrying similar meanings will receive similar scores. In addition, the negative pair is constructed by replacing the key answer word in the description, therefore guiding the model to be sensitive to the key words and to ignore the noise.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Model Framework</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Following previous works <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2020</a>; Gao et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021</a>; Conneau and Kiela, <a href="#bib.bib17" title="" class="ltx_ref">2018</a>; Li and Li, <a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite> on the STS task <cite class="ltx_cite ltx_citemacro_citep">(Agirre et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2012</a>, <a href="#bib.bib6" title="" class="ltx_ref">2013</a>, <a href="#bib.bib3" title="" class="ltx_ref">2014</a>, <a href="#bib.bib2" title="" class="ltx_ref">2015</a>, <a href="#bib.bib4" title="" class="ltx_ref">2016</a>; Cer et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite>, we use cosine similarity for distance calculation between embeddings<cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021</a>; Li and Li, <a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite>.
As shown in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2. Model Framework ‣ 4. Semantically Flexible VQA Evaluator ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we adopt the simple contrastive learning framework <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite> and contrastive learning with in-batch hard negatives <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The backbone encoder in this paper is RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2019</a>)</cite>. In order to gain better generalization and comprehension ability, we apply the decoder-only LLM LLAMA2 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2023</a>)</cite> with the prompt<cite class="ltx_cite ltx_citemacro_citep">(Li and Li, <a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite> of <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_italic">Summarize the text </span>{<span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_italic">text</span>}<span id="S4.SS2.p2.1.3" class="ltx_text ltx_font_italic"> in a single word:</span>. Then, the hidden states of the first generated new token is considered as the embedding vector.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="" id="S4.F3.1.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Framework of contrastive learning in the proposed Semantically Flexible VQA Evaluator (SFVE). The original sample is augmented into two variations and form a positive pair and a negative pair. The example in the figure shows the procedure of the pretraining task <span id="S4.F3.3.1" class="ltx_text ltx_font_italic">Generated descriptions</span>. In the positive pair, the semantics of the sentence is considered same as the original, while in the negative pair, as the answer word is replaced with a random answer, the sentence contains unmatched meaning with the original.</figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">The contrastive learning with in-batch hard negatives loss <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021</a>)</cite> is defined as follows:</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<table id="S4.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(6)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E6.m1.6" class="ltx_math_unparsed" alttext="loss_{ibn}=-\log{\frac{e^{sim({h_{i},h_{i}^{+}})/\tau}}{\sum^{N}_{j=1}(e^{sim({h_{i},h_{j}^{+}})/\tau}+e^{sim({h_{i},h_{i}^{-}})/\tau}})}" display="block"><semantics id="S4.E6.m1.6a"><mrow id="S4.E6.m1.6b"><mi id="S4.E6.m1.6.7">l</mi><mi id="S4.E6.m1.6.8">o</mi><mi id="S4.E6.m1.6.9">s</mi><msub id="S4.E6.m1.6.10"><mi id="S4.E6.m1.6.10.2">s</mi><mrow id="S4.E6.m1.6.10.3"><mi id="S4.E6.m1.6.10.3.2">i</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.6.10.3.1">​</mo><mi id="S4.E6.m1.6.10.3.3">b</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.6.10.3.1a">​</mo><mi id="S4.E6.m1.6.10.3.4">n</mi></mrow></msub><mo rspace="0em" id="S4.E6.m1.6.11">=</mo><mo lspace="0em" id="S4.E6.m1.6.12">−</mo><mi id="S4.E6.m1.6.13">log</mi><mfrac id="S4.E6.m1.6.6"><msup id="S4.E6.m1.2.2.2"><mi id="S4.E6.m1.2.2.2.4">e</mi><mrow id="S4.E6.m1.2.2.2.2.2"><mrow id="S4.E6.m1.2.2.2.2.2.2"><mi id="S4.E6.m1.2.2.2.2.2.2.4">s</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.2.2.2.2.3">​</mo><mi id="S4.E6.m1.2.2.2.2.2.2.5">i</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.2.2.2.2.3a">​</mo><mi id="S4.E6.m1.2.2.2.2.2.2.6">m</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.2.2.2.2.3b">​</mo><mrow id="S4.E6.m1.2.2.2.2.2.2.2.2"><mo stretchy="false" id="S4.E6.m1.2.2.2.2.2.2.2.2.3">(</mo><msub id="S4.E6.m1.1.1.1.1.1.1.1.1.1"><mi id="S4.E6.m1.1.1.1.1.1.1.1.1.1.2">h</mi><mi id="S4.E6.m1.1.1.1.1.1.1.1.1.1.3">i</mi></msub><mo id="S4.E6.m1.2.2.2.2.2.2.2.2.4">,</mo><msubsup id="S4.E6.m1.2.2.2.2.2.2.2.2.2"><mi id="S4.E6.m1.2.2.2.2.2.2.2.2.2.2.2">h</mi><mi id="S4.E6.m1.2.2.2.2.2.2.2.2.2.2.3">i</mi><mo id="S4.E6.m1.2.2.2.2.2.2.2.2.2.3">+</mo></msubsup><mo stretchy="false" id="S4.E6.m1.2.2.2.2.2.2.2.2.5">)</mo></mrow></mrow><mo id="S4.E6.m1.2.2.2.2.2.3">/</mo><mi id="S4.E6.m1.2.2.2.2.2.4">τ</mi></mrow></msup><mrow id="S4.E6.m1.6.6.6"><msubsup id="S4.E6.m1.6.6.6.5"><mo id="S4.E6.m1.6.6.6.5.2.2">∑</mo><mrow id="S4.E6.m1.6.6.6.5.3"><mi id="S4.E6.m1.6.6.6.5.3.2">j</mi><mo id="S4.E6.m1.6.6.6.5.3.1">=</mo><mn id="S4.E6.m1.6.6.6.5.3.3">1</mn></mrow><mi id="S4.E6.m1.6.6.6.5.2.3">N</mi></msubsup><mrow id="S4.E6.m1.6.6.6.6"><mo lspace="0em" stretchy="false" id="S4.E6.m1.6.6.6.6.1">(</mo><msup id="S4.E6.m1.6.6.6.6.2"><mi id="S4.E6.m1.6.6.6.6.2.2">e</mi><mrow id="S4.E6.m1.4.4.4.2.2"><mrow id="S4.E6.m1.4.4.4.2.2.2"><mi id="S4.E6.m1.4.4.4.2.2.2.4">s</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.4.2.2.2.3">​</mo><mi id="S4.E6.m1.4.4.4.2.2.2.5">i</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.4.2.2.2.3a">​</mo><mi id="S4.E6.m1.4.4.4.2.2.2.6">m</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.4.2.2.2.3b">​</mo><mrow id="S4.E6.m1.4.4.4.2.2.2.2.2"><mo stretchy="false" id="S4.E6.m1.4.4.4.2.2.2.2.2.3">(</mo><msub id="S4.E6.m1.3.3.3.1.1.1.1.1.1"><mi id="S4.E6.m1.3.3.3.1.1.1.1.1.1.2">h</mi><mi id="S4.E6.m1.3.3.3.1.1.1.1.1.1.3">i</mi></msub><mo id="S4.E6.m1.4.4.4.2.2.2.2.2.4">,</mo><msubsup id="S4.E6.m1.4.4.4.2.2.2.2.2.2"><mi id="S4.E6.m1.4.4.4.2.2.2.2.2.2.2.2">h</mi><mi id="S4.E6.m1.4.4.4.2.2.2.2.2.2.2.3">j</mi><mo id="S4.E6.m1.4.4.4.2.2.2.2.2.2.3">+</mo></msubsup><mo stretchy="false" id="S4.E6.m1.4.4.4.2.2.2.2.2.5">)</mo></mrow></mrow><mo id="S4.E6.m1.4.4.4.2.2.3">/</mo><mi id="S4.E6.m1.4.4.4.2.2.4">τ</mi></mrow></msup><mo id="S4.E6.m1.6.6.6.6.3">+</mo><msup id="S4.E6.m1.6.6.6.6.4"><mi id="S4.E6.m1.6.6.6.6.4.2">e</mi><mrow id="S4.E6.m1.6.6.6.4.2"><mrow id="S4.E6.m1.6.6.6.4.2.2"><mi id="S4.E6.m1.6.6.6.4.2.2.4">s</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.6.6.6.4.2.2.3">​</mo><mi id="S4.E6.m1.6.6.6.4.2.2.5">i</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.6.6.6.4.2.2.3a">​</mo><mi id="S4.E6.m1.6.6.6.4.2.2.6">m</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.6.6.6.4.2.2.3b">​</mo><mrow id="S4.E6.m1.6.6.6.4.2.2.2.2"><mo stretchy="false" id="S4.E6.m1.6.6.6.4.2.2.2.2.3">(</mo><msub id="S4.E6.m1.5.5.5.3.1.1.1.1.1"><mi id="S4.E6.m1.5.5.5.3.1.1.1.1.1.2">h</mi><mi id="S4.E6.m1.5.5.5.3.1.1.1.1.1.3">i</mi></msub><mo id="S4.E6.m1.6.6.6.4.2.2.2.2.4">,</mo><msubsup id="S4.E6.m1.6.6.6.4.2.2.2.2.2"><mi id="S4.E6.m1.6.6.6.4.2.2.2.2.2.2.2">h</mi><mi id="S4.E6.m1.6.6.6.4.2.2.2.2.2.2.3">i</mi><mo id="S4.E6.m1.6.6.6.4.2.2.2.2.2.3">−</mo></msubsup><mo stretchy="false" id="S4.E6.m1.6.6.6.4.2.2.2.2.5">)</mo></mrow></mrow><mo id="S4.E6.m1.6.6.6.4.2.3">/</mo><mi id="S4.E6.m1.6.6.6.4.2.4">τ</mi></mrow></msup></mrow></mrow></mfrac><mo stretchy="false" id="S4.E6.m1.6.14">)</mo></mrow><annotation encoding="application/x-tex" id="S4.E6.m1.6c">loss_{ibn}=-\log{\frac{e^{sim({h_{i},h_{i}^{+}})/\tau}}{\sum^{N}_{j=1}(e^{sim({h_{i},h_{j}^{+}})/\tau}+e^{sim({h_{i},h_{i}^{-}})/\tau}})}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.5" class="ltx_p">where <math id="S4.SS2.p5.1.m1.1" class="ltx_Math" alttext="h_{i}" display="inline"><semantics id="S4.SS2.p5.1.m1.1a"><msub id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml"><mi id="S4.SS2.p5.1.m1.1.1.2" xref="S4.SS2.p5.1.m1.1.1.2.cmml">h</mi><mi id="S4.SS2.p5.1.m1.1.1.3" xref="S4.SS2.p5.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.1b"><apply id="S4.SS2.p5.1.m1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p5.1.m1.1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p5.1.m1.1.1.2.cmml" xref="S4.SS2.p5.1.m1.1.1.2">ℎ</ci><ci id="S4.SS2.p5.1.m1.1.1.3.cmml" xref="S4.SS2.p5.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.1c">h_{i}</annotation></semantics></math> is the embedding representation of sample <math id="S4.SS2.p5.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.p5.2.m2.1a"><mi id="S4.SS2.p5.2.m2.1.1" xref="S4.SS2.p5.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.2.m2.1b"><ci id="S4.SS2.p5.2.m2.1.1.cmml" xref="S4.SS2.p5.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.2.m2.1c">i</annotation></semantics></math>, <math id="S4.SS2.p5.3.m3.1" class="ltx_Math" alttext="h_{i}^{+}" display="inline"><semantics id="S4.SS2.p5.3.m3.1a"><msubsup id="S4.SS2.p5.3.m3.1.1" xref="S4.SS2.p5.3.m3.1.1.cmml"><mi id="S4.SS2.p5.3.m3.1.1.2.2" xref="S4.SS2.p5.3.m3.1.1.2.2.cmml">h</mi><mi id="S4.SS2.p5.3.m3.1.1.2.3" xref="S4.SS2.p5.3.m3.1.1.2.3.cmml">i</mi><mo id="S4.SS2.p5.3.m3.1.1.3" xref="S4.SS2.p5.3.m3.1.1.3.cmml">+</mo></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.3.m3.1b"><apply id="S4.SS2.p5.3.m3.1.1.cmml" xref="S4.SS2.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p5.3.m3.1.1.1.cmml" xref="S4.SS2.p5.3.m3.1.1">superscript</csymbol><apply id="S4.SS2.p5.3.m3.1.1.2.cmml" xref="S4.SS2.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p5.3.m3.1.1.2.1.cmml" xref="S4.SS2.p5.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p5.3.m3.1.1.2.2.cmml" xref="S4.SS2.p5.3.m3.1.1.2.2">ℎ</ci><ci id="S4.SS2.p5.3.m3.1.1.2.3.cmml" xref="S4.SS2.p5.3.m3.1.1.2.3">𝑖</ci></apply><plus id="S4.SS2.p5.3.m3.1.1.3.cmml" xref="S4.SS2.p5.3.m3.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.3.m3.1c">h_{i}^{+}</annotation></semantics></math> and <math id="S4.SS2.p5.4.m4.1" class="ltx_Math" alttext="h_{i}^{-}" display="inline"><semantics id="S4.SS2.p5.4.m4.1a"><msubsup id="S4.SS2.p5.4.m4.1.1" xref="S4.SS2.p5.4.m4.1.1.cmml"><mi id="S4.SS2.p5.4.m4.1.1.2.2" xref="S4.SS2.p5.4.m4.1.1.2.2.cmml">h</mi><mi id="S4.SS2.p5.4.m4.1.1.2.3" xref="S4.SS2.p5.4.m4.1.1.2.3.cmml">i</mi><mo id="S4.SS2.p5.4.m4.1.1.3" xref="S4.SS2.p5.4.m4.1.1.3.cmml">−</mo></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.4.m4.1b"><apply id="S4.SS2.p5.4.m4.1.1.cmml" xref="S4.SS2.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p5.4.m4.1.1.1.cmml" xref="S4.SS2.p5.4.m4.1.1">superscript</csymbol><apply id="S4.SS2.p5.4.m4.1.1.2.cmml" xref="S4.SS2.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p5.4.m4.1.1.2.1.cmml" xref="S4.SS2.p5.4.m4.1.1">subscript</csymbol><ci id="S4.SS2.p5.4.m4.1.1.2.2.cmml" xref="S4.SS2.p5.4.m4.1.1.2.2">ℎ</ci><ci id="S4.SS2.p5.4.m4.1.1.2.3.cmml" xref="S4.SS2.p5.4.m4.1.1.2.3">𝑖</ci></apply><minus id="S4.SS2.p5.4.m4.1.1.3.cmml" xref="S4.SS2.p5.4.m4.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.4.m4.1c">h_{i}^{-}</annotation></semantics></math> respectively denote the representation of the positive sample and in-batch hard negative sample of sample <math id="S4.SS2.p5.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.p5.5.m5.1a"><mi id="S4.SS2.p5.5.m5.1.1" xref="S4.SS2.p5.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.5.m5.1b"><ci id="S4.SS2.p5.5.m5.1.1.cmml" xref="S4.SS2.p5.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.5.m5.1c">i</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Experiments</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Implementation Details</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Experiments in this paper is based on <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_italic">transformer</span> package<cite class="ltx_cite ltx_citemacro_citep">(Wolf et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2020</a>)</cite> on Pytorch. We use AdamW <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov and Hutter, <a href="#bib.bib39" title="" class="ltx_ref">2017</a>)</cite> optimizer, and the hyper-parameters of AdamW, betas, eps and weight-decay are set to 0.9, 0.999, 1e-8 and 0.01. We use a cosine scheduler and the batch size and peak learning rate for encoders are 128, 1e-5 for RoBERTa-base <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2019</a>)</cite>, VisualBERT <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite> and LXMERT <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib52" title="" class="ltx_ref">2019</a>)</cite>, 32, 6e-6 for RoBERTa-large and 8, 4e-6 for LLAMA2 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Baselines</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">To assess to what extent existing models are competent for the VQA response evaluation, this paper collects four types of common methods for semantic similarity evaluation and refer to them as: formulaic, PLM, LLM, and API.</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">Formulaic methods contain BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2002</a>)</cite>, ROUGE <cite class="ltx_cite ltx_citemacro_citep">(Lin, <a href="#bib.bib34" title="" class="ltx_ref">2004</a>)</cite> and METEOR <cite class="ltx_cite ltx_citemacro_citep">(Banerjee and Lavie, <a href="#bib.bib11" title="" class="ltx_ref">2005</a>)</cite>. These methods base on n-grams for assessing the overlap. As VQA answers are usually short, we also report 2-gram results for BLEU and ROUGE.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">PLM refers to the Pretrained Language Models, which are generally small in sizes and typically in BERT-like encoder-only structures.
SBERT (Sentence BERT) <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a href="#bib.bib47" title="" class="ltx_ref">2019</a>)</cite> embeds texts into vectors with BERT and apply cosine similarity to measure the distance as textual similarity.
SIMCSE <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021</a>)</cite> provides both unsupervised and supervised methods, and this paper selects the supervised and better-performing one trained on NLI datasets for comparison.
BGE <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite> follows a multi-task learning scheme that collects and pretrains on multifarious datasets for better generalization.
AnglE <cite class="ltx_cite ltx_citemacro_citep">(Li and Li, <a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite> aims to mitigate the gradient saturation issue encountered when using cosine distance by projecting vectors onto the complex plane and introducing an angular loss.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">LLM refers to large language models. This paper selects four of the well-performing LLMs, Baichuan2 <cite class="ltx_cite ltx_citemacro_citep">(Baichuan, <a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite>, Qwen <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2023a</a>)</cite>, LLAMA-2 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2023</a>)</cite> and Mistral <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.1" class="ltx_p">API refers to the remote usage of models by API online, including ChatGPT and Text-embedding-v3-large from OpenAI, and Voyage-lite-02-instruct from Voyage AI. Refer to Appendix for prompts. The latter two are embedding models that produce text embedding of given text, which are then used to calculate similarity score by cosine distance.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Main Experiments</h3>

<figure id="S5.T1" class="ltx_table">
<div id="S5.T1.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:397.1pt;height:277.9pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-85.1pt,59.4pt) scale(0.7,0.7) ;">
<table id="S5.T1.4.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.4.4.4" class="ltx_tr">
<th id="S5.T1.4.4.4.5" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S5.T1.4.4.4.5.1" class="ltx_text">Types</span></th>
<th id="S5.T1.4.4.4.6" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S5.T1.4.4.4.6.1" class="ltx_text">Methods</span></th>
<td id="S5.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="4">Alignment <math id="S5.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T1.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T1.1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S5.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2"><span id="S5.T1.2.2.2.2.1" class="ltx_text">Consistency <math id="S5.T1.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T1.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S5.T1.2.2.2.2.1.m1.1.1" xref="S5.T1.2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.2.1.m1.1b"><ci id="S5.T1.2.2.2.2.1.m1.1.1.cmml" xref="S5.T1.2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="S5.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2"><span id="S5.T1.3.3.3.3.1" class="ltx_text">Generalization <math id="S5.T1.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T1.3.3.3.3.1.m1.1a"><mo stretchy="false" id="S5.T1.3.3.3.3.1.m1.1.1" xref="S5.T1.3.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.3.1.m1.1b"><ci id="S5.T1.3.3.3.3.1.m1.1.1.cmml" xref="S5.T1.3.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="S5.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S5.T1.4.4.4.4.1" class="ltx_text">STS Avg. <math id="S5.T1.4.4.4.4.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T1.4.4.4.4.1.m1.1a"><mo stretchy="false" id="S5.T1.4.4.4.4.1.m1.1.1" xref="S5.T1.4.4.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.4.4.1.m1.1b"><ci id="S5.T1.4.4.4.4.1.m1.1.1.cmml" xref="S5.T1.4.4.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.4.4.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
</tr>
<tr id="S5.T1.4.4.5.1" class="ltx_tr">
<td id="S5.T1.4.4.5.1.1" class="ltx_td ltx_align_center">Part 1</td>
<td id="S5.T1.4.4.5.1.2" class="ltx_td ltx_align_center">Part 2</td>
<td id="S5.T1.4.4.5.1.3" class="ltx_td ltx_align_center">Part 3</td>
<td id="S5.T1.4.4.5.1.4" class="ltx_td ltx_align_center ltx_border_r">Avg.</td>
</tr>
<tr id="S5.T1.4.4.6.2" class="ltx_tr">
<th id="S5.T1.4.4.6.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="5"><span id="S5.T1.4.4.6.2.1.1" class="ltx_text">Formulaic</span></th>
<th id="S5.T1.4.4.6.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">BLEU-2</th>
<td id="S5.T1.4.4.6.2.3" class="ltx_td ltx_align_center ltx_border_t">-1.7</td>
<td id="S5.T1.4.4.6.2.4" class="ltx_td ltx_align_center ltx_border_t">-2.6</td>
<td id="S5.T1.4.4.6.2.5" class="ltx_td ltx_align_center ltx_border_t">3.2</td>
<td id="S5.T1.4.4.6.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-0.4</td>
<td id="S5.T1.4.4.6.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.35</td>
<td id="S5.T1.4.4.6.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.30</td>
<td id="S5.T1.4.4.6.2.9" class="ltx_td ltx_align_center ltx_border_t">50.1</td>
</tr>
<tr id="S5.T1.4.4.7.3" class="ltx_tr">
<th id="S5.T1.4.4.7.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">BLEU-4</th>
<td id="S5.T1.4.4.7.3.2" class="ltx_td ltx_align_center">-2.9</td>
<td id="S5.T1.4.4.7.3.3" class="ltx_td ltx_align_center">-3.5</td>
<td id="S5.T1.4.4.7.3.4" class="ltx_td ltx_align_center">2.0</td>
<td id="S5.T1.4.4.7.3.5" class="ltx_td ltx_align_center ltx_border_r">-1.5</td>
<td id="S5.T1.4.4.7.3.6" class="ltx_td ltx_align_center ltx_border_r">4.23</td>
<td id="S5.T1.4.4.7.3.7" class="ltx_td ltx_align_center ltx_border_r">11.25</td>
<td id="S5.T1.4.4.7.3.8" class="ltx_td ltx_align_center">47.6</td>
</tr>
<tr id="S5.T1.4.4.8.4" class="ltx_tr">
<th id="S5.T1.4.4.8.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ROUGE-2</th>
<td id="S5.T1.4.4.8.4.2" class="ltx_td ltx_align_center">-2.8</td>
<td id="S5.T1.4.4.8.4.3" class="ltx_td ltx_align_center">-4.5</td>
<td id="S5.T1.4.4.8.4.4" class="ltx_td ltx_align_center">1.2</td>
<td id="S5.T1.4.4.8.4.5" class="ltx_td ltx_align_center ltx_border_r">-2.0</td>
<td id="S5.T1.4.4.8.4.6" class="ltx_td ltx_align_center ltx_border_r">5.79</td>
<td id="S5.T1.4.4.8.4.7" class="ltx_td ltx_align_center ltx_border_r">11.82</td>
<td id="S5.T1.4.4.8.4.8" class="ltx_td ltx_align_center">53.9</td>
</tr>
<tr id="S5.T1.4.4.9.5" class="ltx_tr">
<th id="S5.T1.4.4.9.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ROUGE-L</th>
<td id="S5.T1.4.4.9.5.2" class="ltx_td ltx_align_center">4.9</td>
<td id="S5.T1.4.4.9.5.3" class="ltx_td ltx_align_center">-0.1</td>
<td id="S5.T1.4.4.9.5.4" class="ltx_td ltx_align_center">3.3</td>
<td id="S5.T1.4.4.9.5.5" class="ltx_td ltx_align_center ltx_border_r">2.7</td>
<td id="S5.T1.4.4.9.5.6" class="ltx_td ltx_align_center ltx_border_r">6.73</td>
<td id="S5.T1.4.4.9.5.7" class="ltx_td ltx_align_center ltx_border_r">10.78</td>
<td id="S5.T1.4.4.9.5.8" class="ltx_td ltx_align_center">48.3</td>
</tr>
<tr id="S5.T1.4.4.10.6" class="ltx_tr">
<th id="S5.T1.4.4.10.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">METEOR</th>
<td id="S5.T1.4.4.10.6.2" class="ltx_td ltx_align_center">12.4</td>
<td id="S5.T1.4.4.10.6.3" class="ltx_td ltx_align_center">4.4</td>
<td id="S5.T1.4.4.10.6.4" class="ltx_td ltx_align_center">15.3</td>
<td id="S5.T1.4.4.10.6.5" class="ltx_td ltx_align_center ltx_border_r">10.7</td>
<td id="S5.T1.4.4.10.6.6" class="ltx_td ltx_align_center ltx_border_r">7.25</td>
<td id="S5.T1.4.4.10.6.7" class="ltx_td ltx_align_center ltx_border_r">9.34</td>
<td id="S5.T1.4.4.10.6.8" class="ltx_td ltx_align_center">53.4</td>
</tr>
<tr id="S5.T1.4.4.11.7" class="ltx_tr">
<th id="S5.T1.4.4.11.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="5"><span id="S5.T1.4.4.11.7.1.1" class="ltx_text">PLM</span></th>
<th id="S5.T1.4.4.11.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">RoBERTa-large (w/o CL)</th>
<td id="S5.T1.4.4.11.7.3" class="ltx_td ltx_align_center ltx_border_t">11.9</td>
<td id="S5.T1.4.4.11.7.4" class="ltx_td ltx_align_center ltx_border_t">0.7</td>
<td id="S5.T1.4.4.11.7.5" class="ltx_td ltx_align_center ltx_border_t">23.4</td>
<td id="S5.T1.4.4.11.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.0</td>
<td id="S5.T1.4.4.11.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.91</td>
<td id="S5.T1.4.4.11.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.36</td>
<td id="S5.T1.4.4.11.7.9" class="ltx_td ltx_align_center ltx_border_t">27.9</td>
</tr>
<tr id="S5.T1.4.4.12.8" class="ltx_tr">
<th id="S5.T1.4.4.12.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">SBERT</th>
<td id="S5.T1.4.4.12.8.2" class="ltx_td ltx_align_center">47.7</td>
<td id="S5.T1.4.4.12.8.3" class="ltx_td ltx_align_center">44.3</td>
<td id="S5.T1.4.4.12.8.4" class="ltx_td ltx_align_center">40.6</td>
<td id="S5.T1.4.4.12.8.5" class="ltx_td ltx_align_center ltx_border_r">44.2</td>
<td id="S5.T1.4.4.12.8.6" class="ltx_td ltx_align_center ltx_border_r">8.79</td>
<td id="S5.T1.4.4.12.8.7" class="ltx_td ltx_align_center ltx_border_r">8.55</td>
<td id="S5.T1.4.4.12.8.8" class="ltx_td ltx_align_center">76.8</td>
</tr>
<tr id="S5.T1.4.4.13.9" class="ltx_tr">
<th id="S5.T1.4.4.13.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">SIMCSE</th>
<td id="S5.T1.4.4.13.9.2" class="ltx_td ltx_align_center">44.9</td>
<td id="S5.T1.4.4.13.9.3" class="ltx_td ltx_align_center">44.7</td>
<td id="S5.T1.4.4.13.9.4" class="ltx_td ltx_align_center">41.7</td>
<td id="S5.T1.4.4.13.9.5" class="ltx_td ltx_align_center ltx_border_r">43.7</td>
<td id="S5.T1.4.4.13.9.6" class="ltx_td ltx_align_center ltx_border_r">9.37</td>
<td id="S5.T1.4.4.13.9.7" class="ltx_td ltx_align_center ltx_border_r">8.30</td>
<td id="S5.T1.4.4.13.9.8" class="ltx_td ltx_align_center">83.8</td>
</tr>
<tr id="S5.T1.4.4.14.10" class="ltx_tr">
<th id="S5.T1.4.4.14.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">BGE</th>
<td id="S5.T1.4.4.14.10.2" class="ltx_td ltx_align_center">42.3</td>
<td id="S5.T1.4.4.14.10.3" class="ltx_td ltx_align_center">36.5</td>
<td id="S5.T1.4.4.14.10.4" class="ltx_td ltx_align_center">41.0</td>
<td id="S5.T1.4.4.14.10.5" class="ltx_td ltx_align_center ltx_border_r">39.9</td>
<td id="S5.T1.4.4.14.10.6" class="ltx_td ltx_align_center ltx_border_r">8.93</td>
<td id="S5.T1.4.4.14.10.7" class="ltx_td ltx_align_center ltx_border_r">8.25</td>
<td id="S5.T1.4.4.14.10.8" class="ltx_td ltx_align_center">84.9</td>
</tr>
<tr id="S5.T1.4.4.15.11" class="ltx_tr">
<th id="S5.T1.4.4.15.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AnglE</th>
<td id="S5.T1.4.4.15.11.2" class="ltx_td ltx_align_center">43.4</td>
<td id="S5.T1.4.4.15.11.3" class="ltx_td ltx_align_center">38.2</td>
<td id="S5.T1.4.4.15.11.4" class="ltx_td ltx_align_center">40.2</td>
<td id="S5.T1.4.4.15.11.5" class="ltx_td ltx_align_center ltx_border_r">40.6</td>
<td id="S5.T1.4.4.15.11.6" class="ltx_td ltx_align_center ltx_border_r">9.01</td>
<td id="S5.T1.4.4.15.11.7" class="ltx_td ltx_align_center ltx_border_r">7.78</td>
<td id="S5.T1.4.4.15.11.8" class="ltx_td ltx_align_center">86.4</td>
</tr>
<tr id="S5.T1.4.4.16.12" class="ltx_tr">
<th id="S5.T1.4.4.16.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="4"><span id="S5.T1.4.4.16.12.1.1" class="ltx_text">LLM</span></th>
<th id="S5.T1.4.4.16.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Baichuan2-7b</th>
<td id="S5.T1.4.4.16.12.3" class="ltx_td ltx_align_center ltx_border_t">28.1</td>
<td id="S5.T1.4.4.16.12.4" class="ltx_td ltx_align_center ltx_border_t">27.8</td>
<td id="S5.T1.4.4.16.12.5" class="ltx_td ltx_align_center ltx_border_t">31.8</td>
<td id="S5.T1.4.4.16.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.2</td>
<td id="S5.T1.4.4.16.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.30</td>
<td id="S5.T1.4.4.16.12.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.86</td>
<td id="S5.T1.4.4.16.12.9" class="ltx_td ltx_align_center ltx_border_t">64.6</td>
</tr>
<tr id="S5.T1.4.4.17.13" class="ltx_tr">
<th id="S5.T1.4.4.17.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Qwen-7b</th>
<td id="S5.T1.4.4.17.13.2" class="ltx_td ltx_align_center">25.9</td>
<td id="S5.T1.4.4.17.13.3" class="ltx_td ltx_align_center">26.3</td>
<td id="S5.T1.4.4.17.13.4" class="ltx_td ltx_align_center">24.2</td>
<td id="S5.T1.4.4.17.13.5" class="ltx_td ltx_align_center ltx_border_r">25.5</td>
<td id="S5.T1.4.4.17.13.6" class="ltx_td ltx_align_center ltx_border_r">9.07</td>
<td id="S5.T1.4.4.17.13.7" class="ltx_td ltx_align_center ltx_border_r">10.10</td>
<td id="S5.T1.4.4.17.13.8" class="ltx_td ltx_align_center">68.3</td>
</tr>
<tr id="S5.T1.4.4.18.14" class="ltx_tr">
<th id="S5.T1.4.4.18.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">LLaMA2-7b</th>
<td id="S5.T1.4.4.18.14.2" class="ltx_td ltx_align_center">32.7</td>
<td id="S5.T1.4.4.18.14.3" class="ltx_td ltx_align_center">27.9</td>
<td id="S5.T1.4.4.18.14.4" class="ltx_td ltx_align_center">34.6</td>
<td id="S5.T1.4.4.18.14.5" class="ltx_td ltx_align_center ltx_border_r">31.7</td>
<td id="S5.T1.4.4.18.14.6" class="ltx_td ltx_align_center ltx_border_r">7.45</td>
<td id="S5.T1.4.4.18.14.7" class="ltx_td ltx_align_center ltx_border_r">8.53</td>
<td id="S5.T1.4.4.18.14.8" class="ltx_td ltx_align_center">61.9</td>
</tr>
<tr id="S5.T1.4.4.19.15" class="ltx_tr">
<th id="S5.T1.4.4.19.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Mistral-7b</th>
<td id="S5.T1.4.4.19.15.2" class="ltx_td ltx_align_center">16.8</td>
<td id="S5.T1.4.4.19.15.3" class="ltx_td ltx_align_center">14.5</td>
<td id="S5.T1.4.4.19.15.4" class="ltx_td ltx_align_center">20.7</td>
<td id="S5.T1.4.4.19.15.5" class="ltx_td ltx_align_center ltx_border_r">17.3</td>
<td id="S5.T1.4.4.19.15.6" class="ltx_td ltx_align_center ltx_border_r">4.61</td>
<td id="S5.T1.4.4.19.15.7" class="ltx_td ltx_align_center ltx_border_r">8.49</td>
<td id="S5.T1.4.4.19.15.8" class="ltx_td ltx_align_center">72.1</td>
</tr>
<tr id="S5.T1.4.4.20.16" class="ltx_tr">
<th id="S5.T1.4.4.20.16.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S5.T1.4.4.20.16.1.1" class="ltx_text">API</span></th>
<th id="S5.T1.4.4.20.16.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">ChatGPT</th>
<td id="S5.T1.4.4.20.16.3" class="ltx_td ltx_align_center ltx_border_t">21.2</td>
<td id="S5.T1.4.4.20.16.4" class="ltx_td ltx_align_center ltx_border_t">15.2</td>
<td id="S5.T1.4.4.20.16.5" class="ltx_td ltx_align_center ltx_border_t">24.6</td>
<td id="S5.T1.4.4.20.16.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20.3</td>
<td id="S5.T1.4.4.20.16.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.21</td>
<td id="S5.T1.4.4.20.16.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.35</td>
<td id="S5.T1.4.4.20.16.9" class="ltx_td ltx_align_center ltx_border_t">73.7</td>
</tr>
<tr id="S5.T1.4.4.21.17" class="ltx_tr">
<th id="S5.T1.4.4.21.17.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Text-embedding-v3-large</th>
<td id="S5.T1.4.4.21.17.2" class="ltx_td ltx_align_center">32.5</td>
<td id="S5.T1.4.4.21.17.3" class="ltx_td ltx_align_center">28.6</td>
<td id="S5.T1.4.4.21.17.4" class="ltx_td ltx_align_center">36.3</td>
<td id="S5.T1.4.4.21.17.5" class="ltx_td ltx_align_center ltx_border_r">32.5</td>
<td id="S5.T1.4.4.21.17.6" class="ltx_td ltx_align_center ltx_border_r">9.40</td>
<td id="S5.T1.4.4.21.17.7" class="ltx_td ltx_align_center ltx_border_r">8.00</td>
<td id="S5.T1.4.4.21.17.8" class="ltx_td ltx_align_center">82.3</td>
</tr>
<tr id="S5.T1.4.4.22.18" class="ltx_tr">
<th id="S5.T1.4.4.22.18.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Voyage-lite-02-instruct</th>
<td id="S5.T1.4.4.22.18.2" class="ltx_td ltx_align_center">29.1</td>
<td id="S5.T1.4.4.22.18.3" class="ltx_td ltx_align_center">28.9</td>
<td id="S5.T1.4.4.22.18.4" class="ltx_td ltx_align_center">29.3</td>
<td id="S5.T1.4.4.22.18.5" class="ltx_td ltx_align_center ltx_border_r">29.1</td>
<td id="S5.T1.4.4.22.18.6" class="ltx_td ltx_align_center ltx_border_r">11.81</td>
<td id="S5.T1.4.4.22.18.7" class="ltx_td ltx_align_center ltx_border_r">6.78</td>
<td id="S5.T1.4.4.22.18.8" class="ltx_td ltx_align_center">86.3</td>
</tr>
<tr id="S5.T1.4.4.23.19" class="ltx_tr">
<th id="S5.T1.4.4.23.19.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" rowspan="3"><span id="S5.T1.4.4.23.19.1.1" class="ltx_text">SFVE (ours)</span></th>
<th id="S5.T1.4.4.23.19.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">SFVE-base</th>
<td id="S5.T1.4.4.23.19.3" class="ltx_td ltx_align_center ltx_border_t">58.4</td>
<td id="S5.T1.4.4.23.19.4" class="ltx_td ltx_align_center ltx_border_t">57.1</td>
<td id="S5.T1.4.4.23.19.5" class="ltx_td ltx_align_center ltx_border_t">53.7</td>
<td id="S5.T1.4.4.23.19.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">56.4</td>
<td id="S5.T1.4.4.23.19.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.12</td>
<td id="S5.T1.4.4.23.19.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.34</td>
<td id="S5.T1.4.4.23.19.9" class="ltx_td ltx_align_center ltx_border_t">81.2</td>
</tr>
<tr id="S5.T1.4.4.24.20" class="ltx_tr">
<th id="S5.T1.4.4.24.20.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">SFVE-large</th>
<td id="S5.T1.4.4.24.20.2" class="ltx_td ltx_align_center">58.1</td>
<td id="S5.T1.4.4.24.20.3" class="ltx_td ltx_align_center">57.5</td>
<td id="S5.T1.4.4.24.20.4" class="ltx_td ltx_align_center">56.0</td>
<td id="S5.T1.4.4.24.20.5" class="ltx_td ltx_align_center ltx_border_r">57.2</td>
<td id="S5.T1.4.4.24.20.6" class="ltx_td ltx_align_center ltx_border_r">9.53</td>
<td id="S5.T1.4.4.24.20.7" class="ltx_td ltx_align_center ltx_border_r">8.67</td>
<td id="S5.T1.4.4.24.20.8" class="ltx_td ltx_align_center">82.0</td>
</tr>
<tr id="S5.T1.4.4.25.21" class="ltx_tr">
<th id="S5.T1.4.4.25.21.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">SFVE-LLAMA2-7b</th>
<td id="S5.T1.4.4.25.21.2" class="ltx_td ltx_align_center ltx_border_bb">60.2</td>
<td id="S5.T1.4.4.25.21.3" class="ltx_td ltx_align_center ltx_border_bb">57.0</td>
<td id="S5.T1.4.4.25.21.4" class="ltx_td ltx_align_center ltx_border_bb">57.2</td>
<td id="S5.T1.4.4.25.21.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">58.1</td>
<td id="S5.T1.4.4.25.21.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">9.46</td>
<td id="S5.T1.4.4.25.21.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">8.87</td>
<td id="S5.T1.4.4.25.21.8" class="ltx_td ltx_align_center ltx_border_bb">77.9</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>The comparison of performance on our proposed AVE dataset. The STS Avg. denotes the average scores over STS 2012 to STS 2016 <cite class="ltx_cite ltx_citemacro_citep">(Agirre et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2012</a>, <a href="#bib.bib6" title="" class="ltx_ref">2013</a>, <a href="#bib.bib3" title="" class="ltx_ref">2014</a>, <a href="#bib.bib2" title="" class="ltx_ref">2015</a>, <a href="#bib.bib4" title="" class="ltx_ref">2016</a>)</cite>, SICK-R <cite class="ltx_cite ltx_citemacro_citep">(Marelli et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2014</a>)</cite> STS-B <cite class="ltx_cite ltx_citemacro_citep">(Cer et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite>, providing a reference of methods’ general discriminating ability. RoBERTa-large <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2019</a>)</cite> (w/o CL) refers to the original pretrained checkpoint without contrastive learning. SFVE-base, SFVE-large and SFVE-LLAMA2-7b are RoBERTa-base, RoBERTa-large and LLAMA2-7b trained by contrastive learning on our proposed pretraining tasks. The specific model checkpoints in experiments are as follows: SBERT<cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a href="#bib.bib47" title="" class="ltx_ref">2019</a>)</cite>: SRoBERTa-NLI-large, BGE<cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite>:BAAI-bge-large-en, SIMCSE<cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021</a>)</cite>: RoBERTa-NLI-large, AnglE <cite class="ltx_cite ltx_citemacro_citep">(Li and Li, <a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite>: RoBERTa-large.</figcaption>
</figure>
<figure id="S5.T2" class="ltx_table">
<div id="S5.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:358.9pt;height:173.6pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-44.9pt,21.6pt) scale(0.8,0.8) ;">
<table id="S5.T2.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.3.3.3" class="ltx_tr">
<th id="S5.T2.3.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S5.T2.3.3.3.4.1" class="ltx_text">Settings</span></th>
<td id="S5.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="4">Alignment <math id="S5.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T2.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T2.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S5.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2"><span id="S5.T2.2.2.2.2.1" class="ltx_text">Consistency <math id="S5.T2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T2.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S5.T2.2.2.2.2.1.m1.1.1" xref="S5.T2.2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.2.1.m1.1b"><ci id="S5.T2.2.2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="S5.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S5.T2.3.3.3.3.1" class="ltx_text">Generalization <math id="S5.T2.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T2.3.3.3.3.1.m1.1a"><mo stretchy="false" id="S5.T2.3.3.3.3.1.m1.1.1" xref="S5.T2.3.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.3.1.m1.1b"><ci id="S5.T2.3.3.3.3.1.m1.1.1.cmml" xref="S5.T2.3.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
</tr>
<tr id="S5.T2.3.3.4.1" class="ltx_tr">
<td id="S5.T2.3.3.4.1.1" class="ltx_td ltx_align_center">Part 1</td>
<td id="S5.T2.3.3.4.1.2" class="ltx_td ltx_align_center">Part 2</td>
<td id="S5.T2.3.3.4.1.3" class="ltx_td ltx_align_center">Part 3</td>
<td id="S5.T2.3.3.4.1.4" class="ltx_td ltx_align_center ltx_border_r">Avg.</td>
</tr>
<tr id="S5.T2.3.3.5.2" class="ltx_tr">
<th id="S5.T2.3.3.5.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">All tasks</th>
<td id="S5.T2.3.3.5.2.2" class="ltx_td ltx_align_center ltx_border_t">58.4</td>
<td id="S5.T2.3.3.5.2.3" class="ltx_td ltx_align_center ltx_border_t">57.1</td>
<td id="S5.T2.3.3.5.2.4" class="ltx_td ltx_align_center ltx_border_t">53.7</td>
<td id="S5.T2.3.3.5.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">56.4</td>
<td id="S5.T2.3.3.5.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.12</td>
<td id="S5.T2.3.3.5.2.7" class="ltx_td ltx_align_center ltx_border_t">8.34</td>
</tr>
<tr id="S5.T2.3.3.6.3" class="ltx_tr">
<th id="S5.T2.3.3.6.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">w/o <span id="S5.T2.3.3.6.3.1.1" class="ltx_text ltx_font_italic">NLI data</span>
</th>
<td id="S5.T2.3.3.6.3.2" class="ltx_td ltx_align_center ltx_border_t">53.3</td>
<td id="S5.T2.3.3.6.3.3" class="ltx_td ltx_align_center ltx_border_t">52.3</td>
<td id="S5.T2.3.3.6.3.4" class="ltx_td ltx_align_center ltx_border_t">50.9</td>
<td id="S5.T2.3.3.6.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">52.2</td>
<td id="S5.T2.3.3.6.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.71</td>
<td id="S5.T2.3.3.6.3.7" class="ltx_td ltx_align_center ltx_border_t">8.50</td>
</tr>
<tr id="S5.T2.3.3.7.4" class="ltx_tr">
<th id="S5.T2.3.3.7.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">w/o <span id="S5.T2.3.3.7.4.1.1" class="ltx_text ltx_font_italic">Candidate answers</span>
</th>
<td id="S5.T2.3.3.7.4.2" class="ltx_td ltx_align_center">57.3</td>
<td id="S5.T2.3.3.7.4.3" class="ltx_td ltx_align_center">56.5</td>
<td id="S5.T2.3.3.7.4.4" class="ltx_td ltx_align_center">53.0</td>
<td id="S5.T2.3.3.7.4.5" class="ltx_td ltx_align_center ltx_border_r">55.6</td>
<td id="S5.T2.3.3.7.4.6" class="ltx_td ltx_align_center ltx_border_r">9.33</td>
<td id="S5.T2.3.3.7.4.7" class="ltx_td ltx_align_center">8.54</td>
</tr>
<tr id="S5.T2.3.3.8.5" class="ltx_tr">
<th id="S5.T2.3.3.8.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">w/o <span id="S5.T2.3.3.8.5.1.1" class="ltx_text ltx_font_italic">Synonym and Antonym</span>
</th>
<td id="S5.T2.3.3.8.5.2" class="ltx_td ltx_align_center">42.1</td>
<td id="S5.T2.3.3.8.5.3" class="ltx_td ltx_align_center">40.3</td>
<td id="S5.T2.3.3.8.5.4" class="ltx_td ltx_align_center">38.8</td>
<td id="S5.T2.3.3.8.5.5" class="ltx_td ltx_align_center ltx_border_r">40.4</td>
<td id="S5.T2.3.3.8.5.6" class="ltx_td ltx_align_center ltx_border_r">9.19</td>
<td id="S5.T2.3.3.8.5.7" class="ltx_td ltx_align_center">8.41</td>
</tr>
<tr id="S5.T2.3.3.9.6" class="ltx_tr">
<th id="S5.T2.3.3.9.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">w/o <span id="S5.T2.3.3.9.6.1.1" class="ltx_text ltx_font_italic">generated descriptions</span>
</th>
<td id="S5.T2.3.3.9.6.2" class="ltx_td ltx_align_center">56.9</td>
<td id="S5.T2.3.3.9.6.3" class="ltx_td ltx_align_center">47.0</td>
<td id="S5.T2.3.3.9.6.4" class="ltx_td ltx_align_center">52.3</td>
<td id="S5.T2.3.3.9.6.5" class="ltx_td ltx_align_center ltx_border_r">52.1</td>
<td id="S5.T2.3.3.9.6.6" class="ltx_td ltx_align_center ltx_border_r">8.07</td>
<td id="S5.T2.3.3.9.6.7" class="ltx_td ltx_align_center">8.93</td>
</tr>
<tr id="S5.T2.3.3.10.7" class="ltx_tr">
<th id="S5.T2.3.3.10.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">w/o All tasks</th>
<td id="S5.T2.3.3.10.7.2" class="ltx_td ltx_align_center ltx_border_t">12.5</td>
<td id="S5.T2.3.3.10.7.3" class="ltx_td ltx_align_center ltx_border_t">3.1</td>
<td id="S5.T2.3.3.10.7.4" class="ltx_td ltx_align_center ltx_border_t">20.0</td>
<td id="S5.T2.3.3.10.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.8</td>
<td id="S5.T2.3.3.10.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.58</td>
<td id="S5.T2.3.3.10.7.7" class="ltx_td ltx_align_center ltx_border_t">9.90</td>
</tr>
<tr id="S5.T2.3.3.11.8" class="ltx_tr">
<th id="S5.T2.3.3.11.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S5.T2.3.3.11.8.1.1" class="ltx_text ltx_font_italic">NLI data</span> only</th>
<td id="S5.T2.3.3.11.8.2" class="ltx_td ltx_align_center ltx_border_t">44.3</td>
<td id="S5.T2.3.3.11.8.3" class="ltx_td ltx_align_center ltx_border_t">42.0</td>
<td id="S5.T2.3.3.11.8.4" class="ltx_td ltx_align_center ltx_border_t">33.1</td>
<td id="S5.T2.3.3.11.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">39.8</td>
<td id="S5.T2.3.3.11.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.46</td>
<td id="S5.T2.3.3.11.8.7" class="ltx_td ltx_align_center ltx_border_t">10.00</td>
</tr>
<tr id="S5.T2.3.3.12.9" class="ltx_tr">
<th id="S5.T2.3.3.12.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S5.T2.3.3.12.9.1.1" class="ltx_text ltx_font_italic">Candidate answers</span> only</th>
<td id="S5.T2.3.3.12.9.2" class="ltx_td ltx_align_center">37.4</td>
<td id="S5.T2.3.3.12.9.3" class="ltx_td ltx_align_center">29.8</td>
<td id="S5.T2.3.3.12.9.4" class="ltx_td ltx_align_center">39.6</td>
<td id="S5.T2.3.3.12.9.5" class="ltx_td ltx_align_center ltx_border_r">35.6</td>
<td id="S5.T2.3.3.12.9.6" class="ltx_td ltx_align_center ltx_border_r">8.10</td>
<td id="S5.T2.3.3.12.9.7" class="ltx_td ltx_align_center">8.11</td>
</tr>
<tr id="S5.T2.3.3.13.10" class="ltx_tr">
<th id="S5.T2.3.3.13.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S5.T2.3.3.13.10.1.1" class="ltx_text ltx_font_italic">Synonym and Antonym</span> only</th>
<td id="S5.T2.3.3.13.10.2" class="ltx_td ltx_align_center">53.8</td>
<td id="S5.T2.3.3.13.10.3" class="ltx_td ltx_align_center">43.7</td>
<td id="S5.T2.3.3.13.10.4" class="ltx_td ltx_align_center">50.6</td>
<td id="S5.T2.3.3.13.10.5" class="ltx_td ltx_align_center ltx_border_r">49.4</td>
<td id="S5.T2.3.3.13.10.6" class="ltx_td ltx_align_center ltx_border_r">7.89</td>
<td id="S5.T2.3.3.13.10.7" class="ltx_td ltx_align_center">9.08</td>
</tr>
<tr id="S5.T2.3.3.14.11" class="ltx_tr">
<th id="S5.T2.3.3.14.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">
<span id="S5.T2.3.3.14.11.1.1" class="ltx_text ltx_font_italic">Generated descriptions</span> only</th>
<td id="S5.T2.3.3.14.11.2" class="ltx_td ltx_align_center ltx_border_bb">42.8</td>
<td id="S5.T2.3.3.14.11.3" class="ltx_td ltx_align_center ltx_border_bb">49.1</td>
<td id="S5.T2.3.3.14.11.4" class="ltx_td ltx_align_center ltx_border_bb">42.1</td>
<td id="S5.T2.3.3.14.11.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">44.6</td>
<td id="S5.T2.3.3.14.11.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">8.17</td>
<td id="S5.T2.3.3.14.11.7" class="ltx_td ltx_align_center ltx_border_bb">7.67</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Ablation experiments of designed pretraining tasks on RoBERTa-base. The row of All tasks represents the best performance of RoBERTa-base with all pretraining tasks, and the row of w/o All tasks contains results from testing on the RoBERTa-base checkpoint without further training. w/o represents without the corresponding pretraining task, contrary to the setting in lower part of the table where the model is trained only on a single task each time.</figcaption>
</figure>
<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Table <a href="#S5.T1" title="Table 1 ‣ 5.3. Main Experiments ‣ 5. Experiments ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> exhibits the performance comparison of various evaluators. The results are on the test set of AVE, with specific scores on each part of the dataset, as described in Section <a href="#S3.SS3" title="3.3. A Dataset Assessing VQA Evaluators ‣ 3. Semantic Evaluation of VQA ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>. To promote a comprehensive assessment of existing methods, this paper compares the performance with four common types of methods for semantic evaluation, as introduced in Section <a href="#S5.SS2" title="5.2. Baselines ‣ 5. Experiments ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>. The last row of Types contains our results from training with the proposed pretraining tasks on the corresponding model. Then, the column of Alignment contains the separate results on each of the three parts in AVE datasets and their average.</p>
</div>
<section id="S5.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1. </span>Performance of Formulaic Methods</h4>

<div id="S5.SS3.SSS1.p1" class="ltx_para">
<p id="S5.SS3.SSS1.p1.1" class="ltx_p">Formulaic methods, i.e., BLEU, ROUGE and METEOR perform poorly in Alignment scores, and some of them drop below 0, indicating
adverse scores to the human annotation. Such phenomenon is expected, as the n-gram matching strategy of BLEU and ROUGE is unable to handle the synonyms or variations in tenses and singular or plural forms. For METEOR, however, it applies port stem <cite class="ltx_cite ltx_citemacro_citep">(Banerjee and Lavie, <a href="#bib.bib11" title="" class="ltx_ref">2005</a>)</cite> and synonym matching to preprocess the 1-gram in both the candidate and reference, restoring words to stems and thus performs better.</p>
</div>
<div id="S5.SS3.SSS1.p2" class="ltx_para">
<p id="S5.SS3.SSS1.p2.1" class="ltx_p">In addition, it is interesting to notice that although the Alignment of BLEU and ROUGE are much lower that that of METEOR, their Generalization scores are much higher. There are two reasons to this anomaly. First, BLEU and ROUGE fail to handle the task well and their prediction can be considered random, thus the sources of data do not affect the results, just like RoBERTa-large (w/o CL). Second, these n-gram evaluators do not involve semantics, therefore the sources of data that causes word distribution shift matter less.</p>
</div>
</section>
<section id="S5.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.2. </span>Performance of PLMs</h4>

<div id="S5.SS3.SSS2.p1" class="ltx_para">
<p id="S5.SS3.SSS2.p1.1" class="ltx_p">The BERT-like models pretrained for textual similarity prediction, i.e., SBERT, SIMCSE, BGE, AnglE (the latter four models), show much better performance than RoBERTa-large (w/o CL) and formulaic methods, indicating the basic textual similarity tasks are helpful to the VQA response evaluation task, but they fail to align well with human judgement, compared to SFVE results under the same structure of BERT-large. In addition, the performance on Part 1 and 3 of PLM models are similar, and the major gap lies in the capability of processing long responses.
</p>
</div>
</section>
<section id="S5.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.3. </span>Performance of LLMs</h4>

<div id="S5.SS3.SSS3.p1" class="ltx_para">
<p id="S5.SS3.SSS3.p1.1" class="ltx_p">For LLMs (refer to Appendix for the detailed prompt) including ChatGPT, they fail to gain satisfactory results on AVE. Naturally, LLM performs better than RoBERTa w/o CL, and Generalization scores are slightly higher than PLMs, which we attribute to the better generalization ability of LLMs. Although LLMs obtain acceptable results on STS tasks, just like the formulaic methods, they encounter significant performance drop on the VQA response evaluation. Such phenomenon verifies the significant difference between the evaluation of STS and VQA responses and the necessity in the task of VQA evaluation.</p>
</div>
<div id="S5.SS3.SSS3.p2" class="ltx_para">
<p id="S5.SS3.SSS3.p2.1" class="ltx_p">The performance of embedding models (the latter two models) on STS is higher than LLMs but the VQA response evaluation performance is still low. The reason to their incompetence on AVE, as we speculate, is that these models focus more on retrieving and capturing the general meaning of given texts than discovering the fine-grained difference between given pair of texts. In addition, such focus of capturing the general meaning has also empowered them with the ability to ignore noise in morphology and text length, thus gaining high scores of Consistency despite the low Alignment scores.</p>
</div>
</section>
<section id="S5.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.4. </span>Performance of SFVE</h4>

<div id="S5.SS3.SSS4.p1" class="ltx_para">
<p id="S5.SS3.SSS4.p1.1" class="ltx_p">The section of SFVE (ours) in the table presents our results on AVE. The pretraining tasks effectively improve the Alignment scores of all three models and bring moderately better Consistency and Generalization performance. From the prospective of model sizes, the 125M Roberta-base demonstrates similar capability with the 355M Roberta-large with merely a gap of 0.8%. The same applies for the 7b LLAMA2, which surpasses RoBERTa-large by 0.9%. Giant increase in model sizes brings minor improvement in scores.
We believe the reason is that the similarity measure, either in STS or AVE, is relatively simple for models to comprehend and implement, where a simple structure with limited parameters achieves excellent performance with proper training.</p>
</div>
<div id="S5.SS3.SSS4.p2" class="ltx_para">
<p id="S5.SS3.SSS4.p2.1" class="ltx_p">Therefore, for practical usage of evaluators during the training of generative VQA models, considering the significantly larger computation cost in LLAMA2-7b than RoBERTa, we recommend utilizing SFVE-RoBERTa base or large for a rough validation of model performance each certain steps or epoch, and use SFVE-LLAMA2 for more accurate evaluation near the best steps or epochs.</p>
</div>
</section>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Ablation Experiments</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">To analyze the influence of each pretraining task, Table <a href="#S5.T2" title="Table 2 ‣ 5.3. Main Experiments ‣ 5. Experiments ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides ablation results by removing a pretraining task each time and by training on a single task alone. From the table it is clear that all pretraining tasks contribute to the final performance more or less.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">The most important task is <span id="S5.SS4.p2.1.1" class="ltx_text ltx_font_italic">Synonym and Antonym</span>, which causes a drop of 16.0% in Alignment scores on average and damages Consistency as well. In addition, when trained only on such data, the model performs the best. We believe the importance of training on <span id="S5.SS4.p2.1.2" class="ltx_text ltx_font_italic">Synonym and Antonym</span> task lies in aligning the representation of synonyms and increasing the difference towards antonyms and other answers.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">The second influencing pretraining task is <span id="S5.SS4.p3.1.1" class="ltx_text ltx_font_italic">Generated descriptions</span>, without which the model can not directly learn to align the representation between semantically similar texts with different length. Yet the removal of it does not substantially damage the results on other parts than Part 2, which consists of long responses.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.1" class="ltx_p">Meanwhile, the removal of <span id="S5.SS4.p4.1.1" class="ltx_text ltx_font_italic">NLI data</span> matters almost the same as <span id="S5.SS4.p4.1.2" class="ltx_text ltx_font_italic">Generated descriptions</span>. As mentioned before, NLI data focuses more on the coarse-grained meaning between text pairs while AVE requires a finer semantic discrimination. However, for a model that barely handles the task (shown in the row of <span id="S5.SS4.p4.1.3" class="ltx_text ltx_font_italic">w/o All tasks</span>), we believe the easier data in NLI aid to fertilizing the basic capability in semantic evaluation. Yet the NLI data alone is insufficient, as shown in the row of <span id="S5.SS4.p4.1.4" class="ltx_text ltx_font_italic">NLI data only</span>.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2408.00300/assets/x4.png" id="S5.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="151" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Cases for analysis. The samples come from the open-ended part of A-OKVQA <cite class="ltx_cite ltx_citemacro_citep">(Schwenk et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite> validation set. The first row comes from results of SFVE-large and SBERT, and the second comes from SFVE-large and BGE.</figcaption>
</figure>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5. </span>Practical Application</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">To demonstrate the practical values of our proposed evaluator in flexible VQA response evaluation, we collect responses of multiple MLLMs and compare the results with different evaluators by overall scores and case study.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<div id="S5.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.6pt;height:139.3pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-74.3pt,29.7pt) scale(0.7,0.7) ;">
<table id="S5.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.1.1.2.1" class="ltx_tr">
<th id="S5.T3.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S5.T3.1.1.2.1.1.1" class="ltx_text">Model</span></th>
<td id="S5.T3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">Evaluation Metric</td>
</tr>
<tr id="S5.T3.1.1.3.2" class="ltx_tr">
<td id="S5.T3.1.1.3.2.1" class="ltx_td ltx_align_center">VQA Score</td>
<td id="S5.T3.1.1.3.2.2" class="ltx_td ltx_align_center">BGE</td>
<td id="S5.T3.1.1.3.2.3" class="ltx_td ltx_align_center">SBERT</td>
<td id="S5.T3.1.1.3.2.4" class="ltx_td ltx_align_center">SFVE (ours)</td>
</tr>
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">LXMERT <sup id="S5.T3.1.1.1.1.1" class="ltx_sup"><span id="S5.T3.1.1.1.1.1.1" class="ltx_text ltx_font_italic">†</span></sup> <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib52" title="" class="ltx_ref">2019</a>)</cite>
</th>
<td id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">19.5</td>
<td id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">83.3</td>
<td id="S5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">75.6</td>
<td id="S5.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">43.6</td>
</tr>
<tr id="S5.T3.1.1.4.3" class="ltx_tr">
<th id="S5.T3.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">LXMERT</th>
<td id="S5.T3.1.1.4.3.2" class="ltx_td ltx_align_center">37.3</td>
<td id="S5.T3.1.1.4.3.3" class="ltx_td ltx_align_center">94.9</td>
<td id="S5.T3.1.1.4.3.4" class="ltx_td ltx_align_center">83.6</td>
<td id="S5.T3.1.1.4.3.5" class="ltx_td ltx_align_center">67.9</td>
</tr>
<tr id="S5.T3.1.1.5.4" class="ltx_tr">
<th id="S5.T3.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">VisualBERT <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite>
</th>
<td id="S5.T3.1.1.5.4.2" class="ltx_td ltx_align_center">37.6</td>
<td id="S5.T3.1.1.5.4.3" class="ltx_td ltx_align_center">94.8</td>
<td id="S5.T3.1.1.5.4.4" class="ltx_td ltx_align_center">83.4</td>
<td id="S5.T3.1.1.5.4.5" class="ltx_td ltx_align_center">66.3</td>
</tr>
<tr id="S5.T3.1.1.6.5" class="ltx_tr">
<th id="S5.T3.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">LLaVA-7b <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2024a</a>)</cite>
</th>
<td id="S5.T3.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_t">3.6</td>
<td id="S5.T3.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t">89.1</td>
<td id="S5.T3.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t">82.5</td>
<td id="S5.T3.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_t">72.3</td>
</tr>
<tr id="S5.T3.1.1.7.6" class="ltx_tr">
<th id="S5.T3.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">BLIP2-opt-2.7b <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S5.T3.1.1.7.6.2" class="ltx_td ltx_align_center">15.5</td>
<td id="S5.T3.1.1.7.6.3" class="ltx_td ltx_align_center">94.1</td>
<td id="S5.T3.1.1.7.6.4" class="ltx_td ltx_align_center">83.1</td>
<td id="S5.T3.1.1.7.6.5" class="ltx_td ltx_align_center">70.2</td>
</tr>
<tr id="S5.T3.1.1.8.7" class="ltx_tr">
<th id="S5.T3.1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">InstructBLIP-Vicuna <cite class="ltx_cite ltx_citemacro_citep">(Dai et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S5.T3.1.1.8.7.2" class="ltx_td ltx_align_center">21.4</td>
<td id="S5.T3.1.1.8.7.3" class="ltx_td ltx_align_center">94.8</td>
<td id="S5.T3.1.1.8.7.4" class="ltx_td ltx_align_center">86.4</td>
<td id="S5.T3.1.1.8.7.5" class="ltx_td ltx_align_center">74.3</td>
</tr>
<tr id="S5.T3.1.1.9.8" class="ltx_tr">
<th id="S5.T3.1.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">mPLUG-Owl <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S5.T3.1.1.9.8.2" class="ltx_td ltx_align_center">0.0</td>
<td id="S5.T3.1.1.9.8.3" class="ltx_td ltx_align_center">91.0</td>
<td id="S5.T3.1.1.9.8.4" class="ltx_td ltx_align_center">82.7</td>
<td id="S5.T3.1.1.9.8.5" class="ltx_td ltx_align_center">69.1</td>
</tr>
<tr id="S5.T3.1.1.10.9" class="ltx_tr">
<th id="S5.T3.1.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">OFA-large <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S5.T3.1.1.10.9.2" class="ltx_td ltx_align_center">39.5</td>
<td id="S5.T3.1.1.10.9.3" class="ltx_td ltx_align_center">95.3</td>
<td id="S5.T3.1.1.10.9.4" class="ltx_td ltx_align_center">86.5</td>
<td id="S5.T3.1.1.10.9.5" class="ltx_td ltx_align_center">78.0</td>
</tr>
<tr id="S5.T3.1.1.11.10" class="ltx_tr">
<th id="S5.T3.1.1.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">Qwen-VL-chat <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2023b</a>)</cite>
</th>
<td id="S5.T3.1.1.11.10.2" class="ltx_td ltx_align_center ltx_border_bb">54.9</td>
<td id="S5.T3.1.1.11.10.3" class="ltx_td ltx_align_center ltx_border_bb">96.1</td>
<td id="S5.T3.1.1.11.10.4" class="ltx_td ltx_align_center ltx_border_bb">89.7</td>
<td id="S5.T3.1.1.11.10.5" class="ltx_td ltx_align_center ltx_border_bb">83.5</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>Practical application of utilizing our proposed evaluator for assessing the responses from MLLMs. The VQA dataset for response generation is the open-ended validation set of A-OKVQA<cite class="ltx_cite ltx_citemacro_citep">(Schwenk et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>. Models in the upper part of the table are smaller than 0.5B.
VisualBERT and LXMERT are fine-tuned on VQA v2<cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2017</a>)</cite>. LXMERT <sup id="S5.T3.5.1" class="ltx_sup"><span id="S5.T3.5.1.1" class="ltx_text ltx_font_italic">†</span></sup> means the LXMERT that is not sufficiently trained, which ends training at the half of the first epoch to provide comparison. SFVE (ours) uses the RoBERTa-large evaluator trained with our proposed pretraining tasks. Refer to Appendix for the calculation of VQA Score. Note that the scores are for comparison within an evaluator itself, and it is meaningless to compare scores across evaluators, as evaluators are not aligned.  </figcaption>
</figure>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.1" class="ltx_p">As shown in Table <a href="#S5.T3" title="Table 3 ‣ 5.5. Practical Application ‣ 5. Experiments ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, VQA score is clearly incompetent for assessing assorted responses from MLLMs. Since all responses from mPLUG-Owl are sentences, VQA Score even comes to 0. In the comparison of LXMERT and mPLUG-Owl, both BGE and SBERT indicate LXMERT generates better responses than mPLUG-Owl. However, taking the case study in Figure <a href="#S5.F4" title="Figure 4 ‣ 5.4. Ablation Experiments ‣ 5. Experiments ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> into consideration, we verify that existing well-performing methods, BGE and SBERT, fail to perform consistent evaluation and bias towards short responses while penalizing longer ones. For example, in (a) of Figure <a href="#S5.F4" title="Figure 4 ‣ 5.4. Ablation Experiments ‣ 5. Experiments ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, LXMERT response <span id="S5.SS5.p2.1.1" class="ltx_text ltx_font_italic">pan</span> receives a much higher score than the mPLUG-Owl response which is a descriptive sentence containing the correct answer. In (d), the descriptive text and the single word response receive similar scores under our SFVE, but SBERT considers the short answer of LXMERT is much better than the descriptive sentence of mPLUG-Owl. Similar phenomena exist in BGE as well. As in (e), mPLUG-Owl response describing <span id="S5.SS5.p2.1.2" class="ltx_text ltx_font_italic">garbage collector</span> is a better response than LXMERT output <span id="S5.SS5.p2.1.3" class="ltx_text ltx_font_italic">police</span>, yet the latter receives even higher scores.</p>
</div>
<div id="S5.SS5.p3" class="ltx_para">
<p id="S5.SS5.p3.1" class="ltx_p">In addition, not only does the length impede a fair evaluation, but the incompetence in fine-grained semantics discrimination also causes absurd results. Like in (c) and (g), where LXMERT answers are less correct but they receive competitive or even higher scores than reasonable responses from BGE and SBERT. Such error, we speculate, is caused by focusing more on the overall meaning of text, as the questions are the same within a pair, while neglecting fine-grained difference.</p>
</div>
<div id="S5.SS5.p4" class="ltx_para">
<p id="S5.SS5.p4.1" class="ltx_p">Due to the phenomena above, it is clear the superficial superiority of LXMERT over mPLUG-Owl is merely a mistake by incompetent VQA evaluators, which also demonstrates the importance of fairness and consistency in VQA evaluation. Therefore, we consider the proposed pretraining tasks and SFVE effective, not only on our proposed AVE dataset, but also in practical application where previous methods fail to perform fair and insightful evaluation.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper proposes a practical task of utilizing semantic correctness to evaluate unconstrained open-ended VQA responses, facilitating the assessment of MLLMs’ multimodal comprehension abilities by VQA data.
We propose three key properties for assessing VQA evaluators, i.e., Alignment, Consistency and Generalization.
In addition, this paper proposes a new dataset assessing VQA evaluators (AVE) to comprehensively analyze multiple aspects of evaluators. Based on contrastive learning with meticulously designed pretraining tasks, this paper provides a Semantically Flexible VQA Evaluator (SFVE) that performs significantly better than existing evaluators on VQA evaluation and the training scheme generalizes to both the encoder-only and decoder-only models.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agirre et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, et al<span id="bib.bib2.3.1" class="ltx_text">.</span> 2015.

</span>
<span class="ltx_bibblock">Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability. In <em id="bib.bib2.4.1" class="ltx_emph ltx_font_italic">Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015)</em>. 252–263.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agirre et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014.

</span>
<span class="ltx_bibblock">Semeval-2014 task 10: Multilingual semantic textual similarity. In <em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 8th international workshop on semantic evaluation (SemEval 2014)</em>. 81–91.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agirre et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez Agirre, Rada Mihalcea, German Rigau Claramunt, and Janyce Wiebe. 2016.

</span>
<span class="ltx_bibblock">Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. In <em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">SemEval-2016. 10th International Workshop on Semantic Evaluation; 2016 Jun 16-17; San Diego, CA. Stroudsburg (PA): ACL; 2016. p. 497-511.</em> ACL (Association for Computational Linguistics).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agirre et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012.

</span>
<span class="ltx_bibblock">SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity.* SEM 2012: The First Joint Conference on Lexical and Computational Semantics—. In <em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), Montréal, QC, Canada</em>. 7–8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agirre et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo. 2013.

</span>
<span class="ltx_bibblock">* SEM 2013 shared task: Semantic textual similarity. In <em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">Second joint conference on lexical and computational semantics (* SEM), volume 1: proceedings of the Main conference and the shared task: semantic textual similarity</em>. 32–43.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering. In <em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>. 2425–2433.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,
Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023a.

</span>
<span class="ltx_bibblock">Qwen Technical Report.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.16609</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023b.

</span>
<span class="ltx_bibblock">Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.12966</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baichuan (2023)</span>
<span class="ltx_bibblock">
Baichuan. 2023.

</span>
<span class="ltx_bibblock">Baichuan 2: Open Large-scale Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.10305</em> (2023).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://arxiv.org/abs/2309.10305" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2309.10305</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee and Lavie (2005)</span>
<span class="ltx_bibblock">
Satanjeev Banerjee and Alon Lavie. 2005.

</span>
<span class="ltx_bibblock">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</em>. 65–72.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhandari et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020.

</span>
<span class="ltx_bibblock">Re-evaluating evaluation in text summarization.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.07100</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cer et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. 2017.

</span>
<span class="ltx_bibblock">Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1708.00055</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chaoyou et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Fu Chaoyou, Chen Peixian, Shen Yunhang, Qin Yulei, Zhang Mengdan, Lin Xu, Yang Jinrui, Zheng Xiawu, Li Ke, Sun Xing, Wu Yunsheng, and Ji Rongrong. 2023.

</span>
<span class="ltx_bibblock">MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.13394</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2023.

</span>
<span class="ltx_bibblock">BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2309.07597 [cs.CL]

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020.

</span>
<span class="ltx_bibblock">A simple framework for contrastive learning of visual representations. In <em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>. PMLR, 1597–1607.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau and Kiela (2018)</span>
<span class="ltx_bibblock">
Alexis Conneau and Douwe Kiela. 2018.

</span>
<span class="ltx_bibblock">Senteval: An evaluation toolkit for universal sentence representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.05449</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. 2017.

</span>
<span class="ltx_bibblock">Supervised learning of universal sentence representations from natural language inference data.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1705.02364</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023.

</span>
<span class="ltx_bibblock">InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.06500</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fabbri et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Alexander R Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021.

</span>
<span class="ltx_bibblock">Summeval: Re-evaluating summarization evaluation.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em> 9 (2021), 391–409.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Francis and Kucera (1979)</span>
<span class="ltx_bibblock">
W Nelson Francis and Henry Kucera. 1979.

</span>
<span class="ltx_bibblock">Brown corpus manual.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Letters to the Editor</em> 5, 2 (1979), 7.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.

</span>
<span class="ltx_bibblock">Simcse: Simple contrastive learning of sentence embeddings.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.08821</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In <em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 6904–6913.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grusky et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Max Grusky, Mor Naaman, and Yoav Artzi. 2018.

</span>
<span class="ltx_bibblock">Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1804.11283</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gurari et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. 2018.

</span>
<span class="ltx_bibblock">Vizwiz grand challenge: Answering visual questions from blind people. In <em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 3608–3617.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning (2019)</span>
<span class="ltx_bibblock">
Drew A Hudson and Christopher D Manning. 2019.

</span>
<span class="ltx_bibblock">Gqa: A new dataset for real-world visual reasoning and compositional question answering. In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 6700–6709.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023.

</span>
<span class="ltx_bibblock">Mistral 7B.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.06825</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kafle and Kanan (2017)</span>
<span class="ltx_bibblock">
Kushal Kafle and Christopher Kanan. 2017.

</span>
<span class="ltx_bibblock">An analysis of visual question answering algorithms. In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>. 1965–1973.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krippendorff (2011)</span>
<span class="ltx_bibblock">
Klaus Krippendorff. 2011.

</span>
<span class="ltx_bibblock">Computing Krippendorff’s alpha-reliability.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al<span id="bib.bib30.3.1" class="ltx_text">.</span> 2017.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced dense image annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.4.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em> 123 (2017), 32–73.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023.

</span>
<span class="ltx_bibblock">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.12597</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019.

</span>
<span class="ltx_bibblock">Visualbert: A simple and performant baseline for vision and language.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.03557</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Li (2023)</span>
<span class="ltx_bibblock">
Xianming Li and Jing Li. 2023.

</span>
<span class="ltx_bibblock">AnglE-optimized Text Embeddings.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.12871</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock">Rouge: A package for automatic evaluation of summaries. In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Text summarization branches out</em>. 74–81.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023.

</span>
<span class="ltx_bibblock">Improved baselines with visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.03744</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2024a)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024a.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2024b)</span>
<span class="ltx_bibblock">
Shuo Liu, Kaining Ying, Hao Zhang, Yue Yang, Yuqi Lin, Tianle Zhang, Chuanhao Li, Yu Qiao, Ping Luo, Wenqi Shao, et al<span id="bib.bib37.3.1" class="ltx_text">.</span> 2024b.

</span>
<span class="ltx_bibblock">Convbench: A multi-turn conversation evaluation benchmark with hierarchical capability for large vision-language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.20194</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2017)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2017.

</span>
<span class="ltx_bibblock">Fixing weight decay regularization in adam.

</span>
<span class="ltx_bibblock">(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski and Fritz (2014)</span>
<span class="ltx_bibblock">
Mateusz Malinowski and Mario Fritz. 2014.

</span>
<span class="ltx_bibblock">A multi-world approach to question answering about real-world scenes based on uncertain input.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 27 (2014).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marelli et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014.

</span>
<span class="ltx_bibblock">A SICK cure for the evaluation of compositional distributional semantic models. In <em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">9th Language Resources and Evaluation Conference</em>. Reykjavik.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino et al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019.

</span>
<span class="ltx_bibblock">Ok-vqa: A visual question answering benchmark requiring external knowledge. In <em id="bib.bib42.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/cvf conference on computer vision and pattern recognition</em>. 3195–3204.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller et al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (1990)</span>
<span class="ltx_bibblock">
George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J. Miller. 1990.

</span>
<span class="ltx_bibblock">Introduction to WordNet: An On-line Lexical Database.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">International Journal of Lexicography</em> 3, 4 (1990), 235–244.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Niklas et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Muennighoff Niklas, Su Hongjin, Wang Liang, Yang Nan, Wei Furu, Yu Tao, Singh Amanpreet, and Kiela Douwe. 2024.

</span>
<span class="ltx_bibblock">Generative Representational Instruction Tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.09906</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Noh et al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han. 2016.

</span>
<span class="ltx_bibblock">Image question answering using convolutional neural network with dynamic parameter prediction. In <em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 30–38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation. In <em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</em>. 311–318.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock">Sentence-bert: Sentence embeddings using siamese bert-networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.10084</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Mengye Ren, Ryan Kiros, and Richard Zemel. 2015.

</span>
<span class="ltx_bibblock">Exploring models and data for image question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 28 (2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwenk et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022.

</span>
<span class="ltx_bibblock">A-okvqa: A benchmark for visual question answering using world knowledge. In <em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part VIII</em>. Springer, 146–162.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Wenqi Shao, Yutao Hu, Peng Gao, Meng Lei, Kaipeng Zhang, Fanqing Meng, Peng Xu, Siyuan Huang, Hongsheng Li, Yu Qiao, et al<span id="bib.bib50.3.1" class="ltx_text">.</span> 2023.

</span>
<span class="ltx_bibblock">Tiny lvlm-ehub: Early multimodal experiments with bard.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.03729</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Chuanqi Tan, Furu Wei, Wenhui Wang, Weifeng Lv, and Ming Zhou. 2018.

</span>
<span class="ltx_bibblock">Multiway attention networks for modeling sentence pairs.. In <em id="bib.bib51.3.1" class="ltx_emph ltx_font_italic">IJCAI</em>. 4411–4417.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Bansal (2019)</span>
<span class="ltx_bibblock">
Hao Tan and Mohit Bansal. 2019.

</span>
<span class="ltx_bibblock">Lxmert: Learning cross-modality encoder representations from transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.07490</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al<span id="bib.bib53.3.1" class="ltx_text">.</span> 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. 2017.

</span>
<span class="ltx_bibblock">Fvqa: Fact-based visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</em> 40, 10 (2017), 2413–2427.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, and Anthony Dick. 2015.

</span>
<span class="ltx_bibblock">Explicit knowledge-based reasoning for visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1511.02570</em> (2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib56.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022.

</span>
<span class="ltx_bibblock">Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In <em id="bib.bib56.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>. PMLR, 23318–23340.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Williams et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017.

</span>
<span class="ltx_bibblock">A broad-coverage challenge corpus for sentence understanding through inference.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1704.05426</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020.

</span>
<span class="ltx_bibblock">Transformers: State-of-the-Art Natural Language Processing. In <em id="bib.bib58.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>. Association for Computational Linguistics, Online, 38–45.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.aclweb.org/anthology/2020.emnlp-demos.6" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/2020.emnlp-demos.6</a>

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Qi Wu, Chunhua Shen, Peng Wang, Anthony Dick, and Anton Van Den Hengel. 2017.

</span>
<span class="ltx_bibblock">Image captioning and visual question answering based on attributes and external knowledge.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.3.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</em> 40, 6 (2017), 1367–1381.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib60.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. 2016.

</span>
<span class="ltx_bibblock">Stacked attention networks for image question answering. In <em id="bib.bib60.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 21–29.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al<span id="bib.bib61.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al<span id="bib.bib61.3.1" class="ltx_text">.</span> 2023.

</span>
<span class="ltx_bibblock">mplug-owl: Modularization empowers large language models with multimodality.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.14178</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al<span id="bib.bib62.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Wenpeng Yin, Hinrich Schütze, Bing Xiang, and Bowen Zhou. 2016.

</span>
<span class="ltx_bibblock">Abcnn: Attention-based convolutional neural network for modeling sentence pairs.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.3.1" class="ltx_emph ltx_font_italic">Transactions of the Association for computational linguistics</em> 4 (2016), 259–272.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ying et al<span id="bib.bib63.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, et al<span id="bib.bib63.3.1" class="ltx_text">.</span> 2024.

</span>
<span class="ltx_bibblock">Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.16006</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span id="bib.bib64.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023.

</span>
<span class="ltx_bibblock">Mm-vet: Evaluating large multimodal models for integrated capabilities.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.02490</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib65.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019.

</span>
<span class="ltx_bibblock">Bertscore: Evaluating text generation with bert.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.09675</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Traditional VQA Evaluation Metrics</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Traditional VQA evaluation metrics contain Exact Match <cite class="ltx_cite ltx_citemacro_citep">(Malinowski and Fritz, <a href="#bib.bib40" title="" class="ltx_ref">2014</a>)</cite> and VQA Score <cite class="ltx_cite ltx_citemacro_citep">(Antol et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2015</a>)</cite>. They apply for different settings in VQA datasets. For datasets where each sample contains only one correct answer, like DAQUAR <cite class="ltx_cite ltx_citemacro_citep">(Malinowski and Fritz, <a href="#bib.bib40" title="" class="ltx_ref">2014</a>)</cite>, TDIUC <cite class="ltx_cite ltx_citemacro_citep">(Kafle and Kanan, <a href="#bib.bib28" title="" class="ltx_ref">2017</a>)</cite>, GQA <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>, the Exact Match is used. If each sample contains ten candidate answers, like VQA v2 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2017</a>)</cite>, OKVQA <cite class="ltx_cite ltx_citemacro_citep">(Marino et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite>, VizWiz <cite class="ltx_cite ltx_citemacro_citep">(Gurari et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2018</a>)</cite>, VQA Score is commonly used.</p>
</div>
<section id="A1.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Exact Match</h5>

<div id="A1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px1.p1.1" class="ltx_p">Exact Match calculates by judging whether the response is identical to the annotated ground-truth answer, and if matches, the score is 1, otherwise 0.</p>
</div>
</section>
<section id="A1.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">VQA Score</h5>

<div id="A1.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px2.p1.1" class="ltx_p">VQA Score evaluates how many times the response appear in the ten candidate answers, and is computed as follows:</p>
<table id="A1.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A1.Ex1.m1.2" class="ltx_Math" alttext="accuracy=min(\frac{\#\ correct\ hits}{3},1)" display="block"><semantics id="A1.Ex1.m1.2a"><mrow id="A1.Ex1.m1.2.3" xref="A1.Ex1.m1.2.3.cmml"><mrow id="A1.Ex1.m1.2.3.2" xref="A1.Ex1.m1.2.3.2.cmml"><mi id="A1.Ex1.m1.2.3.2.2" xref="A1.Ex1.m1.2.3.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="A1.Ex1.m1.2.3.2.1" xref="A1.Ex1.m1.2.3.2.1.cmml">​</mo><mi id="A1.Ex1.m1.2.3.2.3" xref="A1.Ex1.m1.2.3.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="A1.Ex1.m1.2.3.2.1a" xref="A1.Ex1.m1.2.3.2.1.cmml">​</mo><mi id="A1.Ex1.m1.2.3.2.4" xref="A1.Ex1.m1.2.3.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="A1.Ex1.m1.2.3.2.1b" xref="A1.Ex1.m1.2.3.2.1.cmml">​</mo><mi id="A1.Ex1.m1.2.3.2.5" xref="A1.Ex1.m1.2.3.2.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="A1.Ex1.m1.2.3.2.1c" xref="A1.Ex1.m1.2.3.2.1.cmml">​</mo><mi id="A1.Ex1.m1.2.3.2.6" xref="A1.Ex1.m1.2.3.2.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="A1.Ex1.m1.2.3.2.1d" xref="A1.Ex1.m1.2.3.2.1.cmml">​</mo><mi id="A1.Ex1.m1.2.3.2.7" xref="A1.Ex1.m1.2.3.2.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="A1.Ex1.m1.2.3.2.1e" xref="A1.Ex1.m1.2.3.2.1.cmml">​</mo><mi id="A1.Ex1.m1.2.3.2.8" xref="A1.Ex1.m1.2.3.2.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="A1.Ex1.m1.2.3.2.1f" xref="A1.Ex1.m1.2.3.2.1.cmml">​</mo><mi id="A1.Ex1.m1.2.3.2.9" xref="A1.Ex1.m1.2.3.2.9.cmml">y</mi></mrow><mo id="A1.Ex1.m1.2.3.1" xref="A1.Ex1.m1.2.3.1.cmml">=</mo><mrow id="A1.Ex1.m1.2.3.3" xref="A1.Ex1.m1.2.3.3.cmml"><mi id="A1.Ex1.m1.2.3.3.2" xref="A1.Ex1.m1.2.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="A1.Ex1.m1.2.3.3.1" xref="A1.Ex1.m1.2.3.3.1.cmml">​</mo><mi id="A1.Ex1.m1.2.3.3.3" xref="A1.Ex1.m1.2.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="A1.Ex1.m1.2.3.3.1a" xref="A1.Ex1.m1.2.3.3.1.cmml">​</mo><mi id="A1.Ex1.m1.2.3.3.4" xref="A1.Ex1.m1.2.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="A1.Ex1.m1.2.3.3.1b" xref="A1.Ex1.m1.2.3.3.1.cmml">​</mo><mrow id="A1.Ex1.m1.2.3.3.5.2" xref="A1.Ex1.m1.2.3.3.5.1.cmml"><mo stretchy="false" id="A1.Ex1.m1.2.3.3.5.2.1" xref="A1.Ex1.m1.2.3.3.5.1.cmml">(</mo><mfrac id="A1.Ex1.m1.1.1" xref="A1.Ex1.m1.1.1.cmml"><mrow id="A1.Ex1.m1.1.1.2" xref="A1.Ex1.m1.1.1.2.cmml"><mi mathvariant="normal" id="A1.Ex1.m1.1.1.2.2" xref="A1.Ex1.m1.1.1.2.2.cmml">#</mi><mo lspace="0.500em" rspace="0em" id="A1.Ex1.m1.1.1.2.1" xref="A1.Ex1.m1.1.1.2.1.cmml">​</mo><mi id="A1.Ex1.m1.1.1.2.3" xref="A1.Ex1.m1.1.1.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="A1.Ex1.m1.1.1.2.1a" xref="A1.Ex1.m1.1.1.2.1.cmml">​</mo><mi id="A1.Ex1.m1.1.1.2.4" xref="A1.Ex1.m1.1.1.2.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="A1.Ex1.m1.1.1.2.1b" xref="A1.Ex1.m1.1.1.2.1.cmml">​</mo><mi id="A1.Ex1.m1.1.1.2.5" xref="A1.Ex1.m1.1.1.2.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="A1.Ex1.m1.1.1.2.1c" xref="A1.Ex1.m1.1.1.2.1.cmml">​</mo><mi id="A1.Ex1.m1.1.1.2.6" xref="A1.Ex1.m1.1.1.2.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="A1.Ex1.m1.1.1.2.1d" xref="A1.Ex1.m1.1.1.2.1.cmml">​</mo><mi id="A1.Ex1.m1.1.1.2.7" xref="A1.Ex1.m1.1.1.2.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="A1.Ex1.m1.1.1.2.1e" xref="A1.Ex1.m1.1.1.2.1.cmml">​</mo><mi id="A1.Ex1.m1.1.1.2.8" xref="A1.Ex1.m1.1.1.2.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="A1.Ex1.m1.1.1.2.1f" xref="A1.Ex1.m1.1.1.2.1.cmml">​</mo><mi id="A1.Ex1.m1.1.1.2.9" xref="A1.Ex1.m1.1.1.2.9.cmml">t</mi><mo lspace="0.500em" rspace="0em" id="A1.Ex1.m1.1.1.2.1g" xref="A1.Ex1.m1.1.1.2.1.cmml">​</mo><mi id="A1.Ex1.m1.1.1.2.10" xref="A1.Ex1.m1.1.1.2.10.cmml">h</mi><mo lspace="0em" rspace="0em" id="A1.Ex1.m1.1.1.2.1h" xref="A1.Ex1.m1.1.1.2.1.cmml">​</mo><mi id="A1.Ex1.m1.1.1.2.11" xref="A1.Ex1.m1.1.1.2.11.cmml">i</mi><mo lspace="0em" rspace="0em" id="A1.Ex1.m1.1.1.2.1i" xref="A1.Ex1.m1.1.1.2.1.cmml">​</mo><mi id="A1.Ex1.m1.1.1.2.12" xref="A1.Ex1.m1.1.1.2.12.cmml">t</mi><mo lspace="0em" rspace="0em" id="A1.Ex1.m1.1.1.2.1j" xref="A1.Ex1.m1.1.1.2.1.cmml">​</mo><mi id="A1.Ex1.m1.1.1.2.13" xref="A1.Ex1.m1.1.1.2.13.cmml">s</mi></mrow><mn id="A1.Ex1.m1.1.1.3" xref="A1.Ex1.m1.1.1.3.cmml">3</mn></mfrac><mo id="A1.Ex1.m1.2.3.3.5.2.2" xref="A1.Ex1.m1.2.3.3.5.1.cmml">,</mo><mn id="A1.Ex1.m1.2.2" xref="A1.Ex1.m1.2.2.cmml">1</mn><mo stretchy="false" id="A1.Ex1.m1.2.3.3.5.2.3" xref="A1.Ex1.m1.2.3.3.5.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.Ex1.m1.2b"><apply id="A1.Ex1.m1.2.3.cmml" xref="A1.Ex1.m1.2.3"><eq id="A1.Ex1.m1.2.3.1.cmml" xref="A1.Ex1.m1.2.3.1"></eq><apply id="A1.Ex1.m1.2.3.2.cmml" xref="A1.Ex1.m1.2.3.2"><times id="A1.Ex1.m1.2.3.2.1.cmml" xref="A1.Ex1.m1.2.3.2.1"></times><ci id="A1.Ex1.m1.2.3.2.2.cmml" xref="A1.Ex1.m1.2.3.2.2">𝑎</ci><ci id="A1.Ex1.m1.2.3.2.3.cmml" xref="A1.Ex1.m1.2.3.2.3">𝑐</ci><ci id="A1.Ex1.m1.2.3.2.4.cmml" xref="A1.Ex1.m1.2.3.2.4">𝑐</ci><ci id="A1.Ex1.m1.2.3.2.5.cmml" xref="A1.Ex1.m1.2.3.2.5">𝑢</ci><ci id="A1.Ex1.m1.2.3.2.6.cmml" xref="A1.Ex1.m1.2.3.2.6">𝑟</ci><ci id="A1.Ex1.m1.2.3.2.7.cmml" xref="A1.Ex1.m1.2.3.2.7">𝑎</ci><ci id="A1.Ex1.m1.2.3.2.8.cmml" xref="A1.Ex1.m1.2.3.2.8">𝑐</ci><ci id="A1.Ex1.m1.2.3.2.9.cmml" xref="A1.Ex1.m1.2.3.2.9">𝑦</ci></apply><apply id="A1.Ex1.m1.2.3.3.cmml" xref="A1.Ex1.m1.2.3.3"><times id="A1.Ex1.m1.2.3.3.1.cmml" xref="A1.Ex1.m1.2.3.3.1"></times><ci id="A1.Ex1.m1.2.3.3.2.cmml" xref="A1.Ex1.m1.2.3.3.2">𝑚</ci><ci id="A1.Ex1.m1.2.3.3.3.cmml" xref="A1.Ex1.m1.2.3.3.3">𝑖</ci><ci id="A1.Ex1.m1.2.3.3.4.cmml" xref="A1.Ex1.m1.2.3.3.4">𝑛</ci><interval closure="open" id="A1.Ex1.m1.2.3.3.5.1.cmml" xref="A1.Ex1.m1.2.3.3.5.2"><apply id="A1.Ex1.m1.1.1.cmml" xref="A1.Ex1.m1.1.1"><divide id="A1.Ex1.m1.1.1.1.cmml" xref="A1.Ex1.m1.1.1"></divide><apply id="A1.Ex1.m1.1.1.2.cmml" xref="A1.Ex1.m1.1.1.2"><times id="A1.Ex1.m1.1.1.2.1.cmml" xref="A1.Ex1.m1.1.1.2.1"></times><ci id="A1.Ex1.m1.1.1.2.2.cmml" xref="A1.Ex1.m1.1.1.2.2">#</ci><ci id="A1.Ex1.m1.1.1.2.3.cmml" xref="A1.Ex1.m1.1.1.2.3">𝑐</ci><ci id="A1.Ex1.m1.1.1.2.4.cmml" xref="A1.Ex1.m1.1.1.2.4">𝑜</ci><ci id="A1.Ex1.m1.1.1.2.5.cmml" xref="A1.Ex1.m1.1.1.2.5">𝑟</ci><ci id="A1.Ex1.m1.1.1.2.6.cmml" xref="A1.Ex1.m1.1.1.2.6">𝑟</ci><ci id="A1.Ex1.m1.1.1.2.7.cmml" xref="A1.Ex1.m1.1.1.2.7">𝑒</ci><ci id="A1.Ex1.m1.1.1.2.8.cmml" xref="A1.Ex1.m1.1.1.2.8">𝑐</ci><ci id="A1.Ex1.m1.1.1.2.9.cmml" xref="A1.Ex1.m1.1.1.2.9">𝑡</ci><ci id="A1.Ex1.m1.1.1.2.10.cmml" xref="A1.Ex1.m1.1.1.2.10">ℎ</ci><ci id="A1.Ex1.m1.1.1.2.11.cmml" xref="A1.Ex1.m1.1.1.2.11">𝑖</ci><ci id="A1.Ex1.m1.1.1.2.12.cmml" xref="A1.Ex1.m1.1.1.2.12">𝑡</ci><ci id="A1.Ex1.m1.1.1.2.13.cmml" xref="A1.Ex1.m1.1.1.2.13">𝑠</ci></apply><cn type="integer" id="A1.Ex1.m1.1.1.3.cmml" xref="A1.Ex1.m1.1.1.3">3</cn></apply><cn type="integer" id="A1.Ex1.m1.2.2.cmml" xref="A1.Ex1.m1.2.2">1</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.Ex1.m1.2c">accuracy=min(\frac{\#\ correct\ hits}{3},1)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="A1.SS0.SSS0.Px2.p1.2" class="ltx_p">As there are ten candidate answers, # correct hits represents numbers of matched answers, which means as long as there are three or more candidates are the same with the predicted answer, the answer will be considered fully correct, and gets a score of 1.</p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Prompt for Decoder Models</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">The following is the similarity calculation prompt provided to the LLMs and ChatGPT in the experiments:</p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.1" class="ltx_p"><span id="A2.p2.1.1" class="ltx_text ltx_font_italic">Sentence similarity evaluation here refers to the task of measuring the semantic similarity score between two sentences. For example, ”what a good day” and ”how nice the weather is” are almost the same, your output shall be </span>{<span id="A2.p2.1.2" class="ltx_text ltx_font_italic">”score”:0.91</span>}<span id="A2.p2.1.3" class="ltx_text ltx_font_italic">. Now please evaluate the similarity score between the following two sentences: sentence1: sample[”sentence1”]. sentence2: sample[”sentence2”]. The score shall range continuously from 0-1. DO NOT output anything else but the .json-style dictionary, like </span>{<span id="A2.p2.1.4" class="ltx_text ltx_font_italic">”score”:x</span>}<span id="A2.p2.1.5" class="ltx_text ltx_font_italic">, where x is your predicted score.</span></p>
</div>
<div id="A2.p3" class="ltx_para">
<p id="A2.p3.1" class="ltx_p">The sample[”sentence1”] and sample[”sentence2”] indicate a pair of texts for similarity calculation. The question and answer are concatenated with the prompt ”Question: {question} Answer: {answer}” before similarity calculation, importing contextual information, just the same as other models in experiments.</p>
</div>
<div id="A2.p4" class="ltx_para">
<p id="A2.p4.1" class="ltx_p">ChatGPT prompt for converting each question-answer pair into a description:</p>
</div>
<div id="A2.p5" class="ltx_para">
<p id="A2.p5.1" class="ltx_p">Concatenate the question with the answer and form assertions. For example, Question:What kind of dog is in the photo? Answer:golden retriever. Assertion: The dog in the photo is a golden retriever. Infer for the following: Question: {question} Answer: {answer}. Please think of three different forms of naturally-sounded assertions for this question-answer pair with small disturbance but do not output them. Choose the two assertions that are closest in meaning to the original question-answer for output. Output shall be in .json style so that I can directly save them in a .txt and open by json. Do not output anything else including explanation, reasoning or instructions.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Annotation Score Distribution</h2>

<figure id="A3.F5" class="ltx_figure"><img src="/html/2408.00300/assets/pictures/histogram_vre_distribution.png" id="A3.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="314" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>The annotated scores distribution of AVE.</figcaption>
</figure>
<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">The score distribution of the dataset Assessing VQA Evaluators (AVE) is shown in Figure <a href="#A3.F5" title="Figure 5 ‣ Appendix C Annotation Score Distribution ‣ Towards Flexible Evaluation for Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. The annotation covers all scores.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Selection of Annotating Aspect</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">For the evaluation of the quality of responses, we considered adopting multiple aspects for analyzing. The text summary task <cite class="ltx_cite ltx_citemacro_citep">(Grusky et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2018</a>; Bhandari et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2020</a>; Fabbri et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite> adopts four aspects, i.e., <span id="A4.p1.1.1" class="ltx_text ltx_font_italic">relevance</span>, <span id="A4.p1.1.2" class="ltx_text ltx_font_italic">consistency</span>, <span id="A4.p1.1.3" class="ltx_text ltx_font_italic">fluency</span> and <span id="A4.p1.1.4" class="ltx_text ltx_font_italic">coherence</span> for analyzing the generated summary based on the reference text. As VQA responses are generally short, the <span id="A4.p1.1.5" class="ltx_text ltx_font_italic">fluency</span> and <span id="A4.p1.1.6" class="ltx_text ltx_font_italic">coherence</span> that measure the fluency of the text is less necessary. The aspect of <span id="A4.p1.1.7" class="ltx_text ltx_font_italic">consistency</span> measures whether the generated summary contains hallucination that generates untrue information. In the scene of VQA response evaluation, such measure corresponds to the overlap in semantics from the response towards the ground-truth answer, which is similar to <span id="A4.p1.1.8" class="ltx_text ltx_font_italic">relevance</span> that measures how well the generated text captures the key points. Therefore, we decide to use semantic similarity as the only score of annotation.</p>
</div>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Annotation Rules</h2>

<section id="A5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1. </span>Scores Annotation</h3>

<div id="A5.SS1.p1" class="ltx_para">
<p id="A5.SS1.p1.1" class="ltx_p">All of the following in this section is the annotation rules provided to the three annotators during the annotation of scores.</p>
</div>
<div id="A5.SS1.p2" class="ltx_para">
<p id="A5.SS1.p2.1" class="ltx_p">Scoring Format: Discrete integer scoring from 0 to 10 (e.g., 0, 1, 2, 3).</p>
</div>
<div id="A5.SS1.p3" class="ltx_para">
<p id="A5.SS1.p3.1" class="ltx_p">Scoring Rules:</p>
</div>
<div id="A5.SS1.p4" class="ltx_para">
<p id="A5.SS1.p4.1" class="ltx_p">Note that it is not about judging whether the response to the question is correct, but whether the response and the standard answer are semantically the same under the question (For example, even if the standard answer is obviously unreasonable, as long as the answer and response are semantically similar, a high score shall be given).</p>
</div>
<div id="A5.SS1.p5" class="ltx_para">
<p id="A5.SS1.p5.1" class="ltx_p">Semantically similar (the answer is fairly correct in meaning) but different in specific form (for example, the meaning expressed is similar but different in word choice, tense, number), score 6-10 based on the degree of semantic similarity.
Examples:</p>
</div>
<div id="A5.SS1.p6" class="ltx_para">
<p id="A5.SS1.p6.1" class="ltx_p">(1) Question: What is the last letter on the license plate?
Standard Answer: letter j
Response: j
Scoring (this is a reasonable range, just mark a specific score when actually annotating): 9-10
Reason (no need to mark, this is to help understand the rules): Under this question, the response and the standard answer are semantically the same, only the form is different. Similarly, synonyms should also be scored highly.</p>
</div>
<div id="A5.SS1.p7" class="ltx_para">
<p id="A5.SS1.p7.1" class="ltx_p">(2) Question: The young man above the swimming pool is wearing what?
Standard Answer: swimsuit
Response: trunks
Scoring: 7-8
Reason: The question asks what is being worn, and swimsuit (swimwear) includes trunks (swim shorts), so the answer is quite correct. This kind of inclusive or included relationship should be scored based on the semantic similarity of the two words. In addition, trunks as swim shorts is a less common meaning and requires more attention to the different meanings of words, not entirely based on experience.</p>
</div>
<div id="A5.SS1.p8" class="ltx_para">
<p id="A5.SS1.p8.1" class="ltx_p">(3) Question: How many trees are there?
Standard Answer: 3
Response: three
Scoring: 10
Reason: The meanings are exactly the same.</p>
</div>
<div id="A5.SS1.p9" class="ltx_para">
<p id="A5.SS1.p9.1" class="ltx_p">Semantically dissimilar, the answer is incorrect, but the answer is a possible answer for that type of question, score 1-5 based on the degree of semantic similarity.
Examples:</p>
</div>
<div id="A5.SS1.p10" class="ltx_para">
<p id="A5.SS1.p10.1" class="ltx_p">(1) Question: What color do you think the trousers the boy is wearing have?
Standard Answer: white
Response: blue
Scoring: 2-3
Reason: The question asks about color, and although the answer blue is different from the standard answer white, both are common answers under the category of color questions. Furthermore, if it’s white and black, the difference between the two is greater than between white and blue, so the scoring range should be further reduced to 1-2.</p>
</div>
<div id="A5.SS1.p11" class="ltx_para">
<p id="A5.SS1.p11.1" class="ltx_p">Semantically dissimilar, but the answer and the standard answer mean the same under the question, then score 4-7 based on the correctness.
Examples:</p>
</div>
<div id="A5.SS1.p12" class="ltx_para">
<p id="A5.SS1.p12.1" class="ltx_p">(1) Question: What is lit?
Standard Answer: cake
Response: candle
Scoring: 5-7
Reason: Although cake and candle are very different semantically, in this question, they actually mean the same thing. Therefore, one should not only look at the answer but also focus on the question.</p>
</div>
<div id="A5.SS1.p13" class="ltx_para">
<p id="A5.SS1.p13.1" class="ltx_p">For numerical type answers, score 1-8 based on how much the number in the standard answer and the number in the response differ.
Examples:</p>
</div>
<div id="A5.SS1.p14" class="ltx_para">
<p id="A5.SS1.p14.1" class="ltx_p">(1) Question: How many trees are there?
Standard Answer: 4
Response: 5
Scoring: 3-5
Reason: Although 4 and 5 are different, the difference between them is not particularly large. If the standard answer is still 4, but the response becomes 1, then the scoring should be appropriately lowered to 1-3. If the standard answer is 70, and the response is 75, then it can be considered quite correct, scoring 5-7. If the standard answer is 1 and the response is 0, then score 1-2. Judge the score based on whether the numbers are relatively close to each other.</p>
</div>
<div id="A5.SS1.p15" class="ltx_para">
<p id="A5.SS1.p15.1" class="ltx_p">(2) Question: When did this accident happen?
Standard Answer: 1945
Response: 1940
Scoring: 5-7
Reason: The two years are quite close. But if the answer becomes the 1940s (i.e., 1941-1949), it includes 1945, and the range is not particularly large, so it is quite correct, scoring 6-8.</p>
</div>
<div id="A5.SS1.p16" class="ltx_para">
<p id="A5.SS1.p16.1" class="ltx_p">For answers with significantly different meanings, score based on semantic similarity without range restrictions.
Examples:</p>
</div>
<div id="A5.SS1.p17" class="ltx_para">
<p id="A5.SS1.p17.1" class="ltx_p">(1) Question: What color bathing suit is the woman wearing?
Standard Answer: no woman
Response: red
Scoring: 0-1
Reason: The meanings are very different.</p>
</div>
</section>
<section id="A5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2. </span>Manual Filter</h3>

<div id="A5.SS2.p1" class="ltx_para">
<p id="A5.SS2.p1.1" class="ltx_p">The following is the annotation rules provided to the three annotators during the last stage, manual filter, in the construction of our AVE dataset.</p>
</div>
<div id="A5.SS2.p2" class="ltx_para">
<p id="A5.SS2.p2.1" class="ltx_p">Read the question, answer, response and all augmentation results carefully, and decide whether the augmentation has changed the original meaning. Labels shall be in yes, no, unsure.</p>
</div>
</section>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Answer Templates</h2>

<div id="A6.p1" class="ltx_para">
<p id="A6.p1.1" class="ltx_p">In the fourth step of constructing the proposed dataset, i.e., description generation, beside collecting ChatGPT-transformed results, we augment each short response with manual written templates: (1) <span id="A6.p1.1.1" class="ltx_text ltx_font_italic">Answer: </span>{<span id="A6.p1.1.2" class="ltx_text ltx_font_italic">response</span>}<span id="A6.p1.1.3" class="ltx_text ltx_font_italic">.</span>, (2) <span id="A6.p1.1.4" class="ltx_text ltx_font_italic">The answer to this question is </span>{<span id="A6.p1.1.5" class="ltx_text ltx_font_italic">response</span>}<span id="A6.p1.1.6" class="ltx_text ltx_font_italic">.</span>, (3) <span id="A6.p1.1.7" class="ltx_text ltx_font_italic">As shown in the image and question, the answer is </span>{<span id="A6.p1.1.8" class="ltx_text ltx_font_italic">response</span>}<span id="A6.p1.1.9" class="ltx_text ltx_font_italic">.</span>, (4) <span id="A6.p1.1.10" class="ltx_text ltx_font_italic">The answer you are asking for is </span>{<span id="A6.p1.1.11" class="ltx_text ltx_font_italic">response</span>}<span id="A6.p1.1.12" class="ltx_text ltx_font_italic">.</span>, (5) <span id="A6.p1.1.13" class="ltx_text ltx_font_italic">As can be deduced from the image, the answer to this question is </span>{<span id="A6.p1.1.14" class="ltx_text ltx_font_italic">response</span>}<span id="A6.p1.1.15" class="ltx_text ltx_font_italic">.</span>, (6) <span id="A6.p1.1.16" class="ltx_text ltx_font_italic">The answer to your question appears to be </span>{<span id="A6.p1.1.17" class="ltx_text ltx_font_italic">response</span>}<span id="A6.p1.1.18" class="ltx_text ltx_font_italic">, as shown in the image.</span>.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.00299" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.00300" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.00300">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.00300" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.00301" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 16:31:39 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
