<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.01603] Decentralized Federated Learning: A Survey and Perspective</title><meta property="og:description" content="Federated learning (FL) has been gaining attention for its ability to share knowledge while maintaining user data, protecting privacy, increasing learning efficiency, and reducing communication overhead. Decentralized …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Decentralized Federated Learning: A Survey and Perspective">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Decentralized Federated Learning: A Survey and Perspective">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.01603">

<!--Generated on Thu Feb 29 03:06:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated learning,  decentralized learning,  network,  privacy preservation,  internet of things (IoT).
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Decentralized Federated Learning: 
<br class="ltx_break">A Survey and Perspective</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Liangqi Yuan, ,
Lichao Sun, ,
Philip S. Yu, ,
Ziran Wang
</span><span class="ltx_author_notes">Manuscript received June 01, 2023.L. Yuan and Z. Wang are with the College of Engineering, Purdue University, West Lafayette, IN 47907, USA (e-mail: liangqiy@purdue.edu; ryanwang11@hotmail.com).L. Sun is with the Department of Computer Science and Engineering, Lehigh University, Bethlehem, PA
18015 USA (e-mail: lis221@lehigh.edu).P. S. Yu is with the Information Technology, University of Illinois at Chicago, Chicago, IL 60607, USA (e-mail: psyu@uic.edu).</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Federated learning (FL) has been gaining attention for its ability to share knowledge while maintaining user data, protecting privacy, increasing learning efficiency, and reducing communication overhead. Decentralized FL (DFL) is a decentralized network architecture that eliminates the need for a central server in contrast to centralized FL (CFL). DFL enables direct communication between clients, resulting in significant savings in communication resources. In this paper, a comprehensive survey and profound perspective is provided for DFL. First, a review of the methodology, challenges, and variants of CFL is conducted, laying the background of DFL. Then, a systematic and detailed perspective on DFL is introduced, including iteration order, communication protocols, network topologies, paradigm proposals, and temporal variability. Next, based on the definition of DFL, several extended variants and categorizations are proposed with state-of-the-art technologies. Lastly, in addition to summarizing the current challenges in the DFL, some possible solutions and future research directions are also discussed.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated learning, decentralized learning, network, privacy preservation, internet of things (IoT).

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated learning (FL) is a decentralized learning paradigm with natural privacy-preserving capabilities, which shares only model weights instead of user data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Federated learning was first proposed by Google researchers in 2016 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and was applied to build a language model collaboration framework on Google Keyboard to learn whether people clicked on recommended suggestions and contextual information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. FL has demonstrated its excellent capabilities in various areas, including intelligent transportation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, healthcare <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, manufacturing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, agriculture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, energy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, and more. FL also breaks geographical limitations allowing efficient collaboration worldwide. Researchers employed FL to aggregate data from 20 institutes worldwide to train a universal model to predict clinical outcomes of COVID-19 patients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. FL improves the generalization capability of the model to include knowledge of diverse data. Other researchers have also used FL to aggregate data from 71 sites for rare cancer boundary detection, which greatly enriches the dataset to support research on rare diseases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Traditional FL focuses on the decentralized learning and centralized aggregation paradigm established by data parallelism. Data parallelism refers to the situation where the raw data of the clients is generated in parallel locally, and this raw data is neither sent out nor visible to others. Each client trains a model based on its local data and then communicates the model parameters with the server to ensure the effective integration of learning results from each client and obtain a global model. A FL taxonomy refers to the number and nature of clients participating in the learning network, including cross-silo and cross-device FL frameworks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The clients in cross-silo FL usually are different organizations, research institutions, data centers, etc., which may have more reliable communication, computational resources, and a large amount of data. The clients in cross-device FL are huge mobile or internet of things (IoT) devices, which can encounter potential bottlenecks in communication and computation. Another FL taxonomy is considered for differences in data distribution among clients, including horizontal, vertical, and transfer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. In horizontal FL, clients have more similar sample features and fewer identical users, i.e., statistical heterogeneity. Clients in vertical FL have more similar users and fewer similar sample features, i.e., systematic heterogeneity. Federated transfer learning clients have neither many similar sample features nor similar users, i.e., clients include both statistical heterogeneity and systematic heterogeneity.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we conduct a comprehensive investigation of decentralized FL (DFL) and proposed our own perspectives on the taxonomies in DFL. Compared to the conventional centralized FL (CFL) that relies on a central server for aggregation, we specifically focus on the often overlooked DFL framework, which operates without a central server. Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the illustration of local learning, centralized learning, CFL, and DFL. In the local learning strategy, the user data and trained model of each client are only used locally, and they do not communicate with any other clients or servers, as shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(a), but this may lead to overfitting. Alternatively shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(b), the centralized learning strategy involves the transmission of raw data in the communication between clients and the server, which consolidates and centralizes the learning process, but does not guarantee the privacy of the users. Both of these strategies are often used by researchers as baselines to compare with FL.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">CFL is a centralized structure where a server will communicate, coordinate, and manage all clients. Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(c) shows the communication between clients and the server. Clients learn on local data and then upload the trained model parameters to the server. The server aggregates the local models and then shares the global model with the clients. The idea is that all clients contribute to one global model, and the one global model is applied to all clients. For CFL, clients only share the trained local model parameters with the server but not users’ raw data. FL not only protects users’ privacy and improves learning efficiency, but also saves communication resources when the model size is much smaller than the data size.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<p id="S1.F1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S1.F1.1.1" class="ltx_text"><img src="/html/2306.01603/assets/x1.png" id="S1.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="148" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of local learning, centralized learning, CFL, and DFL. (a) Clients are trained with local user data only. The clients neither share raw data nor communicate with each other. (b) After clients send the user data packets to the server, the server trains a general model using all the data. The generalized model is then shared with all clients. (c) Clients send the locally trained model parameters to the server. The server aggregates all the local models and then transmits the aggregated global model parameters to all the clients. (d) Clients share their locally trained model with other clients. Subsequent clients then continue to learn, personalize, and adapt the model locally, while also exchanging and propagating the model parameters that possess local knowledge.</figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">DFL is a decentralized structure in which clients communicate and share model parameters with each other without any server. There are relevant designations in the recent literature, such as peer-to-peer FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, server free FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, serverless FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, device-to-device FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, swarm learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, etc. Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(d) shows clients communicating directly with other clients without server coordination. Since there is no unified coordination and configuration of servers, the communication network between clients is more diverse. For the DFL discarding the server is considered to be more customizable, which can further save communication and computational resources with higher confidence in diverse variants. The pointing and peer connections in the communication network are adaptively configured and changed according to the scenario, which is one of the advantages of DFL. In addition to the typical line, ring, and fully-connected peer connection types, it is conceivable to connect based on geographical neighbors, the similarity of clients, communication protocols, etc.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The concept of DFL was first proposed in the year 2018 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. As of June 1, 2023, a search on Google Scholar yields 1,350 results related to DFL, with a substantial number of 652 contributions coming from the year 2022 alone. The research associated with DFL exhibits a persistent exponential growth trajectory. DFL has received extensive attention as an emerging framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. The most significant advantage of DFL is that it eliminates the server as an intermediate step, resulting in extreme communication resource savings. Xu <span id="S1.p6.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> listed DFL, model compression, selective client communication, and low communication frequency as four ways to reduce communication costs. Lian <span id="S1.p6.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> demonstrated the advantages of decentralized learning over centralized learning, especially since the number of clients in decentralized learning is proportional to the speedup.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Although FL has shown unprecedented advantages, most of the current research has been limited to CFL. DFL, as an essential branch in FL, is proliferating and offering benefits over CFL. Recent surveys have focused more on CFL, with less attention given to DFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Furthermore, there is a lack of a comprehensive, in-depth, and insightful survey that establishes the logic of building a DFL system, including iteration, communication protocol, network topology, paradigm, and more. This paper begins with a review of CFL, summarizing its challenges and various extended variants as potential solutions that can be compared and analogized with DFL. As an emerging field, this perspective paper addresses the current neglect of DFL in the survey literature and systematically integrates and categorizes state-of-the-art in DFL. The contributions of this paper are:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We provide a description of CFL, summarize the challenges, and offer a detailed introduction to the various variants, their roles, addressed issues, and advantages.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We systematically define and describe five taxonomies of DFL, including iteration order, communication protocol, network topology, paradigm proposal, and temporal variability. To the best of our knowledge, this is the first comprehensive and insightful perspective paper for DFL.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Based on the network topology, we propose and envision five variants of DFL to categorize the recent literature, anticipate potential application scenarios, and highlight the advantages.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We summarize five current challenges, possible solutions, and future research directions for DFL.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F2" class="ltx_figure">
<p id="S1.F2.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S1.F2.1.1" class="ltx_text"><img src="/html/2306.01603/assets/x2.png" id="S1.F2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="296" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Roadmap for this perspective paper.</figcaption>
</figure>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">The presentation of this paper is summarized as shown in Fig. <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Section <a href="#S2" title="II Centralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> reviews the history of CFL, the existing challenges, and some variants as potential solutions. Section <a href="#S3" title="III Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> provides the definitions and descriptions of DFL communication protocol, network topology, and paradigm proposal. Section <a href="#S4" title="IV Variants of Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> demonstrates several variants in DFL, followed by Section <a href="#S5" title="V Challenge and Potential Solutions in DFL ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> analyzing the challenges of DFL.
Finally, Section <a href="#S6" title="VI Conclusion ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> provides a summary of this paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Centralized Federated Learning</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">McMahan <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> proposed the first mature and most popular FL algorithm, federated averaging (FedAvg). At each communication round, clients upload their trained local models to the server, and the server weighted averages all local models according to the number of client samples. Based on FedAvg, various derivation and optimization schemes exist to address the challenges in the FL algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Li <span id="S2.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> developed an advanced algorithm FedProx to penalize the bias of the local model to the global model by a proximal term. The advantage is to limit the significant variance and unstable convergence of local models due to overfitting on clients with system heterogeneity. Wei <span id="S2.p1.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> took into account the privacy leakage concern of model parameters uploaded by clients in FL and proposed to improve the differential privacy by adding noise before the client sends it to the server for aggregation. Also, the game trade-off between FL convergence and privacy preservation and the optimal communication rounds were highlighted.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Although the diverse derivations that exist complement the performance of FL, there are undeniable drawbacks, such as a single point of failure (SPoF) on the server. In this section, after presenting some of the challenges and limitations of the server, we show some variants of the solution and state-of-the-art technologies.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Challenges in Centralized Federated Learning</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">For CFL, the server takes on many responsibilities and challenges, with large service providers, such as large organizations and research institutions, playing the role of server. While these large providers have unparalleled resources compared to small workshops, there are some concerns here as the number of clients grows endlessly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS1.4.1.1" class="ltx_text">II-A</span>1 </span>Communication Resource</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">Communication resources are limited on both the server and client sides. Although FL has dramatically reduced the consumption of communication resources by sharing only model parameters instead of user raw data, communication resources are a serious problem considering the large amount of parallel clients (up to one billion). In particular, when delays in communication cause the server to wait for clients with communication problems, it can also cause the whole FL framework to become highly inefficient. Some FL communication proposals, such as model compression, have been proposed to improve communication efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS2.4.1.1" class="ltx_text">II-A</span>2 </span>Computational and Storage Resource</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">In addition to communication, computing and storage resources on the server side are also challenged. The server needs to store and aggregate the models of these billions of clients. Even though lightweight models are emerging recently <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, the need to compute and store model data can easily reach petabytes in size <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. Besides the current version of the massive local model, sub conditionals and versioned storage of the global model may also be required.</p>
</div>
</section>
<section id="S2.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS3.4.1.1" class="ltx_text">II-A</span>3 </span>Fairness, Security, and Trust Issue</h4>

<div id="S2.SS1.SSS3.p1" class="ltx_para">
<p id="S2.SS1.SSS3.p1.1" class="ltx_p">A series of questions related to security and trust form the chain of suspicion: whether the server aggregation model is reasonable, whether the global model will have high performance across all clients, whether the global model is validated, how to use the global model securely, and whether the server is secure from attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. For security issues, there are different directions of research, including malicious attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, data poisoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, anomaly detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, and privacy protection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. For trust issues, fairness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, incentive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, and interpretability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> in FL are also worthy research directions.</p>
</div>
</section>
<section id="S2.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS4.4.1.1" class="ltx_text">II-A</span>4 </span>Single Point of Failure</h4>

<div id="S2.SS1.SSS4.p1" class="ltx_para">
<p id="S2.SS1.SSS4.p1.1" class="ltx_p">Since all clients are communicating with the server on a single line, when the server has a SPoF, the entire system update iteration will stop completely. Although the multi-server setup of some edge servers can spread the risk of SPoF, it will still cause the connected slice of this edge server to stop responding. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> tried to use blockchain technology to replace the role of the server to solve SPoF, but it is no longer part of CFL.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Variants of Centralized Federated Learning</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The network variants and extensions of CFL are designed to address the above challenges and adapt to different real-world application scenarios.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS1.4.1.1" class="ltx_text">II-B</span>1 </span>Edge Variant</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">FL in edge networks usually perform additional aggregations by setting up additional edge servers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, which aim to spread the communication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> and computing pressure and reduce the impact of SPoF. These additional edge servers are geographically closer to clients, resulting in less communication resource consumption and lower latency. After one or more edge server aggregations, the edge servers then upload the edge global model to the cloud for aggregation into a global model. In addition to communication optimizations, the geographic proximity of edge servers to clients may also lead to better adaptation of edge servers to the connected clients. Edge servers and connected clients can be considered as geographically personalized clusters. For example, by assigning edge servers to states in the United States, the state edge servers can be more personalized to the state’s user scenarios and user habits, such as weather, number of users, time zone, ethnicity, age distribution, etc.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p">Blockchain is one of the key technologies driving the edge variant, which creates a strong security barrier, resource management, and reliable access for communication in the network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. FL with blockchain aggregates the global model through a distributed shared ledger. The benefits include reduced burden and demand on the central server, higher security and confidence, and reduced communication costs. Nguyen <span id="S2.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> proposed FLchain to meet the requirements for servers and communication resources in the blockchain by mobile edge computing servers.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Definitions and Descriptions of DFL Terminologies</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S2.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Taxonomy</span></th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S2.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Category</span></th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span id="S2.T1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.3.1.1" class="ltx_p"><span id="S2.T1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Description</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<td id="S2.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_align_top ltx_border_r ltx_border_t" rowspan="4"><span id="S2.T1.1.2.1.1.1" class="ltx_text">Iteration Order</span></td>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Sequential</td>
<td id="S2.T1.1.2.1.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="S2.T1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.3.1.1" class="ltx_p">Clients are synchronized to communicate one by one in a certain order.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<td id="S2.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_r">Random</td>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S2.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.2.1.1" class="ltx_p">Clients are synchronized to communicate one by one in a random order.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<td id="S2.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r">Cycle</td>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S2.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.2.1.1" class="ltx_p">Clients are synchronized to communicate one by one in cycle.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<td id="S2.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_r">Parallel</td>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S2.T1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.2.1.1" class="ltx_p">All clients communicate asynchronously.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.6.5" class="ltx_tr">
<td id="S2.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_align_top ltx_border_r ltx_border_t" rowspan="3"><span id="S2.T1.1.6.5.1.1" class="ltx_text">Communication Protocol</span></td>
<td id="S2.T1.1.6.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Pointing</td>
<td id="S2.T1.1.6.5.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="S2.T1.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.6.5.3.1.1" class="ltx_p">Clients communicate in a specific form of one-peer-to-one-peer.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.7.6" class="ltx_tr">
<td id="S2.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_r">Gossip</td>
<td id="S2.T1.1.7.6.2" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S2.T1.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.7.6.2.1.1" class="ltx_p">Clients communicate in a random form of one-peer-to-one-peer, which may be determined by the neighborhood principle, client model version, complete randomness, etc.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.8.7" class="ltx_tr">
<td id="S2.T1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_r">Broadcast</td>
<td id="S2.T1.1.8.7.2" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S2.T1.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.8.7.2.1.1" class="ltx_p">Clients communicate in a form of one-peer-to-all-peers.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.9.8" class="ltx_tr">
<td id="S2.T1.1.9.8.1" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.1.9.8.2" class="ltx_td ltx_align_left ltx_border_r">Broadcast-gossip</td>
<td id="S2.T1.1.9.8.3" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S2.T1.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.9.8.3.1.1" class="ltx_p">Clients communicate in a form of one-peer-to-multiple-peers, which is also a combination form of Gossip and Broadcast.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.10.9" class="ltx_tr">
<td id="S2.T1.1.10.9.1" class="ltx_td ltx_align_left ltx_align_top ltx_border_r ltx_border_t" rowspan="7"><span id="S2.T1.1.10.9.1.1" class="ltx_text">Network Topology</span></td>
<td id="S2.T1.1.10.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Line</td>
<td id="S2.T1.1.10.9.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="S2.T1.1.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.10.9.3.1.1" class="ltx_p">Clients communicate in a sequential pointing form.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.11.10" class="ltx_tr">
<td id="S2.T1.1.11.10.1" class="ltx_td ltx_align_left ltx_border_r">Bus</td>
<td id="S2.T1.1.11.10.2" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S2.T1.1.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.11.10.2.1.1" class="ltx_p">Clients send the model to all clients behind them in order.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.12.11" class="ltx_tr">
<td id="S2.T1.1.12.11.1" class="ltx_td ltx_align_left ltx_border_r">Ring</td>
<td id="S2.T1.1.12.11.2" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S2.T1.1.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.12.11.2.1.1" class="ltx_p">Clients communicate in a cycle pointing form.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.13.12" class="ltx_tr">
<td id="S2.T1.1.13.12.1" class="ltx_td ltx_align_left ltx_border_r">Mesh</td>
<td id="S2.T1.1.13.12.2" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S2.T1.1.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.13.12.2.1.1" class="ltx_p">Clients communicate with all other clients.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.14.13" class="ltx_tr">
<td id="S2.T1.1.14.13.1" class="ltx_td ltx_align_left ltx_border_r">Star</td>
<td id="S2.T1.1.14.13.2" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S2.T1.1.14.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.14.13.2.1.1" class="ltx_p">Clients communicate only with the central client.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.15.14" class="ltx_tr">
<td id="S2.T1.1.15.14.1" class="ltx_td ltx_align_left ltx_border_r">Tree</td>
<td id="S2.T1.1.15.14.2" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S2.T1.1.15.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.15.14.2.1.1" class="ltx_p">Clients communicate only with their central clients.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.16.15" class="ltx_tr">
<td id="S2.T1.1.16.15.1" class="ltx_td ltx_align_left ltx_border_r">Hybrid</td>
<td id="S2.T1.1.16.15.2" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S2.T1.1.16.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.16.15.2.1.1" class="ltx_p">Combination of multiple communication topologies.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.17.16" class="ltx_tr">
<td id="S2.T1.1.17.16.1" class="ltx_td ltx_align_left ltx_align_top ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T1.1.17.16.1.1" class="ltx_text">Paradigm Proposal</span></td>
<td id="S2.T1.1.17.16.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Continual</td>
<td id="S2.T1.1.17.16.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="S2.T1.1.17.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.17.16.3.1.1" class="ltx_p">Client learns directly on the model of the previous client.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.18.17" class="ltx_tr">
<td id="S2.T1.1.18.17.1" class="ltx_td ltx_align_left ltx_border_r">Aggregate</td>
<td id="S2.T1.1.18.17.2" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S2.T1.1.18.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.18.17.2.1.1" class="ltx_p">Client first aggregates the models of past clients, and then learns on the aggregated models.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.19.18" class="ltx_tr">
<td id="S2.T1.1.19.18.1" class="ltx_td ltx_align_left ltx_align_top ltx_border_bb ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T1.1.19.18.1.1" class="ltx_text">Temporal Variability</span></td>
<td id="S2.T1.1.19.18.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Static</td>
<td id="S2.T1.1.19.18.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="S2.T1.1.19.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.19.18.3.1.1" class="ltx_p">Communication architecture will not change.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.20.19" class="ltx_tr">
<td id="S2.T1.1.20.19.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Dynamic</td>
<td id="S2.T1.1.20.19.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_bb">
<span id="S2.T1.1.20.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.20.19.2.1.1" class="ltx_p">Communication architecture may change with external factors, resource saving purpose, fairness purpose, concept drift, etc.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS2.4.1.1" class="ltx_text">II-B</span>2 </span>Personalization Variant</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">Personalized FL can be classified into two categories, i.e., global model personalization and personalized model architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. Global model personalization usually starts with a global model, and then the client personalizes this global model to fit the local user. Personalization is the behavior of the client independent of the server, such as federated transfer learning to transfer global model knowledge locally <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. The personalized model architecture changes the traditional FL architecture to develop a personalized model with user knowledge, which is the behavior of the server. A famous architecture is clustered FL that has been of interest to researchers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. The client model in the personalized FL framework is closer to the user, so it is known for its high accuracy and confidence. In particular, it is a highly effective solution for non-independent and identically distributed (non-IID) data. When the aggregated global model deviates from the user, personalization can transfer the model and adapt it to different heterogeneities.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS3.4.1.1" class="ltx_text">II-B</span>3 </span>Split Variant</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">Split FL splits the model for learning, where the server is responsible for some model layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>. The only data sent by the client to the server are the hidden representations and/or gradients in the cut layer of the model. The client not only shifts part of the learning task to the server but also does not share the user data. Compared to traditional FL, the split FL framework has similar accuracy and communication efficiency with a lower learning burden on the client side and more robust privacy protection. However, split FL is still in its early stages and has significant limitations, such as the need to consume more communication resources. Especially the presence of SPoF on the server can have even greater consequences.</p>
</div>
<div id="S2.SS2.SSS3.p2" class="ltx_para">
<p id="S2.SS2.SSS3.p2.1" class="ltx_p">Variants of CFL currently exist with exotic frameworks that may include single, multiple, sub, and master servers to optimize and target different problems. In addition to various variants, a popular approach is to assemble various variants of the FL framework to target multiple issues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Decentralized Federated Learning</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we begin by analyzing and comparing DFL and other related designations. Subsequently, we provide a well-organized, clear, and precise description of the various iterations, protocols, network topologies, paradigms, and variations in DFL, as presented in Table <a href="#S2.T1" title="Table I ‣ II-B1 Edge Variant ‣ II-B Variants of Centralized Federated Learning ‣ II Centralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. It is worth noting that the table comprises five distinct taxonomies, which may exhibit overlapping meanings as well as conflicting aspects, and can also be applied in a complementary manner. These taxonomies, representing the viewpoints of the authors, include summarizations of existing literature, extensions of understanding, and even inferences regarding potential definitions. This comprehensive approach aims to strengthen the comprehension and categorization of concepts in the field of DFL.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Iteration Order</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In general, FL requires multiple iterations to converge, and iteration order represents the order of each client in each iteration or the way client queues are formed in DFL. In CFL, clients iterate in a parallel manner, and the order in which the server receives the client models does not affect the convergence of the system. However, in DFL, the iteration order of clients will significantly affect the performance of client models, and we continue to discuss this issue in depth in Section <a href="#S3.SS4" title="III-D Paradigm Proposal ‣ III Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-D</span></span></a>. Depending on the specific usage scenario and task requirements, the client iteration order in DFL can be determined to be sequential, cyclic, random, parallel, dynamic, or other strategies. The choice of iteration order can impact the convergence and performance of the system, and it is important to consider the specific characteristics and constraints of the application when determining the appropriate order.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Communication Protocol</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">DFL is a network framework for sharing model weights based on the pointing, gossip, or broadcast protocol, with the goal of obtaining optimal models across all clients. Pointing is one of the simplest and most straightforward forms of establishing a communication relationship between two peers in a unidirectional, one-to-one, and specified form. The algorithms of gossip and broadcast have been well established for use in networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>. Gossip protocol is essentially a random one-peer-to-one-peer way for clients to share, disseminate, and learn knowledge in a stochastic communication method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. It is a standard communication protocol in DFL and is already in its infancy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>. The broadcast protocol is a one-peer-to-all-peers approach that allows the client to broadcast its model to all clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Hybrid protocols are now more popular, with different gossip, broadcast, and their combined communication structures designed for different scenarios and constraints. Aysal <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> proposed a method that combines gossip and broadcast protocols and can be considered as a one-peer-to-neighbor-peers approach, where the client first broadcasts to its neighbors before gossiping. Bellet <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> introduced an algorithm that operates the agent asynchronously and performs broadcast communication between similar clients with a focus on obtaining personalized local models.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Network Topology</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">DFL networks can be considered inspired by the network topology. Nedić <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> summarized and emphasized 11 network topologies, such as grid, star, fully connected network topologies, etc., and their convergence proofs. Due to the loss of server adaptation, management, and propagation constraints, DFL networks show their diversity, as shown in Fig <a href="#S3.F3" title="Figure 3 ‣ III-C Network Topology ‣ III Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Note that the line segment only means that the client is connected and can be unidirectional or bidirectional. The content transmitted does not necessarily have to be the model of the client but can also include the model of the past client. The computational content of the client can also include local learning and aggregation.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<p id="S3.F3.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S3.F3.1.1" class="ltx_text"><img src="/html/2306.01603/assets/x3.png" id="S3.F3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="240" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Illustration of communication network topology.</figcaption>
</figure>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Considering the objective driving role of the communication protocols in DFL, we have an undeniable doubt for DFL, <span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">how can knowledge dissemination be made more efficient?</span> Using Fig. <a href="#S3.F3" title="Figure 3 ‣ III-C Network Topology ‣ III Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(a) as an example, we briefly illustrate the flow of a sequential pointing line DFL in the following:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">Step 1)</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Client 1 learns from the initial model based on its local knowledge.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">Step 2)</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Client 1 sends the locally trained model, called Model 1, to Client 2.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">Step 3)</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Client 2 continual learning on Model 1, called Model 2.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">Step 4)</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Client 2 transmits two types of content to Client 3:</p>
<ol id="S3.I1.i4.I1" class="ltx_enumerate">
<li id="S3.I1.i4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">Opt. a)</span> 
<div id="S3.I1.i4.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i4.I1.i1.p1.1" class="ltx_p">Only Model 2.</p>
</div>
</li>
<li id="S3.I1.i4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">Opt. b)</span> 
<div id="S3.I1.i4.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i4.I1.i2.p1.1" class="ltx_p">Including Model 1 and Model 2.</p>
</div>
</li>
</ol>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">Step 5)</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">Client 3 has two ways to get Model 3:</p>
<ol id="S3.I1.i5.I1" class="ltx_enumerate">
<li id="S3.I1.i5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">Opt. a)</span> 
<div id="S3.I1.i5.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i5.I1.i1.p1.1" class="ltx_p">Client 3 uses Model 2 for continual learning to obtain Model 3.</p>
</div>
</li>
<li id="S3.I1.i5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">Opt. b)</span> 
<div id="S3.I1.i5.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i5.I1.i2.p1.2" class="ltx_p">Client 3 aggregates Model 1 and Model 2 to obtain Model 3<sup id="S3.I1.i5.I1.i2.p1.2.1" class="ltx_sup"><span id="S3.I1.i5.I1.i2.p1.2.1.1" class="ltx_text ltx_font_italic">′</span></sup>, and then uses Model 3<sup id="S3.I1.i5.I1.i2.p1.2.2" class="ltx_sup"><span id="S3.I1.i5.I1.i2.p1.2.2.1" class="ltx_text ltx_font_italic">′</span></sup> for local learning to obtain Model 3.</p>
</div>
</li>
</ol>
</div>
</li>
<li id="S3.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">Step 6)</span> 
<div id="S3.I1.i6.p1" class="ltx_para">
<p id="S3.I1.i6.p1.1" class="ltx_p">Continue this process until the last client.</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">It is worth noting that while the sequential pointing line form of DFL has been established, there are several variations to consider, including different transmission and aggregation options. For CFL, aggregation is the fastest and most efficient way to integrate all client knowledge since all client knowledge will be centralized in the server. However, for DFL, the situation is much more complex. First, the network topology is diverse. There are diverse network topologies in DFL. At each communication round, clients may obey different protocols to transmit models to one or more other clients. Second, there are different versions of the model. Except for the synchronous DFL, there must be different versions of the model for other DFLs. The subsequent clients in the learning process will have models that incorporate more knowledge compared to the previous clients. Third, acquiring all client knowledge becomes more challenging. Without a centralized server for collaborative management, future clients face difficulties in accessing the knowledge of all previous clients, except for the immediate preceding clients, such as Fig. <a href="#S3.F3" title="Figure 3 ‣ III-C Network Topology ‣ III Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(a). Therefore, there is an urgent need for an alternative paradigm to complement and expand the FL landscape that is not well compatible with aggregation.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Paradigms of DFL</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<th id="S3.T2.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r ltx_border_tt">
<span id="S3.T2.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.1.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T2.1.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Paradigm</span></span>
</span>
</th>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt">
<span id="S3.T2.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.2.1.1" class="ltx_p"><span id="S3.T2.1.2.1.2.1.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">Continual</span></span>
</span>
</td>
<td id="S3.T2.1.2.1.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_tt">
<span id="S3.T2.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.3.1.1" class="ltx_p"><span id="S3.T2.1.2.1.3.1.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">Aggregate</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.1.3.2" class="ltx_tr">
<th id="S3.T2.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S3.T2.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.2.1.1.1" class="ltx_p" style="width:50.0pt;">Intrinsic</span>
</span>
</th>
<td id="S3.T2.1.3.2.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T2.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.2.2.1.1" class="ltx_p">Client receives a single model per iteration.</span>
<span id="S3.T2.1.3.2.2.1.2" class="ltx_p">Learning is performed on the received model without aggregation.</span>
</span>
</td>
<td id="S3.T2.1.3.2.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="S3.T2.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.2.3.1.1" class="ltx_p">Client receives multiple models per iteration.</span>
<span id="S3.T2.1.3.2.3.1.2" class="ltx_p">Learning is performed locally after aggregating the previous models.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.4.3" class="ltx_tr">
<th id="S3.T2.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S3.T2.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.3.1.1.1" class="ltx_p" style="width:50.0pt;">Algorithm</span>
</span>
</th>
<td id="S3.T2.1.4.3.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S3.T2.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.3.2.1.1" class="ltx_p"><span id="S3.T2.1.4.3.2.1.1.1" class="ltx_text ltx_font_bold">For</span> each client <span id="S3.T2.1.4.3.2.1.1.2" class="ltx_text ltx_font_bold">until convergence do:</span></span>
<span id="S3.I2" class="ltx_enumerate">
<span id="S3.I2.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span> 
<span id="S3.I2.i1.p1" class="ltx_para">
<span id="S3.I2.i1.p1.1" class="ltx_p">Receive the model from the previous client.</span>
</span></span>
<span id="S3.I2.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span> 
<span id="S3.I2.i2.p1" class="ltx_para">
<span id="S3.I2.i2.p1.1" class="ltx_p">Perform local learning on the model.</span>
</span></span>
<span id="S3.I2.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3.</span> 
<span id="S3.I2.i3.p1" class="ltx_para">
<span id="S3.I2.i3.p1.1" class="ltx_p">Transmit the trained model to the next client.</span>
</span></span>
</span>
</span>
</td>
<td id="S3.T2.1.4.3.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="S3.T2.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.3.3.1.1" class="ltx_p"><span id="S3.T2.1.4.3.3.1.1.1" class="ltx_text ltx_font_bold">For</span> each client <span id="S3.T2.1.4.3.3.1.1.2" class="ltx_text ltx_font_bold">until convergence do:</span></span>
<span id="S3.I3" class="ltx_enumerate">
<span id="S3.I3.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span> 
<span id="S3.I3.i1.p1" class="ltx_para">
<span id="S3.I3.i1.p1.1" class="ltx_p">Receive all other client models from the previous client (pointing and gossip) or other clients (broadcast and broadcast-gossip).</span>
</span></span>
<span id="S3.I3.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span> 
<span id="S3.I3.i2.p1" class="ltx_para">
<span id="S3.I3.i2.p1.1" class="ltx_p">Aggregate all received models and perform local learning on the aggregated model.</span>
</span></span>
<span id="S3.I3.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3.</span> 
<span id="S3.I3.i3.p1" class="ltx_para">
<span id="S3.I3.i3.p1.1" class="ltx_p">Transmit the trained model and other client models to the next client (pointing and gossip) or transmit the trained model to all clients (broadcast and broadcast-gossip).</span>
</span></span>
</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.5.4" class="ltx_tr">
<th id="S3.T2.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2">
<span id="S3.T2.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.4.1.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T2.1.5.4.1.1.1.1" class="ltx_text">Advantage</span></span>
</span>
</th>
<td id="S3.T2.1.5.4.2" class="ltx_td ltx_align_justify ltx_border_t" colspan="2">
<span id="S3.T2.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.4.2.1.1" class="ltx_p" style="width:450.0pt;">
<span id="S3.T2.1.5.4.2.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S3.I4" class="ltx_itemize">
<span id="S3.I4.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S3.I4.i1.p1" class="ltx_para">
<span id="S3.I4.i1.p1.1" class="ltx_p">Each client involved in learning has a highly accurate, personalized, high-confidence local model.</span>
</span></span>
<span id="S3.I4.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S3.I4.i2.p1" class="ltx_para">
<span id="S3.I4.i2.p1.1" class="ltx_p">Compared to CFL, they do not have the same set of issues on the server side, such as aggregation fairness.</span>
</span></span>
</span>
</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.1.6.5" class="ltx_tr">
<td id="S3.T2.1.6.5.1" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S3.T2.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.5.1.1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S3.I5" class="ltx_itemize">
<span id="S3.I5.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S3.I5.i1.p1" class="ltx_para">
<span id="S3.I5.i1.p1.1" class="ltx_p">Fewer communication, computation, and storage resources are required.</span>
</span></span>
<span id="S3.I5.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S3.I5.i2.p1" class="ltx_para">
<span id="S3.I5.i2.p1.1" class="ltx_p">More simple and straightforward, suitable for all scenarios.</span>
</span></span>
</span>
</span>
</span>
</td>
<td id="S3.T2.1.6.5.2" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S3.T2.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.5.2.1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S3.I6" class="ltx_itemize">
<span id="S3.I6.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S3.I6.i1.p1" class="ltx_para">
<span id="S3.I6.i1.p1.1" class="ltx_p">More powerful generalization ability on the obtained model.</span>
</span></span>
<span id="S3.I6.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S3.I6.i2.p1" class="ltx_para">
<span id="S3.I6.i2.p1.1" class="ltx_p">Stronger ability to update new knowledge generated by the client.</span>
</span></span>
</span>
</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.7.6" class="ltx_tr">
<th id="S3.T2.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2">
<span id="S3.T2.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.7.6.1.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T2.1.7.6.1.1.1.1" class="ltx_text">Challenge</span></span>
</span>
</th>
<td id="S3.T2.1.7.6.2" class="ltx_td ltx_align_justify ltx_border_t" colspan="2">
<span id="S3.T2.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.7.6.2.1.1" class="ltx_p" style="width:450.0pt;">
<span id="S3.T2.1.7.6.2.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S3.I7" class="ltx_itemize">
<span id="S3.I7.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S3.I7.i1.p1" class="ltx_para">
<span id="S3.I7.i1.p1.1" class="ltx_p">Model performance strongly depends on the client iteration order.</span>
</span></span>
<span id="S3.I7.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S3.I7.i2.p1" class="ltx_para">
<span id="S3.I7.i2.p1.1" class="ltx_p">Appropriate loss function, learning rate, and training epoch, which allows the model to learn the current client’s knowledge while ensuring that the previous knowledge is not forgotten.</span>
</span></span>
</span>
</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.1.8.7" class="ltx_tr">
<td id="S3.T2.1.8.7.1" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S3.T2.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.8.7.1.1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S3.I8" class="ltx_itemize">
<span id="S3.I8.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S3.I8.i1.p1" class="ltx_para">
<span id="S3.I8.i1.p1.1" class="ltx_p">Catastrophic forgetting of past client knowledge.</span>
</span></span>
</span>
</span>
</span>
</td>
<td id="S3.T2.1.8.7.2" class="ltx_td ltx_nopad_r ltx_align_justify">
<span id="S3.T2.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.8.7.2.1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S3.I9" class="ltx_itemize">
<span id="S3.I9.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S3.I9.i1.p1" class="ltx_para">
<span id="S3.I9.i1.p1.1" class="ltx_p">Repetition and overemphasis on learning from past clients.</span>
</span></span>
</span>
</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1" class="ltx_tr">
<th id="S3.T2.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.2.1.1" class="ltx_p" style="width:50.0pt;">Network Topology (Take sequential pointing line DFL architecture as an example)</span>
</span>
</th>
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" colspan="2">
<span id="S3.T2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.1.1" class="ltx_p" style="width:450.0pt;">
<span id="S3.T2.1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;"><img src="/html/2306.01603/assets/x4.png" id="S3.T2.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="145" alt="[Uncaptioned image]">
</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Paradigm Proposal</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">We introduce an innovative taxonomy of DFL into two paradigms: <span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_typewriter">Continual</span> and <span id="S3.SS4.p1.1.2" class="ltx_text ltx_font_typewriter">Aggregate</span>. The main differences between these two paradigms lie in the number of model updates exchanged between clients and whether aggregation takes place. The distinction between the paradigms also entails variations in other settings, such as learning rates. The <span id="S3.SS4.p1.1.3" class="ltx_text ltx_font_typewriter">Aggregate</span> paradigm represents the archetypal FL algorithm, where each client receives the model from other clients, aggregates these models, and subsequently conducts local learning. Conversely, within the <span id="S3.SS4.p1.1.4" class="ltx_text ltx_font_typewriter">Continual</span> paradigm, each client receives the model from merely one peer client and proceeds to learn directly based upon this particular model. Continual learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, or be called incremental learning, provides a solid and grounded theory for <span id="S3.SS4.p1.1.5" class="ltx_text ltx_font_typewriter">Continual</span>. A number of concepts and algorithms for federated continual learning are mentioned in the recent literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>, which consider the process of dynamic data collection in the real world while addressing the issues of non-IID data, concept drift, and catastrophic forgetting. In DFL, the role of continual learning is more extensive:</p>
<ul id="S3.I10" class="ltx_itemize">
<li id="S3.I10.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I10.i1.p1" class="ltx_para">
<p id="S3.I10.i1.p1.1" class="ltx_p">The subsequent client will directly learn on the model of the previous client. Compared to local learning and <span id="S3.I10.i1.p1.1.1" class="ltx_text ltx_font_typewriter">Aggregate</span>, the client is able to obtain a more personalized model while saving communication, computational, and storage resources.</p>
</div>
</li>
<li id="S3.I10.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I10.i2.p1" class="ltx_para">
<p id="S3.I10.i2.p1.1" class="ltx_p">In storage-constrained frameworks, clients do not need to retain any additional model parameter data.</p>
</div>
</li>
<li id="S3.I10.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I10.i3.p1" class="ltx_para">
<p id="S3.I10.i3.p1.1" class="ltx_p">In computation-constrained frameworks, clients also do not need to consume additional resources for aggregation calculations.</p>
</div>
</li>
<li id="S3.I10.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I10.i4.p1" class="ltx_para">
<p id="S3.I10.i4.p1.1" class="ltx_p">The continuous generation of new data by clients is accommodated, and they do not need to wait for all data to be collected before starting the local learning process.</p>
</div>
</li>
<li id="S3.I10.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I10.i5.p1" class="ltx_para">
<p id="S3.I10.i5.p1.1" class="ltx_p">For tasks that may have concept drift, clients are always provided with the latest version of the model.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Table <a href="#S3.T2" title="Table II ‣ III-C Network Topology ‣ III Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> analyzes and summarizes the intrinsic, algorithm, advantage, challenge, and network topology of these two paradigms. The difference between these two paradigms is illustrated by the example of sequential pointing line DFL. In the <span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_typewriter">Continual</span> paradigm, the only content delivered to the subsequent client is the trained model. The subsequent client just continue learning on this model, as shown in Table <a href="#S3.T2" title="Table II ‣ III-C Network Topology ‣ III Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>(a). In the <span id="S3.SS4.p2.1.2" class="ltx_text ltx_font_typewriter">Aggregate</span> paradigm, the previous client transmits not only the trained local model but also all the previous models. The learning process performed by the subsequent client is divided into two parts, first aggregation and then learning, as shown in Table <a href="#S3.T2" title="Table II ‣ III-C Network Topology ‣ III Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>(b).</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">In order to compare and illustrate the difference between the <span id="S3.SS4.p3.1.1" class="ltx_text ltx_font_typewriter">Continual</span> and <span id="S3.SS4.p3.1.2" class="ltx_text ltx_font_typewriter">Aggregate</span> paradigms, pointing, gossip, and broadcast, and different network topologies. Algorithm <a href="#alg1" title="Algorithm 1 ‣ III-D Paradigm Proposal ‣ III Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows two paradigms in the sequential pointing line DFL topology and Algorithm <a href="#alg2" title="Algorithm 2 ‣ III-E Temporal Variability ‣ III Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows pointing ring <span id="S3.SS4.p3.1.3" class="ltx_text ltx_font_typewriter">Continual</span> and broadcast mesh <span id="S3.SS4.p3.1.4" class="ltx_text ltx_font_typewriter">Aggregate</span> DFL. The difference between the <span id="S3.SS4.p3.1.5" class="ltx_text ltx_font_typewriter">Continual</span> and <span id="S3.SS4.p3.1.6" class="ltx_text ltx_font_typewriter">Aggregate</span> paradigms can be clearly seen in the pre-processing of the client before learning and the sharing of the model after learning. The additional requirements of the <span id="S3.SS4.p3.1.7" class="ltx_text ltx_font_typewriter">Aggregate</span> paradigm for communication, computation, and storage have been highlighted. Under the <span id="S3.SS4.p3.1.8" class="ltx_text ltx_font_typewriter">Aggregate</span> paradigm, the pointing and gossip protocols require the client to send more model data at once, while the broadcast and broadcast-gossip protocols require the client to send at a higher frequency. The ring topology can be seen as a cyclic variant of the line topology, and both network topologies are widely used by researchers due to their simple and straightforward structure. The line topology is a sufficient knowledge learning system for systems that do not generate new knowledge. However, in a system that is constantly generating new knowledge, the ring topology may be a more reasonable topology. It is not only able to re-update the knowledge in the system but also a feasible solution to catastrophic forgetting.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">To further illustrate the learning and communication process among clients in these two paradigms, Fig. <a href="#S4.F4" title="Figure 4 ‣ IV Variants of Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> demonstrates the learning process from the first client to the final client in the parameter space. It is important to note that we actually have several assumptions here. Firstly, the optimal solutions of the local models of all clients follow a multivariate normal distribution in the parameter space. Secondly, considering the systemic and statistical heterogeneity, some clients exhibit significant biases. Thirdly, although reasonable loss functions and learning rates are chosen, the models are not always trained to achieve the optimal solutions. The communication and learning processes of the two paradigms, <span id="S3.SS4.p4.1.1" class="ltx_text ltx_font_typewriter">Continual</span> and <span id="S3.SS4.p4.1.2" class="ltx_text ltx_font_typewriter">Aggregate</span>, are as follows.</p>
<ol id="S3.I11" class="ltx_enumerate">
<li id="S3.I11.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">Step 1)</span> 
<div id="S3.I11.i1.p1" class="ltx_para">
<p id="S3.I11.i1.p1.1" class="ltx_p">Both paradigms initiate learning with the same initial model parameters and obtain the same Model 1 in Client 1.</p>
</div>
</li>
<li id="S3.I11.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">Step 2)</span> 
<div id="S3.I11.i2.p1" class="ltx_para">
<p id="S3.I11.i2.p1.1" class="ltx_p">Both paradigms learn from Model 1 and reach the same Model 2 in Client 2. It’s worth noting that the <span id="S3.I11.i2.p1.1.1" class="ltx_text ltx_font_typewriter">Aggregate</span> paradigm is meaningful when there are two or more aggregated models available.</p>
</div>
</li>
<li id="S3.I11.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">Step 3)</span> 
<div id="S3.I11.i3.p1" class="ltx_para">
<p id="S3.I11.i3.p1.1" class="ltx_p">In the <span id="S3.I11.i3.p1.1.1" class="ltx_text ltx_font_typewriter">Continual</span> paradigm, Client 3 learns directly from Model 2 to obtain Model 3, while in the <span id="S3.I11.i3.p1.1.2" class="ltx_text ltx_font_typewriter">Aggregate</span> paradigm, Model 1 and Model 2 are first aggregated, and then Client 3 learns from the aggregated model to obtain Model 3. Note that the <span id="S3.I11.i3.p1.1.3" class="ltx_text ltx_font_typewriter">Continual</span> paradigm is less complex than the <span id="S3.I11.i3.p1.1.4" class="ltx_text ltx_font_typewriter">Aggregate</span> paradigm, as indicated by the length of the black arrow.</p>
</div>
</li>
<li id="S3.I11.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">Step 4)</span> 
<div id="S3.I11.i4.p1" class="ltx_para">
<p id="S3.I11.i4.p1.1" class="ltx_p">In the <span id="S3.I11.i4.p1.1.1" class="ltx_text ltx_font_typewriter">Aggregate</span> paradigm, the model aggregated by Client 4 is closer to the center of the Normal distribution than Client 3, so it is expected that the learning process for subsequent clients will be easier.</p>
</div>
</li>
<li id="S3.I11.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">Step <math id="S3.I11.ix1.1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.I11.ix1.1.1.m1.1b"><mi id="S3.I11.ix1.1.1.m1.1.1" xref="S3.I11.ix1.1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.I11.ix1.1.1.m1.1c"><ci id="S3.I11.ix1.1.1.m1.1.1.cmml" xref="S3.I11.ix1.1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I11.ix1.1.1.m1.1d">n</annotation></semantics></math>)</span> 
<div id="S3.I11.ix1.p1" class="ltx_para">
<p id="S3.I11.ix1.p1.1" class="ltx_p">When the client is positioned towards the end of the queue, the learning difficulty in the <span id="S3.I11.ix1.p1.1.1" class="ltx_text ltx_font_typewriter">Continual</span> paradigm becomes random, depending on the deviation between the previous client and the current client. However, in the <span id="S3.I11.ix1.p1.1.2" class="ltx_text ltx_font_typewriter">Aggregate</span> paradigm, the learning difficulty is only influenced by the current client since the aggregated model is expected to be extremely close to the center of the normal distribution.</p>
</div>
</li>
</ol>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_float"><span id="alg1.8.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Sequential <span id="alg1.9.2" class="ltx_text" style="background-color:#BDFF3D;">pointing line <span id="alg1.9.2.1" class="ltx_text ltx_font_typewriter">Continual</span></span> and <span id="alg1.10.3" class="ltx_text" style="background-color:#E069FF;">pointing line <span id="alg1.10.3.1" class="ltx_text ltx_font_typewriter">Aggregate</span></span> decentralized federated learning.</figcaption>
<div id="alg1.11" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span id="alg1.l1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Input:</span><span id="alg1.l1.2" class="ltx_text" style="font-size:90%;"> Client set (</span><math id="alg1.l1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="alg1.l1.m1.1a"><mi mathsize="90%" id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">C</annotation></semantics></math><span id="alg1.l1.3" class="ltx_text" style="font-size:90%;">), training epoch (</span><math id="alg1.l1.m2.1" class="ltx_Math" alttext="E" display="inline"><semantics id="alg1.l1.m2.1a"><mi mathsize="90%" id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><ci id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">E</annotation></semantics></math><span id="alg1.l1.4" class="ltx_text" style="font-size:90%;">), initial model (</span><math id="alg1.l1.m3.1" class="ltx_Math" alttext="\omega_{0}" display="inline"><semantics id="alg1.l1.m3.1a"><msub id="alg1.l1.m3.1.1" xref="alg1.l1.m3.1.1.cmml"><mi mathsize="90%" id="alg1.l1.m3.1.1.2" xref="alg1.l1.m3.1.1.2.cmml">ω</mi><mn mathsize="90%" id="alg1.l1.m3.1.1.3" xref="alg1.l1.m3.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="alg1.l1.m3.1b"><apply id="alg1.l1.m3.1.1.cmml" xref="alg1.l1.m3.1.1"><csymbol cd="ambiguous" id="alg1.l1.m3.1.1.1.cmml" xref="alg1.l1.m3.1.1">subscript</csymbol><ci id="alg1.l1.m3.1.1.2.cmml" xref="alg1.l1.m3.1.1.2">𝜔</ci><cn type="integer" id="alg1.l1.m3.1.1.3.cmml" xref="alg1.l1.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m3.1c">\omega_{0}</annotation></semantics></math><span id="alg1.l1.5" class="ltx_text" style="font-size:90%;">), loss function (</span><math id="alg1.l1.m4.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="alg1.l1.m4.1a"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="alg1.l1.m4.1.1" xref="alg1.l1.m4.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m4.1b"><ci id="alg1.l1.m4.1.1.cmml" xref="alg1.l1.m4.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m4.1c">\mathcal{L}</annotation></semantics></math><span id="alg1.l1.6" class="ltx_text" style="font-size:90%;">), learning rate (</span><math id="alg1.l1.m5.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="alg1.l1.m5.1a"><mi mathsize="90%" id="alg1.l1.m5.1.1" xref="alg1.l1.m5.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m5.1b"><ci id="alg1.l1.m5.1.1.cmml" xref="alg1.l1.m5.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m5.1c">\eta</annotation></semantics></math><span id="alg1.l1.7" class="ltx_text" style="font-size:90%;">)
</span>
</div>
<div id="alg1.l2" class="ltx_listingline">
<span id="alg1.l2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Output:</span><span id="alg1.l2.2" class="ltx_text" style="font-size:90%;"> Local models (</span><math id="alg1.l2.m1.2" class="ltx_Math" alttext="\{\omega_{c}|c\in C\}" display="inline"><semantics id="alg1.l2.m1.2a"><mrow id="alg1.l2.m1.2.2.2" xref="alg1.l2.m1.2.2.3.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l2.m1.2.2.2.3" xref="alg1.l2.m1.2.2.3.1.cmml">{</mo><msub id="alg1.l2.m1.1.1.1.1" xref="alg1.l2.m1.1.1.1.1.cmml"><mi mathsize="90%" id="alg1.l2.m1.1.1.1.1.2" xref="alg1.l2.m1.1.1.1.1.2.cmml">ω</mi><mi mathsize="90%" id="alg1.l2.m1.1.1.1.1.3" xref="alg1.l2.m1.1.1.1.1.3.cmml">c</mi></msub><mo lspace="0em" mathsize="90%" rspace="0em" id="alg1.l2.m1.2.2.2.4" xref="alg1.l2.m1.2.2.3.1.cmml">|</mo><mrow id="alg1.l2.m1.2.2.2.2" xref="alg1.l2.m1.2.2.2.2.cmml"><mi mathsize="90%" id="alg1.l2.m1.2.2.2.2.2" xref="alg1.l2.m1.2.2.2.2.2.cmml">c</mi><mo mathsize="90%" id="alg1.l2.m1.2.2.2.2.1" xref="alg1.l2.m1.2.2.2.2.1.cmml">∈</mo><mi mathsize="90%" id="alg1.l2.m1.2.2.2.2.3" xref="alg1.l2.m1.2.2.2.2.3.cmml">C</mi></mrow><mo maxsize="90%" minsize="90%" id="alg1.l2.m1.2.2.2.5" xref="alg1.l2.m1.2.2.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.2b"><apply id="alg1.l2.m1.2.2.3.cmml" xref="alg1.l2.m1.2.2.2"><csymbol cd="latexml" id="alg1.l2.m1.2.2.3.1.cmml" xref="alg1.l2.m1.2.2.2.3">conditional-set</csymbol><apply id="alg1.l2.m1.1.1.1.1.cmml" xref="alg1.l2.m1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l2.m1.1.1.1.1.1.cmml" xref="alg1.l2.m1.1.1.1.1">subscript</csymbol><ci id="alg1.l2.m1.1.1.1.1.2.cmml" xref="alg1.l2.m1.1.1.1.1.2">𝜔</ci><ci id="alg1.l2.m1.1.1.1.1.3.cmml" xref="alg1.l2.m1.1.1.1.1.3">𝑐</ci></apply><apply id="alg1.l2.m1.2.2.2.2.cmml" xref="alg1.l2.m1.2.2.2.2"><in id="alg1.l2.m1.2.2.2.2.1.cmml" xref="alg1.l2.m1.2.2.2.2.1"></in><ci id="alg1.l2.m1.2.2.2.2.2.cmml" xref="alg1.l2.m1.2.2.2.2.2">𝑐</ci><ci id="alg1.l2.m1.2.2.2.2.3.cmml" xref="alg1.l2.m1.2.2.2.2.3">𝐶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.2c">\{\omega_{c}|c\in C\}</annotation></semantics></math><span id="alg1.l2.3" class="ltx_text" style="font-size:90%;">)
</span>
</div>
<div id="alg1.l3" class="ltx_listingline">
<span id="alg1.l3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span id="alg1.l3.2" class="ltx_text" style="font-size:90%;"> </span><math id="alg1.l3.m1.1" class="ltx_Math" alttext="c\in C" display="inline"><semantics id="alg1.l3.m1.1a"><mrow id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><mi mathsize="90%" id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2.cmml">c</mi><mo mathsize="90%" id="alg1.l3.m1.1.1.1" xref="alg1.l3.m1.1.1.1.cmml">∈</mo><mi mathsize="90%" id="alg1.l3.m1.1.1.3" xref="alg1.l3.m1.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><in id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1.1"></in><ci id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2">𝑐</ci><ci id="alg1.l3.m1.1.1.3.cmml" xref="alg1.l3.m1.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">c\in C</annotation></semantics></math><span id="alg1.l3.3" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l3.4" class="ltx_text ltx_font_bold" style="font-size:90%;">in sequence</span><span id="alg1.l3.5" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l3.6" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span><span id="alg1.l3.7" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l4" class="ltx_listingline">
<span id="alg1.l4.2" class="ltx_text" style="font-size:90%;">    </span><span id="alg1.l4.1" class="ltx_text" style="font-size:90%;background-color:#BDFF3D;">Copy the model from previous client <math id="alg1.l4.1.1.m1.1" class="ltx_Math" alttext="\omega_{c}\leftarrow\omega_{c-1}" display="inline"><semantics id="alg1.l4.1.1.m1.1a"><mrow id="alg1.l4.1.1.m1.1.1" xref="alg1.l4.1.1.m1.1.1.cmml"><msub id="alg1.l4.1.1.m1.1.1.2" xref="alg1.l4.1.1.m1.1.1.2.cmml"><mi mathbackground="#BDFF3D" id="alg1.l4.1.1.m1.1.1.2.2" xref="alg1.l4.1.1.m1.1.1.2.2.cmml">ω</mi><mi mathbackground="#BDFF3D" id="alg1.l4.1.1.m1.1.1.2.3" xref="alg1.l4.1.1.m1.1.1.2.3.cmml">c</mi></msub><mo mathbackground="#BDFF3D" stretchy="false" id="alg1.l4.1.1.m1.1.1.1" xref="alg1.l4.1.1.m1.1.1.1.cmml">←</mo><msub id="alg1.l4.1.1.m1.1.1.3" xref="alg1.l4.1.1.m1.1.1.3.cmml"><mi mathbackground="#BDFF3D" id="alg1.l4.1.1.m1.1.1.3.2" xref="alg1.l4.1.1.m1.1.1.3.2.cmml">ω</mi><mrow id="alg1.l4.1.1.m1.1.1.3.3" xref="alg1.l4.1.1.m1.1.1.3.3.cmml"><mi mathbackground="#BDFF3D" id="alg1.l4.1.1.m1.1.1.3.3.2" xref="alg1.l4.1.1.m1.1.1.3.3.2.cmml">c</mi><mo mathbackground="#BDFF3D" id="alg1.l4.1.1.m1.1.1.3.3.1" xref="alg1.l4.1.1.m1.1.1.3.3.1.cmml">−</mo><mn mathbackground="#BDFF3D" id="alg1.l4.1.1.m1.1.1.3.3.3" xref="alg1.l4.1.1.m1.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.1.1.m1.1b"><apply id="alg1.l4.1.1.m1.1.1.cmml" xref="alg1.l4.1.1.m1.1.1"><ci id="alg1.l4.1.1.m1.1.1.1.cmml" xref="alg1.l4.1.1.m1.1.1.1">←</ci><apply id="alg1.l4.1.1.m1.1.1.2.cmml" xref="alg1.l4.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l4.1.1.m1.1.1.2.1.cmml" xref="alg1.l4.1.1.m1.1.1.2">subscript</csymbol><ci id="alg1.l4.1.1.m1.1.1.2.2.cmml" xref="alg1.l4.1.1.m1.1.1.2.2">𝜔</ci><ci id="alg1.l4.1.1.m1.1.1.2.3.cmml" xref="alg1.l4.1.1.m1.1.1.2.3">𝑐</ci></apply><apply id="alg1.l4.1.1.m1.1.1.3.cmml" xref="alg1.l4.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="alg1.l4.1.1.m1.1.1.3.1.cmml" xref="alg1.l4.1.1.m1.1.1.3">subscript</csymbol><ci id="alg1.l4.1.1.m1.1.1.3.2.cmml" xref="alg1.l4.1.1.m1.1.1.3.2">𝜔</ci><apply id="alg1.l4.1.1.m1.1.1.3.3.cmml" xref="alg1.l4.1.1.m1.1.1.3.3"><minus id="alg1.l4.1.1.m1.1.1.3.3.1.cmml" xref="alg1.l4.1.1.m1.1.1.3.3.1"></minus><ci id="alg1.l4.1.1.m1.1.1.3.3.2.cmml" xref="alg1.l4.1.1.m1.1.1.3.3.2">𝑐</ci><cn type="integer" id="alg1.l4.1.1.m1.1.1.3.3.3.cmml" xref="alg1.l4.1.1.m1.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.1.1.m1.1c">\omega_{c}\leftarrow\omega_{c-1}</annotation></semantics></math></span><span id="alg1.l4.3" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l5" class="ltx_listingline">
<span id="alg1.l5.2" class="ltx_text" style="font-size:90%;">    </span><span id="alg1.l5.1" class="ltx_text" style="font-size:90%;background-color:#E069FF;">Aggregate received models <math id="alg1.l5.1.1.m1.4" class="ltx_Math" alttext="\omega_{c}\leftarrow\text{Aggregation}\{\omega_{1},\omega_{2},...,\omega_{c-1}\}" display="inline"><semantics id="alg1.l5.1.1.m1.4a"><mrow id="alg1.l5.1.1.m1.4.4" xref="alg1.l5.1.1.m1.4.4.cmml"><msub id="alg1.l5.1.1.m1.4.4.5" xref="alg1.l5.1.1.m1.4.4.5.cmml"><mi mathbackground="#E069FF" id="alg1.l5.1.1.m1.4.4.5.2" xref="alg1.l5.1.1.m1.4.4.5.2.cmml">ω</mi><mi mathbackground="#E069FF" id="alg1.l5.1.1.m1.4.4.5.3" xref="alg1.l5.1.1.m1.4.4.5.3.cmml">c</mi></msub><mo mathbackground="#E069FF" stretchy="false" id="alg1.l5.1.1.m1.4.4.4" xref="alg1.l5.1.1.m1.4.4.4.cmml">←</mo><mrow id="alg1.l5.1.1.m1.4.4.3" xref="alg1.l5.1.1.m1.4.4.3.cmml"><mtext mathbackground="#E069FF" id="alg1.l5.1.1.m1.4.4.3.5" xref="alg1.l5.1.1.m1.4.4.3.5a.cmml">Aggregation</mtext><mo lspace="0em" rspace="0em" id="alg1.l5.1.1.m1.4.4.3.4" xref="alg1.l5.1.1.m1.4.4.3.4.cmml">​</mo><mrow id="alg1.l5.1.1.m1.4.4.3.3.3" xref="alg1.l5.1.1.m1.4.4.3.3.4.cmml"><mo mathbackground="#E069FF" stretchy="false" id="alg1.l5.1.1.m1.4.4.3.3.3.4" xref="alg1.l5.1.1.m1.4.4.3.3.4.cmml">{</mo><msub id="alg1.l5.1.1.m1.2.2.1.1.1.1" xref="alg1.l5.1.1.m1.2.2.1.1.1.1.cmml"><mi mathbackground="#E069FF" id="alg1.l5.1.1.m1.2.2.1.1.1.1.2" xref="alg1.l5.1.1.m1.2.2.1.1.1.1.2.cmml">ω</mi><mn mathbackground="#E069FF" id="alg1.l5.1.1.m1.2.2.1.1.1.1.3" xref="alg1.l5.1.1.m1.2.2.1.1.1.1.3.cmml">1</mn></msub><mo mathbackground="#E069FF" id="alg1.l5.1.1.m1.4.4.3.3.3.5" xref="alg1.l5.1.1.m1.4.4.3.3.4.cmml">,</mo><msub id="alg1.l5.1.1.m1.3.3.2.2.2.2" xref="alg1.l5.1.1.m1.3.3.2.2.2.2.cmml"><mi mathbackground="#E069FF" id="alg1.l5.1.1.m1.3.3.2.2.2.2.2" xref="alg1.l5.1.1.m1.3.3.2.2.2.2.2.cmml">ω</mi><mn mathbackground="#E069FF" id="alg1.l5.1.1.m1.3.3.2.2.2.2.3" xref="alg1.l5.1.1.m1.3.3.2.2.2.2.3.cmml">2</mn></msub><mo mathbackground="#E069FF" id="alg1.l5.1.1.m1.4.4.3.3.3.6" xref="alg1.l5.1.1.m1.4.4.3.3.4.cmml">,</mo><mi mathbackground="#E069FF" mathvariant="normal" id="alg1.l5.1.1.m1.1.1" xref="alg1.l5.1.1.m1.1.1.cmml">…</mi><mo mathbackground="#E069FF" id="alg1.l5.1.1.m1.4.4.3.3.3.7" xref="alg1.l5.1.1.m1.4.4.3.3.4.cmml">,</mo><msub id="alg1.l5.1.1.m1.4.4.3.3.3.3" xref="alg1.l5.1.1.m1.4.4.3.3.3.3.cmml"><mi mathbackground="#E069FF" id="alg1.l5.1.1.m1.4.4.3.3.3.3.2" xref="alg1.l5.1.1.m1.4.4.3.3.3.3.2.cmml">ω</mi><mrow id="alg1.l5.1.1.m1.4.4.3.3.3.3.3" xref="alg1.l5.1.1.m1.4.4.3.3.3.3.3.cmml"><mi mathbackground="#E069FF" id="alg1.l5.1.1.m1.4.4.3.3.3.3.3.2" xref="alg1.l5.1.1.m1.4.4.3.3.3.3.3.2.cmml">c</mi><mo mathbackground="#E069FF" id="alg1.l5.1.1.m1.4.4.3.3.3.3.3.1" xref="alg1.l5.1.1.m1.4.4.3.3.3.3.3.1.cmml">−</mo><mn mathbackground="#E069FF" id="alg1.l5.1.1.m1.4.4.3.3.3.3.3.3" xref="alg1.l5.1.1.m1.4.4.3.3.3.3.3.3.cmml">1</mn></mrow></msub><mo mathbackground="#E069FF" stretchy="false" id="alg1.l5.1.1.m1.4.4.3.3.3.8" xref="alg1.l5.1.1.m1.4.4.3.3.4.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.1.1.m1.4b"><apply id="alg1.l5.1.1.m1.4.4.cmml" xref="alg1.l5.1.1.m1.4.4"><ci id="alg1.l5.1.1.m1.4.4.4.cmml" xref="alg1.l5.1.1.m1.4.4.4">←</ci><apply id="alg1.l5.1.1.m1.4.4.5.cmml" xref="alg1.l5.1.1.m1.4.4.5"><csymbol cd="ambiguous" id="alg1.l5.1.1.m1.4.4.5.1.cmml" xref="alg1.l5.1.1.m1.4.4.5">subscript</csymbol><ci id="alg1.l5.1.1.m1.4.4.5.2.cmml" xref="alg1.l5.1.1.m1.4.4.5.2">𝜔</ci><ci id="alg1.l5.1.1.m1.4.4.5.3.cmml" xref="alg1.l5.1.1.m1.4.4.5.3">𝑐</ci></apply><apply id="alg1.l5.1.1.m1.4.4.3.cmml" xref="alg1.l5.1.1.m1.4.4.3"><times id="alg1.l5.1.1.m1.4.4.3.4.cmml" xref="alg1.l5.1.1.m1.4.4.3.4"></times><ci id="alg1.l5.1.1.m1.4.4.3.5a.cmml" xref="alg1.l5.1.1.m1.4.4.3.5"><mtext mathbackground="#E069FF" id="alg1.l5.1.1.m1.4.4.3.5.cmml" xref="alg1.l5.1.1.m1.4.4.3.5">Aggregation</mtext></ci><set id="alg1.l5.1.1.m1.4.4.3.3.4.cmml" xref="alg1.l5.1.1.m1.4.4.3.3.3"><apply id="alg1.l5.1.1.m1.2.2.1.1.1.1.cmml" xref="alg1.l5.1.1.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l5.1.1.m1.2.2.1.1.1.1.1.cmml" xref="alg1.l5.1.1.m1.2.2.1.1.1.1">subscript</csymbol><ci id="alg1.l5.1.1.m1.2.2.1.1.1.1.2.cmml" xref="alg1.l5.1.1.m1.2.2.1.1.1.1.2">𝜔</ci><cn type="integer" id="alg1.l5.1.1.m1.2.2.1.1.1.1.3.cmml" xref="alg1.l5.1.1.m1.2.2.1.1.1.1.3">1</cn></apply><apply id="alg1.l5.1.1.m1.3.3.2.2.2.2.cmml" xref="alg1.l5.1.1.m1.3.3.2.2.2.2"><csymbol cd="ambiguous" id="alg1.l5.1.1.m1.3.3.2.2.2.2.1.cmml" xref="alg1.l5.1.1.m1.3.3.2.2.2.2">subscript</csymbol><ci id="alg1.l5.1.1.m1.3.3.2.2.2.2.2.cmml" xref="alg1.l5.1.1.m1.3.3.2.2.2.2.2">𝜔</ci><cn type="integer" id="alg1.l5.1.1.m1.3.3.2.2.2.2.3.cmml" xref="alg1.l5.1.1.m1.3.3.2.2.2.2.3">2</cn></apply><ci id="alg1.l5.1.1.m1.1.1.cmml" xref="alg1.l5.1.1.m1.1.1">…</ci><apply id="alg1.l5.1.1.m1.4.4.3.3.3.3.cmml" xref="alg1.l5.1.1.m1.4.4.3.3.3.3"><csymbol cd="ambiguous" id="alg1.l5.1.1.m1.4.4.3.3.3.3.1.cmml" xref="alg1.l5.1.1.m1.4.4.3.3.3.3">subscript</csymbol><ci id="alg1.l5.1.1.m1.4.4.3.3.3.3.2.cmml" xref="alg1.l5.1.1.m1.4.4.3.3.3.3.2">𝜔</ci><apply id="alg1.l5.1.1.m1.4.4.3.3.3.3.3.cmml" xref="alg1.l5.1.1.m1.4.4.3.3.3.3.3"><minus id="alg1.l5.1.1.m1.4.4.3.3.3.3.3.1.cmml" xref="alg1.l5.1.1.m1.4.4.3.3.3.3.3.1"></minus><ci id="alg1.l5.1.1.m1.4.4.3.3.3.3.3.2.cmml" xref="alg1.l5.1.1.m1.4.4.3.3.3.3.3.2">𝑐</ci><cn type="integer" id="alg1.l5.1.1.m1.4.4.3.3.3.3.3.3.cmml" xref="alg1.l5.1.1.m1.4.4.3.3.3.3.3.3">1</cn></apply></apply></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.1.1.m1.4c">\omega_{c}\leftarrow\text{Aggregation}\{\omega_{1},\omega_{2},...,\omega_{c-1}\}</annotation></semantics></math></span><span id="alg1.l5.3" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l6" class="ltx_listingline">
<span id="alg1.l6.1" class="ltx_text" style="font-size:90%;">    </span><span id="alg1.l6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span id="alg1.l6.3" class="ltx_text" style="font-size:90%;"> </span><math id="alg1.l6.m1.1" class="ltx_Math" alttext="e=1" display="inline"><semantics id="alg1.l6.m1.1a"><mrow id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml"><mi mathsize="90%" id="alg1.l6.m1.1.1.2" xref="alg1.l6.m1.1.1.2.cmml">e</mi><mo mathsize="90%" id="alg1.l6.m1.1.1.1" xref="alg1.l6.m1.1.1.1.cmml">=</mo><mn mathsize="90%" id="alg1.l6.m1.1.1.3" xref="alg1.l6.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><apply id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1"><eq id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1"></eq><ci id="alg1.l6.m1.1.1.2.cmml" xref="alg1.l6.m1.1.1.2">𝑒</ci><cn type="integer" id="alg1.l6.m1.1.1.3.cmml" xref="alg1.l6.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">e=1</annotation></semantics></math><span id="alg1.l6.4" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l6.5" class="ltx_text ltx_font_bold" style="font-size:90%;">to</span><span id="alg1.l6.6" class="ltx_text" style="font-size:90%;"> </span><math id="alg1.l6.m2.1" class="ltx_Math" alttext="E-1" display="inline"><semantics id="alg1.l6.m2.1a"><mrow id="alg1.l6.m2.1.1" xref="alg1.l6.m2.1.1.cmml"><mi mathsize="90%" id="alg1.l6.m2.1.1.2" xref="alg1.l6.m2.1.1.2.cmml">E</mi><mo mathsize="90%" id="alg1.l6.m2.1.1.1" xref="alg1.l6.m2.1.1.1.cmml">−</mo><mn mathsize="90%" id="alg1.l6.m2.1.1.3" xref="alg1.l6.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m2.1b"><apply id="alg1.l6.m2.1.1.cmml" xref="alg1.l6.m2.1.1"><minus id="alg1.l6.m2.1.1.1.cmml" xref="alg1.l6.m2.1.1.1"></minus><ci id="alg1.l6.m2.1.1.2.cmml" xref="alg1.l6.m2.1.1.2">𝐸</ci><cn type="integer" id="alg1.l6.m2.1.1.3.cmml" xref="alg1.l6.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m2.1c">E-1</annotation></semantics></math><span id="alg1.l6.7" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l6.8" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span><span id="alg1.l6.9" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l7" class="ltx_listingline">
<span id="alg1.l7.1" class="ltx_text" style="font-size:90%;">         Backpropagate and update the local model </span><math id="alg1.l7.m1.1" class="ltx_Math" alttext="\omega_{c}^{e+1}\leftarrow\omega_{c}^{e}-\eta\nabla\mathcal{L}" display="inline"><semantics id="alg1.l7.m1.1a"><mrow id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml"><msubsup id="alg1.l7.m1.1.1.2" xref="alg1.l7.m1.1.1.2.cmml"><mi mathsize="90%" id="alg1.l7.m1.1.1.2.2.2" xref="alg1.l7.m1.1.1.2.2.2.cmml">ω</mi><mi mathsize="90%" id="alg1.l7.m1.1.1.2.2.3" xref="alg1.l7.m1.1.1.2.2.3.cmml">c</mi><mrow id="alg1.l7.m1.1.1.2.3" xref="alg1.l7.m1.1.1.2.3.cmml"><mi mathsize="90%" id="alg1.l7.m1.1.1.2.3.2" xref="alg1.l7.m1.1.1.2.3.2.cmml">e</mi><mo mathsize="90%" id="alg1.l7.m1.1.1.2.3.1" xref="alg1.l7.m1.1.1.2.3.1.cmml">+</mo><mn mathsize="90%" id="alg1.l7.m1.1.1.2.3.3" xref="alg1.l7.m1.1.1.2.3.3.cmml">1</mn></mrow></msubsup><mo mathsize="90%" stretchy="false" id="alg1.l7.m1.1.1.1" xref="alg1.l7.m1.1.1.1.cmml">←</mo><mrow id="alg1.l7.m1.1.1.3" xref="alg1.l7.m1.1.1.3.cmml"><msubsup id="alg1.l7.m1.1.1.3.2" xref="alg1.l7.m1.1.1.3.2.cmml"><mi mathsize="90%" id="alg1.l7.m1.1.1.3.2.2.2" xref="alg1.l7.m1.1.1.3.2.2.2.cmml">ω</mi><mi mathsize="90%" id="alg1.l7.m1.1.1.3.2.2.3" xref="alg1.l7.m1.1.1.3.2.2.3.cmml">c</mi><mi mathsize="90%" id="alg1.l7.m1.1.1.3.2.3" xref="alg1.l7.m1.1.1.3.2.3.cmml">e</mi></msubsup><mo mathsize="90%" id="alg1.l7.m1.1.1.3.1" xref="alg1.l7.m1.1.1.3.1.cmml">−</mo><mrow id="alg1.l7.m1.1.1.3.3" xref="alg1.l7.m1.1.1.3.3.cmml"><mi mathsize="90%" id="alg1.l7.m1.1.1.3.3.2" xref="alg1.l7.m1.1.1.3.3.2.cmml">η</mi><mo lspace="0.167em" rspace="0em" id="alg1.l7.m1.1.1.3.3.1" xref="alg1.l7.m1.1.1.3.3.1.cmml">​</mo><mrow id="alg1.l7.m1.1.1.3.3.3" xref="alg1.l7.m1.1.1.3.3.3.cmml"><mo mathsize="90%" rspace="0.167em" id="alg1.l7.m1.1.1.3.3.3.1" xref="alg1.l7.m1.1.1.3.3.3.1.cmml">∇</mo><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="alg1.l7.m1.1.1.3.3.3.2" xref="alg1.l7.m1.1.1.3.3.3.2.cmml">ℒ</mi></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><apply id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1"><ci id="alg1.l7.m1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1">←</ci><apply id="alg1.l7.m1.1.1.2.cmml" xref="alg1.l7.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l7.m1.1.1.2.1.cmml" xref="alg1.l7.m1.1.1.2">superscript</csymbol><apply id="alg1.l7.m1.1.1.2.2.cmml" xref="alg1.l7.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l7.m1.1.1.2.2.1.cmml" xref="alg1.l7.m1.1.1.2">subscript</csymbol><ci id="alg1.l7.m1.1.1.2.2.2.cmml" xref="alg1.l7.m1.1.1.2.2.2">𝜔</ci><ci id="alg1.l7.m1.1.1.2.2.3.cmml" xref="alg1.l7.m1.1.1.2.2.3">𝑐</ci></apply><apply id="alg1.l7.m1.1.1.2.3.cmml" xref="alg1.l7.m1.1.1.2.3"><plus id="alg1.l7.m1.1.1.2.3.1.cmml" xref="alg1.l7.m1.1.1.2.3.1"></plus><ci id="alg1.l7.m1.1.1.2.3.2.cmml" xref="alg1.l7.m1.1.1.2.3.2">𝑒</ci><cn type="integer" id="alg1.l7.m1.1.1.2.3.3.cmml" xref="alg1.l7.m1.1.1.2.3.3">1</cn></apply></apply><apply id="alg1.l7.m1.1.1.3.cmml" xref="alg1.l7.m1.1.1.3"><minus id="alg1.l7.m1.1.1.3.1.cmml" xref="alg1.l7.m1.1.1.3.1"></minus><apply id="alg1.l7.m1.1.1.3.2.cmml" xref="alg1.l7.m1.1.1.3.2"><csymbol cd="ambiguous" id="alg1.l7.m1.1.1.3.2.1.cmml" xref="alg1.l7.m1.1.1.3.2">superscript</csymbol><apply id="alg1.l7.m1.1.1.3.2.2.cmml" xref="alg1.l7.m1.1.1.3.2"><csymbol cd="ambiguous" id="alg1.l7.m1.1.1.3.2.2.1.cmml" xref="alg1.l7.m1.1.1.3.2">subscript</csymbol><ci id="alg1.l7.m1.1.1.3.2.2.2.cmml" xref="alg1.l7.m1.1.1.3.2.2.2">𝜔</ci><ci id="alg1.l7.m1.1.1.3.2.2.3.cmml" xref="alg1.l7.m1.1.1.3.2.2.3">𝑐</ci></apply><ci id="alg1.l7.m1.1.1.3.2.3.cmml" xref="alg1.l7.m1.1.1.3.2.3">𝑒</ci></apply><apply id="alg1.l7.m1.1.1.3.3.cmml" xref="alg1.l7.m1.1.1.3.3"><times id="alg1.l7.m1.1.1.3.3.1.cmml" xref="alg1.l7.m1.1.1.3.3.1"></times><ci id="alg1.l7.m1.1.1.3.3.2.cmml" xref="alg1.l7.m1.1.1.3.3.2">𝜂</ci><apply id="alg1.l7.m1.1.1.3.3.3.cmml" xref="alg1.l7.m1.1.1.3.3.3"><ci id="alg1.l7.m1.1.1.3.3.3.1.cmml" xref="alg1.l7.m1.1.1.3.3.3.1">∇</ci><ci id="alg1.l7.m1.1.1.3.3.3.2.cmml" xref="alg1.l7.m1.1.1.3.3.3.2">ℒ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">\omega_{c}^{e+1}\leftarrow\omega_{c}^{e}-\eta\nabla\mathcal{L}</annotation></semantics></math><span id="alg1.l7.2" class="ltx_text" style="font-size:90%;">.
</span>
</div>
<div id="alg1.l8" class="ltx_listingline">
<span id="alg1.l8.1" class="ltx_text" style="font-size:90%;">    </span><span id="alg1.l8.2" class="ltx_text ltx_font_bold" style="font-size:90%;">end</span><span id="alg1.l8.3" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l8.4" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span>
</div>
<div id="alg1.l9" class="ltx_listingline">
<span id="alg1.l9.1" class="ltx_text" style="font-size:90%;">    Update the local model </span><math id="alg1.l9.m1.1" class="ltx_Math" alttext="\omega_{c}\leftarrow\omega_{c}^{E}" display="inline"><semantics id="alg1.l9.m1.1a"><mrow id="alg1.l9.m1.1.1" xref="alg1.l9.m1.1.1.cmml"><msub id="alg1.l9.m1.1.1.2" xref="alg1.l9.m1.1.1.2.cmml"><mi mathsize="90%" id="alg1.l9.m1.1.1.2.2" xref="alg1.l9.m1.1.1.2.2.cmml">ω</mi><mi mathsize="90%" id="alg1.l9.m1.1.1.2.3" xref="alg1.l9.m1.1.1.2.3.cmml">c</mi></msub><mo mathsize="90%" stretchy="false" id="alg1.l9.m1.1.1.1" xref="alg1.l9.m1.1.1.1.cmml">←</mo><msubsup id="alg1.l9.m1.1.1.3" xref="alg1.l9.m1.1.1.3.cmml"><mi mathsize="90%" id="alg1.l9.m1.1.1.3.2.2" xref="alg1.l9.m1.1.1.3.2.2.cmml">ω</mi><mi mathsize="90%" id="alg1.l9.m1.1.1.3.2.3" xref="alg1.l9.m1.1.1.3.2.3.cmml">c</mi><mi mathsize="90%" id="alg1.l9.m1.1.1.3.3" xref="alg1.l9.m1.1.1.3.3.cmml">E</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.1b"><apply id="alg1.l9.m1.1.1.cmml" xref="alg1.l9.m1.1.1"><ci id="alg1.l9.m1.1.1.1.cmml" xref="alg1.l9.m1.1.1.1">←</ci><apply id="alg1.l9.m1.1.1.2.cmml" xref="alg1.l9.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l9.m1.1.1.2.1.cmml" xref="alg1.l9.m1.1.1.2">subscript</csymbol><ci id="alg1.l9.m1.1.1.2.2.cmml" xref="alg1.l9.m1.1.1.2.2">𝜔</ci><ci id="alg1.l9.m1.1.1.2.3.cmml" xref="alg1.l9.m1.1.1.2.3">𝑐</ci></apply><apply id="alg1.l9.m1.1.1.3.cmml" xref="alg1.l9.m1.1.1.3"><csymbol cd="ambiguous" id="alg1.l9.m1.1.1.3.1.cmml" xref="alg1.l9.m1.1.1.3">superscript</csymbol><apply id="alg1.l9.m1.1.1.3.2.cmml" xref="alg1.l9.m1.1.1.3"><csymbol cd="ambiguous" id="alg1.l9.m1.1.1.3.2.1.cmml" xref="alg1.l9.m1.1.1.3">subscript</csymbol><ci id="alg1.l9.m1.1.1.3.2.2.cmml" xref="alg1.l9.m1.1.1.3.2.2">𝜔</ci><ci id="alg1.l9.m1.1.1.3.2.3.cmml" xref="alg1.l9.m1.1.1.3.2.3">𝑐</ci></apply><ci id="alg1.l9.m1.1.1.3.3.cmml" xref="alg1.l9.m1.1.1.3.3">𝐸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.1c">\omega_{c}\leftarrow\omega_{c}^{E}</annotation></semantics></math><span id="alg1.l9.2" class="ltx_text" style="font-size:90%;">.
</span>
</div>
<div id="alg1.l10" class="ltx_listingline">
<span id="alg1.l10.2" class="ltx_text" style="font-size:90%;">    Client </span><math id="alg1.l10.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="alg1.l10.m1.1a"><mi mathsize="90%" id="alg1.l10.m1.1.1" xref="alg1.l10.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="alg1.l10.m1.1b"><ci id="alg1.l10.m1.1.1.cmml" xref="alg1.l10.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m1.1c">c</annotation></semantics></math><span id="alg1.l10.3" class="ltx_text" style="font-size:90%;"> sends </span><span id="alg1.l10.1" class="ltx_text" style="font-size:90%;"><math id="alg1.l10.1.m1.4" class="ltx_Math" style="background-color:#E069FF;" alttext="\{\omega_{1},\omega_{2},...,\omega_{c-1}\}" display="inline"><semantics id="alg1.l10.1.m1.4a"><mrow id="alg1.l10.1.m1.4.4.3" xref="alg1.l10.1.m1.4.4.4.cmml"><mo mathbackground="#E069FF" stretchy="false" id="alg1.l10.1.m1.4.4.3.4" xref="alg1.l10.1.m1.4.4.4.cmml">{</mo><msub id="alg1.l10.1.m1.2.2.1.1" xref="alg1.l10.1.m1.2.2.1.1.cmml"><mi mathbackground="#E069FF" id="alg1.l10.1.m1.2.2.1.1.2" xref="alg1.l10.1.m1.2.2.1.1.2.cmml">ω</mi><mn mathbackground="#E069FF" id="alg1.l10.1.m1.2.2.1.1.3" xref="alg1.l10.1.m1.2.2.1.1.3.cmml">1</mn></msub><mo mathbackground="#E069FF" id="alg1.l10.1.m1.4.4.3.5" xref="alg1.l10.1.m1.4.4.4.cmml">,</mo><msub id="alg1.l10.1.m1.3.3.2.2" xref="alg1.l10.1.m1.3.3.2.2.cmml"><mi mathbackground="#E069FF" id="alg1.l10.1.m1.3.3.2.2.2" xref="alg1.l10.1.m1.3.3.2.2.2.cmml">ω</mi><mn mathbackground="#E069FF" id="alg1.l10.1.m1.3.3.2.2.3" xref="alg1.l10.1.m1.3.3.2.2.3.cmml">2</mn></msub><mo mathbackground="#E069FF" id="alg1.l10.1.m1.4.4.3.6" xref="alg1.l10.1.m1.4.4.4.cmml">,</mo><mi mathbackground="#E069FF" mathvariant="normal" id="alg1.l10.1.m1.1.1" xref="alg1.l10.1.m1.1.1.cmml">…</mi><mo mathbackground="#E069FF" id="alg1.l10.1.m1.4.4.3.7" xref="alg1.l10.1.m1.4.4.4.cmml">,</mo><msub id="alg1.l10.1.m1.4.4.3.3" xref="alg1.l10.1.m1.4.4.3.3.cmml"><mi mathbackground="#E069FF" id="alg1.l10.1.m1.4.4.3.3.2" xref="alg1.l10.1.m1.4.4.3.3.2.cmml">ω</mi><mrow id="alg1.l10.1.m1.4.4.3.3.3" xref="alg1.l10.1.m1.4.4.3.3.3.cmml"><mi mathbackground="#E069FF" id="alg1.l10.1.m1.4.4.3.3.3.2" xref="alg1.l10.1.m1.4.4.3.3.3.2.cmml">c</mi><mo mathbackground="#E069FF" id="alg1.l10.1.m1.4.4.3.3.3.1" xref="alg1.l10.1.m1.4.4.3.3.3.1.cmml">−</mo><mn mathbackground="#E069FF" id="alg1.l10.1.m1.4.4.3.3.3.3" xref="alg1.l10.1.m1.4.4.3.3.3.3.cmml">1</mn></mrow></msub><mo mathbackground="#E069FF" stretchy="false" id="alg1.l10.1.m1.4.4.3.8" xref="alg1.l10.1.m1.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="alg1.l10.1.m1.4b"><set id="alg1.l10.1.m1.4.4.4.cmml" xref="alg1.l10.1.m1.4.4.3"><apply id="alg1.l10.1.m1.2.2.1.1.cmml" xref="alg1.l10.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="alg1.l10.1.m1.2.2.1.1.1.cmml" xref="alg1.l10.1.m1.2.2.1.1">subscript</csymbol><ci id="alg1.l10.1.m1.2.2.1.1.2.cmml" xref="alg1.l10.1.m1.2.2.1.1.2">𝜔</ci><cn type="integer" id="alg1.l10.1.m1.2.2.1.1.3.cmml" xref="alg1.l10.1.m1.2.2.1.1.3">1</cn></apply><apply id="alg1.l10.1.m1.3.3.2.2.cmml" xref="alg1.l10.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="alg1.l10.1.m1.3.3.2.2.1.cmml" xref="alg1.l10.1.m1.3.3.2.2">subscript</csymbol><ci id="alg1.l10.1.m1.3.3.2.2.2.cmml" xref="alg1.l10.1.m1.3.3.2.2.2">𝜔</ci><cn type="integer" id="alg1.l10.1.m1.3.3.2.2.3.cmml" xref="alg1.l10.1.m1.3.3.2.2.3">2</cn></apply><ci id="alg1.l10.1.m1.1.1.cmml" xref="alg1.l10.1.m1.1.1">…</ci><apply id="alg1.l10.1.m1.4.4.3.3.cmml" xref="alg1.l10.1.m1.4.4.3.3"><csymbol cd="ambiguous" id="alg1.l10.1.m1.4.4.3.3.1.cmml" xref="alg1.l10.1.m1.4.4.3.3">subscript</csymbol><ci id="alg1.l10.1.m1.4.4.3.3.2.cmml" xref="alg1.l10.1.m1.4.4.3.3.2">𝜔</ci><apply id="alg1.l10.1.m1.4.4.3.3.3.cmml" xref="alg1.l10.1.m1.4.4.3.3.3"><minus id="alg1.l10.1.m1.4.4.3.3.3.1.cmml" xref="alg1.l10.1.m1.4.4.3.3.3.1"></minus><ci id="alg1.l10.1.m1.4.4.3.3.3.2.cmml" xref="alg1.l10.1.m1.4.4.3.3.3.2">𝑐</ci><cn type="integer" id="alg1.l10.1.m1.4.4.3.3.3.3.cmml" xref="alg1.l10.1.m1.4.4.3.3.3.3">1</cn></apply></apply></set></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.1.m1.4c">\{\omega_{1},\omega_{2},...,\omega_{c-1}\}</annotation></semantics></math><span id="alg1.l10.1.1" class="ltx_text" style="background-color:#E069FF;"> and</span></span><span id="alg1.l10.4" class="ltx_text" style="font-size:90%;"> </span><math id="alg1.l10.m2.1" class="ltx_Math" alttext="\omega_{c}" display="inline"><semantics id="alg1.l10.m2.1a"><msub id="alg1.l10.m2.1.1" xref="alg1.l10.m2.1.1.cmml"><mi mathsize="90%" id="alg1.l10.m2.1.1.2" xref="alg1.l10.m2.1.1.2.cmml">ω</mi><mi mathsize="90%" id="alg1.l10.m2.1.1.3" xref="alg1.l10.m2.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l10.m2.1b"><apply id="alg1.l10.m2.1.1.cmml" xref="alg1.l10.m2.1.1"><csymbol cd="ambiguous" id="alg1.l10.m2.1.1.1.cmml" xref="alg1.l10.m2.1.1">subscript</csymbol><ci id="alg1.l10.m2.1.1.2.cmml" xref="alg1.l10.m2.1.1.2">𝜔</ci><ci id="alg1.l10.m2.1.1.3.cmml" xref="alg1.l10.m2.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m2.1c">\omega_{c}</annotation></semantics></math><span id="alg1.l10.5" class="ltx_text" style="font-size:90%;"> to the next client.
</span>
</div>
<div id="alg1.l11" class="ltx_listingline">
<span id="alg1.l11.1" class="ltx_text" style="font-size:90%;">    </span>
</div>
<div id="alg1.l12" class="ltx_listingline">
<span id="alg1.l12.1" class="ltx_text ltx_font_bold" style="font-size:90%;">end</span><span id="alg1.l12.2" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l12.3" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span>
</div>
</div>
</figure>
<div id="S3.SS4.p5" class="ltx_para">
<p id="S3.SS4.p5.1" class="ltx_p">Based on the aforementioned assumptions and iterative process, we can make certain expectations regarding the accuracy, loss, convergence, and communication complexity of the clients in both paradigms during training. We come up with the following speculations:</p>
<ol id="S3.I12" class="ltx_enumerate">
<li id="S3.I12.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I12.i1.p1" class="ltx_para">
<p id="S3.I12.i1.p1.1" class="ltx_p">The learning loss during the learning process of the first two clients is identical in both paradigms.</p>
</div>
</li>
<li id="S3.I12.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I12.i2.p1" class="ltx_para">
<p id="S3.I12.i2.p1.1" class="ltx_p">The learning loss will exhibit periodic oscillations across client iterations and eventually converge in both paradigms.</p>
</div>
</li>
<li id="S3.I12.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I12.i3.p1" class="ltx_para">
<p id="S3.I12.i3.p1.1" class="ltx_p">The convergence of learning loss in the <span id="S3.I12.i3.p1.1.1" class="ltx_text ltx_font_typewriter">Aggregate</span> paradigm is expected to be more stable. In the <span id="S3.I12.i3.p1.1.2" class="ltx_text ltx_font_typewriter">Continual</span> paradigm, the learning difficulty depends on the discrepancy between the previous client and the current client’s local data, in other words, it depends on the iteration order of clients. Under the assumption of a normal distribution, the learning difficulty in the <span id="S3.I12.i3.p1.1.3" class="ltx_text ltx_font_typewriter">Aggregate</span> paradigm is determined by the heterogeneity of the current client’s data, and most clients may have similar data distributions.</p>
</div>
</li>
<li id="S3.I12.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I12.i4.p1" class="ltx_para">
<p id="S3.I12.i4.p1.1" class="ltx_p">The convergence of learning loss in the <span id="S3.I12.i4.p1.1.1" class="ltx_text ltx_font_typewriter">Aggregate</span> paradigm is also expected to be faster due to the decreasing learning difficulty as the client iterations progress.</p>
</div>
</li>
<li id="S3.I12.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S3.I12.i5.p1" class="ltx_para">
<p id="S3.I12.i5.p1.1" class="ltx_p">Similarly, the average accuracy in the <span id="S3.I12.i5.p1.1.1" class="ltx_text ltx_font_typewriter">Aggregate</span> paradigm is also expected to be higher.</p>
</div>
</li>
<li id="S3.I12.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S3.I12.i6.p1" class="ltx_para">
<p id="S3.I12.i6.p1.1" class="ltx_p">Due to the aggregation computation and the requirement to transmit multiple times the model parameter packets, the overall learning time and communication overhead in the <span id="S3.I12.i6.p1.1.1" class="ltx_text ltx_font_typewriter">Aggregate</span> paradigm are expected to be longer and greater, respectively.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.4.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.5.2" class="ltx_text ltx_font_italic">Temporal Variability</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">The network topology of DFLs has recently undergone a shift from static to dynamic trends, adapting to the time-varying external environment. The inspiration for the separation and clustering of network topologies comes from group behaviors observed in nature, such as fish schools and bee swarms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>. When a school of fish encounters a predator, the entire school separates to avoid it. Similarly, in a bee swarm, a small number of scouts can lead the entire swarm, demonstrating the herd effect. Interestingly, migratory birds form V-shaped formations during long-distance flights to conserve energy, and the birds at the front rotate over time to distribute flight fatigue evenly. In the context of DFL networks, dynamic topologies may exhibit more robust, fair, and efficient performance compared to static topologies. The determination of dynamic topologies in DFL networks can be influenced by various factors, including:</p>
<ul id="S3.I13" class="ltx_itemize">
<li id="S3.I13.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I13.i1.p1" class="ltx_para">
<p id="S3.I13.i1.p1.1" class="ltx_p">External interference. Strong and unbreakable communication barriers, SPoF, malicious attacks, and other external factors can lead to changes in the network topology. In order to avoid the failure of the entire network, topology adjustments and discards are made.</p>
</div>
</li>
<li id="S3.I13.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I13.i2.p1" class="ltx_para">
<p id="S3.I13.i2.p1.1" class="ltx_p">Communication resource saving. Clients have the ability to dynamically select their neighbors for each communication. By selectively choosing nearby clients, communication resources can be optimized and saved. Additionally, clients can dynamically elect the most central client as a leader during each communication, enhancing the efficiency and effectiveness of communication within the network.</p>
</div>
</li>
<li id="S3.I13.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I13.i3.p1" class="ltx_para">
<p id="S3.I13.i3.p1.1" class="ltx_p">Fairness. In order to ensure fairness among clients, a random selection process is employed for determining the communication target. This helps to prevent any bias or preference towards specific clients, ensuring equal opportunities for all participants.</p>
</div>
</li>
</ul>
<p id="S3.SS5.p1.2" class="ltx_p">The development of dynamic topological structures based on these factors shapes DFL networks to facilitate robust, efficient, and fair communication among clients.</p>
</div>
<figure id="alg2" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_float"><span id="alg2.8.1.1" class="ltx_text ltx_font_bold">Algorithm 2</span> </span> Cycle <span id="alg2.9.2" class="ltx_text" style="background-color:#BDFF3D;">pointing ring <span id="alg2.9.2.1" class="ltx_text ltx_font_typewriter">Continual</span></span> and <span id="alg2.10.3" class="ltx_text" style="background-color:#E069FF;">broadcast mesh <span id="alg2.10.3.1" class="ltx_text ltx_font_typewriter">Aggregate</span></span> Decentralized Federated Learning.</figcaption>
<div id="alg2.11" class="ltx_listing ltx_listing">
<div id="alg2.l1" class="ltx_listingline">
<span id="alg2.l1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Input:</span><span id="alg2.l1.2" class="ltx_text" style="font-size:90%;"> Client set (</span><math id="alg2.l1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="alg2.l1.m1.1a"><mi mathsize="90%" id="alg2.l1.m1.1.1" xref="alg2.l1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="alg2.l1.m1.1b"><ci id="alg2.l1.m1.1.1.cmml" xref="alg2.l1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.l1.m1.1c">C</annotation></semantics></math><span id="alg2.l1.3" class="ltx_text" style="font-size:90%;">), training epoch (</span><math id="alg2.l1.m2.1" class="ltx_Math" alttext="E" display="inline"><semantics id="alg2.l1.m2.1a"><mi mathsize="90%" id="alg2.l1.m2.1.1" xref="alg2.l1.m2.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="alg2.l1.m2.1b"><ci id="alg2.l1.m2.1.1.cmml" xref="alg2.l1.m2.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.l1.m2.1c">E</annotation></semantics></math><span id="alg2.l1.4" class="ltx_text" style="font-size:90%;">), initial model (</span><math id="alg2.l1.m3.1" class="ltx_Math" alttext="\omega_{0}" display="inline"><semantics id="alg2.l1.m3.1a"><msub id="alg2.l1.m3.1.1" xref="alg2.l1.m3.1.1.cmml"><mi mathsize="90%" id="alg2.l1.m3.1.1.2" xref="alg2.l1.m3.1.1.2.cmml">ω</mi><mn mathsize="90%" id="alg2.l1.m3.1.1.3" xref="alg2.l1.m3.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="alg2.l1.m3.1b"><apply id="alg2.l1.m3.1.1.cmml" xref="alg2.l1.m3.1.1"><csymbol cd="ambiguous" id="alg2.l1.m3.1.1.1.cmml" xref="alg2.l1.m3.1.1">subscript</csymbol><ci id="alg2.l1.m3.1.1.2.cmml" xref="alg2.l1.m3.1.1.2">𝜔</ci><cn type="integer" id="alg2.l1.m3.1.1.3.cmml" xref="alg2.l1.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l1.m3.1c">\omega_{0}</annotation></semantics></math><span id="alg2.l1.5" class="ltx_text" style="font-size:90%;">), loss function (</span><math id="alg2.l1.m4.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="alg2.l1.m4.1a"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="alg2.l1.m4.1.1" xref="alg2.l1.m4.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="alg2.l1.m4.1b"><ci id="alg2.l1.m4.1.1.cmml" xref="alg2.l1.m4.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.l1.m4.1c">\mathcal{L}</annotation></semantics></math><span id="alg2.l1.6" class="ltx_text" style="font-size:90%;">), learning rate (</span><math id="alg2.l1.m5.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="alg2.l1.m5.1a"><mi mathsize="90%" id="alg2.l1.m5.1.1" xref="alg2.l1.m5.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="alg2.l1.m5.1b"><ci id="alg2.l1.m5.1.1.cmml" xref="alg2.l1.m5.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.l1.m5.1c">\eta</annotation></semantics></math><span id="alg2.l1.7" class="ltx_text" style="font-size:90%;">)
</span>
</div>
<div id="alg2.l2" class="ltx_listingline">
<span id="alg2.l2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Output:</span><span id="alg2.l2.2" class="ltx_text" style="font-size:90%;"> Local models (</span><math id="alg2.l2.m1.2" class="ltx_Math" alttext="\{\omega_{c}|c\in C\}" display="inline"><semantics id="alg2.l2.m1.2a"><mrow id="alg2.l2.m1.2.2.2" xref="alg2.l2.m1.2.2.3.cmml"><mo maxsize="90%" minsize="90%" id="alg2.l2.m1.2.2.2.3" xref="alg2.l2.m1.2.2.3.1.cmml">{</mo><msub id="alg2.l2.m1.1.1.1.1" xref="alg2.l2.m1.1.1.1.1.cmml"><mi mathsize="90%" id="alg2.l2.m1.1.1.1.1.2" xref="alg2.l2.m1.1.1.1.1.2.cmml">ω</mi><mi mathsize="90%" id="alg2.l2.m1.1.1.1.1.3" xref="alg2.l2.m1.1.1.1.1.3.cmml">c</mi></msub><mo lspace="0em" mathsize="90%" rspace="0em" id="alg2.l2.m1.2.2.2.4" xref="alg2.l2.m1.2.2.3.1.cmml">|</mo><mrow id="alg2.l2.m1.2.2.2.2" xref="alg2.l2.m1.2.2.2.2.cmml"><mi mathsize="90%" id="alg2.l2.m1.2.2.2.2.2" xref="alg2.l2.m1.2.2.2.2.2.cmml">c</mi><mo mathsize="90%" id="alg2.l2.m1.2.2.2.2.1" xref="alg2.l2.m1.2.2.2.2.1.cmml">∈</mo><mi mathsize="90%" id="alg2.l2.m1.2.2.2.2.3" xref="alg2.l2.m1.2.2.2.2.3.cmml">C</mi></mrow><mo maxsize="90%" minsize="90%" id="alg2.l2.m1.2.2.2.5" xref="alg2.l2.m1.2.2.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="alg2.l2.m1.2b"><apply id="alg2.l2.m1.2.2.3.cmml" xref="alg2.l2.m1.2.2.2"><csymbol cd="latexml" id="alg2.l2.m1.2.2.3.1.cmml" xref="alg2.l2.m1.2.2.2.3">conditional-set</csymbol><apply id="alg2.l2.m1.1.1.1.1.cmml" xref="alg2.l2.m1.1.1.1.1"><csymbol cd="ambiguous" id="alg2.l2.m1.1.1.1.1.1.cmml" xref="alg2.l2.m1.1.1.1.1">subscript</csymbol><ci id="alg2.l2.m1.1.1.1.1.2.cmml" xref="alg2.l2.m1.1.1.1.1.2">𝜔</ci><ci id="alg2.l2.m1.1.1.1.1.3.cmml" xref="alg2.l2.m1.1.1.1.1.3">𝑐</ci></apply><apply id="alg2.l2.m1.2.2.2.2.cmml" xref="alg2.l2.m1.2.2.2.2"><in id="alg2.l2.m1.2.2.2.2.1.cmml" xref="alg2.l2.m1.2.2.2.2.1"></in><ci id="alg2.l2.m1.2.2.2.2.2.cmml" xref="alg2.l2.m1.2.2.2.2.2">𝑐</ci><ci id="alg2.l2.m1.2.2.2.2.3.cmml" xref="alg2.l2.m1.2.2.2.2.3">𝐶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l2.m1.2c">\{\omega_{c}|c\in C\}</annotation></semantics></math><span id="alg2.l2.3" class="ltx_text" style="font-size:90%;">)
</span>
</div>
<div id="alg2.l3" class="ltx_listingline">
<span id="alg2.l3.2" class="ltx_text ltx_font_bold" style="font-size:90%;">while</span><span id="alg2.l3.3" class="ltx_text" style="font-size:90%;"> </span><math id="alg2.l3.m1.1" class="ltx_Math" alttext="c\in C" display="inline"><semantics id="alg2.l3.m1.1a"><mrow id="alg2.l3.m1.1.1" xref="alg2.l3.m1.1.1.cmml"><mi mathsize="90%" id="alg2.l3.m1.1.1.2" xref="alg2.l3.m1.1.1.2.cmml">c</mi><mo mathsize="90%" id="alg2.l3.m1.1.1.1" xref="alg2.l3.m1.1.1.1.cmml">∈</mo><mi mathsize="90%" id="alg2.l3.m1.1.1.3" xref="alg2.l3.m1.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="alg2.l3.m1.1b"><apply id="alg2.l3.m1.1.1.cmml" xref="alg2.l3.m1.1.1"><in id="alg2.l3.m1.1.1.1.cmml" xref="alg2.l3.m1.1.1.1"></in><ci id="alg2.l3.m1.1.1.2.cmml" xref="alg2.l3.m1.1.1.2">𝑐</ci><ci id="alg2.l3.m1.1.1.3.cmml" xref="alg2.l3.m1.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l3.m1.1c">c\in C</annotation></semantics></math><span id="alg2.l3.4" class="ltx_text" style="font-size:90%;"> </span><span id="alg2.l3.5" class="ltx_text ltx_font_bold" style="font-size:90%;">in cyclic</span><span id="alg2.l3.6" class="ltx_text" style="font-size:90%;"> </span><span id="alg2.l3.7" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span><span id="alg2.l3.8" class="ltx_text" style="font-size:90%;"> </span><span id="alg2.l3.1" class="ltx_text" style="font-size:90%;float:right;"><math id="alg2.l3.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg2.l3.1.m1.1a"><mo id="alg2.l3.1.m1.1.1" xref="alg2.l3.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg2.l3.1.m1.1b"><ci id="alg2.l3.1.m1.1.1.cmml" xref="alg2.l3.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.l3.1.m1.1c">\triangleright</annotation></semantics></math> line vs. ring
</span>
</div>
<div id="alg2.l4" class="ltx_listingline">
<span id="alg2.l4.2" class="ltx_text" style="font-size:90%;">    </span><span id="alg2.l4.1" class="ltx_text" style="font-size:90%;background-color:#BDFF3D;">Copy the model from previous client <math id="alg2.l4.1.1.m1.1" class="ltx_Math" alttext="\omega_{c}\leftarrow\omega_{c-1}" display="inline"><semantics id="alg2.l4.1.1.m1.1a"><mrow id="alg2.l4.1.1.m1.1.1" xref="alg2.l4.1.1.m1.1.1.cmml"><msub id="alg2.l4.1.1.m1.1.1.2" xref="alg2.l4.1.1.m1.1.1.2.cmml"><mi mathbackground="#BDFF3D" id="alg2.l4.1.1.m1.1.1.2.2" xref="alg2.l4.1.1.m1.1.1.2.2.cmml">ω</mi><mi mathbackground="#BDFF3D" id="alg2.l4.1.1.m1.1.1.2.3" xref="alg2.l4.1.1.m1.1.1.2.3.cmml">c</mi></msub><mo mathbackground="#BDFF3D" stretchy="false" id="alg2.l4.1.1.m1.1.1.1" xref="alg2.l4.1.1.m1.1.1.1.cmml">←</mo><msub id="alg2.l4.1.1.m1.1.1.3" xref="alg2.l4.1.1.m1.1.1.3.cmml"><mi mathbackground="#BDFF3D" id="alg2.l4.1.1.m1.1.1.3.2" xref="alg2.l4.1.1.m1.1.1.3.2.cmml">ω</mi><mrow id="alg2.l4.1.1.m1.1.1.3.3" xref="alg2.l4.1.1.m1.1.1.3.3.cmml"><mi mathbackground="#BDFF3D" id="alg2.l4.1.1.m1.1.1.3.3.2" xref="alg2.l4.1.1.m1.1.1.3.3.2.cmml">c</mi><mo mathbackground="#BDFF3D" id="alg2.l4.1.1.m1.1.1.3.3.1" xref="alg2.l4.1.1.m1.1.1.3.3.1.cmml">−</mo><mn mathbackground="#BDFF3D" id="alg2.l4.1.1.m1.1.1.3.3.3" xref="alg2.l4.1.1.m1.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="alg2.l4.1.1.m1.1b"><apply id="alg2.l4.1.1.m1.1.1.cmml" xref="alg2.l4.1.1.m1.1.1"><ci id="alg2.l4.1.1.m1.1.1.1.cmml" xref="alg2.l4.1.1.m1.1.1.1">←</ci><apply id="alg2.l4.1.1.m1.1.1.2.cmml" xref="alg2.l4.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="alg2.l4.1.1.m1.1.1.2.1.cmml" xref="alg2.l4.1.1.m1.1.1.2">subscript</csymbol><ci id="alg2.l4.1.1.m1.1.1.2.2.cmml" xref="alg2.l4.1.1.m1.1.1.2.2">𝜔</ci><ci id="alg2.l4.1.1.m1.1.1.2.3.cmml" xref="alg2.l4.1.1.m1.1.1.2.3">𝑐</ci></apply><apply id="alg2.l4.1.1.m1.1.1.3.cmml" xref="alg2.l4.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="alg2.l4.1.1.m1.1.1.3.1.cmml" xref="alg2.l4.1.1.m1.1.1.3">subscript</csymbol><ci id="alg2.l4.1.1.m1.1.1.3.2.cmml" xref="alg2.l4.1.1.m1.1.1.3.2">𝜔</ci><apply id="alg2.l4.1.1.m1.1.1.3.3.cmml" xref="alg2.l4.1.1.m1.1.1.3.3"><minus id="alg2.l4.1.1.m1.1.1.3.3.1.cmml" xref="alg2.l4.1.1.m1.1.1.3.3.1"></minus><ci id="alg2.l4.1.1.m1.1.1.3.3.2.cmml" xref="alg2.l4.1.1.m1.1.1.3.3.2">𝑐</ci><cn type="integer" id="alg2.l4.1.1.m1.1.1.3.3.3.cmml" xref="alg2.l4.1.1.m1.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l4.1.1.m1.1c">\omega_{c}\leftarrow\omega_{c-1}</annotation></semantics></math></span><span id="alg2.l4.3" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg2.l5" class="ltx_listingline">
<span id="alg2.l5.2" class="ltx_text" style="font-size:90%;">    </span><span id="alg2.l5.1" class="ltx_text" style="font-size:90%;background-color:#E069FF;">Aggregate received models <math id="alg2.l5.1.1.m1.4" class="ltx_Math" alttext="\omega_{c}\leftarrow\text{Aggregation}\{\omega_{1},\omega_{2},...,\omega_{c-1}\}" display="inline"><semantics id="alg2.l5.1.1.m1.4a"><mrow id="alg2.l5.1.1.m1.4.4" xref="alg2.l5.1.1.m1.4.4.cmml"><msub id="alg2.l5.1.1.m1.4.4.5" xref="alg2.l5.1.1.m1.4.4.5.cmml"><mi mathbackground="#E069FF" id="alg2.l5.1.1.m1.4.4.5.2" xref="alg2.l5.1.1.m1.4.4.5.2.cmml">ω</mi><mi mathbackground="#E069FF" id="alg2.l5.1.1.m1.4.4.5.3" xref="alg2.l5.1.1.m1.4.4.5.3.cmml">c</mi></msub><mo mathbackground="#E069FF" stretchy="false" id="alg2.l5.1.1.m1.4.4.4" xref="alg2.l5.1.1.m1.4.4.4.cmml">←</mo><mrow id="alg2.l5.1.1.m1.4.4.3" xref="alg2.l5.1.1.m1.4.4.3.cmml"><mtext mathbackground="#E069FF" id="alg2.l5.1.1.m1.4.4.3.5" xref="alg2.l5.1.1.m1.4.4.3.5a.cmml">Aggregation</mtext><mo lspace="0em" rspace="0em" id="alg2.l5.1.1.m1.4.4.3.4" xref="alg2.l5.1.1.m1.4.4.3.4.cmml">​</mo><mrow id="alg2.l5.1.1.m1.4.4.3.3.3" xref="alg2.l5.1.1.m1.4.4.3.3.4.cmml"><mo mathbackground="#E069FF" stretchy="false" id="alg2.l5.1.1.m1.4.4.3.3.3.4" xref="alg2.l5.1.1.m1.4.4.3.3.4.cmml">{</mo><msub id="alg2.l5.1.1.m1.2.2.1.1.1.1" xref="alg2.l5.1.1.m1.2.2.1.1.1.1.cmml"><mi mathbackground="#E069FF" id="alg2.l5.1.1.m1.2.2.1.1.1.1.2" xref="alg2.l5.1.1.m1.2.2.1.1.1.1.2.cmml">ω</mi><mn mathbackground="#E069FF" id="alg2.l5.1.1.m1.2.2.1.1.1.1.3" xref="alg2.l5.1.1.m1.2.2.1.1.1.1.3.cmml">1</mn></msub><mo mathbackground="#E069FF" id="alg2.l5.1.1.m1.4.4.3.3.3.5" xref="alg2.l5.1.1.m1.4.4.3.3.4.cmml">,</mo><msub id="alg2.l5.1.1.m1.3.3.2.2.2.2" xref="alg2.l5.1.1.m1.3.3.2.2.2.2.cmml"><mi mathbackground="#E069FF" id="alg2.l5.1.1.m1.3.3.2.2.2.2.2" xref="alg2.l5.1.1.m1.3.3.2.2.2.2.2.cmml">ω</mi><mn mathbackground="#E069FF" id="alg2.l5.1.1.m1.3.3.2.2.2.2.3" xref="alg2.l5.1.1.m1.3.3.2.2.2.2.3.cmml">2</mn></msub><mo mathbackground="#E069FF" id="alg2.l5.1.1.m1.4.4.3.3.3.6" xref="alg2.l5.1.1.m1.4.4.3.3.4.cmml">,</mo><mi mathbackground="#E069FF" mathvariant="normal" id="alg2.l5.1.1.m1.1.1" xref="alg2.l5.1.1.m1.1.1.cmml">…</mi><mo mathbackground="#E069FF" id="alg2.l5.1.1.m1.4.4.3.3.3.7" xref="alg2.l5.1.1.m1.4.4.3.3.4.cmml">,</mo><msub id="alg2.l5.1.1.m1.4.4.3.3.3.3" xref="alg2.l5.1.1.m1.4.4.3.3.3.3.cmml"><mi mathbackground="#E069FF" id="alg2.l5.1.1.m1.4.4.3.3.3.3.2" xref="alg2.l5.1.1.m1.4.4.3.3.3.3.2.cmml">ω</mi><mrow id="alg2.l5.1.1.m1.4.4.3.3.3.3.3" xref="alg2.l5.1.1.m1.4.4.3.3.3.3.3.cmml"><mi mathbackground="#E069FF" id="alg2.l5.1.1.m1.4.4.3.3.3.3.3.2" xref="alg2.l5.1.1.m1.4.4.3.3.3.3.3.2.cmml">c</mi><mo mathbackground="#E069FF" id="alg2.l5.1.1.m1.4.4.3.3.3.3.3.1" xref="alg2.l5.1.1.m1.4.4.3.3.3.3.3.1.cmml">−</mo><mn mathbackground="#E069FF" id="alg2.l5.1.1.m1.4.4.3.3.3.3.3.3" xref="alg2.l5.1.1.m1.4.4.3.3.3.3.3.3.cmml">1</mn></mrow></msub><mo mathbackground="#E069FF" stretchy="false" id="alg2.l5.1.1.m1.4.4.3.3.3.8" xref="alg2.l5.1.1.m1.4.4.3.3.4.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg2.l5.1.1.m1.4b"><apply id="alg2.l5.1.1.m1.4.4.cmml" xref="alg2.l5.1.1.m1.4.4"><ci id="alg2.l5.1.1.m1.4.4.4.cmml" xref="alg2.l5.1.1.m1.4.4.4">←</ci><apply id="alg2.l5.1.1.m1.4.4.5.cmml" xref="alg2.l5.1.1.m1.4.4.5"><csymbol cd="ambiguous" id="alg2.l5.1.1.m1.4.4.5.1.cmml" xref="alg2.l5.1.1.m1.4.4.5">subscript</csymbol><ci id="alg2.l5.1.1.m1.4.4.5.2.cmml" xref="alg2.l5.1.1.m1.4.4.5.2">𝜔</ci><ci id="alg2.l5.1.1.m1.4.4.5.3.cmml" xref="alg2.l5.1.1.m1.4.4.5.3">𝑐</ci></apply><apply id="alg2.l5.1.1.m1.4.4.3.cmml" xref="alg2.l5.1.1.m1.4.4.3"><times id="alg2.l5.1.1.m1.4.4.3.4.cmml" xref="alg2.l5.1.1.m1.4.4.3.4"></times><ci id="alg2.l5.1.1.m1.4.4.3.5a.cmml" xref="alg2.l5.1.1.m1.4.4.3.5"><mtext mathbackground="#E069FF" id="alg2.l5.1.1.m1.4.4.3.5.cmml" xref="alg2.l5.1.1.m1.4.4.3.5">Aggregation</mtext></ci><set id="alg2.l5.1.1.m1.4.4.3.3.4.cmml" xref="alg2.l5.1.1.m1.4.4.3.3.3"><apply id="alg2.l5.1.1.m1.2.2.1.1.1.1.cmml" xref="alg2.l5.1.1.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="alg2.l5.1.1.m1.2.2.1.1.1.1.1.cmml" xref="alg2.l5.1.1.m1.2.2.1.1.1.1">subscript</csymbol><ci id="alg2.l5.1.1.m1.2.2.1.1.1.1.2.cmml" xref="alg2.l5.1.1.m1.2.2.1.1.1.1.2">𝜔</ci><cn type="integer" id="alg2.l5.1.1.m1.2.2.1.1.1.1.3.cmml" xref="alg2.l5.1.1.m1.2.2.1.1.1.1.3">1</cn></apply><apply id="alg2.l5.1.1.m1.3.3.2.2.2.2.cmml" xref="alg2.l5.1.1.m1.3.3.2.2.2.2"><csymbol cd="ambiguous" id="alg2.l5.1.1.m1.3.3.2.2.2.2.1.cmml" xref="alg2.l5.1.1.m1.3.3.2.2.2.2">subscript</csymbol><ci id="alg2.l5.1.1.m1.3.3.2.2.2.2.2.cmml" xref="alg2.l5.1.1.m1.3.3.2.2.2.2.2">𝜔</ci><cn type="integer" id="alg2.l5.1.1.m1.3.3.2.2.2.2.3.cmml" xref="alg2.l5.1.1.m1.3.3.2.2.2.2.3">2</cn></apply><ci id="alg2.l5.1.1.m1.1.1.cmml" xref="alg2.l5.1.1.m1.1.1">…</ci><apply id="alg2.l5.1.1.m1.4.4.3.3.3.3.cmml" xref="alg2.l5.1.1.m1.4.4.3.3.3.3"><csymbol cd="ambiguous" id="alg2.l5.1.1.m1.4.4.3.3.3.3.1.cmml" xref="alg2.l5.1.1.m1.4.4.3.3.3.3">subscript</csymbol><ci id="alg2.l5.1.1.m1.4.4.3.3.3.3.2.cmml" xref="alg2.l5.1.1.m1.4.4.3.3.3.3.2">𝜔</ci><apply id="alg2.l5.1.1.m1.4.4.3.3.3.3.3.cmml" xref="alg2.l5.1.1.m1.4.4.3.3.3.3.3"><minus id="alg2.l5.1.1.m1.4.4.3.3.3.3.3.1.cmml" xref="alg2.l5.1.1.m1.4.4.3.3.3.3.3.1"></minus><ci id="alg2.l5.1.1.m1.4.4.3.3.3.3.3.2.cmml" xref="alg2.l5.1.1.m1.4.4.3.3.3.3.3.2">𝑐</ci><cn type="integer" id="alg2.l5.1.1.m1.4.4.3.3.3.3.3.3.cmml" xref="alg2.l5.1.1.m1.4.4.3.3.3.3.3.3">1</cn></apply></apply></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l5.1.1.m1.4c">\omega_{c}\leftarrow\text{Aggregation}\{\omega_{1},\omega_{2},...,\omega_{c-1}\}</annotation></semantics></math></span><span id="alg2.l5.3" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg2.l6" class="ltx_listingline">
<span id="alg2.l6.1" class="ltx_text" style="font-size:90%;">    </span><span id="alg2.l6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span id="alg2.l6.3" class="ltx_text" style="font-size:90%;"> </span><math id="alg2.l6.m1.1" class="ltx_Math" alttext="e=1" display="inline"><semantics id="alg2.l6.m1.1a"><mrow id="alg2.l6.m1.1.1" xref="alg2.l6.m1.1.1.cmml"><mi mathsize="90%" id="alg2.l6.m1.1.1.2" xref="alg2.l6.m1.1.1.2.cmml">e</mi><mo mathsize="90%" id="alg2.l6.m1.1.1.1" xref="alg2.l6.m1.1.1.1.cmml">=</mo><mn mathsize="90%" id="alg2.l6.m1.1.1.3" xref="alg2.l6.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="alg2.l6.m1.1b"><apply id="alg2.l6.m1.1.1.cmml" xref="alg2.l6.m1.1.1"><eq id="alg2.l6.m1.1.1.1.cmml" xref="alg2.l6.m1.1.1.1"></eq><ci id="alg2.l6.m1.1.1.2.cmml" xref="alg2.l6.m1.1.1.2">𝑒</ci><cn type="integer" id="alg2.l6.m1.1.1.3.cmml" xref="alg2.l6.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l6.m1.1c">e=1</annotation></semantics></math><span id="alg2.l6.4" class="ltx_text" style="font-size:90%;"> </span><span id="alg2.l6.5" class="ltx_text ltx_font_bold" style="font-size:90%;">to</span><span id="alg2.l6.6" class="ltx_text" style="font-size:90%;"> </span><math id="alg2.l6.m2.1" class="ltx_Math" alttext="E-1" display="inline"><semantics id="alg2.l6.m2.1a"><mrow id="alg2.l6.m2.1.1" xref="alg2.l6.m2.1.1.cmml"><mi mathsize="90%" id="alg2.l6.m2.1.1.2" xref="alg2.l6.m2.1.1.2.cmml">E</mi><mo mathsize="90%" id="alg2.l6.m2.1.1.1" xref="alg2.l6.m2.1.1.1.cmml">−</mo><mn mathsize="90%" id="alg2.l6.m2.1.1.3" xref="alg2.l6.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="alg2.l6.m2.1b"><apply id="alg2.l6.m2.1.1.cmml" xref="alg2.l6.m2.1.1"><minus id="alg2.l6.m2.1.1.1.cmml" xref="alg2.l6.m2.1.1.1"></minus><ci id="alg2.l6.m2.1.1.2.cmml" xref="alg2.l6.m2.1.1.2">𝐸</ci><cn type="integer" id="alg2.l6.m2.1.1.3.cmml" xref="alg2.l6.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l6.m2.1c">E-1</annotation></semantics></math><span id="alg2.l6.7" class="ltx_text" style="font-size:90%;"> </span><span id="alg2.l6.8" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span><span id="alg2.l6.9" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg2.l7" class="ltx_listingline">
<span id="alg2.l7.1" class="ltx_text" style="font-size:90%;">         Backpropagate and update the local model </span><math id="alg2.l7.m1.1" class="ltx_Math" alttext="\omega_{c}^{e+1}\leftarrow\omega_{c}^{e}-\eta\nabla\mathcal{L}" display="inline"><semantics id="alg2.l7.m1.1a"><mrow id="alg2.l7.m1.1.1" xref="alg2.l7.m1.1.1.cmml"><msubsup id="alg2.l7.m1.1.1.2" xref="alg2.l7.m1.1.1.2.cmml"><mi mathsize="90%" id="alg2.l7.m1.1.1.2.2.2" xref="alg2.l7.m1.1.1.2.2.2.cmml">ω</mi><mi mathsize="90%" id="alg2.l7.m1.1.1.2.2.3" xref="alg2.l7.m1.1.1.2.2.3.cmml">c</mi><mrow id="alg2.l7.m1.1.1.2.3" xref="alg2.l7.m1.1.1.2.3.cmml"><mi mathsize="90%" id="alg2.l7.m1.1.1.2.3.2" xref="alg2.l7.m1.1.1.2.3.2.cmml">e</mi><mo mathsize="90%" id="alg2.l7.m1.1.1.2.3.1" xref="alg2.l7.m1.1.1.2.3.1.cmml">+</mo><mn mathsize="90%" id="alg2.l7.m1.1.1.2.3.3" xref="alg2.l7.m1.1.1.2.3.3.cmml">1</mn></mrow></msubsup><mo mathsize="90%" stretchy="false" id="alg2.l7.m1.1.1.1" xref="alg2.l7.m1.1.1.1.cmml">←</mo><mrow id="alg2.l7.m1.1.1.3" xref="alg2.l7.m1.1.1.3.cmml"><msubsup id="alg2.l7.m1.1.1.3.2" xref="alg2.l7.m1.1.1.3.2.cmml"><mi mathsize="90%" id="alg2.l7.m1.1.1.3.2.2.2" xref="alg2.l7.m1.1.1.3.2.2.2.cmml">ω</mi><mi mathsize="90%" id="alg2.l7.m1.1.1.3.2.2.3" xref="alg2.l7.m1.1.1.3.2.2.3.cmml">c</mi><mi mathsize="90%" id="alg2.l7.m1.1.1.3.2.3" xref="alg2.l7.m1.1.1.3.2.3.cmml">e</mi></msubsup><mo mathsize="90%" id="alg2.l7.m1.1.1.3.1" xref="alg2.l7.m1.1.1.3.1.cmml">−</mo><mrow id="alg2.l7.m1.1.1.3.3" xref="alg2.l7.m1.1.1.3.3.cmml"><mi mathsize="90%" id="alg2.l7.m1.1.1.3.3.2" xref="alg2.l7.m1.1.1.3.3.2.cmml">η</mi><mo lspace="0.167em" rspace="0em" id="alg2.l7.m1.1.1.3.3.1" xref="alg2.l7.m1.1.1.3.3.1.cmml">​</mo><mrow id="alg2.l7.m1.1.1.3.3.3" xref="alg2.l7.m1.1.1.3.3.3.cmml"><mo mathsize="90%" rspace="0.167em" id="alg2.l7.m1.1.1.3.3.3.1" xref="alg2.l7.m1.1.1.3.3.3.1.cmml">∇</mo><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="alg2.l7.m1.1.1.3.3.3.2" xref="alg2.l7.m1.1.1.3.3.3.2.cmml">ℒ</mi></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg2.l7.m1.1b"><apply id="alg2.l7.m1.1.1.cmml" xref="alg2.l7.m1.1.1"><ci id="alg2.l7.m1.1.1.1.cmml" xref="alg2.l7.m1.1.1.1">←</ci><apply id="alg2.l7.m1.1.1.2.cmml" xref="alg2.l7.m1.1.1.2"><csymbol cd="ambiguous" id="alg2.l7.m1.1.1.2.1.cmml" xref="alg2.l7.m1.1.1.2">superscript</csymbol><apply id="alg2.l7.m1.1.1.2.2.cmml" xref="alg2.l7.m1.1.1.2"><csymbol cd="ambiguous" id="alg2.l7.m1.1.1.2.2.1.cmml" xref="alg2.l7.m1.1.1.2">subscript</csymbol><ci id="alg2.l7.m1.1.1.2.2.2.cmml" xref="alg2.l7.m1.1.1.2.2.2">𝜔</ci><ci id="alg2.l7.m1.1.1.2.2.3.cmml" xref="alg2.l7.m1.1.1.2.2.3">𝑐</ci></apply><apply id="alg2.l7.m1.1.1.2.3.cmml" xref="alg2.l7.m1.1.1.2.3"><plus id="alg2.l7.m1.1.1.2.3.1.cmml" xref="alg2.l7.m1.1.1.2.3.1"></plus><ci id="alg2.l7.m1.1.1.2.3.2.cmml" xref="alg2.l7.m1.1.1.2.3.2">𝑒</ci><cn type="integer" id="alg2.l7.m1.1.1.2.3.3.cmml" xref="alg2.l7.m1.1.1.2.3.3">1</cn></apply></apply><apply id="alg2.l7.m1.1.1.3.cmml" xref="alg2.l7.m1.1.1.3"><minus id="alg2.l7.m1.1.1.3.1.cmml" xref="alg2.l7.m1.1.1.3.1"></minus><apply id="alg2.l7.m1.1.1.3.2.cmml" xref="alg2.l7.m1.1.1.3.2"><csymbol cd="ambiguous" id="alg2.l7.m1.1.1.3.2.1.cmml" xref="alg2.l7.m1.1.1.3.2">superscript</csymbol><apply id="alg2.l7.m1.1.1.3.2.2.cmml" xref="alg2.l7.m1.1.1.3.2"><csymbol cd="ambiguous" id="alg2.l7.m1.1.1.3.2.2.1.cmml" xref="alg2.l7.m1.1.1.3.2">subscript</csymbol><ci id="alg2.l7.m1.1.1.3.2.2.2.cmml" xref="alg2.l7.m1.1.1.3.2.2.2">𝜔</ci><ci id="alg2.l7.m1.1.1.3.2.2.3.cmml" xref="alg2.l7.m1.1.1.3.2.2.3">𝑐</ci></apply><ci id="alg2.l7.m1.1.1.3.2.3.cmml" xref="alg2.l7.m1.1.1.3.2.3">𝑒</ci></apply><apply id="alg2.l7.m1.1.1.3.3.cmml" xref="alg2.l7.m1.1.1.3.3"><times id="alg2.l7.m1.1.1.3.3.1.cmml" xref="alg2.l7.m1.1.1.3.3.1"></times><ci id="alg2.l7.m1.1.1.3.3.2.cmml" xref="alg2.l7.m1.1.1.3.3.2">𝜂</ci><apply id="alg2.l7.m1.1.1.3.3.3.cmml" xref="alg2.l7.m1.1.1.3.3.3"><ci id="alg2.l7.m1.1.1.3.3.3.1.cmml" xref="alg2.l7.m1.1.1.3.3.3.1">∇</ci><ci id="alg2.l7.m1.1.1.3.3.3.2.cmml" xref="alg2.l7.m1.1.1.3.3.3.2">ℒ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l7.m1.1c">\omega_{c}^{e+1}\leftarrow\omega_{c}^{e}-\eta\nabla\mathcal{L}</annotation></semantics></math><span id="alg2.l7.2" class="ltx_text" style="font-size:90%;">.
</span>
</div>
<div id="alg2.l8" class="ltx_listingline">
<span id="alg2.l8.1" class="ltx_text" style="font-size:90%;">    </span><span id="alg2.l8.2" class="ltx_text ltx_font_bold" style="font-size:90%;">end</span><span id="alg2.l8.3" class="ltx_text" style="font-size:90%;"> </span><span id="alg2.l8.4" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span>
</div>
<div id="alg2.l9" class="ltx_listingline">
<span id="alg2.l9.1" class="ltx_text" style="font-size:90%;">    Update the local model </span><math id="alg2.l9.m1.1" class="ltx_Math" alttext="\omega_{c}\leftarrow\omega_{c}^{E}" display="inline"><semantics id="alg2.l9.m1.1a"><mrow id="alg2.l9.m1.1.1" xref="alg2.l9.m1.1.1.cmml"><msub id="alg2.l9.m1.1.1.2" xref="alg2.l9.m1.1.1.2.cmml"><mi mathsize="90%" id="alg2.l9.m1.1.1.2.2" xref="alg2.l9.m1.1.1.2.2.cmml">ω</mi><mi mathsize="90%" id="alg2.l9.m1.1.1.2.3" xref="alg2.l9.m1.1.1.2.3.cmml">c</mi></msub><mo mathsize="90%" stretchy="false" id="alg2.l9.m1.1.1.1" xref="alg2.l9.m1.1.1.1.cmml">←</mo><msubsup id="alg2.l9.m1.1.1.3" xref="alg2.l9.m1.1.1.3.cmml"><mi mathsize="90%" id="alg2.l9.m1.1.1.3.2.2" xref="alg2.l9.m1.1.1.3.2.2.cmml">ω</mi><mi mathsize="90%" id="alg2.l9.m1.1.1.3.2.3" xref="alg2.l9.m1.1.1.3.2.3.cmml">c</mi><mi mathsize="90%" id="alg2.l9.m1.1.1.3.3" xref="alg2.l9.m1.1.1.3.3.cmml">E</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="alg2.l9.m1.1b"><apply id="alg2.l9.m1.1.1.cmml" xref="alg2.l9.m1.1.1"><ci id="alg2.l9.m1.1.1.1.cmml" xref="alg2.l9.m1.1.1.1">←</ci><apply id="alg2.l9.m1.1.1.2.cmml" xref="alg2.l9.m1.1.1.2"><csymbol cd="ambiguous" id="alg2.l9.m1.1.1.2.1.cmml" xref="alg2.l9.m1.1.1.2">subscript</csymbol><ci id="alg2.l9.m1.1.1.2.2.cmml" xref="alg2.l9.m1.1.1.2.2">𝜔</ci><ci id="alg2.l9.m1.1.1.2.3.cmml" xref="alg2.l9.m1.1.1.2.3">𝑐</ci></apply><apply id="alg2.l9.m1.1.1.3.cmml" xref="alg2.l9.m1.1.1.3"><csymbol cd="ambiguous" id="alg2.l9.m1.1.1.3.1.cmml" xref="alg2.l9.m1.1.1.3">superscript</csymbol><apply id="alg2.l9.m1.1.1.3.2.cmml" xref="alg2.l9.m1.1.1.3"><csymbol cd="ambiguous" id="alg2.l9.m1.1.1.3.2.1.cmml" xref="alg2.l9.m1.1.1.3">subscript</csymbol><ci id="alg2.l9.m1.1.1.3.2.2.cmml" xref="alg2.l9.m1.1.1.3.2.2">𝜔</ci><ci id="alg2.l9.m1.1.1.3.2.3.cmml" xref="alg2.l9.m1.1.1.3.2.3">𝑐</ci></apply><ci id="alg2.l9.m1.1.1.3.3.cmml" xref="alg2.l9.m1.1.1.3.3">𝐸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l9.m1.1c">\omega_{c}\leftarrow\omega_{c}^{E}</annotation></semantics></math><span id="alg2.l9.2" class="ltx_text" style="font-size:90%;">.
</span>
</div>
<div id="alg2.l10" class="ltx_listingline">
<span id="alg2.l10.1" class="ltx_text" style="font-size:90%;">    Client </span><math id="alg2.l10.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="alg2.l10.m1.1a"><mi mathsize="90%" id="alg2.l10.m1.1.1" xref="alg2.l10.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="alg2.l10.m1.1b"><ci id="alg2.l10.m1.1.1.cmml" xref="alg2.l10.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.l10.m1.1c">c</annotation></semantics></math><span id="alg2.l10.2" class="ltx_text" style="font-size:90%;"> sends </span><math id="alg2.l10.m2.1" class="ltx_Math" alttext="\omega_{c}" display="inline"><semantics id="alg2.l10.m2.1a"><msub id="alg2.l10.m2.1.1" xref="alg2.l10.m2.1.1.cmml"><mi mathsize="90%" id="alg2.l10.m2.1.1.2" xref="alg2.l10.m2.1.1.2.cmml">ω</mi><mi mathsize="90%" id="alg2.l10.m2.1.1.3" xref="alg2.l10.m2.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="alg2.l10.m2.1b"><apply id="alg2.l10.m2.1.1.cmml" xref="alg2.l10.m2.1.1"><csymbol cd="ambiguous" id="alg2.l10.m2.1.1.1.cmml" xref="alg2.l10.m2.1.1">subscript</csymbol><ci id="alg2.l10.m2.1.1.2.cmml" xref="alg2.l10.m2.1.1.2">𝜔</ci><ci id="alg2.l10.m2.1.1.3.cmml" xref="alg2.l10.m2.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l10.m2.1c">\omega_{c}</annotation></semantics></math><span id="alg2.l10.3" class="ltx_text" style="font-size:90%;"> to the next client </span><span id="alg2.l10.4" class="ltx_text" style="font-size:90%;background-color:#E069FF;">and all other clients</span><span id="alg2.l10.5" class="ltx_text" style="font-size:90%;">.
</span>
</div>
<div id="alg2.l11" class="ltx_listingline">
<span id="alg2.l11.2" class="ltx_text" style="font-size:90%;">    </span><span id="alg2.l11.1" class="ltx_text" style="font-size:90%;float:right;"><math id="alg2.l11.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg2.l11.1.m1.1a"><mo id="alg2.l11.1.m1.1.1" xref="alg2.l11.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg2.l11.1.m1.1b"><ci id="alg2.l11.1.m1.1.1.cmml" xref="alg2.l11.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.l11.1.m1.1c">\triangleright</annotation></semantics></math> pointing vs. broadcast
</span>
</div>
<div id="alg2.l12" class="ltx_listingline">
<span id="alg2.l12.1" class="ltx_text ltx_font_bold" style="font-size:90%;">end</span><span id="alg2.l12.2" class="ltx_text" style="font-size:90%;"> </span><span id="alg2.l12.3" class="ltx_text ltx_font_bold" style="font-size:90%;">while</span>
</div>
</div>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Variants of Decentralized Federated Learning</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In Section <a href="#S3" title="III Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, we provide a comprehensive definition, introduction, and propose two paradigm for DFLs. In this section, we review the state-of-the-art development in DFL, with a specific focus on its diverse applications across various domains and its real-world deployment. Taking inspiration from the CFL variants and considering the underlying network topologies depicted in Fig. <a href="#S3.F3" title="Figure 3 ‣ III-C Network Topology ‣ III Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we propose several viable topology variants for DFL. These topology variants serve as alternative options for researchers to consider when deploying DFL. We discuss the advantages and limitations associated with each variant, enabling researchers to make informed decisions regarding the most suitable topology for specific usage scenarios.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<p id="S4.F4.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S4.F4.1.1" class="ltx_text"><img src="/html/2306.01603/assets/Figure/Parameter_Space.png" id="S4.F4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="182" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Illustration of the two paradigms, <span id="S4.F4.6.1" class="ltx_text ltx_font_typewriter">Continual</span> and <span id="S4.F4.7.2" class="ltx_text ltx_font_typewriter">Aggregate</span>, for sequential pointing line DFL in the parameter space, showcasing their respective learning and communication processes. The length of the arrow represents both the learning difficulty and the magnitude of the model parameters that undergo changes during learning, which can be measured using the <math id="S4.F4.3.m1.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="S4.F4.3.m1.1b"><msub id="S4.F4.3.m1.1.1" xref="S4.F4.3.m1.1.1.cmml"><mi mathvariant="normal" id="S4.F4.3.m1.1.1.2" xref="S4.F4.3.m1.1.1.2.cmml">ℓ</mi><mn id="S4.F4.3.m1.1.1.3" xref="S4.F4.3.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.F4.3.m1.1c"><apply id="S4.F4.3.m1.1.1.cmml" xref="S4.F4.3.m1.1.1"><csymbol cd="ambiguous" id="S4.F4.3.m1.1.1.1.cmml" xref="S4.F4.3.m1.1.1">subscript</csymbol><ci id="S4.F4.3.m1.1.1.2.cmml" xref="S4.F4.3.m1.1.1.2">ℓ</ci><cn type="integer" id="S4.F4.3.m1.1.1.3.cmml" xref="S4.F4.3.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.3.m1.1d">\ell_{2}</annotation></semantics></math> norm. Shorter arrows are desired as they indicate more accessible, stable, and accurate model learning and convergence. Excessively long arrows suggest that the given loss function and learning rate may not produce the desired model outcome.</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Some Inspiring DFLs with Different Protocols, Topologies, Paradigms, and Variants.</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Literature</span></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Year</span></th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">Paradigm</span></th>
<th id="S4.T3.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.1.4.1.1" class="ltx_p" style="width:90.0pt;"><span id="S4.T3.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Type</span></span>
</span>
</th>
<th id="S4.T3.1.1.1.5" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.1.5.1.1" class="ltx_p"><span id="S4.T3.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Highlight</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr" style="background-color:#D9D9D9;">
<td id="S4.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.2.1.1.1" class="ltx_text" style="background-color:#D9D9D9;">Chang <span id="S4.T3.1.2.1.1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite></span></td>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.2.1.2.1" class="ltx_text" style="background-color:#D9D9D9;">2018</span></td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.2.1.3.1" class="ltx_text ltx_font_typewriter" style="background-color:#D9D9D9;">Continual</span></td>
<td id="S4.T3.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.2.1.4.1" class="ltx_inline-block ltx_align_top" style="background-color:#D9D9D9;">
<span id="S4.T3.1.2.1.4.1.1" class="ltx_p" style="width:90.0pt;">
<span id="S4.T3.1.2.1.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I1" class="ltx_itemize">
<span id="S4.I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I1.i1.p1" class="ltx_para">
<span id="S4.I1.i1.p1.1" class="ltx_p">Sequential pointing line</span>
</span></span>
<span id="S4.I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I1.i2.p1" class="ltx_para">
<span id="S4.I1.i2.p1.1" class="ltx_p">Cycle pointing ring</span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S4.T3.1.2.1.5" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.2.1.5.1" class="ltx_inline-block ltx_align_top" style="background-color:#D9D9D9;">
<span id="S4.T3.1.2.1.5.1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I2" class="ltx_itemize">
<span id="S4.I2.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I2.i1.p1" class="ltx_para">
<span id="S4.I2.i1.p1.1" class="ltx_p">Introduced system heterogeneity artificially.</span>
</span></span>
</span>
</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<td id="S4.T3.1.3.2.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">Sheller <span id="S4.T3.1.3.2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>
</td>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">2019</td>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.3.2.3.1" class="ltx_text ltx_font_typewriter">Continual</span></td>
<td id="S4.T3.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.3.2.4.1.1" class="ltx_p" style="width:90.0pt;">
<span id="S4.T3.1.3.2.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I3" class="ltx_itemize">
<span id="S4.I3.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I3.i1.p1" class="ltx_para">
<span id="S4.I3.i1.p1.1" class="ltx_p">Sequential pointing line</span>
</span></span>
<span id="S4.I3.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I3.i2.p1" class="ltx_para">
<span id="S4.I3.i2.p1.1" class="ltx_p">Cycle pointing ring</span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S4.T3.1.3.2.5" class="ltx_td ltx_align_justify" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.3.2.5.1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I4" class="ltx_itemize">
<span id="S4.I4.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I4.i1.p1" class="ltx_para">
<span id="S4.I4.i1.p1.1" class="ltx_p">Obtained a conclusion that catastrophic forgetting worsens as the number of clients increases.</span>
</span></span>
</span>
</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr" style="background-color:#D9D9D9;">
<td id="S4.T3.1.4.3.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.4.3.1.1" class="ltx_text" style="background-color:#D9D9D9;">Sheller <span id="S4.T3.1.4.3.1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite></span></td>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.4.3.2.1" class="ltx_text" style="background-color:#D9D9D9;">2020</span></td>
<td id="S4.T3.1.4.3.3" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.4.3.3.1" class="ltx_text ltx_font_typewriter" style="background-color:#D9D9D9;">Continual</span></td>
<td id="S4.T3.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.4.3.4.1" class="ltx_inline-block ltx_align_top" style="background-color:#D9D9D9;">
<span id="S4.T3.1.4.3.4.1.1" class="ltx_p" style="width:90.0pt;">
<span id="S4.T3.1.4.3.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I5" class="ltx_itemize">
<span id="S4.I5.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I5.i1.p1" class="ltx_para">
<span id="S4.I5.i1.p1.1" class="ltx_p">Sequential pointing line</span>
</span></span>
<span id="S4.I5.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I5.i2.p1" class="ltx_para">
<span id="S4.I5.i2.p1.1" class="ltx_p">Cycle pointing ring</span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S4.T3.1.4.3.5" class="ltx_td ltx_align_justify" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.4.3.5.1" class="ltx_inline-block ltx_align_top" style="background-color:#D9D9D9;">
<span id="S4.T3.1.4.3.5.1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I6" class="ltx_itemize">
<span id="S4.I6.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I6.i1.p1" class="ltx_para">
<span id="S4.I6.i1.p1.1" class="ltx_p">Considered the DFL framework to output a final model approach.</span>
</span></span>
</span>
</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<td id="S4.T3.1.5.4.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">Huang <span id="S4.T3.1.5.4.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>
</td>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">2022</td>
<td id="S4.T3.1.5.4.3" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.5.4.3.1" class="ltx_text ltx_font_typewriter">Continual</span></td>
<td id="S4.T3.1.5.4.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.5.4.4.1.1" class="ltx_p" style="width:90.0pt;">
<span id="S4.T3.1.5.4.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I7" class="ltx_itemize">
<span id="S4.I7.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I7.i1.p1" class="ltx_para">
<span id="S4.I7.i1.p1.1" class="ltx_p">Sequential pointing line</span>
</span></span>
<span id="S4.I7.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I7.i2.p1" class="ltx_para">
<span id="S4.I7.i2.p1.1" class="ltx_p">Cycle pointing ring</span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S4.T3.1.5.4.5" class="ltx_td ltx_align_justify" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.5.4.5.1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I8" class="ltx_itemize">
<span id="S4.I8.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I8.i1.p1" class="ltx_para">
<span id="S4.I8.i1.p1.1" class="ltx_p">Introduced synaptic intelligence in <span id="S4.I8.i1.p1.1.1" class="ltx_text ltx_font_typewriter">Continual</span> DFL to effectively improve model stability, especially for sequential pointing line topology.</span>
</span></span>
</span>
</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.6.5" class="ltx_tr" style="background-color:#D9D9D9;">
<td id="S4.T3.1.6.5.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.6.5.1.1" class="ltx_text" style="background-color:#D9D9D9;">Yuan <span id="S4.T3.1.6.5.1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite></span></td>
<td id="S4.T3.1.6.5.2" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.6.5.2.1" class="ltx_text" style="background-color:#D9D9D9;">2023</span></td>
<td id="S4.T3.1.6.5.3" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.6.5.3.1" class="ltx_text ltx_font_typewriter" style="background-color:#D9D9D9;">Continual</span></td>
<td id="S4.T3.1.6.5.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.6.5.4.1" class="ltx_inline-block ltx_align_top" style="background-color:#D9D9D9;">
<span id="S4.T3.1.6.5.4.1.1" class="ltx_p" style="width:90.0pt;">
<span id="S4.T3.1.6.5.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I9" class="ltx_itemize">
<span id="S4.I9.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I9.i1.p1" class="ltx_para">
<span id="S4.I9.i1.p1.1" class="ltx_p">Random gossip ring</span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S4.T3.1.6.5.5" class="ltx_td ltx_align_justify" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.6.5.5.1" class="ltx_inline-block ltx_align_top" style="background-color:#D9D9D9;">
<span id="S4.T3.1.6.5.5.1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I10" class="ltx_itemize">
<span id="S4.I10.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I10.i1.p1" class="ltx_para">
<span id="S4.I10.i1.p1.1" class="ltx_p">Considered the highly dynamic and random nature of vehicle connectivity in vehicular networks and employed gossip-based communication to simulate this characteristic when deploying <span id="S4.I10.i1.p1.1.1" class="ltx_text ltx_font_typewriter">Continual</span> DFL.</span>
</span></span>
<span id="S4.I10.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I10.i2.p1" class="ltx_para">
<span id="S4.I10.i2.p1.1" class="ltx_p">Provided a comprehensive comparison between CFL and DFL, such as knowledge dissemination mechanism, communication complexity, generalizability, compatibility, overhead, hidden concerns, etc.</span>
</span></span>
</span>
</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.7.6" class="ltx_tr">
<td id="S4.T3.1.7.6.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">Assran <span id="S4.T3.1.7.6.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>
</td>
<td id="S4.T3.1.7.6.2" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">2019</td>
<td id="S4.T3.1.7.6.3" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.7.6.3.1" class="ltx_text ltx_font_typewriter">Aggregate</span></td>
<td id="S4.T3.1.7.6.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.7.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.7.6.4.1.1" class="ltx_p" style="width:90.0pt;">
<span id="S4.T3.1.7.6.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I11" class="ltx_itemize">
<span id="S4.I11.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I11.i1.p1" class="ltx_para">
<span id="S4.I11.i1.p1.1" class="ltx_p">Cycle broadcast-gossip mesh</span>
</span></span>
<span id="S4.I11.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I11.i2.p1" class="ltx_para">
<span id="S4.I11.i2.p1.1" class="ltx_p">Parallel broadcast mesh</span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S4.T3.1.7.6.5" class="ltx_td ltx_align_justify" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.7.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.7.6.5.1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I12" class="ltx_itemize">
<span id="S4.I12.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I12.i1.p1" class="ltx_para">
<span id="S4.I12.i1.p1.1" class="ltx_p">Performed a comparison of broadcast-gossip and broadcast protocols.</span>
</span></span>
</span>
</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.8.7" class="ltx_tr" style="background-color:#D9D9D9;">
<td id="S4.T3.1.8.7.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.8.7.1.1" class="ltx_text" style="background-color:#D9D9D9;">Roy <span id="S4.T3.1.8.7.1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite></span></td>
<td id="S4.T3.1.8.7.2" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.8.7.2.1" class="ltx_text" style="background-color:#D9D9D9;">2019</span></td>
<td id="S4.T3.1.8.7.3" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.8.7.3.1" class="ltx_text ltx_font_typewriter" style="background-color:#D9D9D9;">Aggregate</span></td>
<td id="S4.T3.1.8.7.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.8.7.4.1" class="ltx_inline-block ltx_align_top" style="background-color:#D9D9D9;">
<span id="S4.T3.1.8.7.4.1.1" class="ltx_p" style="width:90.0pt;">
<span id="S4.T3.1.8.7.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I13" class="ltx_itemize">
<span id="S4.I13.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I13.i1.p1" class="ltx_para">
<span id="S4.I13.i1.p1.1" class="ltx_p">Random broadcast-gossip mesh</span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S4.T3.1.8.7.5" class="ltx_td ltx_align_justify" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.8.7.5.1" class="ltx_inline-block ltx_align_top" style="background-color:#D9D9D9;">
<span id="S4.T3.1.8.7.5.1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I14" class="ltx_itemize">
<span id="S4.I14.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I14.i1.p1" class="ltx_para">
<span id="S4.I14.i1.p1.1" class="ltx_p">Pre-requested model versions from other clients.</span>
</span></span>
<span id="S4.I14.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I14.i2.p1" class="ltx_para">
<span id="S4.I14.i2.p1.1" class="ltx_p">Considered the scenario where the DFL framework outputs a model to a new client.</span>
</span></span>
</span>
</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.9.8" class="ltx_tr">
<td id="S4.T3.1.9.8.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">Pappas <span id="S4.T3.1.9.8.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>
</td>
<td id="S4.T3.1.9.8.2" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">2021</td>
<td id="S4.T3.1.9.8.3" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.9.8.3.1" class="ltx_text ltx_font_typewriter">Aggregate</span></td>
<td id="S4.T3.1.9.8.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.9.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.9.8.4.1.1" class="ltx_p" style="width:90.0pt;">
<span id="S4.T3.1.9.8.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I15" class="ltx_itemize">
<span id="S4.I15.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I15.i1.p1" class="ltx_para">
<span id="S4.I15.i1.p1.1" class="ltx_p">Parallel broadcast star</span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S4.T3.1.9.8.5" class="ltx_td ltx_align_justify" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.9.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.9.8.5.1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I16" class="ltx_itemize">
<span id="S4.I16.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I16.i1.p1" class="ltx_para">
<span id="S4.I16.i1.p1.1" class="ltx_p">Proposed a framework that combines star DFL with split learning.</span>
</span></span>
</span>
</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.10.9" class="ltx_tr" style="background-color:#D9D9D9;">
<td id="S4.T3.1.10.9.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.10.9.1.1" class="ltx_text" style="background-color:#D9D9D9;">Warnat <span id="S4.T3.1.10.9.1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite></span></td>
<td id="S4.T3.1.10.9.2" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.10.9.2.1" class="ltx_text" style="background-color:#D9D9D9;">2021</span></td>
<td id="S4.T3.1.10.9.3" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.10.9.3.1" class="ltx_text ltx_font_typewriter" style="background-color:#D9D9D9;">Aggregate</span></td>
<td id="S4.T3.1.10.9.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.10.9.4.1" class="ltx_inline-block ltx_align_top" style="background-color:#D9D9D9;">
<span id="S4.T3.1.10.9.4.1.1" class="ltx_p" style="width:90.0pt;">
<span id="S4.T3.1.10.9.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I17" class="ltx_itemize">
<span id="S4.I17.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I17.i1.p1" class="ltx_para">
<span id="S4.I17.i1.p1.1" class="ltx_p">Dynamic pointing star</span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S4.T3.1.10.9.5" class="ltx_td ltx_align_justify" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.10.9.5.1" class="ltx_inline-block ltx_align_top" style="background-color:#D9D9D9;">
<span id="S4.T3.1.10.9.5.1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I18" class="ltx_itemize">
<span id="S4.I18.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I18.i1.p1" class="ltx_para">
<span id="S4.I18.i1.p1.1" class="ltx_p">Elected a leader dynamically via a blockchain smart contract that is used to aggregate model parameters.</span>
</span></span>
</span>
</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.11.10" class="ltx_tr">
<td id="S4.T3.1.11.10.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">Shi <span id="S4.T3.1.11.10.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>
</td>
<td id="S4.T3.1.11.10.2" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">2021</td>
<td id="S4.T3.1.11.10.3" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.11.10.3.1" class="ltx_text ltx_font_typewriter">Aggregate</span></td>
<td id="S4.T3.1.11.10.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.11.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.11.10.4.1.1" class="ltx_p" style="width:90.0pt;">
<span id="S4.T3.1.11.10.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I19" class="ltx_itemize">
<span id="S4.I19.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I19.i1.p1" class="ltx_para">
<span id="S4.I19.i1.p1.1" class="ltx_p">Cycle broadcast-gossip hybrid</span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S4.T3.1.11.10.5" class="ltx_td ltx_align_justify" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.11.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.11.10.5.1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I20" class="ltx_itemize">
<span id="S4.I20.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I20.i1.p1" class="ltx_para">
<span id="S4.I20.i1.p1.1" class="ltx_p">Analyzed the convergence of broadcast-gossip in a hybrid network.</span>
</span></span>
</span>
</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.12.11" class="ltx_tr" style="background-color:#D9D9D9;">
<td id="S4.T3.1.12.11.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.12.11.1.1" class="ltx_text" style="background-color:#D9D9D9;">Chen <span id="S4.T3.1.12.11.1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite></span></td>
<td id="S4.T3.1.12.11.2" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.12.11.2.1" class="ltx_text" style="background-color:#D9D9D9;">2022</span></td>
<td id="S4.T3.1.12.11.3" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.12.11.3.1" class="ltx_text ltx_font_typewriter" style="background-color:#D9D9D9;">Aggregate</span></td>
<td id="S4.T3.1.12.11.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.12.11.4.1" class="ltx_inline-block ltx_align_top" style="background-color:#D9D9D9;">
<span id="S4.T3.1.12.11.4.1.1" class="ltx_p" style="width:90.0pt;">
<span id="S4.T3.1.12.11.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I21" class="ltx_itemize">
<span id="S4.I21.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I21.i1.p1" class="ltx_para">
<span id="S4.I21.i1.p1.1" class="ltx_p">Cycle broadcast mesh</span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S4.T3.1.12.11.5" class="ltx_td ltx_align_justify" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.12.11.5.1" class="ltx_inline-block ltx_align_top" style="background-color:#D9D9D9;">
<span id="S4.T3.1.12.11.5.1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I22" class="ltx_itemize">
<span id="S4.I22.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I22.i1.p1" class="ltx_para">
<span id="S4.I22.i1.p1.1" class="ltx_p">Introduced the superposition property of the analog scheme to improve the parallelism of communication, which enables a significant reduction communication rounds.</span>
</span></span>
</span>
</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.13.12" class="ltx_tr">
<td id="S4.T3.1.13.12.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">Wang <span id="S4.T3.1.13.12.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>
</td>
<td id="S4.T3.1.13.12.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">2022</td>
<td id="S4.T3.1.13.12.3" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.1.13.12.3.1" class="ltx_text ltx_font_typewriter">Aggregate</span></td>
<td id="S4.T3.1.13.12.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.13.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.13.12.4.1.1" class="ltx_p" style="width:90.0pt;">
<span id="S4.T3.1.13.12.4.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I23" class="ltx_itemize">
<span id="S4.I23.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I23.i1.p1" class="ltx_para">
<span id="S4.I23.i1.p1.1" class="ltx_p">Dynamic parallel broadcast-gossip hybrid</span>
</span></span>
</span>
</span></span>
</span>
</td>
<td id="S4.T3.1.13.12.5" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.1.13.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.13.12.5.1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:433.6pt;">
<span id="S4.I24" class="ltx_itemize">
<span id="S4.I24.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I24.i1.p1" class="ltx_para">
<span id="S4.I24.i1.p1.1" class="ltx_p">Promoted more frequent communication in the central client to achieve fast convergence.</span>
</span></span>
<span id="S4.I24.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I24.i2.p1" class="ltx_para">
<span id="S4.I24.i2.p1.1" class="ltx_p">Promoted less frequent communication in the other clients to achieve low communication latency.</span>
</span></span>
<span id="S4.I24.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I24.i3.p1" class="ltx_para">
<span id="S4.I24.i3.p1.1" class="ltx_p">The proposed algorithm is applicable and generalized to all hybrid networks.</span>
</span></span>
</span>
</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">State-of-the-art Development</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The development of a DFL framework relies on several key factors, such as relevant application scenarios, sources of information acquisition, information processing units, perceptual prediction modules, among others. With the establishment of the theoretical framework for networks, DFL has been adopted in various application domains, including vehicles, research institutions, and mobile service interconnection networks.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">1) Connected and automated vehicles (CAV)</span> serve as a robust hardware infrastructure for DFL, leveraging onboard batteries, diverse sensors, computing units, storage devices, and more. Existing vehicle networking frameworks, such as vehicle-to-vehicle (V2V), have also laid the foundation for communication and networking experiences in DFL for CAV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>. Referred to as V2V FL, this approach enables the exchange and sharing of up-to-date knowledge among vehicles and has been explored in recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>, <a href="#bib.bib88" title="" class="ltx_ref">88</a>, <a href="#bib.bib89" title="" class="ltx_ref">89</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>, <a href="#bib.bib91" title="" class="ltx_ref">91</a>, <a href="#bib.bib92" title="" class="ltx_ref">92</a>, <a href="#bib.bib93" title="" class="ltx_ref">93</a>, <a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>. Lu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> proposed a vehicular DFL approach with a focus on privacy protection and mitigating data leakage risks in vehicular cyber-physical systems (VCPS). In their framework, roadside units (RSUs) are responsible for forwarding vehicle identities, vehicle data retrieval information, data profiles, data sharing requests, and related tasks. Once the V2V connection is established through the RSU intermediary, the model data is directly transmitted to the requesting vehicle.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_italic">2) Research institutions</span> are also inclined towards DFL frameworks over CFL due to their abundant data, computational resources, and storage capabilities. As key stakeholders in research institutions, researchers play a crucial role in data collection, model training, data analysis, characterization, and providing experimental results and solutions. Unlike traditional server-centric approaches, researchers have the flexibility to observe, analyze, fine-tune, and match models manually, offering more control and adaptability. Research institutions, particularly in the healthcare sector, widely employ DFL frameworks in various studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib96" title="" class="ltx_ref">96</a>, <a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>. Warnat-Herresthal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> introduced a DFL framework called Swarm Learning, which addresses four use cases of heterogeneous diseases, including COVID-19, tuberculosis, leukemia, and lung pathologies. This framework incorporates a blockchain smart contract for enhanced security and dynamically selects a leader for aggregating model parameters in each iteration.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_italic">3) Mobile services</span> based on IoT devices provide a significant application scenario for DFL, leveraging the capabilities of smartphones, laptops, and tablets. These mobile IoT devices are equipped with various sensors, such as global positioning system (GPS), inertial measurement unit (IMU), cameras, sound sensors, and magnetic sensors, enabling them to acquire diverse sources of information. Unlike the relatively fixed connectivity of CAVs, mobile IoT devices offer more flexible systems and platforms to support a wide range of applications. In recent studies, DFL frameworks have been developed specifically for mobile IoT devices, aiming to leverage their computational power and sensor capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>, <a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>. While the traditional example of CFL, such as Google mobile keyboard prediction, is well-known <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, the transfer of such applications to DFLs is of great interest. For instance, building DFLs among individuals with similar professions, such as doctors, lawyers, or engineers, can enable personalized word recommendations tailored to their specific needs. Belal <span id="S4.SS1.p4.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> developed a smartphone-based DFL personalized recommendation system for New York City attractions and movies. By sharing model parameters with neighbors who have similar interests, the system achieves higher hit rates and faster convergence, enhancing the recommendation accuracy and user experience.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.01603/assets/Figure/network_ring.png" id="S4.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="299" height="150" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure">(a) </span>Line/Ring</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.01603/assets/Figure/network_mesh.png" id="S4.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="299" height="150" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure">(b) </span>Mesh</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.01603/assets/Figure/network_star.png" id="S4.F5.sf3.g1" class="ltx_graphics ltx_img_landscape" width="299" height="150" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure">(c) </span>Star</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.01603/assets/Figure/network_hybrid.png" id="S4.F5.sf4.g1" class="ltx_graphics ltx_img_landscape" width="299" height="150" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure">(d) </span>Hybrid</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Illustrations of imagined DFL network topologies in the real world: (a) line/ring, (b) mesh, (c) star, and (d) hybrid. The red dots represent clients, which can be universities, institutions, or organizations in some of the major cities in the world (determined by population). The blue lines depict the communication network among these clients. Depending on the chosen topology, the communication networks exhibit different communication distances, number of communication links, complexity, and other characteristics.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Variant: Line</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The base variant of DFL can be considered as a sequential pointing line, depicted in Fig. <a href="#S3.F3" title="Figure 3 ‣ III-C Network Topology ‣ III Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(a) and Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-A State-of-the-art Development ‣ IV Variants of Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(a). This topology serves as the simplest and most straightforward illustration and comparison in this paper, as demonstrated in Table <a href="#S3.T2" title="Table II ‣ III-C Network Topology ‣ III Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>(a), (b), and Algorithm <a href="#alg1" title="Algorithm 1 ‣ III-D Paradigm Proposal ‣ III Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The line variant is frequently used as a baseline for comparison due to its ease of implementation, intuitiveness, and efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>. However, it has notable limitations, such as the inability to accommodate continuous learning of new knowledge within the system, the risk of catastrophic forgetting in the <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">Continual</span> paradigm, or redundant and excessive learning in the <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_typewriter">Aggregate</span> paradigm, as well as limited generalization ability for starting clients and the vulnerability to a SPoF. Furthermore, the line variant lacks cyclic connections, limiting each client to a single iteration and preventing the system from fully converging. In particular, in the line variant, the clients at the front of the queue will have worse model performance. Given its prominent disadvantages and advantages, it can serve as a baseline or initial implementation for further research and development.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Variant: Ring</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The ring variant corresponds to the cycle pointing line DFL, as depicted in Fig. <a href="#S3.F3" title="Figure 3 ‣ III-C Network Topology ‣ III Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(b) and Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-A State-of-the-art Development ‣ IV Variants of Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(a). The cyclic form is commonly used in DFL as the model needs to be trained between clients to acquire new knowledge collected from other clients, thereby enhancing generalization. Based on the framework, not all past models need to be transferred for aggregation in each communication since many of them may already be outdated. The ring variant not only inherits the simplicity of the line variant but also becomes a popular approach in various research papers due to its ability to iterate indefinitely until convergence.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The ring topology is already considered mature in decentralized learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> and is beginning to gain traction in DFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite>. For instance, Chang <span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> proposed two heuristics for DFL, including sequential pointing communication on each client for one iteration and multiple iterations to obtain the final model. Similarly, Sheller <span id="S4.SS3.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> also considered sequential pointing or cycle continual learning in the client to generate the final model. Nguyen <span id="S4.SS3.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> applied cycle pointing DFL to autonomous driving applications. Yuan <span id="S4.SS3.p2.1.4" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> proposed a random ring topology DFL framework, named FedPC, based on the gossip communication protocol for naturalistic driving action recognition. FedPC emphasizes the highly dynamic, random, and data heterogeneous nature of vehicle connections in this context.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Variant: Mesh</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">A multidirectional ring, also known as a fully connected topology, or be called mesh, is a variant of the ring basic variant, depicted in Fig. <a href="#S3.F3" title="Figure 3 ‣ III-C Network Topology ‣ III Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(c) and Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-A State-of-the-art Development ‣ IV Variants of Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(b). In the ring variant, each client needs to transmit multiple model parameters in each communication round, which can pose a burden on the network bandwidth. In contrast, the mesh variant requires each client to transmit its local model parameters to all other clients in each communication round. This approach entails higher communication frequency for the clients while also reducing the size of model packets transmitted per communication. The higher communication frequency and larger per-communication data packet overhead have their respective advantages and disadvantages, which can be traded off depending on the specific application context. However, when compared to the ring variant, the mesh variant significantly mitigates the impact of SPoF, which is a notable advantage of this variant.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">Recent research has witnessed the emergence of mesh-based DFL approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib103" title="" class="ltx_ref">103</a>, <a href="#bib.bib92" title="" class="ltx_ref">92</a>, <a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>. Assran <span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> proposed Stochastic Gradient Push (SGP), a parallel broadcast-gossip mesh DFL approach. In the broadcast-gossip iteration, clients in SGP send their trained local models to a sparse selection of other clients in a parallel manner, and they also receive models from other selected clients. Each client then performs a weighted aggregation of its local model with the received models. Roy <span id="S4.SS4.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite> introduced the BrainTorrent framework, in which a requesting client communicates with all clients to obtain information about available model versions, and clients with new versions send their models to the requesting client for aggregation.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS5.4.1.1" class="ltx_text">IV-E</span> </span><span id="S4.SS5.5.2" class="ltx_text ltx_font_italic">Variant: Star</span>
</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">The star variant resembles the CFL model, where one client assumes the role of the server to coordinate and interact with other clients, as depicted in Fig. <a href="#S3.F3" title="Figure 3 ‣ III-C Network Topology ‣ III Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(d) and Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-A State-of-the-art Development ‣ IV Variants of Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(c). The star variant operates in two different modes. In the first mode, similar to CFL, the central client is responsible for receiving, aggregating, and distributing the local models. However, unlike CFL, the central client also generates original data and utilizes the models for perception and decision-making. This mode emphasizes a family-like relationship, where one member has more computational and communication power to assist the other clients. In the second mode of operation, the focus is on geographic interoperability among clients. As some clients in the community are geographically dispersed, there is a client that serves as the geographical center for these clients. To conserve communication resources, the surrounding clients transmit their models to the central client, which then forwards the models to the other clients.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">Pappas <span id="S4.SS5.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> introduced a split learning framework within a star DFL architecture, where clients train different layers of a model and update the model parameters with the central client. This approach allows for distributed model training and collaboration among clients. Another example of a star variant is the Swarm Learning framework proposed by Warnat-Herresthal <span id="S4.SS5.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, which involves the dynamic election of a leader to aggregate model parameters. In Swarm Learning, the leader plays a central role in coordinating the aggregation process and facilitating collaboration among the clients.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS6.4.1.1" class="ltx_text">IV-F</span> </span><span id="S4.SS6.5.2" class="ltx_text ltx_font_italic">Variant: Hybrid</span>
</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">The Hybrid variant of DFL encompasses a wide range of configurations, combining elements from various other variants. It is considered the most promising option for practical applications due to its adaptability to different scenarios. However, the complexity of configuring a hybrid variant can pose challenges. One example of a hybrid variant, as depicted in Fig. <a href="#S3.F3" title="Figure 3 ‣ III-C Network Topology ‣ III Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(g), involves connecting two ring variants through a central client. In this configuration, the hybrid variant provides global connectivity, allowing for the sharing of client models and knowledge within the framework. The two ring variants can also be treated as a single entity, with only one communication channel connected to the two central clients. Another illustration of a hybrid variant, shown in Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-A State-of-the-art Development ‣ IV Variants of Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(d), involves dividing clients into organizations based on geographical locations (e.g., continents). Within each organization, a mesh topology network is established, and a leader is elected. These leaders then form a ring topology network among themselves. The hybrid variants do not have a fixed structure and can be customized to meet the specific requirements of real-world scenarios. The hybrid variant offers several advantages.</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.1" class="ltx_p">Firstly, the hybrid variant helps in saving communication resources. This is achieved through the knowledge dissemination between the leaders of two organizations, where only the aggregated global model is shared. By transmitting only the essential information, the hybrid variant reduces the communication overhead. Additionally, the organized knowledge dissemination further enhances resource efficiency by minimizing the sharing of irrelevant or invalid information. This approach is particularly advantageous when establishing communication between two geographically distant organizations, as the single-line connection reduces the resource requirements for long-distance communication. Considering that the communication between organizations represents the dissemination of knowledge across states, countries, and continents <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>, the clients representing the research institutions establish a stable and well-structured communication connection to facilitate the exchange of knowledge between their respective organizations.</p>
</div>
<div id="S4.SS6.p3" class="ltx_para">
<p id="S4.SS6.p3.1" class="ltx_p">Secondly, the hybrid variant offers enhanced security. With two central clients in control, they have the ability to unilaterally disconnect the communication between organizations, ensuring the protection of their respective knowledge from potential leaks or unauthorized access. This adds an extra layer of security to the DFL system.</p>
</div>
<div id="S4.SS6.p4" class="ltx_para">
<p id="S4.SS6.p4.1" class="ltx_p">Thirdly, the hybrid variant provides a more personalized approach. Each organization’s aggregated model is organization-specific, tailored to the unique characteristics of its local data. This personalized model may offer better applicability to the specific needs and requirements of the organization. While model knowledge is shared between the two organizations, the decision of whether to utilize the other organization’s model is subject to further investigation and discussion. By thoroughly assessing the performance of the other organization’s model, clients can ensure that their own model remains uncontaminated and unaffected by potentially inferior or incompatible models.</p>
</div>
<div id="S4.SS6.p5" class="ltx_para">
<p id="S4.SS6.p5.1" class="ltx_p">Xing <span id="S4.SS6.p5.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> proposed a hybrid DFL network that establishes connections only with neighboring clients, and model parameters are broadcast-gossiped only among these neighboring clients. Their approach takes into account various factors such as link blockages, channel fading, and mutual interference, to ensure efficient and reliable communication. Building upon this work, Shi <span id="S4.SS6.p5.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> further improved the convergence performance by incorporating coding strategies, gradient tracking, and variance reduction algorithms. In a similar vein, Wang <span id="S4.SS6.p5.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> developed a dynamic hybrid DFL framework called Matcha. Matcha introduces the concept of creating different network topologies at each iteration to enhance convergence speed. The algorithm consists of two main parts. Firstly, an initial network topology pre-processing step where Matcha performs matching decomposition on a base communication topology to obtain disjoint sub-graphs, including sub-graphs with only two-peer connections. Next, matching activation probabilities are computed to maximize the connectivity of the graph, and a new random topology graph is generated for each iteration. The key idea behind Matcha is to achieve faster convergence by enabling more frequent communication on connectivity-critical links (e.g., central clients) and reducing communication latency by decreasing the frequency of communication on other connections. Matcha is particularly advantageous for hybrid networks with unknown or dynamic central clients. However, it may not exhibit the same advantages in scenarios involving research institutions where central clients are known and pre-determined.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Challenge and Potential Solutions in DFL</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Based on the current state-of-the-art technology, this section aims to discuss and analyze potential challenges and future research directions for DFL. Additionally, the variants mentioned in Section <a href="#S4" title="IV Variants of Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> can be regarded as potential solutions to address these challenges.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">High Communication Overhead</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">DFL is widely recognized as an extremely communication resource-efficient approach compared to CFL. However, researchers are still striving for further savings in communication resources and reduced communication complexity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite>. In Section <a href="#S2.SS1.SSS1" title="II-A1 Communication Resource ‣ II-A Challenges in Centralized Federated Learning ‣ II Centralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span>1</span></a>, we discussed the introduction of feasible approaches from the existing CFL framework to achieve efficient communication in DFL. Wang <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite> introduced a method called optimization of topology construction and model compression (CoCo) that aims to improve communication efficiency and convergence speed in DFL. CoCo achieves this by employing adaptive techniques for constructing the DFL network topology and assigning an appropriate model compression ratio to each participating client. It achieves this by adaptively constructing the DFL network topology and assigning an appropriate model compression ratio to each client.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">In addition to model compression, it is also important to investigate how to leverage efficient communication lines and reduce the overall communication length. Variants such as star and hybrid variants, which select geocentric clients and resource-rich clients as leaders, have been proven to be effective solutions in this regard. Some researchers have also focused on addressing the bandwidth differences among different communication lines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>, <a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>. It is worth noting that the dynamic hybrid variant proposed by Wang <span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> emphasizes the importance of communication efficiency and suggests frequent communication with key clients to achieve faster convergence. A considerable body of research emphasizes the importance of efficient communication in DFL and proposes various strategies and methods to reduce complexity and optimize communication resources. Further exploration in this direction is expected to facilitate the potential deployment of DFL frameworks in real-world applications.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Computational and Storage Burden</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Compared to the CFL and <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">Continual</span> paradigm, the <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_typewriter">Aggregate</span> paradigm imposes significantly higher demands on client-side computational and storage resources. As there is no dedicated server in the <span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_typewriter">Aggregate</span> paradigm, clients are responsible for storing previous model parameters and performing aggregation computations alongside local model training. Consequently, the computational and storage burdens pose challenges for client hardware.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">One potential solution is to adopt the transfer learning concept and fix the weights of the lower layers in all models. In this approach, the lower layers serve as feature extractors for a specific task and are expected to be similar across models, while the higher-level representations remain task-specific. By fixing these parameters, there is no need for gradient descent, aggregation computations, or communication related to these layers. Moreover, this approach reduces storage requirements, thereby substantially mitigating the resource consumption of the client. Currently, with the widespread availability of high-performance GPU computing resources, the challenges related to computational and storage burdens are gradually diminishing. This is especially true in DFL scenarios where institutions and organizations serve as clients. However, in contexts such as mobile services dominated by smartphones and on-board units in vehicular edge devices, there is still value in researching ways to reduce computational complexity and optimize storage efficiency.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Vulnerability in Cybersecurity</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Network security has always been a major challenge in FL, and this challenge is particularly prominent in DFL. In the traditional CFL setting, clients communicate with a central server, typically operated by a research institution or a large commercial organization. While there is still potential for attacks and data poisoning between clients and the server, the communication is generally more regulated and protected compared to DFL. In DFL, the knowledge exchange occurs directly among users within a local area network, with free and unrestricted sharing agreements, which poses an increased risk of privacy exposure. Malicious attacks from clients, poisoned data, free-riding attacks, and other malicious behaviors are all possible in this decentralized setting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>, <a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite>.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Kuo <span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite> proposed the integration of blockchain into a decentralized learning framework to enhance privacy protection, which can also be applied to DFL. Chen <span id="S5.SS3.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> integrated a differential privacy mechanism based on blockchain technology. Bellet <span id="S5.SS3.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> introduced an asynchronous and differential privacy algorithm in DFL to safeguard user privacy. He <span id="S5.SS3.p2.1.4" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> addressed trust issues between clients by employing an online push-sum algorithm to actively push local models to trusted clients. Shayan <span id="S5.SS3.p2.1.5" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite> proposed the Biscotti DFL system, which incorporates multiple privacy and security protection techniques, including the Multi-Krum defense to prevent poisoning attacks, differential privacy noise to protect privacy, and secure aggregation. The future research direction in cybersecurity will involve the roles of attackers and defenders, focusing on developing targeted attack and defense mechanisms for different DFL variants.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.4.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.5.2" class="ltx_text ltx_font_italic">Lack of Incentive Mechanism</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">In the absence of server management, the issue of fairness in aggregation has been effectively addressed in DFL. However, the lack of incentives and mutual distrust among clients can significantly impact their willingness to contribute knowledge. A key issue is the lack of incentives, which may lead to free-riding attacks where clients choose to benefit from the models without contributing their own knowledge.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">In the context of DFL, the feasibility of incentive mechanisms based on game theory, such as Stackelberg games <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite>, raises questions due to the requirements on game leaders, participants, and rewards. One potential solution could be the integration of reputation-based incentive mechanisms using blockchain and smart contracts. Kang <span id="S5.SS4.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> proposed assigning reputation scores to clients to represent and quantify their reliability. Clients with higher contributions and reputation can receive greater rewards. However, designing effective and practical incentive mechanisms for DFL remains an open problem.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">In cases where task providers do not exist or there are no explicit rewards, punitive incentives may also be a potential solution. Clients who fail to contribute or engage in malicious behavior could face penalties or reduced access to the benefits of the DFL framework. Further research is needed to explore and develop robust incentive mechanisms tailored specifically for DFL systems. Designing effective incentive mechanisms to encourage active participation, foster trust, and stimulate enthusiastic knowledge sharing will greatly facilitate the dissemination of knowledge in DFL.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS5.4.1.1" class="ltx_text">V-E</span> </span><span id="S5.SS5.5.2" class="ltx_text ltx_font_italic">Lack of Management</span>
</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">In DFL, the absence of a central server for managing all clients poses a significant challenge in receiving and sharing knowledge in an organized manner. The lack of central management can lead to confusion, particularly among clients with varying sample sizes, computational resources, and communication capabilities. In the ring variant, a client only needs to wait for the model parameters from the previous client, while in the mesh variant, a client needs to wait for model parameters from all other clients. Such dependencies on other clients for model transmission can result in deadlocks, causing the entire system to halt. Moreover, the communication among clients may not be robust, considering the possibility of SPoF. The absence of management is particularly problematic in the hybrid variant depicted in Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-A State-of-the-art Development ‣ IV Variants of Decentralized Federated Learning ‣ Decentralized Federated Learning: A Survey and Perspective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(d), where clients communicate globally. The lack of management can lead to reduced operational efficiency, confusion regarding model versions, and performance degradation.</p>
</div>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.1" class="ltx_p">To address the challenge of lack of management in DFL, researchers have proposed several approaches. One approach is to pre-request the status of other clients, such as their model versions, before initiating knowledge transfer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>. By obtaining accurate information, a client can then request the transfer of the entire model data. Some star variants enforce the knowledge dissemination flow among clients by designating a leader <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>. This leader is responsible for regulating the knowledge dissemination among the remaining clients. Additionally, Chen <span id="S5.SS5.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite> introduced the BDFL framework, a mesh DFL framework specifically designed for autonomous vehicles. In this framework, a leader is randomly selected in each communication round, offering advantages such as increased privacy and security protection against Byzantine faults, as well as enhanced management through the leader’s command issuance. In real-world scenarios, clients may face challenges where they lack knowledge about each other’s statuses, leading to issues such as model version discrepancies and even system paralysis, such as in the case of a SPoF. Therefore, future research directions aim to ensure the smooth operation of the system by incorporating additional information or establishing contingency plans. These measures can help mitigate the impact of uncertainty and improve the reliability and robustness of the DFL framework.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we provided an extensive exploration of the DFL framework, covering communication protocols, network topologies, paradigm proposals, extension variants, challenges, and potential solutions. Our aim is to offer a comprehensive, well-defined, and systematic perspective that organizes and synthesizes the existing literature and definitions, thereby facilitating a comprehensive introduction to DFL for new researchers. Given that DFL is a rapidly evolving area, we established a solid theoretical foundation by defining and discussing five variants in this paper. This not only provides researchers with a comprehensive understanding of the field but also fosters the generation of new ideas and collaborations among peers.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">It is important to note that our approach differs from traditional surveys, as we presented our own insights and innovative thinking on DFL. Moreover, this paper uncovers a considerable number of previously unexplored types within the DFL framework. By considering diverse usage scenarios, we aim to stimulate and extend the research interest of other DFL practitioners, enabling them to adapt the framework to their specific needs.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji,
K. Bonawitz, Z. Charles, G. Cormode, R. Cummings </span><em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al.</em><span id="bib.bib1.3.3" class="ltx_text" style="font-size:90%;">, “Advances
and open problems in federated learning,” </span><em id="bib.bib1.4.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Found. Trends Mach. Learn.</em><span id="bib.bib1.5.5" class="ltx_text" style="font-size:90%;">,
vol. 14, no. 1–2, pp. 1–210, Jun. 2021.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in </span><em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Artificial intelligence and statistics</em><span id="bib.bib2.3.3" class="ltx_text" style="font-size:90%;">.   PMLR, 2017, pp. 1273–1282.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
A. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augenstein,
H. Eichner, C. Kiddon, and D. Ramage, “Federated learning for mobile
keyboard prediction,” </span><em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1811.03604</em><span id="bib.bib3.3.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Z. Wang, R. Gupta, K. Han, H. Wang, A. Ganlath, N. Ammar, and P. Tiwari,
“Mobility digital twin: Concept, architecture, case study, and future
challenges,” </span><em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Internet Things J.</em><span id="bib.bib4.3.3" class="ltx_text" style="font-size:90%;">, Mar. 2022.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
G. A. Kaissis, M. R. Makowski, D. Rückert, and R. F. Braren, “Secure,
privacy-preserving and federated machine learning in medical imaging,”
</span><em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Nat. Mach. Intell.</em><span id="bib.bib5.3.3" class="ltx_text" style="font-size:90%;">, vol. 2, no. 6, pp. 305–311, Jun. 2020.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Y. Qu, S. R. Pokhrel, S. Garg, L. Gao, and Y. Xiang, “A blockchained federated
learning framework for cognitive computing in industry 4.0 networks,”
</span><em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Trans. Industr. Inform.</em><span id="bib.bib6.3.3" class="ltx_text" style="font-size:90%;">, vol. 17, no. 4, pp. 2964–2973, Jul.
2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
A. Durrant, M. Markovic, D. Matthews, D. May, J. Enright, and G. Leontidis,
“The role of cross-silo federated learning in facilitating data sharing in
the agri-food sector,” </span><em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Comput. Electron. Agric.</em><span id="bib.bib7.3.3" class="ltx_text" style="font-size:90%;">, vol. 193, p. 106648,
Feb. 2022.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Y. M. Saputra, D. T. Hoang, D. N. Nguyen, E. Dutkiewicz, M. D. Mueck, and
S. Srikanteswara, “Energy demand prediction with federated learning for
electric vehicle networks,” in </span><em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2019 IEEE Global Communications
Conference (GLOBECOM)</em><span id="bib.bib8.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2019,
pp. 1–6.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
I. Dayan, H. R. Roth, A. Zhong, A. Harouni, A. Gentili, A. Z. Abidin, A. Liu,
A. B. Costa, B. J. Wood, C.-S. Tsai </span><em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al.</em><span id="bib.bib9.3.3" class="ltx_text" style="font-size:90%;">, “Federated learning for
predicting clinical outcomes in patients with covid-19,” </span><em id="bib.bib9.4.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Nat. Med.</em><span id="bib.bib9.5.5" class="ltx_text" style="font-size:90%;">,
vol. 27, no. 10, pp. 1735–1743, Oct. 2021.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
S. Pati, U. Baid, B. Edwards, M. Sheller, S.-H. Wang, G. A. Reina, P. Foley,
A. Gruzdev, D. Karkada, C. Davatzikos </span><em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al.</em><span id="bib.bib10.3.3" class="ltx_text" style="font-size:90%;">, “Federated learning
enables big data for rare cancer boundary detection,” </span><em id="bib.bib10.4.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Nat. Commun.</em><span id="bib.bib10.5.5" class="ltx_text" style="font-size:90%;">,
vol. 13, no. 1, p. 7346, Dec. 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learning: Concept
and applications,” </span><em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM Trans. Intell. Syst. (TIST)</em><span id="bib.bib11.3.3" class="ltx_text" style="font-size:90%;">, vol. 10, no. 2,
pp. 1–19, Jan. 2019.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
A. Lalitha, O. C. Kilinc, T. Javidi, and F. Koushanfar, “Peer-to-peer
federated learning on graphs,” </span><em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1901.11173</em><span id="bib.bib12.3.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
C. He, C. Tan, H. Tang, S. Qiu, and J. Liu, “Central server free federated
learning over single-sided trust social networks,” </span><em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint
arXiv:1910.04956</em><span id="bib.bib13.3.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
C. He, E. Ceyani, K. Balasubramanian, M. Annavaram, and S. Avestimehr,
“Spreadgnn: Serverless multi-task federated learning for graph neural
networks,” </span><em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2106.02743</em><span id="bib.bib14.3.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
H. Xing, O. Simeone, and S. Bi, “Federated learning over wireless
device-to-device networks: Algorithms and convergence analysis,” </span><em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE
J. Sel. Areas Commun.</em><span id="bib.bib15.3.3" class="ltx_text" style="font-size:90%;">, vol. 39, no. 12, pp. 3723–3741, 2021.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
S. Warnat-Herresthal, H. Schultze, K. L. Shastry, S. Manamohan, S. Mukherjee,
V. Garg, R. Sarveswara, K. Händler, P. Pickkers, N. A. Aziz
</span><em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al.</em><span id="bib.bib16.3.3" class="ltx_text" style="font-size:90%;">, “Swarm learning for decentralized and confidential clinical
machine learning,” </span><em id="bib.bib16.4.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Nature</em><span id="bib.bib16.5.5" class="ltx_text" style="font-size:90%;">, vol. 594, no. 7862, pp. 265–270, Jun.
2021.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
A. Lalitha, S. Shekhar, T. Javidi, and F. Koushanfar, “Fully decentralized
federated learning,” in </span><em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Third workshop on Bayesian Deep Learning
(NeurIPS)</em><span id="bib.bib17.3.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Q. Li, Z. Wen, and B. He, “Practical federated gradient boosting decision
trees,” in </span><em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI conference on artificial
intelligence</em><span id="bib.bib18.3.3" class="ltx_text" style="font-size:90%;">, vol. 34, no. 04, 2020, pp. 4642–4649.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
S. Savazzi, M. Nicoli, and V. Rampa, “Federated learning with cooperating
devices: A consensus approach for massive iot networks,” </span><em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Internet
Things J.</em><span id="bib.bib19.3.3" class="ltx_text" style="font-size:90%;">, vol. 7, no. 5, pp. 4641–4654, Jan. 2020.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
D. Monschein, J. A. P. Pérez, T. Piotrowski, Z. Nochta, O. P. Waldhorst,
and C. Zirpins, “Towards a peer-to-peer federated machine learning
environment for continuous authentication,” in </span><em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 IEEE Symposium on
Computers and Communications (ISCC)</em><span id="bib.bib20.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2021, pp. 1–6.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
S. Savazzi, M. Nicoli, M. Bennis, S. Kianoush, and L. Barbieri, “Opportunities
of federated learning in connected, cooperative, and automated industrial
systems,” </span><em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Commun. Mag.</em><span id="bib.bib21.3.3" class="ltx_text" style="font-size:90%;">, vol. 59, no. 2, pp. 16–21, Mar. 2021.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Y. Shi, L. Shen, K. Wei, Y. Sun, B. Yuan, X. Wang, and D. Tao, “Improving the
model consistency of decentralized federated learning,” </span><em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint
arXiv:2302.04083</em><span id="bib.bib22.3.3" class="ltx_text" style="font-size:90%;">, Feb. 2023.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
J. Xu, B. S. Glicksberg, C. Su, P. Walker, J. Bian, and F. Wang, “Federated
learning for healthcare informatics,” </span><em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">J. Healthc. Inform. Res.</em><span id="bib.bib23.3.3" class="ltx_text" style="font-size:90%;">,
vol. 5, no. 1, pp. 1–19, Mar. 2021.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and J. Liu, “Can
decentralized algorithms outperform centralized algorithms? a case study for
decentralized parallel stochastic gradient descent,” </span><em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in
Neural Information Processing Systems</em><span id="bib.bib24.3.3" class="ltx_text" style="font-size:90%;">, vol. 30, 2017.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
N. Rieke, J. Hancox, W. Li, F. Milletari, H. R. Roth, S. Albarqouni, S. Bakas,
M. N. Galtier, B. A. Landman, K. Maier-Hein </span><em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al.</em><span id="bib.bib25.3.3" class="ltx_text" style="font-size:90%;">, “The future of
digital health with federated learning,” </span><em id="bib.bib25.4.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">NPJ Digit. Med.</em><span id="bib.bib25.5.5" class="ltx_text" style="font-size:90%;">, vol. 3,
no. 1, pp. 1–7, Sep. 2020.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
D. C. Nguyen, M. Ding, P. N. Pathirana, A. Seneviratne, J. Li, and H. V. Poor,
“Federated learning for internet of things: A comprehensive survey,”
</span><em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Commun. Surv. Tutor.</em><span id="bib.bib26.3.3" class="ltx_text" style="font-size:90%;">, vol. 23, no. 3, pp. 1622–1658, Apr. 2021.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
V. Mothukuri, R. M. Parizi, S. Pouriyeh, Y. Huang, A. Dehghantanha, and
G. Srivastava, “A survey on security and privacy of federated learning,”
</span><em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Future Gener. Comput. Syst.</em><span id="bib.bib27.3.3" class="ltx_text" style="font-size:90%;">, vol. 115, pp. 619–640, Feb. 2021.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
L. Witt, M. Heyer, K. Toyoda, W. Samek, and D. Li, “Decentral and incentivized
federated learning frameworks: A systematic literature review,” </span><em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv
preprint arXiv:2205.07855</em><span id="bib.bib28.3.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
V. P. Chellapandi, L. Yuan, S. H. Zak, and Z. Wang, “A survey of federated
learning for connected and automated vehicles,” </span><em id="bib.bib29.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint
arXiv:2303.10677</em><span id="bib.bib29.3.3" class="ltx_text" style="font-size:90%;">, March 2023.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
S. P. Karimireddy, S. Kale, M. Mohri, S. Reddi, S. Stich, and A. T. Suresh,
“Scaffold: Stochastic controlled averaging for federated learning,” in
</span><em id="bib.bib30.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib30.3.3" class="ltx_text" style="font-size:90%;">.   PMLR, 2020, pp. 5132–5143.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, “Federated learning
with non-iid data,” </span><em id="bib.bib31.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1806.00582</em><span id="bib.bib31.3.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith,
“Federated optimization in heterogeneous networks,” </span><em id="bib.bib32.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of
Machine Learning and Systems</em><span id="bib.bib32.3.3" class="ltx_text" style="font-size:90%;">, vol. 2, pp. 429–450, 2020.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
K. Wei, J. Li, M. Ding, C. Ma, H. H. Yang, F. Farokhi, S. Jin, T. Q. Quek, and
H. V. Poor, “Federated learning with differential privacy: Algorithms and
performance analysis,” </span><em id="bib.bib33.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Trans. Inf. Forensics Secur.</em><span id="bib.bib33.3.3" class="ltx_text" style="font-size:90%;">, vol. 15,
pp. 3454–3469, Apr. 2020.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
T. R. Gadekallu, Q.-V. Pham, T. Huynh-The, S. Bhattacharya, P. K. R.
Maddikunta, and M. Liyanage, “Federated learning for big data: A survey on
opportunities, applications, and future directions,” </span><em id="bib.bib34.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint
arXiv:2110.04160</em><span id="bib.bib34.3.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
J. Konečnỳ, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh,
and D. Bacon, “Federated learning: Strategies for improving communication
efficiency,” </span><em id="bib.bib35.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1610.05492</em><span id="bib.bib35.3.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
F. Sattler, S. Wiedemann, K.-R. Müller, and W. Samek, “Robust and
communication-efficient federated learning from non-iid data,” </span><em id="bib.bib36.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE
Trans. Neural Netw. Learn. Syst.</em><span id="bib.bib36.3.3" class="ltx_text" style="font-size:90%;">, vol. 31, no. 9, pp. 3400–3413, Nov. 2019.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mobilenetv2:
Inverted residuals and linear bottlenecks,” in </span><em id="bib.bib37.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE
conference on computer vision and pattern recognition</em><span id="bib.bib37.3.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 4510–4520.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
M. Tan and Q. Le, “Efficientnet: Rethinking model scaling for convolutional
neural networks,” in </span><em id="bib.bib38.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International conference on machine
learning</em><span id="bib.bib38.3.3" class="ltx_text" style="font-size:90%;">.   PMLR, 2019, pp. 6105–6114.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
F. Sattler, S. Wiedemann, K.-R. Müller, and W. Samek, “Sparse binary
compression: Towards distributed deep learning with minimal communication,”
in </span><em id="bib.bib39.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2019 International Joint Conference on Neural Networks
(IJCNN)</em><span id="bib.bib39.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2019, pp. 1–8.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
C. Ma, J. Li, M. Ding, H. H. Yang, F. Shu, T. Q. Quek, and H. V. Poor, “On
safeguarding privacy and security in the framework of federated learning,”
</span><em id="bib.bib40.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Netw.</em><span id="bib.bib40.3.3" class="ltx_text" style="font-size:90%;">, vol. 34, no. 4, pp. 242–248, Mar. 2020.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How to backdoor
federated learning,” in </span><em id="bib.bib41.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Artificial
Intelligence and Statistics</em><span id="bib.bib41.3.3" class="ltx_text" style="font-size:90%;">.   PMLR,
2020, pp. 2938–2948.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
V. Tolpegin, S. Truex, M. E. Gursoy, and L. Liu, “Data poisoning attacks
against federated learning systems,” in </span><em id="bib.bib42.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European Symposium on Research
in Computer Security</em><span id="bib.bib42.3.3" class="ltx_text" style="font-size:90%;">.   Springer, 2020,
pp. 480–501.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
V. Mothukuri, P. Khare, R. M. Parizi, S. Pouriyeh, A. Dehghantanha, and
G. Srivastava, “Federated-learning-based anomaly detection for iot security
attacks,” </span><em id="bib.bib43.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Internet Things J.</em><span id="bib.bib43.3.3" class="ltx_text" style="font-size:90%;">, vol. 9, no. 4, pp. 2545–2554, May
2021.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
T. Li, M. Sanjabi, A. Beirami, and V. Smith, “Fair resource allocation in
federated learning,” </span><em id="bib.bib44.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.10497</em><span id="bib.bib44.3.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
J. Kang, Z. Xiong, D. Niyato, S. Xie, and J. Zhang, “Incentive mechanism for
reliable federated learning: A joint optimization approach to combining
reputation and contract theory,” </span><em id="bib.bib45.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Internet Things J.</em><span id="bib.bib45.3.3" class="ltx_text" style="font-size:90%;">, vol. 6,
no. 6, pp. 10 700–10 714, Sep. 2019.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
D. Roschewitz, M.-A. Hartley, L. Corinzia, and M. Jaggi, “Ifedavg:
Interpretable data-interoperability for federated learning,” </span><em id="bib.bib46.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv
preprint arXiv:2107.06580</em><span id="bib.bib46.3.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
H. Kim, J. Park, M. Bennis, and S.-L. Kim, “Blockchained on-device federated
learning,” </span><em id="bib.bib47.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Commun. Lett.</em><span id="bib.bib47.3.3" class="ltx_text" style="font-size:90%;">, vol. 24, no. 6, pp. 1279–1283, Jun.
2019.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Y. Qu, L. Gao, T. H. Luan, Y. Xiang, S. Yu, B. Li, and G. Zheng,
“Decentralized privacy using blockchain-enabled federated learning in fog
computing,” </span><em id="bib.bib48.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Internet Things J.</em><span id="bib.bib48.3.3" class="ltx_text" style="font-size:90%;">, vol. 7, no. 6, pp. 5171–5183,
Mar. 2020.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
W. Y. B. Lim, N. C. Luong, D. T. Hoang, Y. Jiao, Y.-C. Liang, Q. Yang,
D. Niyato, and C. Miao, “Federated learning in mobile edge networks: A
comprehensive survey,” </span><em id="bib.bib49.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Commun. Surv. Tutor.</em><span id="bib.bib49.3.3" class="ltx_text" style="font-size:90%;">, vol. 22, no. 3, pp.
2031–2063, Apr. 2020.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Y. Ye, S. Li, F. Liu, Y. Tang, and W. Hu, “Edgefed: Optimized federated
learning based on edge computing,” </span><em id="bib.bib50.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Access</em><span id="bib.bib50.3.3" class="ltx_text" style="font-size:90%;">, vol. 8, pp.
209 191–209 198, Nov. 2020.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
R. Yang, F. R. Yu, P. Si, Z. Yang, and Y. Zhang, “Integrated blockchain and
edge computing systems: A survey, some research issues and challenges,”
</span><em id="bib.bib51.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Commun. Surv.</em><span id="bib.bib51.3.3" class="ltx_text" style="font-size:90%;">, vol. 21, no. 2, pp. 1508–1532, Jan. 2019.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
D. C. Nguyen, M. Ding, Q.-V. Pham, P. N. Pathirana, L. B. Le, A. Seneviratne,
J. Li, D. Niyato, and H. V. Poor, “Federated learning meets blockchain in
edge computing: Opportunities and challenges,” </span><em id="bib.bib52.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Internet Things
J.</em><span id="bib.bib52.3.3" class="ltx_text" style="font-size:90%;">, vol. 8, no. 16, pp. 12 806–12 825, Apr. 2021.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
A. Z. Tan, H. Yu, L. Cui, and Q. Yang, “Towards personalized federated
learning,” </span><em id="bib.bib53.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Trans. Neural Netw. Learn. Syst.</em><span id="bib.bib53.3.3" class="ltx_text" style="font-size:90%;">, Mar. 2022.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Y. Chen, X. Qin, J. Wang, C. Yu, and W. Gao, “Fedhealth: A federated transfer
learning framework for wearable healthcare,” </span><em id="bib.bib54.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Intell. Syst.</em><span id="bib.bib54.3.3" class="ltx_text" style="font-size:90%;">,
vol. 35, no. 4, pp. 83–93, Apr. 2020.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
L. Yuan, L. Su, and Z. Wang, “Federated transfer-ordered-personalized learning
for driver monitoring application,” </span><em id="bib.bib55.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2301.04829</em><span id="bib.bib55.3.3" class="ltx_text" style="font-size:90%;">,
2023.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
F. Sattler, K.-R. Müller, and W. Samek, “Clustered federated learning:
Model-agnostic distributed multitask optimization under privacy
constraints,” </span><em id="bib.bib56.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Trans. Neural Netw. Learn. Syst.</em><span id="bib.bib56.3.3" class="ltx_text" style="font-size:90%;">, vol. 32, no. 8,
pp. 3710–3722, Aug. 2020.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
A. Ghosh, J. Chung, D. Yin, and K. Ramchandran, “An efficient framework for
clustered federated learning,” </span><em id="bib.bib57.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information
Processing Systems</em><span id="bib.bib57.3.3" class="ltx_text" style="font-size:90%;">, vol. 33, pp. 19 586–19 597, 2020.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
C. Thapa, P. C. M. Arachchige, S. Camtepe, and L. Sun, “Splitfed: When
federated learning meets split learning,” in </span><em id="bib.bib58.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI
Conference on Artificial Intelligence</em><span id="bib.bib58.3.3" class="ltx_text" style="font-size:90%;">, vol. 36, no. 8, 2022, pp. 8485–8493.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
J. Chen, K. Li, and S. Y. Philip, “Privacy-preserving deep learning model for
decentralized vanets using fully homomorphic encryption and blockchain,”
</span><em id="bib.bib59.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Trans. Intell. Transp. Syst.</em><span id="bib.bib59.3.3" class="ltx_text" style="font-size:90%;">, Aug. 2021.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
A. Nedić, A. Olshevsky, and M. G. Rabbat, “Network topology and
communication-computation tradeoffs in decentralized optimization,”
</span><em id="bib.bib60.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE</em><span id="bib.bib60.3.3" class="ltx_text" style="font-size:90%;">, vol. 106, no. 5, pp. 953–976, Apr. 2018.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
S. Boyd, A. Ghosh, B. Prabhakar, and D. Shah, “Randomized gossip algorithms,”
</span><em id="bib.bib61.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Trans. Inf. Theory</em><span id="bib.bib61.3.3" class="ltx_text" style="font-size:90%;">, vol. 52, no. 6, pp. 2508–2530, Jun. 2006.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
D. Kempe, A. Dobra, and J. Gehrke, “Gossip-based computation of aggregate
information,” in </span><em id="bib.bib62.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">44th Annual IEEE Symposium on Foundations of Computer
Science, 2003. Proceedings.</em><span id="bib.bib62.3.3" class="ltx_text" style="font-size:90%;">   IEEE,
2003, pp. 482–491.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:90%;">
A. Koloskova, S. Stich, and M. Jaggi, “Decentralized stochastic optimization
and gossip algorithms with compressed communication,” in </span><em id="bib.bib63.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International
Conference on Machine Learning</em><span id="bib.bib63.3.3" class="ltx_text" style="font-size:90%;">.   PMLR,
2019, pp. 3478–3487.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text" style="font-size:90%;">
C. Hu, J. Jiang, and Z. Wang, “Decentralized federated learning: A segmented
gossip approach,” </span><em id="bib.bib64.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1908.07782</em><span id="bib.bib64.3.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text" style="font-size:90%;">
A. Nedic, “Asynchronous broadcast-based convex optimization over a network,”
</span><em id="bib.bib65.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Trans. Automat. Contr.</em><span id="bib.bib65.3.3" class="ltx_text" style="font-size:90%;">, vol. 56, no. 6, pp. 1337–1351, Sep.
2010.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text" style="font-size:90%;">
T. C. Aysal, M. E. Yildiz, A. D. Sarwate, and A. Scaglione, “Broadcast gossip
algorithms for consensus,” </span><em id="bib.bib66.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Trans. Signal Process.</em><span id="bib.bib66.3.3" class="ltx_text" style="font-size:90%;">, vol. 57,
no. 7, pp. 2748–2761, Feb. 2009.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text" style="font-size:90%;">
A. Bellet, R. Guerraoui, M. Taziki, and M. Tommasi, “Personalized and private
peer-to-peer machine learning,” in </span><em id="bib.bib67.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International conference on
artificial intelligence and statistics</em><span id="bib.bib67.3.3" class="ltx_text" style="font-size:90%;">.   PMLR, 2018, pp. 473–481.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text" style="font-size:90%;">
T. Lesort, V. Lomonaco, A. Stoian, D. Maltoni, D. Filliat, and
N. Díaz-Rodríguez, “Continual learning for robotics: Definition,
framework, learning strategies, opportunities and challenges,” </span><em id="bib.bib68.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Inf.
Fusion</em><span id="bib.bib68.3.3" class="ltx_text" style="font-size:90%;">, vol. 58, pp. 52–68, Jun. 2020.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text" style="font-size:90%;">
M. Delange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis,
G. Slabaugh, and T. Tuytelaars, “A continual learning survey: Defying
forgetting in classification tasks,” </span><em id="bib.bib69.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Trans. Pattern Anal. Mach.
Intell.</em><span id="bib.bib69.3.3" class="ltx_text" style="font-size:90%;">, Feb. 2021.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text" style="font-size:90%;">
M. F. Criado, F. E. Casado, R. Iglesias, C. V. Regueiro, and S. Barro,
“Non-iid data and continual learning processes in federated learning: A long
road ahead,” </span><em id="bib.bib70.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Inf. Fusion</em><span id="bib.bib70.3.3" class="ltx_text" style="font-size:90%;">, vol. 88, pp. 263–280, Dec. 2022.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text" style="font-size:90%;">
A. Usmanova, F. Portet, P. Lalanda, and G. Vega, “A distillation-based
approach integrating continual learning and federated learning for pervasive
services,” </span><em id="bib.bib71.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2109.04197</em><span id="bib.bib71.3.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text" style="font-size:90%;">
T. J. Park, K. Kumatani, and D. Dimitriadis, “Tackling dynamics in federated
incremental learning with variational embedding rehearsal,” </span><em id="bib.bib72.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv
preprint arXiv:2110.09695</em><span id="bib.bib72.3.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text" style="font-size:90%;">
J. Yoon, W. Jeong, G. Lee, E. Yang, and S. J. Hwang, “Federated continual
learning with weighted inter-client transfer,” in </span><em id="bib.bib73.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International
Conference on Machine Learning</em><span id="bib.bib73.3.3" class="ltx_text" style="font-size:90%;">.   PMLR,
2021, pp. 12 073–12 086.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text" style="font-size:90%;">
A. H. Sayed, S.-Y. Tu, J. Chen, X. Zhao, and Z. J. Towfic, “Diffusion
strategies for adaptation and learning over networks: an examination of
distributed strategies and network behavior,” </span><em id="bib.bib74.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Signal Process.
Mag.</em><span id="bib.bib74.3.3" class="ltx_text" style="font-size:90%;">, vol. 30, no. 3, pp. 155–171, Apr. 2013.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock"><span id="bib.bib75.1.1" class="ltx_text" style="font-size:90%;">
K. Chang, N. Balachandar, C. Lam, D. Yi, J. Brown, A. Beers, B. Rosen, D. L.
Rubin, and J. Kalpathy-Cramer, “Distributed deep learning networks among
institutions for medical imaging,” </span><em id="bib.bib75.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">J. Am. Med. Inform. Assoc.</em><span id="bib.bib75.3.3" class="ltx_text" style="font-size:90%;">,
vol. 25, no. 8, pp. 945–954, Aug. 2018.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock"><span id="bib.bib76.1.1" class="ltx_text" style="font-size:90%;">
M. J. Sheller, G. A. Reina, B. Edwards, J. Martin, and S. Bakas,
“Multi-institutional deep learning modeling without sharing patient data: A
feasibility study on brain tumor segmentation,” in </span><em id="bib.bib76.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International
MICCAI Brainlesion Workshop</em><span id="bib.bib76.3.3" class="ltx_text" style="font-size:90%;">.   Springer, 2019, pp. 92–104.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock"><span id="bib.bib77.1.1" class="ltx_text" style="font-size:90%;">
M. J. Sheller, B. Edwards, G. A. Reina, J. Martin, S. Pati, A. Kotrotsou,
M. Milchenko, W. Xu, D. Marcus, R. R. Colen </span><em id="bib.bib77.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al.</em><span id="bib.bib77.3.3" class="ltx_text" style="font-size:90%;">, “Federated
learning in medicine: facilitating multi-institutional collaborations without
sharing patient data,” </span><em id="bib.bib77.4.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Sci. Rep.</em><span id="bib.bib77.5.5" class="ltx_text" style="font-size:90%;">, vol. 10, no. 1, pp. 1–12, Jul.
2020.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock"><span id="bib.bib78.1.1" class="ltx_text" style="font-size:90%;">
Y. Huang, C. Bert, S. Fischer, M. Schmidt, A. Dörfler, A. Maier,
R. Fietkau, and F. Putz, “Continual learning for peer-to-peer federated
learning: A study on automated brain metastasis identification,” </span><em id="bib.bib78.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv
preprint arXiv:2204.13591</em><span id="bib.bib78.3.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock"><span id="bib.bib79.1.1" class="ltx_text" style="font-size:90%;">
L. Yuan, Y. Ma, L. Su, and Z. Wang, “Peer-to-peer federated continual learning
for naturalistic driving action recognition,” </span><em id="bib.bib79.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint
arXiv:2304.07421</em><span id="bib.bib79.3.3" class="ltx_text" style="font-size:90%;">, April 2023.
</span>
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock"><span id="bib.bib80.1.1" class="ltx_text" style="font-size:90%;">
M. Assran, N. Loizou, N. Ballas, and M. Rabbat, “Stochastic gradient push for
distributed deep learning,” in </span><em id="bib.bib80.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine
Learning</em><span id="bib.bib80.3.3" class="ltx_text" style="font-size:90%;">.   PMLR, 2019, pp. 344–353.
</span>
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock"><span id="bib.bib81.1.1" class="ltx_text" style="font-size:90%;">
A. G. Roy, S. Siddiqui, S. Pölsterl, N. Navab, and C. Wachinger,
“Braintorrent: A peer-to-peer environment for decentralized federated
learning,” </span><em id="bib.bib81.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.06731</em><span id="bib.bib81.3.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock"><span id="bib.bib82.1.1" class="ltx_text" style="font-size:90%;">
C. Pappas, D. Chatzopoulos, S. Lalis, and M. Vavalis, “Ipls: A framework for
decentralized federated learning,” in </span><em id="bib.bib82.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 IFIP Networking Conference
(IFIP Networking)</em><span id="bib.bib82.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2021, pp.
1–6.
</span>
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock"><span id="bib.bib83.1.1" class="ltx_text" style="font-size:90%;">
Y. Shi, Y. Zhou, and Y. Shi, “Over-the-air decentralized federated learning,”
in </span><em id="bib.bib83.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 IEEE International Symposium on Information Theory
(ISIT)</em><span id="bib.bib83.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2021, pp. 455–460.
</span>
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock"><span id="bib.bib84.1.1" class="ltx_text" style="font-size:90%;">
S. Chen, D. Yu, Y. Zou, J. Yu, and X. Cheng, “Decentralized wireless federated
learning with differential privacy,” </span><em id="bib.bib84.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Trans. Industr. Inform.</em><span id="bib.bib84.3.3" class="ltx_text" style="font-size:90%;">,
vol. 18, no. 9, pp. 6273–6282, Jan. 2022.
</span>
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock"><span id="bib.bib85.1.1" class="ltx_text" style="font-size:90%;">
J. Wang, A. K. Sahu, G. Joshi, and S. Kar, “Matcha: A matching-based link
scheduling strategy to speed up distributed optimization,” </span><em id="bib.bib85.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Trans.
Signal Process.</em><span id="bib.bib85.3.3" class="ltx_text" style="font-size:90%;">, vol. 70, pp. 5208–5221, Nov. 2022.
</span>
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock"><span id="bib.bib86.1.1" class="ltx_text" style="font-size:90%;">
J. Harding, G. Powell, R. Yoon, J. Fikentscher, C. Doyle, D. Sade, M. Lukuc,
J. Simons, J. Wang </span><em id="bib.bib86.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al.</em><span id="bib.bib86.3.3" class="ltx_text" style="font-size:90%;">, “Vehicle-to-vehicle communications:
readiness of v2v technology for application.” United States. National
Highway Traffic Safety Administration, Tech. Rep., Aug. 2014.
</span>
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock"><span id="bib.bib87.1.1" class="ltx_text" style="font-size:90%;">
S. Samarakoon, M. Bennis, W. Saad, and M. Debbah, “Distributed federated
learning for ultra-reliable low-latency vehicular communications,”
</span><em id="bib.bib87.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Trans. Commun.</em><span id="bib.bib87.3.3" class="ltx_text" style="font-size:90%;">, vol. 68, no. 2, pp. 1146–1159, Nov. 2019.
</span>
</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock"><span id="bib.bib88.1.1" class="ltx_text" style="font-size:90%;">
Z. Du, C. Wu, T. Yoshinaga, K.-L. A. Yau, Y. Ji, and J. Li, “Federated
learning for vehicular internet of things: Recent advances and open issues,”
</span><em id="bib.bib88.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Open J. Comput. Soc.</em><span id="bib.bib88.3.3" class="ltx_text" style="font-size:90%;">, vol. 1, pp. 45–61, May 2020.
</span>
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock"><span id="bib.bib89.1.1" class="ltx_text" style="font-size:90%;">
S. R. Pokhrel and J. Choi, “A decentralized federated learning approach for
connected autonomous vehicles,” in </span><em id="bib.bib89.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2020 IEEE Wireless Communications
and Networking Conference Workshops (WCNCW)</em><span id="bib.bib89.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2020, pp. 1–6.
</span>
</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock"><span id="bib.bib90.1.1" class="ltx_text" style="font-size:90%;">
Z. Yu, J. Hu, G. Min, H. Xu, and J. Mills, “Proactive content caching for
internet-of-vehicles based on peer-to-peer federated learning,” in
</span><em id="bib.bib90.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2020 IEEE 26th International Conference on Parallel and Distributed
Systems (ICPADS)</em><span id="bib.bib90.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2020, pp.
601–608.
</span>
</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock"><span id="bib.bib91.1.1" class="ltx_text" style="font-size:90%;">
J.-H. Chen, M.-R. Chen, G.-Q. Zeng, and J.-S. Weng, “Bdfl: a
byzantine-fault-tolerance decentralized federated learning method for
autonomous vehicle,” </span><em id="bib.bib91.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Trans. Veh. Technol.</em><span id="bib.bib91.3.3" class="ltx_text" style="font-size:90%;">, vol. 70, no. 9, pp.
8639–8652, Aug. 2021.
</span>
</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock"><span id="bib.bib92.1.1" class="ltx_text" style="font-size:90%;">
L. Barbieri, S. Savazzi, M. Brambilla, and M. Nicoli, “Decentralized federated
learning for extended sensing in 6g connected vehicles,” </span><em id="bib.bib92.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Veh.
Commun.</em><span id="bib.bib92.3.3" class="ltx_text" style="font-size:90%;">, vol. 33, p. 100396, Jan. 2022.
</span>
</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock"><span id="bib.bib93.1.1" class="ltx_text" style="font-size:90%;">
A. Nguyen, T. Do, M. Tran, B. X. Nguyen, C. Duong, T. Phan, E. Tjiputra, and
Q. D. Tran, “Deep federated learning for autonomous driving,” in </span><em id="bib.bib93.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2022
IEEE Intelligent Vehicles Symposium (IV)</em><span id="bib.bib93.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2022, pp. 1824–1830.
</span>
</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock"><span id="bib.bib94.1.1" class="ltx_text" style="font-size:90%;">
D. Su, Y. Zhou, and L. Cui, “Boost decentralized federated learning in
vehicular networks by diversifying data sources,” in </span><em id="bib.bib94.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2022 IEEE 30th
International Conference on Network Protocols (ICNP)</em><span id="bib.bib94.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2022, pp. 1–11.
</span>
</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock"><span id="bib.bib95.1.1" class="ltx_text" style="font-size:90%;">
Y. Lu, X. Huang, Y. Dai, S. Maharjan, and Y. Zhang, “Federated learning for
data privacy preservation in vehicular cyber-physical systems,” </span><em id="bib.bib95.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE
Netw.</em><span id="bib.bib95.3.3" class="ltx_text" style="font-size:90%;">, vol. 34, no. 3, pp. 50–56, Jun. 2020.
</span>
</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock"><span id="bib.bib96.1.1" class="ltx_text" style="font-size:90%;">
B. C. Tedeschini, S. Savazzi, R. Stoklasa, L. Barbieri, I. Stathopoulos,
M. Nicoli, and L. Serio, “Decentralized federated learning for healthcare
networks: A case study on tumor segmentation,” </span><em id="bib.bib96.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Access</em><span id="bib.bib96.3.3" class="ltx_text" style="font-size:90%;">, vol. 10,
pp. 8693–8708, Jan. 2022.
</span>
</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock"><span id="bib.bib97.1.1" class="ltx_text" style="font-size:90%;">
T. Nguyen, M. Dakka, S. Diakiw, M. VerMilyea, M. Perugini, J. Hall, and
D. Perugini, “A novel decentralized federated learning approach to train on
globally distributed, poor quality, and protected private medical data,”
</span><em id="bib.bib97.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Sci. Rep.</em><span id="bib.bib97.3.3" class="ltx_text" style="font-size:90%;">, vol. 12, no. 1, p. 8888, May 2022.
</span>
</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock"><span id="bib.bib98.1.1" class="ltx_text" style="font-size:90%;">
F. Wilhelmi, E. Guerra, and P. Dini, “On the decentralization of
blockchain-enabled asynchronous federated learning,” </span><em id="bib.bib98.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint
arXiv:2205.10201</em><span id="bib.bib98.3.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock"><span id="bib.bib99.1.1" class="ltx_text" style="font-size:90%;">
A. Koloskova, T. Lin, S. U. Stich, and M. Jaggi, “Decentralized deep learning
with arbitrary communication compression,” in </span><em id="bib.bib99.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference
on Learning Representations</em><span id="bib.bib99.3.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock"><span id="bib.bib100.1.1" class="ltx_text" style="font-size:90%;">
Y. Belal, A. Bellet, S. B. Mokhtar, and V. Nitu, “Pepper: Empowering
user-centric recommender systems over gossip learning,” </span><em id="bib.bib100.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. ACM
Interact. Mob. Wearable Ubiquitous Technol.</em><span id="bib.bib100.3.3" class="ltx_text" style="font-size:90%;">, vol. 6, no. 3, pp. 1–27, Sep.
2022.
</span>
</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock"><span id="bib.bib101.1.1" class="ltx_text" style="font-size:90%;">
X. Lian, W. Zhang, C. Zhang, and J. Liu, “Asynchronous decentralized parallel
stochastic gradient descent,” in </span><em id="bib.bib101.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine
Learning</em><span id="bib.bib101.3.3" class="ltx_text" style="font-size:90%;">.   PMLR, 2018, pp. 3043–3052.
</span>
</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock"><span id="bib.bib102.1.1" class="ltx_text" style="font-size:90%;">
Z. Wang, Y. Hu, S. Yan, Z. Wang, R. Hou, and C. Wu, “Efficient ring-topology
decentralized federated learning with deep generative models for medical data
in ehealthcare systems,” </span><em id="bib.bib102.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Electronics</em><span id="bib.bib102.3.3" class="ltx_text" style="font-size:90%;">, vol. 11, no. 10, p. 1548, May
2022.
</span>
</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock"><span id="bib.bib103.1.1" class="ltx_text" style="font-size:90%;">
T. Wink and Z. Nochta, “An approach for peer-to-peer federated learning,” in
</span><em id="bib.bib103.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 51st Annual IEEE/IFIP International Conference on Dependable
Systems and Networks Workshops (DSN-W)</em><span id="bib.bib103.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2021, pp. 150–157.
</span>
</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock"><span id="bib.bib104.1.1" class="ltx_text" style="font-size:90%;">
M. Gerla and L. Kleinrock, “On the topological design of distributed computer
networks,” </span><em id="bib.bib104.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Trans. Commun.</em><span id="bib.bib104.3.3" class="ltx_text" style="font-size:90%;">, vol. 25, no. 1, pp. 48–60, Jan.
1977.
</span>
</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock"><span id="bib.bib105.1.1" class="ltx_text" style="font-size:90%;">
H. Xing, O. Simeone, and S. Bi, “Decentralized federated learning via sgd over
wireless d2d networks,” in </span><em id="bib.bib105.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2020 IEEE 21st international workshop on
signal processing advances in wireless communications (SPAWC)</em><span id="bib.bib105.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2020, pp. 1–5.
</span>
</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock"><span id="bib.bib106.1.1" class="ltx_text" style="font-size:90%;">
S. Kalra, J. Wen, J. C. Cresswell, M. Volkovs, and H. Tizhoosh, “Decentralized
federated learning through proxy model sharing,” </span><em id="bib.bib106.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Nat. Commun.</em><span id="bib.bib106.3.3" class="ltx_text" style="font-size:90%;">,
vol. 14, no. 1, p. 2899, May. 2023.
</span>
</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock"><span id="bib.bib107.1.1" class="ltx_text" style="font-size:90%;">
L. Wang, Y. Xu, H. Xu, M. Chen, and L. Huang, “Accelerating decentralized
federated learning in heterogeneous edge computing,” </span><em id="bib.bib107.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Trans. Mob.
Comput.</em><span id="bib.bib107.3.3" class="ltx_text" style="font-size:90%;">, May 2022.
</span>
</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock"><span id="bib.bib108.1.1" class="ltx_text" style="font-size:90%;">
Z. Tang, S. Shi, and X. Chu, “Communication-efficient decentralized learning
with sparsification and adaptive peer selection,” </span><em id="bib.bib108.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint
arXiv:2002.09692</em><span id="bib.bib108.3.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock"><span id="bib.bib109.1.1" class="ltx_text" style="font-size:90%;">
P. Zhou, Q. Lin, D. Loghin, B. C. Ooi, Y. Wu, and H. Yu,
“Communication-efficient decentralized machine learning over heterogeneous
networks,” in </span><em id="bib.bib109.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 IEEE 37th International Conference on Data
Engineering (ICDE)</em><span id="bib.bib109.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2021, pp.
384–395.
</span>
</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock"><span id="bib.bib110.1.1" class="ltx_text" style="font-size:90%;">
T.-T. Kuo and A. Pham, “Detecting model misconducts in decentralized
healthcare federated learning,” </span><em id="bib.bib110.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Int. J. Med. Inform.</em><span id="bib.bib110.3.3" class="ltx_text" style="font-size:90%;">, vol. 158, p.
104658, Feb. 2022.
</span>
</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock"><span id="bib.bib111.1.1" class="ltx_text" style="font-size:90%;">
L. Wang, X. Zhao, Z. Lu, L. Wang, and S. Zhang, “Enhancing privacy
preservation and trustworthiness for decentralized federated learning,”
</span><em id="bib.bib111.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Inf. Sci.</em><span id="bib.bib111.3.3" class="ltx_text" style="font-size:90%;">, vol. 628, pp. 449–468, May 2023.
</span>
</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock"><span id="bib.bib112.1.1" class="ltx_text" style="font-size:90%;">
T.-T. Kuo and L. Ohno-Machado, “Modelchain: Decentralized privacy-preserving
healthcare predictive modeling framework on private blockchain networks,”
</span><em id="bib.bib112.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1802.01746</em><span id="bib.bib112.3.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock"><span id="bib.bib113.1.1" class="ltx_text" style="font-size:90%;">
X. Chen, J. Ji, C. Luo, W. Liao, and P. Li, “When machine learning meets
blockchain: A decentralized, privacy-preserving and secure design,” in
</span><em id="bib.bib113.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2018 IEEE international conference on big data (big data)</em><span id="bib.bib113.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2018, pp. 1178–1187.
</span>
</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock"><span id="bib.bib114.1.1" class="ltx_text" style="font-size:90%;">
M. Shayan, C. Fung, C. J. Yoon, and I. Beschastnikh, “Biscotti: A blockchain
system for private and secure federated learning,” </span><em id="bib.bib114.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Trans.
Parallel Distrib. Syst.</em><span id="bib.bib114.3.3" class="ltx_text" style="font-size:90%;">, vol. 32, no. 7, pp. 1513–1525, Dec. 2020.
</span>
</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock"><span id="bib.bib115.1.1" class="ltx_text" style="font-size:90%;">
L. U. Khan, S. R. Pandey, N. H. Tran, W. Saad, Z. Han, M. N. Nguyen, and C. S.
Hong, “Federated learning for edge networks: Resource optimization and
incentive mechanism,” </span><em id="bib.bib115.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Commun. Mag.</em><span id="bib.bib115.3.3" class="ltx_text" style="font-size:90%;">, vol. 58, no. 10, pp. 88–93,
Nov. 2020.
</span>
</span>
</li>
</ul>
</section>
<figure id="id1" class="ltx_float biography">
<table id="id1.1" class="ltx_tabular">
<tr id="id1.1.1" class="ltx_tr">
<td id="id1.1.1.1" class="ltx_td"><img src="/html/2306.01603/assets/authors/Yuan.jpg" id="id1.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="100" height="125" alt="[Uncaptioned image]"></td>
<td id="id1.1.1.2" class="ltx_td">
<span id="id1.1.1.2.1" class="ltx_inline-block">
<span id="id1.1.1.2.1.1" class="ltx_p"><span id="id1.1.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Liangqi Yuan</span><span id="id1.1.1.2.1.1.2" class="ltx_text" style="font-size:90%;"> 
(S’22) received the B.E. degree from the Beijing Information Science and Technology University, Beijing, China, in 2020, and the M.Sc. degree from the Oakland University, Rochester, MI, USA, in 2022. He is currently pursuing the Ph.D. degree with the School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA. His research interests are in the areas of sensors, the internet of things, human–computer interaction, signal processing, and machine learning.</span></span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id2" class="ltx_float biography">
<table id="id2.1" class="ltx_tabular">
<tr id="id2.1.1" class="ltx_tr">
<td id="id2.1.1.1" class="ltx_td"><img src="/html/2306.01603/assets/authors/Sun.jpg" id="id2.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="94" height="125" alt="[Uncaptioned image]"></td>
<td id="id2.1.1.2" class="ltx_td">
<span id="id2.1.1.2.1" class="ltx_inline-block">
<span id="id2.1.1.2.1.1" class="ltx_p"><span id="id2.1.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Lichao Sun</span><span id="id2.1.1.2.1.1.2" class="ltx_text" style="font-size:90%;">  received the Ph.D. degree in computer science from the University of Illinois Chicago, Chicago, IL, USA, in 2020, under the supervision of Prof. Philip S. Yu.</span></span>
<span id="id2.1.1.2.1.2" class="ltx_p"><span id="id2.1.1.2.1.2.1" class="ltx_text" style="font-size:90%;">He is currently an Assistant Professor with the Department of Computer Science and Engineering, Lehigh University, Bethlehem, PA, USA. He has published more than 45 research articles in top conferences and journals, such as CCS, USENIXSecurity, NeurIPS, KDD, ICLR, the Advancement of AI (AAAI), the International Joint Conference on AI (IJCAI), ACL, NAACL, TII, TNNLS, and TMC. His research interests include security and privacy in deep learning and data mining. He mainly focuses on artificial intelligence (AI) security and privacy, social networks, and NLP applications.</span></span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id3" class="ltx_float biography">
<table id="id3.1" class="ltx_tabular">
<tr id="id3.1.1" class="ltx_tr">
<td id="id3.1.1.1" class="ltx_td"><img src="/html/2306.01603/assets/x5.png" id="id3.1.1.1.g1" class="ltx_graphics ltx_img_square" width="77" height="77" alt="[Uncaptioned image]"></td>
<td id="id3.1.1.2" class="ltx_td">
<span id="id3.1.1.2.1" class="ltx_inline-block">
<span id="id3.1.1.2.1.1" class="ltx_p"><span id="id3.1.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Philip S. Yu</span><span id="id3.1.1.2.1.1.2" class="ltx_text" style="font-size:90%;">  (Life Fellow, IEEE) received the B.S. degree in electrical engineering (E.E.) from the National Taiwan University, New Taipei, Taiwan, in 1992, the M.S. and Ph.D. degrees in E.E. from Stanford University, Stanford, CA, USA, in 1976 and 1978, respectively, and the M.B.A. degree from New York University, New York, NY, USA, in 1982.</span></span>
<span id="id3.1.1.2.1.2" class="ltx_p"><span id="id3.1.1.2.1.2.1" class="ltx_text" style="font-size:90%;">He is currently a Distinguished Professor of computer science with the University of Illinois Chicago (UIC), Chicago, IL, USA, and also holds the Wexler Chair in Information Technology. Before joining UIC, he was with IBM, USA, where he was the Manager of the Software Tools and Techniques Department, Watson Research Center. He has published more than 1200 papers in refereed journals and conferences. He holds or has applied for more than 300 U.S. patents. His research interest is on big data, including data mining, data stream, database, and privacy.</span></span>
<span id="id3.1.1.2.1.3" class="ltx_p"><span id="id3.1.1.2.1.3.1" class="ltx_text" style="font-size:90%;">Dr. Yu is a fellow of the ACM. He was a recipient of the ACM SIGKDD 2016 Innovation Award for his influential research and scientific contributions to mining, fusion, and anonymization of big data, the IEEE Computer Society’s 2013 Technical Achievement Award for pioneering and fundamentally innovative contributions to the scalable indexing, querying, searching, mining, and anonymization of big data, and the Research Contributions Award from IEEE International Conference on Data Mining (ICDM) in 2003 for his pioneering contributions to the field of data mining. He received the ICDM 2013 10-Year Highest-Impact Paper Award and the EDBT Test of Time Award in 2014. He was the Editor-in-Chief of ACM Transactions on Knowledge Discovery from Data from 2011 to 2017 and IEEE Transactions on Knowledge and Data Engineering from 2001 to 2004.</span></span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id4" class="ltx_float biography">
<table id="id4.1" class="ltx_tabular">
<tr id="id4.1.1" class="ltx_tr">
<td id="id4.1.1.1" class="ltx_td"><img src="/html/2306.01603/assets/authors/Ziran_IEEE.jpg" id="id4.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="100" height="125" alt="[Uncaptioned image]"></td>
<td id="id4.1.1.2" class="ltx_td">
<span id="id4.1.1.2.1" class="ltx_inline-block">
<span id="id4.1.1.2.1.1" class="ltx_p"><span id="id4.1.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Ziran Wang</span><span id="id4.1.1.2.1.1.2" class="ltx_text" style="font-size:90%;"> 
(S’16-M’19) received the Ph.D. degree from the University of California, Riverside in 2019. He is an Assistant Professor in the College of Engineering at Purdue University, and was a Principal Researcher at Toyota Motor North America. He serves as Founding Chair of IEEE Technical Committee on Internet of Things in Intelligent Transportation Systems, and Associate Editor of four academic journals, including IEEE Internet of Things Journal and IEEE Transactions on Intelligent Vehicles. His research focuses on automated driving, human-autonomy teaming, and digital twin.</span></span>
</span>
</td>
</tr>
</table>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.01602" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.01603" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.01603">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.01603" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.01604" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 03:06:47 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
