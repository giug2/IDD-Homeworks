<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>On Synthetic Texture Datasets: Challenges, Creation, and Curation</title>
<!--Generated on Mon Sep 16 14:01:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.10297v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx1" title="In On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_title">Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx2" title="In On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_title">Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx2.SS0.SSSx1" title="In Background ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_title">Texture images.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx2.SS0.SSSx2" title="In Background ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_title">Text-to-image models and data metrics.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx3" title="In On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_title">The Prompted Textures Dataset (PTD)</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx3.SSx1" title="In The Prompted Textures Dataset (PTD) ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_title">Creating Prompts</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx3.SSx1.SSSx1" title="In Creating Prompts ‣ The Prompted Textures Dataset (PTD) ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_title">Selecting Descriptors.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx3.SSx2" title="In The Prompted Textures Dataset (PTD) ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_title">Generating Images</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx3.SSx2.SSSx1" title="In Generating Images ‣ The Prompted Textures Dataset (PTD) ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_title">Investigating Safety Filtering.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx3.SSx3" title="In The Prompted Textures Dataset (PTD) ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_title">Refinement</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx3.SSx3.SSSx1" title="In Refinement ‣ The Prompted Textures Dataset (PTD) ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_title">Prompt Quality Trends.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx4" title="In On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_title">Quality evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx4.SSx1" title="In Quality evaluation ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_title">Standard metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx4.SSx2" title="In Quality evaluation ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_title">Human Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx4.SSx2.SSSx1" title="In Human Evaluation ‣ Quality evaluation ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_title">Suitability of CLIP Scores.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx5" title="In On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_title">Conclusions</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">On Synthetic Texture Datasets: Challenges, Creation, and Curation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Blaine Hoak<sup class="ltx_sup" id="id1.1.id1">1</sup>,
Patrick McDaniel<sup class="ltx_sup" id="id2.2.id2">1</sup>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">The influence of textures on machine learning models has been an ongoing investigation, specifically in texture bias/learning, interpretability, and robustness. However, due to the lack of large and diverse texture data available, the findings in these works have been limited, as more comprehensive evaluations have not been feasible. Image generative models are able to provide data creation at scale, but utilizing these models for texture synthesis has been unexplored and poses additional challenges both in creating accurate texture images and validating those images. In this work, we introduce an extensible methodology and corresponding new dataset for generating high-quality, diverse texture images capable of supporting a broad set of texture-based tasks. Our pipeline consists of: (1) developing prompts from a range of descriptors to serve as input to text-to-image models, (2) adopting and adapting Stable Diffusion pipelines to generate and filter the corresponding images, and (3) further filtering down to the highest quality images. Through this, we create the Prompted Textures Dataset (PTD), a dataset of 362,880 texture images that span 56 textures. During the process of generating images, we find that NSFW safety filters in image generation pipelines are highly sensitive to texture (and flag up to 60% of our texture images), uncovering a potential bias in these models and presenting unique challenges when working with texture data. Through both standard metrics and a human evaluation, we find that our dataset is high quality and diverse.</p>
</div>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Introduction</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Large, high-quality data sources have long been a defining factor in the success of AI research, and have enabled progress in a plethora of fields including (but not limited to): object classification <cite class="ltx_cite ltx_citemacro_citep">(Russakovsky et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib16" title="">2015</a>)</cite>, visual emotion recognition <cite class="ltx_cite ltx_citemacro_citep">(You et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib20" title="">2016</a>)</cite>, medical image interpretation <cite class="ltx_cite ltx_citemacro_citep">(Irvin et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib11" title="">2019</a>)</cite>, scene recognition <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib22" title="">2018</a>)</cite>, and more. Texture image data and corresponding analysis has played a vital role in uncovering the high-level features image classifiers learn and their implications. Works have shown how models exhibit texture bias <cite class="ltx_cite ltx_citemacro_citep">(Geirhos et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib8" title="">2019</a>)</cite>, how to use texture data to construct texture-object associations <cite class="ltx_cite ltx_citemacro_citep">(Hoak and McDaniel <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib10" title="">2024</a>)</cite>, and even how patterned images can create adversarial examples <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib21" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">However, the findings from prior texture-based works are limited due to the lack of a large and diverse texture dataset. For example,
the most widely used texture dataset, the Describable Textures Dataset (DTD) <cite class="ltx_cite ltx_citemacro_citep">(Cimpoi et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib3" title="">2014</a>)</cite>, contains only 5640 images across 47 texture
categories. This limitation, coupled with the need for diverse texture data, has led to the creation of one-off texture datasets
that are only applicable for specific use cases, such as the shape-cue conflict
dataset in <cite class="ltx_cite ltx_citemacro_citep">(Geirhos et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib8" title="">2019</a>)</cite> and patterned images from
 <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib21" title="">2022</a>)</cite>. Furthermore, current approaches for collating
texture images rely on manual search from
public sources like Flikr <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib5" title="">2014</a>)</cite>. The result of manual methods is omnipresent in the literature; works often rely on <span class="ltx_text ltx_font_italic" id="Sx1.p2.1.1">fewer than 100 texture
images</span> for their findings. As a result, larger scale analysis on the relationship between textures and models has simply not been
feasible.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">The advent of generative AI models has significantly reduced the manual effort required to create new datasets. However, generative models are not explicitly designed
for texture image generation, presenting several challenges including: designing
prompts to achieve specific texture, modifying Stable Diffusion models
to better handle texture data, and ensuring that the generated textures are
diverse, representative, and of high quality. These challenges necessitate novel adaptations to the entire pipeline to enable the generation of texture data.</p>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1">In this work, we introduce an extensible methodology for generating high-quality, representative, diverse texture images
capable of supporting a broad range of texture-based tasks. Our
approach: (1) constructs prompts from a range of
descriptors to serve as input to text-to-image models, (2) adopts and adapts
Stable Diffusion pipelines, which generates and filters the
corresponding images created from our prompts, and (3) further filters the generated textures down to the highest-quality samples through CLIP scores. With this, we create the Prompted Textures Dataset (PTD): a dataset of
362,880 texture images (examples in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx1.F1.sf2" title="In Figure 1 ‣ Introduction ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_tag">1(b)</span></a>) across 56 different texture classes.</p>
</div>
<figure class="ltx_figure" id="Sx1.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="Sx1.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="300" id="Sx1.F1.sf1.g1" src="extracted/5854019/figures/examples/dtd_images_n7.png" width="698"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>DTD <cite class="ltx_cite ltx_citemacro_citep">(Cimpoi et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib3" title="">2014</a>)</cite></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="Sx1.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="300" id="Sx1.F1.sf2.g1" src="extracted/5854019/figures/examples/texture_images_n7.png" width="698"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>PTD (our work)</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples of textures from the Describable Textures Dataset (DTD) <cite class="ltx_cite ltx_citemacro_citep">(Cimpoi et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib3" title="">2014</a>)</cite> and the Prompted Textures Dataset (PTD) (our work).</figcaption>
</figure>
<div class="ltx_para" id="Sx1.p5">
<p class="ltx_p" id="Sx1.p5.1">In creating the PTD, we conduct a series of experiments at each stage of our pipeline. At prompt generation, we investigate the effect of different descriptor combinations on the quality and representativity of generated images. At image generation, we observe that Stable Diffusion pipelines and their built-in filtering mechanisms cause many of our images to be flagged as unsafe content and find that the descriptors most often responsible for this are the texture classes themselves. Finally, at image refinement, we evaluate the suitability of using CLIP scores on texture images to filter down to the highest quality images.</p>
</div>
<div class="ltx_para" id="Sx1.p6">
<p class="ltx_p" id="Sx1.p6.1">In addition to our pipeline experiments, we perform a two-part evaluation on PTD. First, we measure quality and diversity on two established metrics: Inception and FID scores. Second, we conduct a human evaluation study on 900 of our images. We find that our dataset is both high-quality and well representative of the prompts used to for generation. We also find that each part of our pipeline contributes to creating high-quality texture images.
Our contributions are as follows:</p>
<ul class="ltx_itemize" id="Sx1.I1">
<li class="ltx_item" id="Sx1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i1.p1">
<p class="ltx_p" id="Sx1.I1.i1.p1.1">We provide a novel and extensible methodology for creating and refining
prompts that yield high-quality texture images using generative AI.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i2.p1">
<p class="ltx_p" id="Sx1.I1.i2.p1.1">We release a large, high-quality synthetic dataset, called the Prompted Textures Dataset (PTD), that captures a greater breadth and depth of textures than prior work.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i3.p1">
<p class="ltx_p" id="Sx1.I1.i3.p1.1">We uncover interesting and unique challenges that arise from working with texture data; for example, safety filters are overly sensitive to textures, causing up to 60% of our initial images to be flagged as NSFW.
</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Background</h2>
<section class="ltx_subsubsection" id="Sx2.SS0.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Texture images.</h4>
<div class="ltx_para" id="Sx2.SS0.SSSx1.p1">
<p class="ltx_p" id="Sx2.SS0.SSSx1.p1.1">The Describable Textures Dataset (DTD) <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib3" title="">2014</a>)</cite> is perhaps the
most popular texture dataset to date. It contains 5640 images sourced from
Flickr in 47 texture categories such as polka-dotted, scaly, and striped. This
texture dataset has had a variety of uses in computer vision and machine
learning. Most recently, <cite class="ltx_cite ltx_citemacro_citep">(Hoak and McDaniel <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib10" title="">2024</a>)</cite> used the DTD to
construct
texture-object associations to quantify the extent to which specific textures
are
learned by object classification models.
Aside from the DTD, other works operating on texture datasets have created their
own sets of textures to suit their specific use cases.
In <cite class="ltx_cite ltx_citemacro_citep">(Geirhos et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib8" title="">2019</a>)</cite>, the authors construct a shape-cue
conflict dataset, which contains images with the texture of one object and the
shape of another for the purposes of studying if CNNs were more biased towards
texture or shape.
Finally, in <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib21" title="">2022</a>)</cite> patterned images were created and
overlayed onto existing object images; the authors found that this method produced an
effective attack against machine learning models, wherein these patterns caused
the model to misclassify the images and could be constructed even without any
access to the model weights or training data.</p>
</div>
<div class="ltx_para" id="Sx2.SS0.SSSx1.p2">
<p class="ltx_p" id="Sx2.SS0.SSSx1.p2.1">Aside from benchmark texture datasets, there have also been works focused on synthesizing textures. <cite class="ltx_cite ltx_citemacro_citep">(Portilla and Simoncelli <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib13" title="">2000</a>)</cite> introduced the first parametric model for texture synthesis. This model worked by taking reference images of a texture and using the introduced statistical model to expand the textures provided. More recent works have introduced a similar approach using CNNs rather than the previous statistical model <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib6" title="">2015</a>; <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib18" title="">2016</a>)</cite>, which has proved to be extremely useful in style transfer <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib7" title="">2016</a>)</cite>. Additionally, some texture synthesis methods opt for a “quilting” approach that tiles together multiple textures to make a new texture <cite class="ltx_cite ltx_citemacro_citep">(Efros and Freeman <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib4" title="">2001</a>)</cite>. While these approaches are performant at extending existing textures, one of the main drawbacks is the reliance on starting texture images.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Sx2.SS0.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Text-to-image models and data metrics.</h4>
<div class="ltx_para" id="Sx2.SS0.SSSx2.p1">
<p class="ltx_p" id="Sx2.SS0.SSSx2.p1.1">Text-to-image models are generative models that take textual
descriptors as input and aim to generate representative images based on the
provided input. Stable Diffusion (SD) models have become the state-of-the-art in
image generation, demonstrating great promise in producing high-quality images
that closely align with the given prompts <cite class="ltx_cite ltx_citemacro_citep">(Rombach et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib15" title="">2022</a>)</cite>.
These models are trained by iteratively adding noise to latent representations
of images and then learning a denoising process to recover the original images.
During evaluation, the models utilize the learned denoising process to
transform random noise into coherent images, guided by textual descriptions.</p>
</div>
<div class="ltx_para" id="Sx2.SS0.SSSx2.p2">
<p class="ltx_p" id="Sx2.SS0.SSSx2.p2.1">The quality of generated data can be assessed using a variety of metrics,
depending on the desired properties of the generated data. Here, we focus on
quality, diversity, and representativeness of images. When generating image
data using text-to-image models such as Stable Diffusion, it is important to
ensure not only that the images themselves are good, but that they also well
represent the text descriptions that they are generated from. CLIP scores
<cite class="ltx_cite ltx_citemacro_citep">(Hessel et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib9" title="">2022</a>)</cite> were originally introduced as a means to measure
the quality of image captions but have become a popular metric for measuring the
representativeness of generated images with respect to any portion of text and
also play an important role in Stable Diffusion models, both in training and evaluation. CLIP
scores calculate the cosine similarity between the image and text embeddings of a pre-trained CLIP model. Higher CLIP
scores indicate that the images are more representative of the text descriptions
that they are generated from.</p>
</div>
<div class="ltx_para" id="Sx2.SS0.SSSx2.p3">
<p class="ltx_p" id="Sx2.SS0.SSSx2.p3.1">Inception Scores <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib17" title="">2016</a>)</cite> are a popular metric for measuring both the quality and diversity of the
generated images. Using a pre-trained Inception model, KL divergence is measured between the conditional and marginal probability distributions of the generated data. Thus, high Inception Scores will occur when images are strongly predicted as belonging to a single class, and when predictions across the entire set are spread across classes.</p>
</div>
<div class="ltx_para" id="Sx2.SS0.SSSx2.p4">
<p class="ltx_p" id="Sx2.SS0.SSSx2.p4.1">FID scores <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib2" title="">2019</a>)</cite> measure the dissimilarity of the generated images to a set of real images. FID scores measure the Fréchet distance between the feature distributions of the generated and real images from an Inception model. Lower FID scores indicate that the generated images have feature distributions more similar to the real images, and thus are more realistic. When measuring FID, we use the Describable Textures Dataset (DTD) <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib3" title="">2014</a>)</cite> as the source of real images.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">The Prompted Textures Dataset (PTD)</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">To address the gaps in current texture image datasets, we introduce our methodology for creating high quality and diverse texture data, and how we apply this methodology to create the Prompted Textures Dataset (PTD).
An overview of our methodology is shown in
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx3.F2" title="Figure 2 ‣ The Prompted Textures Dataset (PTD) ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_tag">2</span></a>.
All experiments in the following subsections were run on 12 A100 GPUs with 40GB of memory each using CUDA version 11.8.</p>
</div>
<figure class="ltx_figure" id="Sx3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="349" id="Sx3.F2.g1" src="extracted/5854019/figures/dataset-overview.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of the Prompted Textures Dataset methodology. We
(1) create a set of prompts from a variety of descriptors,
(2) input these prompts to the Stable Diffusion model, which then
generates and filters (3) a set of images. From here, we (4) further refine the image sets using CLIP scores, which leaves us
with (5) our final dataset.</figcaption>
</figure>
<section class="ltx_subsection" id="Sx3.SSx1">
<h3 class="ltx_title ltx_title_subsection">Creating Prompts</h3>
<div class="ltx_para" id="Sx3.SSx1.p1">
<p class="ltx_p" id="Sx3.SSx1.p1.1">To create texture data using text-to-image models, we first construct
prompts that describe the textures we want to generate. Later on, these prompts are input to Stable Diffusion <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib15" title="">2022</a>)</cite> to produce the corresponding images.</p>
</div>
<section class="ltx_subsubsection" id="Sx3.SSx1.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Selecting Descriptors.</h4>
<div class="ltx_para" id="Sx3.SSx1.SSSx1.p1">
<p class="ltx_p" id="Sx3.SSx1.SSSx1.p1.1">In creating our texture images, the goal is to produce high quality, diverse, and descriptive prompts that will result in a wide variety of high-quality texture images. To achieve this, we want to not only capture a wide variety of textures and patterns, but also want each texture/pattern to be varied itself, and have examples of many different styles, colors, size, shape, and other elements that effect how each texture or pattern may be represented. In other words, we want to produce more than just “a striped image”. To achieve this, we introduce these elements as additional descriptive words in our prompts. By providing <span class="ltx_text ltx_font_italic" id="Sx3.SSx1.SSSx1.p1.1.1">specific</span> descriptions of the textures we want to generate, we can ensure that the images we ultimately generate diverse in a controlled manner, rather than solely relying on diversity coming from randomness introduced by the generative model.</p>
</div>
<div class="ltx_para" id="Sx3.SSx1.SSSx1.p2">
<p class="ltx_p" id="Sx3.SSx1.SSSx1.p2.1">When producing the Describable Textures Dataset, <cite class="ltx_cite ltx_citemacro_citet">Cimpoi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib3" title="">2014</a>)</cite> cultivated a list of 47 different texture classes to serve as the building blocks for their dataset. We start from this list and then source other lists of textures <cite class="ltx_cite ltx_citemacro_citep">(Barnett <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib1" title="">2023</a>)</cite> to identify new candidate textures for us to generate. We consolidate down to only the textures that are meaningfully different from our starting 47 classes. From this, we add 9 additional texture classes. In total, we have 56 texture classes to serve as the
foundation for our prompts.</p>
</div>
<div class="ltx_para" id="Sx3.SSx1.SSSx1.p3">
<p class="ltx_p" id="Sx3.SSx1.SSSx1.p3.1">We then provide more specificity to our prompting by introducing other descriptive categories, which we choose by drawing inspiration from the 7 basic elements of art: line, shape, form, texture, space, color, and value <cite class="ltx_cite ltx_citemacro_citep">(Ocvirk et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib12" title="">2001</a>)</cite>. When selecting these descriptive categories, we focus on those that will allow us to capture multiple elements in a single category, and particularly on the elements that are not already captured by the texture classes we introduce (e.g., texture, shape, line) in order to keep our prompt space tractable. For example, we choose to include color enhancers as a category to not only enhance the colors presented in the color category, but also as a way to introduce value and form into the prompts. We aim to select a few words for each category that are sufficiently different from each other to capture as many unique textures as possible.</p>
</div>
<div class="ltx_para" id="Sx3.SSx1.SSSx1.p4">
<p class="ltx_p" id="Sx3.SSx1.SSSx1.p4.1">An overview of all the descriptor categories and the specific words
used in each category are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx3.T1" title="Table 1 ‣ Selecting Descriptors. ‣ Creating Prompts ‣ The Prompted Textures Dataset (PTD) ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_tag">1</span></a>. We produce prompts combining one word from each category, in standard English adjective ordering:</p>
</div>
<div class="ltx_para ltx_noindent" id="Sx3.SSx1.SSSx1.p5">
<p class="ltx_p" id="Sx3.SSx1.SSSx1.p5.1">{<span class="ltx_text ltx_font_italic" id="Sx3.SSx1.SSSx1.p5.1.1">Artistic</span>}<span class="ltx_text ltx_font_italic" id="Sx3.SSx1.SSSx1.p5.1.2"> </span>{<span class="ltx_text ltx_font_italic" id="Sx3.SSx1.SSSx1.p5.1.3">Spatial</span>}<span class="ltx_text ltx_font_italic" id="Sx3.SSx1.SSSx1.p5.1.4"> </span>{<span class="ltx_text ltx_font_italic" id="Sx3.SSx1.SSSx1.p5.1.5">Color Enhancer</span>}<span class="ltx_text ltx_font_italic" id="Sx3.SSx1.SSSx1.p5.1.6"> </span>{<span class="ltx_text ltx_font_italic" id="Sx3.SSx1.SSSx1.p5.1.7">Color</span>}<span class="ltx_text ltx_font_italic" id="Sx3.SSx1.SSSx1.p5.1.8"> </span>{<span class="ltx_text ltx_font_italic" id="Sx3.SSx1.SSSx1.p5.1.9">Texture</span>}<span class="ltx_text ltx_font_italic" id="Sx3.SSx1.SSSx1.p5.1.10">
</span></p>
</div>
<div class="ltx_para ltx_noindent" id="Sx3.SSx1.SSSx1.p6">
<p class="ltx_p" id="Sx3.SSx1.SSSx1.p6.1">We enumerate across all possible combinations of words, and produce prompts such as “impressionistic randomized vivid red striped texture”. This results in a total
of 96,768 prompts to use when subsequently producing our texture images.</p>
</div>
<figure class="ltx_table" id="Sx3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Sx3.T1.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx3.T1.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="Sx3.T1.4.5.1.1"><span class="ltx_text ltx_font_bold" id="Sx3.T1.4.5.1.1.1">Categories</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="Sx3.T1.4.5.1.2">
<span class="ltx_inline-block ltx_align_top" id="Sx3.T1.4.5.1.2.1">
<span class="ltx_p" id="Sx3.T1.4.5.1.2.1.1"><span class="ltx_text ltx_font_bold" id="Sx3.T1.4.5.1.2.1.1.1">Descriptors</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx3.T1.4.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Sx3.T1.4.6.1.1"><span class="ltx_text ltx_font_bold" id="Sx3.T1.4.6.1.1.1">textures</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Sx3.T1.4.6.1.2">
<span class="ltx_inline-block ltx_align_top" id="Sx3.T1.4.6.1.2.1">
<span class="ltx_p" id="Sx3.T1.4.6.1.2.1.1">banded, blotchy, braided, bubbly, bumpy, checkered,
cobwebbed, cracked, crosshatched, crystalline, dotted, fibrous, flecked,
freckled, frilly, gauzy, grid, grooved, honeycombed, interlaced, knitted,
lacelike, lined, marbled, matted, meshed, paisley, perforated, pitted, pleated,
polka-dotted, porous, potholed, scaly, smeared, spiraled, sprinkled, stained,
stratified, striped, studded, swirly, veined, waffled, woven, wrinkled,
zigzagged, flaky, chapped, hairy, leathery, feathered, spiky, fluffy, ribbed,
wavy</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Sx3.T1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx3.T1.1.1.2.1">artistic</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Sx3.T1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="Sx3.T1.1.1.1.1">
<span class="ltx_p" id="Sx3.T1.1.1.1.1.1"><math alttext="\emptyset" class="ltx_Math" display="inline" id="Sx3.T1.1.1.1.1.1.m1.1"><semantics id="Sx3.T1.1.1.1.1.1.m1.1a"><mi id="Sx3.T1.1.1.1.1.1.m1.1.1" mathvariant="normal" xref="Sx3.T1.1.1.1.1.1.m1.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="Sx3.T1.1.1.1.1.1.m1.1b"><emptyset id="Sx3.T1.1.1.1.1.1.m1.1.1.cmml" xref="Sx3.T1.1.1.1.1.1.m1.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.1.1.1.1.1.m1.1c">\emptyset</annotation><annotation encoding="application/x-llamapun" id="Sx3.T1.1.1.1.1.1.m1.1d">∅</annotation></semantics></math>, impressionist, photorealistic, minimal</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Sx3.T1.2.2.2"><span class="ltx_text ltx_font_bold" id="Sx3.T1.2.2.2.1">spatial</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Sx3.T1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="Sx3.T1.2.2.1.1">
<span class="ltx_p" id="Sx3.T1.2.2.1.1.1"><math alttext="\emptyset" class="ltx_Math" display="inline" id="Sx3.T1.2.2.1.1.1.m1.1"><semantics id="Sx3.T1.2.2.1.1.1.m1.1a"><mi id="Sx3.T1.2.2.1.1.1.m1.1.1" mathvariant="normal" xref="Sx3.T1.2.2.1.1.1.m1.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="Sx3.T1.2.2.1.1.1.m1.1b"><emptyset id="Sx3.T1.2.2.1.1.1.m1.1.1.cmml" xref="Sx3.T1.2.2.1.1.1.m1.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.2.2.1.1.1.m1.1c">\emptyset</annotation><annotation encoding="application/x-llamapun" id="Sx3.T1.2.2.1.1.1.m1.1d">∅</annotation></semantics></math>, randomized, symmetrical</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Sx3.T1.3.3.2"><span class="ltx_text ltx_font_bold" id="Sx3.T1.3.3.2.1">enhancer</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="Sx3.T1.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="Sx3.T1.3.3.1.1">
<span class="ltx_p" id="Sx3.T1.3.3.1.1.1"><math alttext="\emptyset" class="ltx_Math" display="inline" id="Sx3.T1.3.3.1.1.1.m1.1"><semantics id="Sx3.T1.3.3.1.1.1.m1.1a"><mi id="Sx3.T1.3.3.1.1.1.m1.1.1" mathvariant="normal" xref="Sx3.T1.3.3.1.1.1.m1.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="Sx3.T1.3.3.1.1.1.m1.1b"><emptyset id="Sx3.T1.3.3.1.1.1.m1.1.1.cmml" xref="Sx3.T1.3.3.1.1.1.m1.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.3.3.1.1.1.m1.1c">\emptyset</annotation><annotation encoding="application/x-llamapun" id="Sx3.T1.3.3.1.1.1.m1.1d">∅</annotation></semantics></math>, gradient, vivid, muted, iridescent, neon,
faded, watercolor, earthy</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="Sx3.T1.4.4.2"><span class="ltx_text ltx_font_bold" id="Sx3.T1.4.4.2.1">color</span></th>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="Sx3.T1.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="Sx3.T1.4.4.1.1">
<span class="ltx_p" id="Sx3.T1.4.4.1.1.1"><math alttext="\emptyset" class="ltx_Math" display="inline" id="Sx3.T1.4.4.1.1.1.m1.1"><semantics id="Sx3.T1.4.4.1.1.1.m1.1a"><mi id="Sx3.T1.4.4.1.1.1.m1.1.1" mathvariant="normal" xref="Sx3.T1.4.4.1.1.1.m1.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="Sx3.T1.4.4.1.1.1.m1.1b"><emptyset id="Sx3.T1.4.4.1.1.1.m1.1.1.cmml" xref="Sx3.T1.4.4.1.1.1.m1.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.4.4.1.1.1.m1.1c">\emptyset</annotation><annotation encoding="application/x-llamapun" id="Sx3.T1.4.4.1.1.1.m1.1d">∅</annotation></semantics></math>, red, green, blue, yellow, black-and-white, pastel,
neutral</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Descriptors for texture prompts. <math alttext="\emptyset" class="ltx_Math" display="inline" id="Sx3.T1.6.m1.1"><semantics id="Sx3.T1.6.m1.1b"><mi id="Sx3.T1.6.m1.1.1" mathvariant="normal" xref="Sx3.T1.6.m1.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="Sx3.T1.6.m1.1c"><emptyset id="Sx3.T1.6.m1.1.1.cmml" xref="Sx3.T1.6.m1.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.6.m1.1d">\emptyset</annotation><annotation encoding="application/x-llamapun" id="Sx3.T1.6.m1.1e">∅</annotation></semantics></math> indicates an empty string.</figcaption>
</figure>
<div class="ltx_para" id="Sx3.SSx1.SSSx1.p7">
<p class="ltx_p" id="Sx3.SSx1.SSSx1.p7.1">It is important to note that this methodology for creating prompts is not limited to texture prompts alone. The methodology we present here can be applied to any task that generates images from prompts. For example, by swapping out the texture descriptors for shape descriptors, one could generate prompts such as “impressionistic vivid red circle” or “photo-realistic green square” to generate shape images. This way, the diversity we introduce through prompt descriptors can be applied to a wide variety of image generation tasks.</p>
</div>
<div class="ltx_para" id="Sx3.SSx1.SSSx1.p8">
<p class="ltx_p" id="Sx3.SSx1.SSSx1.p8.1">In addition to being applicable to non-texture data, this methodology can also be applied to generate other texture phenomena. For example, the images used in <cite class="ltx_cite ltx_citemacro_citet">Geirhos et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib8" title="">2019</a>)</cite> to study the existence of texture bias were selected due to their likeness of objects present in the ImageNet <cite class="ltx_cite ltx_citemacro_citep">(Russakovsky et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib16" title="">2015</a>)</cite> object classes. To replicate a similar study, texture classes such as “elephant skin” or “wood grain” could be introduced. Given that the requirements for texture data are highly dependant on the specific task, a primary goal when building our pipeline was to ensure extensibility and applicability to a wide variety of texture-based tasks.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="Sx3.SSx2">
<h3 class="ltx_title ltx_title_subsection">Generating Images</h3>
<div class="ltx_para" id="Sx3.SSx2.p1">
<p class="ltx_p" id="Sx3.SSx2.p1.1">Next, we discuss how we use and adapt Stable Diffusion pipelines to generate and filter images from our prompts, and investigate specific challenges we uncover when using this pipeline on texture data.</p>
</div>
<div class="ltx_para" id="Sx3.SSx2.p2">
<p class="ltx_p" id="Sx3.SSx2.p2.1">To generate the images for our dataset, we use our created texture prompts as input to
text-to-image generative models. Specifically, we use the Stable Diffusion v1.5 model
<cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib15" title="">2022</a>)</cite> from
HuggingFace <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib19" title="">2020</a>)</cite>.
The Stable Diffusion pipeline also contains a content safety filter to check the generated images for content that is deemed inappropriate, explicit, or NSFW (Not Safe For Work). These safety checks measure the CLIP score of the images with secret NSFW content words (though there have been efforts to reverse engineer the words <cite class="ltx_cite ltx_citemacro_citep">(Rando et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib14" title="">2022</a>)</cite>). If this score is above a certain threshold, the image will be flagged and replaced by an all black image. These NSFW filters have been added to many of the large HuggingFace generative models as a way to safeguard against producing potentially harmful or inappropriate images.</p>
</div>
<div class="ltx_para" id="Sx3.SSx2.p3">
<p class="ltx_p" id="Sx3.SSx2.p3.1">Interestingly, one of the main challenges that we face during the generation of our images is to actually get images <span class="ltx_text ltx_font_italic" id="Sx3.SSx2.p3.1.1">not</span> flagged as “NSFW”, even despite the fact that our prompts are void of anything that would suggest the generative model to produce NSFW content. To ensure we were producing the same number of images per prompt, we regenerate images in the case that they are flagged by the safety filter. For further analysis in the next section, we further adapt the Stable Diffusion Pipeline by disabling the safety filter to record when an image is flagged as NSFW but still get back the original (non-corrupted) image. For ethical reasons, we do not detail how we disable the safety filter, or release the images flagged as NSFW. However, it is important to note that we <span class="ltx_text ltx_font_bold" id="Sx3.SSx2.p3.1.2">find no images that actually represent explicit content</span>.</p>
</div>
<div class="ltx_para" id="Sx3.SSx2.p4">
<p class="ltx_p" id="Sx3.SSx2.p4.1">With these adaptations made to the Stable Diffusion pipeline, we then generate 5 images for each of our 96,768 prompts. This results in the creation of 483,840 images before any additional filtering, not including the images flagged by the safety filter. A randomly selected subset of the images we generate can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx1.F1.sf2" title="In Figure 1 ‣ Introduction ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_tag">1(b)</span></a>.</p>
</div>
<section class="ltx_subsubsection" id="Sx3.SSx2.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Investigating Safety Filtering.</h4>
<div class="ltx_para" id="Sx3.SSx2.SSSx1.p1">
<p class="ltx_p" id="Sx3.SSx2.SSSx1.p1.1">With as many as 60% of our total images being flagged as NSFW, this was initially a major
barrier in producing our dataset and a perplexing issue. Our prompts are
relatively simple and void of any suggestions that the generative model <span class="ltx_text ltx_font_italic" id="Sx3.SSx2.SSSx1.p1.1.1">should</span> produce
NSFW content. This leads us to two questions: (1) <span class="ltx_text ltx_font_italic" id="Sx3.SSx2.SSSx1.p1.1.2">Do the flagged images actually represent explicit content?</span> and (2) <span class="ltx_text ltx_font_italic" id="Sx3.SSx2.SSSx1.p1.1.3">What is causing the NSFW filter to flag our images?</span></p>
</div>
<div class="ltx_para" id="Sx3.SSx2.SSSx1.p2">
<p class="ltx_p" id="Sx3.SSx2.SSSx1.p2.1">With our modified Stable Diffusion pipeline and the safety filter disabled, we investigate the flagged images to see if they may actually contain explicit content or if this phenomenon is demonstrating a larger issue with safety filters. In the process of generating images that pass the safety filter until we have 483,840 images (5 images per prompt), we save all the images that are flagged as NSFW. This results in a total of 133,857 flagged images. From all the images we observe, we find no examples of images that actually represent explicit content. Randomly chosen examples from the entire set of images that are flagged as NSFW are shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx3.F3" title="Figure 3 ‣ Investigating Safety Filtering. ‣ Generating Images ‣ The Prompted Textures Dataset (PTD) ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="Sx3.SSx2.SSSx1.p3">
<p class="ltx_p" id="Sx3.SSx2.SSSx1.p3.1">When observing differences between images that passed the safety filter and the images that did not (e.g., images from Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx1.F1.sf2" title="In Figure 1 ‣ Introduction ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_tag">1(b)</span></a> vs. the images in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx3.F3" title="Figure 3 ‣ Investigating Safety Filtering. ‣ Generating Images ‣ The Prompted Textures Dataset (PTD) ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_tag">3</span></a>), we find that the flagged images are typically smoother, more dull in color, and appear “noisier”. Besides these observations, we do not find any clear and obvious differences between the image sets or any indication as to why these images are flagged.</p>
</div>
<figure class="ltx_figure" id="Sx3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="252" id="Sx3.F3.g1" src="extracted/5854019/figures/examples/nsfw_texture_images_n7.png" width="586"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Examples of images flagged as NSFW.</figcaption>
</figure>
<div class="ltx_para" id="Sx3.SSx2.SSSx1.p4">
<p class="ltx_p" id="Sx3.SSx2.SSSx1.p4.1">To better understand what causes the NSFW filter to flag images, we investigate if some prompts are more likely to produce images that are flagged as NSFW based on the descriptors in the prompts. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx3.F4" title="Figure 4 ‣ Investigating Safety Filtering. ‣ Generating Images ‣ The Prompted Textures Dataset (PTD) ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_tag">4</span></a> we show the ratio of total images flagged as NSFW (in red) as well as the ratio of of prompts that result in at least one image being flagged as NSFW (in blue), separated by word present in the prompt.</p>
</div>
<figure class="ltx_figure" id="Sx3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="221" id="Sx3.F4.g1" src="extracted/5854019/figures/nsfw/nsfw_ratio_vertical_stacked.png" width="663"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Ratio of total images flagged as NSFW (red) and ratio of prompts with at least one images flagged as NSFW (blue) separated by word present in the prompt.</figcaption>
</figure>
<div class="ltx_para" id="Sx3.SSx2.SSSx1.p5">
<p class="ltx_p" id="Sx3.SSx2.SSSx1.p5.1">From these results, we see fairly high ratios in both figures, suggesting that not only are many of the total images across all
prompts containing each word getting flagged, but also many of the prompts have
at least one image getting flagged. This means that the high rates of NSFW
filtering are not due to a few prompts that are producing NSFW content, but
rather a more widespread issue with prompting generative models to produce
texture images. Second, we find that many of the words that lead to high NSFW
flagging rates were, surprisingly, <span class="ltx_text ltx_font_bold" id="Sx3.SSx2.SSSx1.p5.1.1">the texture classes themselves</span>. In particular, nearly 100% of prompts containing the word “paisley” produce at least one image that is flagged as NSFW.</p>
</div>
<div class="ltx_para" id="Sx3.SSx2.SSSx1.p6">
<p class="ltx_p" id="Sx3.SSx2.SSSx1.p6.1">Though we do not observe any instances of true NSFW content in any of the
images we generate, we filter them out of the dataset for the remainder of the
analysis. We also do not release the images that were flagged as NSFW as part of
our dataset for ethical reasons since we cannot guarantee that all images do not contain explicit content.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="Sx3.SSx3">
<h3 class="ltx_title ltx_title_subsection">Refinement</h3>
<div class="ltx_para" id="Sx3.SSx3.p1">
<p class="ltx_p" id="Sx3.SSx3.p1.1">At this point, there are 483,840 images that have been generated from our prompts via a Stable Diffusion pipeline. Besides just filtering for safety content that is intrinsic to the generative model, we also want to perform additional refinement on our dataset to filter such that we select the best images and ensure that the final dataset we produce is as high quality as possible. Unlike other refinement processes such as prompt engineering, we are refining our dataset rather than the prompt itself; at this point, we have generated all our data and are picking which of the generated images we are including in our final dataset.</p>
</div>
<div class="ltx_para" id="Sx3.SSx3.p2">
<p class="ltx_p" id="Sx3.SSx3.p2.1">To perform our refinement, we first calculate the CLIP score of each image with respect to its prompt.
Using these CLIP scores, we then filter out any images that fall below a certain threshold. In practice, in order to keep our dataset balanced, for each of the 56 texture classes, we remove images that fall below the 25th percentile of the CLIP scores of that texture class. We chose the 25th percentile because we observed a separation between the CLIP scores above and below the cutoff at this point. This also allows us to keep the majority of our data, while also ensuring that the images we keep are of higher quality. This parameter can be easily chosen to be more or less strict depending on the desired balance between data quality and quantity. After refinement, we are left with 362,880 images in the Prompted Textures Dataset, spread evenly across the 56 texture classes.</p>
</div>
<section class="ltx_subsubsection" id="Sx3.SSx3.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Prompt Quality Trends.</h4>
<div class="ltx_para" id="Sx3.SSx3.SSSx1.p1">
<p class="ltx_p" id="Sx3.SSx3.SSSx1.p1.1">Given the volume of prompts we generate, we naturally want to investigate if there are trends in prompts producing images that are not filtered out by our refinement process. In other words, <span class="ltx_text ltx_font_italic" id="Sx3.SSx3.SSSx1.p1.1.1">How do different descriptors affect the quality of the images produced?</span></p>
</div>
<div class="ltx_para" id="Sx3.SSx3.SSSx1.p2">
<p class="ltx_p" id="Sx3.SSx3.SSSx1.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx3.T2" title="Table 2 ‣ Prompt Quality Trends. ‣ Refinement ‣ The Prompted Textures Dataset (PTD) ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_tag">2</span></a> shows the top and bottom 5 mean CLIP scores across all descriptor word pairs.
Among the prompt pairs that do tend toward the top or bottom, we see some word pairing clusters. The texture “woven” appears to lead to higher quality images when paired with basic colors such as red, green, and blue. In contrast, more subtle textures such as “gauzy” and “veined” seem to result in lower CLIP scores when paired with descriptors that are designed to make the image more subtle, such as “muted” and “earthy”. From this, we find that some word pairs are more compatible than others and that this can influence resulting image quality.</p>
</div>
<figure class="ltx_table" id="Sx3.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="Sx3.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx3.T2.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="Sx3.T2.1.2.1.1">Word pair</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="Sx3.T2.1.2.1.2">Mean</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="Sx3.T2.1.2.1.3">Median</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="Sx3.T2.1.2.1.4"># Samples</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx3.T2.1.3.2.1">woven blue</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx3.T2.1.3.2.2">29.52</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx3.T2.1.3.2.3">29.84</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx3.T2.1.3.2.4">1080</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.4.3">
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.4.3.1">woven red</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.4.3.2">29.49</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.4.3.3">29.86</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.4.3.4">1080</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.5.4">
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.5.4.1">marbled photo-realistic</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.5.4.2">29.48</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.5.4.3">29.71</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.5.4.4">2160</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.6.5">
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.6.5.1">woven green</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.6.5.2">29.47</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.6.5.3">29.91</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.6.5.4">1080</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.1">
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.1.1">woven <math alttext="\emptyset" class="ltx_Math" display="inline" id="Sx3.T2.1.1.1.m1.1"><semantics id="Sx3.T2.1.1.1.m1.1a"><mi id="Sx3.T2.1.1.1.m1.1.1" mathvariant="normal" xref="Sx3.T2.1.1.1.m1.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="Sx3.T2.1.1.1.m1.1b"><emptyset id="Sx3.T2.1.1.1.m1.1.1.cmml" xref="Sx3.T2.1.1.1.m1.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T2.1.1.1.m1.1c">\emptyset</annotation><annotation encoding="application/x-llamapun" id="Sx3.T2.1.1.1.m1.1d">∅</annotation></semantics></math>(<span class="ltx_text ltx_font_italic" id="Sx3.T2.1.1.1.1">color enhancer</span>)</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.1.2">29.37</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.1.3">29.5</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.1.4">960</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.7.6">
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.7.6.1">…</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.7.6.2">…</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.7.6.3">…</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.7.6.4">…</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.8.7">
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.8.7.1">frilly neutral-colored</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.8.7.2">23.74</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.8.7.3">23.88</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.8.7.4">1080</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.9.8">
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.9.8.1">veined earthy</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.9.8.2">23.68</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.9.8.3">24.05</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.9.8.4">960</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.10.9">
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.10.9.1">gauzy earthy</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.10.9.2">23.51</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.10.9.3">23.86</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.10.9.4">960</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.11.10">
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.11.10.1">gauzy muted</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.11.10.2">23.28</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.11.10.3">23.62</td>
<td class="ltx_td ltx_align_left" id="Sx3.T2.1.11.10.4">960</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.12.11">
<td class="ltx_td ltx_align_left ltx_border_bb" id="Sx3.T2.1.12.11.1">veined muted</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Sx3.T2.1.12.11.2">23.2</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Sx3.T2.1.12.11.3">23.35</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Sx3.T2.1.12.11.4">960</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Top and bottom 5 CLIP scores across descriptor word pairs.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="Sx4">
<h2 class="ltx_title ltx_title_section">Quality evaluation</h2>
<div class="ltx_para" id="Sx4.p1">
<p class="ltx_p" id="Sx4.p1.1">Now that we have our final Prompted Textures Dataset (PTD), totaling 362,880 images, we evaluate the quality, diversity, and representativeness of the dataset in two parts: (1) using standard metrics such as Inception Scores and FID scores, and (2) through a human evaluation study.</p>
</div>
<section class="ltx_subsection" id="Sx4.SSx1">
<h3 class="ltx_title ltx_title_subsection">Standard metrics</h3>
<div class="ltx_para" id="Sx4.SSx1.p1">
<p class="ltx_p" id="Sx4.SSx1.p1.1">Historically, works have evaluated generated image datasets using Inception and FID scores. Inception scores measure the quality and diversity of the images in a dataset, while FID scores measure the dissimilarity of the generated images to a set of real images. We use these metrics to evaluate the quality of our dataset. However, the underlying assumption in these metrics is that the optimal generated image contains objects, since both scores build their metric on output distributions of the images when run through an object classification model. Given that our dataset consists of textures, we expect that these metrics will not be particularly useful at evaluating the quality of our dataset.</p>
</div>
<div class="ltx_para" id="Sx4.SSx1.p2">
<p class="ltx_p" id="Sx4.SSx1.p2.1">Given the wide variety of interesting challenges we have uncovered when working with texture images, we are naturally curious to see how standard metrics respond to texture images. For this, and to provide a complete quality evaluation of our dataset, we measure both the Inception scores and FID scores of our dataset. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx4.F5" title="Figure 5 ‣ Standard metrics ‣ Quality evaluation ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_tag">5</span></a>, we report both the Inception score (higher is better) and FID score (lower is better), separated by texture
class, on both the initial dataset generated from Stable Diffusion, and our dataset after we perform our refinement process. For comparison to real data, we measure the FID score with respect to the DTD and also report the Inception Scores of the DTD.</p>
</div>
<figure class="ltx_figure" id="Sx4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="249" id="Sx4.F5.g1" src="extracted/5854019/figures/quality/inception_fid_postclip_score_perclass_line_postclip25.png" width="663"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Inception and FID Scores of each texture class. Classes are sorted by Inception Score.</figcaption>
</figure>
<div class="ltx_para" id="Sx4.SSx1.p3">
<p class="ltx_p" id="Sx4.SSx1.p3.1">From this, the Inception scores are higher than we would anticipate, and are much higher than the Inception scores on real data (DTD).
Given that our Inception scores reach up to 9.02, despite not being directly related to ImageNet object classes, demonstrates that perhaps object classification models are able to classify based on texture alone. This theory is also supported by prior work on texture bias <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib8" title="">2019</a>)</cite> and texture learning <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib10" title="">2024</a>)</cite>. While this could also imply that the images being generated actually do contain objects, we see later in our human evaluation that this is not the case.</p>
</div>
<div class="ltx_para" id="Sx4.SSx1.p4">
<p class="ltx_p" id="Sx4.SSx1.p4.1">In both Inception and FID scores we see that the scores get worse
(Inception gets lower and FID gets higher) when evaluating on our data after our
refinement step. Given that our refinement step filters out images with lower
CLIP scores, this shows a dichotomy between CLIP scores and FID/Inception scores. This would suggest one of two things, either (1) our refinement
step does not actually help improve the quality, diversity, and realism of our
dataset, suggesting that CLIP is not well suited for use in texture data or (2) Inception scores and FID scores are an imperfect indicator of
quality in this domain, a plausible outcome given the hypothesized texture bias
<cite class="ltx_cite ltx_citemacro_citep">(Geirhos et al. <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#bib.bib8" title="">2019</a>)</cite> of the underlying models. While this may
initially appear to be a negative result, this illustrates an unexpected
disagreement between the two classes of metrics, and opens a new avenue for
investigating the suitability of these metrics for texture data, which we explore
in the next section.</p>
</div>
<div class="ltx_para" id="Sx4.SSx1.p5">
<p class="ltx_p" id="Sx4.SSx1.p5.1">Given the potential weaknesses of automated evaluation metrics, obtaining valid
metrics for image quality necessitates an approach that avoids the
pitfalls of texture-biased models. Towards this, we next conduct a
human-evaluation of both our generated samples and–indirectly–the suitability
of automated metrics for texture sample evaluation.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx4.SSx2">
<h3 class="ltx_title ltx_title_subsection">Human Evaluation</h3>
<div class="ltx_para" id="Sx4.SSx2.p1">
<p class="ltx_p" id="Sx4.SSx2.p1.1">For our human evaluation, we recruited 9 participants to evaluate the images. Each participant was shown 100 images in random order and asked two questions for each image: (1) How would you rate the overall quality of the image? and (2) How well does the image represent the provided descriptor? Participants were asked to supply a rating on a scale of 1 to 5, with 1 being the worst and 5 being the best, for each of these questions for every image.</p>
</div>
<div class="ltx_para" id="Sx4.SSx2.p2">
<p class="ltx_p" id="Sx4.SSx2.p2.1">The images for the image sets provided to the participants were selected randomly from the dataset, but we ensured there were no duplicate images between or within the sets, meaning that we evaluated 900 unique images from our dataset. These images were selected before the refinement stage in our pipeline, such that some of the evaluated images were removed as part of our refinement process. This was done to compare the human evaluation scores before and after refinement to see if our refinement process does indeed help to improve the overall quality of our dataset.</p>
</div>
<div class="ltx_para" id="Sx4.SSx2.p3">
<p class="ltx_p" id="Sx4.SSx2.p3.1">The results of the human evaluation are shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx4.F6" title="Figure 6 ‣ Human Evaluation ‣ Quality evaluation ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_tag">6</span></a>. This histogram shows the distribution of scores given by the participants, normalized by the number of images. Here, we show both the quality and representative scores, and for each, we show the scores before and after the refinement process.</p>
</div>
<div class="ltx_para" id="Sx4.SSx2.p4">
<p class="ltx_p" id="Sx4.SSx2.p4.1">From this, we find that our pipeline does help to produce good images. Comparing the scores on the images before and after the CLIP refinement, we can see that low quality and representative scores make up a larger majority of the unrefined dataset compared to the post-refined dataset and the post-refined dataset has a larger majority of images that were scored higher in our human eval. Additionally, the average quality score increases from 3.87 to 3.94, and the average representative score increases from 3.56 to 3.66. This indicates that the refinement process helps to improve the quality of the images in our dataset. Finally, the majority of the images receive scores of 3 or higher, indicating that the images are generally of good quality.</p>
</div>
<figure class="ltx_figure" id="Sx4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="Sx4.F6.g1" src="extracted/5854019/figures/quality/human_eval_hist25_norm_nostack.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Human evaluation scores histogram.</figcaption>
</figure>
<div class="ltx_para" id="Sx4.SSx2.p5">
<p class="ltx_p" id="Sx4.SSx2.p5.1">In addition to the raw scores, the participants also had the option of commenting on any trends they may have observed when evaluating the images. Some of the comments are: the descriptors containing the word “muted” often had “destroyed” images in terms of quality (P1), describing colors often alters the background (P2), symmetric images are sometimes not symmetric (P2), colors are very well represented in the images (P3), and descriptions with fewer words looked more realistic (P4).</p>
</div>
<div class="ltx_para" id="Sx4.SSx2.p6">
<p class="ltx_p" id="Sx4.SSx2.p6.1">These comments also agree with our results on assessing prompt quality. In particular, when looking at prompt pairs that generated images with higher or lower mean CLIP scores, we find that “muted” is one example of a descriptor whose images tend to be toward the bottom of the mean CLIP scores. For example, both <span class="ltx_text ltx_font_italic" id="Sx4.SSx2.p6.1.1">veined muted</span> and <span class="ltx_text ltx_font_italic" id="Sx4.SSx2.p6.1.2">gauzy muted</span> had mean CLIP scores of 23.20 and 23.28, respectively. Despite all of this, the provided metrics do support the fact that our dataset is both high quality and diverse. However, for a deeper evaluation of our pipeline we also further analyze our user study data in order to understand where future metrics on texture data can be improved.</p>
</div>
<section class="ltx_subsubsection" id="Sx4.SSx2.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Suitability of CLIP Scores.</h4>
<div class="ltx_para" id="Sx4.SSx2.SSSx1.p1">
<p class="ltx_p" id="Sx4.SSx2.SSSx1.p1.1">Given the previously discussed disagreement between FID/Inception and CLIP scores, we want to ensure that the CLIP scores we are using to filter our images are still a suitable metric for determining image representativeness, even on texture images. To investigate this, we compare the CLIP scores against our human evaluation scores of the same images. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10297v1#Sx4.F7" title="Figure 7 ‣ Suitability of CLIP Scores. ‣ Human Evaluation ‣ Quality evaluation ‣ On Synthetic Texture Datasets: Challenges, Creation, and Curation"><span class="ltx_text ltx_ref_tag">7</span></a> we show the average representative score (provided in the human evaluation) across various CLIP score quantiles.</p>
</div>
<figure class="ltx_figure" id="Sx4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="Sx4.F7.g1" src="extracted/5854019/figures/quality/clip_score_vs_rep_score.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Average human representative score for all images at or below a given CLIP score quantile cutoff.</figcaption>
</figure>
<div class="ltx_para" id="Sx4.SSx2.SSSx1.p2">
<p class="ltx_p" id="Sx4.SSx2.SSSx1.p2.1">From these results, we can see that as we increase our CLIP score quantile cutoff, and thus filter out more images, the average representative score provided by our human evaluation increases. This means that CLIP scores are indeed a good metric to use for image filtering, even on texture images, as they roughly track human scores.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="Sx5">
<h2 class="ltx_title ltx_title_section">Conclusions</h2>
<div class="ltx_para" id="Sx5.p1">
<p class="ltx_p" id="Sx5.p1.1">In this work we provide an extensible methodology for generating high-quality texture images by leveraging generative AI. Following this methodology, we create a new texture image dataset, the Prompted Textures Dataset (PTD). In creating and evaluating PTD, we find (a) that our dataset is diverse, representative, and high-quality and (b) uncover a wide variety of previously undiscovered challenges when working with texture data. We find, broadly speaking, that existing metrics and safety filters are not well-suited for texture images. We make our data and code publicly available, and encourage future work on texture based tasks to use our pipeline or dataset as a resource to continue exploring texture bias and texture-based tasks.</p>
</div>
</section>
<section class="ltx_section" id="Sx6">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx6.p1">
<p class="ltx_p" id="Sx6.p1.1">This material is based upon work supported in part by the National Science Foundation under Grant No. CNS-2343611 and in part by the U.S. Army Research Office under Grant W911NF2110317. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p>
</div>
<div class="ltx_para" id="Sx6.p2">
<p class="ltx_p" id="Sx6.p2.1">The authors would also like to thank all the participants that made our human evaluation possible, as well as Eric Pauley for his feedback on early drafts of the paper and experiment design.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barnett (2023)</span>
<span class="ltx_bibblock">
Barnett, A. 2023.

</span>
<span class="ltx_bibblock">400 Words to Describe Texture.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bynagari (2019)</span>
<span class="ltx_bibblock">
Bynagari, N. B. 2019.

</span>
<span class="ltx_bibblock">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Asian Journal of Applied Science and Engineering</em>, 8(1): 25–34.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cimpoi et al. (2014)</span>
<span class="ltx_bibblock">
Cimpoi, M.; Maji, S.; Kokkinos, I.; Mohamed, S.; and Vedaldi, A. 2014.

</span>
<span class="ltx_bibblock">Describing Textures in the Wild.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">2014 IEEE Conference on Computer Vision and Pattern Recognition</em>, 3606–3613. Columbus, OH, USA: IEEE.

</span>
<span class="ltx_bibblock">ISBN 978-1-4799-5118-5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Efros and Freeman (2001)</span>
<span class="ltx_bibblock">
Efros, A. A.; and Freeman, W. T. 2001.

</span>
<span class="ltx_bibblock">Image quilting for texture synthesis and transfer.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</em>, SIGGRAPH ’01, 341–346. New York, NY, USA: Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 978-1-58113-374-5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fearon (2014)</span>
<span class="ltx_bibblock">
Fearon, D. 2014.

</span>
<span class="ltx_bibblock">263/365.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gatys, Ecker, and Bethge (2015)</span>
<span class="ltx_bibblock">
Gatys, L. A.; Ecker, A. S.; and Bethge, M. 2015.

</span>
<span class="ltx_bibblock">Texture Synthesis Using Convolutional Neural Networks.

</span>
<span class="ltx_bibblock">ArXiv:1505.07376 [cs, q-bio].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gatys, Ecker, and Bethge (2016)</span>
<span class="ltx_bibblock">
Gatys, L. A.; Ecker, A. S.; and Bethge, M. 2016.

</span>
<span class="ltx_bibblock">Image Style Transfer Using Convolutional Neural Networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2414–2423. Las Vegas, NV, USA: IEEE.

</span>
<span class="ltx_bibblock">ISBN 978-1-4673-8851-1.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geirhos et al. (2019)</span>
<span class="ltx_bibblock">
Geirhos, R.; Rubisch, P.; Michaelis, C.; Bethge, M.; Wichmann, F. A.; and Brendel, W. 2019.

</span>
<span class="ltx_bibblock">ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness.

</span>
<span class="ltx_bibblock">ArXiv:1811.12231 [cs, q-bio, stat].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hessel et al. (2022)</span>
<span class="ltx_bibblock">
Hessel, J.; Holtzman, A.; Forbes, M.; Bras, R. L.; and Choi, Y. 2022.

</span>
<span class="ltx_bibblock">CLIPScore: A Reference-free Evaluation Metric for Image Captioning.

</span>
<span class="ltx_bibblock">ArXiv:2104.08718 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoak and McDaniel (2024)</span>
<span class="ltx_bibblock">
Hoak, B.; and McDaniel, P. 2024.

</span>
<span class="ltx_bibblock">Explorations in Texture Learning.

</span>
<span class="ltx_bibblock">ArXiv:2403.09543 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Irvin et al. (2019)</span>
<span class="ltx_bibblock">
Irvin, J.; Rajpurkar, P.; Ko, M.; Yu, Y.; Ciurea-Ilcus, S.; Chute, C.; Marklund, H.; Haghgoo, B.; Ball, R.; Shpanskaya, K.; Seekins, J.; Mong, D. A.; Halabi, S. S.; Sandberg, J. K.; Jones, R.; Larson, D. B.; Langlotz, C. P.; Patel, B. N.; Lungren, M. P.; and Ng, A. Y. 2019.

</span>
<span class="ltx_bibblock">CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison.

</span>
<span class="ltx_bibblock">ArXiv:1901.07031 [cs, eess].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ocvirk et al. (2001)</span>
<span class="ltx_bibblock">
Ocvirk, O.; Stinson, R.; Wigg, P.; Bone, R.; and Cayton, D. 2001.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Art fundamentals: Theory and practice</em>.

</span>
<span class="ltx_bibblock">Art fundamentals: Theory and practice. McGraw-Hill.

</span>
<span class="ltx_bibblock">ISBN 978-0-07-248351-2.

</span>
<span class="ltx_bibblock">Tex.lccn: 2001034257.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Portilla and Simoncelli (2000)</span>
<span class="ltx_bibblock">
Portilla, J.; and Simoncelli, E. P. 2000.

</span>
<span class="ltx_bibblock">A Parametric Texture Model Based on Joint Statistics of Complex Wavelet Coefﬁcients.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">International Journal of Computer Vision</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rando et al. (2022)</span>
<span class="ltx_bibblock">
Rando, J.; Paleka, D.; Lindner, D.; Heim, L.; and Tramèr, F. 2022.

</span>
<span class="ltx_bibblock">Red-Teaming the Stable Diffusion Safety Filter.

</span>
<span class="ltx_bibblock">ArXiv:2210.04610 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al. (2022)</span>
<span class="ltx_bibblock">
Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B. 2022.

</span>
<span class="ltx_bibblock">High-Resolution Image Synthesis with Latent Diffusion Models.

</span>
<span class="ltx_bibblock">ArXiv:2112.10752 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Russakovsky et al. (2015)</span>
<span class="ltx_bibblock">
Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.; Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.; Berg, A. C.; and Fei-Fei, L. 2015.

</span>
<span class="ltx_bibblock">ImageNet Large Scale Visual Recognition Challenge.

</span>
<span class="ltx_bibblock">ArXiv:1409.0575 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salimans et al. (2016)</span>
<span class="ltx_bibblock">
Salimans, T.; Goodfellow, I.; Zaremba, W.; Cheung, V.; Radford, A.; and Chen, X. 2016.

</span>
<span class="ltx_bibblock">Improved Techniques for Training GANs.

</span>
<span class="ltx_bibblock">ArXiv:1606.03498 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ustyuzhaninov et al. (2016)</span>
<span class="ltx_bibblock">
Ustyuzhaninov, I.; Brendel, W.; Gatys, L. A.; and Bethge, M. 2016.

</span>
<span class="ltx_bibblock">Texture Synthesis Using Shallow Convolutional Networks with Random Filters.

</span>
<span class="ltx_bibblock">ArXiv:1606.00021 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2020)</span>
<span class="ltx_bibblock">
Wolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.; Moi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davison, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y.; Plu, J.; Xu, C.; Scao, T. L.; Gugger, S.; Drame, M.; Lhoest, Q.; and Rush, A. M. 2020.

</span>
<span class="ltx_bibblock">HuggingFace’s Transformers: State-of-the-art Natural Language Processing.

</span>
<span class="ltx_bibblock">ArXiv:1910.03771 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">You et al. (2016)</span>
<span class="ltx_bibblock">
You, Q.; Luo, J.; Jin, H.; and Yang, J. 2016.

</span>
<span class="ltx_bibblock">Building a Large Scale Dataset for Image Emotion Recognition: The Fine Print and The Benchmark.

</span>
<span class="ltx_bibblock">ArXiv:1605.02677 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Zhang, Q.; Zhang, C.; Li, C.; Song, J.; and Gao, L. 2022.

</span>
<span class="ltx_bibblock">Practical No-box Adversarial Attacks with Training-free Hybrid Image Transformation.

</span>
<span class="ltx_bibblock">ArXiv:2203.04607 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2018)</span>
<span class="ltx_bibblock">
Zhou, B.; Lapedriza, A.; Khosla, A.; Oliva, A.; and Torralba, A. 2018.

</span>
<span class="ltx_bibblock">Places: A 10 Million Image Database for Scene Recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 40(6): 1452–1464.

</span>
<span class="ltx_bibblock">Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 16 14:01:42 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
