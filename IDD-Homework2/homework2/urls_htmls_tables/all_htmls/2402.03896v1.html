<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.03896] Convincing Rationales for Visual Question Answering Reasoning</title><meta property="og:description" content="Visual Question Answering (VQA) is a challenging task of predicting the answer to a question about the content of an image.
It requires deep understanding of both the textual question and visual image.
Prior works dire…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Convincing Rationales for Visual Question Answering Reasoning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Convincing Rationales for Visual Question Answering Reasoning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.03896">

<!--Generated on Tue Mar  5 17:49:06 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Convincing Rationales for Visual Question Answering Reasoning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kun Li, George Vosselman, Michael Ying Yang
<br class="ltx_break">University of Twente
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Visual Question Answering (VQA) is a challenging task of predicting the answer to a question about the content of an image.
It requires deep understanding of both the textual question and visual image.
Prior works directly evaluate the answering models by simply calculating the accuracy of the predicted answers.
However, the inner reasoning behind the prediction is disregarded in such a “black box” system, and we do not even know if one can trust the predictions.
In some cases, the models still get the correct answers even when they focus on irrelevant visual regions or textual tokens, which makes the models unreliable and illogical.
To generate both visual and textual rationales next to the predicted answer to the given image/question pair,
we propose <span id="id1.id1.1" class="ltx_text ltx_font_bold">C</span>onvincing <span id="id1.id1.2" class="ltx_text ltx_font_bold">R</span>ationales for <span id="id1.id1.3" class="ltx_text ltx_font_bold">VQA</span>, <span id="id1.id1.4" class="ltx_text ltx_font_bold">CRVQA</span>.
Considering the extra annotations brought by the new outputs, CRVQA is trained and evaluated by samples converted from some existing VQA datasets and their visual labels.
The extensive experiments demonstrate that the visual and textual rationales support the prediction of the answers, and further improve the accuracy.
Furthermore, CRVQA achieves competitive performance on generic VQA datatsets in the zero-shot evaluation setting.
The dataset and source code will be released under <a target="_blank" href="https://github.com/lik1996/CRVQA2024" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/lik1996/CRVQA2024</a>.</p>
</div>
<section id="S1" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering (VQA) takes as input an image and a natural language query, and predicts the corresponding answer based on the understanding of the vision-language content. Typically, answer accuracy is employed in most VQA systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite> to measure the correctness of the predictions. As such, the users cannot know how reliable the answers are. In contrast, VQA with explanations tries to tie the predicted answer to the given image/question pair by providing rationales in different formats (interesting image regions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> or textual explanations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>). This explanatory mechanism enables VQA systems to reason for complex multi-modal scene understanding, which is comparable to human comprehension and decision process.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/illustration1.png" id="S1.F1.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="149" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F1.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/illustration8.png" id="S1.F1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="150" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.4.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.5.2" class="ltx_text" style="font-size:90%;">Illustration the difference of our CRVQA and the generic VQA. CRVQA answers the question about a related image and further predicts the textual and visual rationales to support the answering. The green bounding boxes show the involved objects for the given image/question pair. Both the visual and linguistic outputs provide explanations for VQA reasoning. <span id="S1.F1.5.2.1" class="ltx_text ltx_font_italic">vtS</span> represents the new metric for textual rationale evaluation.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Interpretation is considered as one of the essential properties of the recent artificial intelligent systems, and much progress has been made to achieve such an “explainable AI” purpose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite>.
However, there exists a large gap for incorporating human-friendly explanations into interactive vision-language tasks such as VQA.
The prevailing solutions to address this challenge are single-track methods: either from visual hints (“V-Exp”) or textual explanations (“T-Exp”).
Specifically, V-Exp methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>, <a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> predict the answer to the given image/question pair, and further extract the visually interesting regions by analyzing the heat maps or attention maps in the inner neural network layers.
Since the maps are representing the most salient regions when the answering system makes decisions, V-Exp methods aim to provide visual evidences by such thresholded image regions.
However, the provided information is usually coarse and inaccurate and the interesting image regions (region-level) are not human-friendly for understanding the decision-making process.
T-Exp methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>, <a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>, <a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite> try to describe the visual content that helps answer the question via a short natural language sentence. Take the upper part in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> as an example, with the predicted textual explanation “one zebra gazes … opposite direction … the rest”, we can easily infer the answer “No” when given such an image/question pair. However, it still lacks of the rich comprehension on the vision-language input when we would like to know the specific target directly (i.e., the zebra in green bounding box).
Another drawback is brought by the inconvenience of the explanation when the scene gets more complex (imagine there are more zebras lying on the ground or some horses present). Based on the above concerns in mind, we observe that the current single-track explanatory VQA methods cannot satisfy the interactive vision-language answering system. In addition, the widely-used metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>, <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite> for text comparison only measure the similarity between the generated text and ground-truth by analyzing the present tokens in common for “fluency”, which cannot indicate whether they relate to the given input logically.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To generate both visual and textual rationales next to the predicted answer to the given image/question pair,
in this paper we propose <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">Convincing Rationale for Visual Question Answering</span> (<span id="S1.p3.1.2" class="ltx_text ltx_font_bold">CRVQA</span>).
To illustrate the difference between the generic VQA and our proposed CRVQA, two examples are shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. As shown in the bottom part, both two models can predict the answer “the boy on the right” correctly. In addition, CRVQA can provide more detailed information that explains the reasoning process of the decision-making: the textual rationale “the right boy swings the baseball bat” and <span id="S1.p3.1.3" class="ltx_text ltx_font_italic">the right boy</span>, <span id="S1.p3.1.4" class="ltx_text ltx_font_italic">ball</span>, and <span id="S1.p3.1.5" class="ltx_text ltx_font_italic">baseball bat</span> in green bounding boxes.
We build our model on top of a Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite>, and project the features to the latent space of a large language model for the textual prediction. We leverage a pre-trained open-vocabulary detector to accurately extract the bounding boxes for the visual rationales.
Furthermore, we introduce a new metric to evaluate the generated textual rationale from both vision and language perspectives.
The experimental results demonstrate the superior performance of the proposed model, and the generated textual and visual rationales are verified to improve both the model’s interpretation capability and the answer accuracy in the extensive ablation studies.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The <span id="S1.p4.1.1" class="ltx_text ltx_font_bold">main contributions</span> of this paper are summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">CRVQA</span>, a novel VQA model that enables the system to explain the answer prediction based on both generated textual rationales (textual descriptions) and visual rationales (object-level bounding boxes).</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">To measure the quality and reliability of generated textual rationales, we are the first of proposing a metric to evaluate the natural language predictions from the view of vision.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">The experiments show the effectiveness of our explainable VQA model.
In addition, CRVQA generalizes well to the generic VQA task in the zero-shot evaluation setting.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Visual Question Answering.</span> Visual question answering task is commonly considered as a classification problem with the pre-defined answer candidate pool <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>, which relies on a simple accuracy metric to measure its performance. As the multi-modal task takes as input both images and questions, VQA models address the problem through diverse approaches in different stages, including feature extraction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>, alignment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>, fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>, and prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>. With the introduction and fast pace developments of self-attention Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite>, both natural language processing and computer vision have witnessed advances in vision-language tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>, <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite>. Prior works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>, <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>, <a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite> investigate leveraging the vision-language pre-training from large-scale data to enhance the efficacy of VQA models and enable complex reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite>, pioneering the burgeoning trend in vision-language community <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>. However, these methods require access to internal parameters for fine-tuning and face the challenge in elucidating the reasoning process behind their predictions. In contrast, we explore convincing rationales to support answer predictions.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Explanations.</span> Deep neural networks are typically “black boxes” because of the end-to-end training procedure <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>. Models with explanations enable the users to see how the inner system works.
Recently, the explanatory visual question answering task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>, <a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> is proposed to provide the decision process that includes either the textual explanation or the visual focus. Specifically, the textual explanation reflects the corresponding reasoning descriptions why the system predicts the answer, while the visual focus is often represented in the form of the interesting region.
VQA-E <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> takes the joint representation from the input to generate the textual explanation based on an LSTM language model. Early models for visual explanations generate heat maps or attention maps that highlight the most crucial regions in an image (region-level), e.g., VQA-HAT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> and PJ-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite>. Recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> leverage the large multi-modal models and pre-trained object detector or segmentor to explain the decisions visually. However, such techniques demand substantial computational resources, making them challenging to train from scratch. A recent similar work to ours is VCIN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>]</cite> that proposes a variational causal inference to build the causal correlation between the answer and explanation for VQA, which generates the restricted textual explanation but requires additional relations among the objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>. In contrast, our method supports the open-ended textual rationale generation only under the supervision of answers and explanations.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Localization Models with Text.</span> Referring expression comprehension, also known as visual grounding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>, is a task of locating the targets conditioned on the provided natural language expression. Recent years have witnessed significant progress in developing models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite> for such a localization problem with the textual descriptions. These models can be roughly categorized as either two-stage or one-stage methods. Two-stage methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite> first generate region proposals and then select the best matching results, while one-stage methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> directly predict the results. However, these models rely on the specific texts to identify the target objects or categories, but the query text is not limited to the given expression. Normally, the target objects are not always mentioned in the questions for VQA. In our work, the gap is filled by the generation of the textual rationale that explains the reasoning chain with the specific involved targets.</p>
</div>
</section>
<section id="S3" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.2" class="ltx_p">We propose CRVQA, an explanatory visual question answering model that generates both visual and textual rationales to support the answer prediction. We start with the problem formulation. Given an image <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="{I}" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">{I}</annotation></semantics></math> and a natural language question <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="{Q}" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">{Q}</annotation></semantics></math>, our goal is to learn the predictions of (1) an answer <span id="S3.p1.2.1" class="ltx_text ltx_font_italic">A</span> to this question/image pair, (2) a textual rationale <span id="S3.p1.2.2" class="ltx_text ltx_font_italic">TR</span> for explaining the predicted answer in text, (3) a visual rationale <span id="S3.p1.2.3" class="ltx_text ltx_font_italic">VR</span> to show the involved objects. This can be formulated as follows:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.5" class="ltx_Math" alttext="\{A,TR,VR\}=P({I},{Q};\theta)," display="block"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1.2.2" xref="S3.E1.m1.5.5.1.1.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.5.5.1.1.2.2.3" xref="S3.E1.m1.5.5.1.1.2.3.cmml">{</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">A</mi><mo id="S3.E1.m1.5.5.1.1.2.2.4" xref="S3.E1.m1.5.5.1.1.2.3.cmml">,</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.cmml">​</mo><mi id="S3.E1.m1.5.5.1.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.3.cmml">R</mi></mrow><mo id="S3.E1.m1.5.5.1.1.2.2.5" xref="S3.E1.m1.5.5.1.1.2.3.cmml">,</mo><mrow id="S3.E1.m1.5.5.1.1.2.2.2" xref="S3.E1.m1.5.5.1.1.2.2.2.cmml"><mi id="S3.E1.m1.5.5.1.1.2.2.2.2" xref="S3.E1.m1.5.5.1.1.2.2.2.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.1.1.2.2.2.1" xref="S3.E1.m1.5.5.1.1.2.2.2.1.cmml">​</mo><mi id="S3.E1.m1.5.5.1.1.2.2.2.3" xref="S3.E1.m1.5.5.1.1.2.2.2.3.cmml">R</mi></mrow><mo stretchy="false" id="S3.E1.m1.5.5.1.1.2.2.6" xref="S3.E1.m1.5.5.1.1.2.3.cmml">}</mo></mrow><mo id="S3.E1.m1.5.5.1.1.3" xref="S3.E1.m1.5.5.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.5.5.1.1.4" xref="S3.E1.m1.5.5.1.1.4.cmml"><mi id="S3.E1.m1.5.5.1.1.4.2" xref="S3.E1.m1.5.5.1.1.4.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.1.1.4.1" xref="S3.E1.m1.5.5.1.1.4.1.cmml">​</mo><mrow id="S3.E1.m1.5.5.1.1.4.3.2" xref="S3.E1.m1.5.5.1.1.4.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.5.5.1.1.4.3.2.1" xref="S3.E1.m1.5.5.1.1.4.3.1.cmml">(</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">I</mi><mo id="S3.E1.m1.5.5.1.1.4.3.2.2" xref="S3.E1.m1.5.5.1.1.4.3.1.cmml">,</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">Q</mi><mo id="S3.E1.m1.5.5.1.1.4.3.2.3" xref="S3.E1.m1.5.5.1.1.4.3.1.cmml">;</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">θ</mi><mo stretchy="false" id="S3.E1.m1.5.5.1.1.4.3.2.4" xref="S3.E1.m1.5.5.1.1.4.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.5.5.1.2" xref="S3.E1.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1"><eq id="S3.E1.m1.5.5.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.3"></eq><set id="S3.E1.m1.5.5.1.1.2.3.cmml" xref="S3.E1.m1.5.5.1.1.2.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝐴</ci><apply id="S3.E1.m1.5.5.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1"><times id="S3.E1.m1.5.5.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1"></times><ci id="S3.E1.m1.5.5.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.2">𝑇</ci><ci id="S3.E1.m1.5.5.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.3">𝑅</ci></apply><apply id="S3.E1.m1.5.5.1.1.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2"><times id="S3.E1.m1.5.5.1.1.2.2.2.1.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2.1"></times><ci id="S3.E1.m1.5.5.1.1.2.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2.2">𝑉</ci><ci id="S3.E1.m1.5.5.1.1.2.2.2.3.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2.3">𝑅</ci></apply></set><apply id="S3.E1.m1.5.5.1.1.4.cmml" xref="S3.E1.m1.5.5.1.1.4"><times id="S3.E1.m1.5.5.1.1.4.1.cmml" xref="S3.E1.m1.5.5.1.1.4.1"></times><ci id="S3.E1.m1.5.5.1.1.4.2.cmml" xref="S3.E1.m1.5.5.1.1.4.2">𝑃</ci><vector id="S3.E1.m1.5.5.1.1.4.3.1.cmml" xref="S3.E1.m1.5.5.1.1.4.3.2"><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝐼</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝑄</ci><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">𝜃</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">\{A,TR,VR\}=P({I},{Q};\theta),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.p1.3" class="ltx_p">where <math id="S3.p1.3.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.p1.3.m1.1a"><mi id="S3.p1.3.m1.1.1" xref="S3.p1.3.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m1.1b"><ci id="S3.p1.3.m1.1.1.cmml" xref="S3.p1.3.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m1.1c">\theta</annotation></semantics></math> represents the parameters of the CRVQA model.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Given the above settings, there does not exist an available dataset designed for this task. To avoid heavy human annotation effort, we build <span id="S3.p2.1.1" class="ltx_text ltx_font_bold">VQA-R</span> (<span id="S3.p2.1.2" class="ltx_text ltx_font_bold">V</span>isual <span id="S3.p2.1.3" class="ltx_text ltx_font_bold">Q</span>uestion <span id="S3.p2.1.4" class="ltx_text ltx_font_bold">A</span>nswering <span id="S3.p2.1.5" class="ltx_text ltx_font_bold">R</span>easoning), an explanatory VQA dataset synthesized with a matching procedure. In this section, we first introduce the process of data synthesis. Then we elaborate the proposed CRVQA network in detail. Finally, a new metric on textual rationale evaluation is introduced.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2402.03896/assets/figures/dataset-synthesis.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="551" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">The overall process of VQA-R dataset synthesis. The text in the brackets and the aligned figure show an example.</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>VQA-R Dataset Synthesis</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We review the available explanatory VQA datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>, <a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> as the source of candidates for our task. We observe that either visual evidence or textual explanation is absent or incomplete in these datasets. Then VQA-E <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> is taken as the base version because it includes sufficient numbers of answer/text pairs and the related images are from the COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> that supports bounding box annotations for the object detection task. We synthesize the visual rationales by matching the available annotations from COCO with the question/answer/text triplets present in the validation set (88K) of VQA-E. To ensure the quality of the matched bounding boxes, we further manually edit the results. Fig. <a href="#S3.F2" title="Figure 2 ‣ 3 Method ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the overall process of VQA-R synthesis.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2402.03896/assets/figures/framework10.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="145" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.10.5.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.8.4" class="ltx_text" style="font-size:90%;">The framework of the proposed CRVQA model. It takes two inputs, including an image and a natural language question. <math id="S3.F3.5.1.m1.1" class="ltx_Math" alttext="\underline{I}" display="inline"><semantics id="S3.F3.5.1.m1.1b"><munder accentunder="true" id="S3.F3.5.1.m1.1.1" xref="S3.F3.5.1.m1.1.1.cmml"><mi id="S3.F3.5.1.m1.1.1.2" xref="S3.F3.5.1.m1.1.1.2.cmml">I</mi><mo id="S3.F3.5.1.m1.1.1.1" xref="S3.F3.5.1.m1.1.1.1.cmml">¯</mo></munder><annotation-xml encoding="MathML-Content" id="S3.F3.5.1.m1.1c"><apply id="S3.F3.5.1.m1.1.1.cmml" xref="S3.F3.5.1.m1.1.1"><ci id="S3.F3.5.1.m1.1.1.1.cmml" xref="S3.F3.5.1.m1.1.1.1">¯</ci><ci id="S3.F3.5.1.m1.1.1.2.cmml" xref="S3.F3.5.1.m1.1.1.2">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.5.1.m1.1d">\underline{I}</annotation></semantics></math> and <math id="S3.F3.6.2.m2.1" class="ltx_Math" alttext="\underline{Q}" display="inline"><semantics id="S3.F3.6.2.m2.1b"><munder accentunder="true" id="S3.F3.6.2.m2.1.1" xref="S3.F3.6.2.m2.1.1.cmml"><mi id="S3.F3.6.2.m2.1.1.2" xref="S3.F3.6.2.m2.1.1.2.cmml">Q</mi><mo id="S3.F3.6.2.m2.1.1.1" xref="S3.F3.6.2.m2.1.1.1.cmml">¯</mo></munder><annotation-xml encoding="MathML-Content" id="S3.F3.6.2.m2.1c"><apply id="S3.F3.6.2.m2.1.1.cmml" xref="S3.F3.6.2.m2.1.1"><ci id="S3.F3.6.2.m2.1.1.1.cmml" xref="S3.F3.6.2.m2.1.1.1">¯</ci><ci id="S3.F3.6.2.m2.1.1.2.cmml" xref="S3.F3.6.2.m2.1.1.2">𝑄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.6.2.m2.1d">\underline{Q}</annotation></semantics></math> denote the CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> representations for the image and the question, respectively. <math id="S3.F3.7.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.F3.7.3.m3.1b"><mi id="S3.F3.7.3.m3.1.1" xref="S3.F3.7.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.F3.7.3.m3.1c"><ci id="S3.F3.7.3.m3.1.1.cmml" xref="S3.F3.7.3.m3.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.7.3.m3.1d">M</annotation></semantics></math> and <math id="S3.F3.8.4.m4.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.F3.8.4.m4.1b"><mi id="S3.F3.8.4.m4.1.1" xref="S3.F3.8.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.F3.8.4.m4.1c"><ci id="S3.F3.8.4.m4.1.1.cmml" xref="S3.F3.8.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.8.4.m4.1d">N</annotation></semantics></math> represent the numbers of the attention layers. The dashed arrows indicate the flow exclusively during the inference stage. The model outputs the corresponding answer and its visual and textual rationale. Note that the parameters of CLIP model are frozen during training.</span></figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Specifically, we collect all the nouns present in the question/answer/text triplets from VQA-E <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>. These nouns always refer to the objects that are involved in the reasoning process of VQA. For instance, given a question “What color is the tie of that boy?”, the visual evidence points to the targets “tie” and “boy” shown in the image. However, analyzing all the nouns is not a practical approach as there are more than 10K categories included. We proceed to perform a frequency analysis on the nouns and select those that appear more than 20 times. With this step, we obtain 1247 different nouns that can be matched with the visual labels. Note that COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> only labels 90 category objects for object detection. Then we introduce a strategy by classifying the candidate nouns into larger groups that align with the original annotations. For instance, nouns such as <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">man, woman, farmer, people, boy, girl</span> can be grouped to the <span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_italic">Person</span> category in COCO.
Up to this step, we obtain the bounding box annotations for the image/question pairs.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">However, not all the bounding boxes in one category are valid for a given image/question pair. For instance in Fig. <a href="#S3.F2" title="Figure 2 ‣ 3 Method ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, only one sheep can serve as the visual rationale given the image that includes two sheep and the question about the “right sheep”. Similarly, the extracted objects cannot always be sufficient to represent the reasoning process. For instance, given a question “What is the man doing?”, an answer “surfing” and a text “a man on the wave”, we can only extract the bounding box for the valid noun “man” as the candidate while the visual rationale should be the labels for a man and a surfboard. To ensure the quality of visual rationales, we further manually edit the bounding boxes obtained above by removing irrelevant objects and adding involved objects for explanation. Finally, we synthesize VQA-R dataset that contains 33726 image/question pairs annotated by answers, textual rationales and visual rationales for explanatory VQA.
This dataset will be made publicly available.
More details are provided in the supplementary material.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>CRVQA Network</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The overall framework of the proposed CRVQA is shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.1 VQA-R Dataset Synthesis ‣ 3 Method ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. In the following section, we introduce its input representations, encoder-decoder, projection module, predictors, and loss functions.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Input Representations.</span> The CLIP (Constractive Language-Image Pre-training) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> model is pre-trained on large-scale image/caption pairs, demonstrating its power in vision-language tasks compared with visual encoders purely trained on in-domain data. Inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>, we employ a pre-trained CLIP model to extract the visual and textual features from the input image and question, respectively.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Encoder-Decoder.</span> For the VQA training part, we build the network on top of several Modular Co-Attention layers (MCA) proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite>. The MCA layer refers to a modular composition of self-attention units and guided-attention units. The self-attention unit extracts intra-modal features for either vision or language modality, while the guided-attention unit facilitates cross-modal interactions.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.4" class="ltx_p"><span id="S3.SS2.p4.4.1" class="ltx_text ltx_font_bold">Projection Module.</span> The projection module refers to the connection part between the pre-trained models and the large language model in recent vision-language works. A simple linear projection is widely used and has been demonstrated to be effective in various scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>. We propose a Transformer-based module to project the CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> features to the latent space of a large language model (GPT-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>, discussed below). We leverage the self-attention mechanism in Transformers, which enables the learning of global features between tokens even in the context of long sentences. In our scenario, this capability can enhance the projection when dealing with complex questions. As the input for the projection module, we utilize the CLIP visual and question representations, <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="\underline{I}" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><munder accentunder="true" id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">I</mi><mo id="S3.SS2.p4.1.m1.1.1.1" xref="S3.SS2.p4.1.m1.1.1.1.cmml">¯</mo></munder><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><ci id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1.1">¯</ci><ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\underline{I}</annotation></semantics></math> and <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="\underline{Q}" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><munder accentunder="true" id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><mi id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml">Q</mi><mo id="S3.SS2.p4.2.m2.1.1.1" xref="S3.SS2.p4.2.m2.1.1.1.cmml">¯</mo></munder><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><ci id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1.1">¯</ci><ci id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2">𝑄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">\underline{Q}</annotation></semantics></math> respectively, along with two additional learned constant inputs. The constant inputs help extract the robust and valuable CLIP embeddings via the Transformers. We employ eight self-attention layers for the dual vision-language branch. The self-attended features <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="I_{proj}" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><msub id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml"><mi id="S3.SS2.p4.3.m3.1.1.2" xref="S3.SS2.p4.3.m3.1.1.2.cmml">I</mi><mrow id="S3.SS2.p4.3.m3.1.1.3" xref="S3.SS2.p4.3.m3.1.1.3.cmml"><mi id="S3.SS2.p4.3.m3.1.1.3.2" xref="S3.SS2.p4.3.m3.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.3.m3.1.1.3.1" xref="S3.SS2.p4.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.3.m3.1.1.3.3" xref="S3.SS2.p4.3.m3.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.3.m3.1.1.3.1a" xref="S3.SS2.p4.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.3.m3.1.1.3.4" xref="S3.SS2.p4.3.m3.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.3.m3.1.1.3.1b" xref="S3.SS2.p4.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.3.m3.1.1.3.5" xref="S3.SS2.p4.3.m3.1.1.3.5.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2">𝐼</ci><apply id="S3.SS2.p4.3.m3.1.1.3.cmml" xref="S3.SS2.p4.3.m3.1.1.3"><times id="S3.SS2.p4.3.m3.1.1.3.1.cmml" xref="S3.SS2.p4.3.m3.1.1.3.1"></times><ci id="S3.SS2.p4.3.m3.1.1.3.2.cmml" xref="S3.SS2.p4.3.m3.1.1.3.2">𝑝</ci><ci id="S3.SS2.p4.3.m3.1.1.3.3.cmml" xref="S3.SS2.p4.3.m3.1.1.3.3">𝑟</ci><ci id="S3.SS2.p4.3.m3.1.1.3.4.cmml" xref="S3.SS2.p4.3.m3.1.1.3.4">𝑜</ci><ci id="S3.SS2.p4.3.m3.1.1.3.5.cmml" xref="S3.SS2.p4.3.m3.1.1.3.5">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">I_{proj}</annotation></semantics></math> and <math id="S3.SS2.p4.4.m4.1" class="ltx_Math" alttext="Q_{proj}" display="inline"><semantics id="S3.SS2.p4.4.m4.1a"><msub id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml"><mi id="S3.SS2.p4.4.m4.1.1.2" xref="S3.SS2.p4.4.m4.1.1.2.cmml">Q</mi><mrow id="S3.SS2.p4.4.m4.1.1.3" xref="S3.SS2.p4.4.m4.1.1.3.cmml"><mi id="S3.SS2.p4.4.m4.1.1.3.2" xref="S3.SS2.p4.4.m4.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.4.m4.1.1.3.1" xref="S3.SS2.p4.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.4.m4.1.1.3.3" xref="S3.SS2.p4.4.m4.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.4.m4.1.1.3.1a" xref="S3.SS2.p4.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.4.m4.1.1.3.4" xref="S3.SS2.p4.4.m4.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.4.m4.1.1.3.1b" xref="S3.SS2.p4.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.4.m4.1.1.3.5" xref="S3.SS2.p4.4.m4.1.1.3.5.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><apply id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m4.1.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p4.4.m4.1.1.2.cmml" xref="S3.SS2.p4.4.m4.1.1.2">𝑄</ci><apply id="S3.SS2.p4.4.m4.1.1.3.cmml" xref="S3.SS2.p4.4.m4.1.1.3"><times id="S3.SS2.p4.4.m4.1.1.3.1.cmml" xref="S3.SS2.p4.4.m4.1.1.3.1"></times><ci id="S3.SS2.p4.4.m4.1.1.3.2.cmml" xref="S3.SS2.p4.4.m4.1.1.3.2">𝑝</ci><ci id="S3.SS2.p4.4.m4.1.1.3.3.cmml" xref="S3.SS2.p4.4.m4.1.1.3.3">𝑟</ci><ci id="S3.SS2.p4.4.m4.1.1.3.4.cmml" xref="S3.SS2.p4.4.m4.1.1.3.4">𝑜</ci><ci id="S3.SS2.p4.4.m4.1.1.3.5.cmml" xref="S3.SS2.p4.4.m4.1.1.3.5">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">Q_{proj}</annotation></semantics></math> are calculated as follows:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.3" class="ltx_Math" alttext="I_{proj}=SA(Cat(\underline{I},C))," display="block"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><msub id="S3.E2.m1.3.3.1.1.3" xref="S3.E2.m1.3.3.1.1.3.cmml"><mi id="S3.E2.m1.3.3.1.1.3.2" xref="S3.E2.m1.3.3.1.1.3.2.cmml">I</mi><mrow id="S3.E2.m1.3.3.1.1.3.3" xref="S3.E2.m1.3.3.1.1.3.3.cmml"><mi id="S3.E2.m1.3.3.1.1.3.3.2" xref="S3.E2.m1.3.3.1.1.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.3.3.1" xref="S3.E2.m1.3.3.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.3.3.1.1.3.3.3" xref="S3.E2.m1.3.3.1.1.3.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.3.3.1a" xref="S3.E2.m1.3.3.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.3.3.1.1.3.3.4" xref="S3.E2.m1.3.3.1.1.3.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.3.3.1b" xref="S3.E2.m1.3.3.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.3.3.1.1.3.3.5" xref="S3.E2.m1.3.3.1.1.3.3.5.cmml">j</mi></mrow></msub><mo id="S3.E2.m1.3.3.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.3.3.1.1.1" xref="S3.E2.m1.3.3.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.3.3.1.1.1.4" xref="S3.E2.m1.3.3.1.1.1.4.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.1.2a" xref="S3.E2.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.1.1.1.1.1a" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.4" xref="S3.E2.m1.3.3.1.1.1.1.1.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.1.1.1.1.1b" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.5.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.1.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.1.1.5.2.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.1.cmml">(</mo><munder accentunder="true" id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">I</mi><mo id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">¯</mo></munder><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.5.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.1.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">C</mi><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.1.1.5.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.3.3.1.2" xref="S3.E2.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1"><eq id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2"></eq><apply id="S3.E2.m1.3.3.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.2">𝐼</ci><apply id="S3.E2.m1.3.3.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.3.3"><times id="S3.E2.m1.3.3.1.1.3.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1"></times><ci id="S3.E2.m1.3.3.1.1.3.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.3.2">𝑝</ci><ci id="S3.E2.m1.3.3.1.1.3.3.3.cmml" xref="S3.E2.m1.3.3.1.1.3.3.3">𝑟</ci><ci id="S3.E2.m1.3.3.1.1.3.3.4.cmml" xref="S3.E2.m1.3.3.1.1.3.3.4">𝑜</ci><ci id="S3.E2.m1.3.3.1.1.3.3.5.cmml" xref="S3.E2.m1.3.3.1.1.3.3.5">𝑗</ci></apply></apply><apply id="S3.E2.m1.3.3.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1"><times id="S3.E2.m1.3.3.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.2"></times><ci id="S3.E2.m1.3.3.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.3">𝑆</ci><ci id="S3.E2.m1.3.3.1.1.1.4.cmml" xref="S3.E2.m1.3.3.1.1.1.4">𝐴</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1"><times id="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1"></times><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2">𝐶</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3">𝑎</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.4">𝑡</ci><interval closure="open" id="S3.E2.m1.3.3.1.1.1.1.1.1.5.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.2"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><ci id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1">¯</ci><ci id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">𝐼</ci></apply><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝐶</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">I_{proj}=SA(Cat(\underline{I},C)),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.3" class="ltx_Math" alttext="Q_{proj}=SA(Cat(\underline{Q},C))," display="block"><semantics id="S3.E3.m1.3a"><mrow id="S3.E3.m1.3.3.1" xref="S3.E3.m1.3.3.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1" xref="S3.E3.m1.3.3.1.1.cmml"><msub id="S3.E3.m1.3.3.1.1.3" xref="S3.E3.m1.3.3.1.1.3.cmml"><mi id="S3.E3.m1.3.3.1.1.3.2" xref="S3.E3.m1.3.3.1.1.3.2.cmml">Q</mi><mrow id="S3.E3.m1.3.3.1.1.3.3" xref="S3.E3.m1.3.3.1.1.3.3.cmml"><mi id="S3.E3.m1.3.3.1.1.3.3.2" xref="S3.E3.m1.3.3.1.1.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.3.3.1" xref="S3.E3.m1.3.3.1.1.3.3.1.cmml">​</mo><mi id="S3.E3.m1.3.3.1.1.3.3.3" xref="S3.E3.m1.3.3.1.1.3.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.3.3.1a" xref="S3.E3.m1.3.3.1.1.3.3.1.cmml">​</mo><mi id="S3.E3.m1.3.3.1.1.3.3.4" xref="S3.E3.m1.3.3.1.1.3.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.3.3.1b" xref="S3.E3.m1.3.3.1.1.3.3.1.cmml">​</mo><mi id="S3.E3.m1.3.3.1.1.3.3.5" xref="S3.E3.m1.3.3.1.1.3.3.5.cmml">j</mi></mrow></msub><mo id="S3.E3.m1.3.3.1.1.2" xref="S3.E3.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.3.3.1.1.1" xref="S3.E3.m1.3.3.1.1.1.cmml"><mi id="S3.E3.m1.3.3.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.2.cmml">​</mo><mi id="S3.E3.m1.3.3.1.1.1.4" xref="S3.E3.m1.3.3.1.1.1.4.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.1.2a" xref="S3.E3.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.1.1.1.1.1a" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.4" xref="S3.E3.m1.3.3.1.1.1.1.1.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.1.1.1.1.1b" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.5.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.5.1.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.1.1.1.1.1.1.5.2.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.5.1.cmml">(</mo><munder accentunder="true" id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mi id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">Q</mi><mo id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml">¯</mo></munder><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.5.2.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.5.1.cmml">,</mo><mi id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml">C</mi><mo stretchy="false" id="S3.E3.m1.3.3.1.1.1.1.1.1.5.2.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.5.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E3.m1.3.3.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.3.3.1.2" xref="S3.E3.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.3b"><apply id="S3.E3.m1.3.3.1.1.cmml" xref="S3.E3.m1.3.3.1"><eq id="S3.E3.m1.3.3.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.2"></eq><apply id="S3.E3.m1.3.3.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.3.1.cmml" xref="S3.E3.m1.3.3.1.1.3">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.3.2.cmml" xref="S3.E3.m1.3.3.1.1.3.2">𝑄</ci><apply id="S3.E3.m1.3.3.1.1.3.3.cmml" xref="S3.E3.m1.3.3.1.1.3.3"><times id="S3.E3.m1.3.3.1.1.3.3.1.cmml" xref="S3.E3.m1.3.3.1.1.3.3.1"></times><ci id="S3.E3.m1.3.3.1.1.3.3.2.cmml" xref="S3.E3.m1.3.3.1.1.3.3.2">𝑝</ci><ci id="S3.E3.m1.3.3.1.1.3.3.3.cmml" xref="S3.E3.m1.3.3.1.1.3.3.3">𝑟</ci><ci id="S3.E3.m1.3.3.1.1.3.3.4.cmml" xref="S3.E3.m1.3.3.1.1.3.3.4">𝑜</ci><ci id="S3.E3.m1.3.3.1.1.3.3.5.cmml" xref="S3.E3.m1.3.3.1.1.3.3.5">𝑗</ci></apply></apply><apply id="S3.E3.m1.3.3.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1"><times id="S3.E3.m1.3.3.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.2"></times><ci id="S3.E3.m1.3.3.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.3">𝑆</ci><ci id="S3.E3.m1.3.3.1.1.1.4.cmml" xref="S3.E3.m1.3.3.1.1.1.4">𝐴</ci><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1"><times id="S3.E3.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1"></times><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2">𝐶</ci><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.3">𝑎</ci><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.4.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.4">𝑡</ci><interval closure="open" id="S3.E3.m1.3.3.1.1.1.1.1.1.5.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.5.2"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><ci id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1">¯</ci><ci id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2">𝑄</ci></apply><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">𝐶</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.3c">Q_{proj}=SA(Cat(\underline{Q},C)),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.5" class="ltx_Math" alttext="SA(q,K,V)=Softmax(\frac{qK}{\sqrt{d}})V," display="block"><semantics id="S3.E4.m1.5a"><mrow id="S3.E4.m1.5.5.1" xref="S3.E4.m1.5.5.1.1.cmml"><mrow id="S3.E4.m1.5.5.1.1" xref="S3.E4.m1.5.5.1.1.cmml"><mrow id="S3.E4.m1.5.5.1.1.2" xref="S3.E4.m1.5.5.1.1.2.cmml"><mi id="S3.E4.m1.5.5.1.1.2.2" xref="S3.E4.m1.5.5.1.1.2.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.2.1" xref="S3.E4.m1.5.5.1.1.2.1.cmml">​</mo><mi id="S3.E4.m1.5.5.1.1.2.3" xref="S3.E4.m1.5.5.1.1.2.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.2.1a" xref="S3.E4.m1.5.5.1.1.2.1.cmml">​</mo><mrow id="S3.E4.m1.5.5.1.1.2.4.2" xref="S3.E4.m1.5.5.1.1.2.4.1.cmml"><mo stretchy="false" id="S3.E4.m1.5.5.1.1.2.4.2.1" xref="S3.E4.m1.5.5.1.1.2.4.1.cmml">(</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">q</mi><mo id="S3.E4.m1.5.5.1.1.2.4.2.2" xref="S3.E4.m1.5.5.1.1.2.4.1.cmml">,</mo><mi id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml">K</mi><mo id="S3.E4.m1.5.5.1.1.2.4.2.3" xref="S3.E4.m1.5.5.1.1.2.4.1.cmml">,</mo><mi id="S3.E4.m1.3.3" xref="S3.E4.m1.3.3.cmml">V</mi><mo stretchy="false" id="S3.E4.m1.5.5.1.1.2.4.2.4" xref="S3.E4.m1.5.5.1.1.2.4.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.5.5.1.1.1" xref="S3.E4.m1.5.5.1.1.1.cmml">=</mo><mrow id="S3.E4.m1.5.5.1.1.3" xref="S3.E4.m1.5.5.1.1.3.cmml"><mi id="S3.E4.m1.5.5.1.1.3.2" xref="S3.E4.m1.5.5.1.1.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.3.1" xref="S3.E4.m1.5.5.1.1.3.1.cmml">​</mo><mi id="S3.E4.m1.5.5.1.1.3.3" xref="S3.E4.m1.5.5.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.3.1a" xref="S3.E4.m1.5.5.1.1.3.1.cmml">​</mo><mi id="S3.E4.m1.5.5.1.1.3.4" xref="S3.E4.m1.5.5.1.1.3.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.3.1b" xref="S3.E4.m1.5.5.1.1.3.1.cmml">​</mo><mi id="S3.E4.m1.5.5.1.1.3.5" xref="S3.E4.m1.5.5.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.3.1c" xref="S3.E4.m1.5.5.1.1.3.1.cmml">​</mo><mi id="S3.E4.m1.5.5.1.1.3.6" xref="S3.E4.m1.5.5.1.1.3.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.3.1d" xref="S3.E4.m1.5.5.1.1.3.1.cmml">​</mo><mi id="S3.E4.m1.5.5.1.1.3.7" xref="S3.E4.m1.5.5.1.1.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.3.1e" xref="S3.E4.m1.5.5.1.1.3.1.cmml">​</mo><mi id="S3.E4.m1.5.5.1.1.3.8" xref="S3.E4.m1.5.5.1.1.3.8.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.3.1f" xref="S3.E4.m1.5.5.1.1.3.1.cmml">​</mo><mrow id="S3.E4.m1.5.5.1.1.3.9.2" xref="S3.E4.m1.4.4.cmml"><mo stretchy="false" id="S3.E4.m1.5.5.1.1.3.9.2.1" xref="S3.E4.m1.4.4.cmml">(</mo><mfrac id="S3.E4.m1.4.4" xref="S3.E4.m1.4.4.cmml"><mrow id="S3.E4.m1.4.4.2" xref="S3.E4.m1.4.4.2.cmml"><mi id="S3.E4.m1.4.4.2.2" xref="S3.E4.m1.4.4.2.2.cmml">q</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.2.1" xref="S3.E4.m1.4.4.2.1.cmml">​</mo><mi id="S3.E4.m1.4.4.2.3" xref="S3.E4.m1.4.4.2.3.cmml">K</mi></mrow><msqrt id="S3.E4.m1.4.4.3" xref="S3.E4.m1.4.4.3.cmml"><mi id="S3.E4.m1.4.4.3.2" xref="S3.E4.m1.4.4.3.2.cmml">d</mi></msqrt></mfrac><mo stretchy="false" id="S3.E4.m1.5.5.1.1.3.9.2.2" xref="S3.E4.m1.4.4.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.3.1g" xref="S3.E4.m1.5.5.1.1.3.1.cmml">​</mo><mi id="S3.E4.m1.5.5.1.1.3.10" xref="S3.E4.m1.5.5.1.1.3.10.cmml">V</mi></mrow></mrow><mo id="S3.E4.m1.5.5.1.2" xref="S3.E4.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.5b"><apply id="S3.E4.m1.5.5.1.1.cmml" xref="S3.E4.m1.5.5.1"><eq id="S3.E4.m1.5.5.1.1.1.cmml" xref="S3.E4.m1.5.5.1.1.1"></eq><apply id="S3.E4.m1.5.5.1.1.2.cmml" xref="S3.E4.m1.5.5.1.1.2"><times id="S3.E4.m1.5.5.1.1.2.1.cmml" xref="S3.E4.m1.5.5.1.1.2.1"></times><ci id="S3.E4.m1.5.5.1.1.2.2.cmml" xref="S3.E4.m1.5.5.1.1.2.2">𝑆</ci><ci id="S3.E4.m1.5.5.1.1.2.3.cmml" xref="S3.E4.m1.5.5.1.1.2.3">𝐴</ci><vector id="S3.E4.m1.5.5.1.1.2.4.1.cmml" xref="S3.E4.m1.5.5.1.1.2.4.2"><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">𝑞</ci><ci id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2">𝐾</ci><ci id="S3.E4.m1.3.3.cmml" xref="S3.E4.m1.3.3">𝑉</ci></vector></apply><apply id="S3.E4.m1.5.5.1.1.3.cmml" xref="S3.E4.m1.5.5.1.1.3"><times id="S3.E4.m1.5.5.1.1.3.1.cmml" xref="S3.E4.m1.5.5.1.1.3.1"></times><ci id="S3.E4.m1.5.5.1.1.3.2.cmml" xref="S3.E4.m1.5.5.1.1.3.2">𝑆</ci><ci id="S3.E4.m1.5.5.1.1.3.3.cmml" xref="S3.E4.m1.5.5.1.1.3.3">𝑜</ci><ci id="S3.E4.m1.5.5.1.1.3.4.cmml" xref="S3.E4.m1.5.5.1.1.3.4">𝑓</ci><ci id="S3.E4.m1.5.5.1.1.3.5.cmml" xref="S3.E4.m1.5.5.1.1.3.5">𝑡</ci><ci id="S3.E4.m1.5.5.1.1.3.6.cmml" xref="S3.E4.m1.5.5.1.1.3.6">𝑚</ci><ci id="S3.E4.m1.5.5.1.1.3.7.cmml" xref="S3.E4.m1.5.5.1.1.3.7">𝑎</ci><ci id="S3.E4.m1.5.5.1.1.3.8.cmml" xref="S3.E4.m1.5.5.1.1.3.8">𝑥</ci><apply id="S3.E4.m1.4.4.cmml" xref="S3.E4.m1.5.5.1.1.3.9.2"><divide id="S3.E4.m1.4.4.1.cmml" xref="S3.E4.m1.5.5.1.1.3.9.2"></divide><apply id="S3.E4.m1.4.4.2.cmml" xref="S3.E4.m1.4.4.2"><times id="S3.E4.m1.4.4.2.1.cmml" xref="S3.E4.m1.4.4.2.1"></times><ci id="S3.E4.m1.4.4.2.2.cmml" xref="S3.E4.m1.4.4.2.2">𝑞</ci><ci id="S3.E4.m1.4.4.2.3.cmml" xref="S3.E4.m1.4.4.2.3">𝐾</ci></apply><apply id="S3.E4.m1.4.4.3.cmml" xref="S3.E4.m1.4.4.3"><root id="S3.E4.m1.4.4.3a.cmml" xref="S3.E4.m1.4.4.3"></root><ci id="S3.E4.m1.4.4.3.2.cmml" xref="S3.E4.m1.4.4.3.2">𝑑</ci></apply></apply><ci id="S3.E4.m1.5.5.1.1.3.10.cmml" xref="S3.E4.m1.5.5.1.1.3.10">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.5c">SA(q,K,V)=Softmax(\frac{qK}{\sqrt{d}})V,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p4.6" class="ltx_p">where <math id="S3.SS2.p4.5.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS2.p4.5.m1.1a"><mi id="S3.SS2.p4.5.m1.1.1" xref="S3.SS2.p4.5.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m1.1b"><ci id="S3.SS2.p4.5.m1.1.1.cmml" xref="S3.SS2.p4.5.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m1.1c">C</annotation></semantics></math> represents the learned constant input. We concatenate the projected features <math id="S3.SS2.p4.6.m2.1" class="ltx_Math" alttext="f_{proj}" display="inline"><semantics id="S3.SS2.p4.6.m2.1a"><msub id="S3.SS2.p4.6.m2.1.1" xref="S3.SS2.p4.6.m2.1.1.cmml"><mi id="S3.SS2.p4.6.m2.1.1.2" xref="S3.SS2.p4.6.m2.1.1.2.cmml">f</mi><mrow id="S3.SS2.p4.6.m2.1.1.3" xref="S3.SS2.p4.6.m2.1.1.3.cmml"><mi id="S3.SS2.p4.6.m2.1.1.3.2" xref="S3.SS2.p4.6.m2.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.6.m2.1.1.3.1" xref="S3.SS2.p4.6.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.6.m2.1.1.3.3" xref="S3.SS2.p4.6.m2.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.6.m2.1.1.3.1a" xref="S3.SS2.p4.6.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.6.m2.1.1.3.4" xref="S3.SS2.p4.6.m2.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.6.m2.1.1.3.1b" xref="S3.SS2.p4.6.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.6.m2.1.1.3.5" xref="S3.SS2.p4.6.m2.1.1.3.5.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m2.1b"><apply id="S3.SS2.p4.6.m2.1.1.cmml" xref="S3.SS2.p4.6.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.6.m2.1.1.1.cmml" xref="S3.SS2.p4.6.m2.1.1">subscript</csymbol><ci id="S3.SS2.p4.6.m2.1.1.2.cmml" xref="S3.SS2.p4.6.m2.1.1.2">𝑓</ci><apply id="S3.SS2.p4.6.m2.1.1.3.cmml" xref="S3.SS2.p4.6.m2.1.1.3"><times id="S3.SS2.p4.6.m2.1.1.3.1.cmml" xref="S3.SS2.p4.6.m2.1.1.3.1"></times><ci id="S3.SS2.p4.6.m2.1.1.3.2.cmml" xref="S3.SS2.p4.6.m2.1.1.3.2">𝑝</ci><ci id="S3.SS2.p4.6.m2.1.1.3.3.cmml" xref="S3.SS2.p4.6.m2.1.1.3.3">𝑟</ci><ci id="S3.SS2.p4.6.m2.1.1.3.4.cmml" xref="S3.SS2.p4.6.m2.1.1.3.4">𝑜</ci><ci id="S3.SS2.p4.6.m2.1.1.3.5.cmml" xref="S3.SS2.p4.6.m2.1.1.3.5">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m2.1c">f_{proj}</annotation></semantics></math> before feeding them into GPT-2.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">Predictors.</span> As the predictions consist of answers, textual rationales, and visual rationales, we use different predictors in our framework. For the answer prediction, we treat it as a classification problem like the prior methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> and adopt a linear fusion module followed by a sigmoid function proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite>. To generate the textual rationales, we utilize an auto-regressive large language model, GPT-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite> that predicts the next token without taking future tokens into account. For each current token in the text, GPT-2, conditioned on the projected features, produces probability scores for all vocabulary tokens, and these scores are utilized to determine the subsequent token. The prediction of visual rationales is based on an open-vocabulary object detector, Grounding-DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>, which can detect arbitrary objects with manual inputs. Given the input image and the generated textual rationale, we leverage such a powerful detector to localize the involved objects with bounding boxes. The distinction from referring object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> is that we treat the detected bounding boxes as single category. There are two reasons behind this: (1) it is hard to analyze the category of involved objects based on the input question, for instance, a question like “What color is this object?”; (2) the descriptions of the objects in the textual rationale are inconsistent from various perspectives.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.p6.6" class="ltx_p"><span id="S3.SS2.p6.6.1" class="ltx_text ltx_font_bold">Loss Functions.</span> In the training stage, we optimize the model under full supervision of answers and textual rationales with different loss functions. For VQA, the sigmoid function predicts the score <math id="S3.SS2.p6.1.m1.1" class="ltx_Math" alttext="\hat{s}" display="inline"><semantics id="S3.SS2.p6.1.m1.1a"><mover accent="true" id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml"><mi id="S3.SS2.p6.1.m1.1.1.2" xref="S3.SS2.p6.1.m1.1.1.2.cmml">s</mi><mo id="S3.SS2.p6.1.m1.1.1.1" xref="S3.SS2.p6.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><apply id="S3.SS2.p6.1.m1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1"><ci id="S3.SS2.p6.1.m1.1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1">^</ci><ci id="S3.SS2.p6.1.m1.1.1.2.cmml" xref="S3.SS2.p6.1.m1.1.1.2">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">\hat{s}</annotation></semantics></math> in the range from 0 to 1 as the probability of the answer. We adopt the widely-used binary cross-entropy loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite> for answering. To learn the explanation generation, we employ the simple yet effective cross-entropy loss based on the projection module and the GPT-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite> language model. Given <math id="S3.SS2.p6.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.p6.2.m2.1a"><mi id="S3.SS2.p6.2.m2.1.1" xref="S3.SS2.p6.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.2.m2.1b"><ci id="S3.SS2.p6.2.m2.1.1.cmml" xref="S3.SS2.p6.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.2.m2.1c">M</annotation></semantics></math> questions, <math id="S3.SS2.p6.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p6.3.m3.1a"><mi id="S3.SS2.p6.3.m3.1.1" xref="S3.SS2.p6.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.3.m3.1b"><ci id="S3.SS2.p6.3.m3.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.3.m3.1c">N</annotation></semantics></math> candidate answers, <math id="S3.SS2.p6.4.m4.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.p6.4.m4.1a"><mi id="S3.SS2.p6.4.m4.1.1" xref="S3.SS2.p6.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.4.m4.1b"><ci id="S3.SS2.p6.4.m4.1.1.cmml" xref="S3.SS2.p6.4.m4.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.4.m4.1c">M</annotation></semantics></math> textual rationales, the answering loss <math id="S3.SS2.p6.5.m5.1" class="ltx_Math" alttext="L_{ans}" display="inline"><semantics id="S3.SS2.p6.5.m5.1a"><msub id="S3.SS2.p6.5.m5.1.1" xref="S3.SS2.p6.5.m5.1.1.cmml"><mi id="S3.SS2.p6.5.m5.1.1.2" xref="S3.SS2.p6.5.m5.1.1.2.cmml">L</mi><mrow id="S3.SS2.p6.5.m5.1.1.3" xref="S3.SS2.p6.5.m5.1.1.3.cmml"><mi id="S3.SS2.p6.5.m5.1.1.3.2" xref="S3.SS2.p6.5.m5.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.5.m5.1.1.3.1" xref="S3.SS2.p6.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p6.5.m5.1.1.3.3" xref="S3.SS2.p6.5.m5.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.5.m5.1.1.3.1a" xref="S3.SS2.p6.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p6.5.m5.1.1.3.4" xref="S3.SS2.p6.5.m5.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.5.m5.1b"><apply id="S3.SS2.p6.5.m5.1.1.cmml" xref="S3.SS2.p6.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.5.m5.1.1.1.cmml" xref="S3.SS2.p6.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p6.5.m5.1.1.2.cmml" xref="S3.SS2.p6.5.m5.1.1.2">𝐿</ci><apply id="S3.SS2.p6.5.m5.1.1.3.cmml" xref="S3.SS2.p6.5.m5.1.1.3"><times id="S3.SS2.p6.5.m5.1.1.3.1.cmml" xref="S3.SS2.p6.5.m5.1.1.3.1"></times><ci id="S3.SS2.p6.5.m5.1.1.3.2.cmml" xref="S3.SS2.p6.5.m5.1.1.3.2">𝑎</ci><ci id="S3.SS2.p6.5.m5.1.1.3.3.cmml" xref="S3.SS2.p6.5.m5.1.1.3.3">𝑛</ci><ci id="S3.SS2.p6.5.m5.1.1.3.4.cmml" xref="S3.SS2.p6.5.m5.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.5.m5.1c">L_{ans}</annotation></semantics></math> and explanation generation loss <math id="S3.SS2.p6.6.m6.1" class="ltx_Math" alttext="L_{tr}" display="inline"><semantics id="S3.SS2.p6.6.m6.1a"><msub id="S3.SS2.p6.6.m6.1.1" xref="S3.SS2.p6.6.m6.1.1.cmml"><mi id="S3.SS2.p6.6.m6.1.1.2" xref="S3.SS2.p6.6.m6.1.1.2.cmml">L</mi><mrow id="S3.SS2.p6.6.m6.1.1.3" xref="S3.SS2.p6.6.m6.1.1.3.cmml"><mi id="S3.SS2.p6.6.m6.1.1.3.2" xref="S3.SS2.p6.6.m6.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.6.m6.1.1.3.1" xref="S3.SS2.p6.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p6.6.m6.1.1.3.3" xref="S3.SS2.p6.6.m6.1.1.3.3.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.6.m6.1b"><apply id="S3.SS2.p6.6.m6.1.1.cmml" xref="S3.SS2.p6.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.6.m6.1.1.1.cmml" xref="S3.SS2.p6.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p6.6.m6.1.1.2.cmml" xref="S3.SS2.p6.6.m6.1.1.2">𝐿</ci><apply id="S3.SS2.p6.6.m6.1.1.3.cmml" xref="S3.SS2.p6.6.m6.1.1.3"><times id="S3.SS2.p6.6.m6.1.1.3.1.cmml" xref="S3.SS2.p6.6.m6.1.1.3.1"></times><ci id="S3.SS2.p6.6.m6.1.1.3.2.cmml" xref="S3.SS2.p6.6.m6.1.1.3.2">𝑡</ci><ci id="S3.SS2.p6.6.m6.1.1.3.3.cmml" xref="S3.SS2.p6.6.m6.1.1.3.3">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.6.m6.1c">L_{tr}</annotation></semantics></math> are:</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.3" class="ltx_Math" alttext="L_{ans}=-\sum_{i}^{M}\sum_{j}^{N}s_{ij}\log(\hat{s}_{ij})-(1-s_{ij})\log(1-\hat{s}_{ij})," display="block"><semantics id="S3.E5.m1.3a"><mrow id="S3.E5.m1.3.3.1" xref="S3.E5.m1.3.3.1.1.cmml"><mrow id="S3.E5.m1.3.3.1.1" xref="S3.E5.m1.3.3.1.1.cmml"><msub id="S3.E5.m1.3.3.1.1.5" xref="S3.E5.m1.3.3.1.1.5.cmml"><mi id="S3.E5.m1.3.3.1.1.5.2" xref="S3.E5.m1.3.3.1.1.5.2.cmml">L</mi><mrow id="S3.E5.m1.3.3.1.1.5.3" xref="S3.E5.m1.3.3.1.1.5.3.cmml"><mi id="S3.E5.m1.3.3.1.1.5.3.2" xref="S3.E5.m1.3.3.1.1.5.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.1.1.5.3.1" xref="S3.E5.m1.3.3.1.1.5.3.1.cmml">​</mo><mi id="S3.E5.m1.3.3.1.1.5.3.3" xref="S3.E5.m1.3.3.1.1.5.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.1.1.5.3.1a" xref="S3.E5.m1.3.3.1.1.5.3.1.cmml">​</mo><mi id="S3.E5.m1.3.3.1.1.5.3.4" xref="S3.E5.m1.3.3.1.1.5.3.4.cmml">s</mi></mrow></msub><mo id="S3.E5.m1.3.3.1.1.4" xref="S3.E5.m1.3.3.1.1.4.cmml">=</mo><mrow id="S3.E5.m1.3.3.1.1.3" xref="S3.E5.m1.3.3.1.1.3.cmml"><mrow id="S3.E5.m1.3.3.1.1.1.1" xref="S3.E5.m1.3.3.1.1.1.1.cmml"><mo id="S3.E5.m1.3.3.1.1.1.1a" xref="S3.E5.m1.3.3.1.1.1.1.cmml">−</mo><mrow id="S3.E5.m1.3.3.1.1.1.1.1" xref="S3.E5.m1.3.3.1.1.1.1.1.cmml"><munderover id="S3.E5.m1.3.3.1.1.1.1.1.2" xref="S3.E5.m1.3.3.1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E5.m1.3.3.1.1.1.1.1.2.2.2" xref="S3.E5.m1.3.3.1.1.1.1.1.2.2.2.cmml">∑</mo><mi id="S3.E5.m1.3.3.1.1.1.1.1.2.2.3" xref="S3.E5.m1.3.3.1.1.1.1.1.2.2.3.cmml">i</mi><mi id="S3.E5.m1.3.3.1.1.1.1.1.2.3" xref="S3.E5.m1.3.3.1.1.1.1.1.2.3.cmml">M</mi></munderover><mrow id="S3.E5.m1.3.3.1.1.1.1.1.1" xref="S3.E5.m1.3.3.1.1.1.1.1.1.cmml"><munderover id="S3.E5.m1.3.3.1.1.1.1.1.1.2" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E5.m1.3.3.1.1.1.1.1.1.2.2.2" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mi id="S3.E5.m1.3.3.1.1.1.1.1.1.2.2.3" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2.2.3.cmml">j</mi><mi id="S3.E5.m1.3.3.1.1.1.1.1.1.2.3" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S3.E5.m1.3.3.1.1.1.1.1.1.1" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.cmml"><msub id="S3.E5.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">s</mi><mrow id="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.3" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.3.2" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.3.1" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.3.3" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.3.3.cmml">j</mi></mrow></msub><mo lspace="0.167em" rspace="0em" id="S3.E5.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">log</mi><mo id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1a" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">(</mo><msub id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">s</mi><mo id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mrow id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo stretchy="false" id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo id="S3.E5.m1.3.3.1.1.3.4" xref="S3.E5.m1.3.3.1.1.3.4.cmml">−</mo><mrow id="S3.E5.m1.3.3.1.1.3.3" xref="S3.E5.m1.3.3.1.1.3.3.cmml"><mrow id="S3.E5.m1.3.3.1.1.2.2.1.1" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.E5.m1.3.3.1.1.2.2.1.1.2" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.3.3.1.1.2.2.1.1.1" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.cmml"><mn id="S3.E5.m1.3.3.1.1.2.2.1.1.1.2" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.2.cmml">1</mn><mo id="S3.E5.m1.3.3.1.1.2.2.1.1.1.1" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.1.cmml">−</mo><msub id="S3.E5.m1.3.3.1.1.2.2.1.1.1.3" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.cmml"><mi id="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.2" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.2.cmml">s</mi><mrow id="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.3" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.3.cmml"><mi id="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.3.2" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.3.1" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.3.3" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.3.3.cmml">j</mi></mrow></msub></mrow><mo stretchy="false" id="S3.E5.m1.3.3.1.1.2.2.1.1.3" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.cmml">)</mo></mrow><mo lspace="0.167em" rspace="0em" id="S3.E5.m1.3.3.1.1.3.3.3" xref="S3.E5.m1.3.3.1.1.3.3.3.cmml">​</mo><mrow id="S3.E5.m1.3.3.1.1.3.3.2.1" xref="S3.E5.m1.3.3.1.1.3.3.2.2.cmml"><mi id="S3.E5.m1.2.2" xref="S3.E5.m1.2.2.cmml">log</mi><mo id="S3.E5.m1.3.3.1.1.3.3.2.1a" xref="S3.E5.m1.3.3.1.1.3.3.2.2.cmml">⁡</mo><mrow id="S3.E5.m1.3.3.1.1.3.3.2.1.1" xref="S3.E5.m1.3.3.1.1.3.3.2.2.cmml"><mo stretchy="false" id="S3.E5.m1.3.3.1.1.3.3.2.1.1.2" xref="S3.E5.m1.3.3.1.1.3.3.2.2.cmml">(</mo><mrow id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.cmml"><mn id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.2" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.2.cmml">1</mn><mo id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.1" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.1.cmml">−</mo><msub id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.cmml"><mover accent="true" id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.2" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.2.cmml"><mi id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.2.2" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.2.2.cmml">s</mi><mo id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.2.1" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.2.1.cmml">^</mo></mover><mrow id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.3" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.3.cmml"><mi id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.3.2" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.3.1" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.3.3" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.3.3.cmml">j</mi></mrow></msub></mrow><mo stretchy="false" id="S3.E5.m1.3.3.1.1.3.3.2.1.1.3" xref="S3.E5.m1.3.3.1.1.3.3.2.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E5.m1.3.3.1.2" xref="S3.E5.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.3b"><apply id="S3.E5.m1.3.3.1.1.cmml" xref="S3.E5.m1.3.3.1"><eq id="S3.E5.m1.3.3.1.1.4.cmml" xref="S3.E5.m1.3.3.1.1.4"></eq><apply id="S3.E5.m1.3.3.1.1.5.cmml" xref="S3.E5.m1.3.3.1.1.5"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.5.1.cmml" xref="S3.E5.m1.3.3.1.1.5">subscript</csymbol><ci id="S3.E5.m1.3.3.1.1.5.2.cmml" xref="S3.E5.m1.3.3.1.1.5.2">𝐿</ci><apply id="S3.E5.m1.3.3.1.1.5.3.cmml" xref="S3.E5.m1.3.3.1.1.5.3"><times id="S3.E5.m1.3.3.1.1.5.3.1.cmml" xref="S3.E5.m1.3.3.1.1.5.3.1"></times><ci id="S3.E5.m1.3.3.1.1.5.3.2.cmml" xref="S3.E5.m1.3.3.1.1.5.3.2">𝑎</ci><ci id="S3.E5.m1.3.3.1.1.5.3.3.cmml" xref="S3.E5.m1.3.3.1.1.5.3.3">𝑛</ci><ci id="S3.E5.m1.3.3.1.1.5.3.4.cmml" xref="S3.E5.m1.3.3.1.1.5.3.4">𝑠</ci></apply></apply><apply id="S3.E5.m1.3.3.1.1.3.cmml" xref="S3.E5.m1.3.3.1.1.3"><minus id="S3.E5.m1.3.3.1.1.3.4.cmml" xref="S3.E5.m1.3.3.1.1.3.4"></minus><apply id="S3.E5.m1.3.3.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1"><minus id="S3.E5.m1.3.3.1.1.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1"></minus><apply id="S3.E5.m1.3.3.1.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1"><apply id="S3.E5.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E5.m1.3.3.1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.1.1.1.2.2.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E5.m1.3.3.1.1.1.1.1.2.2.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.2.2.2"></sum><ci id="S3.E5.m1.3.3.1.1.1.1.1.2.2.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.2.2.3">𝑖</ci></apply><ci id="S3.E5.m1.3.3.1.1.1.1.1.2.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.2.3">𝑀</ci></apply><apply id="S3.E5.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1"><apply id="S3.E5.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E5.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E5.m1.3.3.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2.2.2"></sum><ci id="S3.E5.m1.3.3.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2.2.3">𝑗</ci></apply><ci id="S3.E5.m1.3.3.1.1.1.1.1.1.2.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2.3">𝑁</ci></apply><apply id="S3.E5.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1"><times id="S3.E5.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.2"></times><apply id="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.2">𝑠</ci><apply id="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.3"><times id="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.3.1"></times><ci id="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.3.2">𝑖</ci><ci id="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.3.3.3">𝑗</ci></apply></apply><apply id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1"><log id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"></log><apply id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2"><ci id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑠</ci></apply><apply id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3"><times id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑖</ci><ci id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply></apply></apply></apply></apply><apply id="S3.E5.m1.3.3.1.1.3.3.cmml" xref="S3.E5.m1.3.3.1.1.3.3"><times id="S3.E5.m1.3.3.1.1.3.3.3.cmml" xref="S3.E5.m1.3.3.1.1.3.3.3"></times><apply id="S3.E5.m1.3.3.1.1.2.2.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.2.2.1.1"><minus id="S3.E5.m1.3.3.1.1.2.2.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.1"></minus><cn type="integer" id="S3.E5.m1.3.3.1.1.2.2.1.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.2">1</cn><apply id="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.cmml" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.1.cmml" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.2.cmml" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.2">𝑠</ci><apply id="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.3.cmml" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.3"><times id="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.3.1.cmml" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.3.1"></times><ci id="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.3.2.cmml" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.3.2">𝑖</ci><ci id="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.3.3.cmml" xref="S3.E5.m1.3.3.1.1.2.2.1.1.1.3.3.3">𝑗</ci></apply></apply></apply><apply id="S3.E5.m1.3.3.1.1.3.3.2.2.cmml" xref="S3.E5.m1.3.3.1.1.3.3.2.1"><log id="S3.E5.m1.2.2.cmml" xref="S3.E5.m1.2.2"></log><apply id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1"><minus id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.1"></minus><cn type="integer" id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.2">1</cn><apply id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.cmml" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.1.cmml" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3">subscript</csymbol><apply id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.2.cmml" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.2"><ci id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.2.1.cmml" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.2.1">^</ci><ci id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.2.2.cmml" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.2.2">𝑠</ci></apply><apply id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.3.cmml" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.3"><times id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.3.1.cmml" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.3.1"></times><ci id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.3.2.cmml" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.3.2">𝑖</ci><ci id="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.3.3.cmml" xref="S3.E5.m1.3.3.1.1.3.3.2.1.1.1.3.3.3">𝑗</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.3c">L_{ans}=-\sum_{i}^{M}\sum_{j}^{N}s_{ij}\log(\hat{s}_{ij})-(1-s_{ij})\log(1-\hat{s}_{ij}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.2" class="ltx_Math" alttext="L_{tr}=-\sum_{i}^{M}\sum_{t}^{l}\log p_{\theta}(w_{t}^{i}|f_{proj}^{i},w_{1}^{i},...,w_{t-1}^{i})," display="block"><semantics id="S3.E6.m1.2a"><mrow id="S3.E6.m1.2.2.1" xref="S3.E6.m1.2.2.1.1.cmml"><mrow id="S3.E6.m1.2.2.1.1" xref="S3.E6.m1.2.2.1.1.cmml"><msub id="S3.E6.m1.2.2.1.1.3" xref="S3.E6.m1.2.2.1.1.3.cmml"><mi id="S3.E6.m1.2.2.1.1.3.2" xref="S3.E6.m1.2.2.1.1.3.2.cmml">L</mi><mrow id="S3.E6.m1.2.2.1.1.3.3" xref="S3.E6.m1.2.2.1.1.3.3.cmml"><mi id="S3.E6.m1.2.2.1.1.3.3.2" xref="S3.E6.m1.2.2.1.1.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.1.1.3.3.1" xref="S3.E6.m1.2.2.1.1.3.3.1.cmml">​</mo><mi id="S3.E6.m1.2.2.1.1.3.3.3" xref="S3.E6.m1.2.2.1.1.3.3.3.cmml">r</mi></mrow></msub><mo id="S3.E6.m1.2.2.1.1.2" xref="S3.E6.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E6.m1.2.2.1.1.1" xref="S3.E6.m1.2.2.1.1.1.cmml"><mo id="S3.E6.m1.2.2.1.1.1a" xref="S3.E6.m1.2.2.1.1.1.cmml">−</mo><mrow id="S3.E6.m1.2.2.1.1.1.1" xref="S3.E6.m1.2.2.1.1.1.1.cmml"><munderover id="S3.E6.m1.2.2.1.1.1.1.2" xref="S3.E6.m1.2.2.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E6.m1.2.2.1.1.1.1.2.2.2" xref="S3.E6.m1.2.2.1.1.1.1.2.2.2.cmml">∑</mo><mi id="S3.E6.m1.2.2.1.1.1.1.2.2.3" xref="S3.E6.m1.2.2.1.1.1.1.2.2.3.cmml">i</mi><mi id="S3.E6.m1.2.2.1.1.1.1.2.3" xref="S3.E6.m1.2.2.1.1.1.1.2.3.cmml">M</mi></munderover><mrow id="S3.E6.m1.2.2.1.1.1.1.1" xref="S3.E6.m1.2.2.1.1.1.1.1.cmml"><munderover id="S3.E6.m1.2.2.1.1.1.1.1.2" xref="S3.E6.m1.2.2.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E6.m1.2.2.1.1.1.1.1.2.2.2" xref="S3.E6.m1.2.2.1.1.1.1.1.2.2.2.cmml">∑</mo><mi id="S3.E6.m1.2.2.1.1.1.1.1.2.2.3" xref="S3.E6.m1.2.2.1.1.1.1.1.2.2.3.cmml">t</mi><mi id="S3.E6.m1.2.2.1.1.1.1.1.2.3" xref="S3.E6.m1.2.2.1.1.1.1.1.2.3.cmml">l</mi></munderover><mrow id="S3.E6.m1.2.2.1.1.1.1.1.1" xref="S3.E6.m1.2.2.1.1.1.1.1.1.cmml"><mrow id="S3.E6.m1.2.2.1.1.1.1.1.1.3" xref="S3.E6.m1.2.2.1.1.1.1.1.1.3.cmml"><mi id="S3.E6.m1.2.2.1.1.1.1.1.1.3.1" xref="S3.E6.m1.2.2.1.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E6.m1.2.2.1.1.1.1.1.1.3a" xref="S3.E6.m1.2.2.1.1.1.1.1.1.3.cmml">⁡</mo><msub id="S3.E6.m1.2.2.1.1.1.1.1.1.3.2" xref="S3.E6.m1.2.2.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E6.m1.2.2.1.1.1.1.1.1.3.2.2" xref="S3.E6.m1.2.2.1.1.1.1.1.1.3.2.2.cmml">p</mi><mi id="S3.E6.m1.2.2.1.1.1.1.1.1.3.2.3" xref="S3.E6.m1.2.2.1.1.1.1.1.1.3.2.3.cmml">θ</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.1.1.1.1.1.1.2" xref="S3.E6.m1.2.2.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5.cmml"><mi id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5.2.2" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5.2.2.cmml">w</mi><mi id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5.2.3" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5.2.3.cmml">t</mi><mi id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5.3" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5.3.cmml">i</mi></msubsup><mo fence="false" id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.4" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.4.cmml">|</mo><mrow id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.4.cmml"><msubsup id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">f</mi><mrow id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1a" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.4" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1b" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.5" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.5.cmml">j</mi></mrow><mi id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msubsup><mo id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.4" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.4.cmml">,</mo><msubsup id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml">w</mi><mn id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml">1</mn><mi id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">i</mi></msubsup><mo id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.5" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml">…</mi><mo id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.6" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.4.cmml">,</mo><msubsup id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.2" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.2.cmml">w</mi><mrow id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.3" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.3.cmml"><mi id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.3.2" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.3.2.cmml">t</mi><mo id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.3.1" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.3.1.cmml">−</mo><mn id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.3.3" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.3.3.cmml">1</mn></mrow><mi id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.3" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.3.cmml">i</mi></msubsup></mrow></mrow><mo stretchy="false" id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo id="S3.E6.m1.2.2.1.2" xref="S3.E6.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.2b"><apply id="S3.E6.m1.2.2.1.1.cmml" xref="S3.E6.m1.2.2.1"><eq id="S3.E6.m1.2.2.1.1.2.cmml" xref="S3.E6.m1.2.2.1.1.2"></eq><apply id="S3.E6.m1.2.2.1.1.3.cmml" xref="S3.E6.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.3.1.cmml" xref="S3.E6.m1.2.2.1.1.3">subscript</csymbol><ci id="S3.E6.m1.2.2.1.1.3.2.cmml" xref="S3.E6.m1.2.2.1.1.3.2">𝐿</ci><apply id="S3.E6.m1.2.2.1.1.3.3.cmml" xref="S3.E6.m1.2.2.1.1.3.3"><times id="S3.E6.m1.2.2.1.1.3.3.1.cmml" xref="S3.E6.m1.2.2.1.1.3.3.1"></times><ci id="S3.E6.m1.2.2.1.1.3.3.2.cmml" xref="S3.E6.m1.2.2.1.1.3.3.2">𝑡</ci><ci id="S3.E6.m1.2.2.1.1.3.3.3.cmml" xref="S3.E6.m1.2.2.1.1.3.3.3">𝑟</ci></apply></apply><apply id="S3.E6.m1.2.2.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1"><minus id="S3.E6.m1.2.2.1.1.1.2.cmml" xref="S3.E6.m1.2.2.1.1.1"></minus><apply id="S3.E6.m1.2.2.1.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1"><apply id="S3.E6.m1.2.2.1.1.1.1.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2">superscript</csymbol><apply id="S3.E6.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.2.2.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2">subscript</csymbol><sum id="S3.E6.m1.2.2.1.1.1.1.2.2.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2.2.2"></sum><ci id="S3.E6.m1.2.2.1.1.1.1.2.2.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2.2.3">𝑖</ci></apply><ci id="S3.E6.m1.2.2.1.1.1.1.2.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2.3">𝑀</ci></apply><apply id="S3.E6.m1.2.2.1.1.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1"><apply id="S3.E6.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E6.m1.2.2.1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.1.2.2.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E6.m1.2.2.1.1.1.1.1.2.2.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.2.2.2"></sum><ci id="S3.E6.m1.2.2.1.1.1.1.1.2.2.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.2.2.3">𝑡</ci></apply><ci id="S3.E6.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.2.3">𝑙</ci></apply><apply id="S3.E6.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1"><times id="S3.E6.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.2"></times><apply id="S3.E6.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.3"><log id="S3.E6.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.3.1"></log><apply id="S3.E6.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E6.m1.2.2.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.3.2.2">𝑝</ci><ci id="S3.E6.m1.2.2.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.3.2.3">𝜃</ci></apply></apply><apply id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.4">conditional</csymbol><apply id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5">superscript</csymbol><apply id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5.2.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5">subscript</csymbol><ci id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5.2.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5.2.2">𝑤</ci><ci id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5.2.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5.2.3">𝑡</ci></apply><ci id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.5.3">𝑖</ci></apply><list id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.4.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3"><apply id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑓</ci><apply id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3"><times id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1"></times><ci id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2">𝑝</ci><ci id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3">𝑟</ci><ci id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.4.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.4">𝑜</ci><ci id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.5.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.5">𝑗</ci></apply></apply><ci id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2">superscript</csymbol><apply id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.2.2">𝑤</ci><cn type="integer" id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.2.3">1</cn></apply><ci id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.3">𝑖</ci></apply><ci id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1">…</ci><apply id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3">superscript</csymbol><apply id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.2">𝑤</ci><apply id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.3"><minus id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.3.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.3.1"></minus><ci id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.3.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.3.2">𝑡</ci><cn type="integer" id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.3.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.2.3.3">1</cn></apply></apply><ci id="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.3.3">𝑖</ci></apply></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.2c">L_{tr}=-\sum_{i}^{M}\sum_{t}^{l}\log p_{\theta}(w_{t}^{i}|f_{proj}^{i},w_{1}^{i},...,w_{t-1}^{i}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p6.11" class="ltx_p">where <math id="S3.SS2.p6.7.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS2.p6.7.m1.1a"><mi id="S3.SS2.p6.7.m1.1.1" xref="S3.SS2.p6.7.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.7.m1.1b"><ci id="S3.SS2.p6.7.m1.1.1.cmml" xref="S3.SS2.p6.7.m1.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.7.m1.1c">l</annotation></semantics></math>, <math id="S3.SS2.p6.8.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS2.p6.8.m2.1a"><mi id="S3.SS2.p6.8.m2.1.1" xref="S3.SS2.p6.8.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.8.m2.1b"><ci id="S3.SS2.p6.8.m2.1.1.cmml" xref="S3.SS2.p6.8.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.8.m2.1c">\theta</annotation></semantics></math> represent the maximum length of tokens and the trainable parameters, respectively. The sequenced tokens are referred as <math id="S3.SS2.p6.9.m3.3" class="ltx_Math" alttext="w^{i}=w_{1}^{i},...,w_{l}^{i}" display="inline"><semantics id="S3.SS2.p6.9.m3.3a"><mrow id="S3.SS2.p6.9.m3.3.3" xref="S3.SS2.p6.9.m3.3.3.cmml"><msup id="S3.SS2.p6.9.m3.3.3.4" xref="S3.SS2.p6.9.m3.3.3.4.cmml"><mi id="S3.SS2.p6.9.m3.3.3.4.2" xref="S3.SS2.p6.9.m3.3.3.4.2.cmml">w</mi><mi id="S3.SS2.p6.9.m3.3.3.4.3" xref="S3.SS2.p6.9.m3.3.3.4.3.cmml">i</mi></msup><mo id="S3.SS2.p6.9.m3.3.3.3" xref="S3.SS2.p6.9.m3.3.3.3.cmml">=</mo><mrow id="S3.SS2.p6.9.m3.3.3.2.2" xref="S3.SS2.p6.9.m3.3.3.2.3.cmml"><msubsup id="S3.SS2.p6.9.m3.2.2.1.1.1" xref="S3.SS2.p6.9.m3.2.2.1.1.1.cmml"><mi id="S3.SS2.p6.9.m3.2.2.1.1.1.2.2" xref="S3.SS2.p6.9.m3.2.2.1.1.1.2.2.cmml">w</mi><mn id="S3.SS2.p6.9.m3.2.2.1.1.1.2.3" xref="S3.SS2.p6.9.m3.2.2.1.1.1.2.3.cmml">1</mn><mi id="S3.SS2.p6.9.m3.2.2.1.1.1.3" xref="S3.SS2.p6.9.m3.2.2.1.1.1.3.cmml">i</mi></msubsup><mo id="S3.SS2.p6.9.m3.3.3.2.2.3" xref="S3.SS2.p6.9.m3.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p6.9.m3.1.1" xref="S3.SS2.p6.9.m3.1.1.cmml">…</mi><mo id="S3.SS2.p6.9.m3.3.3.2.2.4" xref="S3.SS2.p6.9.m3.3.3.2.3.cmml">,</mo><msubsup id="S3.SS2.p6.9.m3.3.3.2.2.2" xref="S3.SS2.p6.9.m3.3.3.2.2.2.cmml"><mi id="S3.SS2.p6.9.m3.3.3.2.2.2.2.2" xref="S3.SS2.p6.9.m3.3.3.2.2.2.2.2.cmml">w</mi><mi id="S3.SS2.p6.9.m3.3.3.2.2.2.2.3" xref="S3.SS2.p6.9.m3.3.3.2.2.2.2.3.cmml">l</mi><mi id="S3.SS2.p6.9.m3.3.3.2.2.2.3" xref="S3.SS2.p6.9.m3.3.3.2.2.2.3.cmml">i</mi></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.9.m3.3b"><apply id="S3.SS2.p6.9.m3.3.3.cmml" xref="S3.SS2.p6.9.m3.3.3"><eq id="S3.SS2.p6.9.m3.3.3.3.cmml" xref="S3.SS2.p6.9.m3.3.3.3"></eq><apply id="S3.SS2.p6.9.m3.3.3.4.cmml" xref="S3.SS2.p6.9.m3.3.3.4"><csymbol cd="ambiguous" id="S3.SS2.p6.9.m3.3.3.4.1.cmml" xref="S3.SS2.p6.9.m3.3.3.4">superscript</csymbol><ci id="S3.SS2.p6.9.m3.3.3.4.2.cmml" xref="S3.SS2.p6.9.m3.3.3.4.2">𝑤</ci><ci id="S3.SS2.p6.9.m3.3.3.4.3.cmml" xref="S3.SS2.p6.9.m3.3.3.4.3">𝑖</ci></apply><list id="S3.SS2.p6.9.m3.3.3.2.3.cmml" xref="S3.SS2.p6.9.m3.3.3.2.2"><apply id="S3.SS2.p6.9.m3.2.2.1.1.1.cmml" xref="S3.SS2.p6.9.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.9.m3.2.2.1.1.1.1.cmml" xref="S3.SS2.p6.9.m3.2.2.1.1.1">superscript</csymbol><apply id="S3.SS2.p6.9.m3.2.2.1.1.1.2.cmml" xref="S3.SS2.p6.9.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.9.m3.2.2.1.1.1.2.1.cmml" xref="S3.SS2.p6.9.m3.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.p6.9.m3.2.2.1.1.1.2.2.cmml" xref="S3.SS2.p6.9.m3.2.2.1.1.1.2.2">𝑤</ci><cn type="integer" id="S3.SS2.p6.9.m3.2.2.1.1.1.2.3.cmml" xref="S3.SS2.p6.9.m3.2.2.1.1.1.2.3">1</cn></apply><ci id="S3.SS2.p6.9.m3.2.2.1.1.1.3.cmml" xref="S3.SS2.p6.9.m3.2.2.1.1.1.3">𝑖</ci></apply><ci id="S3.SS2.p6.9.m3.1.1.cmml" xref="S3.SS2.p6.9.m3.1.1">…</ci><apply id="S3.SS2.p6.9.m3.3.3.2.2.2.cmml" xref="S3.SS2.p6.9.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p6.9.m3.3.3.2.2.2.1.cmml" xref="S3.SS2.p6.9.m3.3.3.2.2.2">superscript</csymbol><apply id="S3.SS2.p6.9.m3.3.3.2.2.2.2.cmml" xref="S3.SS2.p6.9.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p6.9.m3.3.3.2.2.2.2.1.cmml" xref="S3.SS2.p6.9.m3.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.p6.9.m3.3.3.2.2.2.2.2.cmml" xref="S3.SS2.p6.9.m3.3.3.2.2.2.2.2">𝑤</ci><ci id="S3.SS2.p6.9.m3.3.3.2.2.2.2.3.cmml" xref="S3.SS2.p6.9.m3.3.3.2.2.2.2.3">𝑙</ci></apply><ci id="S3.SS2.p6.9.m3.3.3.2.2.2.3.cmml" xref="S3.SS2.p6.9.m3.3.3.2.2.2.3">𝑖</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.9.m3.3c">w^{i}=w_{1}^{i},...,w_{l}^{i}</annotation></semantics></math>. To train the entire model for multi-task learning, we take the linear combination of the answering loss <math id="S3.SS2.p6.10.m4.1" class="ltx_Math" alttext="L_{ans}" display="inline"><semantics id="S3.SS2.p6.10.m4.1a"><msub id="S3.SS2.p6.10.m4.1.1" xref="S3.SS2.p6.10.m4.1.1.cmml"><mi id="S3.SS2.p6.10.m4.1.1.2" xref="S3.SS2.p6.10.m4.1.1.2.cmml">L</mi><mrow id="S3.SS2.p6.10.m4.1.1.3" xref="S3.SS2.p6.10.m4.1.1.3.cmml"><mi id="S3.SS2.p6.10.m4.1.1.3.2" xref="S3.SS2.p6.10.m4.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.10.m4.1.1.3.1" xref="S3.SS2.p6.10.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p6.10.m4.1.1.3.3" xref="S3.SS2.p6.10.m4.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.10.m4.1.1.3.1a" xref="S3.SS2.p6.10.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p6.10.m4.1.1.3.4" xref="S3.SS2.p6.10.m4.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.10.m4.1b"><apply id="S3.SS2.p6.10.m4.1.1.cmml" xref="S3.SS2.p6.10.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.10.m4.1.1.1.cmml" xref="S3.SS2.p6.10.m4.1.1">subscript</csymbol><ci id="S3.SS2.p6.10.m4.1.1.2.cmml" xref="S3.SS2.p6.10.m4.1.1.2">𝐿</ci><apply id="S3.SS2.p6.10.m4.1.1.3.cmml" xref="S3.SS2.p6.10.m4.1.1.3"><times id="S3.SS2.p6.10.m4.1.1.3.1.cmml" xref="S3.SS2.p6.10.m4.1.1.3.1"></times><ci id="S3.SS2.p6.10.m4.1.1.3.2.cmml" xref="S3.SS2.p6.10.m4.1.1.3.2">𝑎</ci><ci id="S3.SS2.p6.10.m4.1.1.3.3.cmml" xref="S3.SS2.p6.10.m4.1.1.3.3">𝑛</ci><ci id="S3.SS2.p6.10.m4.1.1.3.4.cmml" xref="S3.SS2.p6.10.m4.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.10.m4.1c">L_{ans}</annotation></semantics></math> and explanation generation loss <math id="S3.SS2.p6.11.m5.1" class="ltx_Math" alttext="L_{tr}" display="inline"><semantics id="S3.SS2.p6.11.m5.1a"><msub id="S3.SS2.p6.11.m5.1.1" xref="S3.SS2.p6.11.m5.1.1.cmml"><mi id="S3.SS2.p6.11.m5.1.1.2" xref="S3.SS2.p6.11.m5.1.1.2.cmml">L</mi><mrow id="S3.SS2.p6.11.m5.1.1.3" xref="S3.SS2.p6.11.m5.1.1.3.cmml"><mi id="S3.SS2.p6.11.m5.1.1.3.2" xref="S3.SS2.p6.11.m5.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.11.m5.1.1.3.1" xref="S3.SS2.p6.11.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p6.11.m5.1.1.3.3" xref="S3.SS2.p6.11.m5.1.1.3.3.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.11.m5.1b"><apply id="S3.SS2.p6.11.m5.1.1.cmml" xref="S3.SS2.p6.11.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.11.m5.1.1.1.cmml" xref="S3.SS2.p6.11.m5.1.1">subscript</csymbol><ci id="S3.SS2.p6.11.m5.1.1.2.cmml" xref="S3.SS2.p6.11.m5.1.1.2">𝐿</ci><apply id="S3.SS2.p6.11.m5.1.1.3.cmml" xref="S3.SS2.p6.11.m5.1.1.3"><times id="S3.SS2.p6.11.m5.1.1.3.1.cmml" xref="S3.SS2.p6.11.m5.1.1.3.1"></times><ci id="S3.SS2.p6.11.m5.1.1.3.2.cmml" xref="S3.SS2.p6.11.m5.1.1.3.2">𝑡</ci><ci id="S3.SS2.p6.11.m5.1.1.3.3.cmml" xref="S3.SS2.p6.11.m5.1.1.3.3">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.11.m5.1c">L_{tr}</annotation></semantics></math> as follows:</p>
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1.1" class="ltx_Math" alttext="L=L_{ans}+L_{tr}," display="block"><semantics id="S3.E7.m1.1a"><mrow id="S3.E7.m1.1.1.1" xref="S3.E7.m1.1.1.1.1.cmml"><mrow id="S3.E7.m1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.cmml"><mi id="S3.E7.m1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.2.cmml">L</mi><mo id="S3.E7.m1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E7.m1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.3.cmml"><msub id="S3.E7.m1.1.1.1.1.3.2" xref="S3.E7.m1.1.1.1.1.3.2.cmml"><mi id="S3.E7.m1.1.1.1.1.3.2.2" xref="S3.E7.m1.1.1.1.1.3.2.2.cmml">L</mi><mrow id="S3.E7.m1.1.1.1.1.3.2.3" xref="S3.E7.m1.1.1.1.1.3.2.3.cmml"><mi id="S3.E7.m1.1.1.1.1.3.2.3.2" xref="S3.E7.m1.1.1.1.1.3.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.1.1.3.2.3.1" xref="S3.E7.m1.1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S3.E7.m1.1.1.1.1.3.2.3.3" xref="S3.E7.m1.1.1.1.1.3.2.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.1.1.3.2.3.1a" xref="S3.E7.m1.1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S3.E7.m1.1.1.1.1.3.2.3.4" xref="S3.E7.m1.1.1.1.1.3.2.3.4.cmml">s</mi></mrow></msub><mo id="S3.E7.m1.1.1.1.1.3.1" xref="S3.E7.m1.1.1.1.1.3.1.cmml">+</mo><msub id="S3.E7.m1.1.1.1.1.3.3" xref="S3.E7.m1.1.1.1.1.3.3.cmml"><mi id="S3.E7.m1.1.1.1.1.3.3.2" xref="S3.E7.m1.1.1.1.1.3.3.2.cmml">L</mi><mrow id="S3.E7.m1.1.1.1.1.3.3.3" xref="S3.E7.m1.1.1.1.1.3.3.3.cmml"><mi id="S3.E7.m1.1.1.1.1.3.3.3.2" xref="S3.E7.m1.1.1.1.1.3.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.1.1.3.3.3.1" xref="S3.E7.m1.1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E7.m1.1.1.1.1.3.3.3.3" xref="S3.E7.m1.1.1.1.1.3.3.3.3.cmml">r</mi></mrow></msub></mrow></mrow><mo id="S3.E7.m1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><apply id="S3.E7.m1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1"><eq id="S3.E7.m1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1"></eq><ci id="S3.E7.m1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.2">𝐿</ci><apply id="S3.E7.m1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.3"><plus id="S3.E7.m1.1.1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.1.1.3.1"></plus><apply id="S3.E7.m1.1.1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.3.2.1.cmml" xref="S3.E7.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E7.m1.1.1.1.1.3.2.2.cmml" xref="S3.E7.m1.1.1.1.1.3.2.2">𝐿</ci><apply id="S3.E7.m1.1.1.1.1.3.2.3.cmml" xref="S3.E7.m1.1.1.1.1.3.2.3"><times id="S3.E7.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.E7.m1.1.1.1.1.3.2.3.1"></times><ci id="S3.E7.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.E7.m1.1.1.1.1.3.2.3.2">𝑎</ci><ci id="S3.E7.m1.1.1.1.1.3.2.3.3.cmml" xref="S3.E7.m1.1.1.1.1.3.2.3.3">𝑛</ci><ci id="S3.E7.m1.1.1.1.1.3.2.3.4.cmml" xref="S3.E7.m1.1.1.1.1.3.2.3.4">𝑠</ci></apply></apply><apply id="S3.E7.m1.1.1.1.1.3.3.cmml" xref="S3.E7.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.3.3.1.cmml" xref="S3.E7.m1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E7.m1.1.1.1.1.3.3.2.cmml" xref="S3.E7.m1.1.1.1.1.3.3.2">𝐿</ci><apply id="S3.E7.m1.1.1.1.1.3.3.3.cmml" xref="S3.E7.m1.1.1.1.1.3.3.3"><times id="S3.E7.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E7.m1.1.1.1.1.3.3.3.1"></times><ci id="S3.E7.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E7.m1.1.1.1.1.3.3.3.2">𝑡</ci><ci id="S3.E7.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E7.m1.1.1.1.1.3.3.3.3">𝑟</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">L=L_{ans}+L_{tr},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Visual-Textual Similarity Score</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">Prior methods for text generation task commonly adopt the language evaluation metrics, such as BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> and SPICE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>. Recently, with the advance of the embedding and transcription models in the field of natural language processing, the similarity score between the sentence embeddings can be a good option to measure the quality of the generated text. For instance, pre-trained BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> is good at converting the text to dense vectors that can be used to calculate the distance between two sentences.
We employ a powerful pre-trained text embedding model, GTE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>, to project the generated textual rationale to the high-dimensional space, and utilize the cosine similarity score to measure the textual similarity compared with the ground truth. Furthermore, we assess the quality of the generated texts from a visual standpoint. Specifically, the predicted bounding boxes are compared with the ground truth by calculating the Average Precision (AP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>.
We introduce a new metric <span id="S3.SS3.p1.2.1" class="ltx_text ltx_font_bold">vtS</span> (<span id="S3.SS3.p1.2.2" class="ltx_text ltx_font_bold">v</span>isual-<span id="S3.SS3.p1.2.3" class="ltx_text ltx_font_bold">t</span>extural <span id="S3.SS3.p1.2.4" class="ltx_text ltx_font_bold">S</span>imilarity score), to assess the generated textual rationales, providing a comprehensive measure of VQA reasoning capability.
Given the generated textual rationale <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">P</annotation></semantics></math>, ground-truth <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="GT" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><times id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1"></times><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">𝐺</ci><ci id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">GT</annotation></semantics></math>, the metric <span id="S3.SS3.p1.2.5" class="ltx_text ltx_font_bold">vtS</span> is defined as follows:</p>
<table id="S3.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E8.m1.7" class="ltx_Math" alttext="vtS=\frac{2\times\cos{({\rm GTE}(P,GT))}\times AP}{\cos{({\rm GTE}(P,GT))}+AP}," display="block"><semantics id="S3.E8.m1.7a"><mrow id="S3.E8.m1.7.7.1" xref="S3.E8.m1.7.7.1.1.cmml"><mrow id="S3.E8.m1.7.7.1.1" xref="S3.E8.m1.7.7.1.1.cmml"><mrow id="S3.E8.m1.7.7.1.1.2" xref="S3.E8.m1.7.7.1.1.2.cmml"><mi id="S3.E8.m1.7.7.1.1.2.2" xref="S3.E8.m1.7.7.1.1.2.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.7.7.1.1.2.1" xref="S3.E8.m1.7.7.1.1.2.1.cmml">​</mo><mi id="S3.E8.m1.7.7.1.1.2.3" xref="S3.E8.m1.7.7.1.1.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.7.7.1.1.2.1a" xref="S3.E8.m1.7.7.1.1.2.1.cmml">​</mo><mi id="S3.E8.m1.7.7.1.1.2.4" xref="S3.E8.m1.7.7.1.1.2.4.cmml">S</mi></mrow><mo id="S3.E8.m1.7.7.1.1.1" xref="S3.E8.m1.7.7.1.1.1.cmml">=</mo><mfrac id="S3.E8.m1.6.6" xref="S3.E8.m1.6.6.cmml"><mrow id="S3.E8.m1.3.3.3" xref="S3.E8.m1.3.3.3.cmml"><mrow id="S3.E8.m1.3.3.3.3" xref="S3.E8.m1.3.3.3.3.cmml"><mn id="S3.E8.m1.3.3.3.3.3" xref="S3.E8.m1.3.3.3.3.3.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E8.m1.3.3.3.3.2" xref="S3.E8.m1.3.3.3.3.2.cmml">×</mo><mrow id="S3.E8.m1.3.3.3.3.1.1" xref="S3.E8.m1.3.3.3.3.1.2.cmml"><mi id="S3.E8.m1.2.2.2.2" xref="S3.E8.m1.2.2.2.2.cmml">cos</mi><mo id="S3.E8.m1.3.3.3.3.1.1a" xref="S3.E8.m1.3.3.3.3.1.2.cmml">⁡</mo><mrow id="S3.E8.m1.3.3.3.3.1.1.1" xref="S3.E8.m1.3.3.3.3.1.2.cmml"><mo stretchy="false" id="S3.E8.m1.3.3.3.3.1.1.1.2" xref="S3.E8.m1.3.3.3.3.1.2.cmml">(</mo><mrow id="S3.E8.m1.3.3.3.3.1.1.1.1" xref="S3.E8.m1.3.3.3.3.1.1.1.1.cmml"><mi id="S3.E8.m1.3.3.3.3.1.1.1.1.3" xref="S3.E8.m1.3.3.3.3.1.1.1.1.3.cmml">GTE</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.3.3.3.3.1.1.1.1.2" xref="S3.E8.m1.3.3.3.3.1.1.1.1.2.cmml">​</mo><mrow id="S3.E8.m1.3.3.3.3.1.1.1.1.1.1" xref="S3.E8.m1.3.3.3.3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E8.m1.3.3.3.3.1.1.1.1.1.1.2" xref="S3.E8.m1.3.3.3.3.1.1.1.1.1.2.cmml">(</mo><mi id="S3.E8.m1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.cmml">P</mi><mo id="S3.E8.m1.3.3.3.3.1.1.1.1.1.1.3" xref="S3.E8.m1.3.3.3.3.1.1.1.1.1.2.cmml">,</mo><mrow id="S3.E8.m1.3.3.3.3.1.1.1.1.1.1.1" xref="S3.E8.m1.3.3.3.3.1.1.1.1.1.1.1.cmml"><mi id="S3.E8.m1.3.3.3.3.1.1.1.1.1.1.1.2" xref="S3.E8.m1.3.3.3.3.1.1.1.1.1.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.3.3.3.3.1.1.1.1.1.1.1.1" xref="S3.E8.m1.3.3.3.3.1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.E8.m1.3.3.3.3.1.1.1.1.1.1.1.3" xref="S3.E8.m1.3.3.3.3.1.1.1.1.1.1.1.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.E8.m1.3.3.3.3.1.1.1.1.1.1.4" xref="S3.E8.m1.3.3.3.3.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo rspace="0.055em" stretchy="false" id="S3.E8.m1.3.3.3.3.1.1.1.3" xref="S3.E8.m1.3.3.3.3.1.2.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S3.E8.m1.3.3.3.3.2a" xref="S3.E8.m1.3.3.3.3.2.cmml">×</mo><mi id="S3.E8.m1.3.3.3.3.4" xref="S3.E8.m1.3.3.3.3.4.cmml">A</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E8.m1.3.3.3.4" xref="S3.E8.m1.3.3.3.4.cmml">​</mo><mi id="S3.E8.m1.3.3.3.5" xref="S3.E8.m1.3.3.3.5.cmml">P</mi></mrow><mrow id="S3.E8.m1.6.6.6" xref="S3.E8.m1.6.6.6.cmml"><mrow id="S3.E8.m1.6.6.6.3.1" xref="S3.E8.m1.6.6.6.3.2.cmml"><mi id="S3.E8.m1.5.5.5.2" xref="S3.E8.m1.5.5.5.2.cmml">cos</mi><mo id="S3.E8.m1.6.6.6.3.1a" xref="S3.E8.m1.6.6.6.3.2.cmml">⁡</mo><mrow id="S3.E8.m1.6.6.6.3.1.1" xref="S3.E8.m1.6.6.6.3.2.cmml"><mo stretchy="false" id="S3.E8.m1.6.6.6.3.1.1.2" xref="S3.E8.m1.6.6.6.3.2.cmml">(</mo><mrow id="S3.E8.m1.6.6.6.3.1.1.1" xref="S3.E8.m1.6.6.6.3.1.1.1.cmml"><mi id="S3.E8.m1.6.6.6.3.1.1.1.3" xref="S3.E8.m1.6.6.6.3.1.1.1.3.cmml">GTE</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.6.6.6.3.1.1.1.2" xref="S3.E8.m1.6.6.6.3.1.1.1.2.cmml">​</mo><mrow id="S3.E8.m1.6.6.6.3.1.1.1.1.1" xref="S3.E8.m1.6.6.6.3.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E8.m1.6.6.6.3.1.1.1.1.1.2" xref="S3.E8.m1.6.6.6.3.1.1.1.1.2.cmml">(</mo><mi id="S3.E8.m1.4.4.4.1" xref="S3.E8.m1.4.4.4.1.cmml">P</mi><mo id="S3.E8.m1.6.6.6.3.1.1.1.1.1.3" xref="S3.E8.m1.6.6.6.3.1.1.1.1.2.cmml">,</mo><mrow id="S3.E8.m1.6.6.6.3.1.1.1.1.1.1" xref="S3.E8.m1.6.6.6.3.1.1.1.1.1.1.cmml"><mi id="S3.E8.m1.6.6.6.3.1.1.1.1.1.1.2" xref="S3.E8.m1.6.6.6.3.1.1.1.1.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.6.6.6.3.1.1.1.1.1.1.1" xref="S3.E8.m1.6.6.6.3.1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.E8.m1.6.6.6.3.1.1.1.1.1.1.3" xref="S3.E8.m1.6.6.6.3.1.1.1.1.1.1.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.E8.m1.6.6.6.3.1.1.1.1.1.4" xref="S3.E8.m1.6.6.6.3.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E8.m1.6.6.6.3.1.1.3" xref="S3.E8.m1.6.6.6.3.2.cmml">)</mo></mrow></mrow><mo id="S3.E8.m1.6.6.6.4" xref="S3.E8.m1.6.6.6.4.cmml">+</mo><mrow id="S3.E8.m1.6.6.6.5" xref="S3.E8.m1.6.6.6.5.cmml"><mi id="S3.E8.m1.6.6.6.5.2" xref="S3.E8.m1.6.6.6.5.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.6.6.6.5.1" xref="S3.E8.m1.6.6.6.5.1.cmml">​</mo><mi id="S3.E8.m1.6.6.6.5.3" xref="S3.E8.m1.6.6.6.5.3.cmml">P</mi></mrow></mrow></mfrac></mrow><mo id="S3.E8.m1.7.7.1.2" xref="S3.E8.m1.7.7.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.7b"><apply id="S3.E8.m1.7.7.1.1.cmml" xref="S3.E8.m1.7.7.1"><eq id="S3.E8.m1.7.7.1.1.1.cmml" xref="S3.E8.m1.7.7.1.1.1"></eq><apply id="S3.E8.m1.7.7.1.1.2.cmml" xref="S3.E8.m1.7.7.1.1.2"><times id="S3.E8.m1.7.7.1.1.2.1.cmml" xref="S3.E8.m1.7.7.1.1.2.1"></times><ci id="S3.E8.m1.7.7.1.1.2.2.cmml" xref="S3.E8.m1.7.7.1.1.2.2">𝑣</ci><ci id="S3.E8.m1.7.7.1.1.2.3.cmml" xref="S3.E8.m1.7.7.1.1.2.3">𝑡</ci><ci id="S3.E8.m1.7.7.1.1.2.4.cmml" xref="S3.E8.m1.7.7.1.1.2.4">𝑆</ci></apply><apply id="S3.E8.m1.6.6.cmml" xref="S3.E8.m1.6.6"><divide id="S3.E8.m1.6.6.7.cmml" xref="S3.E8.m1.6.6"></divide><apply id="S3.E8.m1.3.3.3.cmml" xref="S3.E8.m1.3.3.3"><times id="S3.E8.m1.3.3.3.4.cmml" xref="S3.E8.m1.3.3.3.4"></times><apply id="S3.E8.m1.3.3.3.3.cmml" xref="S3.E8.m1.3.3.3.3"><times id="S3.E8.m1.3.3.3.3.2.cmml" xref="S3.E8.m1.3.3.3.3.2"></times><cn type="integer" id="S3.E8.m1.3.3.3.3.3.cmml" xref="S3.E8.m1.3.3.3.3.3">2</cn><apply id="S3.E8.m1.3.3.3.3.1.2.cmml" xref="S3.E8.m1.3.3.3.3.1.1"><cos id="S3.E8.m1.2.2.2.2.cmml" xref="S3.E8.m1.2.2.2.2"></cos><apply id="S3.E8.m1.3.3.3.3.1.1.1.1.cmml" xref="S3.E8.m1.3.3.3.3.1.1.1.1"><times id="S3.E8.m1.3.3.3.3.1.1.1.1.2.cmml" xref="S3.E8.m1.3.3.3.3.1.1.1.1.2"></times><ci id="S3.E8.m1.3.3.3.3.1.1.1.1.3.cmml" xref="S3.E8.m1.3.3.3.3.1.1.1.1.3">GTE</ci><interval closure="open" id="S3.E8.m1.3.3.3.3.1.1.1.1.1.2.cmml" xref="S3.E8.m1.3.3.3.3.1.1.1.1.1.1"><ci id="S3.E8.m1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1">𝑃</ci><apply id="S3.E8.m1.3.3.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.3.3.3.3.1.1.1.1.1.1.1"><times id="S3.E8.m1.3.3.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.3.3.3.3.1.1.1.1.1.1.1.1"></times><ci id="S3.E8.m1.3.3.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E8.m1.3.3.3.3.1.1.1.1.1.1.1.2">𝐺</ci><ci id="S3.E8.m1.3.3.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.E8.m1.3.3.3.3.1.1.1.1.1.1.1.3">𝑇</ci></apply></interval></apply></apply><ci id="S3.E8.m1.3.3.3.3.4.cmml" xref="S3.E8.m1.3.3.3.3.4">𝐴</ci></apply><ci id="S3.E8.m1.3.3.3.5.cmml" xref="S3.E8.m1.3.3.3.5">𝑃</ci></apply><apply id="S3.E8.m1.6.6.6.cmml" xref="S3.E8.m1.6.6.6"><plus id="S3.E8.m1.6.6.6.4.cmml" xref="S3.E8.m1.6.6.6.4"></plus><apply id="S3.E8.m1.6.6.6.3.2.cmml" xref="S3.E8.m1.6.6.6.3.1"><cos id="S3.E8.m1.5.5.5.2.cmml" xref="S3.E8.m1.5.5.5.2"></cos><apply id="S3.E8.m1.6.6.6.3.1.1.1.cmml" xref="S3.E8.m1.6.6.6.3.1.1.1"><times id="S3.E8.m1.6.6.6.3.1.1.1.2.cmml" xref="S3.E8.m1.6.6.6.3.1.1.1.2"></times><ci id="S3.E8.m1.6.6.6.3.1.1.1.3.cmml" xref="S3.E8.m1.6.6.6.3.1.1.1.3">GTE</ci><interval closure="open" id="S3.E8.m1.6.6.6.3.1.1.1.1.2.cmml" xref="S3.E8.m1.6.6.6.3.1.1.1.1.1"><ci id="S3.E8.m1.4.4.4.1.cmml" xref="S3.E8.m1.4.4.4.1">𝑃</ci><apply id="S3.E8.m1.6.6.6.3.1.1.1.1.1.1.cmml" xref="S3.E8.m1.6.6.6.3.1.1.1.1.1.1"><times id="S3.E8.m1.6.6.6.3.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.6.6.6.3.1.1.1.1.1.1.1"></times><ci id="S3.E8.m1.6.6.6.3.1.1.1.1.1.1.2.cmml" xref="S3.E8.m1.6.6.6.3.1.1.1.1.1.1.2">𝐺</ci><ci id="S3.E8.m1.6.6.6.3.1.1.1.1.1.1.3.cmml" xref="S3.E8.m1.6.6.6.3.1.1.1.1.1.1.3">𝑇</ci></apply></interval></apply></apply><apply id="S3.E8.m1.6.6.6.5.cmml" xref="S3.E8.m1.6.6.6.5"><times id="S3.E8.m1.6.6.6.5.1.cmml" xref="S3.E8.m1.6.6.6.5.1"></times><ci id="S3.E8.m1.6.6.6.5.2.cmml" xref="S3.E8.m1.6.6.6.5.2">𝐴</ci><ci id="S3.E8.m1.6.6.6.5.3.cmml" xref="S3.E8.m1.6.6.6.5.3">𝑃</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.7c">vtS=\frac{2\times\cos{({\rm GTE}(P,GT))}\times AP}{\cos{({\rm GTE}(P,GT))}+AP},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S4" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiment Setup</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Datasets.</span> We evaluate our proposed CRVQA on three datasets, namely VQA-R, VQA-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite>, and VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>. VQA-R is built on top of VQA-E <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> as discussed in Sec. <a href="#S3.SS1" title="3.1 VQA-R Dataset Synthesis ‣ 3 Method ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. Bounding box annotations are supported in the test stage to analyze the quality of the textual rationales and the visual rationales. VQA-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> contains 30K image/question pairs and their corresponding textual explanations, which are human-annotated and of high quality. VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> is a widely-used VQA benchmark dataset that includes 110K image/question pairs. It only supports the answer prediction evaluation.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.4.2" class="ltx_text" style="font-size:90%;">Results on VQA-R for evaluating the textual rationale generation, with the image/question representation and text predictor each method uses. B, M, R, C, S, and vtS are abbreviated for BLEU-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>, METEOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>, ROUGE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>, CIDEr <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>, SPICE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>, and visual-textual Similarity score. All scores are reported in percentage (%). The best results are highlighted in <span id="S4.T1.4.2.1" class="ltx_text ltx_font_bold">bold</span>.</span></figcaption>
<table id="S4.T1.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T1.5.1" class="ltx_tr">
<td id="S4.T1.5.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">
<span id="S4.T1.5.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.1.1.1.1" class="ltx_p" style="width:71.1pt;">Method</span>
</span>
</td>
<td id="S4.T1.5.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">V-Features</td>
<td id="S4.T1.5.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">Q-Features</td>
<td id="S4.T1.5.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">T-Predictor</td>
<td id="S4.T1.5.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">B</td>
<td id="S4.T1.5.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">M</td>
<td id="S4.T1.5.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">R</td>
<td id="S4.T1.5.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">C</td>
<td id="S4.T1.5.1.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">S</td>
<td id="S4.T1.5.1.10" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">vtS</td>
</tr>
<tr id="S4.T1.5.2" class="ltx_tr">
<td id="S4.T1.5.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">
<span id="S4.T1.5.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.2.1.1.1" class="ltx_p" style="width:71.1pt;">PJ-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite></span>
</span>
</td>
<td id="S4.T1.5.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">CNN</td>
<td id="S4.T1.5.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">LSTM</td>
<td id="S4.T1.5.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">LSTM</td>
<td id="S4.T1.5.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">8.78</td>
<td id="S4.T1.5.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">16.94</td>
<td id="S4.T1.5.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">35.65</td>
<td id="S4.T1.5.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">89.31</td>
<td id="S4.T1.5.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">15.32</td>
<td id="S4.T1.5.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">49.19</td>
</tr>
<tr id="S4.T1.5.3" class="ltx_tr">
<td id="S4.T1.5.3.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:7.0pt;padding-right:7.0pt;">
<span id="S4.T1.5.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.3.1.1.1" class="ltx_p" style="width:71.1pt;">VQA-E <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite></span>
</span>
</td>
<td id="S4.T1.5.3.2" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">CNN</td>
<td id="S4.T1.5.3.3" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">GRU</td>
<td id="S4.T1.5.3.4" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">LSTM</td>
<td id="S4.T1.5.3.5" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">8.93</td>
<td id="S4.T1.5.3.6" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">17.02</td>
<td id="S4.T1.5.3.7" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">35.96</td>
<td id="S4.T1.5.3.8" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">90.84</td>
<td id="S4.T1.5.3.9" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">16.83</td>
<td id="S4.T1.5.3.10" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">51.88</td>
</tr>
<tr id="S4.T1.5.4" class="ltx_tr">
<td id="S4.T1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:7.0pt;padding-right:7.0pt;">
<span id="S4.T1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.4.1.1.1" class="ltx_p" style="width:71.1pt;">FME <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite></span>
</span>
</td>
<td id="S4.T1.5.4.2" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">CNN</td>
<td id="S4.T1.5.4.3" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">GRU</td>
<td id="S4.T1.5.4.4" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">LSTM</td>
<td id="S4.T1.5.4.5" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">12.81</td>
<td id="S4.T1.5.4.6" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">21.26</td>
<td id="S4.T1.5.4.7" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">37.69</td>
<td id="S4.T1.5.4.8" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">89.16</td>
<td id="S4.T1.5.4.9" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">18.77</td>
<td id="S4.T1.5.4.10" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">53.82</td>
</tr>
<tr id="S4.T1.5.5" class="ltx_tr">
<td id="S4.T1.5.5.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:7.0pt;padding-right:7.0pt;">
<span id="S4.T1.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.5.1.1.1" class="ltx_p" style="width:71.1pt;">CCM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite></span>
</span>
</td>
<td id="S4.T1.5.5.2" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">CNN</td>
<td id="S4.T1.5.5.3" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">LSTM</td>
<td id="S4.T1.5.5.4" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">LSTM</td>
<td id="S4.T1.5.5.5" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">8.85</td>
<td id="S4.T1.5.5.6" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">17.86</td>
<td id="S4.T1.5.5.7" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">38.42</td>
<td id="S4.T1.5.5.8" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">92.46</td>
<td id="S4.T1.5.5.9" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">18.35</td>
<td id="S4.T1.5.5.10" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">54.44</td>
</tr>
<tr id="S4.T1.5.6" class="ltx_tr">
<td id="S4.T1.5.6.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:7.0pt;padding-right:7.0pt;">
<span id="S4.T1.5.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.6.1.1.1" class="ltx_p" style="width:71.1pt;">DMRFNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite></span>
</span>
</td>
<td id="S4.T1.5.6.2" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">CNN</td>
<td id="S4.T1.5.6.3" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">GRU</td>
<td id="S4.T1.5.6.4" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">GRU</td>
<td id="S4.T1.5.6.5" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">13.34</td>
<td id="S4.T1.5.6.6" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">19.44</td>
<td id="S4.T1.5.6.7" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">40.76</td>
<td id="S4.T1.5.6.8" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">95.68</td>
<td id="S4.T1.5.6.9" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">20.41</td>
<td id="S4.T1.5.6.10" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">56.06</td>
</tr>
<tr id="S4.T1.5.7" class="ltx_tr">
<td id="S4.T1.5.7.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:7.0pt;padding-right:7.0pt;">
<span id="S4.T1.5.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.7.1.1.1" class="ltx_p" style="width:71.1pt;">CRVQA-v2</span>
</span>
</td>
<td id="S4.T1.5.7.2" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">CLIP</td>
<td id="S4.T1.5.7.3" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">CLIP</td>
<td id="S4.T1.5.7.4" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">GPT</td>
<td id="S4.T1.5.7.5" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">15.08</td>
<td id="S4.T1.5.7.6" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">21.77</td>
<td id="S4.T1.5.7.7" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">42.04</td>
<td id="S4.T1.5.7.8" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">102.86</td>
<td id="S4.T1.5.7.9" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">21.22</td>
<td id="S4.T1.5.7.10" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">57.30</td>
</tr>
<tr id="S4.T1.5.8" class="ltx_tr">
<td id="S4.T1.5.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;">
<span id="S4.T1.5.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.8.1.1.1" class="ltx_p" style="width:71.1pt;">CRVQA-v3</span>
</span>
</td>
<td id="S4.T1.5.8.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;">CLIP</td>
<td id="S4.T1.5.8.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;">CLIP</td>
<td id="S4.T1.5.8.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;">GPT</td>
<td id="S4.T1.5.8.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;"><span id="S4.T1.5.8.5.1" class="ltx_text ltx_font_bold">15.84</span></td>
<td id="S4.T1.5.8.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;"><span id="S4.T1.5.8.6.1" class="ltx_text ltx_font_bold">22.41</span></td>
<td id="S4.T1.5.8.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;"><span id="S4.T1.5.8.7.1" class="ltx_text ltx_font_bold">43.57</span></td>
<td id="S4.T1.5.8.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;"><span id="S4.T1.5.8.8.1" class="ltx_text ltx_font_bold">105.31</span></td>
<td id="S4.T1.5.8.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;"><span id="S4.T1.5.8.9.1" class="ltx_text ltx_font_bold">22.19</span></td>
<td id="S4.T1.5.8.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;"><span id="S4.T1.5.8.10.1" class="ltx_text ltx_font_bold">58.19</span></td>
</tr>
</table>
</figure>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Implementation Details.</span> We adopt the visual encoder of the pre-trained CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> model with the ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> base version for the image representation. We also use the CLIP model to extract the features as the question representation. The encoder-decoder network is built on the 6-layer self-attention and cross-attention Transformers inspired by MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite>. We use the AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> optimizer with the learning rate of <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="2\times 10^{-5}" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mn id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">×</mo><msup id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml"><mn id="S4.SS1.p2.1.m1.1.1.3.2" xref="S4.SS1.p2.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.p2.1.m1.1.1.3.3" xref="S4.SS1.p2.1.m1.1.1.3.3.cmml"><mo id="S4.SS1.p2.1.m1.1.1.3.3a" xref="S4.SS1.p2.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS1.p2.1.m1.1.1.3.3.2" xref="S4.SS1.p2.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><times id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">2</cn><apply id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.p2.1.m1.1.1.3.2.cmml" xref="S4.SS1.p2.1.m1.1.1.3.2">10</cn><apply id="S4.SS1.p2.1.m1.1.1.3.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3"><minus id="S4.SS1.p2.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.SS1.p2.1.m1.1.1.3.3.2.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">2\times 10^{-5}</annotation></semantics></math>. All the models are trained for 50 epochs with a batch size of 64 on two A40 GPUs in the same environment.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Evaluation Metrics.</span> As the predictions include the answer, textual rationale, and bounding box, we evaluate the performance with various metrics. For visual question answering performance, we adopt the classification accuracy and only take the multiple-choice answer as the ground truth. To evaluate the generated textual rationales, we employ five mainstream metrics, namely BLEU-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>, METEOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>, ROUGE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>, CIDEr <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite> and SPICE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> (B, M, R, C, S for their abbreviations, respectively), which measure the language similarity between the generated and ground truth text. In addition, we use the newly introduced metric <span id="S4.SS1.p3.1.2" class="ltx_text ltx_font_bold">vtS</span> to better capture the similarity by leveraging the visual information. We use the GTE-large <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite> as the embedding model for calculating the textual similarity. Since the prediction of the visual rationale is treated as a single-category object bounding box for the given image/question pair, we adopt the widely-used AP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>. To align with human preference, following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>, we also include a user study across four aspects: <span id="S4.SS1.p3.1.3" class="ltx_text ltx_font_italic">fluent</span>, <span id="S4.SS1.p3.1.4" class="ltx_text ltx_font_italic">correct</span>, <span id="S4.SS1.p3.1.5" class="ltx_text ltx_font_italic">relevant</span>, and <span id="S4.SS1.p3.1.6" class="ltx_text ltx_font_italic">complementary</span>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results on VQA-R and VQA-X</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Considering the different predictions for this task, we report the results on VQA-R and VQA-X in three parts.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Experiment Setup ‣ 4 Experiments ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the experimental results of the compared explanatory VQA methods on VQA-R dataset in terms of various textual rationale evaluation metrics. Note that CRVQA-v1 and CRVQA-v2 denote the model only trained and predicted for the answer and the textual rationale, respectively, while the CRVQA-v3 represent the combined version.
We observe that our proposed methods boost the performance of the text similarity (B, M, R, C, S) compared to the prior methods. With the help of the CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> model for the input representation and the GPT-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite> language model for the text generation, our method trained for only textual rationales (CRVQA-v2) improves the scores as well. In addition, our method achieves the highest vtS score over the comparison methods with the better quality of the generated textual and visual rationales. In turn, it helps to evaluate the reliability of the predicted answer and the generated explanation.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Results on VQA-R and VQA-X ‣ 4 Experiments ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> compares the proposed method with other VQA models on VQA-R dataset in term of answer accuracy. The upper part reports the performance of the models trained for generic VQA. The methods in bottom part support not only the answer prediction but the textual explanation generation.
Due to the fact that no specifics regarding the implementation details of answer prediction or the associated VQA accuracy are reported in PJ-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite>, FME <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite> and CCM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>, we do not show the corresponding scores in Tables <a href="#S4.T2" title="Table 2 ‣ 4.2 Results on VQA-R and VQA-X ‣ 4 Experiments ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S4.T3" title="Table 3 ‣ 4.2 Results on VQA-R and VQA-X ‣ 4 Experiments ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
Our proposed method (CRVQA-v3) achieves 74.81% overall accuracy and outperforms other methods on VQA-R. The boosting scores for “Yes/No” and “Other” category questions (3.60% and 0.89%, respectively) validate the effectiveness of the proposed method.
We emphasize that the textual rationale generation can enhance answer accuracy, as demonstrated by comparing the results of CRVQA-v1 and CRVQA-v3.
We argue that the additional supervision of textual rationales can provide more information that the answering model needs.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">We further compare the results with other explanatory VQA methods on VQA-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> in Table <a href="#S4.T3" title="Table 3 ‣ 4.2 Results on VQA-R and VQA-X ‣ 4 Experiments ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. From this table, we can observe similar phenomenon in which our proposed method outperforms all prior methods on both the answer accuracy and the text similarity scores. Although the scale of VQA-X is much smaller than VQA-E <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>, we believe that with more high-quality annotations, our method can generate more robust and reliable textual rationales to support the predicted answers.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.3.2" class="ltx_text" style="font-size:90%;">Results on VQA-R for answer accuracy. The methods on top are trained only for answer prediction, while the methods below are designed for answers as well as textual explanations. OA and AA represent overall and average accuracy, respectively.</span></figcaption>
<table id="S4.T2.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T2.4.1" class="ltx_tr">
<td id="S4.T2.4.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.T2.4.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.1.1.1.1" class="ltx_p" style="width:64.0pt;">Method</span>
</span>
</td>
<td id="S4.T2.4.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Number</td>
<td id="S4.T2.4.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Yes/No</td>
<td id="S4.T2.4.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Other</td>
<td id="S4.T2.4.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">OA</td>
<td id="S4.T2.4.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">AA</td>
</tr>
<tr id="S4.T2.4.2" class="ltx_tr">
<td id="S4.T2.4.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.T2.4.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.2.1.1.1" class="ltx_p" style="width:64.0pt;">MUTAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite></span>
</span>
</td>
<td id="S4.T2.4.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">49.23</td>
<td id="S4.T2.4.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">65.52</td>
<td id="S4.T2.4.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">60.27</td>
<td id="S4.T2.4.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">61.07</td>
<td id="S4.T2.4.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">58.34</td>
</tr>
<tr id="S4.T2.4.3" class="ltx_tr">
<td id="S4.T2.4.3.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.T2.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.3.1.1.1" class="ltx_p" style="width:64.0pt;">BUTD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite></span>
</span>
</td>
<td id="S4.T2.4.3.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">52.68</td>
<td id="S4.T2.4.3.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">64.07</td>
<td id="S4.T2.4.3.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">65.72</td>
<td id="S4.T2.4.3.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">64.13</td>
<td id="S4.T2.4.3.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">60.83</td>
</tr>
<tr id="S4.T2.4.4" class="ltx_tr">
<td id="S4.T2.4.4.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.T2.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.4.1.1.1" class="ltx_p" style="width:64.0pt;">BAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite></span>
</span>
</td>
<td id="S4.T2.4.4.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">54.45</td>
<td id="S4.T2.4.4.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">67.81</td>
<td id="S4.T2.4.4.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">68.63</td>
<td id="S4.T2.4.4.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">67.22</td>
<td id="S4.T2.4.4.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">63.63</td>
</tr>
<tr id="S4.T2.4.5" class="ltx_tr">
<td id="S4.T2.4.5.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.T2.4.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.5.1.1.1" class="ltx_p" style="width:64.0pt;">MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite></span>
</span>
</td>
<td id="S4.T2.4.5.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">59.19</td>
<td id="S4.T2.4.5.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">71.70</td>
<td id="S4.T2.4.5.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">72.37</td>
<td id="S4.T2.4.5.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">71.09</td>
<td id="S4.T2.4.5.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">67.76</td>
</tr>
<tr id="S4.T2.4.6" class="ltx_tr">
<td id="S4.T2.4.6.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.T2.4.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.6.1.1.1" class="ltx_p" style="width:64.0pt;">BLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite></span>
</span>
</td>
<td id="S4.T2.4.6.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T2.4.6.2.1" class="ltx_text ltx_font_bold">63.68</span></td>
<td id="S4.T2.4.6.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">72.43</td>
<td id="S4.T2.4.6.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">74.83</td>
<td id="S4.T2.4.6.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">73.16</td>
<td id="S4.T2.4.6.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">70.31</td>
</tr>
<tr id="S4.T2.4.7" class="ltx_tr">
<td id="S4.T2.4.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.T2.4.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.7.1.1.1" class="ltx_p" style="width:64.0pt;">VQA-E <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite></span>
</span>
</td>
<td id="S4.T2.4.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">54.26</td>
<td id="S4.T2.4.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">63.98</td>
<td id="S4.T2.4.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">66.45</td>
<td id="S4.T2.4.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">64.67</td>
<td id="S4.T2.4.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">61.57</td>
</tr>
<tr id="S4.T2.4.8" class="ltx_tr">
<td id="S4.T2.4.8.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.T2.4.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.8.1.1.1" class="ltx_p" style="width:64.0pt;">DMRFNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite></span>
</span>
</td>
<td id="S4.T2.4.8.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">59.08</td>
<td id="S4.T2.4.8.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">72.28</td>
<td id="S4.T2.4.8.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">73.84</td>
<td id="S4.T2.4.8.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">72.15</td>
<td id="S4.T2.4.8.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">68.40</td>
</tr>
<tr id="S4.T2.4.9" class="ltx_tr">
<td id="S4.T2.4.9.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.T2.4.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.9.1.1.1" class="ltx_p" style="width:64.0pt;">CRVQA-v1</span>
</span>
</td>
<td id="S4.T2.4.9.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">57.02</td>
<td id="S4.T2.4.9.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">72.80</td>
<td id="S4.T2.4.9.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">73.41</td>
<td id="S4.T2.4.9.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">71.89</td>
<td id="S4.T2.4.9.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">67.75</td>
</tr>
<tr id="S4.T2.4.10" class="ltx_tr">
<td id="S4.T2.4.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.T2.4.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.10.1.1.1" class="ltx_p" style="width:64.0pt;">CRVQA-v3</span>
</span>
</td>
<td id="S4.T2.4.10.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">63.20</td>
<td id="S4.T2.4.10.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T2.4.10.3.1" class="ltx_text ltx_font_bold">76.03</span></td>
<td id="S4.T2.4.10.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T2.4.10.4.1" class="ltx_text ltx_font_bold">75.72</span></td>
<td id="S4.T2.4.10.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T2.4.10.5.1" class="ltx_text ltx_font_bold">74.81</span></td>
<td id="S4.T2.4.10.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T2.4.10.6.1" class="ltx_text ltx_font_bold">71.65</span></td>
</tr>
</table>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.4.2" class="ltx_text" style="font-size:90%;">Results on VQA-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> test set for answer and textual rationale accuracy. The best results are highlighted in <span id="S4.T3.4.2.1" class="ltx_text ltx_font_bold">bold</span>.</span></figcaption>
<table id="S4.T3.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T3.5.1" class="ltx_tr">
<td id="S4.T3.5.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T3.5.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.1.1.1.1" class="ltx_p" style="width:64.0pt;">Method</span>
</span>
</td>
<td id="S4.T3.5.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">B</td>
<td id="S4.T3.5.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">M</td>
<td id="S4.T3.5.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">R</td>
<td id="S4.T3.5.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">C</td>
<td id="S4.T3.5.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">S</td>
<td id="S4.T3.5.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">OA</td>
</tr>
<tr id="S4.T3.5.2" class="ltx_tr">
<td id="S4.T3.5.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T3.5.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.2.1.1.1" class="ltx_p" style="width:64.0pt;">PJ-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite></span>
</span>
</td>
<td id="S4.T3.5.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">19.5</td>
<td id="S4.T3.5.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">18.2</td>
<td id="S4.T3.5.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">43.7</td>
<td id="S4.T3.5.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">71.3</td>
<td id="S4.T3.5.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">15.1</td>
<td id="S4.T3.5.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
<tr id="S4.T3.5.3" class="ltx_tr">
<td id="S4.T3.5.3.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T3.5.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.3.1.1.1" class="ltx_p" style="width:64.0pt;">VQA-E <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite></span>
</span>
</td>
<td id="S4.T3.5.3.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">20.7</td>
<td id="S4.T3.5.3.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">18.6</td>
<td id="S4.T3.5.3.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">44.2</td>
<td id="S4.T3.5.3.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">75.6</td>
<td id="S4.T3.5.3.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">15.6</td>
<td id="S4.T3.5.3.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">70.2</td>
</tr>
<tr id="S4.T3.5.4" class="ltx_tr">
<td id="S4.T3.5.4.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T3.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.4.1.1.1" class="ltx_p" style="width:64.0pt;">FME <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite></span>
</span>
</td>
<td id="S4.T3.5.4.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">24.4</td>
<td id="S4.T3.5.4.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">19.5</td>
<td id="S4.T3.5.4.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">47.4</td>
<td id="S4.T3.5.4.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">88.8</td>
<td id="S4.T3.5.4.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">17.9</td>
<td id="S4.T3.5.4.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
<tr id="S4.T3.5.5" class="ltx_tr">
<td id="S4.T3.5.5.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T3.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.5.1.1.1" class="ltx_p" style="width:64.0pt;">CCM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite></span>
</span>
</td>
<td id="S4.T3.5.5.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">21.1</td>
<td id="S4.T3.5.5.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">19.7</td>
<td id="S4.T3.5.5.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">44.9</td>
<td id="S4.T3.5.5.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">73.9</td>
<td id="S4.T3.5.5.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">16.2</td>
<td id="S4.T3.5.5.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
<tr id="S4.T3.5.6" class="ltx_tr">
<td id="S4.T3.5.6.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T3.5.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.6.1.1.1" class="ltx_p" style="width:64.0pt;">DMRFNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite></span>
</span>
</td>
<td id="S4.T3.5.6.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">20.5</td>
<td id="S4.T3.5.6.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">19.9</td>
<td id="S4.T3.5.6.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">41.3</td>
<td id="S4.T3.5.6.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">74.5</td>
<td id="S4.T3.5.6.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">17.6</td>
<td id="S4.T3.5.6.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">72.6</td>
</tr>
<tr id="S4.T3.5.7" class="ltx_tr">
<td id="S4.T3.5.7.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T3.5.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.7.1.1.1" class="ltx_p" style="width:64.0pt;">CRVQA-v2</span>
</span>
</td>
<td id="S4.T3.5.7.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">26.2</td>
<td id="S4.T3.5.7.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">21.2</td>
<td id="S4.T3.5.7.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">46.3</td>
<td id="S4.T3.5.7.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.5.7.5.1" class="ltx_text ltx_font_bold">99.6</span></td>
<td id="S4.T3.5.7.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">18.8</td>
<td id="S4.T3.5.7.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
<tr id="S4.T3.5.8" class="ltx_tr">
<td id="S4.T3.5.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T3.5.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.5.8.1.1.1" class="ltx_p" style="width:64.0pt;">CRVQA-v3</span>
</span>
</td>
<td id="S4.T3.5.8.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.5.8.2.1" class="ltx_text ltx_font_bold">26.6</span></td>
<td id="S4.T3.5.8.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.5.8.3.1" class="ltx_text ltx_font_bold">22.0</span></td>
<td id="S4.T3.5.8.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.5.8.4.1" class="ltx_text ltx_font_bold">48.9</span></td>
<td id="S4.T3.5.8.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">98.4</td>
<td id="S4.T3.5.8.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.5.8.6.1" class="ltx_text ltx_font_bold">19.2</span></td>
<td id="S4.T3.5.8.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.5.8.7.1" class="ltx_text ltx_font_bold">78.8</span></td>
</tr>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Zero-Shot Evaluation on VQAv2</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We perform zero-shot evaluation on VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> and compare the results with prior methods in Tabel <a href="#S4.T4" title="Table 4 ‣ 4.3 Zero-Shot Evaluation on VQAv2 ‣ 4 Experiments ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The upper part shows the methods either directly trained or pre-trained on large-scale data and then fine-tuned on VQAv2.
The results get better with more powerful models and data. For instance, pre-training with more than 129M image/text pairs, BLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite> can improve the accuracy to 78.3% after the downstream fine-tuning. The methods listed in the bottom part are reported with the zero-shot evaluation performance. These methods are trained on other datasets then evaluated on VQAv2. We can observe that even with quite limited training data, our method can achieve competitive results (64.7%) for generic VQA in the zero-shot setting. The comparison with the large pre-training model BLIP2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite> also validates the effectiveness of incorporating the textual rationales in the answering system.
We believe that training with large-scale data can further enhance the answering performance under the new task settings.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.7.2.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.2.1" class="ltx_text" style="font-size:90%;">Comparison with state-of-the-art methods on VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> test-dev. T-Data and T-Scale represent the training dataset and the image/question pairs. Multiple includes pre-training vision-language datasets (129M<sup id="S4.T4.2.1.1" class="ltx_sup">⋆</sup> images) as stated in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>.</span></figcaption>
<table id="S4.T4.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T4.4.3" class="ltx_tr">
<td id="S4.T4.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T4.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.4.3.1.1.1" class="ltx_p" style="width:64.0pt;">Method</span>
</span>
</td>
<td id="S4.T4.4.3.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">T-Data</td>
<td id="S4.T4.4.3.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">T-Scale</td>
<td id="S4.T4.4.3.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">Fine-tuned?</td>
<td id="S4.T4.4.3.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">OA</td>
</tr>
<tr id="S4.T4.4.4" class="ltx_tr">
<td id="S4.T4.4.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T4.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.4.4.1.1.1" class="ltx_p" style="width:64.0pt;">BUTD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite></span>
</span>
</td>
<td id="S4.T4.4.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">VQAv2</td>
<td id="S4.T4.4.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">443K</td>
<td id="S4.T4.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td id="S4.T4.4.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">65.3</td>
</tr>
<tr id="S4.T4.4.5" class="ltx_tr">
<td id="S4.T4.4.5.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T4.4.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.4.5.1.1.1" class="ltx_p" style="width:64.0pt;">VQA-E <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite></span>
</span>
</td>
<td id="S4.T4.4.5.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">VQAv2</td>
<td id="S4.T4.4.5.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">443K</td>
<td id="S4.T4.4.5.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td id="S4.T4.4.5.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">66.2</td>
</tr>
<tr id="S4.T4.4.6" class="ltx_tr">
<td id="S4.T4.4.6.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T4.4.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.4.6.1.1.1" class="ltx_p" style="width:64.0pt;">TRAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite></span>
</span>
</td>
<td id="S4.T4.4.6.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">VQAv2</td>
<td id="S4.T4.4.6.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">443K</td>
<td id="S4.T4.4.6.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td id="S4.T4.4.6.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">72.6</td>
</tr>
<tr id="S4.T4.3.1" class="ltx_tr">
<td id="S4.T4.3.1.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T4.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.3.1.2.1.1" class="ltx_p" style="width:64.0pt;">BLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite></span>
</span>
</td>
<td id="S4.T4.3.1.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">Multiple</td>
<td id="S4.T4.3.1.1" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">129M<sup id="S4.T4.3.1.1.1" class="ltx_sup">⋆</sup>
</td>
<td id="S4.T4.3.1.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S4.T4.3.1.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">78.3</td>
</tr>
<tr id="S4.T4.4.7" class="ltx_tr">
<td id="S4.T4.4.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T4.4.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.4.7.1.1.1" class="ltx_p" style="width:64.0pt;">VQA-E <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite></span>
</span>
</td>
<td id="S4.T4.4.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">VQA-E</td>
<td id="S4.T4.4.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">181K</td>
<td id="S4.T4.4.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td id="S4.T4.4.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">62.5</td>
</tr>
<tr id="S4.T4.4.8" class="ltx_tr">
<td id="S4.T4.4.8.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T4.4.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.4.8.1.1.1" class="ltx_p" style="width:64.0pt;">DMRFNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite></span>
</span>
</td>
<td id="S4.T4.4.8.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">VQA-E</td>
<td id="S4.T4.4.8.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">181K</td>
<td id="S4.T4.4.8.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td id="S4.T4.4.8.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">62.9</td>
</tr>
<tr id="S4.T4.4.2" class="ltx_tr">
<td id="S4.T4.4.2.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T4.4.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.4.2.2.1.1" class="ltx_p" style="width:64.0pt;">BLIP2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite></span>
</span>
</td>
<td id="S4.T4.4.2.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">Multiple</td>
<td id="S4.T4.4.2.1" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">129M<sup id="S4.T4.4.2.1.1" class="ltx_sup">⋆</sup>
</td>
<td id="S4.T4.4.2.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td id="S4.T4.4.2.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">65.0</td>
</tr>
<tr id="S4.T4.4.9" class="ltx_tr">
<td id="S4.T4.4.9.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T4.4.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.4.9.1.1.1" class="ltx_p" style="width:64.0pt;">CRVQA-v1</span>
</span>
</td>
<td id="S4.T4.4.9.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">VQA-E</td>
<td id="S4.T4.4.9.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">181K</td>
<td id="S4.T4.4.9.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td id="S4.T4.4.9.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">63.2</td>
</tr>
<tr id="S4.T4.4.10" class="ltx_tr">
<td id="S4.T4.4.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T4.4.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.4.10.1.1.1" class="ltx_p" style="width:64.0pt;">CRVQA-v3</span>
</span>
</td>
<td id="S4.T4.4.10.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">VQA-E</td>
<td id="S4.T4.4.10.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">181K</td>
<td id="S4.T4.4.10.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td id="S4.T4.4.10.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">64.7</td>
</tr>
</table>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Studies</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We conduct ablation studies to quantify the performance gain resulting from different components in our proposed method. We design several variants: Question or textual Rationale for GLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite> and Grounding-DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>; visual-textual Similarity score or User Study for the evaluation.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.1" class="ltx_p"><span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_bold">Question vs. Textual Rationale.</span> To analyze the effectiveness of the generated textual rationale as the text input for open-vocabulary detectors (GLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>, Grounding-DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>), we conduct experiments on VQA-R using either questions or textual rationales. Table <a href="#S4.T5" title="Table 5 ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the comparison results with average scores. We can observe that both detectors are struggling with question inputs that cannot offer complete and clear object information. Our generated textual rationales largely bridge the gap for pre-trained grounding models, resulting in a 13% performance improvement. In addition, Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows an example for the qualitative comparison. Although there are slight performance gaps between different detectors, they can achieve better results with clearer and richer text.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/q-glip.png" id="S4.F4.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F4.sf1.3.2" class="ltx_text" style="font-size:90%;">I + Q + GLIP</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/t-glip.png" id="S4.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F4.sf2.3.2" class="ltx_text" style="font-size:90%;">I + TR + GLIP</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F4.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/q-gdino.png" id="S4.F4.sf3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F4.sf3.3.2" class="ltx_text" style="font-size:90%;">I + Q + G-DINO</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F4.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/t-gdino.png" id="S4.F4.sf4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S4.F4.sf4.3.2" class="ltx_text" style="font-size:90%;">I + TR + G-DINO</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F4.sf5" class="ltx_figure ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf5.5.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S4.F4.sf5.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Question<span id="S4.F4.sf5.6.2.1" class="ltx_text ltx_font_medium">: What are the crowds doing?
<br class="ltx_break"></span>Answer<span id="S4.F4.sf5.6.2.2" class="ltx_text ltx_font_medium">: Playing frisbee.
<br class="ltx_break"></span>textual Rationale<span id="S4.F4.sf5.6.2.3" class="ltx_text ltx_font_medium">: A father plays frisbee with his two sons.</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Examples of visual rationales generated by different detectors (GLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>, G-DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>) with question (Q) or textual rationale (TR) as the text input. We use the red bounding boxes to better visually show the involved objects because of the green background of the image (I).</span></figcaption>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.2.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.3.2" class="ltx_text" style="font-size:90%;">Results on VQA-R for visual rationale accuracy (AP) with respect to different inputs for open-vocabulary detectors. We feed either “question” or “textual rationale” as the language query for detectors. G-DINO is short for Grounding-DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>.</span></figcaption>
<table id="S4.T5.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T5.4.1" class="ltx_tr">
<td id="S4.T5.4.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">Image</td>
<td id="S4.T5.4.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">Question</td>
<td id="S4.T5.4.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">Textual Rationale</td>
<td id="S4.T5.4.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">Detector</td>
<td id="S4.T5.4.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">AP</td>
</tr>
<tr id="S4.T5.4.2" class="ltx_tr">
<td id="S4.T5.4.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S4.T5.4.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S4.T5.4.2.3" class="ltx_td ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T5.4.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">GLIP</td>
<td id="S4.T5.4.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">32.06</td>
</tr>
<tr id="S4.T5.4.3" class="ltx_tr">
<td id="S4.T5.4.3.1" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S4.T5.4.3.2" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T5.4.3.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S4.T5.4.3.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">GLIP</td>
<td id="S4.T5.4.3.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">44.17</td>
</tr>
<tr id="S4.T5.4.4" class="ltx_tr">
<td id="S4.T5.4.4.1" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S4.T5.4.4.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S4.T5.4.4.3" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T5.4.4.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">G-DINO</td>
<td id="S4.T5.4.4.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">32.57</td>
</tr>
<tr id="S4.T5.4.5" class="ltx_tr">
<td id="S4.T5.4.5.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S4.T5.4.5.2" class="ltx_td ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S4.T5.4.5.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td id="S4.T5.4.5.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">G-DINO</td>
<td id="S4.T5.4.5.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T5.4.5.5.1" class="ltx_text ltx_font_bold">45.86</span></td>
</tr>
</table>
</figure>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F5.sf1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Question: Is the person typing?</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/example1-1.png" id="S4.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="402" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">Answer: No.
<br class="ltx_break">Textual Rationale: A man is looking at the camera.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/example1-2.png" id="S4.F5.sf3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="401" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F5.sf3.3.2" class="ltx_text" style="font-size:90%;">Answer: No.
<br class="ltx_break">Textual Rationale: A person is standing and pointing to a laptop</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/example1-3.png" id="S4.F5.sf4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="400" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S4.F5.sf4.3.2" class="ltx_text" style="font-size:90%;">Answer: No.
<br class="ltx_break">Textual Rationale: The young man is pointing to a laptop.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf5" class="ltx_figure ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf5.3.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S4.F5.sf5.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Question: What is the cat sitting on?</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/example2-1.png" id="S4.F5.sf6.g1" class="ltx_graphics ltx_img_landscape" width="598" height="400" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf6.2.1.1" class="ltx_text" style="font-size:90%;">(f)</span> </span><span id="S4.F5.sf6.3.2" class="ltx_text" style="font-size:90%;">Answer: Sink.
<br class="ltx_break">Textual Rationale: A cat that is sitting on a toilet seat next to a sink.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.sf7" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/example2-2.png" id="S4.F5.sf7.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf7.2.1.1" class="ltx_text" style="font-size:90%;">(g)</span> </span><span id="S4.F5.sf7.3.2" class="ltx_text" style="font-size:90%;">Answer: Sink.
<br class="ltx_break">Textual Rationale: A black cat sitting in the sink.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.sf8" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/example2-3.png" id="S4.F5.sf8.g1" class="ltx_graphics ltx_img_landscape" width="598" height="400" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf8.2.1.1" class="ltx_text" style="font-size:90%;">(h)</span> </span><span id="S4.F5.sf8.3.2" class="ltx_text" style="font-size:90%;">Answer: Sink.
<br class="ltx_break">Textual Rationale: A very cute cat sitting in a sink in the rest room.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.5.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.6.2" class="ltx_text" style="font-size:90%;">Qualitative results of our model (<span id="S4.F5.6.2.1" class="ltx_text ltx_font_italic">Middle</span>) compared to the ground truth (<span id="S4.F5.6.2.2" class="ltx_text ltx_font_italic">Right</span>) and DMRFNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite> (<span id="S4.F5.6.2.3" class="ltx_text ltx_font_italic">Left</span>) on VQA-R. The green bounding boxes show the visual rationales for the given image/question pair.</span></figcaption>
</figure>
<div id="S4.SS4.p3" class="ltx_para ltx_noindent">
<p id="S4.SS4.p3.1" class="ltx_p"><span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_bold">vtS Score vs. User Study.</span> To further evaluate the quality of the generated textual rationales and the confidence of our proposed metric (vtS), we conduct experiments to determine the consistency of the texts in explanations with the predicted answers. We randomly select 200 samples from VQA-R, and predict the answers, textual and visual rationales. In our user study, we employ three annotators to evaluate the quality of the texts across four aspects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> (<span id="S4.SS4.p3.1.2" class="ltx_text ltx_font_italic">Fluent</span>, <span id="S4.SS4.p3.1.3" class="ltx_text ltx_font_italic">Correct</span>, <span id="S4.SS4.p3.1.4" class="ltx_text ltx_font_italic">Relevant</span>, <span id="S4.SS4.p3.1.5" class="ltx_text ltx_font_italic">Complementary</span>), using a grading system that ranges from 1 to 5, where 1 represents “very poor” and 5 signifies “very good”. <span id="S4.SS4.p3.1.6" class="ltx_text ltx_font_italic">Fluent</span> indicates the fluency of the text in grammar, while <span id="S4.SS4.p3.1.7" class="ltx_text ltx_font_italic">Correct</span> represents whether the text is correct based on the image content. <span id="S4.SS4.p3.1.8" class="ltx_text ltx_font_italic">Relevant</span> measures the relevance of the text to the image/question pair, while <span id="S4.SS4.p3.1.9" class="ltx_text ltx_font_italic">Complementary</span> assesses whether the text makes the answering complete and easy to understand.
Note that we perform the same process for the ground truth, and calculate the vtS score as well to assess the quality of VQA-R. Table <a href="#S4.T6" title="Table 6 ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> reports the results, and we can observe that the quality of the textual rationales is good for human preference compared to the previous method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite>. The vtS scores are also consistent with the human evaluation results. Note that for the last row we feed the detector with ground truth textual rationales to generate the bounding-boxes, then we calculate AP by the synthesized ground-truth labels while setting the value of <math id="S4.SS4.p3.1.m1.2" class="ltx_Math" alttext="\cos{\rm(GTE)}" display="inline"><semantics id="S4.SS4.p3.1.m1.2a"><mrow id="S4.SS4.p3.1.m1.2.3.2" xref="S4.SS4.p3.1.m1.2.3.1.cmml"><mi id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml">cos</mi><mo id="S4.SS4.p3.1.m1.2.3.2a" xref="S4.SS4.p3.1.m1.2.3.1.cmml">⁡</mo><mrow id="S4.SS4.p3.1.m1.2.3.2.1" xref="S4.SS4.p3.1.m1.2.3.1.cmml"><mo stretchy="false" id="S4.SS4.p3.1.m1.2.3.2.1.1" xref="S4.SS4.p3.1.m1.2.3.1.cmml">(</mo><mi id="S4.SS4.p3.1.m1.2.2" xref="S4.SS4.p3.1.m1.2.2.cmml">GTE</mi><mo stretchy="false" id="S4.SS4.p3.1.m1.2.3.2.1.2" xref="S4.SS4.p3.1.m1.2.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.2b"><apply id="S4.SS4.p3.1.m1.2.3.1.cmml" xref="S4.SS4.p3.1.m1.2.3.2"><cos id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"></cos><ci id="S4.SS4.p3.1.m1.2.2.cmml" xref="S4.SS4.p3.1.m1.2.2">GTE</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.2c">\cos{\rm(GTE)}</annotation></semantics></math> to 1.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.5.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S4.T6.6.2" class="ltx_text" style="font-size:90%;">Comparison results between user study and the proposed visual-textual Similarity (vtS) score. Here shows the average performance of the 200 samples. Corre., Relev. and Compl. are abbreviated for <span id="S4.T6.6.2.1" class="ltx_text ltx_font_italic">Correct</span>, <span id="S4.T6.6.2.2" class="ltx_text ltx_font_italic">Relevant</span> and <span id="S4.T6.6.2.3" class="ltx_text ltx_font_italic">Complementary</span>, respectively.</span></figcaption>
<table id="S4.T6.7" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T6.7.1" class="ltx_tr">
<td id="S4.T6.7.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.T6.7.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.7.1.1.1.1" class="ltx_p" style="width:64.0pt;">Method</span>
</span>
</td>
<td id="S4.T6.7.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Fluent</td>
<td id="S4.T6.7.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Corre.</td>
<td id="S4.T6.7.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Relev.</td>
<td id="S4.T6.7.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Compl.</td>
<td id="S4.T6.7.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">vtS (%)</td>
</tr>
<tr id="S4.T6.7.2" class="ltx_tr">
<td id="S4.T6.7.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.T6.7.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.7.2.1.1.1" class="ltx_p" style="width:64.0pt;">DMRFNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite></span>
</span>
</td>
<td id="S4.T6.7.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">3.29</td>
<td id="S4.T6.7.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">3.33</td>
<td id="S4.T6.7.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">3.16</td>
<td id="S4.T6.7.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">3.09</td>
<td id="S4.T6.7.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">55.63</td>
</tr>
<tr id="S4.T6.7.3" class="ltx_tr">
<td id="S4.T6.7.3.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.T6.7.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.7.3.1.1.1" class="ltx_p" style="width:64.0pt;">CRVQA-v3</span>
</span>
</td>
<td id="S4.T6.7.3.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">3.56</td>
<td id="S4.T6.7.3.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">3.42</td>
<td id="S4.T6.7.3.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">3.34</td>
<td id="S4.T6.7.3.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">3.25</td>
<td id="S4.T6.7.3.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">58.87</td>
</tr>
<tr id="S4.T6.7.4" class="ltx_tr">
<td id="S4.T6.7.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.T6.7.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.7.4.1.1.1" class="ltx_p" style="width:64.0pt;">GT</span>
</span>
</td>
<td id="S4.T6.7.4.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">4.64</td>
<td id="S4.T6.7.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">4.44</td>
<td id="S4.T6.7.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">4.04</td>
<td id="S4.T6.7.4.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">3.93</td>
<td id="S4.T6.7.4.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">80.82</td>
</tr>
</table>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Qualitative Analysis</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">We conduct a qualitative analysis on the predicted answers and rationales. Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates two specific examples from VQA-R, comparing our proposed model with DMRFNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite>. For both image/question pairs, DMRFNet is able to predict the correct answers, but fails to generate the corresponding textual rationale and visual evidence.
In the bottom part, DMRFNet generates a wrong textual explanation (“sitting on a toilet seat”) but still predicts the correct answer. Our model can comprehend the given image/question pairs exactly and generate reasonable explanations for the answers.
For example, when our model correctly understands the image’s content and the query, it can discern the relationship between “the laptop” and “the person”. Although the visual evidence is not perfectly accurate (“hand” or “entire body”), the generated text indicates the process of the answering.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper we propose a VQA reasoning method (<span id="S5.p1.1.1" class="ltx_text ltx_font_bold">CRVQA</span>) that predicts not only the answer but the textual and visual rationales to support the answering. To reduce the efforts for annotating the visual labels, we synthesize a dataset for VQA reasoning with a matching procedure. We leverage a large language model to generate the textual rationale and an open-vocabulary detector to extract the visual evidence. We demonstrate the effectiveness of the proposed CRVQA on VQA reasoning.
Furthermore, we introduce a new metric to measure the quality of the generated textual explanations from the view of vision.
The experiments conducted on three VQA datasets demonstrate that our method achieves the superior performances compared to the previous approaches.
In addition, CRVQA generalizes well to the generic VQA task in the zero-shot evaluation setting.</p>
</div>
</section>
<section id="Ax1" class="ltx_appendix ltx_indentfirst">
<h2 class="ltx_title ltx_title_appendix">Appendix</h2>

<div id="Ax1.p1" class="ltx_para">
<p id="Ax1.p1.1" class="ltx_p">In this supplementary document, we provide detailed explanations on the synthesized VQA-R dataset in Sec. <a href="#A1" title="Appendix A Synthesized VQA-R Dataset ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>. Additional ablation studies in terms of the projection module, the introduced metric vtS, and frozen/fine-tuning language model are provided in Sec. <a href="#A2" title="Appendix B Additional Ablation Study ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>. Moreover, we also provide more qualitative results in Sec. <a href="#A3" title="Appendix C More Qualitative Results ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
</section>
<section id="A1" class="ltx_appendix ltx_indent_first">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Synthesized VQA-R Dataset</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">In this section, we analyze our synthesized VQA-R dataset, compared with the prior explanatory VQA datasets. We collect the question/answer/explanation triplets from the validation of VQA-E <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> and match them with the available visual labels followed a manual editing step as discussed in the main text. There are 33726 image/question pairs well annotated in total in VQA-R. More detailed statistics about the dataset
are given in Table <a href="#A1.T7" title="Table 7 ‣ Appendix A Synthesized VQA-R Dataset ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. We can see that the proposed VQA-R dataset supports the answering as well as the visual and textual explanations in accurate and sufficient bounding box annotations.</p>
</div>
<figure id="A1.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A1.T7.2.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="A1.T7.3.2" class="ltx_text" style="font-size:90%;">Statistics for the proposed VQA-R dataset. VizWiz-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> and VQA-Therapy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> collect the images from the same mobile phone platforms. We use “-” to represent the missing data from different datasets. VR and B-box are abbreviated for visual rationale and bounding-box.</span></figcaption>
<table id="A1.T7.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A1.T7.4.1" class="ltx_tr">
<td id="A1.T7.4.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T7.4.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.4.1.1.1.1" class="ltx_p" style="width:74.0pt;">Dataset</span>
</span>
</td>
<td id="A1.T7.4.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">Image Source</td>
<td id="A1.T7.4.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">#Image</td>
<td id="A1.T7.4.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">#Question/Answer</td>
<td id="A1.T7.4.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">#Textual Rationale</td>
<td id="A1.T7.4.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">#Visual Rationale</td>
<td id="A1.T7.4.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">VR Format</td>
</tr>
<tr id="A1.T7.4.2" class="ltx_tr">
<td id="A1.T7.4.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T7.4.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.4.2.1.1.1" class="ltx_p" style="width:74.0pt;">VQA-HAT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite></span>
</span>
</td>
<td id="A1.T7.4.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">COCO</td>
<td id="A1.T7.4.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">20,508</td>
<td id="A1.T7.4.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">59,457</td>
<td id="A1.T7.4.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="A1.T7.4.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">62,597</td>
<td id="A1.T7.4.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Region</td>
</tr>
<tr id="A1.T7.4.3" class="ltx_tr">
<td id="A1.T7.4.3.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T7.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.4.3.1.1.1" class="ltx_p" style="width:74.0pt;">VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite></span>
</span>
</td>
<td id="A1.T7.4.3.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">COCO</td>
<td id="A1.T7.4.3.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">204,721</td>
<td id="A1.T7.4.3.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">1,105,904</td>
<td id="A1.T7.4.3.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="A1.T7.4.3.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="A1.T7.4.3.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
<tr id="A1.T7.4.4" class="ltx_tr">
<td id="A1.T7.4.4.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T7.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.4.4.1.1.1" class="ltx_p" style="width:74.0pt;">VQA-E <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite></span>
</span>
</td>
<td id="A1.T7.4.4.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">COCO</td>
<td id="A1.T7.4.4.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">108,325</td>
<td id="A1.T7.4.4.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">269,786</td>
<td id="A1.T7.4.4.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">269786</td>
<td id="A1.T7.4.4.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="A1.T7.4.4.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
<tr id="A1.T7.4.5" class="ltx_tr">
<td id="A1.T7.4.5.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T7.4.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.4.5.1.1.1" class="ltx_p" style="width:74.0pt;">VQA-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite></span>
</span>
</td>
<td id="A1.T7.4.5.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">COCO</td>
<td id="A1.T7.4.5.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">28,180</td>
<td id="A1.T7.4.5.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">32,886</td>
<td id="A1.T7.4.5.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">41,817</td>
<td id="A1.T7.4.5.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">6,000</td>
<td id="A1.T7.4.5.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">Region</td>
</tr>
<tr id="A1.T7.4.6" class="ltx_tr">
<td id="A1.T7.4.6.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T7.4.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.4.6.1.1.1" class="ltx_p" style="width:74.0pt;">VizWiz-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite></span>
</span>
</td>
<td id="A1.T7.4.6.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">Phone</td>
<td id="A1.T7.4.6.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">9,998</td>
<td id="A1.T7.4.6.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">9,998</td>
<td id="A1.T7.4.6.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="A1.T7.4.6.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">9,998</td>
<td id="A1.T7.4.6.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">Boundary</td>
</tr>
<tr id="A1.T7.4.7" class="ltx_tr">
<td id="A1.T7.4.7.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T7.4.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.4.7.1.1.1" class="ltx_p" style="width:74.0pt;">VQA-Therapy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite></span>
</span>
</td>
<td id="A1.T7.4.7.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">Phone</td>
<td id="A1.T7.4.7.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">5,787</td>
<td id="A1.T7.4.7.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">5,825</td>
<td id="A1.T7.4.7.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td id="A1.T7.4.7.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">9,537</td>
<td id="A1.T7.4.7.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">Boundary</td>
</tr>
<tr id="A1.T7.4.8" class="ltx_tr">
<td id="A1.T7.4.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T7.4.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.4.8.1.1.1" class="ltx_p" style="width:74.0pt;">VQA-R</span>
</span>
</td>
<td id="A1.T7.4.8.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">COCO</td>
<td id="A1.T7.4.8.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">20,367</td>
<td id="A1.T7.4.8.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">33,726</td>
<td id="A1.T7.4.8.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">33,726</td>
<td id="A1.T7.4.8.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">93,377</td>
<td id="A1.T7.4.8.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">B-box</td>
</tr>
</table>
</figure>
<figure id="A1.T8" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A1.T8.2.1.1" class="ltx_text" style="font-size:90%;">Table 8</span>: </span><span id="A1.T8.3.2" class="ltx_text" style="font-size:90%;">Results on VQA-R for ablation study on the linear and Transformer projection module. B, M, R, C, S, and vtS are abbreviated for BLEU-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>, METEOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>, ROUGE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>, CIDEr <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>, SPICE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>, and visual-textual Similarity score. All scores are reported in percentage (%).</span></figcaption>
<table id="A1.T8.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A1.T8.4.1" class="ltx_tr">
<td id="A1.T8.4.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A1.T8.4.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.4.1.1.1.1" class="ltx_p" style="width:49.8pt;">Method</span>
</span>
</td>
<td id="A1.T8.4.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">B</td>
<td id="A1.T8.4.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">M</td>
<td id="A1.T8.4.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">R</td>
<td id="A1.T8.4.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">C</td>
<td id="A1.T8.4.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">S</td>
<td id="A1.T8.4.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">vtS</td>
</tr>
<tr id="A1.T8.4.2" class="ltx_tr">
<td id="A1.T8.4.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A1.T8.4.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.4.2.1.1.1" class="ltx_p" style="width:49.8pt;">Linear</span>
</span>
</td>
<td id="A1.T8.4.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">14.60</td>
<td id="A1.T8.4.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">21.04</td>
<td id="A1.T8.4.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">42.72</td>
<td id="A1.T8.4.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">98.84</td>
<td id="A1.T8.4.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">20.29</td>
<td id="A1.T8.4.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">55.85</td>
</tr>
<tr id="A1.T8.4.3" class="ltx_tr">
<td id="A1.T8.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A1.T8.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.4.3.1.1.1" class="ltx_p" style="width:49.8pt;">Transformer</span>
</span>
</td>
<td id="A1.T8.4.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">15.84</td>
<td id="A1.T8.4.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">22.41</td>
<td id="A1.T8.4.3.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">43.57</td>
<td id="A1.T8.4.3.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">105.31</td>
<td id="A1.T8.4.3.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">22.19</td>
<td id="A1.T8.4.3.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">58.19</td>
</tr>
</table>
</figure>
<figure id="A1.T9" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A1.T9.4.1.1" class="ltx_text" style="font-size:90%;">Table 9</span>: </span><span id="A1.T9.5.2" class="ltx_text" style="font-size:90%;">Results on VQA-R for ablation study on the different combination of the visual and textual aspects. (1), (2), and (3) are detailed in Sec. <a href="#A2" title="Appendix B Additional Ablation Study ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</span></figcaption>
<table id="A1.T9.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A1.T9.2.2" class="ltx_tr">
<td id="A1.T9.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T9.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.2.2.3.1.1" class="ltx_p" style="width:66.9pt;">Method</span>
</span>
</td>
<td id="A1.T9.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="A1.T9.1.1.1.m1.1" class="ltx_Math" alttext="AP" display="inline"><semantics id="A1.T9.1.1.1.m1.1a"><mrow id="A1.T9.1.1.1.m1.1.1" xref="A1.T9.1.1.1.m1.1.1.cmml"><mi id="A1.T9.1.1.1.m1.1.1.2" xref="A1.T9.1.1.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="A1.T9.1.1.1.m1.1.1.1" xref="A1.T9.1.1.1.m1.1.1.1.cmml">​</mo><mi id="A1.T9.1.1.1.m1.1.1.3" xref="A1.T9.1.1.1.m1.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.T9.1.1.1.m1.1b"><apply id="A1.T9.1.1.1.m1.1.1.cmml" xref="A1.T9.1.1.1.m1.1.1"><times id="A1.T9.1.1.1.m1.1.1.1.cmml" xref="A1.T9.1.1.1.m1.1.1.1"></times><ci id="A1.T9.1.1.1.m1.1.1.2.cmml" xref="A1.T9.1.1.1.m1.1.1.2">𝐴</ci><ci id="A1.T9.1.1.1.m1.1.1.3.cmml" xref="A1.T9.1.1.1.m1.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.1.1.1.m1.1c">AP</annotation></semantics></math></td>
<td id="A1.T9.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="A1.T9.2.2.2.m1.2" class="ltx_Math" alttext="\cos{(GTE)}" display="inline"><semantics id="A1.T9.2.2.2.m1.2a"><mrow id="A1.T9.2.2.2.m1.2.2.1" xref="A1.T9.2.2.2.m1.2.2.2.cmml"><mi id="A1.T9.2.2.2.m1.1.1" xref="A1.T9.2.2.2.m1.1.1.cmml">cos</mi><mo id="A1.T9.2.2.2.m1.2.2.1a" xref="A1.T9.2.2.2.m1.2.2.2.cmml">⁡</mo><mrow id="A1.T9.2.2.2.m1.2.2.1.1" xref="A1.T9.2.2.2.m1.2.2.2.cmml"><mo stretchy="false" id="A1.T9.2.2.2.m1.2.2.1.1.2" xref="A1.T9.2.2.2.m1.2.2.2.cmml">(</mo><mrow id="A1.T9.2.2.2.m1.2.2.1.1.1" xref="A1.T9.2.2.2.m1.2.2.1.1.1.cmml"><mi id="A1.T9.2.2.2.m1.2.2.1.1.1.2" xref="A1.T9.2.2.2.m1.2.2.1.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="A1.T9.2.2.2.m1.2.2.1.1.1.1" xref="A1.T9.2.2.2.m1.2.2.1.1.1.1.cmml">​</mo><mi id="A1.T9.2.2.2.m1.2.2.1.1.1.3" xref="A1.T9.2.2.2.m1.2.2.1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="A1.T9.2.2.2.m1.2.2.1.1.1.1a" xref="A1.T9.2.2.2.m1.2.2.1.1.1.1.cmml">​</mo><mi id="A1.T9.2.2.2.m1.2.2.1.1.1.4" xref="A1.T9.2.2.2.m1.2.2.1.1.1.4.cmml">E</mi></mrow><mo stretchy="false" id="A1.T9.2.2.2.m1.2.2.1.1.3" xref="A1.T9.2.2.2.m1.2.2.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.T9.2.2.2.m1.2b"><apply id="A1.T9.2.2.2.m1.2.2.2.cmml" xref="A1.T9.2.2.2.m1.2.2.1"><cos id="A1.T9.2.2.2.m1.1.1.cmml" xref="A1.T9.2.2.2.m1.1.1"></cos><apply id="A1.T9.2.2.2.m1.2.2.1.1.1.cmml" xref="A1.T9.2.2.2.m1.2.2.1.1.1"><times id="A1.T9.2.2.2.m1.2.2.1.1.1.1.cmml" xref="A1.T9.2.2.2.m1.2.2.1.1.1.1"></times><ci id="A1.T9.2.2.2.m1.2.2.1.1.1.2.cmml" xref="A1.T9.2.2.2.m1.2.2.1.1.1.2">𝐺</ci><ci id="A1.T9.2.2.2.m1.2.2.1.1.1.3.cmml" xref="A1.T9.2.2.2.m1.2.2.1.1.1.3">𝑇</ci><ci id="A1.T9.2.2.2.m1.2.2.1.1.1.4.cmml" xref="A1.T9.2.2.2.m1.2.2.1.1.1.4">𝐸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.2.2.2.m1.2c">\cos{(GTE)}</annotation></semantics></math></td>
<td id="A1.T9.2.2.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">(1)</td>
<td id="A1.T9.2.2.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">(2)</td>
<td id="A1.T9.2.2.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">(3)</td>
</tr>
<tr id="A1.T9.2.3" class="ltx_tr">
<td id="A1.T9.2.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T9.2.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.2.3.1.1.1" class="ltx_p" style="width:66.9pt;">PJ-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite></span>
</span>
</td>
<td id="A1.T9.2.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">38.43</td>
<td id="A1.T9.2.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">68.32</td>
<td id="A1.T9.2.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">53.38</td>
<td id="A1.T9.2.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">26.26</td>
<td id="A1.T9.2.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">49.19</td>
</tr>
<tr id="A1.T9.2.4" class="ltx_tr">
<td id="A1.T9.2.4.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T9.2.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.2.4.1.1.1" class="ltx_p" style="width:66.9pt;">VQA-E <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite></span>
</span>
</td>
<td id="A1.T9.2.4.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">40.65</td>
<td id="A1.T9.2.4.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">71.67</td>
<td id="A1.T9.2.4.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">56.16</td>
<td id="A1.T9.2.4.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">29.13</td>
<td id="A1.T9.2.4.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">51.88</td>
</tr>
<tr id="A1.T9.2.5" class="ltx_tr">
<td id="A1.T9.2.5.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T9.2.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.2.5.1.1.1" class="ltx_p" style="width:66.9pt;">FME <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite></span>
</span>
</td>
<td id="A1.T9.2.5.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">42.57</td>
<td id="A1.T9.2.5.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">73.16</td>
<td id="A1.T9.2.5.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">57.87</td>
<td id="A1.T9.2.5.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">31.14</td>
<td id="A1.T9.2.5.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">53.82</td>
</tr>
<tr id="A1.T9.2.6" class="ltx_tr">
<td id="A1.T9.2.6.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T9.2.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.2.6.1.1.1" class="ltx_p" style="width:66.9pt;">CCM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite></span>
</span>
</td>
<td id="A1.T9.2.6.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">43.08</td>
<td id="A1.T9.2.6.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">73.94</td>
<td id="A1.T9.2.6.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">58.46</td>
<td id="A1.T9.2.6.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">31.85</td>
<td id="A1.T9.2.6.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">54.44</td>
</tr>
<tr id="A1.T9.2.7" class="ltx_tr">
<td id="A1.T9.2.7.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T9.2.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.2.7.1.1.1" class="ltx_p" style="width:66.9pt;">DMRFNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite></span>
</span>
</td>
<td id="A1.T9.2.7.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">44.74</td>
<td id="A1.T9.2.7.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">75.06</td>
<td id="A1.T9.2.7.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">59.90</td>
<td id="A1.T9.2.7.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">33.58</td>
<td id="A1.T9.2.7.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">56.06</td>
</tr>
<tr id="A1.T9.2.8" class="ltx_tr">
<td id="A1.T9.2.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="A1.T9.2.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.2.8.1.1.1" class="ltx_p" style="width:66.9pt;">CRVQA-v3</span>
</span>
</td>
<td id="A1.T9.2.8.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="A1.T9.2.8.2.1" class="ltx_text ltx_font_bold">45.86</span></td>
<td id="A1.T9.2.8.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="A1.T9.2.8.3.1" class="ltx_text ltx_font_bold">79.57</span></td>
<td id="A1.T9.2.8.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="A1.T9.2.8.4.1" class="ltx_text ltx_font_bold">62.72</span></td>
<td id="A1.T9.2.8.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="A1.T9.2.8.5.1" class="ltx_text ltx_font_bold">36.49</span></td>
<td id="A1.T9.2.8.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="A1.T9.2.8.6.1" class="ltx_text ltx_font_bold">58.19</span></td>
</tr>
</table>
</figure>
<figure id="A1.T10" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A1.T10.2.1.1" class="ltx_text" style="font-size:90%;">Table 10</span>: </span><span id="A1.T10.3.2" class="ltx_text" style="font-size:90%;">Results on VQA-R for ablation study on the frozen and fine-tuning language models.</span></figcaption>
<table id="A1.T10.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A1.T10.4.1" class="ltx_tr">
<td id="A1.T10.4.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A1.T10.4.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T10.4.1.1.1.1" class="ltx_p" style="width:49.8pt;">Method</span>
</span>
</td>
<td id="A1.T10.4.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">B</td>
<td id="A1.T10.4.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">M</td>
<td id="A1.T10.4.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">R</td>
<td id="A1.T10.4.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">C</td>
<td id="A1.T10.4.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">S</td>
<td id="A1.T10.4.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">vtS</td>
</tr>
<tr id="A1.T10.4.2" class="ltx_tr">
<td id="A1.T10.4.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A1.T10.4.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T10.4.2.1.1.1" class="ltx_p" style="width:49.8pt;">Frozen</span>
</span>
</td>
<td id="A1.T10.4.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">13.31</td>
<td id="A1.T10.4.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">19.63</td>
<td id="A1.T10.4.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">36.91</td>
<td id="A1.T10.4.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">89.25</td>
<td id="A1.T10.4.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">19.08</td>
<td id="A1.T10.4.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">53.07</td>
</tr>
<tr id="A1.T10.4.3" class="ltx_tr">
<td id="A1.T10.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A1.T10.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T10.4.3.1.1.1" class="ltx_p" style="width:49.8pt;">Fine-tuning</span>
</span>
</td>
<td id="A1.T10.4.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">15.84</td>
<td id="A1.T10.4.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">22.41</td>
<td id="A1.T10.4.3.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">43.57</td>
<td id="A1.T10.4.3.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">105.31</td>
<td id="A1.T10.4.3.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">22.19</td>
<td id="A1.T10.4.3.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">58.19</td>
</tr>
</table>
</figure>
</section>
<section id="A2" class="ltx_appendix ltx_indent_first">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Ablation Study</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">We further report three groups of ablation studies with respect to the projection, metric, and language model.</p>
</div>
<div id="A2.p2" class="ltx_para ltx_noindent">
<p id="A2.p2.1" class="ltx_p"><span id="A2.p2.1.1" class="ltx_text ltx_font_bold">Projection Module.</span> In the main text, we adopt a transformer projection module to project the features to the latent space of GPT-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite> model. To measure the effectiveness of the proposed Transformer blocks, we conduct an ablation study by comparing the results with that of a linear projection that the prior methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite> adopt. The results are reported in Table <a href="#A1.T8" title="Table 8 ‣ Appendix A Synthesized VQA-R Dataset ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. We observe that the proposed Transformer-based projection module can enhance the performance compared to the widely-used linear module on different metrics (e.g., 2.34% on the introduced vtS).</p>
</div>
<div id="A2.p3" class="ltx_para ltx_noindent">
<p id="A2.p3.4" class="ltx_p"><span id="A2.p3.4.1" class="ltx_text ltx_font_bold">Metrics.</span> To measure the generated textual rationale, we first introduce a metric called vtS (discussed in the main paper). Here we further consider the different combination of visual (AP) and textual (GTE similarity) aspects.
Specifically, there are three options: (1) simply summarizing <math id="A2.p3.1.m1.1" class="ltx_Math" alttext="AP" display="inline"><semantics id="A2.p3.1.m1.1a"><mrow id="A2.p3.1.m1.1.1" xref="A2.p3.1.m1.1.1.cmml"><mi id="A2.p3.1.m1.1.1.2" xref="A2.p3.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="A2.p3.1.m1.1.1.1" xref="A2.p3.1.m1.1.1.1.cmml">​</mo><mi id="A2.p3.1.m1.1.1.3" xref="A2.p3.1.m1.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.p3.1.m1.1b"><apply id="A2.p3.1.m1.1.1.cmml" xref="A2.p3.1.m1.1.1"><times id="A2.p3.1.m1.1.1.1.cmml" xref="A2.p3.1.m1.1.1.1"></times><ci id="A2.p3.1.m1.1.1.2.cmml" xref="A2.p3.1.m1.1.1.2">𝐴</ci><ci id="A2.p3.1.m1.1.1.3.cmml" xref="A2.p3.1.m1.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.1.m1.1c">AP</annotation></semantics></math> and <math id="A2.p3.2.m2.2" class="ltx_Math" alttext="\cos{(GTE)}" display="inline"><semantics id="A2.p3.2.m2.2a"><mrow id="A2.p3.2.m2.2.2.1" xref="A2.p3.2.m2.2.2.2.cmml"><mi id="A2.p3.2.m2.1.1" xref="A2.p3.2.m2.1.1.cmml">cos</mi><mo id="A2.p3.2.m2.2.2.1a" xref="A2.p3.2.m2.2.2.2.cmml">⁡</mo><mrow id="A2.p3.2.m2.2.2.1.1" xref="A2.p3.2.m2.2.2.2.cmml"><mo stretchy="false" id="A2.p3.2.m2.2.2.1.1.2" xref="A2.p3.2.m2.2.2.2.cmml">(</mo><mrow id="A2.p3.2.m2.2.2.1.1.1" xref="A2.p3.2.m2.2.2.1.1.1.cmml"><mi id="A2.p3.2.m2.2.2.1.1.1.2" xref="A2.p3.2.m2.2.2.1.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="A2.p3.2.m2.2.2.1.1.1.1" xref="A2.p3.2.m2.2.2.1.1.1.1.cmml">​</mo><mi id="A2.p3.2.m2.2.2.1.1.1.3" xref="A2.p3.2.m2.2.2.1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="A2.p3.2.m2.2.2.1.1.1.1a" xref="A2.p3.2.m2.2.2.1.1.1.1.cmml">​</mo><mi id="A2.p3.2.m2.2.2.1.1.1.4" xref="A2.p3.2.m2.2.2.1.1.1.4.cmml">E</mi></mrow><mo stretchy="false" id="A2.p3.2.m2.2.2.1.1.3" xref="A2.p3.2.m2.2.2.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p3.2.m2.2b"><apply id="A2.p3.2.m2.2.2.2.cmml" xref="A2.p3.2.m2.2.2.1"><cos id="A2.p3.2.m2.1.1.cmml" xref="A2.p3.2.m2.1.1"></cos><apply id="A2.p3.2.m2.2.2.1.1.1.cmml" xref="A2.p3.2.m2.2.2.1.1.1"><times id="A2.p3.2.m2.2.2.1.1.1.1.cmml" xref="A2.p3.2.m2.2.2.1.1.1.1"></times><ci id="A2.p3.2.m2.2.2.1.1.1.2.cmml" xref="A2.p3.2.m2.2.2.1.1.1.2">𝐺</ci><ci id="A2.p3.2.m2.2.2.1.1.1.3.cmml" xref="A2.p3.2.m2.2.2.1.1.1.3">𝑇</ci><ci id="A2.p3.2.m2.2.2.1.1.1.4.cmml" xref="A2.p3.2.m2.2.2.1.1.1.4">𝐸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.2.m2.2c">\cos{(GTE)}</annotation></semantics></math> then divided by two; (2) multiplying <math id="A2.p3.3.m3.1" class="ltx_Math" alttext="AP" display="inline"><semantics id="A2.p3.3.m3.1a"><mrow id="A2.p3.3.m3.1.1" xref="A2.p3.3.m3.1.1.cmml"><mi id="A2.p3.3.m3.1.1.2" xref="A2.p3.3.m3.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="A2.p3.3.m3.1.1.1" xref="A2.p3.3.m3.1.1.1.cmml">​</mo><mi id="A2.p3.3.m3.1.1.3" xref="A2.p3.3.m3.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.p3.3.m3.1b"><apply id="A2.p3.3.m3.1.1.cmml" xref="A2.p3.3.m3.1.1"><times id="A2.p3.3.m3.1.1.1.cmml" xref="A2.p3.3.m3.1.1.1"></times><ci id="A2.p3.3.m3.1.1.2.cmml" xref="A2.p3.3.m3.1.1.2">𝐴</ci><ci id="A2.p3.3.m3.1.1.3.cmml" xref="A2.p3.3.m3.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.3.m3.1c">AP</annotation></semantics></math> and <math id="A2.p3.4.m4.2" class="ltx_Math" alttext="\cos{(GTE)}" display="inline"><semantics id="A2.p3.4.m4.2a"><mrow id="A2.p3.4.m4.2.2.1" xref="A2.p3.4.m4.2.2.2.cmml"><mi id="A2.p3.4.m4.1.1" xref="A2.p3.4.m4.1.1.cmml">cos</mi><mo id="A2.p3.4.m4.2.2.1a" xref="A2.p3.4.m4.2.2.2.cmml">⁡</mo><mrow id="A2.p3.4.m4.2.2.1.1" xref="A2.p3.4.m4.2.2.2.cmml"><mo stretchy="false" id="A2.p3.4.m4.2.2.1.1.2" xref="A2.p3.4.m4.2.2.2.cmml">(</mo><mrow id="A2.p3.4.m4.2.2.1.1.1" xref="A2.p3.4.m4.2.2.1.1.1.cmml"><mi id="A2.p3.4.m4.2.2.1.1.1.2" xref="A2.p3.4.m4.2.2.1.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="A2.p3.4.m4.2.2.1.1.1.1" xref="A2.p3.4.m4.2.2.1.1.1.1.cmml">​</mo><mi id="A2.p3.4.m4.2.2.1.1.1.3" xref="A2.p3.4.m4.2.2.1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="A2.p3.4.m4.2.2.1.1.1.1a" xref="A2.p3.4.m4.2.2.1.1.1.1.cmml">​</mo><mi id="A2.p3.4.m4.2.2.1.1.1.4" xref="A2.p3.4.m4.2.2.1.1.1.4.cmml">E</mi></mrow><mo stretchy="false" id="A2.p3.4.m4.2.2.1.1.3" xref="A2.p3.4.m4.2.2.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p3.4.m4.2b"><apply id="A2.p3.4.m4.2.2.2.cmml" xref="A2.p3.4.m4.2.2.1"><cos id="A2.p3.4.m4.1.1.cmml" xref="A2.p3.4.m4.1.1"></cos><apply id="A2.p3.4.m4.2.2.1.1.1.cmml" xref="A2.p3.4.m4.2.2.1.1.1"><times id="A2.p3.4.m4.2.2.1.1.1.1.cmml" xref="A2.p3.4.m4.2.2.1.1.1.1"></times><ci id="A2.p3.4.m4.2.2.1.1.1.2.cmml" xref="A2.p3.4.m4.2.2.1.1.1.2">𝐺</ci><ci id="A2.p3.4.m4.2.2.1.1.1.3.cmml" xref="A2.p3.4.m4.2.2.1.1.1.3">𝑇</ci><ci id="A2.p3.4.m4.2.2.1.1.1.4.cmml" xref="A2.p3.4.m4.2.2.1.1.1.4">𝐸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.4.m4.2c">\cos{(GTE)}</annotation></semantics></math>; (3) dividing the result from (2) with that from (1), and we name it vtS in this paper.
We report the results in Table <a href="#A1.T9" title="Table 9 ‣ Appendix A Synthesized VQA-R Dataset ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, and we observe that either (1) or (2) cannot sufficiently reflects the real performance of both visual and textual branches if the two items differ too much.
Therefore, we adopt a the combination format in (3) as we introduced in the main paper.</p>
</div>
<div id="A2.p4" class="ltx_para ltx_noindent">
<p id="A2.p4.1" class="ltx_p"><span id="A2.p4.1.1" class="ltx_text ltx_font_bold">Language Model.</span> We train the proposed model with learnable parameters of the projection module to project the pre-trained CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> features to the space of GPT-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>. To further figure out the role of the large language model GPT-2, we design an ablation study with or without fine-tuning its parameters during the training stage. The results are reported in Table <a href="#A1.T10" title="Table 10 ‣ Appendix A Synthesized VQA-R Dataset ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> in terms of various evaluation metrics. Although it takes more effort to train the language predictor on VQA-E <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>, the quality of the generated textual rationales is improved.
For instance, the performance gain on VQA-R for CIDEr <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite> score is 16.06%. Similarly, fine-tuning the language model supports better textual information for extracting the visual evidence, as shown by the vtS score improvement (5.12%).</p>
</div>
</section>
<section id="A3" class="ltx_appendix ltx_indent_first">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>More Qualitative Results</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">We also provide more qualitative results of the proposed CRVQA on VQA-R.
Fig. <a href="#A3.F6" title="Figure 6 ‣ Appendix C More Qualitative Results ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows some common cases. These examples verify that CRVQA not only supports the answering but also provides robust and reliable visual and textual rationales to explain the prediction process. For instance, even there is no category-level information present in the first question, the proposed method still predicts the correct answer next to a textual explanation and visual bounding-boxes (especially for “tennis racket”). We also show a failure case (the last example in Fig. <a href="#A3.F6" title="Figure 6 ‣ Appendix C More Qualitative Results ‣ Convincing Rationales for Visual Question Answering Reasoning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). Our method predicts a wrong answer “car”, but it provides the corresponding explanations for the users to make a further judgement whether the answer is correct. This case also highlights the significance of the reliability of a VQA model in contrast to a “black box” answering system.</p>
</div>
<figure id="A3.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A3.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/sm-3-1.png" id="A3.F6.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="398" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A3.F6.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="A3.F6.sf1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Question: What is under his arm?
<br class="ltx_break"><span id="A3.F6.sf1.4.2.1" class="ltx_text ltx_font_medium">Answer: Tennis racket.
<br class="ltx_break">Textual Rationale: A man who is standing with a tennis racket.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A3.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/sm-3-2.png" id="A3.F6.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="398" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A3.F6.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span>
<br class="ltx_break"><span id="A3.F6.sf2.3.2" class="ltx_text" style="font-size:90%;">Answer: Tennis racket.
<br class="ltx_break">Textual Rationale: There is a man wearing a hat holding a tennis racket.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A3.F6.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/sm-4-1.png" id="A3.F6.sf3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="400" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A3.F6.sf3.3.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="A3.F6.sf3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Question: How many people?
<br class="ltx_break"><span id="A3.F6.sf3.4.2.1" class="ltx_text ltx_font_medium">Answer: 2.
<br class="ltx_break">Textual Rationale: Two people sitting there.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A3.F6.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/sm-4-2.png" id="A3.F6.sf4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="400" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A3.F6.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span>
<br class="ltx_break"><span id="A3.F6.sf4.3.2" class="ltx_text" style="font-size:90%;">Answer: 2.
<br class="ltx_break">Textual Rationale: Two people sitting on a wooden bench looking at the camera.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A3.F6.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/sm-5-1.png" id="A3.F6.sf5.g1" class="ltx_graphics ltx_img_portrait" width="598" height="836" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A3.F6.sf5.3.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="A3.F6.sf5.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Question: What is the man doing?
<br class="ltx_break"><span id="A3.F6.sf5.4.2.1" class="ltx_text ltx_font_medium">Answer: Skateboarding.
<br class="ltx_break">Textual Rationale: A man is riding a skateboard on a boardwalk in a skate park.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A3.F6.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/sm-5-2.png" id="A3.F6.sf6.g1" class="ltx_graphics ltx_img_portrait" width="598" height="838" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A3.F6.sf6.2.1.1" class="ltx_text" style="font-size:90%;">(f)</span> </span>
<br class="ltx_break"><span id="A3.F6.sf6.3.2" class="ltx_text" style="font-size:90%;">Answer: Skateboarding.
<br class="ltx_break">Textual Rationale: A skateboarder is doing stunts on a skateboard outdoors.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A3.F6.sf7" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/sm-6-1.png" id="A3.F6.sf7.g1" class="ltx_graphics ltx_img_portrait" width="598" height="837" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A3.F6.sf7.3.1.1" class="ltx_text" style="font-size:90%;">(g)</span> </span><span id="A3.F6.sf7.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Question: What is the bird standing on?
<br class="ltx_break"><span id="A3.F6.sf7.4.2.1" class="ltx_text ltx_font_medium">Answer: Car.
<br class="ltx_break">Textual Rationale: A bird is standing on a car door.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A3.F6.sf8" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.03896/assets/figures/sm-6-2.png" id="A3.F6.sf8.g1" class="ltx_graphics ltx_img_portrait" width="598" height="835" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A3.F6.sf8.2.1.1" class="ltx_text" style="font-size:90%;">(h)</span> </span>
<br class="ltx_break">
<br class="ltx_break"><span id="A3.F6.sf8.3.2" class="ltx_text" style="font-size:90%;">Answer: Mirror.
<br class="ltx_break">Textual Rationale: A bird standing on the side mirror of a vehicle.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F6.4.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="A3.F6.5.2" class="ltx_text" style="font-size:90%;">Additional qualitative results of our CRVQA model (<span id="A3.F6.5.2.1" class="ltx_text ltx_font_italic">Left</span>) compared to the ground truth (<span id="A3.F6.5.2.2" class="ltx_text ltx_font_italic">Right</span>) on VQA-R. The green bounding boxes show the visual rationales for the given image/question pair. The last one shows a failure case that predicts a wrong answer.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Anderson et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Spice: Semantic propositional image caption evaluation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib1.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</em><span id="bib.bib1.11.3" class="ltx_text" style="font-size:90%;">, pages 382–398, 2016.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Anderson et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">Bottom-up and top-down attention for image captioning and visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib2.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span id="bib.bib2.11.3" class="ltx_text" style="font-size:90%;">, pages 6077–6086, 2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Antol et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">Vqa: Visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib3.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer vision</em><span id="bib.bib3.11.3" class="ltx_text" style="font-size:90%;">, pages 2425–2433, 2015.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.4.4.1" class="ltx_text" style="font-size:90%;">Banerjee and Lavie [2005]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.6.1" class="ltx_text" style="font-size:90%;">
Satanjeev Banerjee and Alon Lavie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib4.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</em><span id="bib.bib4.10.3" class="ltx_text" style="font-size:90%;">, pages 65–72, 2005.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Ben-Younes et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Hedi Ben-Younes, Rémi Cadene, Matthieu Cord, and Nicolas Thome.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Mutan: Multimodal tucker fusion for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib5.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer vision</em><span id="bib.bib5.11.3" class="ltx_text" style="font-size:90%;">, pages 2612–2620, 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Chongyan Chen, Samreen Anjum, and Danna Gurari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">Vqa therapy: Exploring answer differences by visually grounding answers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib6.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib6.11.3" class="ltx_text" style="font-size:90%;">, pages 15315–15325, 2023.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.4.4.1" class="ltx_text" style="font-size:90%;">Chen and Zhao [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.6.1" class="ltx_text" style="font-size:90%;">
Shi Chen and Qi Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">Rex: Reasoning-aware and grounded explanation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib7.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib7.10.3" class="ltx_text" style="font-size:90%;">, pages 15586–15595, 2022.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Das et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Abhishek Das, Harsh Agrawal, Larry Zitnick, Devi Parikh, and Dhruv Batra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">Human attention in visual question answering: Do humans and deep networks look at the same regions?
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib8.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision and Image Understanding</em><span id="bib.bib8.10.2" class="ltx_text" style="font-size:90%;">, 163:90–100, 2017.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Deng et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Chaorui Deng, Qi Wu, Qingyao Wu, Fuyuan Hu, Fan Lyu, and Mingkui Tan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">Visual grounding via accumulated attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib9.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span id="bib.bib9.11.3" class="ltx_text" style="font-size:90%;">, pages 7746–7755, 2018.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Devlin et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">Bert: Pre-training of deep bidirectional transformers for language understanding.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib10.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1810.04805</em><span id="bib.bib10.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Dosovitskiy et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at scale.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib11.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2010.11929</em><span id="bib.bib11.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Du et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Ye Du, Zehua Fu, Qingjie Liu, and Yunhong Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Visual grounding with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib12.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2022 IEEE International Conference on Multimedia and Expo (ICME)</em><span id="bib.bib12.11.3" class="ltx_text" style="font-size:90%;">, pages 1–6. IEEE, 2022.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Everingham et al. [2010]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">The pascal visual object classes (voc) challenge.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib13.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International journal of computer vision</em><span id="bib.bib13.10.2" class="ltx_text" style="font-size:90%;">, 88:303–338, 2010.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text" style="font-size:90%;">Fukui et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">Multimodal compact bilinear pooling for visual question answering and visual grounding.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib14.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1606.01847</em><span id="bib.bib14.10.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">Gao et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
Peng Gao, Zhengkai Jiang, Haoxuan You, Pan Lu, Steven CH Hoi, Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">Dynamic fusion with intra-and inter-modality attention flow for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib15.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span id="bib.bib15.11.3" class="ltx_text" style="font-size:90%;">, pages 6639–6648, 2019.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Goyal et al. [2017a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">Making the v in vqa matter: Elevating the role of image understanding in visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib16.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span id="bib.bib16.11.3" class="ltx_text" style="font-size:90%;">, pages 6904–6913, 2017a.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Goyal et al. [2017b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Making the v in vqa matter: Elevating the role of image understanding in visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib17.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span id="bib.bib17.11.3" class="ltx_text" style="font-size:90%;">, pages 6904–6913, 2017b.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">Gurari et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">Vizwiz grand challenge: Answering visual questions from blind people.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib18.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span id="bib.bib18.11.3" class="ltx_text" style="font-size:90%;">, pages 3608–3617, 2018.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Hendricks et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Generating visual explanations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib19.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</em><span id="bib.bib19.11.3" class="ltx_text" style="font-size:90%;">, pages 3–19, 2016.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Hong et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
Richang Hong, Daqing Liu, Xiaoyu Mo, Xiangnan He, and Hanwang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">Learning to compose and reason with language tree structures for visual grounding.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib20.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</em><span id="bib.bib20.10.2" class="ltx_text" style="font-size:90%;">, 44(2):684–696, 2019.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.4.4.1" class="ltx_text" style="font-size:90%;">Hu and Singh [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.6.1" class="ltx_text" style="font-size:90%;">
Ronghang Hu and Amanpreet Singh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">Unit: Multimodal multitask learning with a unified transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib21.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib21.10.3" class="ltx_text" style="font-size:90%;">, pages 1439–1449, 2021.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Jiang et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-Miller, and Xinlei Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">In defense of grid features for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib22.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span id="bib.bib22.11.3" class="ltx_text" style="font-size:90%;">, pages 10267–10276, 2020.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.4.4.1" class="ltx_text" style="font-size:90%;">Kafle and Kanan [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.6.1" class="ltx_text" style="font-size:90%;">
Kushal Kafle and Christopher Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">Answer-type prediction for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib23.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span id="bib.bib23.10.3" class="ltx_text" style="font-size:90%;">, pages 4976–4984, 2016.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Kim et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Bilinear attention networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib24.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib24.10.2" class="ltx_text" style="font-size:90%;">, 31, 2018.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.4.4.1" class="ltx_text" style="font-size:90%;">Kingma and Ba [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.6.1" class="ltx_text" style="font-size:90%;">
Diederik P Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib25.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1412.6980</em><span id="bib.bib25.9.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Lai et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Lisa: Reasoning segmentation via large language model.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib26.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2308.00692</em><span id="bib.bib26.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.4.4.1" class="ltx_text" style="font-size:90%;">Li [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.6.1" class="ltx_text" style="font-size:90%;">
Chunyuan Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">Large multimodal models: Notes on cvpr 2023 tutorial.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib27.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.14895</em><span id="bib.bib27.9.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2022a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, and Steven CH Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">Align and prompt: Video-and-language pre-training with entity prompts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib28.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib28.11.3" class="ltx_text" style="font-size:90%;">, pages 4953–4963, 2022a.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2022b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib29.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib29.11.3" class="ltx_text" style="font-size:90%;">, pages 12888–12900, 2022b.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib30.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2301.12597</em><span id="bib.bib30.10.2" class="ltx_text" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2022c]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">
Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="font-size:90%;">Grounded language-image pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib31.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib31.11.3" class="ltx_text" style="font-size:90%;">, pages 10965–10975, 2022c.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">
Qing Li, Qingyi Tao, Shafiq Joty, Jianfei Cai, and Jiebo Luo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">Vqa-e: Explaining, elaborating, and enhancing your answers for visual questions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib32.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision (ECCV)</em><span id="bib.bib32.11.3" class="ltx_text" style="font-size:90%;">, pages 552–567, 2018.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">
Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">Towards general text embeddings with multi-stage contrastive learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib33.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2308.03281</em><span id="bib.bib33.10.2" class="ltx_text" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.4.4.1" class="ltx_text" style="font-size:90%;">Lin [2004]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.6.1" class="ltx_text" style="font-size:90%;">
Chin-Yew Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">Rouge: A package for automatic evaluation of summaries.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib34.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Text summarization branches out</em><span id="bib.bib34.10.3" class="ltx_text" style="font-size:90%;">, pages 74–81, 2004.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text" style="font-size:90%;">Lin et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib35.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em><span id="bib.bib35.11.3" class="ltx_text" style="font-size:90%;">, pages 740–755. Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">
Fenglin Liu, Yuanxin Liu, Xuancheng Ren, Xiaodong He, and Xu Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="font-size:90%;">Aligning visual regions and textual concepts for semantic-grounded image representations.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib36.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib36.10.2" class="ltx_text" style="font-size:90%;">, 32, 2019.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">Grounding dino: Marrying dino with grounded pre-training for open-set object detection.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib37.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.05499</em><span id="bib.bib37.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text" style="font-size:90%;">Lu et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib38.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib38.10.2" class="ltx_text" style="font-size:90%;">, 32, 2019.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.5.5.1" class="ltx_text" style="font-size:90%;">Marino et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">
Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, and Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">Krisp: Integrating implicit and symbolic knowledge for open-domain knowledge-based vqa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib39.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib39.11.3" class="ltx_text" style="font-size:90%;">, pages 14111–14121, 2021.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="font-size:90%;">Mokady et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">
Ron Mokady, Amir Hertz, and Amit H Bermano.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">Clipcap: Clip prefix for image captioning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib40.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2111.09734</em><span id="bib.bib40.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Papineni et al. [2002]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">Bleu: a method for automatic evaluation of machine translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib41.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</em><span id="bib.bib41.11.3" class="ltx_text" style="font-size:90%;">, pages 311–318, 2002.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Park et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, and Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">Multimodal explanations: Justifying decisions and pointing to the evidence.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib42.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span id="bib.bib42.11.3" class="ltx_text" style="font-size:90%;">, pages 8779–8788, 2018.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text" style="font-size:90%;">Patro et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">
Badri Patro, Shivansh Patel, and Vinay Namboodiri.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="font-size:90%;">Robust explanations for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib43.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em><span id="bib.bib43.11.3" class="ltx_text" style="font-size:90%;">, pages 1577–1586, 2020.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text" style="font-size:90%;">Pi et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">
Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, and Lingpeng Kong Tong Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text" style="font-size:90%;">Detgpt: Detect what you need via reasoning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib44.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.14167</em><span id="bib.bib44.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text" style="font-size:90%;">Radford et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="font-size:90%;">Language models are unsupervised multitask learners.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib45.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">OpenAI blog</em><span id="bib.bib45.10.2" class="ltx_text" style="font-size:90%;">, 1(8):9, 2019.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.5.5.1" class="ltx_text" style="font-size:90%;">Radford et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.8.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib46.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International conference on machine learning</em><span id="bib.bib46.11.3" class="ltx_text" style="font-size:90%;">, pages 8748–8763. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.4.4.1" class="ltx_text" style="font-size:90%;">Samek and Müller [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.6.1" class="ltx_text" style="font-size:90%;">
Wojciech Samek and Klaus-Robert Müller.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text" style="font-size:90%;">Towards explainable artificial intelligence.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib47.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Explainable AI: interpreting, explaining and visualizing deep learning</em><span id="bib.bib47.9.2" class="ltx_text" style="font-size:90%;">, pages 5–22, 2019.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text" style="font-size:90%;">Shen et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text" style="font-size:90%;">
Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text" style="font-size:90%;">How much can clip benefit vision-and-language tasks?
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib48.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2107.06383</em><span id="bib.bib48.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.5.5.1" class="ltx_text" style="font-size:90%;">Teney et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.7.1" class="ltx_text" style="font-size:90%;">
Damien Teney, Peter Anderson, Xiaodong He, and Anton Van Den Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.8.1" class="ltx_text" style="font-size:90%;">Tips and tricks for visual question answering: Learnings from the 2017 challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib49.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span id="bib.bib49.11.3" class="ltx_text" style="font-size:90%;">, pages 4223–4232, 2018.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.5.5.1" class="ltx_text" style="font-size:90%;">Vaswani et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.7.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.8.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib50.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib50.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.5.5.1" class="ltx_text" style="font-size:90%;">Vedantam et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.7.1" class="ltx_text" style="font-size:90%;">
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.8.1" class="ltx_text" style="font-size:90%;">Cider: Consensus-based image description evaluation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib51.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span id="bib.bib51.11.3" class="ltx_text" style="font-size:90%;">, pages 4566–4575, 2015.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.7.1" class="ltx_text" style="font-size:90%;">
Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.8.1" class="ltx_text" style="font-size:90%;">Simvlm: Simple visual language model pretraining with weak supervision.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib52.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2108.10904</em><span id="bib.bib52.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.4.4.1" class="ltx_text" style="font-size:90%;">Wu and Mooney [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.6.1" class="ltx_text" style="font-size:90%;">
Jialin Wu and Raymond Mooney.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.7.1" class="ltx_text" style="font-size:90%;">Faithful multimodal explanation for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib53.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</em><span id="bib.bib53.10.3" class="ltx_text" style="font-size:90%;">, pages 103–112, 2019.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.5.5.1" class="ltx_text" style="font-size:90%;">Xue et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.7.1" class="ltx_text" style="font-size:90%;">
Dizhan Xue, Shengsheng Qian, and Changsheng Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.8.1" class="ltx_text" style="font-size:90%;">Variational causal inference network for explanatory visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib54.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib54.11.3" class="ltx_text" style="font-size:90%;">, pages 2515–2525, 2023.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.7.1" class="ltx_text" style="font-size:90%;">
Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.8.1" class="ltx_text" style="font-size:90%;">Vision-language pre-training with triple contrastive learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib55.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib55.11.3" class="ltx_text" style="font-size:90%;">, pages 15671–15680, 2022.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.7.1" class="ltx_text" style="font-size:90%;">
Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, and Jiebo Luo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.8.1" class="ltx_text" style="font-size:90%;">A fast and accurate one-stage approach to visual grounding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib56.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib56.11.3" class="ltx_text" style="font-size:90%;">, pages 4683–4693, 2019.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.5.5.1" class="ltx_text" style="font-size:90%;">Yu et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.7.1" class="ltx_text" style="font-size:90%;">
Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.8.1" class="ltx_text" style="font-size:90%;">Deep modular co-attention networks for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib57.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span id="bib.bib57.11.3" class="ltx_text" style="font-size:90%;">, pages 6281–6290, 2019.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.7.1" class="ltx_text" style="font-size:90%;">
Weifeng Zhang, Jing Yu, Wenhong Zhao, and Chuan Ran.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.8.1" class="ltx_text" style="font-size:90%;">Dmrfnet: deep multimodal reasoning and fusion for visual question answering and explanation generation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib58.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Information Fusion</em><span id="bib.bib58.10.2" class="ltx_text" style="font-size:90%;">, 72:70–79, 2021.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.5.5.1" class="ltx_text" style="font-size:90%;">Zhou et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.7.1" class="ltx_text" style="font-size:90%;">
Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.8.1" class="ltx_text" style="font-size:90%;">End-to-end dense video captioning with masked transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib59.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span id="bib.bib59.11.3" class="ltx_text" style="font-size:90%;">, pages 8739–8748, 2018.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib60.5.5.1" class="ltx_text" style="font-size:90%;">Zhou et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib60.7.1" class="ltx_text" style="font-size:90%;">
Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.8.1" class="ltx_text" style="font-size:90%;">Unified vision-language pre-training for image captioning and vqa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib60.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI conference on artificial intelligence</em><span id="bib.bib60.11.3" class="ltx_text" style="font-size:90%;">, pages 13041–13049, 2020.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib61.5.5.1" class="ltx_text" style="font-size:90%;">Zhou et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib61.7.1" class="ltx_text" style="font-size:90%;">
Yiyi Zhou, Tianhe Ren, Chaoyang Zhu, Xiaoshuai Sun, Jianzhuang Liu, Xinghao Ding, Mingliang Xu, and Rongrong Ji.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.8.1" class="ltx_text" style="font-size:90%;">Trar: Routing the attention spans in transformer for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib61.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib61.11.3" class="ltx_text" style="font-size:90%;">, pages 2074–2084, 2021.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2402.03895" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2402.03896" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2402.03896">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.03896" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2402.03897" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 17:49:06 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
