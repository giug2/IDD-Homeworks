<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.15325] LXMERT Model Compression for Visual Question Answering</title><meta property="og:description" content="Large-scale pretrained models such as LXMERT are becoming popular for learning cross-modal representations on text-image pairs for vision-language tasks. According to the lottery ticket hypothesis, NLP and computer vis…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LXMERT Model Compression for Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="LXMERT Model Compression for Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.15325">

<!--Generated on Tue Feb 27 21:12:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">LXMERT Model Compression for Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Maryam Hashemi    Ghazaleh Mahmoudi <sup id="id4.4.id1" class="ltx_sup">∗</sup>    Sara Kodeiri<sup id="id5.5.id2" class="ltx_sup">∗</sup>    Hadi Sheikhi<sup id="id6.6.id3" class="ltx_sup">∗</sup>   Sauleh Eetemadi 
<br class="ltx_break">School of Computer Engineering,
Iran University of Science and Technology, Iran 
<br class="ltx_break"><span id="id7.7.id4" class="ltx_text ltx_font_typewriter">{m_hashemi94, gh_mahmoodi, sara_kodeiri, ha_sheikhi}@comp.iust.ac.ir, sauleh@iust.ac.ir </span>
</span><span class="ltx_author_notes">  These authors contributed equally.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">Large-scale pretrained models such as LXMERT are becoming popular for learning cross-modal representations on text-image pairs for vision-language tasks. According to the lottery ticket hypothesis, NLP and computer vision models contain smaller subnetworks capable of being trained in isolation to full performance. In this paper, we combine these observations to evaluate whether such trainable subnetworks exist in LXMERT when fine-tuned on the VQA task. In addition, we perform a model size cost-benefit analysis by investigating how much pruning can be done without significant loss in accuracy. Our experiment results demonstrate that LXMERT can be effectively pruned by 40%-60% in size with 3% loss in accuracy.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction and Related Work</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Over the past few years, many single-modal pretrained models have been proposed. Inspired by this, the vision-and-language pretraining seeks to learn joint representations using visual and textual content to improve the efficiency of vision-language tasks.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Both single-modality and cross-modality pretrained models often have hundreds of millions of parameters. Unfortunately, training these overparametrized models can be prohibitively time-consuming and costly, making them impractical for resource-limited devices. However, cross-modality pretrained models suffer more from the increased model size due to the higher input space dimension. With the task of Visual Question Answering (VQA) <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib1" title="" class="ltx_ref">2015</a>)</cite> in mind, and its ultimate goal of being helpful to the visually impaired, decreasing V+L model size makes it feasible to use them in limited-resource devices.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address this problem, model compression techniques such as pruning have been developed. Deep Learning recently enjoyed welcoming a new powerful pruning method: The Lottery Ticket Hypothesis (LTH) <cite class="ltx_cite ltx_citemacro_cite">Frankle and Carbin (<a href="#bib.bib4" title="" class="ltx_ref">2019</a>)</cite>. LTH has been shown great success in various fields. It could be a powerful tool to understand the parameter redundancy in the current pretrained V+L models. Thus, we aim to apply LTH to LXMERT<cite class="ltx_cite ltx_citemacro_cite">Tan and Bansal (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite>, one of the best-performing two-stream V+L models, to fill this gap. We evaluate our work on VQA <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib1" title="" class="ltx_ref">2015</a>)</cite> and compare it with DistillVLM<cite class="ltx_cite ltx_citemacro_cite">Fang et al. (<a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite>, which leverages the knowledge distillation technique to compress large visual-linguistic models.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Similar to this work, <cite class="ltx_cite ltx_citemacro_citet">Gan et al. (<a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite> study LTH for UNITER<cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>. However, UNITER is a single stream V+L model, and LXMERT is a two-stream model; our results are consistent with theirs.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we briefly explain the LXMERT architecture and LTH. Then, we describe how we use LTH to compress the pretrained LXMERT model.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<div id="S2.T1.24" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:455.2pt;height:83.9pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-117.2pt,21.4pt) scale(0.660171388152694,0.660171388152694) ;">
<table id="S2.T1.24.24" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.24.24.25.1" class="ltx_tr">
<th id="S2.T1.24.24.25.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" rowspan="2"><span id="S2.T1.24.24.25.1.1.1" class="ltx_text">Method</span></th>
<th id="S2.T1.24.24.25.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="4">test-dev</th>
<th id="S2.T1.24.24.25.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_t"></th>
<th id="S2.T1.24.24.25.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="4">test-std</th>
</tr>
<tr id="S2.T1.24.24.26.2" class="ltx_tr">
<th id="S2.T1.24.24.26.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Yes/No</th>
<th id="S2.T1.24.24.26.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Number</th>
<th id="S2.T1.24.24.26.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">other</th>
<th id="S2.T1.24.24.26.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Overall</th>
<th id="S2.T1.24.24.26.2.5" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S2.T1.24.24.26.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Yes/No</th>
<th id="S2.T1.24.24.26.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Number</th>
<th id="S2.T1.24.24.26.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">other</th>
<th id="S2.T1.24.24.26.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Overall</th>
</tr>
<tr id="S2.T1.24.24.27.3" class="ltx_tr">
<th id="S2.T1.24.24.27.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">DistillVLM</th>
<th id="S2.T1.24.24.27.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">-</th>
<th id="S2.T1.24.24.27.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">-</th>
<th id="S2.T1.24.24.27.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">-</th>
<th id="S2.T1.24.24.27.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">69.6</th>
<th id="S2.T1.24.24.27.3.6" class="ltx_td ltx_th ltx_th_column ltx_border_t"></th>
<th id="S2.T1.24.24.27.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">-</th>
<th id="S2.T1.24.24.27.3.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">-</th>
<th id="S2.T1.24.24.27.3.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">-</th>
<th id="S2.T1.24.24.27.3.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">69.8</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.24.24.28.1" class="ltx_tr">
<th id="S2.T1.24.24.28.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LXMERT</th>
<td id="S2.T1.24.24.28.1.2" class="ltx_td ltx_align_center ltx_border_t">88.24</td>
<td id="S2.T1.24.24.28.1.3" class="ltx_td ltx_align_center ltx_border_t">54.45</td>
<td id="S2.T1.24.24.28.1.4" class="ltx_td ltx_align_center ltx_border_t">63.05</td>
<td id="S2.T1.24.24.28.1.5" class="ltx_td ltx_align_center ltx_border_t">72.45</td>
<td id="S2.T1.24.24.28.1.6" class="ltx_td ltx_border_t"></td>
<td id="S2.T1.24.24.28.1.7" class="ltx_td ltx_align_center ltx_border_t">88.29</td>
<td id="S2.T1.24.24.28.1.8" class="ltx_td ltx_align_center ltx_border_t">54.37</td>
<td id="S2.T1.24.24.28.1.9" class="ltx_td ltx_align_center ltx_border_t">63.18</td>
<td id="S2.T1.24.24.28.1.10" class="ltx_td ltx_align_center ltx_border_t">72.63</td>
</tr>
<tr id="S2.T1.8.8.8" class="ltx_tr">
<th id="S2.T1.8.8.8.9" class="ltx_td ltx_align_left ltx_th ltx_th_row">LXMERT (low-magnitude)</th>
<td id="S2.T1.1.1.1.1" class="ltx_td ltx_align_center">86.95 <math id="S2.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.1.1.1.1.m1.1a"><mo id="S2.T1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">\pm</annotation></semantics></math> 0.95</td>
<td id="S2.T1.2.2.2.2" class="ltx_td ltx_align_center">52.60 <math id="S2.T1.2.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.2.2.2.2.m1.1a"><mo id="S2.T1.2.2.2.2.m1.1.1" xref="S2.T1.2.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S2.T1.2.2.2.2.m1.1.1.cmml" xref="S2.T1.2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.2.m1.1c">\pm</annotation></semantics></math> 1.87</td>
<td id="S2.T1.3.3.3.3" class="ltx_td ltx_align_center">60.96 <math id="S2.T1.3.3.3.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.3.3.3.3.m1.1a"><mo id="S2.T1.3.3.3.3.m1.1.1" xref="S2.T1.3.3.3.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.3.m1.1b"><csymbol cd="latexml" id="S2.T1.3.3.3.3.m1.1.1.cmml" xref="S2.T1.3.3.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.3.m1.1c">\pm</annotation></semantics></math> 1.76</td>
<td id="S2.T1.4.4.4.4" class="ltx_td ltx_align_center">70.72 <math id="S2.T1.4.4.4.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.4.4.4.4.m1.1a"><mo id="S2.T1.4.4.4.4.m1.1.1" xref="S2.T1.4.4.4.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.4.4.m1.1b"><csymbol cd="latexml" id="S2.T1.4.4.4.4.m1.1.1.cmml" xref="S2.T1.4.4.4.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.4.4.m1.1c">\pm</annotation></semantics></math> 1.44</td>
<td id="S2.T1.8.8.8.10" class="ltx_td"></td>
<td id="S2.T1.5.5.5.5" class="ltx_td ltx_align_center">87.07 <math id="S2.T1.5.5.5.5.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.5.5.5.5.m1.1a"><mo id="S2.T1.5.5.5.5.m1.1.1" xref="S2.T1.5.5.5.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.5.5.m1.1b"><csymbol cd="latexml" id="S2.T1.5.5.5.5.m1.1.1.cmml" xref="S2.T1.5.5.5.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.5.5.m1.1c">\pm</annotation></semantics></math> 1.12</td>
<td id="S2.T1.6.6.6.6" class="ltx_td ltx_align_center">52.28 <math id="S2.T1.6.6.6.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.6.6.6.6.m1.1a"><mo id="S2.T1.6.6.6.6.m1.1.1" xref="S2.T1.6.6.6.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.6.6.6.6.m1.1b"><csymbol cd="latexml" id="S2.T1.6.6.6.6.m1.1.1.cmml" xref="S2.T1.6.6.6.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.6.6.6.m1.1c">\pm</annotation></semantics></math> 1.66</td>
<td id="S2.T1.7.7.7.7" class="ltx_td ltx_align_center">61.02 <math id="S2.T1.7.7.7.7.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.7.7.7.7.m1.1a"><mo id="S2.T1.7.7.7.7.m1.1.1" xref="S2.T1.7.7.7.7.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.7.7.7.7.m1.1b"><csymbol cd="latexml" id="S2.T1.7.7.7.7.m1.1.1.cmml" xref="S2.T1.7.7.7.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.7.7.7.m1.1c">\pm</annotation></semantics></math> 1.83</td>
<td id="S2.T1.8.8.8.8" class="ltx_td ltx_align_center">70.87 <math id="S2.T1.8.8.8.8.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.8.8.8.8.m1.1a"><mo id="S2.T1.8.8.8.8.m1.1.1" xref="S2.T1.8.8.8.8.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.8.8.8.8.m1.1b"><csymbol cd="latexml" id="S2.T1.8.8.8.8.m1.1.1.cmml" xref="S2.T1.8.8.8.8.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.8.8.8.8.m1.1c">\pm</annotation></semantics></math> 1.51</td>
</tr>
<tr id="S2.T1.16.16.16" class="ltx_tr">
<th id="S2.T1.16.16.16.9" class="ltx_td ltx_align_left ltx_th ltx_th_row">LXMERT (high-magnitude)</th>
<td id="S2.T1.9.9.9.1" class="ltx_td ltx_align_center">74.11 <math id="S2.T1.9.9.9.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.9.9.9.1.m1.1a"><mo id="S2.T1.9.9.9.1.m1.1.1" xref="S2.T1.9.9.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.9.9.9.1.m1.1b"><csymbol cd="latexml" id="S2.T1.9.9.9.1.m1.1.1.cmml" xref="S2.T1.9.9.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.9.9.9.1.m1.1c">\pm</annotation></semantics></math> 0.91</td>
<td id="S2.T1.10.10.10.2" class="ltx_td ltx_align_center">42.81 <math id="S2.T1.10.10.10.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.10.10.10.2.m1.1a"><mo id="S2.T1.10.10.10.2.m1.1.1" xref="S2.T1.10.10.10.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.10.10.10.2.m1.1b"><csymbol cd="latexml" id="S2.T1.10.10.10.2.m1.1.1.cmml" xref="S2.T1.10.10.10.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.10.10.10.2.m1.1c">\pm</annotation></semantics></math> 1.36</td>
<td id="S2.T1.11.11.11.3" class="ltx_td ltx_align_center">50.5 <math id="S2.T1.11.11.11.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.11.11.11.3.m1.1a"><mo id="S2.T1.11.11.11.3.m1.1.1" xref="S2.T1.11.11.11.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.11.11.11.3.m1.1b"><csymbol cd="latexml" id="S2.T1.11.11.11.3.m1.1.1.cmml" xref="S2.T1.11.11.11.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.11.11.11.3.m1.1c">\pm</annotation></semantics></math> 0.19</td>
<td id="S2.T1.12.12.12.4" class="ltx_td ltx_align_center">59.35 <math id="S2.T1.12.12.12.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.12.12.12.4.m1.1a"><mo id="S2.T1.12.12.12.4.m1.1.1" xref="S2.T1.12.12.12.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.12.12.12.4.m1.1b"><csymbol cd="latexml" id="S2.T1.12.12.12.4.m1.1.1.cmml" xref="S2.T1.12.12.12.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.12.12.12.4.m1.1c">\pm</annotation></semantics></math> 0.61</td>
<td id="S2.T1.16.16.16.10" class="ltx_td"></td>
<td id="S2.T1.13.13.13.5" class="ltx_td ltx_align_center">74.23 <math id="S2.T1.13.13.13.5.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.13.13.13.5.m1.1a"><mo id="S2.T1.13.13.13.5.m1.1.1" xref="S2.T1.13.13.13.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.13.13.13.5.m1.1b"><csymbol cd="latexml" id="S2.T1.13.13.13.5.m1.1.1.cmml" xref="S2.T1.13.13.13.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.13.13.13.5.m1.1c">\pm</annotation></semantics></math> 0.81</td>
<td id="S2.T1.14.14.14.6" class="ltx_td ltx_align_center">42.99 <math id="S2.T1.14.14.14.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.14.14.14.6.m1.1a"><mo id="S2.T1.14.14.14.6.m1.1.1" xref="S2.T1.14.14.14.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.14.14.14.6.m1.1b"><csymbol cd="latexml" id="S2.T1.14.14.14.6.m1.1.1.cmml" xref="S2.T1.14.14.14.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.14.14.14.6.m1.1c">\pm</annotation></semantics></math> 0.87</td>
<td id="S2.T1.15.15.15.7" class="ltx_td ltx_align_center">50.71 <math id="S2.T1.15.15.15.7.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.15.15.15.7.m1.1a"><mo id="S2.T1.15.15.15.7.m1.1.1" xref="S2.T1.15.15.15.7.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.15.15.15.7.m1.1b"><csymbol cd="latexml" id="S2.T1.15.15.15.7.m1.1.1.cmml" xref="S2.T1.15.15.15.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.15.15.15.7.m1.1c">\pm</annotation></semantics></math> 0.26</td>
<td id="S2.T1.16.16.16.8" class="ltx_td ltx_align_center">59.62 <math id="S2.T1.16.16.16.8.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.16.16.16.8.m1.1a"><mo id="S2.T1.16.16.16.8.m1.1.1" xref="S2.T1.16.16.16.8.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.16.16.16.8.m1.1b"><csymbol cd="latexml" id="S2.T1.16.16.16.8.m1.1.1.cmml" xref="S2.T1.16.16.16.8.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.16.16.16.8.m1.1c">\pm</annotation></semantics></math> 0.55</td>
</tr>
<tr id="S2.T1.24.24.24" class="ltx_tr">
<th id="S2.T1.24.24.24.9" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">LXMERT (random)</th>
<td id="S2.T1.17.17.17.1" class="ltx_td ltx_align_center ltx_border_b">69.26 <math id="S2.T1.17.17.17.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.17.17.17.1.m1.1a"><mo id="S2.T1.17.17.17.1.m1.1.1" xref="S2.T1.17.17.17.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.17.17.17.1.m1.1b"><csymbol cd="latexml" id="S2.T1.17.17.17.1.m1.1.1.cmml" xref="S2.T1.17.17.17.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.17.17.17.1.m1.1c">\pm</annotation></semantics></math> 0.29</td>
<td id="S2.T1.18.18.18.2" class="ltx_td ltx_align_center ltx_border_b">39.84 <math id="S2.T1.18.18.18.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.18.18.18.2.m1.1a"><mo id="S2.T1.18.18.18.2.m1.1.1" xref="S2.T1.18.18.18.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.18.18.18.2.m1.1b"><csymbol cd="latexml" id="S2.T1.18.18.18.2.m1.1.1.cmml" xref="S2.T1.18.18.18.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.18.18.18.2.m1.1c">\pm</annotation></semantics></math> 0.93</td>
<td id="S2.T1.19.19.19.3" class="ltx_td ltx_align_center ltx_border_b">45.96 <math id="S2.T1.19.19.19.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.19.19.19.3.m1.1a"><mo id="S2.T1.19.19.19.3.m1.1.1" xref="S2.T1.19.19.19.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.19.19.19.3.m1.1b"><csymbol cd="latexml" id="S2.T1.19.19.19.3.m1.1.1.cmml" xref="S2.T1.19.19.19.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.19.19.19.3.m1.1c">\pm</annotation></semantics></math> 0.83</td>
<td id="S2.T1.20.20.20.4" class="ltx_td ltx_align_center ltx_border_b">54.86 <math id="S2.T1.20.20.20.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.20.20.20.4.m1.1a"><mo id="S2.T1.20.20.20.4.m1.1.1" xref="S2.T1.20.20.20.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.20.20.20.4.m1.1b"><csymbol cd="latexml" id="S2.T1.20.20.20.4.m1.1.1.cmml" xref="S2.T1.20.20.20.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.20.20.20.4.m1.1c">\pm</annotation></semantics></math> 0.52</td>
<td id="S2.T1.24.24.24.10" class="ltx_td ltx_border_b"></td>
<td id="S2.T1.21.21.21.5" class="ltx_td ltx_align_center ltx_border_b">69.27 <math id="S2.T1.21.21.21.5.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.21.21.21.5.m1.1a"><mo id="S2.T1.21.21.21.5.m1.1.1" xref="S2.T1.21.21.21.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.21.21.21.5.m1.1b"><csymbol cd="latexml" id="S2.T1.21.21.21.5.m1.1.1.cmml" xref="S2.T1.21.21.21.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.21.21.21.5.m1.1c">\pm</annotation></semantics></math> 0.18</td>
<td id="S2.T1.22.22.22.6" class="ltx_td ltx_align_center ltx_border_b">40.34 <math id="S2.T1.22.22.22.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.22.22.22.6.m1.1a"><mo id="S2.T1.22.22.22.6.m1.1.1" xref="S2.T1.22.22.22.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.22.22.22.6.m1.1b"><csymbol cd="latexml" id="S2.T1.22.22.22.6.m1.1.1.cmml" xref="S2.T1.22.22.22.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.22.22.22.6.m1.1c">\pm</annotation></semantics></math> 0.66</td>
<td id="S2.T1.23.23.23.7" class="ltx_td ltx_align_center ltx_border_b">46.33 <math id="S2.T1.23.23.23.7.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.23.23.23.7.m1.1a"><mo id="S2.T1.23.23.23.7.m1.1.1" xref="S2.T1.23.23.23.7.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.23.23.23.7.m1.1b"><csymbol cd="latexml" id="S2.T1.23.23.23.7.m1.1.1.cmml" xref="S2.T1.23.23.23.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.23.23.23.7.m1.1c">\pm</annotation></semantics></math> 0.79</td>
<td id="S2.T1.24.24.24.8" class="ltx_td ltx_align_center ltx_border_b">55.19 <math id="S2.T1.24.24.24.8.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.24.24.24.8.m1.1a"><mo id="S2.T1.24.24.24.8.m1.1.1" xref="S2.T1.24.24.24.8.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.24.24.24.8.m1.1b"><csymbol cd="latexml" id="S2.T1.24.24.24.8.m1.1.1.cmml" xref="S2.T1.24.24.24.8.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.24.24.24.8.m1.1c">\pm</annotation></semantics></math> 0.45</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span> Performance of subnetworks at 50% weights pruning on VQA v2, which reported for both test-dev and test-std. Test-dev is used for debugging and validation experiments. Test-standard is the default test data for the VQA competition. We test each experiment for three different seeds and report the mean and standard deviation of VQA accuracy across three seeds.</figcaption>
</figure>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">LXMERT is a Transformer-based model which takes two inputs: image and text. Internally, LXMERT consists of two types of encoders: single-modality encoders for each modality and a cross-modality encoder using bidirectional cross attention to exchange information and align entities across the modalities.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The Lottery Ticket Hypothesis <cite class="ltx_cite ltx_citemacro_cite">Frankle and Carbin (<a href="#bib.bib4" title="" class="ltx_ref">2019</a>)</cite> shows that by preserving the original weight initializations from the unpruned network, you can train a network with the topology of the pruned network and achieve the same or better test accuracy within the same number of training iterations.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">In order to apply LTH to the LXMERT model, we use iterative magnitude pruning. Therefore, we fine-tune LXMERT on the VQA task and iteratively prune 10% of the lowest magnitude weights across the entire model, excluding embedding and output layers. We keep pruning until our model loses roughly half the weights. We use the default settings and hyperparameters of LXMERT <cite class="ltx_cite ltx_citemacro_cite">Tan and Bansal (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> to finetune on the VQA v2.0 dataset.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setups and Results</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The experiments are designed to investigate the effectiveness and stability of LTH on LXMERT in addition to cost-benefit analysis of the number of parameters in the model. We conduct experiments on the widely-used VQA v2.0 <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite> dataset built based on the MS-COCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib7" title="" class="ltx_ref">2014</a>)</cite> image corpus.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Effectiveness and Stability</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The following steps are performed to compress the LXMERT model.</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">The pretrained LXMERT model plus the VQA classifier’s randomly initialized weights are saved.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">The model is fine-tuned on the 3,129 most frequent answers in the VQA v2.0 dataset.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Iterative magnitude pruning is applied to find the low-magnitude subnetwork (pruning 50% of the low-magnitude weights). The high-magnitude subnetwork is computed as a compliment of the low-magnitude subnetwork with equal size. A random subnetwork with an equal size is generated for comparison.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">The saved weights are restored for all three subnetworks.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">The high-magnitude, low-magnitude, and random subnetworks are fine-tuned and evaluated on the VQA Task using three different seeds for initializing the VQA model to ensure the stability of the results.</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Results of subnetworks at 50% weights pruning on VQA v2.0 are summarized in Table <a href="#S2.T1" title="Table 1 ‣ 2 Methodology ‣ LXMERT Model Compression for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> where DistillVLM <cite class="ltx_cite ltx_citemacro_cite">Fang et al. (<a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite> is also listed for comparison. Row 2 to row 5 reports respectively full finetuned LXMERT, low-magnitude, high-magnitude, and random subnetworks. Low-magnitude pruning achieves 97% of full finetuned LXMERT accuracy in overall for both test-dev and test-std and shows marginal improvement over DistillVLM as the baseline. By comparing performance across the subnetworks, random and high magnitude subnetworks perform far worse than low-magnitude subnetwork. Surprisingly, the results demonstrate high-magnitude subnetwork performing better than random subnetwork. This could be a LXMERT specific phenomenon and required further investigation.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Cost-Benefit Analysis</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We experiment with low-magnitude subnetwork by pruning 10% of the weights all the way up to 90% of the weights in 10% increments. Accuracy of these pruned models on VQA v2.0 are reported in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.2 Cost-Benefit Analysis ‣ 3 Experimental Setups and Results ‣ LXMERT Model Compression for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Our results indicate a significant loss of accuracy after 50% to 60% pruning.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2310.15325/assets/1.jpeg" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="252" height="146" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Model size cost-benefit analysis.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We confirm that LTH pruning is an effective method for pruning V+L pretrained models. We mainly focused on LXMERT, a two-stream V+L pretrained model, but our findings are consistent with <cite class="ltx_cite ltx_citemacro_citet">Gan et al. (<a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite>’s results while using UNITER, a single-stream V+L pretrained model.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV.2015.279" title="" class="ltx_ref ltx_href">VQA: Visual question
answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</em>, volume 2015 Inter, pages 2425–2433.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,
Yu Cheng, and Jingjing Liu. 2020.

</span>
<span class="ltx_bibblock">Uniter: Universal image-text representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 104–120.
Springer.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. (2021)</span>
<span class="ltx_bibblock">
Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lijuan Wang, Yezhou Yang, and Zicheng
Liu. 2021.

</span>
<span class="ltx_bibblock">Compressing visual-linguistic model via knowledge distillation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)</em>, pages 1428–1438.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frankle and Carbin (2019)</span>
<span class="ltx_bibblock">
Jonathan Frankle and Michael Carbin. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=rJl-b3RcF7" title="" class="ltx_ref ltx_href">The lottery
ticket hypothesis: Finding sparse, trainable neural networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan et al. (2021)</span>
<span class="ltx_bibblock">
Zhe Gan, Yen-Chun Chen, Linjie Li, Tianlong Chen, Yu Cheng, Shuohang Wang, and
Jingjing Liu. 2021.

</span>
<span class="ltx_bibblock">Playing lottery tickets with vision and language.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.11832</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
2017.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 6904–6913.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pages 740–755.
Springer.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Bansal (2020)</span>
<span class="ltx_bibblock">
Hao Tan and Mohit Bansal. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/d19-1514" title="" class="ltx_ref ltx_href">LXMert: Learning
cross-modality encoder representations from transformers</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in
Natural Language Processing and 9th International Joint Conference on Natural
Language Processing, Proceedings of the Conference</em>, pages 5100–5111.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.15324" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.15325" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.15325">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.15325" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.15326" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 21:12:38 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
