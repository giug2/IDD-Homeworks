<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2203.09943] Training a Tokenizer for Free with Private Federated Learning</title><meta property="og:description" content="Federated learning with differential privacy, i.e. private federated
learning (PFL), makes it possible to train models on private data
distributed across users’ devices without harming privacy.
PFL is efficient for mod…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Training a Tokenizer for Free with Private Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Training a Tokenizer for Free with Private Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2203.09943">

<!--Generated on Mon Mar 11 07:01:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Training a Tokenizer for Free with Private Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Eugene Bagdasaryan

<br class="ltx_break">Cornell Tech 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">eugene@cs.cornell.edu</span> 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_ERROR undefined">\AND</span>Congzheng Song
</span><span class="ltx_author_notes">  Work done during the internship at Apple.</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rogier van Dalen
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Matt Seigel
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Áine Cahill 
<br class="ltx_break">Apple 
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter">{csong4,rogier_vandalen,mseigel,aine_cahill}@apple.com</span>

<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Federated learning with differential privacy, i.e. private federated
learning (PFL), makes it possible to train models on private data
distributed across users’ devices without harming privacy.
PFL is efficient for models, such as neural networks, that
have a fixed number of parameters, and thus a fixed-dimensional gradient
vector.
Such models include neural-net language models, but not tokenizers, the topic of this work.
Training a tokenizer requires frequencies of words from an unlimited vocabulary, and existing methods for finding an unlimited vocabulary need a separate privacy budget.</p>
<p id="id5.id2" class="ltx_p">A workaround is to train the tokenizer on publicly available data.
However, in this paper we first show that a tokenizer trained on mismatched data results in worse model performance compared to a privacy-violating “oracle”
tokenizer that accesses user data, with perplexity increasing by 20 %.
We also show that sub-word tokenizers are better suited to the federated context than word-level ones, since they can encode new words, though with more tokens per word.</p>
<p id="id6.id3" class="ltx_p">Second, we propose a novel method to obtain a tokenizer without using any additional privacy budget.
During private federated learning of the language model, we sample from
the model, train a new tokenizer on the sampled sequences, and update
the model embeddings.
We then continue private federated learning, and obtain performance within 1 % of the “oracle” tokenizer.
Since this process trains the tokenizer only indirectly on private data, we can use the “postprocessing guarantee” of differential privacy and thus use no additional privacy budget.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Learning a language model (LM) requires text data that in many
situations is private, resides on people’s devices, and should stay
there. In federated learning <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite>, a central server learns
a model by receiving statistics, like parameter updates, from many
devices. Though devices send only statistics and not the raw data,
federated learning by itself can leak information about the data
<cite class="ltx_cite ltx_citemacro_citep">(Shokri et al., <a href="#bib.bib29" title="" class="ltx_ref">2017</a>; Song et al., <a href="#bib.bib30" title="" class="ltx_ref">2017</a>)</cite>. Private federated learning
(PFL) <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib22" title="" class="ltx_ref">2018</a>); Geyer et al. (<a href="#bib.bib11" title="" class="ltx_ref">2017</a>)</cite> uses differential
privacy <cite class="ltx_cite ltx_citemacro_citep">(Dwork et al., <a href="#bib.bib8" title="" class="ltx_ref">2006</a>, <a href="#bib.bib9" title="" class="ltx_ref">2014</a>)</cite> to mitigate
the privacy leaks by limiting the user’s impact on the final model.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">It is known how to train neural-net language models using PFL
<cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite>. However, an important part of language modeling is
tokenization: turning a text into a sequence of symbols from a fixed-size
symbol set. To obtain a tokenizer, published research on private
federated learning of language models uses either of two approaches,
neither of which are satisfactory. One approach is to train the
tokenizer on user data directly. The commonly-used LEAF dataset
<cite class="ltx_cite ltx_citemacro_cite">Caldas et al. (<a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite> and works relying on it <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib19" title="" class="ltx_ref">2021</a>); Hu et al. (<a href="#bib.bib13" title="" class="ltx_ref">2021</a>); Yu et al. (<a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite> assume access to the training data to
create the tokenizer. This is not relevant to real-world use cases and
undermines user privacy. The other approach is to use public
data to obtain the tokenizer <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite>. This is sensible from
a privacy perspective, but as we show the resulting distribution
mismatch harms performance, resulting in 10%-20% drop compared to
using an “oracle” tokenizer trained directly on users’ private data.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2203.09943/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="152" height="86" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Word-level and sub-word-level tokenization.
A word-level tokenizer can generate an “out-of-vocabulary” (OOV) symbol, which it is hard for a language model to use.
</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">There are two common types of tokenization, which are affected by
mismatched distributions in different ways: word and sub-word
tokenization.
Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates these.
A word-level tokenizer produces a symbol for each word, and assigns an out-of-vocabulary
token (OOV) to any unseen word. Text from mismatched distributions
will generally contain unseen words, which means the correct word cannot
be predicted, and the context becomes less meaningful when predicting
the next word.
Sub-word tokenization, on the other hand, splits some words into multiple
smaller tokens. This type of tokenization is generally chosen to
minimize the average number of tokens per word on training data. Current centrally
trained models use sub-word tokenization such as Byte-Pair
Encoding <cite class="ltx_cite ltx_citemacro_cite">Sennrich et al. (<a href="#bib.bib28" title="" class="ltx_ref">2016</a>)</cite>,
SentencePiece <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a href="#bib.bib18" title="" class="ltx_ref">2018</a>)</cite>, or
WordPieces <cite class="ltx_cite ltx_citemacro_cite">Schuster and Nakajima (<a href="#bib.bib27" title="" class="ltx_ref">2012</a>)</cite>. Nevertheless, mismatched
tokenizations in sub-word methods cause an increase in the number of
tokens per word, and thus decrease the amount of context the model can
use to predict the distribution of the next word.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work we present a general framework to approach training
language models in private federated learning by including tokenization
as part of the training pipeline. Our contributions are: (1) we uncover
the performance gaps when the models use the tokenizer obtained from a
different distribution vs the tokenizer obtained from the underlying
distribution. For word-level tokenization we show that a tokenizer
trained on public data reduces the next-word prediction accuracy of
10–20 % compared to a tokenizer estimated on user data. (2) We
demonstrate significant benefits of switching tokenizers from word to
sub-word level, thus eliminating the out-of-vocabulary problem. (3) We
propose a new method that samples data from an existing model, e.g. from
the prior PFL run, and uses that data to initialize a new tokenizer.
Our approach can update the tokenizer between iterations of
the same PFL run by modifying model embeddings with new tokenizations and
significantly boosting performance.
Crucially, since the language model is trained with differential privacy, the “postprocessing guarantee” of differential privacy means that training the tokenizer with our approach does not use any additional privacy budget.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Private federated learning</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Machine-learned models work best if they are trained on the correct distribution of the data, in this paper text data.
In many scenarios text data is private and contained on people’s devices, and should stay there.
To train a global model without harming privacy, we use federated learning <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite> with differential privacy <cite class="ltx_cite ltx_citemacro_cite">Dwork et al. (<a href="#bib.bib8" title="" class="ltx_ref">2006</a>, <a href="#bib.bib9" title="" class="ltx_ref">2014</a>)</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.6" class="ltx_p">Federated learning involves devices sending not the data, but statistics, e.g. model gradients, computed on that data.
To train neural networks, the standard algorithm is <em id="S2.p2.6.1" class="ltx_emph ltx_font_italic">federated averaging</em> <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite>.
At each iteration <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.p2.1.m1.1a"><mi id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">t</annotation></semantics></math>, the server randomly selects a subset of <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S2.p2.2.m2.1a"><mi id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">m</annotation></semantics></math> participants <math id="S2.p2.3.m3.1" class="ltx_Math" alttext="S_{m}" display="inline"><semantics id="S2.p2.3.m3.1a"><msub id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml"><mi id="S2.p2.3.m3.1.1.2" xref="S2.p2.3.m3.1.1.2.cmml">S</mi><mi id="S2.p2.3.m3.1.1.3" xref="S2.p2.3.m3.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><apply id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.p2.3.m3.1.1.1.cmml" xref="S2.p2.3.m3.1.1">subscript</csymbol><ci id="S2.p2.3.m3.1.1.2.cmml" xref="S2.p2.3.m3.1.1.2">𝑆</ci><ci id="S2.p2.3.m3.1.1.3.cmml" xref="S2.p2.3.m3.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">S_{m}</annotation></semantics></math> and distributes the current global model <math id="S2.p2.4.m4.1" class="ltx_Math" alttext="M^{t}" display="inline"><semantics id="S2.p2.4.m4.1a"><msup id="S2.p2.4.m4.1.1" xref="S2.p2.4.m4.1.1.cmml"><mi id="S2.p2.4.m4.1.1.2" xref="S2.p2.4.m4.1.1.2.cmml">M</mi><mi id="S2.p2.4.m4.1.1.3" xref="S2.p2.4.m4.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S2.p2.4.m4.1b"><apply id="S2.p2.4.m4.1.1.cmml" xref="S2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p2.4.m4.1.1.1.cmml" xref="S2.p2.4.m4.1.1">superscript</csymbol><ci id="S2.p2.4.m4.1.1.2.cmml" xref="S2.p2.4.m4.1.1.2">𝑀</ci><ci id="S2.p2.4.m4.1.1.3.cmml" xref="S2.p2.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m4.1c">M^{t}</annotation></semantics></math>.
Each participant takes a number of gradient steps to train on their private
data and submits the sum <math id="S2.p2.5.m5.1" class="ltx_Math" alttext="G_{i}^{t}" display="inline"><semantics id="S2.p2.5.m5.1a"><msubsup id="S2.p2.5.m5.1.1" xref="S2.p2.5.m5.1.1.cmml"><mi id="S2.p2.5.m5.1.1.2.2" xref="S2.p2.5.m5.1.1.2.2.cmml">G</mi><mi id="S2.p2.5.m5.1.1.2.3" xref="S2.p2.5.m5.1.1.2.3.cmml">i</mi><mi id="S2.p2.5.m5.1.1.3" xref="S2.p2.5.m5.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.p2.5.m5.1b"><apply id="S2.p2.5.m5.1.1.cmml" xref="S2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S2.p2.5.m5.1.1.1.cmml" xref="S2.p2.5.m5.1.1">superscript</csymbol><apply id="S2.p2.5.m5.1.1.2.cmml" xref="S2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S2.p2.5.m5.1.1.2.1.cmml" xref="S2.p2.5.m5.1.1">subscript</csymbol><ci id="S2.p2.5.m5.1.1.2.2.cmml" xref="S2.p2.5.m5.1.1.2.2">𝐺</ci><ci id="S2.p2.5.m5.1.1.2.3.cmml" xref="S2.p2.5.m5.1.1.2.3">𝑖</ci></apply><ci id="S2.p2.5.m5.1.1.3.cmml" xref="S2.p2.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m5.1c">G_{i}^{t}</annotation></semantics></math> of the gradients to the server.
The server takes a step (with step size <math id="S2.p2.6.m6.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S2.p2.6.m6.1a"><mi id="S2.p2.6.m6.1.1" xref="S2.p2.6.m6.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="S2.p2.6.m6.1b"><ci id="S2.p2.6.m6.1.1.cmml" xref="S2.p2.6.m6.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.6.m6.1c">\eta</annotation></semantics></math>) in the direction of the average gradient to create the new global model:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_Math" alttext="M^{t+1}=M^{t}+\frac{\eta}{m}\sum_{i=1}^{m}G_{i}^{t}" display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><msup id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml"><mi id="S2.E1.m1.1.1.2.2" xref="S2.E1.m1.1.1.2.2.cmml">M</mi><mrow id="S2.E1.m1.1.1.2.3" xref="S2.E1.m1.1.1.2.3.cmml"><mi id="S2.E1.m1.1.1.2.3.2" xref="S2.E1.m1.1.1.2.3.2.cmml">t</mi><mo id="S2.E1.m1.1.1.2.3.1" xref="S2.E1.m1.1.1.2.3.1.cmml">+</mo><mn id="S2.E1.m1.1.1.2.3.3" xref="S2.E1.m1.1.1.2.3.3.cmml">1</mn></mrow></msup><mo id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><msup id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml"><mi id="S2.E1.m1.1.1.3.2.2" xref="S2.E1.m1.1.1.3.2.2.cmml">M</mi><mi id="S2.E1.m1.1.1.3.2.3" xref="S2.E1.m1.1.1.3.2.3.cmml">t</mi></msup><mo id="S2.E1.m1.1.1.3.1" xref="S2.E1.m1.1.1.3.1.cmml">+</mo><mrow id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml"><mfrac id="S2.E1.m1.1.1.3.3.2" xref="S2.E1.m1.1.1.3.3.2.cmml"><mi id="S2.E1.m1.1.1.3.3.2.2" xref="S2.E1.m1.1.1.3.3.2.2.cmml">η</mi><mi id="S2.E1.m1.1.1.3.3.2.3" xref="S2.E1.m1.1.1.3.3.2.3.cmml">m</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.3.3.1" xref="S2.E1.m1.1.1.3.3.1.cmml">​</mo><mrow id="S2.E1.m1.1.1.3.3.3" xref="S2.E1.m1.1.1.3.3.3.cmml"><munderover id="S2.E1.m1.1.1.3.3.3.1" xref="S2.E1.m1.1.1.3.3.3.1.cmml"><mo movablelimits="false" id="S2.E1.m1.1.1.3.3.3.1.2.2" xref="S2.E1.m1.1.1.3.3.3.1.2.2.cmml">∑</mo><mrow id="S2.E1.m1.1.1.3.3.3.1.2.3" xref="S2.E1.m1.1.1.3.3.3.1.2.3.cmml"><mi id="S2.E1.m1.1.1.3.3.3.1.2.3.2" xref="S2.E1.m1.1.1.3.3.3.1.2.3.2.cmml">i</mi><mo id="S2.E1.m1.1.1.3.3.3.1.2.3.1" xref="S2.E1.m1.1.1.3.3.3.1.2.3.1.cmml">=</mo><mn id="S2.E1.m1.1.1.3.3.3.1.2.3.3" xref="S2.E1.m1.1.1.3.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S2.E1.m1.1.1.3.3.3.1.3" xref="S2.E1.m1.1.1.3.3.3.1.3.cmml">m</mi></munderover><msubsup id="S2.E1.m1.1.1.3.3.3.2" xref="S2.E1.m1.1.1.3.3.3.2.cmml"><mi id="S2.E1.m1.1.1.3.3.3.2.2.2" xref="S2.E1.m1.1.1.3.3.3.2.2.2.cmml">G</mi><mi id="S2.E1.m1.1.1.3.3.3.2.2.3" xref="S2.E1.m1.1.1.3.3.3.2.2.3.cmml">i</mi><mi id="S2.E1.m1.1.1.3.3.3.2.3" xref="S2.E1.m1.1.1.3.3.3.2.3.cmml">t</mi></msubsup></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><eq id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"></eq><apply id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.2">superscript</csymbol><ci id="S2.E1.m1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.2.2">𝑀</ci><apply id="S2.E1.m1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.2.3"><plus id="S2.E1.m1.1.1.2.3.1.cmml" xref="S2.E1.m1.1.1.2.3.1"></plus><ci id="S2.E1.m1.1.1.2.3.2.cmml" xref="S2.E1.m1.1.1.2.3.2">𝑡</ci><cn type="integer" id="S2.E1.m1.1.1.2.3.3.cmml" xref="S2.E1.m1.1.1.2.3.3">1</cn></apply></apply><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><plus id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3.1"></plus><apply id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.2.1.cmml" xref="S2.E1.m1.1.1.3.2">superscript</csymbol><ci id="S2.E1.m1.1.1.3.2.2.cmml" xref="S2.E1.m1.1.1.3.2.2">𝑀</ci><ci id="S2.E1.m1.1.1.3.2.3.cmml" xref="S2.E1.m1.1.1.3.2.3">𝑡</ci></apply><apply id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3"><times id="S2.E1.m1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3.1"></times><apply id="S2.E1.m1.1.1.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3.2"><divide id="S2.E1.m1.1.1.3.3.2.1.cmml" xref="S2.E1.m1.1.1.3.3.2"></divide><ci id="S2.E1.m1.1.1.3.3.2.2.cmml" xref="S2.E1.m1.1.1.3.3.2.2">𝜂</ci><ci id="S2.E1.m1.1.1.3.3.2.3.cmml" xref="S2.E1.m1.1.1.3.3.2.3">𝑚</ci></apply><apply id="S2.E1.m1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.3.3.3"><apply id="S2.E1.m1.1.1.3.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.3.1.1.cmml" xref="S2.E1.m1.1.1.3.3.3.1">superscript</csymbol><apply id="S2.E1.m1.1.1.3.3.3.1.2.cmml" xref="S2.E1.m1.1.1.3.3.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.3.1.2.1.cmml" xref="S2.E1.m1.1.1.3.3.3.1">subscript</csymbol><sum id="S2.E1.m1.1.1.3.3.3.1.2.2.cmml" xref="S2.E1.m1.1.1.3.3.3.1.2.2"></sum><apply id="S2.E1.m1.1.1.3.3.3.1.2.3.cmml" xref="S2.E1.m1.1.1.3.3.3.1.2.3"><eq id="S2.E1.m1.1.1.3.3.3.1.2.3.1.cmml" xref="S2.E1.m1.1.1.3.3.3.1.2.3.1"></eq><ci id="S2.E1.m1.1.1.3.3.3.1.2.3.2.cmml" xref="S2.E1.m1.1.1.3.3.3.1.2.3.2">𝑖</ci><cn type="integer" id="S2.E1.m1.1.1.3.3.3.1.2.3.3.cmml" xref="S2.E1.m1.1.1.3.3.3.1.2.3.3">1</cn></apply></apply><ci id="S2.E1.m1.1.1.3.3.3.1.3.cmml" xref="S2.E1.m1.1.1.3.3.3.1.3">𝑚</ci></apply><apply id="S2.E1.m1.1.1.3.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.3.2.1.cmml" xref="S2.E1.m1.1.1.3.3.3.2">superscript</csymbol><apply id="S2.E1.m1.1.1.3.3.3.2.2.cmml" xref="S2.E1.m1.1.1.3.3.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.3.2.2.1.cmml" xref="S2.E1.m1.1.1.3.3.3.2">subscript</csymbol><ci id="S2.E1.m1.1.1.3.3.3.2.2.2.cmml" xref="S2.E1.m1.1.1.3.3.3.2.2.2">𝐺</ci><ci id="S2.E1.m1.1.1.3.3.3.2.2.3.cmml" xref="S2.E1.m1.1.1.3.3.3.2.2.3">𝑖</ci></apply><ci id="S2.E1.m1.1.1.3.3.3.2.3.cmml" xref="S2.E1.m1.1.1.3.3.3.2.3">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">M^{t+1}=M^{t}+\frac{\eta}{m}\sum_{i=1}^{m}G_{i}^{t}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Federated Learning with Differential Privacy</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.8" class="ltx_p">The global model <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="M^{t+1}" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><msup id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">M</mi><mrow id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml"><mi id="S2.SS1.p1.1.m1.1.1.3.2" xref="S2.SS1.p1.1.m1.1.1.3.2.cmml">t</mi><mo id="S2.SS1.p1.1.m1.1.1.3.1" xref="S2.SS1.p1.1.m1.1.1.3.1.cmml">+</mo><mn id="S2.SS1.p1.1.m1.1.1.3.3" xref="S2.SS1.p1.1.m1.1.1.3.3.cmml">1</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">superscript</csymbol><ci id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">𝑀</ci><apply id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3"><plus id="S2.SS1.p1.1.m1.1.1.3.1.cmml" xref="S2.SS1.p1.1.m1.1.1.3.1"></plus><ci id="S2.SS1.p1.1.m1.1.1.3.2.cmml" xref="S2.SS1.p1.1.m1.1.1.3.2">𝑡</ci><cn type="integer" id="S2.SS1.p1.1.m1.1.1.3.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">M^{t+1}</annotation></semantics></math> might still reveal private
information including user participation in
training <cite class="ltx_cite ltx_citemacro_citep">(Shokri et al., <a href="#bib.bib29" title="" class="ltx_ref">2017</a>; Song et al., <a href="#bib.bib30" title="" class="ltx_ref">2017</a>; Melis et al., <a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite>.
To mitigate this threat, we can combine federated learning with
differential privacy (DP)
<cite class="ltx_cite ltx_citemacro_citep">(Dwork et al., <a href="#bib.bib8" title="" class="ltx_ref">2006</a>, <a href="#bib.bib9" title="" class="ltx_ref">2014</a>)</cite>, to give <em id="S2.SS1.p1.8.1" class="ltx_emph ltx_font_italic">private
federate learning</em> <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite>. Differential privacy gives a
strong guarantee: it limits the advantage that a computationally
unconstrained adversary has in inferring whether an individual’s data is
contained in the data set that the statistics are computed from.
<math id="S2.SS1.p1.2.m2.2" class="ltx_Math" alttext="(\epsilon,\delta)" display="inline"><semantics id="S2.SS1.p1.2.m2.2a"><mrow id="S2.SS1.p1.2.m2.2.3.2" xref="S2.SS1.p1.2.m2.2.3.1.cmml"><mo stretchy="false" id="S2.SS1.p1.2.m2.2.3.2.1" xref="S2.SS1.p1.2.m2.2.3.1.cmml">(</mo><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">ϵ</mi><mo id="S2.SS1.p1.2.m2.2.3.2.2" xref="S2.SS1.p1.2.m2.2.3.1.cmml">,</mo><mi id="S2.SS1.p1.2.m2.2.2" xref="S2.SS1.p1.2.m2.2.2.cmml">δ</mi><mo stretchy="false" id="S2.SS1.p1.2.m2.2.3.2.3" xref="S2.SS1.p1.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.2b"><interval closure="open" id="S2.SS1.p1.2.m2.2.3.1.cmml" xref="S2.SS1.p1.2.m2.2.3.2"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">italic-ϵ</ci><ci id="S2.SS1.p1.2.m2.2.2.cmml" xref="S2.SS1.p1.2.m2.2.2">𝛿</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.2c">(\epsilon,\delta)</annotation></semantics></math>-differential privacy parametrizes this advantage by
<math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">\epsilon</annotation></semantics></math> (the maximum privacy loss) and <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mi id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">\delta</annotation></semantics></math> (a slack term). The
common mechanism to provide differential privacy in a federated learning setting
is the Gaussian mechanism that uses the <em id="S2.SS1.p1.8.2" class="ltx_emph ltx_font_italic">moments
accountant</em> <cite class="ltx_cite ltx_citemacro_citep">(Abadi et al., <a href="#bib.bib1" title="" class="ltx_ref">2016</a>)</cite>. For each participant, the model parameters are
<em id="S2.SS1.p1.8.3" class="ltx_emph ltx_font_italic">clipped</em> to a norm <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><mi id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">S</annotation></semantics></math>, i.e., multiplied by <math id="S2.SS1.p1.6.m6.2" class="ltx_Math" alttext="\textnormal{min}(1,S/{\lVert G^{t}\rVert_{2}})" display="inline"><semantics id="S2.SS1.p1.6.m6.2a"><mrow id="S2.SS1.p1.6.m6.2.2" xref="S2.SS1.p1.6.m6.2.2.cmml"><mtext id="S2.SS1.p1.6.m6.2.2.3" xref="S2.SS1.p1.6.m6.2.2.3a.cmml">min</mtext><mo lspace="0em" rspace="0em" id="S2.SS1.p1.6.m6.2.2.2" xref="S2.SS1.p1.6.m6.2.2.2.cmml">​</mo><mrow id="S2.SS1.p1.6.m6.2.2.1.1" xref="S2.SS1.p1.6.m6.2.2.1.2.cmml"><mo stretchy="false" id="S2.SS1.p1.6.m6.2.2.1.1.2" xref="S2.SS1.p1.6.m6.2.2.1.2.cmml">(</mo><mn id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml">1</mn><mo id="S2.SS1.p1.6.m6.2.2.1.1.3" xref="S2.SS1.p1.6.m6.2.2.1.2.cmml">,</mo><mrow id="S2.SS1.p1.6.m6.2.2.1.1.1" xref="S2.SS1.p1.6.m6.2.2.1.1.1.cmml"><mi id="S2.SS1.p1.6.m6.2.2.1.1.1.3" xref="S2.SS1.p1.6.m6.2.2.1.1.1.3.cmml">S</mi><mo id="S2.SS1.p1.6.m6.2.2.1.1.1.2" xref="S2.SS1.p1.6.m6.2.2.1.1.1.2.cmml">/</mo><msub id="S2.SS1.p1.6.m6.2.2.1.1.1.1" xref="S2.SS1.p1.6.m6.2.2.1.1.1.1.cmml"><mrow id="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.1" xref="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.2.cmml"><mo fence="true" lspace="0em" rspace="0em" id="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.1.2" xref="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.2.1.cmml">∥</mo><msup id="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.1.1" xref="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.1.1.2" xref="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.1.1.2.cmml">G</mi><mi id="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.1.1.3" xref="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.1.1.3.cmml">t</mi></msup><mo fence="true" lspace="0em" rspace="0em" id="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.1.3" xref="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.2.1.cmml">∥</mo></mrow><mn id="S2.SS1.p1.6.m6.2.2.1.1.1.1.3" xref="S2.SS1.p1.6.m6.2.2.1.1.1.1.3.cmml">2</mn></msub></mrow><mo stretchy="false" id="S2.SS1.p1.6.m6.2.2.1.1.4" xref="S2.SS1.p1.6.m6.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.2b"><apply id="S2.SS1.p1.6.m6.2.2.cmml" xref="S2.SS1.p1.6.m6.2.2"><times id="S2.SS1.p1.6.m6.2.2.2.cmml" xref="S2.SS1.p1.6.m6.2.2.2"></times><ci id="S2.SS1.p1.6.m6.2.2.3a.cmml" xref="S2.SS1.p1.6.m6.2.2.3"><mtext id="S2.SS1.p1.6.m6.2.2.3.cmml" xref="S2.SS1.p1.6.m6.2.2.3">min</mtext></ci><interval closure="open" id="S2.SS1.p1.6.m6.2.2.1.2.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1"><cn type="integer" id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">1</cn><apply id="S2.SS1.p1.6.m6.2.2.1.1.1.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1"><divide id="S2.SS1.p1.6.m6.2.2.1.1.1.2.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1.2"></divide><ci id="S2.SS1.p1.6.m6.2.2.1.1.1.3.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1.3">𝑆</ci><apply id="S2.SS1.p1.6.m6.2.2.1.1.1.1.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.2.2.1.1.1.1.2.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1.1">subscript</csymbol><apply id="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.2.1.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.1.2">delimited-∥∥</csymbol><apply id="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.1.1">superscript</csymbol><ci id="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.1.1.2">𝐺</ci><ci id="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1.1.1.1.1.3">𝑡</ci></apply></apply><cn type="integer" id="S2.SS1.p1.6.m6.2.2.1.1.1.1.3.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1.1.3">2</cn></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.2c">\textnormal{min}(1,S/{\lVert G^{t}\rVert_{2}})</annotation></semantics></math>, to bound the sum’s sensitivity to any individual’s data.
Second, Gaussian noise <math id="S2.SS1.p1.7.m7.2" class="ltx_Math" alttext="\mathcal{N}(0,\sigma^{2})" display="inline"><semantics id="S2.SS1.p1.7.m7.2a"><mrow id="S2.SS1.p1.7.m7.2.2" xref="S2.SS1.p1.7.m7.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.7.m7.2.2.3" xref="S2.SS1.p1.7.m7.2.2.3.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.7.m7.2.2.2" xref="S2.SS1.p1.7.m7.2.2.2.cmml">​</mo><mrow id="S2.SS1.p1.7.m7.2.2.1.1" xref="S2.SS1.p1.7.m7.2.2.1.2.cmml"><mo stretchy="false" id="S2.SS1.p1.7.m7.2.2.1.1.2" xref="S2.SS1.p1.7.m7.2.2.1.2.cmml">(</mo><mn id="S2.SS1.p1.7.m7.1.1" xref="S2.SS1.p1.7.m7.1.1.cmml">0</mn><mo id="S2.SS1.p1.7.m7.2.2.1.1.3" xref="S2.SS1.p1.7.m7.2.2.1.2.cmml">,</mo><msup id="S2.SS1.p1.7.m7.2.2.1.1.1" xref="S2.SS1.p1.7.m7.2.2.1.1.1.cmml"><mi id="S2.SS1.p1.7.m7.2.2.1.1.1.2" xref="S2.SS1.p1.7.m7.2.2.1.1.1.2.cmml">σ</mi><mn id="S2.SS1.p1.7.m7.2.2.1.1.1.3" xref="S2.SS1.p1.7.m7.2.2.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S2.SS1.p1.7.m7.2.2.1.1.4" xref="S2.SS1.p1.7.m7.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m7.2b"><apply id="S2.SS1.p1.7.m7.2.2.cmml" xref="S2.SS1.p1.7.m7.2.2"><times id="S2.SS1.p1.7.m7.2.2.2.cmml" xref="S2.SS1.p1.7.m7.2.2.2"></times><ci id="S2.SS1.p1.7.m7.2.2.3.cmml" xref="S2.SS1.p1.7.m7.2.2.3">𝒩</ci><interval closure="open" id="S2.SS1.p1.7.m7.2.2.1.2.cmml" xref="S2.SS1.p1.7.m7.2.2.1.1"><cn type="integer" id="S2.SS1.p1.7.m7.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1">0</cn><apply id="S2.SS1.p1.7.m7.2.2.1.1.1.cmml" xref="S2.SS1.p1.7.m7.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m7.2.2.1.1.1.1.cmml" xref="S2.SS1.p1.7.m7.2.2.1.1.1">superscript</csymbol><ci id="S2.SS1.p1.7.m7.2.2.1.1.1.2.cmml" xref="S2.SS1.p1.7.m7.2.2.1.1.1.2">𝜎</ci><cn type="integer" id="S2.SS1.p1.7.m7.2.2.1.1.1.3.cmml" xref="S2.SS1.p1.7.m7.2.2.1.1.1.3">2</cn></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m7.2c">\mathcal{N}(0,\sigma^{2})</annotation></semantics></math> is added to the final sum.
How much privacy budget is spent depends on the variance <math id="S2.SS1.p1.8.m8.1" class="ltx_Math" alttext="\sigma^{2}" display="inline"><semantics id="S2.SS1.p1.8.m8.1a"><msup id="S2.SS1.p1.8.m8.1.1" xref="S2.SS1.p1.8.m8.1.1.cmml"><mi id="S2.SS1.p1.8.m8.1.1.2" xref="S2.SS1.p1.8.m8.1.1.2.cmml">σ</mi><mn id="S2.SS1.p1.8.m8.1.1.3" xref="S2.SS1.p1.8.m8.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><apply id="S2.SS1.p1.8.m8.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.8.m8.1.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1">superscript</csymbol><ci id="S2.SS1.p1.8.m8.1.1.2.cmml" xref="S2.SS1.p1.8.m8.1.1.2">𝜎</ci><cn type="integer" id="S2.SS1.p1.8.m8.1.1.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">\sigma^{2}</annotation></semantics></math> relative to the magnitude of individual updates, the total population, the number of contributions in each iteration, and the total number of iterations <cite class="ltx_cite ltx_citemacro_citep">(for more details, see McMahan et al., <a href="#bib.bib22" title="" class="ltx_ref">2018</a>; Balle et al., <a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Privately finding vocabulary items</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Central differential privacy with the Gaussian mechanism and the moments accountant is efficient in terms of utility vs privacy loss, but it does come with restrictions.
The sum of individual contributions, which the noise is added to, must be of finite and fixed size.
This is not a problem for training neural networks.
However, training a tokenizer requires frequencies for an exponential-size set of sequences, as does training a traditional <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">N</annotation></semantics></math>-gram model.
Differentially private algorithms to compute histograms over sets of elements (e.g. words) distributed over devices are
called “heavy hitters” algorithms
<cite class="ltx_cite ltx_citemacro_citep">(Bassily et al., <a href="#bib.bib4" title="" class="ltx_ref">2017</a>; Zhu et al., <a href="#bib.bib33" title="" class="ltx_ref">2020</a>; Apple, <a href="#bib.bib2" title="" class="ltx_ref">2017</a>)</cite>.
These algorithms require a separate and large privacy budget.
In section <a href="#S5" title="5 Experiments ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> we will compare with a heavy hitters algorithm.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Another way of finding vocabulary items privately is to train a
neural-net generative model. <cite class="ltx_cite ltx_citemacro_citet">Beaufays et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite> trains a separate,
character-level LSTM model to generate the new words. However, the
proposed method is only shown to work for discover <span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_typewriter">OOV</span>s in a word-level model and
also requires separate training and a privacy budget.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Tokenization in Language Modeling</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.6" class="ltx_p">A language model is a model that assigns
probabilities to sequences of tokens. In this paper, it is always an
autoregressive model with parameters <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\theta</annotation></semantics></math>: <math id="S3.p1.2.m2.4" class="ltx_math_unparsed" alttext="P_{\theta}(s)=P_{\theta}(t_{2}|t_{1}=\texttt{BOS})\cdot P_{\theta}(t_{3}|t_{1}=\texttt{BOS},t_{2})\cdots P_{\theta}(t_{n}=\texttt{EOS}|t_{1}=\texttt{BOS},\ldots,t_{n-1})" display="inline"><semantics id="S3.p1.2.m2.4a"><mrow id="S3.p1.2.m2.4b"><msub id="S3.p1.2.m2.4.5"><mi id="S3.p1.2.m2.4.5.2">P</mi><mi id="S3.p1.2.m2.4.5.3">θ</mi></msub><mrow id="S3.p1.2.m2.4.6"><mo stretchy="false" id="S3.p1.2.m2.4.6.1">(</mo><mi id="S3.p1.2.m2.1.1">s</mi><mo stretchy="false" id="S3.p1.2.m2.4.6.2">)</mo></mrow><mo id="S3.p1.2.m2.4.7">=</mo><msub id="S3.p1.2.m2.4.8"><mi id="S3.p1.2.m2.4.8.2">P</mi><mi id="S3.p1.2.m2.4.8.3">θ</mi></msub><mrow id="S3.p1.2.m2.4.9"><mo stretchy="false" id="S3.p1.2.m2.4.9.1">(</mo><msub id="S3.p1.2.m2.4.9.2"><mi id="S3.p1.2.m2.4.9.2.2">t</mi><mn id="S3.p1.2.m2.4.9.2.3">2</mn></msub><mo fence="false" rspace="0.167em" stretchy="false" id="S3.p1.2.m2.4.9.3">|</mo><msub id="S3.p1.2.m2.4.9.4"><mi id="S3.p1.2.m2.4.9.4.2">t</mi><mn id="S3.p1.2.m2.4.9.4.3">1</mn></msub><mo id="S3.p1.2.m2.4.9.5">=</mo><mtext class="ltx_mathvariant_monospace" id="S3.p1.2.m2.4.9.6">BOS</mtext><mo rspace="0.055em" stretchy="false" id="S3.p1.2.m2.4.9.7">)</mo></mrow><mo rspace="0.222em" id="S3.p1.2.m2.4.10">⋅</mo><msub id="S3.p1.2.m2.4.11"><mi id="S3.p1.2.m2.4.11.2">P</mi><mi id="S3.p1.2.m2.4.11.3">θ</mi></msub><mrow id="S3.p1.2.m2.4.12"><mo stretchy="false" id="S3.p1.2.m2.4.12.1">(</mo><msub id="S3.p1.2.m2.4.12.2"><mi id="S3.p1.2.m2.4.12.2.2">t</mi><mn id="S3.p1.2.m2.4.12.2.3">3</mn></msub><mo fence="false" rspace="0.167em" stretchy="false" id="S3.p1.2.m2.4.12.3">|</mo><msub id="S3.p1.2.m2.4.12.4"><mi id="S3.p1.2.m2.4.12.4.2">t</mi><mn id="S3.p1.2.m2.4.12.4.3">1</mn></msub><mo id="S3.p1.2.m2.4.12.5">=</mo><mtext class="ltx_mathvariant_monospace" id="S3.p1.2.m2.2.2">BOS</mtext><mo id="S3.p1.2.m2.4.12.6">,</mo><msub id="S3.p1.2.m2.4.12.7"><mi id="S3.p1.2.m2.4.12.7.2">t</mi><mn id="S3.p1.2.m2.4.12.7.3">2</mn></msub><mo stretchy="false" id="S3.p1.2.m2.4.12.8">)</mo></mrow><mi mathvariant="normal" id="S3.p1.2.m2.4.13">⋯</mi><msub id="S3.p1.2.m2.4.14"><mi id="S3.p1.2.m2.4.14.2">P</mi><mi id="S3.p1.2.m2.4.14.3">θ</mi></msub><mrow id="S3.p1.2.m2.4.15"><mo stretchy="false" id="S3.p1.2.m2.4.15.1">(</mo><msub id="S3.p1.2.m2.4.15.2"><mi id="S3.p1.2.m2.4.15.2.2">t</mi><mi id="S3.p1.2.m2.4.15.2.3">n</mi></msub><mo id="S3.p1.2.m2.4.15.3">=</mo><mtext class="ltx_mathvariant_monospace" id="S3.p1.2.m2.4.15.4">EOS</mtext><mo fence="false" rspace="0.167em" stretchy="false" id="S3.p1.2.m2.4.15.5">|</mo><msub id="S3.p1.2.m2.4.15.6"><mi id="S3.p1.2.m2.4.15.6.2">t</mi><mn id="S3.p1.2.m2.4.15.6.3">1</mn></msub><mo id="S3.p1.2.m2.4.15.7">=</mo><mtext class="ltx_mathvariant_monospace" id="S3.p1.2.m2.3.3">BOS</mtext><mo id="S3.p1.2.m2.4.15.8">,</mo><mi mathvariant="normal" id="S3.p1.2.m2.4.4">…</mi><mo id="S3.p1.2.m2.4.15.9">,</mo><msub id="S3.p1.2.m2.4.15.10"><mi id="S3.p1.2.m2.4.15.10.2">t</mi><mrow id="S3.p1.2.m2.4.15.10.3"><mi id="S3.p1.2.m2.4.15.10.3.2">n</mi><mo id="S3.p1.2.m2.4.15.10.3.1">−</mo><mn id="S3.p1.2.m2.4.15.10.3.3">1</mn></mrow></msub><mo stretchy="false" id="S3.p1.2.m2.4.15.11">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.p1.2.m2.4c">P_{\theta}(s)=P_{\theta}(t_{2}|t_{1}=\texttt{BOS})\cdot P_{\theta}(t_{3}|t_{1}=\texttt{BOS},t_{2})\cdots P_{\theta}(t_{n}=\texttt{EOS}|t_{1}=\texttt{BOS},\ldots,t_{n-1})</annotation></semantics></math>, where each term in
this equation is normalized over all possible values of the current
token. Local normalization is useful when decoding input, like in
speech recognition or a keyboard <cite class="ltx_cite ltx_citemacro_cite">Hard et al. (<a href="#bib.bib12" title="" class="ltx_ref">2018</a>)</cite>.
For this paper, we assume that a corpus is segmented into sentences. A
tokenizer <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">\tau</annotation></semantics></math> then converts each sentence <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.p1.4.m4.1a"><mi id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">s</annotation></semantics></math> in the dataset into a
sequence of <math id="S3.p1.5.m5.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.p1.5.m5.1a"><mi id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><ci id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">n</annotation></semantics></math> tokens <math id="S3.p1.6.m6.2" class="ltx_math_unparsed" alttext="\tau(s)=[\texttt{BOS},t_{2},..,t_{n-1},\texttt{EOS}]" display="inline"><semantics id="S3.p1.6.m6.2a"><mrow id="S3.p1.6.m6.2b"><mi id="S3.p1.6.m6.2.3">τ</mi><mrow id="S3.p1.6.m6.2.4"><mo stretchy="false" id="S3.p1.6.m6.2.4.1">(</mo><mi id="S3.p1.6.m6.1.1">s</mi><mo stretchy="false" id="S3.p1.6.m6.2.4.2">)</mo></mrow><mo id="S3.p1.6.m6.2.5">=</mo><mrow id="S3.p1.6.m6.2.6"><mo stretchy="false" id="S3.p1.6.m6.2.6.1">[</mo><mtext class="ltx_mathvariant_monospace" id="S3.p1.6.m6.2.2">BOS</mtext><mo id="S3.p1.6.m6.2.6.2">,</mo><msub id="S3.p1.6.m6.2.6.3"><mi id="S3.p1.6.m6.2.6.3.2">t</mi><mn id="S3.p1.6.m6.2.6.3.3">2</mn></msub><mo id="S3.p1.6.m6.2.6.4">,</mo><mo lspace="0em" rspace="0.0835em" id="S3.p1.6.m6.2.6.5">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S3.p1.6.m6.2.6.6">.</mo><mo id="S3.p1.6.m6.2.6.7">,</mo><msub id="S3.p1.6.m6.2.6.8"><mi id="S3.p1.6.m6.2.6.8.2">t</mi><mrow id="S3.p1.6.m6.2.6.8.3"><mi id="S3.p1.6.m6.2.6.8.3.2">n</mi><mo id="S3.p1.6.m6.2.6.8.3.1">−</mo><mn id="S3.p1.6.m6.2.6.8.3.3">1</mn></mrow></msub><mo id="S3.p1.6.m6.2.6.9">,</mo><mtext class="ltx_mathvariant_monospace" id="S3.p1.6.m6.2.6.10">EOS</mtext><mo stretchy="false" id="S3.p1.6.m6.2.6.11">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.p1.6.m6.2c">\tau(s)=[\texttt{BOS},t_{2},..,t_{n-1},\texttt{EOS}]</annotation></semantics></math>, which is fed into the language model.
There are two types of tokenization, highlighted in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>: word-level and sub-word-level.
Using a sub-word tokenizer will be key to the algorithm this paper proposes.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">The next section will discuss the two types of tokenizers and their consequences for out-of-vocabulary tokens and the performance of language models based in them.
Section <a href="#S3.SS2" title="3.2 Evaluating language models across tokenizations ‣ 3 Tokenization in Language Modeling ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> will discuss the complex topic of how to compare performance across different tokenizations.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Word-level vs sub-word-level tokenization</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p">The type of tokenization that papers about language models in federated learning commonly use is
word-level tokenization <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite>. For a vocabulary of size <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">N</annotation></semantics></math>
the tokenizer assigns a unique token for top-<math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">N</annotation></semantics></math> most popular words in
the dataset while other words receive an out-of-vocabulary token <span id="S3.SS1.p1.2.1" class="ltx_text ltx_font_typewriter">OOV</span>, as highlighted in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Some papers <cite class="ltx_cite ltx_citemacro_citep">(e.g. McMahan et al., <a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite> build the tokenizer from a
publicly available dataset, others including the LEAF benchmark
<cite class="ltx_cite ltx_citemacro_cite">Caldas et al. (<a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite> build the tokenizer from users’ training data.
OOV tokens in the word history make it harder for a language model to predict the next word.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The other type of tokenization is sub-word tokenization, for which there are two popular schemes: byte-pair
encoding (BPE) <cite class="ltx_cite ltx_citemacro_cite">Sennrich et al. (<a href="#bib.bib28" title="" class="ltx_ref">2016</a>)</cite>
and WordPieces <cite class="ltx_cite ltx_citemacro_citep">(Schuster and Nakajima, <a href="#bib.bib27" title="" class="ltx_ref">2012</a>)</cite>. We focus on BPE which
unlike WordPieces guarantees the absence of OOVs as there exists a token for every byte.
However, the number of tokens required to encode each word can change significantly depending on the dataset that the tokenizer was trained on.
As highlighted in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, a tokenizer trained on data from before the COVID-19 pandemic would generate multiple tokens for the word “covid”.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Generating longer token sequences makes it harder for the language model to keep track of the context, degrading its performance.
Even LSTMs and transformers, which in theory can use arbitrarily long history,
have imperfect memory.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Evaluating language models across tokenizations</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Comparing language models across tokenizations is a complex problem. For
example, when comparing word-level language models using perplexity,
often OOVs are ignored which gives an edge to the language model with
more OOVs, which is the opposite of what is desired. The following
sections detail the problems when comparing sub-word language models.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Comparing word-level with sub-word</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Since a word-level language model has a closed vocabulary, it outputs
probabilities only on in-vocabulary words, artificially lowering the perplexity of closed-vocabulary LMs, particularly on data with a large number of OOVs.
Removing those same words in evaluating a sub-word language model, would disadvantage it.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">A better alternative, which this paper will use, is to compare model
performance the word-level accuracy.
The most accurate way would be to find the word with the highest probability by summing over sequences of tokens.
However, we choose a simpler,
though less accurate method <cite class="ltx_cite ltx_citemacro_citep">(similar to Likhomanenko et al., <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>:
repeatedly generate the best tokens within each word’s bounds and only
accept the word as accurate if all generated tokens were correct.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Comparing sub-word with sub-word</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">It is possible to meaningfully compare perplexities of two language
models with different sub-word tokenizations <cite class="ltx_cite ltx_citemacro_cite">Mielke (<a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite>.
Though the language model assigns probability mass to all token
sequences, a single sentence can have multiple corresponding
token sequences, only one of which will be chosen by the tokenizer. Some of
the probability mass will therefore be lost to never-occurring token
sequences. However, it is unfeasible to sum over all token sequences
<cite class="ltx_cite ltx_citemacro_citep">(Likhomanenko et al., <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.4" class="ltx_p">The danger with comparing perplexities directly is
that since models with different tokenizers operate on different sets of
tokens the number of tokens needed to encode each sentence is different
in general <cite class="ltx_cite ltx_citemacro_cite">Mielke (<a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite>. Nevertheless, note that all models assign a
probability to a sentence (with the approximation above).
To compute the perplexity in such a way that it can be compared across tokenizers, use the same denominator in computing the
perplexity: the number of words in the sentence instead of number of
tokens, which depends on the tokenizer. Therefore we define the
perplexity as:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.10" class="ltx_Math" alttext="ppl_{\theta,\tau}(s)=\exp\left(\frac{-\log(P_{\theta,\tau}(s))}{\lVert s\rVert_{w}}\right)" display="block"><semantics id="S3.E2.m1.10a"><mrow id="S3.E2.m1.10.11" xref="S3.E2.m1.10.11.cmml"><mrow id="S3.E2.m1.10.11.2" xref="S3.E2.m1.10.11.2.cmml"><mi id="S3.E2.m1.10.11.2.2" xref="S3.E2.m1.10.11.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.10.11.2.1" xref="S3.E2.m1.10.11.2.1.cmml">​</mo><mi id="S3.E2.m1.10.11.2.3" xref="S3.E2.m1.10.11.2.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.10.11.2.1a" xref="S3.E2.m1.10.11.2.1.cmml">​</mo><msub id="S3.E2.m1.10.11.2.4" xref="S3.E2.m1.10.11.2.4.cmml"><mi id="S3.E2.m1.10.11.2.4.2" xref="S3.E2.m1.10.11.2.4.2.cmml">l</mi><mrow id="S3.E2.m1.2.2.2.4" xref="S3.E2.m1.2.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">θ</mi><mo id="S3.E2.m1.2.2.2.4.1" xref="S3.E2.m1.2.2.2.3.cmml">,</mo><mi id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml">τ</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.10.11.2.1b" xref="S3.E2.m1.10.11.2.1.cmml">​</mo><mrow id="S3.E2.m1.10.11.2.5.2" xref="S3.E2.m1.10.11.2.cmml"><mo stretchy="false" id="S3.E2.m1.10.11.2.5.2.1" xref="S3.E2.m1.10.11.2.cmml">(</mo><mi id="S3.E2.m1.9.9" xref="S3.E2.m1.9.9.cmml">s</mi><mo stretchy="false" id="S3.E2.m1.10.11.2.5.2.2" xref="S3.E2.m1.10.11.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.10.11.1" xref="S3.E2.m1.10.11.1.cmml">=</mo><mrow id="S3.E2.m1.10.11.3.2" xref="S3.E2.m1.10.11.3.1.cmml"><mi id="S3.E2.m1.10.10" xref="S3.E2.m1.10.10.cmml">exp</mi><mo id="S3.E2.m1.10.11.3.2a" xref="S3.E2.m1.10.11.3.1.cmml">⁡</mo><mrow id="S3.E2.m1.10.11.3.2.1" xref="S3.E2.m1.10.11.3.1.cmml"><mo id="S3.E2.m1.10.11.3.2.1.1" xref="S3.E2.m1.10.11.3.1.cmml">(</mo><mfrac id="S3.E2.m1.8.8" xref="S3.E2.m1.8.8.cmml"><mrow id="S3.E2.m1.7.7.5" xref="S3.E2.m1.7.7.5.cmml"><mo rspace="0.167em" id="S3.E2.m1.7.7.5a" xref="S3.E2.m1.7.7.5.cmml">−</mo><mrow id="S3.E2.m1.7.7.5.5.1" xref="S3.E2.m1.7.7.5.5.2.cmml"><mi id="S3.E2.m1.6.6.4.4" xref="S3.E2.m1.6.6.4.4.cmml">log</mi><mo id="S3.E2.m1.7.7.5.5.1a" xref="S3.E2.m1.7.7.5.5.2.cmml">⁡</mo><mrow id="S3.E2.m1.7.7.5.5.1.1" xref="S3.E2.m1.7.7.5.5.2.cmml"><mo stretchy="false" id="S3.E2.m1.7.7.5.5.1.1.2" xref="S3.E2.m1.7.7.5.5.2.cmml">(</mo><mrow id="S3.E2.m1.7.7.5.5.1.1.1" xref="S3.E2.m1.7.7.5.5.1.1.1.cmml"><msub id="S3.E2.m1.7.7.5.5.1.1.1.2" xref="S3.E2.m1.7.7.5.5.1.1.1.2.cmml"><mi id="S3.E2.m1.7.7.5.5.1.1.1.2.2" xref="S3.E2.m1.7.7.5.5.1.1.1.2.2.cmml">P</mi><mrow id="S3.E2.m1.4.4.2.2.2.4" xref="S3.E2.m1.4.4.2.2.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.cmml">θ</mi><mo id="S3.E2.m1.4.4.2.2.2.4.1" xref="S3.E2.m1.4.4.2.2.2.3.cmml">,</mo><mi id="S3.E2.m1.4.4.2.2.2.2" xref="S3.E2.m1.4.4.2.2.2.2.cmml">τ</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.7.7.5.5.1.1.1.1" xref="S3.E2.m1.7.7.5.5.1.1.1.1.cmml">​</mo><mrow id="S3.E2.m1.7.7.5.5.1.1.1.3.2" xref="S3.E2.m1.7.7.5.5.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.7.7.5.5.1.1.1.3.2.1" xref="S3.E2.m1.7.7.5.5.1.1.1.cmml">(</mo><mi id="S3.E2.m1.5.5.3.3" xref="S3.E2.m1.5.5.3.3.cmml">s</mi><mo stretchy="false" id="S3.E2.m1.7.7.5.5.1.1.1.3.2.2" xref="S3.E2.m1.7.7.5.5.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m1.7.7.5.5.1.1.3" xref="S3.E2.m1.7.7.5.5.2.cmml">)</mo></mrow></mrow></mrow><msub id="S3.E2.m1.8.8.6" xref="S3.E2.m1.8.8.6.cmml"><mrow id="S3.E2.m1.8.8.6.3.2" xref="S3.E2.m1.8.8.6.3.1.cmml"><mo fence="true" rspace="0em" id="S3.E2.m1.8.8.6.3.2.1" xref="S3.E2.m1.8.8.6.3.1.1.cmml">∥</mo><mi id="S3.E2.m1.8.8.6.1" xref="S3.E2.m1.8.8.6.1.cmml">s</mi><mo fence="true" lspace="0em" id="S3.E2.m1.8.8.6.3.2.2" xref="S3.E2.m1.8.8.6.3.1.1.cmml">∥</mo></mrow><mi id="S3.E2.m1.8.8.6.4" xref="S3.E2.m1.8.8.6.4.cmml">w</mi></msub></mfrac><mo id="S3.E2.m1.10.11.3.2.1.2" xref="S3.E2.m1.10.11.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.10b"><apply id="S3.E2.m1.10.11.cmml" xref="S3.E2.m1.10.11"><eq id="S3.E2.m1.10.11.1.cmml" xref="S3.E2.m1.10.11.1"></eq><apply id="S3.E2.m1.10.11.2.cmml" xref="S3.E2.m1.10.11.2"><times id="S3.E2.m1.10.11.2.1.cmml" xref="S3.E2.m1.10.11.2.1"></times><ci id="S3.E2.m1.10.11.2.2.cmml" xref="S3.E2.m1.10.11.2.2">𝑝</ci><ci id="S3.E2.m1.10.11.2.3.cmml" xref="S3.E2.m1.10.11.2.3">𝑝</ci><apply id="S3.E2.m1.10.11.2.4.cmml" xref="S3.E2.m1.10.11.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.10.11.2.4.1.cmml" xref="S3.E2.m1.10.11.2.4">subscript</csymbol><ci id="S3.E2.m1.10.11.2.4.2.cmml" xref="S3.E2.m1.10.11.2.4.2">𝑙</ci><list id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.4"><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝜃</ci><ci id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">𝜏</ci></list></apply><ci id="S3.E2.m1.9.9.cmml" xref="S3.E2.m1.9.9">𝑠</ci></apply><apply id="S3.E2.m1.10.11.3.1.cmml" xref="S3.E2.m1.10.11.3.2"><exp id="S3.E2.m1.10.10.cmml" xref="S3.E2.m1.10.10"></exp><apply id="S3.E2.m1.8.8.cmml" xref="S3.E2.m1.8.8"><divide id="S3.E2.m1.8.8.7.cmml" xref="S3.E2.m1.8.8"></divide><apply id="S3.E2.m1.7.7.5.cmml" xref="S3.E2.m1.7.7.5"><minus id="S3.E2.m1.7.7.5.6.cmml" xref="S3.E2.m1.7.7.5"></minus><apply id="S3.E2.m1.7.7.5.5.2.cmml" xref="S3.E2.m1.7.7.5.5.1"><log id="S3.E2.m1.6.6.4.4.cmml" xref="S3.E2.m1.6.6.4.4"></log><apply id="S3.E2.m1.7.7.5.5.1.1.1.cmml" xref="S3.E2.m1.7.7.5.5.1.1.1"><times id="S3.E2.m1.7.7.5.5.1.1.1.1.cmml" xref="S3.E2.m1.7.7.5.5.1.1.1.1"></times><apply id="S3.E2.m1.7.7.5.5.1.1.1.2.cmml" xref="S3.E2.m1.7.7.5.5.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.5.5.1.1.1.2.1.cmml" xref="S3.E2.m1.7.7.5.5.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.7.7.5.5.1.1.1.2.2.cmml" xref="S3.E2.m1.7.7.5.5.1.1.1.2.2">𝑃</ci><list id="S3.E2.m1.4.4.2.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.2.4"><ci id="S3.E2.m1.3.3.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1">𝜃</ci><ci id="S3.E2.m1.4.4.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2">𝜏</ci></list></apply><ci id="S3.E2.m1.5.5.3.3.cmml" xref="S3.E2.m1.5.5.3.3">𝑠</ci></apply></apply></apply><apply id="S3.E2.m1.8.8.6.cmml" xref="S3.E2.m1.8.8.6"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.6.2.cmml" xref="S3.E2.m1.8.8.6">subscript</csymbol><apply id="S3.E2.m1.8.8.6.3.1.cmml" xref="S3.E2.m1.8.8.6.3.2"><csymbol cd="latexml" id="S3.E2.m1.8.8.6.3.1.1.cmml" xref="S3.E2.m1.8.8.6.3.2.1">delimited-∥∥</csymbol><ci id="S3.E2.m1.8.8.6.1.cmml" xref="S3.E2.m1.8.8.6.1">𝑠</ci></apply><ci id="S3.E2.m1.8.8.6.4.cmml" xref="S3.E2.m1.8.8.6.4">𝑤</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.10c">ppl_{\theta,\tau}(s)=\exp\left(\frac{-\log(P_{\theta,\tau}(s))}{\lVert s\rVert_{w}}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS2.p2.3" class="ltx_p">where <math id="S3.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="\lVert s\rVert_{w}" display="inline"><semantics id="S3.SS2.SSS2.p2.1.m1.1a"><msub id="S3.SS2.SSS2.p2.1.m1.1.2" xref="S3.SS2.SSS2.p2.1.m1.1.2.cmml"><mrow id="S3.SS2.SSS2.p2.1.m1.1.2.2.2" xref="S3.SS2.SSS2.p2.1.m1.1.2.2.1.cmml"><mo fence="true" rspace="0em" id="S3.SS2.SSS2.p2.1.m1.1.2.2.2.1" xref="S3.SS2.SSS2.p2.1.m1.1.2.2.1.1.cmml">∥</mo><mi id="S3.SS2.SSS2.p2.1.m1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.cmml">s</mi><mo fence="true" lspace="0em" id="S3.SS2.SSS2.p2.1.m1.1.2.2.2.2" xref="S3.SS2.SSS2.p2.1.m1.1.2.2.1.1.cmml">∥</mo></mrow><mi id="S3.SS2.SSS2.p2.1.m1.1.2.3" xref="S3.SS2.SSS2.p2.1.m1.1.2.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.1.m1.1b"><apply id="S3.SS2.SSS2.p2.1.m1.1.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.1.m1.1.2.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.2">subscript</csymbol><apply id="S3.SS2.SSS2.p2.1.m1.1.2.2.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.2.2.2"><csymbol cd="latexml" id="S3.SS2.SSS2.p2.1.m1.1.2.2.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.2.2.2.1">delimited-∥∥</csymbol><ci id="S3.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1">𝑠</ci></apply><ci id="S3.SS2.SSS2.p2.1.m1.1.2.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.2.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.1.m1.1c">\lVert s\rVert_{w}</annotation></semantics></math> counts the number of words in the sentence
<math id="S3.SS2.SSS2.p2.2.m2.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.SS2.SSS2.p2.2.m2.1a"><mi id="S3.SS2.SSS2.p2.2.m2.1.1" xref="S3.SS2.SSS2.p2.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.2.m2.1b"><ci id="S3.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.2.m2.1c">s</annotation></semantics></math>.
To generalize from a single sentence to a dataset, replace <math id="S3.SS2.SSS2.p2.3.m3.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.SS2.SSS2.p2.3.m3.1a"><mi id="S3.SS2.SSS2.p2.3.m3.1.1" xref="S3.SS2.SSS2.p2.3.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.3.m3.1b"><ci id="S3.SS2.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.3.m3.1c">s</annotation></semantics></math> with the concatenation of all sentences in the dataset.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2203.09943/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="126" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>New pipeline for updating the tokenizer through model sampling.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Learning a Tokenizer with Private Federated Learning</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Problem definition.</span>  We aim to obtain a tokenizer that
works well on users’ federated data without compromising user
privacy. First, we aim to find the appropriate tokenization scheme, and
second, given the tokenization scheme obtain the right approximation of
user data to train the tokenizer.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.5" class="ltx_p"><span id="S4.p2.5.1" class="ltx_text ltx_font_bold ltx_font_italic">Setting</span>  We focus on a common application of federated
learning: training a language model, parameterized by <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S4.p2.1.m1.1a"><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">\theta</annotation></semantics></math>, using
federated learning with differential privacy. In our setting each user
<math id="S4.p2.2.m2.1" class="ltx_Math" alttext="u_{i}" display="inline"><semantics id="S4.p2.2.m2.1a"><msub id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mi id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">u</mi><mi id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1">subscript</csymbol><ci id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">𝑢</ci><ci id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">u_{i}</annotation></semantics></math> has a dataset <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="d_{i}" display="inline"><semantics id="S4.p2.3.m3.1a"><msub id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><mi id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml">d</mi><mi id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1">subscript</csymbol><ci id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2">𝑑</ci><ci id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">d_{i}</annotation></semantics></math> of private texts from a private distribution
of user data <math id="S4.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S4.p2.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><ci id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">\mathcal{D}</annotation></semantics></math>. The trained model will be evaluated against
a held-out dataset <math id="S4.p2.5.m5.1" class="ltx_Math" alttext="\mathcal{D}_{test}" display="inline"><semantics id="S4.p2.5.m5.1a"><msub id="S4.p2.5.m5.1.1" xref="S4.p2.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p2.5.m5.1.1.2" xref="S4.p2.5.m5.1.1.2.cmml">𝒟</mi><mrow id="S4.p2.5.m5.1.1.3" xref="S4.p2.5.m5.1.1.3.cmml"><mi id="S4.p2.5.m5.1.1.3.2" xref="S4.p2.5.m5.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p2.5.m5.1.1.3.1" xref="S4.p2.5.m5.1.1.3.1.cmml">​</mo><mi id="S4.p2.5.m5.1.1.3.3" xref="S4.p2.5.m5.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p2.5.m5.1.1.3.1a" xref="S4.p2.5.m5.1.1.3.1.cmml">​</mo><mi id="S4.p2.5.m5.1.1.3.4" xref="S4.p2.5.m5.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.p2.5.m5.1.1.3.1b" xref="S4.p2.5.m5.1.1.3.1.cmml">​</mo><mi id="S4.p2.5.m5.1.1.3.5" xref="S4.p2.5.m5.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p2.5.m5.1b"><apply id="S4.p2.5.m5.1.1.cmml" xref="S4.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.p2.5.m5.1.1.1.cmml" xref="S4.p2.5.m5.1.1">subscript</csymbol><ci id="S4.p2.5.m5.1.1.2.cmml" xref="S4.p2.5.m5.1.1.2">𝒟</ci><apply id="S4.p2.5.m5.1.1.3.cmml" xref="S4.p2.5.m5.1.1.3"><times id="S4.p2.5.m5.1.1.3.1.cmml" xref="S4.p2.5.m5.1.1.3.1"></times><ci id="S4.p2.5.m5.1.1.3.2.cmml" xref="S4.p2.5.m5.1.1.3.2">𝑡</ci><ci id="S4.p2.5.m5.1.1.3.3.cmml" xref="S4.p2.5.m5.1.1.3.3">𝑒</ci><ci id="S4.p2.5.m5.1.1.3.4.cmml" xref="S4.p2.5.m5.1.1.3.4">𝑠</ci><ci id="S4.p2.5.m5.1.1.3.5.cmml" xref="S4.p2.5.m5.1.1.3.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.5.m5.1c">\mathcal{D}_{test}</annotation></semantics></math>, e.g. a mix of all user data,
which in practice must be replaced by federated evaluation.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.5" class="ltx_p">We assume that the central server does not have access to the user data
distribution <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S4.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><ci id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">\mathcal{D}</annotation></semantics></math> and can only approximate it with the publicly
available dataset <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="\mathcal{D}_{pub}" display="inline"><semantics id="S4.p3.2.m2.1a"><msub id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p3.2.m2.1.1.2" xref="S4.p3.2.m2.1.1.2.cmml">𝒟</mi><mrow id="S4.p3.2.m2.1.1.3" xref="S4.p3.2.m2.1.1.3.cmml"><mi id="S4.p3.2.m2.1.1.3.2" xref="S4.p3.2.m2.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.3.1" xref="S4.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.p3.2.m2.1.1.3.3" xref="S4.p3.2.m2.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.3.1a" xref="S4.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.p3.2.m2.1.1.3.4" xref="S4.p3.2.m2.1.1.3.4.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><apply id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p3.2.m2.1.1.1.cmml" xref="S4.p3.2.m2.1.1">subscript</csymbol><ci id="S4.p3.2.m2.1.1.2.cmml" xref="S4.p3.2.m2.1.1.2">𝒟</ci><apply id="S4.p3.2.m2.1.1.3.cmml" xref="S4.p3.2.m2.1.1.3"><times id="S4.p3.2.m2.1.1.3.1.cmml" xref="S4.p3.2.m2.1.1.3.1"></times><ci id="S4.p3.2.m2.1.1.3.2.cmml" xref="S4.p3.2.m2.1.1.3.2">𝑝</ci><ci id="S4.p3.2.m2.1.1.3.3.cmml" xref="S4.p3.2.m2.1.1.3.3">𝑢</ci><ci id="S4.p3.2.m2.1.1.3.4.cmml" xref="S4.p3.2.m2.1.1.3.4">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">\mathcal{D}_{pub}</annotation></semantics></math>. We assume the public data is
some commonly available dataset, such as Wikipedia
<cite class="ltx_cite ltx_citemacro_cite">Merity et al. (<a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite>. The tokenizer trained on this public data
will be <math id="S4.p3.3.m3.1" class="ltx_Math" alttext="\tau_{pub}" display="inline"><semantics id="S4.p3.3.m3.1a"><msub id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml"><mi id="S4.p3.3.m3.1.1.2" xref="S4.p3.3.m3.1.1.2.cmml">τ</mi><mrow id="S4.p3.3.m3.1.1.3" xref="S4.p3.3.m3.1.1.3.cmml"><mi id="S4.p3.3.m3.1.1.3.2" xref="S4.p3.3.m3.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p3.3.m3.1.1.3.1" xref="S4.p3.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.p3.3.m3.1.1.3.3" xref="S4.p3.3.m3.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.p3.3.m3.1.1.3.1a" xref="S4.p3.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.p3.3.m3.1.1.3.4" xref="S4.p3.3.m3.1.1.3.4.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><apply id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p3.3.m3.1.1.1.cmml" xref="S4.p3.3.m3.1.1">subscript</csymbol><ci id="S4.p3.3.m3.1.1.2.cmml" xref="S4.p3.3.m3.1.1.2">𝜏</ci><apply id="S4.p3.3.m3.1.1.3.cmml" xref="S4.p3.3.m3.1.1.3"><times id="S4.p3.3.m3.1.1.3.1.cmml" xref="S4.p3.3.m3.1.1.3.1"></times><ci id="S4.p3.3.m3.1.1.3.2.cmml" xref="S4.p3.3.m3.1.1.3.2">𝑝</ci><ci id="S4.p3.3.m3.1.1.3.3.cmml" xref="S4.p3.3.m3.1.1.3.3">𝑢</ci><ci id="S4.p3.3.m3.1.1.3.4.cmml" xref="S4.p3.3.m3.1.1.3.4">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">\tau_{pub}</annotation></semantics></math>. For comparison we assume the existence of an
<em id="S4.p3.5.1" class="ltx_emph ltx_font_italic">oracle</em> tokenizer <math id="S4.p3.4.m4.1" class="ltx_Math" alttext="\tau_{o}" display="inline"><semantics id="S4.p3.4.m4.1a"><msub id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml"><mi id="S4.p3.4.m4.1.1.2" xref="S4.p3.4.m4.1.1.2.cmml">τ</mi><mi id="S4.p3.4.m4.1.1.3" xref="S4.p3.4.m4.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.1b"><apply id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p3.4.m4.1.1.1.cmml" xref="S4.p3.4.m4.1.1">subscript</csymbol><ci id="S4.p3.4.m4.1.1.2.cmml" xref="S4.p3.4.m4.1.1.2">𝜏</ci><ci id="S4.p3.4.m4.1.1.3.cmml" xref="S4.p3.4.m4.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.1c">\tau_{o}</annotation></semantics></math> initialized on users’ training data
<math id="S4.p3.5.m5.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S4.p3.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p3.5.m5.1.1" xref="S4.p3.5.m5.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S4.p3.5.m5.1b"><ci id="S4.p3.5.m5.1.1.cmml" xref="S4.p3.5.m5.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.5.m5.1c">\mathcal{D}</annotation></semantics></math>.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Papers that study language models in federated learning commonly use
word-level tokenization. While some papers <cite class="ltx_cite ltx_citemacro_citep">(e.g. McMahan et al., <a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite>,
build the vocabulary using publicly available dataset, others
<cite class="ltx_cite ltx_citemacro_citep">(e.g. Yu et al., <a href="#bib.bib32" title="" class="ltx_ref">2020</a>; Caldas et al., <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite> explicitly use the
federated training data, even though in real-world scenarios the
analogous data would be unavailable and it violates privacy guarantees
when used in PFL <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Sampling from a PFL-trained language model</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To address the problem of learning a good tokenizer we first propose to
use a sub-word tokenizer with an open vocabulary. This allows the
language model trained with such a tokenizer to represent any word, if
inefficiently. It is then possible to query the language model to find
new words as the model can utilize this open vocabulary. This is the
core of the Algorithm <a href="#alg1" title="Algorithm 1 ‣ 4.3 Adapting the language model to the new tokenizer ‣ 4 Learning a Tokenizer with Private Federated Learning ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> that this paper introduces.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2.2 Comparing sub-word with sub-word ‣ 3.2 Evaluating language models across tokenizations ‣ 3 Tokenization in Language Modeling ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the proposed pipeline. A language model
is trained with private federated learning. This results (on the left)
in a model matched with an old, stale tokenizer. The next block queries the
language model to produce a better tokenizer, with a method that section
<a href="#S4.SS2" title="4.2 New tokenizer from a trained LM ‣ 4 Learning a Tokenizer with Private Federated Learning ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> will detail. The block after that updates the
language model for the new tokenizer, using reasonable guesses for the
new parameters. This results in a new LM-tokenizer combination that can
be trained further with PFL.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">We assume that the language model obtained with the stale tokenizer is
trained with a certain privacy budget. The postprocessing guarantee of
differential privacy <cite class="ltx_cite ltx_citemacro_cite">Dwork (<a href="#bib.bib7" title="" class="ltx_ref">2011</a>)</cite> means that the steps
other than private federated learning do not consume any further budget.
The function <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_smallcaps">Update</span> in Algorithm <a href="#alg1" title="Algorithm 1 ‣ 4.3 Adapting the language model to the new tokenizer ‣ 4 Learning a Tokenizer with Private Federated Learning ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> performs
the on-server steps. The following sections will give more detail.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>New tokenizer from a trained LM</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Training a tokenizer requires text data. Since the raw data is not
available, we propose to instead sample from the LM matched with the
stale tokenizer, as detailed in Algorithm <a href="#alg1" title="Algorithm 1 ‣ 4.3 Adapting the language model to the new tokenizer ‣ 4 Learning a Tokenizer with Private Federated Learning ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The
<span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">SampleTokens</span> function samples from the language model, drawing
sequences of tokens according to the probabilities that the model
assigns to them. The <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_smallcaps">Sample</span> function then converts these
sequences in the old tokenization into word sequences, by decoding with
<math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\tau_{pub}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><msub id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">τ</mi><mrow id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml"><mi id="S4.SS2.p1.1.m1.1.1.3.2" xref="S4.SS2.p1.1.m1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.3.1" xref="S4.SS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.3.3" xref="S4.SS2.p1.1.m1.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.3.1a" xref="S4.SS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.3.4" xref="S4.SS2.p1.1.m1.1.1.3.4.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝜏</ci><apply id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3"><times id="S4.SS2.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.3.1"></times><ci id="S4.SS2.p1.1.m1.1.1.3.2.cmml" xref="S4.SS2.p1.1.m1.1.1.3.2">𝑝</ci><ci id="S4.SS2.p1.1.m1.1.1.3.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3">𝑢</ci><ci id="S4.SS2.p1.1.m1.1.1.3.4.cmml" xref="S4.SS2.p1.1.m1.1.1.3.4">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\tau_{pub}</annotation></semantics></math>. Once a large enough corpus of word-level sentences has
been produced, training a tokenizer proceeds as normally (the
<span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_smallcaps">TrainTokenizer</span> function is not specified).</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Adapting the language model to the new tokenizer</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.3" class="ltx_p">After a new tokenizer <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\tau</annotation></semantics></math> has been trained, the language model,
trained with <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="\tau_{pub}" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><msub id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mi id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">τ</mi><mrow id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml"><mi id="S4.SS3.p1.2.m2.1.1.3.2" xref="S4.SS3.p1.2.m2.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.2.m2.1.1.3.1" xref="S4.SS3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p1.2.m2.1.1.3.3" xref="S4.SS3.p1.2.m2.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.2.m2.1.1.3.1a" xref="S4.SS3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p1.2.m2.1.1.3.4" xref="S4.SS3.p1.2.m2.1.1.3.4.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">𝜏</ci><apply id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3"><times id="S4.SS3.p1.2.m2.1.1.3.1.cmml" xref="S4.SS3.p1.2.m2.1.1.3.1"></times><ci id="S4.SS3.p1.2.m2.1.1.3.2.cmml" xref="S4.SS3.p1.2.m2.1.1.3.2">𝑝</ci><ci id="S4.SS3.p1.2.m2.1.1.3.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3.3">𝑢</ci><ci id="S4.SS3.p1.2.m2.1.1.3.4.cmml" xref="S4.SS3.p1.2.m2.1.1.3.4">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">\tau_{pub}</annotation></semantics></math>, must be updated to work with the new
tokenizer. Neural-net language models use an embedding layer to convert
the provided tokens into multi-dimensional vectors. It is the embedding
vectors that are most important to modify when changing the
tokenization. The rest of the model only consumes the embedding vector.
It is not possible to find the optimal parameters without further
training of both embeddings and other layers, but we propose an
algorithm to find a reasonable starting point, in the function
<math id="S4.SS3.p1.3.m3.2" class="ltx_Math" alttext="\text{{Remap}}(\tau,\tau_{pub})" display="inline"><semantics id="S4.SS3.p1.3.m3.2a"><mrow id="S4.SS3.p1.3.m3.2.2" xref="S4.SS3.p1.3.m3.2.2.cmml"><mtext class="ltx_font_smallcaps" id="S4.SS3.p1.3.m3.2.2.3" xref="S4.SS3.p1.3.m3.2.2.3a.cmml">Remap</mtext><mo lspace="0em" rspace="0em" id="S4.SS3.p1.3.m3.2.2.2" xref="S4.SS3.p1.3.m3.2.2.2.cmml">​</mo><mrow id="S4.SS3.p1.3.m3.2.2.1.1" xref="S4.SS3.p1.3.m3.2.2.1.2.cmml"><mo stretchy="false" id="S4.SS3.p1.3.m3.2.2.1.1.2" xref="S4.SS3.p1.3.m3.2.2.1.2.cmml">(</mo><mi id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml">τ</mi><mo id="S4.SS3.p1.3.m3.2.2.1.1.3" xref="S4.SS3.p1.3.m3.2.2.1.2.cmml">,</mo><msub id="S4.SS3.p1.3.m3.2.2.1.1.1" xref="S4.SS3.p1.3.m3.2.2.1.1.1.cmml"><mi id="S4.SS3.p1.3.m3.2.2.1.1.1.2" xref="S4.SS3.p1.3.m3.2.2.1.1.1.2.cmml">τ</mi><mrow id="S4.SS3.p1.3.m3.2.2.1.1.1.3" xref="S4.SS3.p1.3.m3.2.2.1.1.1.3.cmml"><mi id="S4.SS3.p1.3.m3.2.2.1.1.1.3.2" xref="S4.SS3.p1.3.m3.2.2.1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.3.m3.2.2.1.1.1.3.1" xref="S4.SS3.p1.3.m3.2.2.1.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p1.3.m3.2.2.1.1.1.3.3" xref="S4.SS3.p1.3.m3.2.2.1.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p1.3.m3.2.2.1.1.1.3.1a" xref="S4.SS3.p1.3.m3.2.2.1.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p1.3.m3.2.2.1.1.1.3.4" xref="S4.SS3.p1.3.m3.2.2.1.1.1.3.4.cmml">b</mi></mrow></msub><mo stretchy="false" id="S4.SS3.p1.3.m3.2.2.1.1.4" xref="S4.SS3.p1.3.m3.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.2b"><apply id="S4.SS3.p1.3.m3.2.2.cmml" xref="S4.SS3.p1.3.m3.2.2"><times id="S4.SS3.p1.3.m3.2.2.2.cmml" xref="S4.SS3.p1.3.m3.2.2.2"></times><ci id="S4.SS3.p1.3.m3.2.2.3a.cmml" xref="S4.SS3.p1.3.m3.2.2.3"><mtext class="ltx_font_smallcaps" id="S4.SS3.p1.3.m3.2.2.3.cmml" xref="S4.SS3.p1.3.m3.2.2.3">Remap</mtext></ci><interval closure="open" id="S4.SS3.p1.3.m3.2.2.1.2.cmml" xref="S4.SS3.p1.3.m3.2.2.1.1"><ci id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1">𝜏</ci><apply id="S4.SS3.p1.3.m3.2.2.1.1.1.cmml" xref="S4.SS3.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.3.m3.2.2.1.1.1.1.cmml" xref="S4.SS3.p1.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S4.SS3.p1.3.m3.2.2.1.1.1.2.cmml" xref="S4.SS3.p1.3.m3.2.2.1.1.1.2">𝜏</ci><apply id="S4.SS3.p1.3.m3.2.2.1.1.1.3.cmml" xref="S4.SS3.p1.3.m3.2.2.1.1.1.3"><times id="S4.SS3.p1.3.m3.2.2.1.1.1.3.1.cmml" xref="S4.SS3.p1.3.m3.2.2.1.1.1.3.1"></times><ci id="S4.SS3.p1.3.m3.2.2.1.1.1.3.2.cmml" xref="S4.SS3.p1.3.m3.2.2.1.1.1.3.2">𝑝</ci><ci id="S4.SS3.p1.3.m3.2.2.1.1.1.3.3.cmml" xref="S4.SS3.p1.3.m3.2.2.1.1.1.3.3">𝑢</ci><ci id="S4.SS3.p1.3.m3.2.2.1.1.1.3.4.cmml" xref="S4.SS3.p1.3.m3.2.2.1.1.1.3.4">𝑏</ci></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.2c">\text{{Remap}}(\tau,\tau_{pub})</annotation></semantics></math> in
Algorithm <a href="#alg1" title="Algorithm 1 ‣ 4.3 Adapting the language model to the new tokenizer ‣ 4 Learning a Tokenizer with Private Federated Learning ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.3" class="ltx_p"><span id="S4.SS3.p2.3.1" class="ltx_text ltx_font_smallcaps">Remap</span> iterates over the tokens from the new tokenizer <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mi id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><ci id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">\tau</annotation></semantics></math>
and creates the mapping from the tokens’ embedding in the public
tokenizer <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="\tau_{pub}" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><msub id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mi id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">τ</mi><mrow id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3.cmml"><mi id="S4.SS3.p2.2.m2.1.1.3.2" xref="S4.SS3.p2.2.m2.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.2.m2.1.1.3.1" xref="S4.SS3.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p2.2.m2.1.1.3.3" xref="S4.SS3.p2.2.m2.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.2.m2.1.1.3.1a" xref="S4.SS3.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p2.2.m2.1.1.3.4" xref="S4.SS3.p2.2.m2.1.1.3.4.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">𝜏</ci><apply id="S4.SS3.p2.2.m2.1.1.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3"><times id="S4.SS3.p2.2.m2.1.1.3.1.cmml" xref="S4.SS3.p2.2.m2.1.1.3.1"></times><ci id="S4.SS3.p2.2.m2.1.1.3.2.cmml" xref="S4.SS3.p2.2.m2.1.1.3.2">𝑝</ci><ci id="S4.SS3.p2.2.m2.1.1.3.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3.3">𝑢</ci><ci id="S4.SS3.p2.2.m2.1.1.3.4.cmml" xref="S4.SS3.p2.2.m2.1.1.3.4">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">\tau_{pub}</annotation></semantics></math> to the new token’s embedding. In some cases it is a one-to-one mapping, but
when the new token accumulates multiple tokens in <math id="S4.SS3.p2.3.m3.1" class="ltx_Math" alttext="\tau_{pub}" display="inline"><semantics id="S4.SS3.p2.3.m3.1a"><msub id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml"><mi id="S4.SS3.p2.3.m3.1.1.2" xref="S4.SS3.p2.3.m3.1.1.2.cmml">τ</mi><mrow id="S4.SS3.p2.3.m3.1.1.3" xref="S4.SS3.p2.3.m3.1.1.3.cmml"><mi id="S4.SS3.p2.3.m3.1.1.3.2" xref="S4.SS3.p2.3.m3.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.3.m3.1.1.3.1" xref="S4.SS3.p2.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p2.3.m3.1.1.3.3" xref="S4.SS3.p2.3.m3.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.3.m3.1.1.3.1a" xref="S4.SS3.p2.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p2.3.m3.1.1.3.4" xref="S4.SS3.p2.3.m3.1.1.3.4.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS3.p2.3.m3.1.1.2.cmml" xref="S4.SS3.p2.3.m3.1.1.2">𝜏</ci><apply id="S4.SS3.p2.3.m3.1.1.3.cmml" xref="S4.SS3.p2.3.m3.1.1.3"><times id="S4.SS3.p2.3.m3.1.1.3.1.cmml" xref="S4.SS3.p2.3.m3.1.1.3.1"></times><ci id="S4.SS3.p2.3.m3.1.1.3.2.cmml" xref="S4.SS3.p2.3.m3.1.1.3.2">𝑝</ci><ci id="S4.SS3.p2.3.m3.1.1.3.3.cmml" xref="S4.SS3.p2.3.m3.1.1.3.3">𝑢</ci><ci id="S4.SS3.p2.3.m3.1.1.3.4.cmml" xref="S4.SS3.p2.3.m3.1.1.3.4">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">\tau_{pub}</annotation></semantics></math> we split
the weight equally between each token.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.3" class="ltx_p">Once we have the mapping <math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="map" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mrow id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mi id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p3.1.m1.1.1.1" xref="S4.SS3.p3.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p3.1.m1.1.1.1a" xref="S4.SS3.p3.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS3.p3.1.m1.1.1.4" xref="S4.SS3.p3.1.m1.1.1.4.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><times id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1.1"></times><ci id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">𝑚</ci><ci id="S4.SS3.p3.1.m1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3">𝑎</ci><ci id="S4.SS3.p3.1.m1.1.1.4.cmml" xref="S4.SS3.p3.1.m1.1.1.4">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">map</annotation></semantics></math> we modify the embedding layer of the
model by performing matrix multiplication, i.e. <math id="S4.SS3.p3.2.m2.3" class="ltx_Math" alttext="\theta.\mathrm{embedding}=map\cdot\theta.\mathrm{embedding}" display="inline"><semantics id="S4.SS3.p3.2.m2.3a"><mrow id="S4.SS3.p3.2.m2.3.3.1" xref="S4.SS3.p3.2.m2.3.3.2.cmml"><mi id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml">θ</mi><mo lspace="0em" rspace="0.167em" id="S4.SS3.p3.2.m2.3.3.1.2" xref="S4.SS3.p3.2.m2.3.3.2a.cmml">.</mo><mrow id="S4.SS3.p3.2.m2.3.3.1.1" xref="S4.SS3.p3.2.m2.3.3.1.1.cmml"><mi id="S4.SS3.p3.2.m2.3.3.1.1.2" xref="S4.SS3.p3.2.m2.3.3.1.1.2.cmml">embedding</mi><mo id="S4.SS3.p3.2.m2.3.3.1.1.1" xref="S4.SS3.p3.2.m2.3.3.1.1.1.cmml">=</mo><mrow id="S4.SS3.p3.2.m2.3.3.1.1.3" xref="S4.SS3.p3.2.m2.3.3.1.1.3.cmml"><mrow id="S4.SS3.p3.2.m2.3.3.1.1.3.2" xref="S4.SS3.p3.2.m2.3.3.1.1.3.2.cmml"><mi id="S4.SS3.p3.2.m2.3.3.1.1.3.2.2" xref="S4.SS3.p3.2.m2.3.3.1.1.3.2.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p3.2.m2.3.3.1.1.3.2.1" xref="S4.SS3.p3.2.m2.3.3.1.1.3.2.1.cmml">​</mo><mi id="S4.SS3.p3.2.m2.3.3.1.1.3.2.3" xref="S4.SS3.p3.2.m2.3.3.1.1.3.2.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p3.2.m2.3.3.1.1.3.2.1a" xref="S4.SS3.p3.2.m2.3.3.1.1.3.2.1.cmml">​</mo><mi id="S4.SS3.p3.2.m2.3.3.1.1.3.2.4" xref="S4.SS3.p3.2.m2.3.3.1.1.3.2.4.cmml">p</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p3.2.m2.3.3.1.1.3.1" xref="S4.SS3.p3.2.m2.3.3.1.1.3.1.cmml">⋅</mo><mi id="S4.SS3.p3.2.m2.3.3.1.1.3.3" xref="S4.SS3.p3.2.m2.3.3.1.1.3.3.cmml">θ</mi></mrow></mrow><mo lspace="0em" rspace="0.167em" id="S4.SS3.p3.2.m2.3.3.1.3" xref="S4.SS3.p3.2.m2.3.3.2a.cmml">.</mo><mi id="S4.SS3.p3.2.m2.2.2" xref="S4.SS3.p3.2.m2.2.2.cmml">embedding</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.3b"><apply id="S4.SS3.p3.2.m2.3.3.2.cmml" xref="S4.SS3.p3.2.m2.3.3.1"><csymbol cd="ambiguous" id="S4.SS3.p3.2.m2.3.3.2a.cmml" xref="S4.SS3.p3.2.m2.3.3.1.2">formulae-sequence</csymbol><ci id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1">𝜃</ci><apply id="S4.SS3.p3.2.m2.3.3.1.1.cmml" xref="S4.SS3.p3.2.m2.3.3.1.1"><eq id="S4.SS3.p3.2.m2.3.3.1.1.1.cmml" xref="S4.SS3.p3.2.m2.3.3.1.1.1"></eq><ci id="S4.SS3.p3.2.m2.3.3.1.1.2.cmml" xref="S4.SS3.p3.2.m2.3.3.1.1.2">embedding</ci><apply id="S4.SS3.p3.2.m2.3.3.1.1.3.cmml" xref="S4.SS3.p3.2.m2.3.3.1.1.3"><ci id="S4.SS3.p3.2.m2.3.3.1.1.3.1.cmml" xref="S4.SS3.p3.2.m2.3.3.1.1.3.1">⋅</ci><apply id="S4.SS3.p3.2.m2.3.3.1.1.3.2.cmml" xref="S4.SS3.p3.2.m2.3.3.1.1.3.2"><times id="S4.SS3.p3.2.m2.3.3.1.1.3.2.1.cmml" xref="S4.SS3.p3.2.m2.3.3.1.1.3.2.1"></times><ci id="S4.SS3.p3.2.m2.3.3.1.1.3.2.2.cmml" xref="S4.SS3.p3.2.m2.3.3.1.1.3.2.2">𝑚</ci><ci id="S4.SS3.p3.2.m2.3.3.1.1.3.2.3.cmml" xref="S4.SS3.p3.2.m2.3.3.1.1.3.2.3">𝑎</ci><ci id="S4.SS3.p3.2.m2.3.3.1.1.3.2.4.cmml" xref="S4.SS3.p3.2.m2.3.3.1.1.3.2.4">𝑝</ci></apply><ci id="S4.SS3.p3.2.m2.3.3.1.1.3.3.cmml" xref="S4.SS3.p3.2.m2.3.3.1.1.3.3">𝜃</ci></apply></apply><ci id="S4.SS3.p3.2.m2.2.2.cmml" xref="S4.SS3.p3.2.m2.2.2">embedding</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.3c">\theta.\mathrm{embedding}=map\cdot\theta.\mathrm{embedding}</annotation></semantics></math>. The resulting model can accept the tokens from
the new tokenizer <math id="S4.SS3.p3.3.m3.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S4.SS3.p3.3.m3.1a"><mi id="S4.SS3.p3.3.m3.1.1" xref="S4.SS3.p3.3.m3.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m3.1b"><ci id="S4.SS3.p3.3.m3.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m3.1c">\tau</annotation></semantics></math>, and can participate in future training in
federated learning.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.2.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Model sampling algorithm</figcaption>
<div id="alg1.3" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span id="alg1.l1.1" class="ltx_text ltx_font_bold ltx_font_italic">Inputs:</span> model <math id="alg1.l1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="alg1.l1.m1.1a"><mi id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">\theta</annotation></semantics></math>, current sentence <math id="alg1.l1.m2.1" class="ltx_Math" alttext="s" display="inline"><semantics id="alg1.l1.m2.1a"><mi id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><ci id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">s</annotation></semantics></math>, new
tokenizer <math id="alg1.l1.m3.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="alg1.l1.m3.1a"><mi id="alg1.l1.m3.1.1" xref="alg1.l1.m3.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m3.1b"><ci id="alg1.l1.m3.1.1.cmml" xref="alg1.l1.m3.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m3.1c">\tau</annotation></semantics></math>, public tokenizer <math id="alg1.l1.m4.1" class="ltx_Math" alttext="\tau_{pub}" display="inline"><semantics id="alg1.l1.m4.1a"><msub id="alg1.l1.m4.1.1" xref="alg1.l1.m4.1.1.cmml"><mi id="alg1.l1.m4.1.1.2" xref="alg1.l1.m4.1.1.2.cmml">τ</mi><mrow id="alg1.l1.m4.1.1.3" xref="alg1.l1.m4.1.1.3.cmml"><mi id="alg1.l1.m4.1.1.3.2" xref="alg1.l1.m4.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="alg1.l1.m4.1.1.3.1" xref="alg1.l1.m4.1.1.3.1.cmml">​</mo><mi id="alg1.l1.m4.1.1.3.3" xref="alg1.l1.m4.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="alg1.l1.m4.1.1.3.1a" xref="alg1.l1.m4.1.1.3.1.cmml">​</mo><mi id="alg1.l1.m4.1.1.3.4" xref="alg1.l1.m4.1.1.3.4.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="alg1.l1.m4.1b"><apply id="alg1.l1.m4.1.1.cmml" xref="alg1.l1.m4.1.1"><csymbol cd="ambiguous" id="alg1.l1.m4.1.1.1.cmml" xref="alg1.l1.m4.1.1">subscript</csymbol><ci id="alg1.l1.m4.1.1.2.cmml" xref="alg1.l1.m4.1.1.2">𝜏</ci><apply id="alg1.l1.m4.1.1.3.cmml" xref="alg1.l1.m4.1.1.3"><times id="alg1.l1.m4.1.1.3.1.cmml" xref="alg1.l1.m4.1.1.3.1"></times><ci id="alg1.l1.m4.1.1.3.2.cmml" xref="alg1.l1.m4.1.1.3.2">𝑝</ci><ci id="alg1.l1.m4.1.1.3.3.cmml" xref="alg1.l1.m4.1.1.3.3">𝑢</ci><ci id="alg1.l1.m4.1.1.3.4.cmml" xref="alg1.l1.m4.1.1.3.4">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m4.1c">\tau_{pub}</annotation></semantics></math>, size of the sampled
dataset <math id="alg1.l1.m5.1" class="ltx_Math" alttext="\mathrm{corpus\_size}" display="inline"><semantics id="alg1.l1.m5.1a"><mrow id="alg1.l1.m5.1.1" xref="alg1.l1.m5.1.1.cmml"><mi id="alg1.l1.m5.1.1.2" xref="alg1.l1.m5.1.1.2.cmml">corpus</mi><mo lspace="0em" rspace="0em" id="alg1.l1.m5.1.1.1" xref="alg1.l1.m5.1.1.1.cmml">​</mo><mi mathvariant="normal" id="alg1.l1.m5.1.1.3" xref="alg1.l1.m5.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="alg1.l1.m5.1.1.1a" xref="alg1.l1.m5.1.1.1.cmml">​</mo><mi id="alg1.l1.m5.1.1.4" xref="alg1.l1.m5.1.1.4.cmml">size</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l1.m5.1b"><apply id="alg1.l1.m5.1.1.cmml" xref="alg1.l1.m5.1.1"><times id="alg1.l1.m5.1.1.1.cmml" xref="alg1.l1.m5.1.1.1"></times><ci id="alg1.l1.m5.1.1.2.cmml" xref="alg1.l1.m5.1.1.2">corpus</ci><ci id="alg1.l1.m5.1.1.3.cmml" xref="alg1.l1.m5.1.1.3">_</ci><ci id="alg1.l1.m5.1.1.4.cmml" xref="alg1.l1.m5.1.1.4">size</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m5.1c">\mathrm{corpus\_size}</annotation></semantics></math>.


</div>
<div id="alg1.l2" class="ltx_listingline">
<span id="alg1.l2.1" class="ltx_text ltx_font_bold">function</span> <span id="alg1.l2.2" class="ltx_text ltx_font_smallcaps">SampleTokens</span>(<math id="alg1.l2.m1.2" class="ltx_Math" alttext="\theta,s" display="inline"><semantics id="alg1.l2.m1.2a"><mrow id="alg1.l2.m1.2.3.2" xref="alg1.l2.m1.2.3.1.cmml"><mi id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml">θ</mi><mo id="alg1.l2.m1.2.3.2.1" xref="alg1.l2.m1.2.3.1.cmml">,</mo><mi id="alg1.l2.m1.2.2" xref="alg1.l2.m1.2.2.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.2b"><list id="alg1.l2.m1.2.3.1.cmml" xref="alg1.l2.m1.2.3.2"><ci id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1">𝜃</ci><ci id="alg1.l2.m1.2.2.cmml" xref="alg1.l2.m1.2.2">𝑠</ci></list></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.2c">\theta,s</annotation></semantics></math>)

</div>
<div id="alg1.l3" class="ltx_listingline">     <math id="alg1.l3.m1.1" class="ltx_Math" alttext="t_{next}\sim_{\theta}t_{k}|s" display="inline"><semantics id="alg1.l3.m1.1a"><mrow id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><msub id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2.cmml"><mi id="alg1.l3.m1.1.1.2.2" xref="alg1.l3.m1.1.1.2.2.cmml">t</mi><mrow id="alg1.l3.m1.1.1.2.3" xref="alg1.l3.m1.1.1.2.3.cmml"><mi id="alg1.l3.m1.1.1.2.3.2" xref="alg1.l3.m1.1.1.2.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="alg1.l3.m1.1.1.2.3.1" xref="alg1.l3.m1.1.1.2.3.1.cmml">​</mo><mi id="alg1.l3.m1.1.1.2.3.3" xref="alg1.l3.m1.1.1.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l3.m1.1.1.2.3.1a" xref="alg1.l3.m1.1.1.2.3.1.cmml">​</mo><mi id="alg1.l3.m1.1.1.2.3.4" xref="alg1.l3.m1.1.1.2.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="alg1.l3.m1.1.1.2.3.1b" xref="alg1.l3.m1.1.1.2.3.1.cmml">​</mo><mi id="alg1.l3.m1.1.1.2.3.5" xref="alg1.l3.m1.1.1.2.3.5.cmml">t</mi></mrow></msub><msub id="alg1.l3.m1.1.1.1" xref="alg1.l3.m1.1.1.1.cmml"><mo id="alg1.l3.m1.1.1.1.2" xref="alg1.l3.m1.1.1.1.2.cmml">∼</mo><mi id="alg1.l3.m1.1.1.1.3" xref="alg1.l3.m1.1.1.1.3.cmml">θ</mi></msub><mrow id="alg1.l3.m1.1.1.3" xref="alg1.l3.m1.1.1.3.cmml"><msub id="alg1.l3.m1.1.1.3.2" xref="alg1.l3.m1.1.1.3.2.cmml"><mi id="alg1.l3.m1.1.1.3.2.2" xref="alg1.l3.m1.1.1.3.2.2.cmml">t</mi><mi id="alg1.l3.m1.1.1.3.2.3" xref="alg1.l3.m1.1.1.3.2.3.cmml">k</mi></msub><mo fence="false" id="alg1.l3.m1.1.1.3.1" xref="alg1.l3.m1.1.1.3.1.cmml">|</mo><mi id="alg1.l3.m1.1.1.3.3" xref="alg1.l3.m1.1.1.3.3.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><apply id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1.1"><csymbol cd="ambiguous" id="alg1.l3.m1.1.1.1.1.cmml" xref="alg1.l3.m1.1.1.1">subscript</csymbol><csymbol cd="latexml" id="alg1.l3.m1.1.1.1.2.cmml" xref="alg1.l3.m1.1.1.1.2">similar-to</csymbol><ci id="alg1.l3.m1.1.1.1.3.cmml" xref="alg1.l3.m1.1.1.1.3">𝜃</ci></apply><apply id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l3.m1.1.1.2.1.cmml" xref="alg1.l3.m1.1.1.2">subscript</csymbol><ci id="alg1.l3.m1.1.1.2.2.cmml" xref="alg1.l3.m1.1.1.2.2">𝑡</ci><apply id="alg1.l3.m1.1.1.2.3.cmml" xref="alg1.l3.m1.1.1.2.3"><times id="alg1.l3.m1.1.1.2.3.1.cmml" xref="alg1.l3.m1.1.1.2.3.1"></times><ci id="alg1.l3.m1.1.1.2.3.2.cmml" xref="alg1.l3.m1.1.1.2.3.2">𝑛</ci><ci id="alg1.l3.m1.1.1.2.3.3.cmml" xref="alg1.l3.m1.1.1.2.3.3">𝑒</ci><ci id="alg1.l3.m1.1.1.2.3.4.cmml" xref="alg1.l3.m1.1.1.2.3.4">𝑥</ci><ci id="alg1.l3.m1.1.1.2.3.5.cmml" xref="alg1.l3.m1.1.1.2.3.5">𝑡</ci></apply></apply><apply id="alg1.l3.m1.1.1.3.cmml" xref="alg1.l3.m1.1.1.3"><csymbol cd="latexml" id="alg1.l3.m1.1.1.3.1.cmml" xref="alg1.l3.m1.1.1.3.1">conditional</csymbol><apply id="alg1.l3.m1.1.1.3.2.cmml" xref="alg1.l3.m1.1.1.3.2"><csymbol cd="ambiguous" id="alg1.l3.m1.1.1.3.2.1.cmml" xref="alg1.l3.m1.1.1.3.2">subscript</csymbol><ci id="alg1.l3.m1.1.1.3.2.2.cmml" xref="alg1.l3.m1.1.1.3.2.2">𝑡</ci><ci id="alg1.l3.m1.1.1.3.2.3.cmml" xref="alg1.l3.m1.1.1.3.2.3">𝑘</ci></apply><ci id="alg1.l3.m1.1.1.3.3.cmml" xref="alg1.l3.m1.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">t_{next}\sim_{\theta}t_{k}|s</annotation></semantics></math>

</div>
<div id="alg1.l4" class="ltx_listingline">     <span id="alg1.l4.1" class="ltx_text ltx_font_bold">if</span> <math id="alg1.l4.m1.1" class="ltx_Math" alttext="t_{next}=\texttt{EOS}" display="inline"><semantics id="alg1.l4.m1.1a"><mrow id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml"><msub id="alg1.l4.m1.1.1.2" xref="alg1.l4.m1.1.1.2.cmml"><mi id="alg1.l4.m1.1.1.2.2" xref="alg1.l4.m1.1.1.2.2.cmml">t</mi><mrow id="alg1.l4.m1.1.1.2.3" xref="alg1.l4.m1.1.1.2.3.cmml"><mi id="alg1.l4.m1.1.1.2.3.2" xref="alg1.l4.m1.1.1.2.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="alg1.l4.m1.1.1.2.3.1" xref="alg1.l4.m1.1.1.2.3.1.cmml">​</mo><mi id="alg1.l4.m1.1.1.2.3.3" xref="alg1.l4.m1.1.1.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l4.m1.1.1.2.3.1a" xref="alg1.l4.m1.1.1.2.3.1.cmml">​</mo><mi id="alg1.l4.m1.1.1.2.3.4" xref="alg1.l4.m1.1.1.2.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="alg1.l4.m1.1.1.2.3.1b" xref="alg1.l4.m1.1.1.2.3.1.cmml">​</mo><mi id="alg1.l4.m1.1.1.2.3.5" xref="alg1.l4.m1.1.1.2.3.5.cmml">t</mi></mrow></msub><mo id="alg1.l4.m1.1.1.1" xref="alg1.l4.m1.1.1.1.cmml">=</mo><mtext class="ltx_mathvariant_monospace" id="alg1.l4.m1.1.1.3" xref="alg1.l4.m1.1.1.3a.cmml">EOS</mtext></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><apply id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1"><eq id="alg1.l4.m1.1.1.1.cmml" xref="alg1.l4.m1.1.1.1"></eq><apply id="alg1.l4.m1.1.1.2.cmml" xref="alg1.l4.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l4.m1.1.1.2.1.cmml" xref="alg1.l4.m1.1.1.2">subscript</csymbol><ci id="alg1.l4.m1.1.1.2.2.cmml" xref="alg1.l4.m1.1.1.2.2">𝑡</ci><apply id="alg1.l4.m1.1.1.2.3.cmml" xref="alg1.l4.m1.1.1.2.3"><times id="alg1.l4.m1.1.1.2.3.1.cmml" xref="alg1.l4.m1.1.1.2.3.1"></times><ci id="alg1.l4.m1.1.1.2.3.2.cmml" xref="alg1.l4.m1.1.1.2.3.2">𝑛</ci><ci id="alg1.l4.m1.1.1.2.3.3.cmml" xref="alg1.l4.m1.1.1.2.3.3">𝑒</ci><ci id="alg1.l4.m1.1.1.2.3.4.cmml" xref="alg1.l4.m1.1.1.2.3.4">𝑥</ci><ci id="alg1.l4.m1.1.1.2.3.5.cmml" xref="alg1.l4.m1.1.1.2.3.5">𝑡</ci></apply></apply><ci id="alg1.l4.m1.1.1.3a.cmml" xref="alg1.l4.m1.1.1.3"><mtext class="ltx_mathvariant_monospace" id="alg1.l4.m1.1.1.3.cmml" xref="alg1.l4.m1.1.1.3">EOS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">t_{next}=\texttt{EOS}</annotation></semantics></math> <span id="alg1.l4.2" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg1.l5" class="ltx_listingline">         <span id="alg1.l5.1" class="ltx_text ltx_font_bold">return</span> <math id="alg1.l5.m1.1" class="ltx_math_unparsed" alttext="s+\!\!\!+\,t_{next}" display="inline"><semantics id="alg1.l5.m1.1a"><mrow id="alg1.l5.m1.1b"><mi id="alg1.l5.m1.1.1">s</mi><mpadded width="0.278em" id="alg1.l5.m1.1c"><mo id="alg1.l5.m1.1.2">+</mo></mpadded><mo rspace="0.392em" id="alg1.l5.m1.1.3">+</mo><msub id="alg1.l5.m1.1.4"><mi id="alg1.l5.m1.1.4.2">t</mi><mrow id="alg1.l5.m1.1.4.3"><mi id="alg1.l5.m1.1.4.3.2">n</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.4.3.1">​</mo><mi id="alg1.l5.m1.1.4.3.3">e</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.4.3.1a">​</mo><mi id="alg1.l5.m1.1.4.3.4">x</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.4.3.1b">​</mo><mi id="alg1.l5.m1.1.4.3.5">t</mi></mrow></msub></mrow><annotation encoding="application/x-tex" id="alg1.l5.m1.1d">s+\!\!\!+\,t_{next}</annotation></semantics></math>

</div>
<div id="alg1.l6" class="ltx_listingline">     <span id="alg1.l6.1" class="ltx_text ltx_font_bold">else</span>
</div>
<div id="alg1.l7" class="ltx_listingline">         <span id="alg1.l7.1" class="ltx_text ltx_font_bold">return</span> <span id="alg1.l7.2" class="ltx_text ltx_font_smallcaps">SampleTokens</span>(<math id="alg1.l7.m1.1" class="ltx_math_unparsed" alttext="\theta,s+\!\!\!+\,t_{next}" display="inline"><semantics id="alg1.l7.m1.1a"><mrow id="alg1.l7.m1.1b"><mi id="alg1.l7.m1.1.1">θ</mi><mo id="alg1.l7.m1.1.2">,</mo><mi id="alg1.l7.m1.1.3">s</mi><mpadded width="0.278em" id="alg1.l7.m1.1c"><mo id="alg1.l7.m1.1.4">+</mo></mpadded><mo rspace="0.392em" id="alg1.l7.m1.1.5">+</mo><msub id="alg1.l7.m1.1.6"><mi id="alg1.l7.m1.1.6.2">t</mi><mrow id="alg1.l7.m1.1.6.3"><mi id="alg1.l7.m1.1.6.3.2">n</mi><mo lspace="0em" rspace="0em" id="alg1.l7.m1.1.6.3.1">​</mo><mi id="alg1.l7.m1.1.6.3.3">e</mi><mo lspace="0em" rspace="0em" id="alg1.l7.m1.1.6.3.1a">​</mo><mi id="alg1.l7.m1.1.6.3.4">x</mi><mo lspace="0em" rspace="0em" id="alg1.l7.m1.1.6.3.1b">​</mo><mi id="alg1.l7.m1.1.6.3.5">t</mi></mrow></msub></mrow><annotation encoding="application/x-tex" id="alg1.l7.m1.1d">\theta,s+\!\!\!+\,t_{next}</annotation></semantics></math>)
     

</div>
<div id="alg1.l8" class="ltx_listingline">
<span id="alg1.l8.1" class="ltx_text ltx_font_bold">function</span> <span id="alg1.l8.2" class="ltx_text ltx_font_smallcaps">Sample</span>(<math id="alg1.l8.m1.2" class="ltx_Math" alttext="\theta,\tau" display="inline"><semantics id="alg1.l8.m1.2a"><mrow id="alg1.l8.m1.2.3.2" xref="alg1.l8.m1.2.3.1.cmml"><mi id="alg1.l8.m1.1.1" xref="alg1.l8.m1.1.1.cmml">θ</mi><mo id="alg1.l8.m1.2.3.2.1" xref="alg1.l8.m1.2.3.1.cmml">,</mo><mi id="alg1.l8.m1.2.2" xref="alg1.l8.m1.2.2.cmml">τ</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l8.m1.2b"><list id="alg1.l8.m1.2.3.1.cmml" xref="alg1.l8.m1.2.3.2"><ci id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1">𝜃</ci><ci id="alg1.l8.m1.2.2.cmml" xref="alg1.l8.m1.2.2">𝜏</ci></list></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m1.2c">\theta,\tau</annotation></semantics></math>)

</div>
<div id="alg1.l9" class="ltx_listingline">     <span id="alg1.l9.1" class="ltx_text ltx_font_bold">return</span> <math id="alg1.l9.m1.2" class="ltx_math_unparsed" alttext="\tau.\mathrm{decode}(" display="inline"><semantics id="alg1.l9.m1.2a"><mrow id="alg1.l9.m1.2b"><mi id="alg1.l9.m1.1.1">τ</mi><mo lspace="0em" rspace="0.167em" id="alg1.l9.m1.2.3">.</mo><mi id="alg1.l9.m1.2.2">decode</mi><mo stretchy="false" id="alg1.l9.m1.2.4">(</mo></mrow><annotation encoding="application/x-tex" id="alg1.l9.m1.2c">\tau.\mathrm{decode}(</annotation></semantics></math>

</div>
<div id="alg1.l10" class="ltx_listingline">     <math id="alg1.l10.m1.2" class="ltx_math_unparsed" alttext="\qquad\text{{SampleTokens}}(\theta,[\texttt{BOS}]))" display="inline"><semantics id="alg1.l10.m1.2a"><mrow id="alg1.l10.m1.2b"><mtext class="ltx_font_smallcaps" id="alg1.l10.m1.2.3">SampleTokens</mtext><mrow id="alg1.l10.m1.2.4"><mo stretchy="false" id="alg1.l10.m1.2.4.1">(</mo><mi id="alg1.l10.m1.2.2">θ</mi><mo id="alg1.l10.m1.2.4.2">,</mo><mrow id="alg1.l10.m1.2.4.3"><mo stretchy="false" id="alg1.l10.m1.2.4.3.1">[</mo><mtext class="ltx_mathvariant_monospace" id="alg1.l10.m1.1.1">BOS</mtext><mo stretchy="false" id="alg1.l10.m1.2.4.3.2">]</mo></mrow><mo stretchy="false" id="alg1.l10.m1.2.4.4">)</mo></mrow><mo stretchy="false" id="alg1.l10.m1.2.5">)</mo></mrow><annotation encoding="application/x-tex" id="alg1.l10.m1.2c">\qquad\text{{SampleTokens}}(\theta,[\texttt{BOS}]))</annotation></semantics></math>


</div>
<div id="alg1.l11" class="ltx_listingline">
<span id="alg1.l11.1" class="ltx_text ltx_font_bold">function</span> <span id="alg1.l11.2" class="ltx_text ltx_font_smallcaps">Remap</span>(<math id="alg1.l11.m1.2" class="ltx_Math" alttext="\tau_{pub},\tau" display="inline"><semantics id="alg1.l11.m1.2a"><mrow id="alg1.l11.m1.2.2.1" xref="alg1.l11.m1.2.2.2.cmml"><msub id="alg1.l11.m1.2.2.1.1" xref="alg1.l11.m1.2.2.1.1.cmml"><mi id="alg1.l11.m1.2.2.1.1.2" xref="alg1.l11.m1.2.2.1.1.2.cmml">τ</mi><mrow id="alg1.l11.m1.2.2.1.1.3" xref="alg1.l11.m1.2.2.1.1.3.cmml"><mi id="alg1.l11.m1.2.2.1.1.3.2" xref="alg1.l11.m1.2.2.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.2.2.1.1.3.1" xref="alg1.l11.m1.2.2.1.1.3.1.cmml">​</mo><mi id="alg1.l11.m1.2.2.1.1.3.3" xref="alg1.l11.m1.2.2.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.2.2.1.1.3.1a" xref="alg1.l11.m1.2.2.1.1.3.1.cmml">​</mo><mi id="alg1.l11.m1.2.2.1.1.3.4" xref="alg1.l11.m1.2.2.1.1.3.4.cmml">b</mi></mrow></msub><mo id="alg1.l11.m1.2.2.1.2" xref="alg1.l11.m1.2.2.2.cmml">,</mo><mi id="alg1.l11.m1.1.1" xref="alg1.l11.m1.1.1.cmml">τ</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.2b"><list id="alg1.l11.m1.2.2.2.cmml" xref="alg1.l11.m1.2.2.1"><apply id="alg1.l11.m1.2.2.1.1.cmml" xref="alg1.l11.m1.2.2.1.1"><csymbol cd="ambiguous" id="alg1.l11.m1.2.2.1.1.1.cmml" xref="alg1.l11.m1.2.2.1.1">subscript</csymbol><ci id="alg1.l11.m1.2.2.1.1.2.cmml" xref="alg1.l11.m1.2.2.1.1.2">𝜏</ci><apply id="alg1.l11.m1.2.2.1.1.3.cmml" xref="alg1.l11.m1.2.2.1.1.3"><times id="alg1.l11.m1.2.2.1.1.3.1.cmml" xref="alg1.l11.m1.2.2.1.1.3.1"></times><ci id="alg1.l11.m1.2.2.1.1.3.2.cmml" xref="alg1.l11.m1.2.2.1.1.3.2">𝑝</ci><ci id="alg1.l11.m1.2.2.1.1.3.3.cmml" xref="alg1.l11.m1.2.2.1.1.3.3">𝑢</ci><ci id="alg1.l11.m1.2.2.1.1.3.4.cmml" xref="alg1.l11.m1.2.2.1.1.3.4">𝑏</ci></apply></apply><ci id="alg1.l11.m1.1.1.cmml" xref="alg1.l11.m1.1.1">𝜏</ci></list></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.2c">\tau_{pub},\tau</annotation></semantics></math>)

</div>
<div id="alg1.l12" class="ltx_listingline">     <math id="alg1.l12.m1.1" class="ltx_math_unparsed" alttext="\mathrm{map}=\mathrm{zeros}(\tau.\mathrm{size},\tau_{pub}.\mathrm{size})" display="inline"><semantics id="alg1.l12.m1.1a"><mrow id="alg1.l12.m1.1b"><mi id="alg1.l12.m1.1.1">map</mi><mo id="alg1.l12.m1.1.2">=</mo><mi id="alg1.l12.m1.1.3">zeros</mi><mrow id="alg1.l12.m1.1.4"><mo stretchy="false" id="alg1.l12.m1.1.4.1">(</mo><mi id="alg1.l12.m1.1.4.2">τ</mi><mo lspace="0em" rspace="0.167em" id="alg1.l12.m1.1.4.3">.</mo><mi id="alg1.l12.m1.1.4.4">size</mi><mo id="alg1.l12.m1.1.4.5">,</mo><msub id="alg1.l12.m1.1.4.6"><mi id="alg1.l12.m1.1.4.6.2">τ</mi><mrow id="alg1.l12.m1.1.4.6.3"><mi id="alg1.l12.m1.1.4.6.3.2">p</mi><mo lspace="0em" rspace="0em" id="alg1.l12.m1.1.4.6.3.1">​</mo><mi id="alg1.l12.m1.1.4.6.3.3">u</mi><mo lspace="0em" rspace="0em" id="alg1.l12.m1.1.4.6.3.1a">​</mo><mi id="alg1.l12.m1.1.4.6.3.4">b</mi></mrow></msub><mo lspace="0em" rspace="0.167em" id="alg1.l12.m1.1.4.7">.</mo><mi id="alg1.l12.m1.1.4.8">size</mi><mo stretchy="false" id="alg1.l12.m1.1.4.9">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="alg1.l12.m1.1c">\mathrm{map}=\mathrm{zeros}(\tau.\mathrm{size},\tau_{pub}.\mathrm{size})</annotation></semantics></math>

</div>
<div id="alg1.l13" class="ltx_listingline">     <span id="alg1.l13.1" class="ltx_text ltx_font_bold">for</span> <math id="alg1.l13.m1.4" class="ltx_Math" alttext="\mathrm{token},\mathrm{tid}\leftarrow\tau.\mathrm{vocab}" display="inline"><semantics id="alg1.l13.m1.4a"><mrow id="alg1.l13.m1.4.4.1" xref="alg1.l13.m1.4.4.2.cmml"><mrow id="alg1.l13.m1.4.4.1.1" xref="alg1.l13.m1.4.4.1.1.cmml"><mrow id="alg1.l13.m1.4.4.1.1.2.2" xref="alg1.l13.m1.4.4.1.1.2.1.cmml"><mi id="alg1.l13.m1.1.1" xref="alg1.l13.m1.1.1.cmml">token</mi><mo id="alg1.l13.m1.4.4.1.1.2.2.1" xref="alg1.l13.m1.4.4.1.1.2.1.cmml">,</mo><mi id="alg1.l13.m1.2.2" xref="alg1.l13.m1.2.2.cmml">tid</mi></mrow><mo stretchy="false" id="alg1.l13.m1.4.4.1.1.1" xref="alg1.l13.m1.4.4.1.1.1.cmml">←</mo><mi id="alg1.l13.m1.4.4.1.1.3" xref="alg1.l13.m1.4.4.1.1.3.cmml">τ</mi></mrow><mo lspace="0em" rspace="0.167em" id="alg1.l13.m1.4.4.1.2" xref="alg1.l13.m1.4.4.2a.cmml">.</mo><mi id="alg1.l13.m1.3.3" xref="alg1.l13.m1.3.3.cmml">vocab</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l13.m1.4b"><apply id="alg1.l13.m1.4.4.2.cmml" xref="alg1.l13.m1.4.4.1"><csymbol cd="ambiguous" id="alg1.l13.m1.4.4.2a.cmml" xref="alg1.l13.m1.4.4.1.2">formulae-sequence</csymbol><apply id="alg1.l13.m1.4.4.1.1.cmml" xref="alg1.l13.m1.4.4.1.1"><ci id="alg1.l13.m1.4.4.1.1.1.cmml" xref="alg1.l13.m1.4.4.1.1.1">←</ci><list id="alg1.l13.m1.4.4.1.1.2.1.cmml" xref="alg1.l13.m1.4.4.1.1.2.2"><ci id="alg1.l13.m1.1.1.cmml" xref="alg1.l13.m1.1.1">token</ci><ci id="alg1.l13.m1.2.2.cmml" xref="alg1.l13.m1.2.2">tid</ci></list><ci id="alg1.l13.m1.4.4.1.1.3.cmml" xref="alg1.l13.m1.4.4.1.1.3">𝜏</ci></apply><ci id="alg1.l13.m1.3.3.cmml" xref="alg1.l13.m1.3.3">vocab</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l13.m1.4c">\mathrm{token},\mathrm{tid}\leftarrow\tau.\mathrm{vocab}</annotation></semantics></math> <span id="alg1.l13.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l14" class="ltx_listingline">         <math id="alg1.l14.m1.3" class="ltx_Math" alttext="\mathrm{tokens}=\tau_{pub}.\mathrm{decode}(\mathrm{token})" display="inline"><semantics id="alg1.l14.m1.3a"><mrow id="alg1.l14.m1.3.3.2" xref="alg1.l14.m1.3.3.3.cmml"><mrow id="alg1.l14.m1.2.2.1.1" xref="alg1.l14.m1.2.2.1.1.cmml"><mi id="alg1.l14.m1.2.2.1.1.2" xref="alg1.l14.m1.2.2.1.1.2.cmml">tokens</mi><mo id="alg1.l14.m1.2.2.1.1.1" xref="alg1.l14.m1.2.2.1.1.1.cmml">=</mo><msub id="alg1.l14.m1.2.2.1.1.3" xref="alg1.l14.m1.2.2.1.1.3.cmml"><mi id="alg1.l14.m1.2.2.1.1.3.2" xref="alg1.l14.m1.2.2.1.1.3.2.cmml">τ</mi><mrow id="alg1.l14.m1.2.2.1.1.3.3" xref="alg1.l14.m1.2.2.1.1.3.3.cmml"><mi id="alg1.l14.m1.2.2.1.1.3.3.2" xref="alg1.l14.m1.2.2.1.1.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="alg1.l14.m1.2.2.1.1.3.3.1" xref="alg1.l14.m1.2.2.1.1.3.3.1.cmml">​</mo><mi id="alg1.l14.m1.2.2.1.1.3.3.3" xref="alg1.l14.m1.2.2.1.1.3.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="alg1.l14.m1.2.2.1.1.3.3.1a" xref="alg1.l14.m1.2.2.1.1.3.3.1.cmml">​</mo><mi id="alg1.l14.m1.2.2.1.1.3.3.4" xref="alg1.l14.m1.2.2.1.1.3.3.4.cmml">b</mi></mrow></msub></mrow><mo lspace="0em" rspace="0.167em" id="alg1.l14.m1.3.3.2.3" xref="alg1.l14.m1.3.3.3a.cmml">.</mo><mrow id="alg1.l14.m1.3.3.2.2" xref="alg1.l14.m1.3.3.2.2.cmml"><mi id="alg1.l14.m1.3.3.2.2.2" xref="alg1.l14.m1.3.3.2.2.2.cmml">decode</mi><mo lspace="0em" rspace="0em" id="alg1.l14.m1.3.3.2.2.1" xref="alg1.l14.m1.3.3.2.2.1.cmml">​</mo><mrow id="alg1.l14.m1.3.3.2.2.3.2" xref="alg1.l14.m1.3.3.2.2.cmml"><mo stretchy="false" id="alg1.l14.m1.3.3.2.2.3.2.1" xref="alg1.l14.m1.3.3.2.2.cmml">(</mo><mi id="alg1.l14.m1.1.1" xref="alg1.l14.m1.1.1.cmml">token</mi><mo stretchy="false" id="alg1.l14.m1.3.3.2.2.3.2.2" xref="alg1.l14.m1.3.3.2.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l14.m1.3b"><apply id="alg1.l14.m1.3.3.3.cmml" xref="alg1.l14.m1.3.3.2"><csymbol cd="ambiguous" id="alg1.l14.m1.3.3.3a.cmml" xref="alg1.l14.m1.3.3.2.3">formulae-sequence</csymbol><apply id="alg1.l14.m1.2.2.1.1.cmml" xref="alg1.l14.m1.2.2.1.1"><eq id="alg1.l14.m1.2.2.1.1.1.cmml" xref="alg1.l14.m1.2.2.1.1.1"></eq><ci id="alg1.l14.m1.2.2.1.1.2.cmml" xref="alg1.l14.m1.2.2.1.1.2">tokens</ci><apply id="alg1.l14.m1.2.2.1.1.3.cmml" xref="alg1.l14.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="alg1.l14.m1.2.2.1.1.3.1.cmml" xref="alg1.l14.m1.2.2.1.1.3">subscript</csymbol><ci id="alg1.l14.m1.2.2.1.1.3.2.cmml" xref="alg1.l14.m1.2.2.1.1.3.2">𝜏</ci><apply id="alg1.l14.m1.2.2.1.1.3.3.cmml" xref="alg1.l14.m1.2.2.1.1.3.3"><times id="alg1.l14.m1.2.2.1.1.3.3.1.cmml" xref="alg1.l14.m1.2.2.1.1.3.3.1"></times><ci id="alg1.l14.m1.2.2.1.1.3.3.2.cmml" xref="alg1.l14.m1.2.2.1.1.3.3.2">𝑝</ci><ci id="alg1.l14.m1.2.2.1.1.3.3.3.cmml" xref="alg1.l14.m1.2.2.1.1.3.3.3">𝑢</ci><ci id="alg1.l14.m1.2.2.1.1.3.3.4.cmml" xref="alg1.l14.m1.2.2.1.1.3.3.4">𝑏</ci></apply></apply></apply><apply id="alg1.l14.m1.3.3.2.2.cmml" xref="alg1.l14.m1.3.3.2.2"><times id="alg1.l14.m1.3.3.2.2.1.cmml" xref="alg1.l14.m1.3.3.2.2.1"></times><ci id="alg1.l14.m1.3.3.2.2.2.cmml" xref="alg1.l14.m1.3.3.2.2.2">decode</ci><ci id="alg1.l14.m1.1.1.cmml" xref="alg1.l14.m1.1.1">token</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l14.m1.3c">\mathrm{tokens}=\tau_{pub}.\mathrm{decode}(\mathrm{token})</annotation></semantics></math>

</div>
<div id="alg1.l15" class="ltx_listingline">         <span id="alg1.l15.1" class="ltx_text ltx_font_bold">for</span> <math id="alg1.l15.m1.1" class="ltx_Math" alttext="\mathrm{token}\leftarrow\mathrm{tokens}" display="inline"><semantics id="alg1.l15.m1.1a"><mrow id="alg1.l15.m1.1.1" xref="alg1.l15.m1.1.1.cmml"><mi id="alg1.l15.m1.1.1.2" xref="alg1.l15.m1.1.1.2.cmml">token</mi><mo stretchy="false" id="alg1.l15.m1.1.1.1" xref="alg1.l15.m1.1.1.1.cmml">←</mo><mi id="alg1.l15.m1.1.1.3" xref="alg1.l15.m1.1.1.3.cmml">tokens</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l15.m1.1b"><apply id="alg1.l15.m1.1.1.cmml" xref="alg1.l15.m1.1.1"><ci id="alg1.l15.m1.1.1.1.cmml" xref="alg1.l15.m1.1.1.1">←</ci><ci id="alg1.l15.m1.1.1.2.cmml" xref="alg1.l15.m1.1.1.2">token</ci><ci id="alg1.l15.m1.1.1.3.cmml" xref="alg1.l15.m1.1.1.3">tokens</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l15.m1.1c">\mathrm{token}\leftarrow\mathrm{tokens}</annotation></semantics></math> <span id="alg1.l15.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l16" class="ltx_listingline">              <math id="alg1.l16.m1.3" class="ltx_Math" alttext="\mathrm{tid}_{pub}=\tau_{pub}.\mathrm{vocab}[\mathrm{token}]" display="inline"><semantics id="alg1.l16.m1.3a"><mrow id="alg1.l16.m1.3.3.2" xref="alg1.l16.m1.3.3.3.cmml"><mrow id="alg1.l16.m1.2.2.1.1" xref="alg1.l16.m1.2.2.1.1.cmml"><msub id="alg1.l16.m1.2.2.1.1.2" xref="alg1.l16.m1.2.2.1.1.2.cmml"><mi id="alg1.l16.m1.2.2.1.1.2.2" xref="alg1.l16.m1.2.2.1.1.2.2.cmml">tid</mi><mrow id="alg1.l16.m1.2.2.1.1.2.3" xref="alg1.l16.m1.2.2.1.1.2.3.cmml"><mi id="alg1.l16.m1.2.2.1.1.2.3.2" xref="alg1.l16.m1.2.2.1.1.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="alg1.l16.m1.2.2.1.1.2.3.1" xref="alg1.l16.m1.2.2.1.1.2.3.1.cmml">​</mo><mi id="alg1.l16.m1.2.2.1.1.2.3.3" xref="alg1.l16.m1.2.2.1.1.2.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="alg1.l16.m1.2.2.1.1.2.3.1a" xref="alg1.l16.m1.2.2.1.1.2.3.1.cmml">​</mo><mi id="alg1.l16.m1.2.2.1.1.2.3.4" xref="alg1.l16.m1.2.2.1.1.2.3.4.cmml">b</mi></mrow></msub><mo id="alg1.l16.m1.2.2.1.1.1" xref="alg1.l16.m1.2.2.1.1.1.cmml">=</mo><msub id="alg1.l16.m1.2.2.1.1.3" xref="alg1.l16.m1.2.2.1.1.3.cmml"><mi id="alg1.l16.m1.2.2.1.1.3.2" xref="alg1.l16.m1.2.2.1.1.3.2.cmml">τ</mi><mrow id="alg1.l16.m1.2.2.1.1.3.3" xref="alg1.l16.m1.2.2.1.1.3.3.cmml"><mi id="alg1.l16.m1.2.2.1.1.3.3.2" xref="alg1.l16.m1.2.2.1.1.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="alg1.l16.m1.2.2.1.1.3.3.1" xref="alg1.l16.m1.2.2.1.1.3.3.1.cmml">​</mo><mi id="alg1.l16.m1.2.2.1.1.3.3.3" xref="alg1.l16.m1.2.2.1.1.3.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="alg1.l16.m1.2.2.1.1.3.3.1a" xref="alg1.l16.m1.2.2.1.1.3.3.1.cmml">​</mo><mi id="alg1.l16.m1.2.2.1.1.3.3.4" xref="alg1.l16.m1.2.2.1.1.3.3.4.cmml">b</mi></mrow></msub></mrow><mo lspace="0em" rspace="0.167em" id="alg1.l16.m1.3.3.2.3" xref="alg1.l16.m1.3.3.3a.cmml">.</mo><mrow id="alg1.l16.m1.3.3.2.2" xref="alg1.l16.m1.3.3.2.2.cmml"><mi id="alg1.l16.m1.3.3.2.2.2" xref="alg1.l16.m1.3.3.2.2.2.cmml">vocab</mi><mo lspace="0em" rspace="0em" id="alg1.l16.m1.3.3.2.2.1" xref="alg1.l16.m1.3.3.2.2.1.cmml">​</mo><mrow id="alg1.l16.m1.3.3.2.2.3.2" xref="alg1.l16.m1.3.3.2.2.3.1.cmml"><mo stretchy="false" id="alg1.l16.m1.3.3.2.2.3.2.1" xref="alg1.l16.m1.3.3.2.2.3.1.1.cmml">[</mo><mi id="alg1.l16.m1.1.1" xref="alg1.l16.m1.1.1.cmml">token</mi><mo stretchy="false" id="alg1.l16.m1.3.3.2.2.3.2.2" xref="alg1.l16.m1.3.3.2.2.3.1.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l16.m1.3b"><apply id="alg1.l16.m1.3.3.3.cmml" xref="alg1.l16.m1.3.3.2"><csymbol cd="ambiguous" id="alg1.l16.m1.3.3.3a.cmml" xref="alg1.l16.m1.3.3.2.3">formulae-sequence</csymbol><apply id="alg1.l16.m1.2.2.1.1.cmml" xref="alg1.l16.m1.2.2.1.1"><eq id="alg1.l16.m1.2.2.1.1.1.cmml" xref="alg1.l16.m1.2.2.1.1.1"></eq><apply id="alg1.l16.m1.2.2.1.1.2.cmml" xref="alg1.l16.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="alg1.l16.m1.2.2.1.1.2.1.cmml" xref="alg1.l16.m1.2.2.1.1.2">subscript</csymbol><ci id="alg1.l16.m1.2.2.1.1.2.2.cmml" xref="alg1.l16.m1.2.2.1.1.2.2">tid</ci><apply id="alg1.l16.m1.2.2.1.1.2.3.cmml" xref="alg1.l16.m1.2.2.1.1.2.3"><times id="alg1.l16.m1.2.2.1.1.2.3.1.cmml" xref="alg1.l16.m1.2.2.1.1.2.3.1"></times><ci id="alg1.l16.m1.2.2.1.1.2.3.2.cmml" xref="alg1.l16.m1.2.2.1.1.2.3.2">𝑝</ci><ci id="alg1.l16.m1.2.2.1.1.2.3.3.cmml" xref="alg1.l16.m1.2.2.1.1.2.3.3">𝑢</ci><ci id="alg1.l16.m1.2.2.1.1.2.3.4.cmml" xref="alg1.l16.m1.2.2.1.1.2.3.4">𝑏</ci></apply></apply><apply id="alg1.l16.m1.2.2.1.1.3.cmml" xref="alg1.l16.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="alg1.l16.m1.2.2.1.1.3.1.cmml" xref="alg1.l16.m1.2.2.1.1.3">subscript</csymbol><ci id="alg1.l16.m1.2.2.1.1.3.2.cmml" xref="alg1.l16.m1.2.2.1.1.3.2">𝜏</ci><apply id="alg1.l16.m1.2.2.1.1.3.3.cmml" xref="alg1.l16.m1.2.2.1.1.3.3"><times id="alg1.l16.m1.2.2.1.1.3.3.1.cmml" xref="alg1.l16.m1.2.2.1.1.3.3.1"></times><ci id="alg1.l16.m1.2.2.1.1.3.3.2.cmml" xref="alg1.l16.m1.2.2.1.1.3.3.2">𝑝</ci><ci id="alg1.l16.m1.2.2.1.1.3.3.3.cmml" xref="alg1.l16.m1.2.2.1.1.3.3.3">𝑢</ci><ci id="alg1.l16.m1.2.2.1.1.3.3.4.cmml" xref="alg1.l16.m1.2.2.1.1.3.3.4">𝑏</ci></apply></apply></apply><apply id="alg1.l16.m1.3.3.2.2.cmml" xref="alg1.l16.m1.3.3.2.2"><times id="alg1.l16.m1.3.3.2.2.1.cmml" xref="alg1.l16.m1.3.3.2.2.1"></times><ci id="alg1.l16.m1.3.3.2.2.2.cmml" xref="alg1.l16.m1.3.3.2.2.2">vocab</ci><apply id="alg1.l16.m1.3.3.2.2.3.1.cmml" xref="alg1.l16.m1.3.3.2.2.3.2"><csymbol cd="latexml" id="alg1.l16.m1.3.3.2.2.3.1.1.cmml" xref="alg1.l16.m1.3.3.2.2.3.2.1">delimited-[]</csymbol><ci id="alg1.l16.m1.1.1.cmml" xref="alg1.l16.m1.1.1">token</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l16.m1.3c">\mathrm{tid}_{pub}=\tau_{pub}.\mathrm{vocab}[\mathrm{token}]</annotation></semantics></math>

</div>
<div id="alg1.l17" class="ltx_listingline">              <math id="alg1.l17.m1.3" class="ltx_Math" alttext="\mathrm{map}[\mathrm{tid}_{pub},\mathrm{tid}]=1/\mathrm{len}(\mathrm{tokens})" display="inline"><semantics id="alg1.l17.m1.3a"><mrow id="alg1.l17.m1.3.3" xref="alg1.l17.m1.3.3.cmml"><mrow id="alg1.l17.m1.3.3.1" xref="alg1.l17.m1.3.3.1.cmml"><mi id="alg1.l17.m1.3.3.1.3" xref="alg1.l17.m1.3.3.1.3.cmml">map</mi><mo lspace="0em" rspace="0em" id="alg1.l17.m1.3.3.1.2" xref="alg1.l17.m1.3.3.1.2.cmml">​</mo><mrow id="alg1.l17.m1.3.3.1.1.1" xref="alg1.l17.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="alg1.l17.m1.3.3.1.1.1.2" xref="alg1.l17.m1.3.3.1.1.2.cmml">[</mo><msub id="alg1.l17.m1.3.3.1.1.1.1" xref="alg1.l17.m1.3.3.1.1.1.1.cmml"><mi id="alg1.l17.m1.3.3.1.1.1.1.2" xref="alg1.l17.m1.3.3.1.1.1.1.2.cmml">tid</mi><mrow id="alg1.l17.m1.3.3.1.1.1.1.3" xref="alg1.l17.m1.3.3.1.1.1.1.3.cmml"><mi id="alg1.l17.m1.3.3.1.1.1.1.3.2" xref="alg1.l17.m1.3.3.1.1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="alg1.l17.m1.3.3.1.1.1.1.3.1" xref="alg1.l17.m1.3.3.1.1.1.1.3.1.cmml">​</mo><mi id="alg1.l17.m1.3.3.1.1.1.1.3.3" xref="alg1.l17.m1.3.3.1.1.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="alg1.l17.m1.3.3.1.1.1.1.3.1a" xref="alg1.l17.m1.3.3.1.1.1.1.3.1.cmml">​</mo><mi id="alg1.l17.m1.3.3.1.1.1.1.3.4" xref="alg1.l17.m1.3.3.1.1.1.1.3.4.cmml">b</mi></mrow></msub><mo id="alg1.l17.m1.3.3.1.1.1.3" xref="alg1.l17.m1.3.3.1.1.2.cmml">,</mo><mi id="alg1.l17.m1.1.1" xref="alg1.l17.m1.1.1.cmml">tid</mi><mo stretchy="false" id="alg1.l17.m1.3.3.1.1.1.4" xref="alg1.l17.m1.3.3.1.1.2.cmml">]</mo></mrow></mrow><mo id="alg1.l17.m1.3.3.2" xref="alg1.l17.m1.3.3.2.cmml">=</mo><mrow id="alg1.l17.m1.3.3.3" xref="alg1.l17.m1.3.3.3.cmml"><mrow id="alg1.l17.m1.3.3.3.2" xref="alg1.l17.m1.3.3.3.2.cmml"><mn id="alg1.l17.m1.3.3.3.2.2" xref="alg1.l17.m1.3.3.3.2.2.cmml">1</mn><mo id="alg1.l17.m1.3.3.3.2.1" xref="alg1.l17.m1.3.3.3.2.1.cmml">/</mo><mi id="alg1.l17.m1.3.3.3.2.3" xref="alg1.l17.m1.3.3.3.2.3.cmml">len</mi></mrow><mo lspace="0em" rspace="0em" id="alg1.l17.m1.3.3.3.1" xref="alg1.l17.m1.3.3.3.1.cmml">​</mo><mrow id="alg1.l17.m1.3.3.3.3.2" xref="alg1.l17.m1.3.3.3.cmml"><mo stretchy="false" id="alg1.l17.m1.3.3.3.3.2.1" xref="alg1.l17.m1.3.3.3.cmml">(</mo><mi id="alg1.l17.m1.2.2" xref="alg1.l17.m1.2.2.cmml">tokens</mi><mo stretchy="false" id="alg1.l17.m1.3.3.3.3.2.2" xref="alg1.l17.m1.3.3.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l17.m1.3b"><apply id="alg1.l17.m1.3.3.cmml" xref="alg1.l17.m1.3.3"><eq id="alg1.l17.m1.3.3.2.cmml" xref="alg1.l17.m1.3.3.2"></eq><apply id="alg1.l17.m1.3.3.1.cmml" xref="alg1.l17.m1.3.3.1"><times id="alg1.l17.m1.3.3.1.2.cmml" xref="alg1.l17.m1.3.3.1.2"></times><ci id="alg1.l17.m1.3.3.1.3.cmml" xref="alg1.l17.m1.3.3.1.3">map</ci><interval closure="closed" id="alg1.l17.m1.3.3.1.1.2.cmml" xref="alg1.l17.m1.3.3.1.1.1"><apply id="alg1.l17.m1.3.3.1.1.1.1.cmml" xref="alg1.l17.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l17.m1.3.3.1.1.1.1.1.cmml" xref="alg1.l17.m1.3.3.1.1.1.1">subscript</csymbol><ci id="alg1.l17.m1.3.3.1.1.1.1.2.cmml" xref="alg1.l17.m1.3.3.1.1.1.1.2">tid</ci><apply id="alg1.l17.m1.3.3.1.1.1.1.3.cmml" xref="alg1.l17.m1.3.3.1.1.1.1.3"><times id="alg1.l17.m1.3.3.1.1.1.1.3.1.cmml" xref="alg1.l17.m1.3.3.1.1.1.1.3.1"></times><ci id="alg1.l17.m1.3.3.1.1.1.1.3.2.cmml" xref="alg1.l17.m1.3.3.1.1.1.1.3.2">𝑝</ci><ci id="alg1.l17.m1.3.3.1.1.1.1.3.3.cmml" xref="alg1.l17.m1.3.3.1.1.1.1.3.3">𝑢</ci><ci id="alg1.l17.m1.3.3.1.1.1.1.3.4.cmml" xref="alg1.l17.m1.3.3.1.1.1.1.3.4">𝑏</ci></apply></apply><ci id="alg1.l17.m1.1.1.cmml" xref="alg1.l17.m1.1.1">tid</ci></interval></apply><apply id="alg1.l17.m1.3.3.3.cmml" xref="alg1.l17.m1.3.3.3"><times id="alg1.l17.m1.3.3.3.1.cmml" xref="alg1.l17.m1.3.3.3.1"></times><apply id="alg1.l17.m1.3.3.3.2.cmml" xref="alg1.l17.m1.3.3.3.2"><divide id="alg1.l17.m1.3.3.3.2.1.cmml" xref="alg1.l17.m1.3.3.3.2.1"></divide><cn type="integer" id="alg1.l17.m1.3.3.3.2.2.cmml" xref="alg1.l17.m1.3.3.3.2.2">1</cn><ci id="alg1.l17.m1.3.3.3.2.3.cmml" xref="alg1.l17.m1.3.3.3.2.3">len</ci></apply><ci id="alg1.l17.m1.2.2.cmml" xref="alg1.l17.m1.2.2">tokens</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l17.m1.3c">\mathrm{map}[\mathrm{tid}_{pub},\mathrm{tid}]=1/\mathrm{len}(\mathrm{tokens})</annotation></semantics></math>
              
</div>
<div id="alg1.l18" class="ltx_listingline">     <span id="alg1.l18.1" class="ltx_text ltx_font_bold">return</span> <math id="alg1.l18.m1.1" class="ltx_Math" alttext="\mathrm{map}" display="inline"><semantics id="alg1.l18.m1.1a"><mi id="alg1.l18.m1.1.1" xref="alg1.l18.m1.1.1.cmml">map</mi><annotation-xml encoding="MathML-Content" id="alg1.l18.m1.1b"><ci id="alg1.l18.m1.1.1.cmml" xref="alg1.l18.m1.1.1">map</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l18.m1.1c">\mathrm{map}</annotation></semantics></math>

</div>
<div id="alg1.l19" class="ltx_listingline">
<span id="alg1.l19.1" class="ltx_text ltx_font_bold">function</span> <span id="alg1.l19.2" class="ltx_text ltx_font_smallcaps">Update</span>(<math id="alg1.l19.m1.2" class="ltx_Math" alttext="\theta,\tau_{pub}" display="inline"><semantics id="alg1.l19.m1.2a"><mrow id="alg1.l19.m1.2.2.1" xref="alg1.l19.m1.2.2.2.cmml"><mi id="alg1.l19.m1.1.1" xref="alg1.l19.m1.1.1.cmml">θ</mi><mo id="alg1.l19.m1.2.2.1.2" xref="alg1.l19.m1.2.2.2.cmml">,</mo><msub id="alg1.l19.m1.2.2.1.1" xref="alg1.l19.m1.2.2.1.1.cmml"><mi id="alg1.l19.m1.2.2.1.1.2" xref="alg1.l19.m1.2.2.1.1.2.cmml">τ</mi><mrow id="alg1.l19.m1.2.2.1.1.3" xref="alg1.l19.m1.2.2.1.1.3.cmml"><mi id="alg1.l19.m1.2.2.1.1.3.2" xref="alg1.l19.m1.2.2.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="alg1.l19.m1.2.2.1.1.3.1" xref="alg1.l19.m1.2.2.1.1.3.1.cmml">​</mo><mi id="alg1.l19.m1.2.2.1.1.3.3" xref="alg1.l19.m1.2.2.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="alg1.l19.m1.2.2.1.1.3.1a" xref="alg1.l19.m1.2.2.1.1.3.1.cmml">​</mo><mi id="alg1.l19.m1.2.2.1.1.3.4" xref="alg1.l19.m1.2.2.1.1.3.4.cmml">b</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="alg1.l19.m1.2b"><list id="alg1.l19.m1.2.2.2.cmml" xref="alg1.l19.m1.2.2.1"><ci id="alg1.l19.m1.1.1.cmml" xref="alg1.l19.m1.1.1">𝜃</ci><apply id="alg1.l19.m1.2.2.1.1.cmml" xref="alg1.l19.m1.2.2.1.1"><csymbol cd="ambiguous" id="alg1.l19.m1.2.2.1.1.1.cmml" xref="alg1.l19.m1.2.2.1.1">subscript</csymbol><ci id="alg1.l19.m1.2.2.1.1.2.cmml" xref="alg1.l19.m1.2.2.1.1.2">𝜏</ci><apply id="alg1.l19.m1.2.2.1.1.3.cmml" xref="alg1.l19.m1.2.2.1.1.3"><times id="alg1.l19.m1.2.2.1.1.3.1.cmml" xref="alg1.l19.m1.2.2.1.1.3.1"></times><ci id="alg1.l19.m1.2.2.1.1.3.2.cmml" xref="alg1.l19.m1.2.2.1.1.3.2">𝑝</ci><ci id="alg1.l19.m1.2.2.1.1.3.3.cmml" xref="alg1.l19.m1.2.2.1.1.3.3">𝑢</ci><ci id="alg1.l19.m1.2.2.1.1.3.4.cmml" xref="alg1.l19.m1.2.2.1.1.3.4">𝑏</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="alg1.l19.m1.2c">\theta,\tau_{pub}</annotation></semantics></math>)

</div>
<div id="alg1.l20" class="ltx_listingline">     <span id="alg1.l20.1" class="ltx_text ltx_font_bold">while</span> <math id="alg1.l20.m1.1" class="ltx_Math" alttext="\mathrm{len}(\mathrm{corpus})&lt;\mathrm{corpus\_size}" display="inline"><semantics id="alg1.l20.m1.1a"><mrow id="alg1.l20.m1.1.2" xref="alg1.l20.m1.1.2.cmml"><mrow id="alg1.l20.m1.1.2.2" xref="alg1.l20.m1.1.2.2.cmml"><mi id="alg1.l20.m1.1.2.2.2" xref="alg1.l20.m1.1.2.2.2.cmml">len</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.1.2.2.1" xref="alg1.l20.m1.1.2.2.1.cmml">​</mo><mrow id="alg1.l20.m1.1.2.2.3.2" xref="alg1.l20.m1.1.2.2.cmml"><mo stretchy="false" id="alg1.l20.m1.1.2.2.3.2.1" xref="alg1.l20.m1.1.2.2.cmml">(</mo><mi id="alg1.l20.m1.1.1" xref="alg1.l20.m1.1.1.cmml">corpus</mi><mo stretchy="false" id="alg1.l20.m1.1.2.2.3.2.2" xref="alg1.l20.m1.1.2.2.cmml">)</mo></mrow></mrow><mo id="alg1.l20.m1.1.2.1" xref="alg1.l20.m1.1.2.1.cmml">&lt;</mo><mrow id="alg1.l20.m1.1.2.3" xref="alg1.l20.m1.1.2.3.cmml"><mi id="alg1.l20.m1.1.2.3.2" xref="alg1.l20.m1.1.2.3.2.cmml">corpus</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.1.2.3.1" xref="alg1.l20.m1.1.2.3.1.cmml">​</mo><mi mathvariant="normal" id="alg1.l20.m1.1.2.3.3" xref="alg1.l20.m1.1.2.3.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.1.2.3.1a" xref="alg1.l20.m1.1.2.3.1.cmml">​</mo><mi id="alg1.l20.m1.1.2.3.4" xref="alg1.l20.m1.1.2.3.4.cmml">size</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l20.m1.1b"><apply id="alg1.l20.m1.1.2.cmml" xref="alg1.l20.m1.1.2"><lt id="alg1.l20.m1.1.2.1.cmml" xref="alg1.l20.m1.1.2.1"></lt><apply id="alg1.l20.m1.1.2.2.cmml" xref="alg1.l20.m1.1.2.2"><times id="alg1.l20.m1.1.2.2.1.cmml" xref="alg1.l20.m1.1.2.2.1"></times><ci id="alg1.l20.m1.1.2.2.2.cmml" xref="alg1.l20.m1.1.2.2.2">len</ci><ci id="alg1.l20.m1.1.1.cmml" xref="alg1.l20.m1.1.1">corpus</ci></apply><apply id="alg1.l20.m1.1.2.3.cmml" xref="alg1.l20.m1.1.2.3"><times id="alg1.l20.m1.1.2.3.1.cmml" xref="alg1.l20.m1.1.2.3.1"></times><ci id="alg1.l20.m1.1.2.3.2.cmml" xref="alg1.l20.m1.1.2.3.2">corpus</ci><ci id="alg1.l20.m1.1.2.3.3.cmml" xref="alg1.l20.m1.1.2.3.3">_</ci><ci id="alg1.l20.m1.1.2.3.4.cmml" xref="alg1.l20.m1.1.2.3.4">size</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l20.m1.1c">\mathrm{len}(\mathrm{corpus})&lt;\mathrm{corpus\_size}</annotation></semantics></math> <span id="alg1.l20.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l21" class="ltx_listingline">         <math id="alg1.l21.m1.3" class="ltx_Math" alttext="\mathrm{corpus}\leftarrow\textsc{Sample}(\theta,\emptyset,l_{max})" display="inline"><semantics id="alg1.l21.m1.3a"><mrow id="alg1.l21.m1.3.3" xref="alg1.l21.m1.3.3.cmml"><mi id="alg1.l21.m1.3.3.3" xref="alg1.l21.m1.3.3.3.cmml">corpus</mi><mo stretchy="false" id="alg1.l21.m1.3.3.2" xref="alg1.l21.m1.3.3.2.cmml">←</mo><mrow id="alg1.l21.m1.3.3.1" xref="alg1.l21.m1.3.3.1.cmml"><mtext class="ltx_font_smallcaps" id="alg1.l21.m1.3.3.1.3" xref="alg1.l21.m1.3.3.1.3a.cmml">Sample</mtext><mo lspace="0em" rspace="0em" id="alg1.l21.m1.3.3.1.2" xref="alg1.l21.m1.3.3.1.2.cmml">​</mo><mrow id="alg1.l21.m1.3.3.1.1.1" xref="alg1.l21.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="alg1.l21.m1.3.3.1.1.1.2" xref="alg1.l21.m1.3.3.1.1.2.cmml">(</mo><mi id="alg1.l21.m1.1.1" xref="alg1.l21.m1.1.1.cmml">θ</mi><mo id="alg1.l21.m1.3.3.1.1.1.3" xref="alg1.l21.m1.3.3.1.1.2.cmml">,</mo><mi mathvariant="normal" id="alg1.l21.m1.2.2" xref="alg1.l21.m1.2.2.cmml">∅</mi><mo id="alg1.l21.m1.3.3.1.1.1.4" xref="alg1.l21.m1.3.3.1.1.2.cmml">,</mo><msub id="alg1.l21.m1.3.3.1.1.1.1" xref="alg1.l21.m1.3.3.1.1.1.1.cmml"><mi id="alg1.l21.m1.3.3.1.1.1.1.2" xref="alg1.l21.m1.3.3.1.1.1.1.2.cmml">l</mi><mrow id="alg1.l21.m1.3.3.1.1.1.1.3" xref="alg1.l21.m1.3.3.1.1.1.1.3.cmml"><mi id="alg1.l21.m1.3.3.1.1.1.1.3.2" xref="alg1.l21.m1.3.3.1.1.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="alg1.l21.m1.3.3.1.1.1.1.3.1" xref="alg1.l21.m1.3.3.1.1.1.1.3.1.cmml">​</mo><mi id="alg1.l21.m1.3.3.1.1.1.1.3.3" xref="alg1.l21.m1.3.3.1.1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="alg1.l21.m1.3.3.1.1.1.1.3.1a" xref="alg1.l21.m1.3.3.1.1.1.1.3.1.cmml">​</mo><mi id="alg1.l21.m1.3.3.1.1.1.1.3.4" xref="alg1.l21.m1.3.3.1.1.1.1.3.4.cmml">x</mi></mrow></msub><mo stretchy="false" id="alg1.l21.m1.3.3.1.1.1.5" xref="alg1.l21.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l21.m1.3b"><apply id="alg1.l21.m1.3.3.cmml" xref="alg1.l21.m1.3.3"><ci id="alg1.l21.m1.3.3.2.cmml" xref="alg1.l21.m1.3.3.2">←</ci><ci id="alg1.l21.m1.3.3.3.cmml" xref="alg1.l21.m1.3.3.3">corpus</ci><apply id="alg1.l21.m1.3.3.1.cmml" xref="alg1.l21.m1.3.3.1"><times id="alg1.l21.m1.3.3.1.2.cmml" xref="alg1.l21.m1.3.3.1.2"></times><ci id="alg1.l21.m1.3.3.1.3a.cmml" xref="alg1.l21.m1.3.3.1.3"><mtext class="ltx_font_smallcaps" id="alg1.l21.m1.3.3.1.3.cmml" xref="alg1.l21.m1.3.3.1.3">Sample</mtext></ci><vector id="alg1.l21.m1.3.3.1.1.2.cmml" xref="alg1.l21.m1.3.3.1.1.1"><ci id="alg1.l21.m1.1.1.cmml" xref="alg1.l21.m1.1.1">𝜃</ci><emptyset id="alg1.l21.m1.2.2.cmml" xref="alg1.l21.m1.2.2"></emptyset><apply id="alg1.l21.m1.3.3.1.1.1.1.cmml" xref="alg1.l21.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l21.m1.3.3.1.1.1.1.1.cmml" xref="alg1.l21.m1.3.3.1.1.1.1">subscript</csymbol><ci id="alg1.l21.m1.3.3.1.1.1.1.2.cmml" xref="alg1.l21.m1.3.3.1.1.1.1.2">𝑙</ci><apply id="alg1.l21.m1.3.3.1.1.1.1.3.cmml" xref="alg1.l21.m1.3.3.1.1.1.1.3"><times id="alg1.l21.m1.3.3.1.1.1.1.3.1.cmml" xref="alg1.l21.m1.3.3.1.1.1.1.3.1"></times><ci id="alg1.l21.m1.3.3.1.1.1.1.3.2.cmml" xref="alg1.l21.m1.3.3.1.1.1.1.3.2">𝑚</ci><ci id="alg1.l21.m1.3.3.1.1.1.1.3.3.cmml" xref="alg1.l21.m1.3.3.1.1.1.1.3.3">𝑎</ci><ci id="alg1.l21.m1.3.3.1.1.1.1.3.4.cmml" xref="alg1.l21.m1.3.3.1.1.1.1.3.4">𝑥</ci></apply></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l21.m1.3c">\mathrm{corpus}\leftarrow\textsc{Sample}(\theta,\emptyset,l_{max})</annotation></semantics></math>
     

</div>
<div id="alg1.l22" class="ltx_listingline">     <math id="alg1.l22.m1.1" class="ltx_Math" alttext="\tau=\textsc{TrainTokenizer}(\mathrm{corpus})" display="inline"><semantics id="alg1.l22.m1.1a"><mrow id="alg1.l22.m1.1.2" xref="alg1.l22.m1.1.2.cmml"><mi id="alg1.l22.m1.1.2.2" xref="alg1.l22.m1.1.2.2.cmml">τ</mi><mo id="alg1.l22.m1.1.2.1" xref="alg1.l22.m1.1.2.1.cmml">=</mo><mrow id="alg1.l22.m1.1.2.3" xref="alg1.l22.m1.1.2.3.cmml"><mtext class="ltx_font_smallcaps" id="alg1.l22.m1.1.2.3.2" xref="alg1.l22.m1.1.2.3.2a.cmml">TrainTokenizer</mtext><mo lspace="0em" rspace="0em" id="alg1.l22.m1.1.2.3.1" xref="alg1.l22.m1.1.2.3.1.cmml">​</mo><mrow id="alg1.l22.m1.1.2.3.3.2" xref="alg1.l22.m1.1.2.3.cmml"><mo stretchy="false" id="alg1.l22.m1.1.2.3.3.2.1" xref="alg1.l22.m1.1.2.3.cmml">(</mo><mi id="alg1.l22.m1.1.1" xref="alg1.l22.m1.1.1.cmml">corpus</mi><mo stretchy="false" id="alg1.l22.m1.1.2.3.3.2.2" xref="alg1.l22.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l22.m1.1b"><apply id="alg1.l22.m1.1.2.cmml" xref="alg1.l22.m1.1.2"><eq id="alg1.l22.m1.1.2.1.cmml" xref="alg1.l22.m1.1.2.1"></eq><ci id="alg1.l22.m1.1.2.2.cmml" xref="alg1.l22.m1.1.2.2">𝜏</ci><apply id="alg1.l22.m1.1.2.3.cmml" xref="alg1.l22.m1.1.2.3"><times id="alg1.l22.m1.1.2.3.1.cmml" xref="alg1.l22.m1.1.2.3.1"></times><ci id="alg1.l22.m1.1.2.3.2a.cmml" xref="alg1.l22.m1.1.2.3.2"><mtext class="ltx_font_smallcaps" id="alg1.l22.m1.1.2.3.2.cmml" xref="alg1.l22.m1.1.2.3.2">TrainTokenizer</mtext></ci><ci id="alg1.l22.m1.1.1.cmml" xref="alg1.l22.m1.1.1">corpus</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l22.m1.1c">\tau=\textsc{TrainTokenizer}(\mathrm{corpus})</annotation></semantics></math>

</div>
<div id="alg1.l23" class="ltx_listingline">     <math id="alg1.l23.m1.2" class="ltx_Math" alttext="\mathrm{map}=\textsc{Remap}(\tau_{pub},\tau)" display="inline"><semantics id="alg1.l23.m1.2a"><mrow id="alg1.l23.m1.2.2" xref="alg1.l23.m1.2.2.cmml"><mi id="alg1.l23.m1.2.2.3" xref="alg1.l23.m1.2.2.3.cmml">map</mi><mo id="alg1.l23.m1.2.2.2" xref="alg1.l23.m1.2.2.2.cmml">=</mo><mrow id="alg1.l23.m1.2.2.1" xref="alg1.l23.m1.2.2.1.cmml"><mtext class="ltx_font_smallcaps" id="alg1.l23.m1.2.2.1.3" xref="alg1.l23.m1.2.2.1.3a.cmml">Remap</mtext><mo lspace="0em" rspace="0em" id="alg1.l23.m1.2.2.1.2" xref="alg1.l23.m1.2.2.1.2.cmml">​</mo><mrow id="alg1.l23.m1.2.2.1.1.1" xref="alg1.l23.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="alg1.l23.m1.2.2.1.1.1.2" xref="alg1.l23.m1.2.2.1.1.2.cmml">(</mo><msub id="alg1.l23.m1.2.2.1.1.1.1" xref="alg1.l23.m1.2.2.1.1.1.1.cmml"><mi id="alg1.l23.m1.2.2.1.1.1.1.2" xref="alg1.l23.m1.2.2.1.1.1.1.2.cmml">τ</mi><mrow id="alg1.l23.m1.2.2.1.1.1.1.3" xref="alg1.l23.m1.2.2.1.1.1.1.3.cmml"><mi id="alg1.l23.m1.2.2.1.1.1.1.3.2" xref="alg1.l23.m1.2.2.1.1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="alg1.l23.m1.2.2.1.1.1.1.3.1" xref="alg1.l23.m1.2.2.1.1.1.1.3.1.cmml">​</mo><mi id="alg1.l23.m1.2.2.1.1.1.1.3.3" xref="alg1.l23.m1.2.2.1.1.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="alg1.l23.m1.2.2.1.1.1.1.3.1a" xref="alg1.l23.m1.2.2.1.1.1.1.3.1.cmml">​</mo><mi id="alg1.l23.m1.2.2.1.1.1.1.3.4" xref="alg1.l23.m1.2.2.1.1.1.1.3.4.cmml">b</mi></mrow></msub><mo id="alg1.l23.m1.2.2.1.1.1.3" xref="alg1.l23.m1.2.2.1.1.2.cmml">,</mo><mi id="alg1.l23.m1.1.1" xref="alg1.l23.m1.1.1.cmml">τ</mi><mo stretchy="false" id="alg1.l23.m1.2.2.1.1.1.4" xref="alg1.l23.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l23.m1.2b"><apply id="alg1.l23.m1.2.2.cmml" xref="alg1.l23.m1.2.2"><eq id="alg1.l23.m1.2.2.2.cmml" xref="alg1.l23.m1.2.2.2"></eq><ci id="alg1.l23.m1.2.2.3.cmml" xref="alg1.l23.m1.2.2.3">map</ci><apply id="alg1.l23.m1.2.2.1.cmml" xref="alg1.l23.m1.2.2.1"><times id="alg1.l23.m1.2.2.1.2.cmml" xref="alg1.l23.m1.2.2.1.2"></times><ci id="alg1.l23.m1.2.2.1.3a.cmml" xref="alg1.l23.m1.2.2.1.3"><mtext class="ltx_font_smallcaps" id="alg1.l23.m1.2.2.1.3.cmml" xref="alg1.l23.m1.2.2.1.3">Remap</mtext></ci><interval closure="open" id="alg1.l23.m1.2.2.1.1.2.cmml" xref="alg1.l23.m1.2.2.1.1.1"><apply id="alg1.l23.m1.2.2.1.1.1.1.cmml" xref="alg1.l23.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l23.m1.2.2.1.1.1.1.1.cmml" xref="alg1.l23.m1.2.2.1.1.1.1">subscript</csymbol><ci id="alg1.l23.m1.2.2.1.1.1.1.2.cmml" xref="alg1.l23.m1.2.2.1.1.1.1.2">𝜏</ci><apply id="alg1.l23.m1.2.2.1.1.1.1.3.cmml" xref="alg1.l23.m1.2.2.1.1.1.1.3"><times id="alg1.l23.m1.2.2.1.1.1.1.3.1.cmml" xref="alg1.l23.m1.2.2.1.1.1.1.3.1"></times><ci id="alg1.l23.m1.2.2.1.1.1.1.3.2.cmml" xref="alg1.l23.m1.2.2.1.1.1.1.3.2">𝑝</ci><ci id="alg1.l23.m1.2.2.1.1.1.1.3.3.cmml" xref="alg1.l23.m1.2.2.1.1.1.1.3.3">𝑢</ci><ci id="alg1.l23.m1.2.2.1.1.1.1.3.4.cmml" xref="alg1.l23.m1.2.2.1.1.1.1.3.4">𝑏</ci></apply></apply><ci id="alg1.l23.m1.1.1.cmml" xref="alg1.l23.m1.1.1">𝜏</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l23.m1.2c">\mathrm{map}=\textsc{Remap}(\tau_{pub},\tau)</annotation></semantics></math>

</div>
<div id="alg1.l24" class="ltx_listingline">     <math id="alg1.l24.m1.3" class="ltx_Math" alttext="\theta.\mathrm{embedding}=\mathrm{map}\cdot\theta.\mathrm{embedding}" display="inline"><semantics id="alg1.l24.m1.3a"><mrow id="alg1.l24.m1.3.3.1" xref="alg1.l24.m1.3.3.2.cmml"><mi id="alg1.l24.m1.1.1" xref="alg1.l24.m1.1.1.cmml">θ</mi><mo lspace="0em" rspace="0.167em" id="alg1.l24.m1.3.3.1.2" xref="alg1.l24.m1.3.3.2a.cmml">.</mo><mrow id="alg1.l24.m1.3.3.1.1" xref="alg1.l24.m1.3.3.1.1.cmml"><mi id="alg1.l24.m1.3.3.1.1.2" xref="alg1.l24.m1.3.3.1.1.2.cmml">embedding</mi><mo id="alg1.l24.m1.3.3.1.1.1" xref="alg1.l24.m1.3.3.1.1.1.cmml">=</mo><mrow id="alg1.l24.m1.3.3.1.1.3" xref="alg1.l24.m1.3.3.1.1.3.cmml"><mi id="alg1.l24.m1.3.3.1.1.3.2" xref="alg1.l24.m1.3.3.1.1.3.2.cmml">map</mi><mo lspace="0.222em" rspace="0.222em" id="alg1.l24.m1.3.3.1.1.3.1" xref="alg1.l24.m1.3.3.1.1.3.1.cmml">⋅</mo><mi id="alg1.l24.m1.3.3.1.1.3.3" xref="alg1.l24.m1.3.3.1.1.3.3.cmml">θ</mi></mrow></mrow><mo lspace="0em" rspace="0.167em" id="alg1.l24.m1.3.3.1.3" xref="alg1.l24.m1.3.3.2a.cmml">.</mo><mi id="alg1.l24.m1.2.2" xref="alg1.l24.m1.2.2.cmml">embedding</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l24.m1.3b"><apply id="alg1.l24.m1.3.3.2.cmml" xref="alg1.l24.m1.3.3.1"><csymbol cd="ambiguous" id="alg1.l24.m1.3.3.2a.cmml" xref="alg1.l24.m1.3.3.1.2">formulae-sequence</csymbol><ci id="alg1.l24.m1.1.1.cmml" xref="alg1.l24.m1.1.1">𝜃</ci><apply id="alg1.l24.m1.3.3.1.1.cmml" xref="alg1.l24.m1.3.3.1.1"><eq id="alg1.l24.m1.3.3.1.1.1.cmml" xref="alg1.l24.m1.3.3.1.1.1"></eq><ci id="alg1.l24.m1.3.3.1.1.2.cmml" xref="alg1.l24.m1.3.3.1.1.2">embedding</ci><apply id="alg1.l24.m1.3.3.1.1.3.cmml" xref="alg1.l24.m1.3.3.1.1.3"><ci id="alg1.l24.m1.3.3.1.1.3.1.cmml" xref="alg1.l24.m1.3.3.1.1.3.1">⋅</ci><ci id="alg1.l24.m1.3.3.1.1.3.2.cmml" xref="alg1.l24.m1.3.3.1.1.3.2">map</ci><ci id="alg1.l24.m1.3.3.1.1.3.3.cmml" xref="alg1.l24.m1.3.3.1.1.3.3">𝜃</ci></apply></apply><ci id="alg1.l24.m1.2.2.cmml" xref="alg1.l24.m1.2.2">embedding</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l24.m1.3c">\theta.\mathrm{embedding}=\mathrm{map}\cdot\theta.\mathrm{embedding}</annotation></semantics></math>

</div>
<div id="alg1.l25" class="ltx_listingline">     <span id="alg1.l25.1" class="ltx_text ltx_font_bold">return</span> <math id="alg1.l25.m1.2" class="ltx_Math" alttext="\theta,\tau" display="inline"><semantics id="alg1.l25.m1.2a"><mrow id="alg1.l25.m1.2.3.2" xref="alg1.l25.m1.2.3.1.cmml"><mi id="alg1.l25.m1.1.1" xref="alg1.l25.m1.1.1.cmml">θ</mi><mo id="alg1.l25.m1.2.3.2.1" xref="alg1.l25.m1.2.3.1.cmml">,</mo><mi id="alg1.l25.m1.2.2" xref="alg1.l25.m1.2.2.cmml">τ</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l25.m1.2b"><list id="alg1.l25.m1.2.3.1.cmml" xref="alg1.l25.m1.2.3.2"><ci id="alg1.l25.m1.1.1.cmml" xref="alg1.l25.m1.1.1">𝜃</ci><ci id="alg1.l25.m1.2.2.cmml" xref="alg1.l25.m1.2.2">𝜏</ci></list></annotation-xml><annotation encoding="application/x-tex" id="alg1.l25.m1.2c">\theta,\tau</annotation></semantics></math>

</div>
</div>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We evaluate our approach by first looking at performance of tokenizers
trained on the distributions matched and mismatched to real data, we
then test the proposed federated sampling on different datasets for
federated learning.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental setup.</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We use two datasets common in the federated learning literature
<cite class="ltx_cite ltx_citemacro_cite">Kairouz et al. (<a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite>. While both use English, there is nothing
about our experiments that is specific to this language, and multilingual datasets can further benefit from using SentencePiece tokenization <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a href="#bib.bib18" title="" class="ltx_ref">2018</a>)</cite>,.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">Reddit data – this dataset is taken from the LEAF benchmark
<cite class="ltx_cite ltx_citemacro_cite">Caldas et al. (<a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite> and contains over a million users that have
multiple posts on the Reddit platform. As proposed by LEAF, we limit
each user to contain at most 1600 tokens and use 10 % of users for
faster training.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">StackOverflow data – this data is taken from Kaggle
<cite class="ltx_cite ltx_citemacro_cite">Kaggle (<a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite> and processed with the TensorFlow Federated
framework. The train split of the dataset contains 342k users and we
select at most 1600 tokens per user.</p>
</div>
</li>
</ul>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Model parameters.</span>  We use an LSTM model with 3 layers, and
total parameters of 14M. We also use a Transformer language model <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a href="#bib.bib31" title="" class="ltx_ref">2017</a>)</cite> with
6 layers and the same total number of parameters as the LSTM (see Appendix <a href="#A1" title="Appendix A Impact of hyperparameters ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>). Each
model is trained from scratch.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.p4.8" class="ltx_p"><span id="S5.SS1.p4.8.1" class="ltx_text ltx_font_bold ltx_font_italic">Hyper-parameters.</span> 
We set the privacy budget to <math id="S5.SS1.p4.1.m1.1" class="ltx_Math" alttext="\epsilon=2" display="inline"><semantics id="S5.SS1.p4.1.m1.1a"><mrow id="S5.SS1.p4.1.m1.1.1" xref="S5.SS1.p4.1.m1.1.1.cmml"><mi id="S5.SS1.p4.1.m1.1.1.2" xref="S5.SS1.p4.1.m1.1.1.2.cmml">ϵ</mi><mo id="S5.SS1.p4.1.m1.1.1.1" xref="S5.SS1.p4.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS1.p4.1.m1.1.1.3" xref="S5.SS1.p4.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.1.m1.1b"><apply id="S5.SS1.p4.1.m1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1"><eq id="S5.SS1.p4.1.m1.1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1.1"></eq><ci id="S5.SS1.p4.1.m1.1.1.2.cmml" xref="S5.SS1.p4.1.m1.1.1.2">italic-ϵ</ci><cn type="integer" id="S5.SS1.p4.1.m1.1.1.3.cmml" xref="S5.SS1.p4.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.1c">\epsilon=2</annotation></semantics></math> and <math id="S5.SS1.p4.2.m2.1" class="ltx_Math" alttext="\delta=10^{-6}" display="inline"><semantics id="S5.SS1.p4.2.m2.1a"><mrow id="S5.SS1.p4.2.m2.1.1" xref="S5.SS1.p4.2.m2.1.1.cmml"><mi id="S5.SS1.p4.2.m2.1.1.2" xref="S5.SS1.p4.2.m2.1.1.2.cmml">δ</mi><mo id="S5.SS1.p4.2.m2.1.1.1" xref="S5.SS1.p4.2.m2.1.1.1.cmml">=</mo><msup id="S5.SS1.p4.2.m2.1.1.3" xref="S5.SS1.p4.2.m2.1.1.3.cmml"><mn id="S5.SS1.p4.2.m2.1.1.3.2" xref="S5.SS1.p4.2.m2.1.1.3.2.cmml">10</mn><mrow id="S5.SS1.p4.2.m2.1.1.3.3" xref="S5.SS1.p4.2.m2.1.1.3.3.cmml"><mo id="S5.SS1.p4.2.m2.1.1.3.3a" xref="S5.SS1.p4.2.m2.1.1.3.3.cmml">−</mo><mn id="S5.SS1.p4.2.m2.1.1.3.3.2" xref="S5.SS1.p4.2.m2.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.2.m2.1b"><apply id="S5.SS1.p4.2.m2.1.1.cmml" xref="S5.SS1.p4.2.m2.1.1"><eq id="S5.SS1.p4.2.m2.1.1.1.cmml" xref="S5.SS1.p4.2.m2.1.1.1"></eq><ci id="S5.SS1.p4.2.m2.1.1.2.cmml" xref="S5.SS1.p4.2.m2.1.1.2">𝛿</ci><apply id="S5.SS1.p4.2.m2.1.1.3.cmml" xref="S5.SS1.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p4.2.m2.1.1.3.1.cmml" xref="S5.SS1.p4.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S5.SS1.p4.2.m2.1.1.3.2.cmml" xref="S5.SS1.p4.2.m2.1.1.3.2">10</cn><apply id="S5.SS1.p4.2.m2.1.1.3.3.cmml" xref="S5.SS1.p4.2.m2.1.1.3.3"><minus id="S5.SS1.p4.2.m2.1.1.3.3.1.cmml" xref="S5.SS1.p4.2.m2.1.1.3.3"></minus><cn type="integer" id="S5.SS1.p4.2.m2.1.1.3.3.2.cmml" xref="S5.SS1.p4.2.m2.1.1.3.3.2">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.2.m2.1c">\delta=10^{-6}</annotation></semantics></math> – a common privacy regime <cite class="ltx_cite ltx_citemacro_cite">Kairouz et al. (<a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite>.
For the “heavy hitters” baseline we use local DP with an additional privacy budget of <math id="S5.SS1.p4.3.m3.1" class="ltx_Math" alttext="\epsilon=8" display="inline"><semantics id="S5.SS1.p4.3.m3.1a"><mrow id="S5.SS1.p4.3.m3.1.1" xref="S5.SS1.p4.3.m3.1.1.cmml"><mi id="S5.SS1.p4.3.m3.1.1.2" xref="S5.SS1.p4.3.m3.1.1.2.cmml">ϵ</mi><mo id="S5.SS1.p4.3.m3.1.1.1" xref="S5.SS1.p4.3.m3.1.1.1.cmml">=</mo><mn id="S5.SS1.p4.3.m3.1.1.3" xref="S5.SS1.p4.3.m3.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.3.m3.1b"><apply id="S5.SS1.p4.3.m3.1.1.cmml" xref="S5.SS1.p4.3.m3.1.1"><eq id="S5.SS1.p4.3.m3.1.1.1.cmml" xref="S5.SS1.p4.3.m3.1.1.1"></eq><ci id="S5.SS1.p4.3.m3.1.1.2.cmml" xref="S5.SS1.p4.3.m3.1.1.2">italic-ϵ</ci><cn type="integer" id="S5.SS1.p4.3.m3.1.1.3.cmml" xref="S5.SS1.p4.3.m3.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.3.m3.1c">\epsilon=8</annotation></semantics></math>.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Budgets for local and central privacy are not immediately comparable, but see <cite class="ltx_cite ltx_citemacro_citet">Feldman et al. (<a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>.</span></span></span>
The overall population for the
moments accountant is assumed to be 10m. We use a cohort size of
<math id="S5.SS1.p4.4.m4.2" class="ltx_Math" alttext="20,000" display="inline"><semantics id="S5.SS1.p4.4.m4.2a"><mrow id="S5.SS1.p4.4.m4.2.3.2" xref="S5.SS1.p4.4.m4.2.3.1.cmml"><mn id="S5.SS1.p4.4.m4.1.1" xref="S5.SS1.p4.4.m4.1.1.cmml">20</mn><mo id="S5.SS1.p4.4.m4.2.3.2.1" xref="S5.SS1.p4.4.m4.2.3.1.cmml">,</mo><mn id="S5.SS1.p4.4.m4.2.2" xref="S5.SS1.p4.4.m4.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.4.m4.2b"><list id="S5.SS1.p4.4.m4.2.3.1.cmml" xref="S5.SS1.p4.4.m4.2.3.2"><cn type="integer" id="S5.SS1.p4.4.m4.1.1.cmml" xref="S5.SS1.p4.4.m4.1.1">20</cn><cn type="integer" id="S5.SS1.p4.4.m4.2.2.cmml" xref="S5.SS1.p4.4.m4.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.4.m4.2c">20,000</annotation></semantics></math> for each round and train all models for <math id="S5.SS1.p4.5.m5.2" class="ltx_Math" alttext="5,000" display="inline"><semantics id="S5.SS1.p4.5.m5.2a"><mrow id="S5.SS1.p4.5.m5.2.3.2" xref="S5.SS1.p4.5.m5.2.3.1.cmml"><mn id="S5.SS1.p4.5.m5.1.1" xref="S5.SS1.p4.5.m5.1.1.cmml">5</mn><mo id="S5.SS1.p4.5.m5.2.3.2.1" xref="S5.SS1.p4.5.m5.2.3.1.cmml">,</mo><mn id="S5.SS1.p4.5.m5.2.2" xref="S5.SS1.p4.5.m5.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.5.m5.2b"><list id="S5.SS1.p4.5.m5.2.3.1.cmml" xref="S5.SS1.p4.5.m5.2.3.2"><cn type="integer" id="S5.SS1.p4.5.m5.1.1.cmml" xref="S5.SS1.p4.5.m5.1.1">5</cn><cn type="integer" id="S5.SS1.p4.5.m5.2.2.cmml" xref="S5.SS1.p4.5.m5.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.5.m5.2c">5,000</annotation></semantics></math> iterations. We use
Adam <cite class="ltx_cite ltx_citemacro_cite">Kingma and Ba (<a href="#bib.bib17" title="" class="ltx_ref">2015</a>)</cite> for central optimization with learning rate
set to 0.5. For the clients we use SGD and train for <math id="S5.SS1.p4.6.m6.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S5.SS1.p4.6.m6.1a"><mn id="S5.SS1.p4.6.m6.1.1" xref="S5.SS1.p4.6.m6.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.6.m6.1b"><cn type="integer" id="S5.SS1.p4.6.m6.1.1.cmml" xref="S5.SS1.p4.6.m6.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.6.m6.1c">1</annotation></semantics></math> local epoch with
batch size set to 16 and local learning rate set to 0.1, and an <math id="S5.SS1.p4.7.m7.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S5.SS1.p4.7.m7.1a"><msub id="S5.SS1.p4.7.m7.1.1" xref="S5.SS1.p4.7.m7.1.1.cmml"><mi id="S5.SS1.p4.7.m7.1.1.2" xref="S5.SS1.p4.7.m7.1.1.2.cmml">L</mi><mn id="S5.SS1.p4.7.m7.1.1.3" xref="S5.SS1.p4.7.m7.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.7.m7.1b"><apply id="S5.SS1.p4.7.m7.1.1.cmml" xref="S5.SS1.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S5.SS1.p4.7.m7.1.1.1.cmml" xref="S5.SS1.p4.7.m7.1.1">subscript</csymbol><ci id="S5.SS1.p4.7.m7.1.1.2.cmml" xref="S5.SS1.p4.7.m7.1.1.2">𝐿</ci><cn type="integer" id="S5.SS1.p4.7.m7.1.1.3.cmml" xref="S5.SS1.p4.7.m7.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.7.m7.1c">L_{2}</annotation></semantics></math> clipping bound for DP of <math id="S5.SS1.p4.8.m8.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S5.SS1.p4.8.m8.1a"><mn id="S5.SS1.p4.8.m8.1.1" xref="S5.SS1.p4.8.m8.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.8.m8.1b"><cn type="float" id="S5.SS1.p4.8.m8.1.1.cmml" xref="S5.SS1.p4.8.m8.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.8.m8.1c">0.5</annotation></semantics></math>.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para ltx_noindent">
<p id="S5.SS1.p5.1" class="ltx_p"><span id="S5.SS1.p5.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Vocabulary size.</span>  We assume that the tokenizer has a
moderate vocabulary size such as 10,000 tokens (we experiment with
larger vocabularies in Appendix <a href="#A1" title="Appendix A Impact of hyperparameters ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>). Smaller
vocabularies reduce model size and, therefore, might be better for
deployment on devices and communication with the global server.</p>
</div>
<div id="S5.SS1.p6" class="ltx_para ltx_noindent">
<p id="S5.SS1.p6.1" class="ltx_p"><span id="S5.SS1.p6.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Tokenizer details.</span>  To train an initial tokenizer we use a
popular and public Wikipedia dataset <cite class="ltx_cite ltx_citemacro_cite">Merity et al. (<a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite>. It may
seem like the distribution of Wikipedia data is artificially far from
the distributions of Reddit and StackOverflow data. However, the server
might not have the right prior possibly due to a natural
<em id="S5.SS1.p6.1.2" class="ltx_emph ltx_font_italic">distribution shift</em> <cite class="ltx_cite ltx_citemacro_cite">Miller et al. (<a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite> of typed texts (such
as an emerging topic of which there were plenty recently).</p>
</div>
<div id="S5.SS1.p7" class="ltx_para">
<p id="S5.SS1.p7.1" class="ltx_p">We use BPE and WordLevel tokenization algorithms from the HuggingFace
Tokenizer library <cite class="ltx_cite ltx_citemacro_cite">Huggingface (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>. Each user post is surrounded
by special tokens <span id="S5.SS1.p7.1.1" class="ltx_text ltx_font_typewriter">BOS</span> and <span id="S5.SS1.p7.1.2" class="ltx_text ltx_font_typewriter">EOS</span>. We also tried WordPieces
tokenization which has slightly better performance than BPE but cannot
encode all words and is therefore less applicable in FL.</p>
</div>
<div id="S5.SS1.p8" class="ltx_para ltx_noindent">
<p id="S5.SS1.p8.1" class="ltx_p"><span id="S5.SS1.p8.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Note on
splitting data.</span>  Whereas the original LEAF dataset for Reddit proposes
to split each user’s data we argue that in real life not every user might
have a chance to participate in the training. Therefore, we split users
into two distinct training and test sets and evaluate the model on data
from the users who have never participated in the training. This results
in notably increased test perplexity but provides a clear separation
between training and inference modes.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Word accuracy suffers for word-level tokenization that uses mismatched data.</figcaption>
<table id="S5.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.2" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T1.1.1.3" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S5.T1.1.1.1" class="ltx_td ltx_align_center" colspan="2">
<math id="S5.T1.1.1.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S5.T1.1.1.1.m1.1a"><mi id="S5.T1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">\tau</annotation></semantics></math> statistics</td>
<td id="S5.T1.1.1.4" class="ltx_td ltx_nopad_l ltx_align_right">Word</td>
</tr>
<tr id="S5.T1.2.3.1" class="ltx_tr">
<th id="S5.T1.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Type</th>
<th id="S5.T1.2.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Data</th>
<td id="S5.T1.2.3.1.3" class="ltx_td ltx_align_right"><span id="S5.T1.2.3.1.3.1" class="ltx_text ltx_font_typewriter">OOV</span></td>
<td id="S5.T1.2.3.1.4" class="ltx_td ltx_nopad_l ltx_align_right">Tokens</td>
<td id="S5.T1.2.3.1.5" class="ltx_td ltx_nopad_l ltx_align_right">Accuracy</td>
</tr>
<tr id="S5.T1.2.2" class="ltx_tr">
<th id="S5.T1.2.2.2" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S5.T1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">to train <math id="S5.T1.2.2.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S5.T1.2.2.1.m1.1a"><mi id="S5.T1.2.2.1.m1.1.1" xref="S5.T1.2.2.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.1.m1.1b"><ci id="S5.T1.2.2.1.m1.1.1.cmml" xref="S5.T1.2.2.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.1.m1.1c">\tau</annotation></semantics></math>
</th>
<td id="S5.T1.2.2.3" class="ltx_td ltx_align_right">(%)</td>
<td id="S5.T1.2.2.4" class="ltx_td ltx_nopad_l ltx_align_right">per word</td>
<td id="S5.T1.2.2.5" class="ltx_td ltx_nopad_l ltx_align_right">(%)</td>
</tr>
<tr id="S5.T1.2.4.2" class="ltx_tr">
<th id="S5.T1.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="5"><span id="S5.T1.2.4.2.1.1" class="ltx_text ltx_font_italic">Reddit</span></th>
</tr>
<tr id="S5.T1.2.5.3" class="ltx_tr">
<th id="S5.T1.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Word-Level</th>
<th id="S5.T1.2.5.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Wiki</th>
<td id="S5.T1.2.5.3.3" class="ltx_td ltx_align_right">13.0</td>
<td id="S5.T1.2.5.3.4" class="ltx_td ltx_nopad_l ltx_align_right">1.00</td>
<td id="S5.T1.2.5.3.5" class="ltx_td ltx_nopad_l ltx_align_right">17.7</td>
</tr>
<tr id="S5.T1.2.6.4" class="ltx_tr">
<th id="S5.T1.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Word-Level</th>
<th id="S5.T1.2.6.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Oracle</th>
<td id="S5.T1.2.6.4.3" class="ltx_td ltx_align_right">5.5</td>
<td id="S5.T1.2.6.4.4" class="ltx_td ltx_nopad_l ltx_align_right">1.00</td>
<td id="S5.T1.2.6.4.5" class="ltx_td ltx_nopad_l ltx_align_right">24.1</td>
</tr>
<tr id="S5.T1.2.7.5" class="ltx_tr">
<th id="S5.T1.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BPE</th>
<th id="S5.T1.2.7.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Wiki</th>
<td id="S5.T1.2.7.5.3" class="ltx_td ltx_align_right">0.0</td>
<td id="S5.T1.2.7.5.4" class="ltx_td ltx_nopad_l ltx_align_right">1.32</td>
<td id="S5.T1.2.7.5.5" class="ltx_td ltx_nopad_l ltx_align_right">22.2</td>
</tr>
<tr id="S5.T1.2.8.6" class="ltx_tr">
<th id="S5.T1.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BPE</th>
<th id="S5.T1.2.8.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Oracle</th>
<td id="S5.T1.2.8.6.3" class="ltx_td ltx_align_right">0.0</td>
<td id="S5.T1.2.8.6.4" class="ltx_td ltx_nopad_l ltx_align_right">1.22</td>
<td id="S5.T1.2.8.6.5" class="ltx_td ltx_nopad_l ltx_align_right">22.5</td>
</tr>
<tr id="S5.T1.2.9.7" class="ltx_tr">
<th id="S5.T1.2.9.7.1" class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_row ltx_border_t" colspan="5">
<span id="S5.T1.2.9.7.1.1" class="ltx_text ltx_font_italic">StackOverflow</span></th>
</tr>
<tr id="S5.T1.2.10.8" class="ltx_tr">
<th id="S5.T1.2.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Word-Level</th>
<th id="S5.T1.2.10.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Wiki</th>
<td id="S5.T1.2.10.8.3" class="ltx_td ltx_align_right">9.8</td>
<td id="S5.T1.2.10.8.4" class="ltx_td ltx_nopad_l ltx_align_right">1.00</td>
<td id="S5.T1.2.10.8.5" class="ltx_td ltx_nopad_l ltx_align_right">30.0</td>
</tr>
<tr id="S5.T1.2.11.9" class="ltx_tr">
<th id="S5.T1.2.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Word-Level</th>
<th id="S5.T1.2.11.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Oracle</th>
<td id="S5.T1.2.11.9.3" class="ltx_td ltx_align_right">2.0</td>
<td id="S5.T1.2.11.9.4" class="ltx_td ltx_nopad_l ltx_align_right">1.00</td>
<td id="S5.T1.2.11.9.5" class="ltx_td ltx_nopad_l ltx_align_right">33.0</td>
</tr>
<tr id="S5.T1.2.12.10" class="ltx_tr">
<th id="S5.T1.2.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BPE</th>
<th id="S5.T1.2.12.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Wiki</th>
<td id="S5.T1.2.12.10.3" class="ltx_td ltx_align_right">0.0</td>
<td id="S5.T1.2.12.10.4" class="ltx_td ltx_nopad_l ltx_align_right">1.41</td>
<td id="S5.T1.2.12.10.5" class="ltx_td ltx_nopad_l ltx_align_right">31.8</td>
</tr>
<tr id="S5.T1.2.13.11" class="ltx_tr">
<th id="S5.T1.2.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">BPE</th>
<th id="S5.T1.2.13.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Oracle</th>
<td id="S5.T1.2.13.11.3" class="ltx_td ltx_align_right ltx_border_bb">0.0</td>
<td id="S5.T1.2.13.11.4" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_bb">1.24</td>
<td id="S5.T1.2.13.11.5" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_bb">32.4</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Comparing tokenization schemes</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Table <a href="#S5.T1" title="Table 1 ‣ 5.1 Experimental setup. ‣ 5 Experiments ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes experiments that use different
tokenization schemes.
We compute statistics on tokenizers: the average share of <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">OOV</span> tokens for the
word-level scheme and the average number of tokens required to encode one
word for the sub-word scheme.
To compare the effect of each tokenizer on the PFL-trained model, we report word-level accuracy, for the reasons described in Section <a href="#S3.SS2" title="3.2 Evaluating language models across tokenizations ‣ 3 Tokenization in Language Modeling ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
The “wiki” tokenizers are trained on the
Wikipedia data, and the “oracle” tokenizers directly on the training
data.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Word-level tokenization provides high word accuracy when it
is trained using “oracle” user training data. However, when the
word-level has access to only public “wiki” dataset that mismatches
user distribution the performance
significantly drops: by 26 % for Reddit and 10 % for StackOverflow
with a significant increase in out-of-vocabulary share. However, BPE
tokenizers that use public data perform more consistently and outperform the
word-level models trained on public data, but still require a large number
of tokens per each word.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Learning a tokenizer with sampling</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">A key part of the proposed algorithm is the sampling from a model that
uses a public tokenizer <math id="S5.SS3.p1.1.m1.1" class="ltx_Math" alttext="\tau_{pub}" display="inline"><semantics id="S5.SS3.p1.1.m1.1a"><msub id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mi id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml">τ</mi><mrow id="S5.SS3.p1.1.m1.1.1.3" xref="S5.SS3.p1.1.m1.1.1.3.cmml"><mi id="S5.SS3.p1.1.m1.1.1.3.2" xref="S5.SS3.p1.1.m1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.1.m1.1.1.3.1" xref="S5.SS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS3.p1.1.m1.1.1.3.3" xref="S5.SS3.p1.1.m1.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.1.m1.1.1.3.1a" xref="S5.SS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS3.p1.1.m1.1.1.3.4" xref="S5.SS3.p1.1.m1.1.1.3.4.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS3.p1.1.m1.1.1.2.cmml" xref="S5.SS3.p1.1.m1.1.1.2">𝜏</ci><apply id="S5.SS3.p1.1.m1.1.1.3.cmml" xref="S5.SS3.p1.1.m1.1.1.3"><times id="S5.SS3.p1.1.m1.1.1.3.1.cmml" xref="S5.SS3.p1.1.m1.1.1.3.1"></times><ci id="S5.SS3.p1.1.m1.1.1.3.2.cmml" xref="S5.SS3.p1.1.m1.1.1.3.2">𝑝</ci><ci id="S5.SS3.p1.1.m1.1.1.3.3.cmml" xref="S5.SS3.p1.1.m1.1.1.3.3">𝑢</ci><ci id="S5.SS3.p1.1.m1.1.1.3.4.cmml" xref="S5.SS3.p1.1.m1.1.1.3.4">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">\tau_{pub}</annotation></semantics></math>, but is trained with private
federated learning and should represent the words in the actual
data. The sampling is implemented as in Algorithm <a href="#alg1" title="Algorithm 1 ‣ 4.3 Adapting the language model to the new tokenizer ‣ 4 Learning a Tokenizer with Private Federated Learning ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S5.F3" class="ltx_figure">
<div id="S5.F3.1" class="ltx_block ltx_minipage ltx_align_center ltx_align_middle" style="width:368.6pt;">
<p id="S5.F3.1.1" class="ltx_p"><em id="S5.F3.1.1.1" class="ltx_emph ltx_font_italic">Reddit</em></p>
<p id="S5.F3.1.2" class="ltx_p ltx_align_left">i would love to know why we may already live in a consolation subreddit and the aforementioned it will almost always be done on the warrior sheet shows from the west . i</p>
<p id="S5.F3.1.3" class="ltx_p ltx_align_left"><em id="S5.F3.1.3.1" class="ltx_emph ltx_font_italic">StackOverflow</em></p>
<p id="S5.F3.1.4" class="ltx_p ltx_align_left">json results are : can anyone provide a complete sample response ( lists of descendants list ) to my page depending on future python functions . in web apps that require patient for many</p>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example of sampling data from the model.</figcaption>
</figure>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">First, Figure <a href="#S5.F3" title="Figure 3 ‣ 5.3 Learning a tokenizer with sampling ‣ 5 Experiments ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows samples from the language
models on the two data sets. Although clearly the samples are less
coherent than the underlying data, it seems plausible that the word
occurrences match that data.</p>
</div>
<figure id="S5.T2" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Tokenizers initialized on sampled data perform very close to using “oracle” data.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S5.T2.3" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.3.4.1" class="ltx_tr">
<th id="S5.T2.3.4.1.1" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S5.T2.3.4.1.2" class="ltx_td ltx_nopad_l ltx_th ltx_th_column ltx_border_r"></th>
<th id="S5.T2.3.4.1.3" class="ltx_td ltx_nopad_l ltx_th ltx_th_column ltx_border_r"></th>
<th id="S5.T2.3.4.1.4" class="ltx_td ltx_th ltx_th_column ltx_border_r"></th>
<th id="S5.T2.3.4.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2">LM</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.3.5.1" class="ltx_tr">
<td id="S5.T2.3.5.1.1" class="ltx_td ltx_align_left">Type</td>
<td id="S5.T2.3.5.1.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r">Data</td>
<td id="S5.T2.3.5.1.3" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_r">Data</td>
<td id="S5.T2.3.5.1.4" class="ltx_td ltx_align_right ltx_border_r">Tokens</td>
<td id="S5.T2.3.5.1.5" class="ltx_td ltx_align_right">Acc.</td>
<td id="S5.T2.3.5.1.6" class="ltx_td ltx_nopad_l ltx_align_right">Perp.</td>
</tr>
<tr id="S5.T2.1.1" class="ltx_tr">
<td id="S5.T2.1.1.2" class="ltx_td"></td>
<td id="S5.T2.1.1.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r">to train <math id="S5.T2.1.1.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S5.T2.1.1.1.m1.1a"><mi id="S5.T2.1.1.1.m1.1.1" xref="S5.T2.1.1.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">\tau</annotation></semantics></math>
</td>
<td id="S5.T2.1.1.3" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_r">KLD</td>
<td id="S5.T2.1.1.4" class="ltx_td ltx_align_right ltx_border_r">p/word</td>
<td id="S5.T2.1.1.5" class="ltx_td ltx_align_right">(%)</td>
<td id="S5.T2.1.1.6" class="ltx_td ltx_nopad_l"></td>
</tr>
<tr id="S5.T2.3.6.2" class="ltx_tr">
<th id="S5.T2.3.6.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-bottom:5.69046pt;" colspan="5"><span id="S5.T2.3.6.2.1.1" class="ltx_text ltx_font_italic">Reddit</span></th>
<th id="S5.T2.3.6.2.2" class="ltx_td ltx_th ltx_th_column ltx_border_t" style="padding-bottom:5.69046pt;"></th>
</tr>
<tr id="S5.T2.3.7.3" class="ltx_tr">
<td id="S5.T2.3.7.3.1" class="ltx_td ltx_align_left">BPE</td>
<td id="S5.T2.3.7.3.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r">Wiki</td>
<td id="S5.T2.3.7.3.3" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_r">0.78</td>
<td id="S5.T2.3.7.3.4" class="ltx_td ltx_align_right ltx_border_r">1.32</td>
<td id="S5.T2.3.7.3.5" class="ltx_td ltx_align_right">22.2</td>
<td id="S5.T2.3.7.3.6" class="ltx_td ltx_nopad_l ltx_align_right">276.5</td>
</tr>
<tr id="S5.T2.3.8.4" class="ltx_tr">
<td id="S5.T2.3.8.4.1" class="ltx_td ltx_align_left" style="padding-bottom:5.69046pt;">BPE</td>
<td id="S5.T2.3.8.4.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r" style="padding-bottom:5.69046pt;">Oracle</td>
<td id="S5.T2.3.8.4.3" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_r" style="padding-bottom:5.69046pt;">0</td>
<td id="S5.T2.3.8.4.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:5.69046pt;">1.22</td>
<td id="S5.T2.3.8.4.5" class="ltx_td ltx_align_right" style="padding-bottom:5.69046pt;">22.5</td>
<td id="S5.T2.3.8.4.6" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-bottom:5.69046pt;">256.9</td>
</tr>
<tr id="S5.T2.2.2" class="ltx_tr">
<td id="S5.T2.2.2.2" class="ltx_td ltx_align_left">BPE</td>
<td id="S5.T2.2.2.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r">Heavy hitters<sup id="S5.T2.2.2.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S5.T2.2.2.3" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_r">0.09</td>
<td id="S5.T2.2.2.4" class="ltx_td ltx_align_right ltx_border_r">1.30</td>
<td id="S5.T2.2.2.5" class="ltx_td ltx_align_right">22.1</td>
<td id="S5.T2.2.2.6" class="ltx_td ltx_nopad_l ltx_align_right">274.2</td>
</tr>
<tr id="S5.T2.3.9.5" class="ltx_tr">
<td id="S5.T2.3.9.5.1" class="ltx_td ltx_align_left">BPE</td>
<td id="S5.T2.3.9.5.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r"><span id="S5.T2.3.9.5.2.1" class="ltx_text ltx_font_bold">Sampled</span></td>
<td id="S5.T2.3.9.5.3" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_r">0.02</td>
<td id="S5.T2.3.9.5.4" class="ltx_td ltx_align_right ltx_border_r">1.22</td>
<td id="S5.T2.3.9.5.5" class="ltx_td ltx_align_right">22.5</td>
<td id="S5.T2.3.9.5.6" class="ltx_td ltx_nopad_l ltx_align_right">257.7</td>
</tr>
<tr id="S5.T2.3.10.6" class="ltx_tr">
<th id="S5.T2.3.10.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-bottom:5.69046pt;" colspan="5"><span id="S5.T2.3.10.6.1.1" class="ltx_text ltx_font_italic">StackOverflow</span></th>
<th id="S5.T2.3.10.6.2" class="ltx_td ltx_th ltx_th_column ltx_border_t" style="padding-bottom:5.69046pt;"></th>
</tr>
<tr id="S5.T2.3.11.7" class="ltx_tr">
<td id="S5.T2.3.11.7.1" class="ltx_td ltx_align_left">BPE</td>
<td id="S5.T2.3.11.7.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r">Wiki</td>
<td id="S5.T2.3.11.7.3" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_r">1.06</td>
<td id="S5.T2.3.11.7.4" class="ltx_td ltx_align_right ltx_border_r">1.41</td>
<td id="S5.T2.3.11.7.5" class="ltx_td ltx_align_right">31.8</td>
<td id="S5.T2.3.11.7.6" class="ltx_td ltx_nopad_l ltx_align_right">124.6</td>
</tr>
<tr id="S5.T2.3.12.8" class="ltx_tr">
<td id="S5.T2.3.12.8.1" class="ltx_td ltx_align_left" style="padding-bottom:5.69046pt;">BPE</td>
<td id="S5.T2.3.12.8.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r" style="padding-bottom:5.69046pt;">Oracle</td>
<td id="S5.T2.3.12.8.3" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_r" style="padding-bottom:5.69046pt;">0</td>
<td id="S5.T2.3.12.8.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-bottom:5.69046pt;">1.24</td>
<td id="S5.T2.3.12.8.5" class="ltx_td ltx_align_right" style="padding-bottom:5.69046pt;">32.4</td>
<td id="S5.T2.3.12.8.6" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-bottom:5.69046pt;">108.2</td>
</tr>
<tr id="S5.T2.3.3" class="ltx_tr">
<td id="S5.T2.3.3.2" class="ltx_td ltx_align_left">BPE</td>
<td id="S5.T2.3.3.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r">Heavy hitters<sup id="S5.T2.3.3.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S5.T2.3.3.3" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_r">0.10</td>
<td id="S5.T2.3.3.4" class="ltx_td ltx_align_right ltx_border_r">1.29</td>
<td id="S5.T2.3.3.5" class="ltx_td ltx_align_right">32.1</td>
<td id="S5.T2.3.3.6" class="ltx_td ltx_nopad_l ltx_align_right">115.9</td>
</tr>
<tr id="S5.T2.3.13.9" class="ltx_tr">
<td id="S5.T2.3.13.9.1" class="ltx_td ltx_align_left ltx_border_bb">BPE</td>
<td id="S5.T2.3.13.9.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_bb ltx_border_r"><span id="S5.T2.3.13.9.2.1" class="ltx_text ltx_font_bold">Sampled</span></td>
<td id="S5.T2.3.13.9.3" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_bb ltx_border_r">0.01</td>
<td id="S5.T2.3.13.9.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r">1.23</td>
<td id="S5.T2.3.13.9.5" class="ltx_td ltx_align_right ltx_border_bb">32.4</td>
<td id="S5.T2.3.13.9.6" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_bb">108.7</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.T2.4" class="ltx_p ltx_figure_panel"><sup id="S5.T2.4.1" class="ltx_sup">∗</sup>The “heavy hitters” algorithm requires additional privacy budget.</p>
</div>
</div>
</figure>
<figure id="S5.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F4.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2203.09943/assets/x3.png" id="S5.F4.sf1.g1" class="ltx_graphics ltx_img_landscape" width="166" height="110" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Reddit dataset</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F4.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2203.09943/assets/x4.png" id="S5.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="166" height="110" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>StackOverflow dataset</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Perplexity for switching the tokenizer at different rounds of federated learning.</figcaption>
</figure>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">Second, Table <a href="#S5.T2" title="Table 2 ‣ 5.3 Learning a tokenizer with sampling ‣ 5 Experiments ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> further investigates the properties of the
sampled text. The “BPE sample” rows refer to the method proposed in
this paper. A language model with the “wiki” tokenizer is trained
with PFL on the first half of the training data. Then samples are drawn
from this language model. Then, the language model is trained from
scratch on the second half of the training data.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p">The “BPE Heavy hitters” rows refer to training with a differentially private
“heavy hitters” algorithm <cite class="ltx_cite ltx_citemacro_cite">Apple (<a href="#bib.bib2" title="" class="ltx_ref">2017</a>)</cite>. Each of the
population of the users from the first half of the
training set contributes three words from the from
the Wikipedia dataset, with a local privacy budget of <math id="S5.SS3.p4.1.m1.1" class="ltx_Math" alttext="\epsilon=8" display="inline"><semantics id="S5.SS3.p4.1.m1.1a"><mrow id="S5.SS3.p4.1.m1.1.1" xref="S5.SS3.p4.1.m1.1.1.cmml"><mi id="S5.SS3.p4.1.m1.1.1.2" xref="S5.SS3.p4.1.m1.1.1.2.cmml">ϵ</mi><mo id="S5.SS3.p4.1.m1.1.1.1" xref="S5.SS3.p4.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS3.p4.1.m1.1.1.3" xref="S5.SS3.p4.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.1b"><apply id="S5.SS3.p4.1.m1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1"><eq id="S5.SS3.p4.1.m1.1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1.1"></eq><ci id="S5.SS3.p4.1.m1.1.1.2.cmml" xref="S5.SS3.p4.1.m1.1.1.2">italic-ϵ</ci><cn type="integer" id="S5.SS3.p4.1.m1.1.1.3.cmml" xref="S5.SS3.p4.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.1c">\epsilon=8</annotation></semantics></math>.
Just like for the sampling approach, the language model is then trained
from scratch on the second half of the training data.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para">
<p id="S5.SS3.p5.1" class="ltx_p">First, we examine the difference between the real training data and the
data used to train the tokenizers. The column “Data KLD” shows the KL
divergence from the user “oracle” training data to the sampled data. The KL
divergence is computed from the unigram counts, which are relevant for
training a tokenizer, over the top
10,000 words from the training data and with add-1 smoothing. The KL divergence to the training
data itself, which the oracle tokenizer is trained on, is 0 by
definition. The KL divergence between the actual data and the Wikipedia
data, on the other hand, is around 1, for both datasets. Both the heavy
hitters algorithm and the algorithm we propose in this paper find a
distribution close to the real distribution.</p>
</div>
<div id="S5.SS3.p6" class="ltx_para">
<p id="S5.SS3.p6.1" class="ltx_p">For sub-word tokenizers, the number of tokens per word is relevant.
Even though they can represent unseen words by multiple tokens, a
language model trained on top of that has a harder task given the
longer context on average. The oracle tokenizer has the lowest number
of tokens per words and the “wiki” tokenizer the highest. The “BPE
sample” tokenizer comes very close to the oracle tokenizer.</p>
</div>
<div id="S5.SS3.p7" class="ltx_para">
<p id="S5.SS3.p7.1" class="ltx_p">However, the heavy hitters experiment shows much smaller gain in
performance, i.e. better than “wiki” tokenizer but still worse than
our proposed sampling method. Furthermore, it requires a separate
privacy budget allocated for the run, while sampling can operate on
existing prior model.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Iterative updates</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">This part implements Algorithm <a href="#alg1" title="Algorithm 1 ‣ 4.3 Adapting the language model to the new tokenizer ‣ 4 Learning a Tokenizer with Private Federated Learning ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> completely. We
again initialize the tokenizer on publicly available data. We then
train the language model with PFL. At a point during training, we
retrain the tokenizer by sampling.
Unlike in the previous section, we
update the language model by remapping its embedding layer, and continue
training. We sample the same data before and after changing the
tokenizer.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">Figure <a href="#S5.F4" title="Figure 4 ‣ 5.3 Learning a tokenizer with sampling ‣ 5 Experiments ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the results for changing tokenizers at
different times.
The “Baseline” curve represents the model trained using public
tokenizer <math id="S5.SS4.p2.1.m1.1" class="ltx_Math" alttext="\tau_{pub}" display="inline"><semantics id="S5.SS4.p2.1.m1.1a"><msub id="S5.SS4.p2.1.m1.1.1" xref="S5.SS4.p2.1.m1.1.1.cmml"><mi id="S5.SS4.p2.1.m1.1.1.2" xref="S5.SS4.p2.1.m1.1.1.2.cmml">τ</mi><mrow id="S5.SS4.p2.1.m1.1.1.3" xref="S5.SS4.p2.1.m1.1.1.3.cmml"><mi id="S5.SS4.p2.1.m1.1.1.3.2" xref="S5.SS4.p2.1.m1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p2.1.m1.1.1.3.1" xref="S5.SS4.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS4.p2.1.m1.1.1.3.3" xref="S5.SS4.p2.1.m1.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p2.1.m1.1.1.3.1a" xref="S5.SS4.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS4.p2.1.m1.1.1.3.4" xref="S5.SS4.p2.1.m1.1.1.3.4.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.1.m1.1b"><apply id="S5.SS4.p2.1.m1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS4.p2.1.m1.1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS4.p2.1.m1.1.1.2.cmml" xref="S5.SS4.p2.1.m1.1.1.2">𝜏</ci><apply id="S5.SS4.p2.1.m1.1.1.3.cmml" xref="S5.SS4.p2.1.m1.1.1.3"><times id="S5.SS4.p2.1.m1.1.1.3.1.cmml" xref="S5.SS4.p2.1.m1.1.1.3.1"></times><ci id="S5.SS4.p2.1.m1.1.1.3.2.cmml" xref="S5.SS4.p2.1.m1.1.1.3.2">𝑝</ci><ci id="S5.SS4.p2.1.m1.1.1.3.3.cmml" xref="S5.SS4.p2.1.m1.1.1.3.3">𝑢</ci><ci id="S5.SS4.p2.1.m1.1.1.3.4.cmml" xref="S5.SS4.p2.1.m1.1.1.3.4">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.1.m1.1c">\tau_{pub}</annotation></semantics></math> from Wikipedia data.
Each of the other curves takes the system from the “Baseline” curve at a
different iteration. As expected, the initial remapping of the
embedding layer is not perfect and needs finetuning. The graph also
shows the tradeoff in when to change tokenizers: too early, e.g. after
only 1000 iterations, and the tokenizer is not representative enough
yet; too late, e.g. after 4000 iterations, and there is not enough time
to converge again.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper has proposed a method that allows a tokenizer to be found together with a language model using private federated learning.
First, it has shown that a mismatched tokenizer can cause a significant performance degradation.
The key to improving this is to use a sub-word tokenizer which allows new words to be represented as a sequence of tokens.
Then, a language model trained with PFL can represent the private data.
This paper has presented a method to produce a new tokenizer from that model, and to convert the model to work with the new tokenizer.
When this is trained further with private federated learning, it outperforms the language model with the mismatched tokenizer, and gets close to one with the oracle tokenizer.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Personalization and Fairness.</span> 
The problem of out-of-vocabulary words might be more acute for some users that use unique vocabulary, such as dialect, and impact individual performance.
Therefore good tokenizers can benefit personalization in federated models <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib19" title="" class="ltx_ref">2021</a>); Yu et al. (<a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadi et al. (2016)</span>
<span class="ltx_bibblock">
Martín Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov,
Kunal Talwar, and Li Zhang. 2016.

</span>
<span class="ltx_bibblock">Deep learning with differential privacy.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">CCS</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Apple (2017)</span>
<span class="ltx_bibblock">
Differential Privacy Team Apple. 2017.

</span>
<span class="ltx_bibblock">Learning with privacy at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Apple Mach. Learn. J</em>, 1(8):1–25.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balle et al. (2018)</span>
<span class="ltx_bibblock">
Borja Balle, Gilles Barthe, and Marco Gaboardi. 2018.

</span>
<span class="ltx_bibblock">Privacy amplification by subsampling: Tight analyses via couplings
and divergences.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">NIPS</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bassily et al. (2017)</span>
<span class="ltx_bibblock">
Raef Bassily, Kobbi Nissim, Uri Stemmer, and Abhradeep Thakurta. 2017.

</span>
<span class="ltx_bibblock">Practical locally private heavy hitters.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1707.04982</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beaufays et al. (2019)</span>
<span class="ltx_bibblock">
Françoise Simone Beaufays, Mingqing Chen, Rajiv Mathews, and Tom Ouyang. 2019.

</span>
<span class="ltx_bibblock">Federated learning of out-of-vocabulary words.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1903.10635</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldas et al. (2018)</span>
<span class="ltx_bibblock">
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub
Konečnỳ, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar.
2018.

</span>
<span class="ltx_bibblock">Leaf: A benchmark for federated settings.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.01097</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork (2011)</span>
<span class="ltx_bibblock">
Cynthia Dwork. 2011.

</span>
<span class="ltx_bibblock">Differential privacy.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Encyclopedia of Cryptography and Security</em>, pages 338–340.
Springer.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork et al. (2006)</span>
<span class="ltx_bibblock">
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006.

</span>
<span class="ltx_bibblock">Calibrating noise to sensitivity in private data analysis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Theory of cryptography conference</em>, pages 265–284.
Springer.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork et al. (2014)</span>
<span class="ltx_bibblock">
Cynthia Dwork, Aaron Roth, et al. 2014.

</span>
<span class="ltx_bibblock">The algorithmic foundations of differential privacy.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Found. Trends Theor. Comput. Sci.</em>, 9(3-4):211–407.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feldman et al. (2021)</span>
<span class="ltx_bibblock">
Vitaly Feldman, Audra McMillan, and Kunal Talwar. 2021.

</span>
<span class="ltx_bibblock">Hiding among the clones: A simple and nearly optimal analysis of
privacy amplification by shuffling.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE Symposium on Foundations of Computer Science (FOCS)</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geyer et al. (2017)</span>
<span class="ltx_bibblock">
Robin C Geyer, Tassilo Klein, and Moin Nabi. 2017.

</span>
<span class="ltx_bibblock">Differentially private federated learning: A client level
perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1712.07557</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hard et al. (2018)</span>
<span class="ltx_bibblock">
Andrew Hard, Kanishka Rao, Rajiv Mathews, Françoise Beaufays, Sean
Augenstein, Hubert Eichner, Chloé Kiddon, and Daniel Ramage. 2018.

</span>
<span class="ltx_bibblock">Federated learning for mobile keyboard prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv:1811.03604</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2021)</span>
<span class="ltx_bibblock">
Shengyuan Hu, Zhiwei Steven Wu, and Virginia Smith. 2021.

</span>
<span class="ltx_bibblock">Private multi-task learning: Formulation and applications to
federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.12978</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huggingface (2021)</span>
<span class="ltx_bibblock">
Huggingface. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/huggingface/tokenizers" title="" class="ltx_ref ltx_href">huggingface/tokenizers: Fast state-of-the-art tokenizers optimized for
research and production</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaggle (2021)</span>
<span class="ltx_bibblock">
Kaggle. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.kaggle.com/stackoverflow/stackoverflow" title="" class="ltx_ref ltx_href">Kaggle
stackoverflow data</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz et al. (2019)</span>
<span class="ltx_bibblock">
Peter Kairouz et al. 2019.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv:1912.04977</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2015)</span>
<span class="ltx_bibblock">
Diederick P Kingma and Jimmy Ba. 2015.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson. 2018.

</span>
<span class="ltx_bibblock">Sentencepiece: A simple and language independent subword tokenizer
and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Demo At EMNLP</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. 2021.

</span>
<span class="ltx_bibblock">Ditto: Fair and robust federated learning through personalization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
6357–6368. PMLR.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Likhomanenko et al. (2019)</span>
<span class="ltx_bibblock">
Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert. 2019.

</span>
<span class="ltx_bibblock">Who needs words? Lexicon-free speech recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of Interspeech</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017)</span>
<span class="ltx_bibblock">
H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise
Agüera y Arcas. 2017.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">AISTATS</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2018)</span>
<span class="ltx_bibblock">
H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. 2018.

</span>
<span class="ltx_bibblock">Learning differentially private recurrent language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">ICLR</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Melis et al. (2019)</span>
<span class="ltx_bibblock">
Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. 2019.

</span>
<span class="ltx_bibblock">Exploiting unintended feature leakage in collaborative learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">S&amp;P</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Merity et al. (2017)</span>
<span class="ltx_bibblock">
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017.

</span>
<span class="ltx_bibblock">Pointer sentinel mixture models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ICLR</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mielke (2019)</span>
<span class="ltx_bibblock">
Sabrina J. Mielke. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://sjmielke.com/comparing-perplexities.htm" title="" class="ltx_ref ltx_href">Can you
compare perplexity across different segmentations?</a>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller et al. (2020)</span>
<span class="ltx_bibblock">
John Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt. 2020.

</span>
<span class="ltx_bibblock">The effect of natural distribution shift on question answering
models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
6905–6916. PMLR.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuster and Nakajima (2012)</span>
<span class="ltx_bibblock">
Mike Schuster and Kaisuke Nakajima. 2012.

</span>
<span class="ltx_bibblock">Japanese and korean voice search.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">International Conference on Acoustics, Speech and Signal
Processing</em>, pages 5149–5152.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et al. (2016)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P16-1162" title="" class="ltx_ref ltx_href">Neural machine
translation of rare words with subword units</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1715–1725,
Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shokri et al. (2017)</span>
<span class="ltx_bibblock">
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017.

</span>
<span class="ltx_bibblock">Membership inference attacks against machine learning models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">S&amp;P</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2017)</span>
<span class="ltx_bibblock">
Congzheng Song, Thomas Ristenpart, and Vitaly Shmatikov. 2017.

</span>
<span class="ltx_bibblock">Machine learning models that remember too much.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">CCS</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2020)</span>
<span class="ltx_bibblock">
Tao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov. 2020.

</span>
<span class="ltx_bibblock">Salvaging federated learning by local adaptation.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.04758</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2020)</span>
<span class="ltx_bibblock">
Wennan Zhu, Peter Kairouz, Brendan McMahan, Haicheng Sun, and Wei Li. 2020.

</span>
<span class="ltx_bibblock">Federated heavy hitters discovery with differential privacy.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence and
Statistics</em>, pages 3837–3847. PMLR.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Impact of hyperparameters</h2>

<figure id="A1.F5" class="ltx_figure"><img src="/html/2203.09943/assets/x5.png" id="A1.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="164" height="110" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Perplexity trained with different privacy parameter <math id="A1.F5.2.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="A1.F5.2.m1.1b"><mi id="A1.F5.2.m1.1.1" xref="A1.F5.2.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="A1.F5.2.m1.1c"><ci id="A1.F5.2.m1.1.1.cmml" xref="A1.F5.2.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F5.2.m1.1d">\epsilon</annotation></semantics></math>.</figcaption>
</figure>
<figure id="A1.F6" class="ltx_figure"><img src="/html/2203.09943/assets/x6.png" id="A1.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="166" height="109" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Perplexity trained with different cohort sizes.</figcaption>
</figure>
<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">This section examines different hyperparameters.</p>
</div>
<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Experimental design</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">First, consider the choice to train the public tokenizer on Wikipedia data.
To examine the effect of using a more conversational style corpus.
To do this, Table <a href="#A1.T3" title="Table 3 ‣ A.1 Experimental design ‣ Appendix A Impact of hyperparameters ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> takes a subset of the numbers from Table <a href="#S5.T2" title="Table 2 ‣ 5.3 Learning a tokenizer with sampling ‣ 5 Experiments ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and adds a scenario where a tokenizer on StackOverflow data is used with Reddit data and vice versa.
The cross-dataset numbers are highlighted bold in the table.</p>
</div>
<div id="A1.SS1.p2" class="ltx_para">
<p id="A1.SS1.p2.1" class="ltx_p">First, in terms of the KL divergence the StackOverflow data seems a slightly better model for the Reddit distribution than the Wikipedia data is.
However, when using PFL to train on Reddit data, but with a StackOverflow-trained tokenizer, the perplexity deteriorates compared to the Wikipedia-trained tokenizer.
Second, the reverse experiment looks a bit better but not hugely better.
Though the KL divergence from the StackOverflow data to the Reddit data is significantly better than the KL divergence to the Wikipedia data, some of that advantage disappears in the final trained model.</p>
</div>
<figure id="A1.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The effect of using the Wikipedia corpus against the results in Table <a href="#S5.T2" title="Table 2 ‣ 5.3 Learning a tokenizer with sampling ‣ 5 Experiments ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</figcaption>
<table id="A1.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T3.1.1" class="ltx_tr">
<td id="A1.T3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><math id="A1.T3.1.1.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="A1.T3.1.1.1.m1.1a"><mi id="A1.T3.1.1.1.m1.1.1" xref="A1.T3.1.1.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="A1.T3.1.1.1.m1.1b"><ci id="A1.T3.1.1.1.m1.1.1.cmml" xref="A1.T3.1.1.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T3.1.1.1.m1.1c">\tau</annotation></semantics></math></td>
<td id="A1.T3.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Data</td>
<td id="A1.T3.1.1.3" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r ltx_border_tt">Data</td>
<td id="A1.T3.1.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt">LM</td>
</tr>
<tr id="A1.T3.1.2.1" class="ltx_tr">
<td id="A1.T3.1.2.1.1" class="ltx_td"></td>
<td id="A1.T3.1.2.1.2" class="ltx_td ltx_border_r"></td>
<td id="A1.T3.1.2.1.3" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r">KLD</td>
<td id="A1.T3.1.2.1.4" class="ltx_td ltx_nopad_l ltx_align_center">perp.</td>
</tr>
<tr id="A1.T3.1.3.2" class="ltx_tr">
<td id="A1.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_border_t" colspan="4"><span id="A1.T3.1.3.2.1.1" class="ltx_text ltx_font_italic">Reddit</span></td>
</tr>
<tr id="A1.T3.1.4.3" class="ltx_tr">
<td id="A1.T3.1.4.3.1" class="ltx_td ltx_align_left">BPE</td>
<td id="A1.T3.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r">Wikipedia</td>
<td id="A1.T3.1.4.3.3" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r">0.7826</td>
<td id="A1.T3.1.4.3.4" class="ltx_td ltx_nopad_l ltx_align_center">276.5</td>
</tr>
<tr id="A1.T3.1.5.4" class="ltx_tr">
<td id="A1.T3.1.5.4.1" class="ltx_td ltx_align_left">BPE</td>
<td id="A1.T3.1.5.4.2" class="ltx_td ltx_align_left ltx_border_r"><span id="A1.T3.1.5.4.2.1" class="ltx_text ltx_font_bold">StackOverflow</span></td>
<td id="A1.T3.1.5.4.3" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r">0.6046</td>
<td id="A1.T3.1.5.4.4" class="ltx_td ltx_nopad_l ltx_align_center">283.6</td>
</tr>
<tr id="A1.T3.1.6.5" class="ltx_tr">
<td id="A1.T3.1.6.5.1" class="ltx_td ltx_align_left">BPE</td>
<td id="A1.T3.1.6.5.2" class="ltx_td ltx_align_left ltx_border_r">Reddit</td>
<td id="A1.T3.1.6.5.3" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r">0</td>
<td id="A1.T3.1.6.5.4" class="ltx_td ltx_nopad_l ltx_align_center">256.9</td>
</tr>
<tr id="A1.T3.1.7.6" class="ltx_tr">
<td id="A1.T3.1.7.6.1" class="ltx_td ltx_align_left ltx_border_t">BPE</td>
<td id="A1.T3.1.7.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">sample</td>
<td id="A1.T3.1.7.6.3" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r ltx_border_t">0.0212</td>
<td id="A1.T3.1.7.6.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">257.7</td>
</tr>
<tr id="A1.T3.1.8.7" class="ltx_tr">
<td id="A1.T3.1.8.7.1" class="ltx_td ltx_align_left ltx_border_t" colspan="4"><span id="A1.T3.1.8.7.1.1" class="ltx_text ltx_font_italic">StackOverflow</span></td>
</tr>
<tr id="A1.T3.1.9.8" class="ltx_tr">
<td id="A1.T3.1.9.8.1" class="ltx_td ltx_align_left">BPE</td>
<td id="A1.T3.1.9.8.2" class="ltx_td ltx_align_left ltx_border_r">Wikipedia</td>
<td id="A1.T3.1.9.8.3" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r">1.0629</td>
<td id="A1.T3.1.9.8.4" class="ltx_td ltx_nopad_l ltx_align_center">124.6</td>
</tr>
<tr id="A1.T3.1.10.9" class="ltx_tr">
<td id="A1.T3.1.10.9.1" class="ltx_td ltx_align_left">BPE</td>
<td id="A1.T3.1.10.9.2" class="ltx_td ltx_align_left ltx_border_r"><span id="A1.T3.1.10.9.2.1" class="ltx_text ltx_font_bold">Reddit</span></td>
<td id="A1.T3.1.10.9.3" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r">0.5315</td>
<td id="A1.T3.1.10.9.4" class="ltx_td ltx_nopad_l ltx_align_center">118.8</td>
</tr>
<tr id="A1.T3.1.11.10" class="ltx_tr">
<td id="A1.T3.1.11.10.1" class="ltx_td ltx_align_left">BPE</td>
<td id="A1.T3.1.11.10.2" class="ltx_td ltx_align_left ltx_border_r">StackOverflow</td>
<td id="A1.T3.1.11.10.3" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_r">0</td>
<td id="A1.T3.1.11.10.4" class="ltx_td ltx_nopad_l ltx_align_center">108.2</td>
</tr>
<tr id="A1.T3.1.12.11" class="ltx_tr">
<td id="A1.T3.1.12.11.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">BPE</td>
<td id="A1.T3.1.12.11.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">sample</td>
<td id="A1.T3.1.12.11.3" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">0.0089</td>
<td id="A1.T3.1.12.11.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_t">108.7</td>
</tr>
</tbody>
</table>
</figure>
<div id="A1.SS1.p3" class="ltx_para">
<p id="A1.SS1.p3.1" class="ltx_p">Then, consider the choice of vocabulary size, here the number of distinct tokens.
Table <a href="#A1.T4" title="Table 4 ‣ A.1 Experimental design ‣ Appendix A Impact of hyperparameters ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the perplexities for the baseline (“Wiki”) and ceiling (“oracle”) experiments.
Though the absolute numbers change, the trends do not change.</p>
</div>
<figure id="A1.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>The effect of varying the vocabulary size.</figcaption>
<table id="A1.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T4.1.1.1" class="ltx_tr">
<th id="A1.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Vocab size</th>
<th id="A1.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2">Reddit</th>
<th id="A1.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">StackOverflow</th>
</tr>
<tr id="A1.T4.1.2.2" class="ltx_tr">
<th id="A1.T4.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="A1.T4.1.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_column">Wiki</th>
<th id="A1.T4.1.2.2.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r">Oracle</th>
<th id="A1.T4.1.2.2.4" class="ltx_td ltx_align_right ltx_th ltx_th_column">Wiki</th>
<th id="A1.T4.1.2.2.5" class="ltx_td ltx_align_right ltx_th ltx_th_column">Oracle</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T4.1.3.1" class="ltx_tr">
<th id="A1.T4.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">5,000</th>
<td id="A1.T4.1.3.1.2" class="ltx_td ltx_align_right ltx_border_t">304.3</td>
<td id="A1.T4.1.3.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">282.2</td>
<td id="A1.T4.1.3.1.4" class="ltx_td ltx_align_right ltx_border_t">136.3</td>
<td id="A1.T4.1.3.1.5" class="ltx_td ltx_align_right ltx_border_t">116.8</td>
</tr>
<tr id="A1.T4.1.4.2" class="ltx_tr">
<th id="A1.T4.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">10,000</th>
<td id="A1.T4.1.4.2.2" class="ltx_td ltx_align_right">276.5</td>
<td id="A1.T4.1.4.2.3" class="ltx_td ltx_align_right ltx_border_r">256.9</td>
<td id="A1.T4.1.4.2.4" class="ltx_td ltx_align_right">124.6</td>
<td id="A1.T4.1.4.2.5" class="ltx_td ltx_align_right">108.2</td>
</tr>
<tr id="A1.T4.1.5.3" class="ltx_tr">
<th id="A1.T4.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">50,000</th>
<td id="A1.T4.1.5.3.2" class="ltx_td ltx_align_right">243.9</td>
<td id="A1.T4.1.5.3.3" class="ltx_td ltx_align_right ltx_border_r">225.4</td>
<td id="A1.T4.1.5.3.4" class="ltx_td ltx_align_right">111.5</td>
<td id="A1.T4.1.5.3.5" class="ltx_td ltx_align_right">101.5</td>
</tr>
<tr id="A1.T4.1.6.4" class="ltx_tr">
<th id="A1.T4.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">100,000</th>
<td id="A1.T4.1.6.4.2" class="ltx_td ltx_align_right ltx_border_bb">231.2</td>
<td id="A1.T4.1.6.4.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r">217.9</td>
<td id="A1.T4.1.6.4.4" class="ltx_td ltx_align_right ltx_border_bb">108.9</td>
<td id="A1.T4.1.6.4.5" class="ltx_td ltx_align_right ltx_border_bb">100.5</td>
</tr>
</tbody>
</table>
</figure>
<div id="A1.SS1.p4" class="ltx_para">
<p id="A1.SS1.p4.1" class="ltx_p">Similarly for changing model architectures.
This paper has presented results on an LSTM model.
Table <a href="#A1.T5" title="Table 5 ‣ A.1 Experimental design ‣ Appendix A Impact of hyperparameters ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows results on a Transformer model.
Again, though the absolute numbers change, the trends do not change.</p>
</div>
<figure id="A1.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>The effect of changing model architectures.</figcaption>
<table id="A1.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T5.1.1.1" class="ltx_tr">
<th id="A1.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Model</th>
<th id="A1.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2">Reddit</th>
<th id="A1.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">StackOverflow</th>
</tr>
<tr id="A1.T5.1.2.2" class="ltx_tr">
<th id="A1.T5.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r">architecture</th>
<th id="A1.T5.1.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_column">Wiki</th>
<th id="A1.T5.1.2.2.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r">Oracle</th>
<th id="A1.T5.1.2.2.4" class="ltx_td ltx_align_right ltx_th ltx_th_column">Wiki</th>
<th id="A1.T5.1.2.2.5" class="ltx_td ltx_align_right ltx_th ltx_th_column">Oracle</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T5.1.3.1" class="ltx_tr">
<th id="A1.T5.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Transformer</th>
<td id="A1.T5.1.3.1.2" class="ltx_td ltx_align_right ltx_border_t">261.9</td>
<td id="A1.T5.1.3.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">244.8</td>
<td id="A1.T5.1.3.1.4" class="ltx_td ltx_align_right ltx_border_t">117.4</td>
<td id="A1.T5.1.3.1.5" class="ltx_td ltx_align_right ltx_border_t">107.0</td>
</tr>
<tr id="A1.T5.1.4.2" class="ltx_tr">
<th id="A1.T5.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">LSTM</th>
<td id="A1.T5.1.4.2.2" class="ltx_td ltx_align_right ltx_border_bb">276.5</td>
<td id="A1.T5.1.4.2.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r">256.9</td>
<td id="A1.T5.1.4.2.4" class="ltx_td ltx_align_right ltx_border_bb">124.6</td>
<td id="A1.T5.1.4.2.5" class="ltx_td ltx_align_right ltx_border_bb">108.2</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Other hyperparameters</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">We consider two hyperparameter choices for experiments: first, the privacy budget, and secondly, the cohort size.</p>
</div>
<div id="A1.SS2.p2" class="ltx_para">
<p id="A1.SS2.p2.1" class="ltx_p">Figure <a href="#A1.F5" title="Figure 5 ‣ Appendix A Impact of hyperparameters ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the effect of different privacy parameters.
The effects are not huge, but clearly differential privacy does impede learning somewhat.</p>
</div>
<div id="A1.SS2.p3" class="ltx_para">
<p id="A1.SS2.p3.1" class="ltx_p">Figure <a href="#A1.F6" title="Figure 6 ‣ Appendix A Impact of hyperparameters ‣ Training a Tokenizer for Free with Private Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the effect of differing cohort sizes.
A larger cohort size implies a better signal-to-noise ratio when training with differential privacy.
However, for practical reasons it is preferable for cohorts to be smaller.
10,000 is a happy medium between good performance and practicality.
Also, again, though the absolute numbers change, the trends do not change.</p>
</div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2203.09940" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2203.09943" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2203.09943">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2203.09943" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2203.09944" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 07:01:40 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
