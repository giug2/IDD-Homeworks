<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>VISTA: A Visual and Textual Attention Dataset for Interpreting Multimodal Models</title>
<!--Generated on Sun Oct  6 19:52:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.04609v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#S1" title="In VISTA: A Visual and Textual Attention Dataset for Interpreting Multimodal Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#S2" title="In VISTA: A Visual and Textual Attention Dataset for Interpreting Multimodal Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#S3" title="In VISTA: A Visual and Textual Attention Dataset for Interpreting Multimodal Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>VISTA: Visual-Textual Attention Saliency Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#S4" title="In VISTA: A Visual and Textual Attention Dataset for Interpreting Multimodal Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Metrics and baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#S5" title="In VISTA: A Visual and Textual Attention Dataset for Interpreting Multimodal Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results and Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#S6" title="In VISTA: A Visual and Textual Attention Dataset for Interpreting Multimodal Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">VISTA: A Visual and Textual Attention Dataset for Interpreting Multimodal Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Harshit <span class="ltx_ERROR undefined" id="id1.1.id1">\orcidlink</span>0000-0002-7600-2219 
<br class="ltx_break"/>School of Computing, 
<br class="ltx_break"/>University of Utah, 
<br class="ltx_break"/>Salt Lake City, UT, USA 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">u1471783@umail.utah.edu</span>
<br class="ltx_break"/>&amp;Tolga Tasdizen <span class="ltx_ERROR undefined" id="id3.3.id3">\orcidlink</span>0000-0001-6574-0366 
<br class="ltx_break"/>Scientific Computing and Imaging Institute, 
<br class="ltx_break"/>University of Utah, 
<br class="ltx_break"/>Salt Lake City, UT, USA 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id4.4.id4">tolga@sci.utah.edu</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">The recent developments in deep learning (DL) led to the integration of natural language processing (NLP) with computer vision, resulting in powerful integrated Vision and Language Models (VLMs). Despite their remarkable capabilities, these models are frequently regarded as black boxes within the machine learning research community. This raises a critical question: which parts of an image correspond to specific segments of text, and how can we decipher these associations? Understanding these connections is essential for enhancing model transparency, interpretability, and trustworthiness. To answer this question, we present an image-text aligned human visual attention dataset that maps specific associations between image regions and corresponding text segments. We then compare the internal heatmaps generated by VL models with this dataset, allowing us to analyze and better understand the model’s decision-making process. This approach aims to enhance model transparency, interpretability, and trustworthiness by providing insights into how these models align visual and linguistic information. We conducted a comprehensive study on text-guided visual saliency detection in these VL models. This study aims to understand how different models prioritize and focus on specific visual elements in response to corresponding text segments, providing deeper insights into their internal mechanisms and improving our ability to interpret their outputs.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Vision-Language Models (VLMs) learn rich representations by leveraging both image and text modalities. They are capable of modeling the complex relationships between these two modalities, largely due to the vast and diverse datasets sourced from the internet on which they have been trained
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx16" title="">Lin et al., 2015</a>]</cite>
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx22" title="">Plummer et al., 2016</a>]</cite>
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx27" title="">Schuhmann et al., 2022</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx26" title="">Schuhmann et al., 2021</a>]</cite>. This extensive training enables VLMs to capture subtle connections between visual and linguistic information, allowing them to perform a wide range of tasks with impressive accuracy and generalization across various domains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx17" title="">Long et al., 2022</a>]</cite>
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx4" title="">Bugliarello et al., 2021</a>]</cite>
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx20" title="">Mogadala et al., 2021</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="304" id="S1.F1.g1" src="x1.png" width="387"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example of heatmaps from VISTA with their corresponding texts.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Despite their impressive performance, VLMs, like many other deep learning models, often lack interpretability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx32" title="">Yue et al., 2024</a>]</cite>. This lack of transparency has prompted researchers to probe into the inner workings of these VL models to understand the nature of the representations they learn <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx11" title="">Lei et al., 2023</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx3" title="">Bousselham et al., 2024</a>]</cite>
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx29" title="">Stan et al., 2024</a>]</cite>. By investigating how these models process and integrate visual and textual information, researchers aim to uncover the underlying mechanisms that drive their decision-making processes and enhance their interpretability. Various interpretability techniques, such as Grad-CAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx28" title="">Selvaraju et al., 2019</a>]</cite> and spatial attention maps, generate saliency maps that highlight the spatial regions most significant for each of the network’s outputs. We propose to measure the similarity between human attention and the saliency maps produced by these methods. This comparison can also serve as a tool for validating and refining these interpretability techniques, ensuring that the models are not only accurate but also aligned with human understanding in critical applications.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Vision-Language Models (VLMs)</span>: In recent years, substantial progress has been made in research on integrating language and vision. Numerous tasks now combine language—ranging from words, phrases, and sentences to paragraphs and full documents—with visual data, typically in the form of images or videos. Initially, much of the work focused on linking low-level linguistic units, such as words, with images or videos to create visual-semantic embeddings (Barnard et al., 2003; Frome et al., 2013; Kiros et al., 2014b; Liu et al., 2015; Cao et al., 2016; Tsai et al., 2017; Guo et al., 2018; Mogadala et al., 2018b; Wang et al., 2019; Kim et al., 2020). These embeddings are valuable for various downstream applications, as well as for understanding adversarial attacks (Wu et al., 2019) to enhance model robustness.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">More recently, leading research labs have consistently introduced cutting-edge VLMs, such as OpenAI’s CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx23" title="">Radford et al., 2021</a>]</cite>, Salesforce’s BLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx13" title="">Li et al., 2022a</a>]</cite>, and DeepMind’s Flamingo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx1" title="">Alayrac et al., 2022</a>]</cite>. Notable examples like GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx21" title="">OpenAI et al., 2024</a>]</cite> and Gemini <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx30" title="">Team et al., 2024</a>]</cite> highlight the ongoing evolution of chatbots within the VLM space. However, not all multimodal models are VLMs; for instance, text-to-image models like DALL-E <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx24" title="">Ramesh et al., 2021</a>]</cite>, Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx25" title="">Rombach et al., 2022</a>]</cite> and Midjourney (https://www.midjourney.com/) do not have a language generation component, illustrating the varied landscape of multimodal AI. A typical VLM architecture includes separate image and text encoders to generate embeddings, which are then combined in an image-text fusion layer, and the fused vector is processed by a large language model (LLM) to produce the final visually-aware text output.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Eye-tracking Saliency Map Datasets</span>: Numerous eye-tracking databases have been developed to study and model visual attention behavior. One such large-scale saliency dataset is MIT1003 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx7" title="">Judd et al., 2009</a>]</cite>, consisting of 1003 images sourced from Flickr and LabelMe. Widely used benchmark databases include MIT300 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx8" title="">Judd et al., 2012</a>]</cite> and CAT2000 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx2" title="">Borji and Itti, 2015</a>]</cite>, containing 300 and 2000 test images, respectively. SALICON <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx6" title="">Jiang et al., 2015</a>]</cite>, the largest crowd-sourced saliency dataset, comprises 10,000 training images, 5,000 validation images, and 5,000 test images, and is commonly used for pretraining saliency prediction models.
In addition to the existing image saliency datasets, only one image-text saliency dataset has been developed. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx10" title="">Lanfredi et al., 2023</a>]</cite> dataset utilizes eye-tracking (ET) data from five radiologists, offering valuable insights into visual attention in medical imaging. The study found that interpretability maps generated from multiple chest X-ray (CXR) classification models could be as similar to the radiologists’ ET maps as those generated by other radiologists.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>VISTA: Visual-Textual Attention Saliency Dataset</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We manually curated a dataset for human image-text alignment with the help of a group of human annotators. The data collection process involved presenting volunteers with images and asking them to describe the scenes depicted in each image. Throughout this process, we recorded the volunteers’ eye movements to capture their visual attention patterns, as well as their verbal descriptions via voice recordings. These voice recordings were later transcribed to text, and the original audio files were deleted to ensure de-identification and protect the privacy of the participants. This careful approach to data collection not only preserved the integrity of the visual attention data but also ensured compliance with ethical standards regarding participant confidentiality. The resulting dataset provides a rich resource for understanding the relationship between visual attention and linguistic descriptions, offering valuable insights for advancing interpretability in vision-language models.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">After gathering the eye movement data and corresponding textual descriptions, we synchronized both datasets using time stamps. During our experiment, we used the EyeLink 1000 Plus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx18" title="">Ltd,</a>]</cite> eye tracker to accurately record participants’ eye movements as they viewed and described images. We adopted the eye-tracking setup from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx18" title="">Ltd,</a>]</cite> recommendations. In this setup, each image was viewed by a single participant during the experiment. After completing the recordings, the initial results contained some noise, as anticipated. To address this, we carefully selected the image-text-saliency triplets that exhibited the closest alignment. This process resulted in a final dataset of 508 well-aligned image-text saliency maps. Although this approach reduced the total amount of data, it ensured high-quality results, resulting in a robust dataset.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Metrics and baselines</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Eye-tracking (ET) maps were created by applying Kernel Density Estimation (KDE), with the density estimate at each point weighted according to the duration of each eye fixation. This method ensures that regions, where participants focused their gaze for longer periods, are given greater emphasis in the resulting map. By assigning more weight to longer fixations, the KDE captures not just the frequency of eye movements, but also the intensity of attention directed towards specific areas within the visual field. This approach provides a more nuanced representation of visual attention patterns, highlighting areas of cognitive interest or importance in a way that simple fixation counting methods might overlook. Figure  <a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ VISTA: A Visual and Textual Attention Dataset for Interpreting Multimodal Models"><span class="ltx_text ltx_ref_tag">1</span></a> shows some of the examples. The resulting ET maps offer insights into how participants distribute their visual focus over time, enabling a deeper understanding of attentional behavior.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Based on the existing literature on automatic generation of human saliency maps, we selected two metrics for comparing saliency maps: normalized cross-correlation (NCC), also known as Pearson’s correlation coefficient, and the Borji formulation of the area under the curve (AUC). The NCC was computed directly between the eye-tracking (ET) map and the generated saliency map. This metric quantifies the similarity between the two maps by measuring their correlation over the entire image. Specifically, NCC calculates how closely the patterns in both signals (saliency maps) match, providing an overall assessment of their alignment.
The NCC uses the following,</p>
</div>
<div class="ltx_para" id="S4.p3">
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S6.EGx1">
<tbody id="S4.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\text{NCC}(x_{1},x_{2})" class="ltx_Math" display="inline" id="S4.Ex1.m1.2"><semantics id="S4.Ex1.m1.2a"><mrow id="S4.Ex1.m1.2.2" xref="S4.Ex1.m1.2.2.cmml"><mtext id="S4.Ex1.m1.2.2.4" xref="S4.Ex1.m1.2.2.4a.cmml">NCC</mtext><mo id="S4.Ex1.m1.2.2.3" xref="S4.Ex1.m1.2.2.3.cmml">⁢</mo><mrow id="S4.Ex1.m1.2.2.2.2" xref="S4.Ex1.m1.2.2.2.3.cmml"><mo id="S4.Ex1.m1.2.2.2.2.3" stretchy="false" xref="S4.Ex1.m1.2.2.2.3.cmml">(</mo><msub id="S4.Ex1.m1.1.1.1.1.1" xref="S4.Ex1.m1.1.1.1.1.1.cmml"><mi id="S4.Ex1.m1.1.1.1.1.1.2" xref="S4.Ex1.m1.1.1.1.1.1.2.cmml">x</mi><mn id="S4.Ex1.m1.1.1.1.1.1.3" xref="S4.Ex1.m1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S4.Ex1.m1.2.2.2.2.4" xref="S4.Ex1.m1.2.2.2.3.cmml">,</mo><msub id="S4.Ex1.m1.2.2.2.2.2" xref="S4.Ex1.m1.2.2.2.2.2.cmml"><mi id="S4.Ex1.m1.2.2.2.2.2.2" xref="S4.Ex1.m1.2.2.2.2.2.2.cmml">x</mi><mn id="S4.Ex1.m1.2.2.2.2.2.3" xref="S4.Ex1.m1.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S4.Ex1.m1.2.2.2.2.5" stretchy="false" xref="S4.Ex1.m1.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m1.2b"><apply id="S4.Ex1.m1.2.2.cmml" xref="S4.Ex1.m1.2.2"><times id="S4.Ex1.m1.2.2.3.cmml" xref="S4.Ex1.m1.2.2.3"></times><ci id="S4.Ex1.m1.2.2.4a.cmml" xref="S4.Ex1.m1.2.2.4"><mtext id="S4.Ex1.m1.2.2.4.cmml" xref="S4.Ex1.m1.2.2.4">NCC</mtext></ci><interval closure="open" id="S4.Ex1.m1.2.2.2.3.cmml" xref="S4.Ex1.m1.2.2.2.2"><apply id="S4.Ex1.m1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1">subscript</csymbol><ci id="S4.Ex1.m1.1.1.1.1.1.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1.2">𝑥</ci><cn id="S4.Ex1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S4.Ex1.m1.1.1.1.1.1.3">1</cn></apply><apply id="S4.Ex1.m1.2.2.2.2.2.cmml" xref="S4.Ex1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.Ex1.m1.2.2.2.2.2.1.cmml" xref="S4.Ex1.m1.2.2.2.2.2">subscript</csymbol><ci id="S4.Ex1.m1.2.2.2.2.2.2.cmml" xref="S4.Ex1.m1.2.2.2.2.2.2">𝑥</ci><cn id="S4.Ex1.m1.2.2.2.2.2.3.cmml" type="integer" xref="S4.Ex1.m1.2.2.2.2.2.3">2</cn></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m1.2c">\displaystyle\text{NCC}(x_{1},x_{2})</annotation><annotation encoding="application/x-llamapun" id="S4.Ex1.m1.2d">NCC ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{P-1}\sum\left(\frac{(x_{1}(p)-\mu_{x_{1}})}{\sigma_{x_{%
1}}}\right." class="ltx_math_unparsed" display="inline" id="S4.Ex1.m2.2"><semantics id="S4.Ex1.m2.2a"><mrow id="S4.Ex1.m2.2b"><mo id="S4.Ex1.m2.2.3">=</mo><mstyle displaystyle="true" id="S4.Ex1.m2.2.4"><mfrac id="S4.Ex1.m2.2.4a"><mn id="S4.Ex1.m2.2.4.2">1</mn><mrow id="S4.Ex1.m2.2.4.3"><mi id="S4.Ex1.m2.2.4.3.2">P</mi><mo id="S4.Ex1.m2.2.4.3.1">−</mo><mn id="S4.Ex1.m2.2.4.3.3">1</mn></mrow></mfrac></mstyle><mstyle displaystyle="true" id="S4.Ex1.m2.2.5"><mo id="S4.Ex1.m2.2.5a" movablelimits="false">∑</mo></mstyle><mrow id="S4.Ex1.m2.2.6"><mo id="S4.Ex1.m2.2.6.1">(</mo><mstyle displaystyle="true" id="S4.Ex1.m2.2.2"><mfrac id="S4.Ex1.m2.2.2a"><mrow id="S4.Ex1.m2.2.2.2.2"><mo id="S4.Ex1.m2.2.2.2.2.2" stretchy="false">(</mo><mrow id="S4.Ex1.m2.2.2.2.2.1"><mrow id="S4.Ex1.m2.2.2.2.2.1.2"><msub id="S4.Ex1.m2.2.2.2.2.1.2.2"><mi id="S4.Ex1.m2.2.2.2.2.1.2.2.2">x</mi><mn id="S4.Ex1.m2.2.2.2.2.1.2.2.3">1</mn></msub><mo id="S4.Ex1.m2.2.2.2.2.1.2.1">⁢</mo><mrow id="S4.Ex1.m2.2.2.2.2.1.2.3.2"><mo id="S4.Ex1.m2.2.2.2.2.1.2.3.2.1" stretchy="false">(</mo><mi id="S4.Ex1.m2.1.1.1.1">p</mi><mo id="S4.Ex1.m2.2.2.2.2.1.2.3.2.2" stretchy="false">)</mo></mrow></mrow><mo id="S4.Ex1.m2.2.2.2.2.1.1">−</mo><msub id="S4.Ex1.m2.2.2.2.2.1.3"><mi id="S4.Ex1.m2.2.2.2.2.1.3.2">μ</mi><msub id="S4.Ex1.m2.2.2.2.2.1.3.3"><mi id="S4.Ex1.m2.2.2.2.2.1.3.3.2">x</mi><mn id="S4.Ex1.m2.2.2.2.2.1.3.3.3">1</mn></msub></msub></mrow><mo id="S4.Ex1.m2.2.2.2.2.3" stretchy="false">)</mo></mrow><msub id="S4.Ex1.m2.2.2.4"><mi id="S4.Ex1.m2.2.2.4.2">σ</mi><msub id="S4.Ex1.m2.2.2.4.3"><mi id="S4.Ex1.m2.2.2.4.3.2">x</mi><mn id="S4.Ex1.m2.2.2.4.3.3">1</mn></msub></msub></mfrac></mstyle></mrow></mrow><annotation encoding="application/x-tex" id="S4.Ex1.m2.2c">\displaystyle=\frac{1}{P-1}\sum\left(\frac{(x_{1}(p)-\mu_{x_{1}})}{\sigma_{x_{%
1}}}\right.</annotation><annotation encoding="application/x-llamapun" id="S4.Ex1.m2.2d">= divide start_ARG 1 end_ARG start_ARG italic_P - 1 end_ARG ∑ ( divide start_ARG ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_p ) - italic_μ start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) end_ARG start_ARG italic_σ start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S4.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\quad\left.\times\frac{(x_{2}(p)-\mu_{x_{2}})}{\sigma_{x_{2}}}\right)" class="ltx_math_unparsed" display="inline" id="S4.Ex2.m1.2"><semantics id="S4.Ex2.m1.2a"><mrow id="S4.Ex2.m1.2b"><mo id="S4.Ex2.m1.2.3" rspace="0.222em">×</mo><mstyle displaystyle="true" id="S4.Ex2.m1.2.2"><mfrac id="S4.Ex2.m1.2.2a"><mrow id="S4.Ex2.m1.2.2.2.2"><mo id="S4.Ex2.m1.2.2.2.2.2" stretchy="false">(</mo><mrow id="S4.Ex2.m1.2.2.2.2.1"><mrow id="S4.Ex2.m1.2.2.2.2.1.2"><msub id="S4.Ex2.m1.2.2.2.2.1.2.2"><mi id="S4.Ex2.m1.2.2.2.2.1.2.2.2">x</mi><mn id="S4.Ex2.m1.2.2.2.2.1.2.2.3">2</mn></msub><mo id="S4.Ex2.m1.2.2.2.2.1.2.1">⁢</mo><mrow id="S4.Ex2.m1.2.2.2.2.1.2.3.2"><mo id="S4.Ex2.m1.2.2.2.2.1.2.3.2.1" stretchy="false">(</mo><mi id="S4.Ex2.m1.1.1.1.1">p</mi><mo id="S4.Ex2.m1.2.2.2.2.1.2.3.2.2" stretchy="false">)</mo></mrow></mrow><mo id="S4.Ex2.m1.2.2.2.2.1.1">−</mo><msub id="S4.Ex2.m1.2.2.2.2.1.3"><mi id="S4.Ex2.m1.2.2.2.2.1.3.2">μ</mi><msub id="S4.Ex2.m1.2.2.2.2.1.3.3"><mi id="S4.Ex2.m1.2.2.2.2.1.3.3.2">x</mi><mn id="S4.Ex2.m1.2.2.2.2.1.3.3.3">2</mn></msub></msub></mrow><mo id="S4.Ex2.m1.2.2.2.2.3" stretchy="false">)</mo></mrow><msub id="S4.Ex2.m1.2.2.4"><mi id="S4.Ex2.m1.2.2.4.2">σ</mi><msub id="S4.Ex2.m1.2.2.4.3"><mi id="S4.Ex2.m1.2.2.4.3.2">x</mi><mn id="S4.Ex2.m1.2.2.4.3.3">2</mn></msub></msub></mfrac></mstyle><mo id="S4.Ex2.m1.2.4">)</mo></mrow><annotation encoding="application/x-tex" id="S4.Ex2.m1.2c">\displaystyle\quad\left.\times\frac{(x_{2}(p)-\mu_{x_{2}})}{\sigma_{x_{2}}}\right)</annotation><annotation encoding="application/x-llamapun" id="S4.Ex2.m1.2d">× divide start_ARG ( italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_p ) - italic_μ start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) end_ARG start_ARG italic_σ start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.p4">
<p class="ltx_p" id="S4.p4.10">where <math alttext="x_{1}" class="ltx_Math" display="inline" id="S4.p4.1.m1.1"><semantics id="S4.p4.1.m1.1a"><msub id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml"><mi id="S4.p4.1.m1.1.1.2" xref="S4.p4.1.m1.1.1.2.cmml">x</mi><mn id="S4.p4.1.m1.1.1.3" xref="S4.p4.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><apply id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p4.1.m1.1.1.1.cmml" xref="S4.p4.1.m1.1.1">subscript</csymbol><ci id="S4.p4.1.m1.1.1.2.cmml" xref="S4.p4.1.m1.1.1.2">𝑥</ci><cn id="S4.p4.1.m1.1.1.3.cmml" type="integer" xref="S4.p4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">x_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.p4.1.m1.1d">italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="x_{2}" class="ltx_Math" display="inline" id="S4.p4.2.m2.1"><semantics id="S4.p4.2.m2.1a"><msub id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml"><mi id="S4.p4.2.m2.1.1.2" xref="S4.p4.2.m2.1.1.2.cmml">x</mi><mn id="S4.p4.2.m2.1.1.3" xref="S4.p4.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><apply id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p4.2.m2.1.1.1.cmml" xref="S4.p4.2.m2.1.1">subscript</csymbol><ci id="S4.p4.2.m2.1.1.2.cmml" xref="S4.p4.2.m2.1.1.2">𝑥</ci><cn id="S4.p4.2.m2.1.1.3.cmml" type="integer" xref="S4.p4.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">x_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.p4.2.m2.1d">italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> are the two images of same size, <math alttext="x_{i}(p)" class="ltx_Math" display="inline" id="S4.p4.3.m3.1"><semantics id="S4.p4.3.m3.1a"><mrow id="S4.p4.3.m3.1.2" xref="S4.p4.3.m3.1.2.cmml"><msub id="S4.p4.3.m3.1.2.2" xref="S4.p4.3.m3.1.2.2.cmml"><mi id="S4.p4.3.m3.1.2.2.2" xref="S4.p4.3.m3.1.2.2.2.cmml">x</mi><mi id="S4.p4.3.m3.1.2.2.3" xref="S4.p4.3.m3.1.2.2.3.cmml">i</mi></msub><mo id="S4.p4.3.m3.1.2.1" xref="S4.p4.3.m3.1.2.1.cmml">⁢</mo><mrow id="S4.p4.3.m3.1.2.3.2" xref="S4.p4.3.m3.1.2.cmml"><mo id="S4.p4.3.m3.1.2.3.2.1" stretchy="false" xref="S4.p4.3.m3.1.2.cmml">(</mo><mi id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml">p</mi><mo id="S4.p4.3.m3.1.2.3.2.2" stretchy="false" xref="S4.p4.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><apply id="S4.p4.3.m3.1.2.cmml" xref="S4.p4.3.m3.1.2"><times id="S4.p4.3.m3.1.2.1.cmml" xref="S4.p4.3.m3.1.2.1"></times><apply id="S4.p4.3.m3.1.2.2.cmml" xref="S4.p4.3.m3.1.2.2"><csymbol cd="ambiguous" id="S4.p4.3.m3.1.2.2.1.cmml" xref="S4.p4.3.m3.1.2.2">subscript</csymbol><ci id="S4.p4.3.m3.1.2.2.2.cmml" xref="S4.p4.3.m3.1.2.2.2">𝑥</ci><ci id="S4.p4.3.m3.1.2.2.3.cmml" xref="S4.p4.3.m3.1.2.2.3">𝑖</ci></apply><ci id="S4.p4.3.m3.1.1.cmml" xref="S4.p4.3.m3.1.1">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">x_{i}(p)</annotation><annotation encoding="application/x-llamapun" id="S4.p4.3.m3.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_p )</annotation></semantics></math> is the value of image <math alttext="x_{i}" class="ltx_Math" display="inline" id="S4.p4.4.m4.1"><semantics id="S4.p4.4.m4.1a"><msub id="S4.p4.4.m4.1.1" xref="S4.p4.4.m4.1.1.cmml"><mi id="S4.p4.4.m4.1.1.2" xref="S4.p4.4.m4.1.1.2.cmml">x</mi><mi id="S4.p4.4.m4.1.1.3" xref="S4.p4.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p4.4.m4.1b"><apply id="S4.p4.4.m4.1.1.cmml" xref="S4.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p4.4.m4.1.1.1.cmml" xref="S4.p4.4.m4.1.1">subscript</csymbol><ci id="S4.p4.4.m4.1.1.2.cmml" xref="S4.p4.4.m4.1.1.2">𝑥</ci><ci id="S4.p4.4.m4.1.1.3.cmml" xref="S4.p4.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.4.m4.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.p4.4.m4.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> in pixel coordinate <math alttext="p" class="ltx_Math" display="inline" id="S4.p4.5.m5.1"><semantics id="S4.p4.5.m5.1a"><mi id="S4.p4.5.m5.1.1" xref="S4.p4.5.m5.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.p4.5.m5.1b"><ci id="S4.p4.5.m5.1.1.cmml" xref="S4.p4.5.m5.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.5.m5.1c">p</annotation><annotation encoding="application/x-llamapun" id="S4.p4.5.m5.1d">italic_p</annotation></semantics></math>, <math alttext="P" class="ltx_Math" display="inline" id="S4.p4.6.m6.1"><semantics id="S4.p4.6.m6.1a"><mi id="S4.p4.6.m6.1.1" xref="S4.p4.6.m6.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.p4.6.m6.1b"><ci id="S4.p4.6.m6.1.1.cmml" xref="S4.p4.6.m6.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.6.m6.1c">P</annotation><annotation encoding="application/x-llamapun" id="S4.p4.6.m6.1d">italic_P</annotation></semantics></math> is the total number of pixels in one image, <math alttext="\mu_{x_{i}}" class="ltx_Math" display="inline" id="S4.p4.7.m7.1"><semantics id="S4.p4.7.m7.1a"><msub id="S4.p4.7.m7.1.1" xref="S4.p4.7.m7.1.1.cmml"><mi id="S4.p4.7.m7.1.1.2" xref="S4.p4.7.m7.1.1.2.cmml">μ</mi><msub id="S4.p4.7.m7.1.1.3" xref="S4.p4.7.m7.1.1.3.cmml"><mi id="S4.p4.7.m7.1.1.3.2" xref="S4.p4.7.m7.1.1.3.2.cmml">x</mi><mi id="S4.p4.7.m7.1.1.3.3" xref="S4.p4.7.m7.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S4.p4.7.m7.1b"><apply id="S4.p4.7.m7.1.1.cmml" xref="S4.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S4.p4.7.m7.1.1.1.cmml" xref="S4.p4.7.m7.1.1">subscript</csymbol><ci id="S4.p4.7.m7.1.1.2.cmml" xref="S4.p4.7.m7.1.1.2">𝜇</ci><apply id="S4.p4.7.m7.1.1.3.cmml" xref="S4.p4.7.m7.1.1.3"><csymbol cd="ambiguous" id="S4.p4.7.m7.1.1.3.1.cmml" xref="S4.p4.7.m7.1.1.3">subscript</csymbol><ci id="S4.p4.7.m7.1.1.3.2.cmml" xref="S4.p4.7.m7.1.1.3.2">𝑥</ci><ci id="S4.p4.7.m7.1.1.3.3.cmml" xref="S4.p4.7.m7.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.7.m7.1c">\mu_{x_{i}}</annotation><annotation encoding="application/x-llamapun" id="S4.p4.7.m7.1d">italic_μ start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> is the average value of <math alttext="x_{i}" class="ltx_Math" display="inline" id="S4.p4.8.m8.1"><semantics id="S4.p4.8.m8.1a"><msub id="S4.p4.8.m8.1.1" xref="S4.p4.8.m8.1.1.cmml"><mi id="S4.p4.8.m8.1.1.2" xref="S4.p4.8.m8.1.1.2.cmml">x</mi><mi id="S4.p4.8.m8.1.1.3" xref="S4.p4.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p4.8.m8.1b"><apply id="S4.p4.8.m8.1.1.cmml" xref="S4.p4.8.m8.1.1"><csymbol cd="ambiguous" id="S4.p4.8.m8.1.1.1.cmml" xref="S4.p4.8.m8.1.1">subscript</csymbol><ci id="S4.p4.8.m8.1.1.2.cmml" xref="S4.p4.8.m8.1.1.2">𝑥</ci><ci id="S4.p4.8.m8.1.1.3.cmml" xref="S4.p4.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.8.m8.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.p4.8.m8.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="\sigma_{x_{i}}" class="ltx_Math" display="inline" id="S4.p4.9.m9.1"><semantics id="S4.p4.9.m9.1a"><msub id="S4.p4.9.m9.1.1" xref="S4.p4.9.m9.1.1.cmml"><mi id="S4.p4.9.m9.1.1.2" xref="S4.p4.9.m9.1.1.2.cmml">σ</mi><msub id="S4.p4.9.m9.1.1.3" xref="S4.p4.9.m9.1.1.3.cmml"><mi id="S4.p4.9.m9.1.1.3.2" xref="S4.p4.9.m9.1.1.3.2.cmml">x</mi><mi id="S4.p4.9.m9.1.1.3.3" xref="S4.p4.9.m9.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S4.p4.9.m9.1b"><apply id="S4.p4.9.m9.1.1.cmml" xref="S4.p4.9.m9.1.1"><csymbol cd="ambiguous" id="S4.p4.9.m9.1.1.1.cmml" xref="S4.p4.9.m9.1.1">subscript</csymbol><ci id="S4.p4.9.m9.1.1.2.cmml" xref="S4.p4.9.m9.1.1.2">𝜎</ci><apply id="S4.p4.9.m9.1.1.3.cmml" xref="S4.p4.9.m9.1.1.3"><csymbol cd="ambiguous" id="S4.p4.9.m9.1.1.3.1.cmml" xref="S4.p4.9.m9.1.1.3">subscript</csymbol><ci id="S4.p4.9.m9.1.1.3.2.cmml" xref="S4.p4.9.m9.1.1.3.2">𝑥</ci><ci id="S4.p4.9.m9.1.1.3.3.cmml" xref="S4.p4.9.m9.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.9.m9.1c">\sigma_{x_{i}}</annotation><annotation encoding="application/x-llamapun" id="S4.p4.9.m9.1d">italic_σ start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> is the unbiased standard deviation of <math alttext="x_{i}" class="ltx_Math" display="inline" id="S4.p4.10.m10.1"><semantics id="S4.p4.10.m10.1a"><msub id="S4.p4.10.m10.1.1" xref="S4.p4.10.m10.1.1.cmml"><mi id="S4.p4.10.m10.1.1.2" xref="S4.p4.10.m10.1.1.2.cmml">x</mi><mi id="S4.p4.10.m10.1.1.3" xref="S4.p4.10.m10.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p4.10.m10.1b"><apply id="S4.p4.10.m10.1.1.cmml" xref="S4.p4.10.m10.1.1"><csymbol cd="ambiguous" id="S4.p4.10.m10.1.1.1.cmml" xref="S4.p4.10.m10.1.1">subscript</csymbol><ci id="S4.p4.10.m10.1.1.2.cmml" xref="S4.p4.10.m10.1.1.2">𝑥</ci><ci id="S4.p4.10.m10.1.1.3.cmml" xref="S4.p4.10.m10.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.10.m10.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.p4.10.m10.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">The NCC measures the degree of linear correlation between the two images, with values ranging from -1 to 1, where 1 indicates perfect positive correlation, 0 indicates no correlation, and -1 indicates perfect negative correlation. This method provides a robust assessment of the global similarity between the ET map and the generated saliency map, as it accounts for the overall distribution of pixel intensities across the image.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Scores for Image-Text Alignment and Open-Vocab Segmentation Models</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1">
<tr class="ltx_tr" id="S4.T1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T1.1.1.1"></td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_tt" id="S4.T1.1.1.2">Model</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T1.1.1.3">NCC</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S4.T1.1.1.4">AUC</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.2.1" rowspan="5"><span class="ltx_text" id="S4.T1.1.2.1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S4.T1.1.2.1.1.1" style="width:56.9pt;">
<span class="ltx_p" id="S4.T1.1.2.1.1.1.1">Image-Text</span>
<span class="ltx_p ltx_align_center" id="S4.T1.1.2.1.1.1.2">Matching Models</span>
</span></span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T1.1.2.2">CLIP</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T1.1.2.3">0.13 [0.14,0.14,0.12,0.13,0.14]</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T1.1.2.4">0.57 [0.58,0.58,0.57,0.57,0.58]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3">
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T1.1.3.1">BLIP-ITM-Base</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T1.1.3.2">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.1">0.24</span> [0.25,0.23,0.24,0.25,0.25]</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T1.1.3.3">0.63 [0.64,0.63,0.63,0.63,0.63]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4">
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T1.1.4.1">BLIP-ITM-Large</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T1.1.4.2">0.17 [0.18,0.17,0.17,0.16,0.18]</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T1.1.4.3">0.60 [0.60,0.60,0.60,0.59,0.61]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5">
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T1.1.5.1">ALBEF</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T1.1.5.2">0.19 [0.20,0.19,0.21,0.18,0.19]</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T1.1.5.3">0.57 [0.58,0.57,0.58,0.57,0.57]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6">
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T1.1.6.1">ViLT</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T1.1.6.2">-0.02 [-0.02,-0.01,0.00,-0.03,-0.015]</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T1.1.6.3">0.49 [0.49,0.50,0.50,0.48,0.49]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T1.1.7.1" rowspan="4"><span class="ltx_text" id="S4.T1.1.7.1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S4.T1.1.7.1.1.1" style="width:56.9pt;">
<span class="ltx_p" id="S4.T1.1.7.1.1.1.1">Open-Vocab</span>
<span class="ltx_p ltx_align_center" id="S4.T1.1.7.1.1.1.2">Saliency Models</span>
</span></span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T1.1.7.2">CLIP-Seg</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T1.1.7.3">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.7.3.1">0.31</span> [0.33,0.31,0.31,0.30,0.31]</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T1.1.7.4">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.7.4.1">0.67</span> [0.68,0.67,0.66,0.66,0.67]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8">
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T1.1.8.1">OV-Seg</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T1.1.8.2">0.18 [0.17,0.18,0.16,0.18,0.17]</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T1.1.8.3">0.59 [0.59,0.59,0.58,0.59,0.59]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9">
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T1.1.9.1">OpenSeg</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T1.1.9.2">0.14 [0.15,0.14,0.13,0.14,0.15]</td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T1.1.9.3">0.58 [0.58,0.57,0.57,0.58,0.58]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10">
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_bb" id="S4.T1.1.10.1">ODISE</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T1.1.10.2">0.16 [0.17,0.17,0.18,0.16,0.16]</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T1.1.10.3">0.59 [0.60,0.59,0.60,0.59,0.59]</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1">The AUC (Area Under the Curve) metric evaluates the performance of a saliency map by treating each pixel as the output of a binary classifier tasked with determining whether the pixel corresponds to a human fixation or not. In this context, the classifier is expected to assign higher values to locations where human fixations occurred (positive locations, derived from the eye-tracking map) and lower values to other locations (negative locations, represented by a uniform heatmap). The AUC provides a measure of how well the saliency map distinguishes between these two types of locations.</p>
</div>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p" id="S4.p7.1">To compute the AUC, we sample from the heatmaps, which represent the spatial probability distributions of visual attention. Specifically, the metric assesses the likelihood that the classifier assigns a higher value to a positive location (where a fixation occurred) than to a negative one (where it did not). The closer the AUC is to 1, the better the saliency map is at predicting human fixations; a score of 0.5 would indicate random guessing, while values below 0.5 suggest the map is performing worse than chance.</p>
</div>
<div class="ltx_para" id="S4.p8">
<p class="ltx_p" id="S4.p8.1">Through experimentation, we varied the number of samples to determine the impact on the accuracy of the AUC calculation. By analyzing the standard deviation of AUC scores across different sample sizes, we found that using 1000 positive samples and 1000 negative samples provided a reliable and sufficiently accurate estimation of the AUC. This approach balances computational efficiency with the need for precise evaluation, ensuring that the metric reflects the true performance of the saliency map without introducing unnecessary noise or variability.</p>
</div>
<div class="ltx_para" id="S4.p9">
<p class="ltx_p" id="S4.p9.1">People often apply center bias adjustments when recalculating metrics like NCC or AUC to account for the natural tendency of humans to focus their gaze toward the center of an image. However, after visual inspection of the dataset, we did not incorporate such bias adjustments in our analysis, as our dataset does not exhibit a center bias. This decision allows for a more direct and authentic evaluation of the saliency map’s ability to capture genuine areas of visual attention as dictated by the participants’ eye movements, rather than artificially emphasizing central regions of the image.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#S4.T1" title="Table 1 ‣ 4 Metrics and baselines ‣ VISTA: A Visual and Textual Attention Dataset for Interpreting Multimodal Models"><span class="ltx_text ltx_ref_tag">1</span></a> presents the results of evaluating multiple models on VISTA. Initially, we conducted standard evaluations on the full dataset to measure each model’s performance. To further ensure the reliability of our findings, we employed a bootstrapping technique, which involved randomly sampling the dataset with replacement over five iterations. The corresponding results are indicated in brackets. Bootstrapping provides a means to estimate the stability and consistency of our metrics by creating and evaluating multiple subsamples, thereby offering a more robust assessment of model performance.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We first conducted experiments on image-text alignment models by visualizing the attention maps produced by various Vision and Language Models (VLMs). The goal was to investigate the extent to which these models accurately align the attention weights with the human eye-tracking (ET) data in our dataset. The VLMs used for this task included CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx23" title="">Radford et al., 2021</a>]</cite>, ViLT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx9" title="">Kim et al., 2021</a>]</cite>, BLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx14" title="">Li et al., 2022b</a>]</cite>, and ALBEF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx12" title="">Li et al., 2021</a>]</cite> all of which were pre-trained on large-scale datasets and fine-tuned for the task of image-text alignment.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.p3.1.1">CLIP</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx23" title="">Radford et al., 2021</a>]</cite> performed moderately on both metrics, with an NCC score of 0.13 and an AUC of 0.57. While these results indicate some alignment with human attention patterns, they suggest that CLIP struggles to capture finer details of human visual focus, as evidenced by the relatively low NCC score.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1"><span class="ltx_text ltx_font_bold" id="S5.p4.1.1">BLIP-Base</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx14" title="">Li et al., 2022b</a>]</cite> trained for image-text matching was one of the top-performing models in this evaluation, achieving an NCC of 0.24 and an AUC of 0.63. These results suggest that BLIP-ITM-Base aligns more closely with human visual attention compared to other models, indicating that it may better capture the relationships between visual regions and corresponding text. Despite its larger architecture, BLIP-Large trained for image-text matching underperformed compared to the base model, with an NCC of 0.17 and an AUC of 0.60. This drop in performance might indicate that the larger model overfits to certain aspects of the training data, reducing its ability to generalize well to human-like attention patterns.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1"><span class="ltx_text ltx_font_bold" id="S5.p5.1.1">ALBEF</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx12" title="">Li et al., 2021</a>]</cite> showed reasonable performance with an NCC of 0.19 and an AUC of 0.57. Its performance was comparable to CLIP, indicating moderate alignment with human visual attention but with room for improvement in capturing nuanced attention cues.</p>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1"><span class="ltx_text ltx_font_bold" id="S5.p6.1.1">ViLT</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx9" title="">Kim et al., 2021</a>]</cite> exhibited the weakest performance in our experiments, with a negative NCC score (-0.02) and an AUC of 0.49, suggesting that the model failed to align with human attention patterns. The negative NCC indicates that the model’s attention maps were not only uncorrelated but possibly misaligned with human attention, highlighting its limited interpretability in this context.</p>
</div>
<div class="ltx_para" id="S5.p7">
<p class="ltx_p" id="S5.p7.1">Next, we evaluated text guided image segmentation model. The task involved providing a textual description of an image and analyzing how well the model’s segmentation map captured the relevant areas described in the text. For example, when the text mentioned "a red car," the segmentation map was expected to highlight the red car in the image. We compared the generated segmentation maps to the human ET maps to assess the degree of alignment.</p>
</div>
<div class="ltx_para" id="S5.p8">
<p class="ltx_p" id="S5.p8.1">Here, the results were mixed. CLIP-Seg <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx19" title="">Lüddecke and Ecker, 2022</a>]</cite> showed the strongest performance among the tested methods, with an NCC of 0.31 and an AUC of 0.67. These scores suggest that CLIP-Seg aligns relatively well with human attention and is effective at detecting salient regions in an open-vocabulary context. It outperforms other methods, especially in terms of NCC, indicating its potential for tasks that require accurate saliency prediction. OV-Seg <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx15" title="">Liang et al., 2023</a>]</cite> performed moderately, achieving an NCC of 0.18 and an AUC of 0.59. Although it does not match the performance of CLIP-Seg, it still demonstrates reasonable alignment with human attention. This suggests that OV-Seg is somewhat effective at identifying salient regions but may lack the finer precision required for more accurate saliency detection. OpenSeg <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx5" title="">Ghiasi et al., 2022</a>]</cite> delivered the lowest performance with an NCC of 0.14 and an AUC of 0.58. These results indicate that the model struggles to align well with human visual attention, likely due to less effective saliency detection mechanisms. ODISE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04609v1#bib.bibx31" title="">Xu et al., 2023</a>]</cite> achieved an NCC of 0.16 and an AUC of 0.59, performing similarly to OV-Seg. While its AUC score shows some capacity to capture salient regions, the NCC indicates a relatively weak correlation with human attention, suggesting room for improvement in its saliency detection capabilities.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this work, we introduced <span class="ltx_text ltx_font_italic" id="S6.p1.1.1">VISTA</span>, a human-annotated visual and textual attention dataset, to explore and enhance the interpretability of Vision-Language Models (VLMs). By aligning eye-tracking data with textual descriptions, our dataset provides a unique perspective on how humans associate visual regions with corresponding text segments. Through the evaluation of multiple VLMs using well-established metrics like NCC and AUC, we demonstrated varying degrees of alignment between human attention and model-generated saliency maps, with models such as BLIP-ITM-Base and CLIP-Seg showing promising results. However, our results also highlight the challenges that VLMs face in capturing nuanced human visual attention, particularly in complex tasks like image-text alignment and segmentation.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">The findings underscore the importance of human-centric datasets like VISTA in advancing the interpretability and transparency of VLMs. By providing insights into the internal mechanisms of these models, this work paves the way for future research aimed at improving the reliability and trustworthiness of multimodal systems. Furthermore, our dataset and methodologies serve as valuable tools for developing more human-aligned interpretability techniques, ultimately contributing to safer and more explainable AI systems in vision and language applications.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bibx1">
<span class="ltx_tag ltx_tag_bibitem">[Alayrac et al., 2022] </span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Flamingo: a visual language model for few-shot learning.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx2">
<span class="ltx_tag ltx_tag_bibitem">[Borji and Itti, 2015] </span>
<span class="ltx_bibblock">
Ali Borji and Laurent Itti.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">Cat2000: A large scale fixation dataset for boosting saliency research.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx3">
<span class="ltx_tag ltx_tag_bibitem">[Bousselham et al., 2024] </span>
<span class="ltx_bibblock">
Walid Bousselham, Angie Boggust, Sofian Chaybouti, Hendrik Strobelt, and Hilde Kuehne.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">Legrad: An explainability method for vision transformers via feature formation sensitivity.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx4">
<span class="ltx_tag ltx_tag_bibitem">[Bugliarello et al., 2021] </span>
<span class="ltx_bibblock">
Emanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki, and Desmond Elliott.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Multimodal pretraining unmasked: A meta-analysis and a unified framework of vision-and-language berts.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx5">
<span class="ltx_tag ltx_tag_bibitem">[Ghiasi et al., 2022] </span>
<span class="ltx_bibblock">
Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Scaling open-vocabulary image segmentation with image-level labels.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx6">
<span class="ltx_tag ltx_tag_bibitem">[Jiang et al., 2015] </span>
<span class="ltx_bibblock">
Ming Jiang, Shengsheng Huang, Juanyong Duan, and Qi Zhao.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">Salicon: Saliency in context.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx6.1.1">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 1072–1080.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx7">
<span class="ltx_tag ltx_tag_bibitem">[Judd et al., 2009] </span>
<span class="ltx_bibblock">
Tilke Judd, Krista Ehinger, Frédo Durand, and Antonio Torralba.

</span>
<span class="ltx_bibblock">2009.

</span>
<span class="ltx_bibblock">Learning to predict where humans look.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx7.1.1">2009 IEEE 12th International Conference on Computer Vision</span>, pages 2106–2113.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx8">
<span class="ltx_tag ltx_tag_bibitem">[Judd et al., 2012] </span>
<span class="ltx_bibblock">
Tilke Judd, Frédo Durand, and Antonio Torralba.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">A benchmark of computational models of saliency to predict human fixations.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx8.1.1">MIT Technical Report</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx9">
<span class="ltx_tag ltx_tag_bibitem">[Kim et al., 2021] </span>
<span class="ltx_bibblock">
Wonjae Kim, Bokyung Son, and Ildoo Kim.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Vilt: Vision-and-language transformer without convolution or region supervision.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx10">
<span class="ltx_tag ltx_tag_bibitem">[Lanfredi et al., 2023] </span>
<span class="ltx_bibblock">
Ricardo Bigolin Lanfredi, Ambuj Arora, Trafton Drew, Joyce D. Schroeder, and Tolga Tasdizen.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">Comparing radiologists’ gaze and saliency maps generated by interpretability methods for chest x-rays.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx11">
<span class="ltx_tag ltx_tag_bibitem">[Lei et al., 2023] </span>
<span class="ltx_bibblock">
Yiming Lei, Zilong Li, Yangyang Li, Junping Zhang, and Hongming Shan.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">Lico: Explainable models with language-image consistency.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx12">
<span class="ltx_tag ltx_tag_bibitem">[Li et al., 2021] </span>
<span class="ltx_bibblock">
Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Align before fuse: Vision and language representation learning with momentum distillation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx13">
<span class="ltx_tag ltx_tag_bibitem">[Li et al., 2022a] </span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.

</span>
<span class="ltx_bibblock">2022a.

</span>
<span class="ltx_bibblock">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx14">
<span class="ltx_tag ltx_tag_bibitem">[Li et al., 2022b] </span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.

</span>
<span class="ltx_bibblock">2022b.

</span>
<span class="ltx_bibblock">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx15">
<span class="ltx_tag ltx_tag_bibitem">[Liang et al., 2023] </span>
<span class="ltx_bibblock">
Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">Open-vocabulary semantic segmentation with mask-adapted clip.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx16">
<span class="ltx_tag ltx_tag_bibitem">[Lin et al., 2015] </span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx17">
<span class="ltx_tag ltx_tag_bibitem">[Long et al., 2022] </span>
<span class="ltx_bibblock">
Siqu Long, Feiqi Cao, Soyeon Caren Han, and Haiqin Yang.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Vision-and-language pretrained models: A survey.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx18">
<span class="ltx_tag ltx_tag_bibitem">[Ltd, ] </span>
<span class="ltx_bibblock">
SR Research Ltd.

</span>
<span class="ltx_bibblock">Eyelink 1000 plus.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx19">
<span class="ltx_tag ltx_tag_bibitem">[Lüddecke and Ecker, 2022] </span>
<span class="ltx_bibblock">
Timo Lüddecke and Alexander S. Ecker.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Image segmentation using text and image prompts.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx20">
<span class="ltx_tag ltx_tag_bibitem">[Mogadala et al., 2021] </span>
<span class="ltx_bibblock">
Aditya Mogadala, Marimuthu Kalimuthu, and Dietrich Klakow.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Trends in integration of vision and language research: A survey of tasks, datasets, and methods.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx20.1.1">Journal of Artificial Intelligence Research</span>, 71:1183–1317, August.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx21">
<span class="ltx_tag ltx_tag_bibitem">[OpenAI et al., 2024] </span>
<span class="ltx_bibblock">
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo
Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan
Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez,
Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia
Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx22">
<span class="ltx_tag ltx_tag_bibitem">[Plummer et al., 2016] </span>
<span class="ltx_bibblock">
Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx23">
<span class="ltx_tag ltx_tag_bibitem">[Radford et al., 2021] </span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx24">
<span class="ltx_tag ltx_tag_bibitem">[Ramesh et al., 2021] </span>
<span class="ltx_bibblock">
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Zero-shot text-to-image generation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx25">
<span class="ltx_tag ltx_tag_bibitem">[Rombach et al., 2022] </span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx26">
<span class="ltx_tag ltx_tag_bibitem">[Schuhmann et al., 2021] </span>
<span class="ltx_bibblock">
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx27">
<span class="ltx_tag ltx_tag_bibitem">[Schuhmann et al., 2022] </span>
<span class="ltx_bibblock">
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Laion-5b: An open large-scale dataset for training next generation image-text models.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx28">
<span class="ltx_tag ltx_tag_bibitem">[Selvaraju et al., 2019] </span>
<span class="ltx_bibblock">
Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Grad-cam: Visual explanations from deep networks via gradient-based localization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx28.1.1">International Journal of Computer Vision</span>, 128(2):336–359, October.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx29">
<span class="ltx_tag ltx_tag_bibitem">[Stan et al., 2024] </span>
<span class="ltx_bibblock">
Gabriela Ben Melech Stan, Estelle Aflalo, Raanan Yehezkel Rohekar, Anahita Bhiwandiwalla, Shao-Yen Tseng, Matthew Lyle Olson, Yaniv Gurwicz, Chenfei Wu, Nan Duan, and Vasudev Lal.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">Lvlm-interpret: An interpretability tool for large vision-language models.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx30">
<span class="ltx_tag ltx_tag_bibitem">[Team et al., 2024] </span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Jack Krawczyk, Cosmo Du, Ed Chi, Heng-Tze Cheng, Eric Ni, Purvi Shah, Patrick Kane, Betty Chan, Manaal Faruqui, Aliaksei Severyn, Hanzhao Lin, YaGuang Li, Yong Cheng, Abe Ittycheriah, Mahdis Mahdieh, Mia Chen, Pei Sun, Dustin Tran, Sumit Bagri, Balaji Lakshminarayanan, Jeremiah Liu, Andras Orban, Fabian Güra, Hao Zhou, Xinying Song, Aurelien Boffy, Harish Ganapathy, Steven Zheng, HyunJeong Choe, Ágoston Weisz, Tao Zhu, Yifeng Lu, Siddharth Gopal, Jarrod Kahn, Maciej Kula, Jeff
Pitman, Rushin Shah, Emanuel Taropa, Majd Al Merey, Martin Baeuml, Zhifeng Chen, Laurent El Shafey, Yujing Zhang, Olcan Sercinoglu, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam Bloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Gaurav Singh Tomar, Evan Senter, Martin Chadwick, Ilya Kornakov, Nithya Attaluri, Iñaki Iturrate, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang,
Jordan Grimstad, Ale Jakse Hartman, Xavier Garcia, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego de Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adrià Puigdomènech Badia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Ravi Addanki, Antoine Miech, Annie Louis, Denis Teplyashin, Geoff Brown, Elliot Catt, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, Sébastien M. R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry
Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozińska, Vitaliy Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Giménez, Legg Yeung, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran
Vodrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van den Driessche, Tao Wang, Fan Yang, Shuo yiin Chang, Paul Komarek, Ross McIlroy, Mario Lučić, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, Paul Kishan Rubenstein, Shivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell,
Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek, Raphaël Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen, Charline Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu, Jaime Alonso Lorenzo, Lars Lowe Sjösund, Sébastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, Léonard Hussenot, Livio Baldini Soares, Kate Baumli, Michael B. Chang, Adrià Recasens, Ben Caine, Alexander Pritzel, Filip Pavetic,
Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, Víctor Campos Campos, Alex Tomala, Yunhao Tang, Dalia El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce Zheng, Phoebe Thacker, Çağlar Ünlü, Zhishuai Zhang, Mohammad Saleh, James Svensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi Hashemi, Richard Ives, Yana Hasson, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivakumar, Joost van
Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Rakićević, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, Nicola De Cao, Charlie Chen, Sidharth Mudgal, Romina Stella, Kevin Brooks, Gautam Vasudevan, Chenxi Liu, Mainak Chain, Nivedita Melinkeri, Aaron Cohen, Venus Wang, Kristie Seymore, Sergey Zubkov, Rahul Goel, Summer Yue, Sai Krishnakumaran, Brian Albert, Nate Hurley, Motoki Sano, Anhad Mohananey, Jonah Joughin, Egor Filonov, Tomasz Kępa, Yomna Eldawy, Jiawern Lim, Rahul Rishi, Shirin Badiezadegan, Taylor Bos, Jerry Chang, Sanil Jain, Sri Gayatri Sundara Padmanabhan, Subha Puttagunta, Kalpesh Krishna, Leslie Baker, Norbert Kalb, Vamsi Bedapudi, Adam Kurzrok, Shuntong Lei, Anthony Yu, Oren Litvin, Xiang Zhou, Zhichun Wu, Sam Sobell, Andrea Siciliano, Alan Papir, Robby Neale, Jonas Bragagnolo, Tej Toor, Tina Chen, Valentin Anklin, Feiran Wang, Richie Feng,
Milad Gholami, Kevin Ling, Lijuan Liu, Jules Walter, Hamid Moghaddam, Arun Kishore, Jakub Adamek, Tyler Mercado, Jonathan Mallinson, Siddhinita Wandekar, Stephen Cagle, Eran Ofek, Guillermo Garrido, Clemens Lombriser, Maksim Mukha, Botu Sun, Hafeezul Rahman Mohammad, Josip Matak, Yadi Qian, Vikas Peswani, Pawel Janus, Quan Yuan, Leif Schelin, Oana David, Ankur Garg, Yifan He, Oleksii Duzhyi, Anton Älgmyr, Timothée Lottaz, Qi Li, Vikas Yadav, Luyao Xu, Alex Chinien, Rakesh Shivanna, Aleksandr Chuklin, Josie Li, Carrie Spadine, Travis Wolfe, Kareem Mohamed, Subhabrata Das, Zihang Dai, Kyle He, Daniel von Dincklage, Shyam Upadhyay, Akanksha Maurya, Luyan Chi, Sebastian Krause, Khalid Salama, Pam G Rabinovitch, Pavan Kumar Reddy M, Aarush Selvan, Mikhail Dektiarev, Golnaz Ghiasi, Erdem Guven, Himanshu Gupta, Boyi Liu, Deepak Sharma, Idan Heimlich Shtacher, Shachi Paul, Oscar Akerlund, François-Xavier Aubet, Terry Huang, Chen Zhu, Eric Zhu, Elico Teixeira, Matthew Fritze, Francesco Bertolini, Liana-Eleonora
Marinescu, Martin Bölle, Dominik Paulus, Khyatti Gupta, Tejasi Latkar, Max Chang, Jason Sanders, Roopa Wilson, Xuewei Wu, Yi-Xuan Tan, Lam Nguyen Thiet, Tulsee Doshi, Sid Lall, Swaroop Mishra, Wanming Chen, Thang Luong, Seth Benjamin, Jasmine Lee, Ewa Andrejczuk, Dominik Rabiej, Vipul Ranjan, Krzysztof Styrc, Pengcheng Yin, Jon Simon, Malcolm Rose Harriott, Mudit Bansal, Alexei Robsky, Geoff Bacon, David Greene, Daniil Mirylenka, Chen Zhou, Obaid Sarvana, Abhimanyu Goyal, Samuel Andermatt, Patrick Siegler, Ben Horn, Assaf Israel, Francesco Pongetti, Chih-Wei "Louis" Chen, Marco Selvatici, Pedro Silva, Kathie Wang, Jackson Tolins, Kelvin Guu, Roey Yogev, Xiaochen Cai, Alessandro Agostini, Maulik Shah, Hung Nguyen, Noah Ó Donnaile, Sébastien Pereira, Linda Friso, Adam Stambler, Adam Kurzrok, Chenkai Kuang, Yan Romanikhin, Mark Geller, ZJ Yan, Kane Jang, Cheng-Chun Lee, Wojciech Fica, Eric Malmi, Qijun Tan, Dan Banica, Daniel Balle, Ryan Pham, Yanping Huang, Diana Avram, Hongzhi Shi, Jasjot Singh, Chris
Hidey, Niharika Ahuja, Pranab Saxena, Dan Dooley, Srividya Pranavi Potharaju, Eileen O’Neill, Anand Gokulchandran, Ryan Foley, Kai Zhao, Mike Dusenberry, Yuan Liu, Pulkit Mehta, Ragha Kotikalapudi, Chalence Safranek-Shrader, Andrew Goodman, Joshua Kessinger, Eran Globen, Prateek Kolhar, Chris Gorgolewski, Ali Ibrahim, Yang Song, Ali Eichenbaum, Thomas Brovelli, Sahitya Potluri, Preethi Lahoti, Cip Baetu, Ali Ghorbani, Charles Chen, Andy Crawford, Shalini Pal, Mukund Sridhar, Petru Gurita, Asier Mujika, Igor Petrovski, Pierre-Louis Cedoz, Chenmei Li, Shiyuan Chen, Niccolò Dal Santo, Siddharth Goyal, Jitesh Punjabi, Karthik Kappaganthu, Chester Kwak, Pallavi LV, Sarmishta Velury, Himadri Choudhury, Jamie Hall, Premal Shah, Ricardo Figueira, Matt Thomas, Minjie Lu, Ting Zhou, Chintu Kumar, Thomas Jurdi, Sharat Chikkerur, Yenai Ma, Adams Yu, Soo Kwak, Victor Ähdel, Sujeevan Rajayogam, Travis Choma, Fei Liu, Aditya Barua, Colin Ji, Ji Ho Park, Vincent Hellendoorn, Alex Bailey, Taylan Bilal, Huanjie Zhou,
Mehrdad Khatir, Charles Sutton, Wojciech Rzadkowski, Fiona Macintosh, Konstantin Shagin, Paul Medina, Chen Liang, Jinjing Zhou, Pararth Shah, Yingying Bi, Attila Dankovics, Shipra Banga, Sabine Lehmann, Marissa Bredesen, Zifan Lin, John Eric Hoffmann, Jonathan Lai, Raynald Chung, Kai Yang, Nihal Balani, Arthur Bražinskas, Andrei Sozanschi, Matthew Hayes, Héctor Fernández Alcalde, Peter Makarov, Will Chen, Antonio Stella, Liselotte Snijders, Michael Mandl, Ante Kärrman, Paweł Nowak, Xinyi Wu, Alex Dyck, Krishnan Vaidyanathan, Raghavender R, Jessica Mallet, Mitch Rudominer, Eric Johnston, Sushil Mittal, Akhil Udathu, Janara Christensen, Vishal Verma, Zach Irving, Andreas Santucci, Gamaleldin Elsayed, Elnaz Davoodi, Marin Georgiev, Ian Tenney, Nan Hua, Geoffrey Cideron, Edouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Dylan Scandinaro, Heinrich Jiang, Jasper Snoek, Mukund Sundararajan, Xuezhi Wang, Zack Ontiveros, Itay Karo, Jeremy Cole, Vinu Rajashekhar, Lara Tumeh, Eyal
Ben-David, Rishub Jain, Jonathan Uesato, Romina Datta, Oskar Bunyan, Shimu Wu, John Zhang, Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Jane Park, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Geoffrey Irving, Edward Loper, Michael Fink, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Ivan Petrychenko, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Peter Grabowski, Yu Mao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Evan Palmer, Paul Suganthan, Alfonso Castaño, Irene Giannoumis, Wooyeol Kim, Mikołaj Rybiński, Ashwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, Diana Gage Wright, Yawen Wei, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo Le,
Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Ginger Perng, Elena Allica Abellan, Mingyang Zhang, Ishita Dasgupta, Nate Kushman, Ivo Penchev, Alena Repina, Xihui Wu, Tom van der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Daniel Andor, Pedro Valenzuela, Minnie Lui, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen, Paula Kurylowicz, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang, Achintya Singhal, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Orgad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney, Ken Franko, Anna Bulanova, Rémi Leblond, Shirley Chung, Harry Askham, Luis C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin,
Chris Alberti, Chu-Cheng Lin, Colin Evans, Alek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Mark Omernick, Colton Bishop, Rachel Sterneck, Rohan Jain, Jiawei Xia, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Daniel J. Mankowitz, Alex Polozov, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom Natan, Matthieu Geist, Ser tan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Kathy Wu, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Tian Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Carpenter, George Papamakarios, Rupert Kemp, Sushant
Kafle, Tanya Grunina, Rishika Sinha, Alice Talbert, Diane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Saaber Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Yeongil Ko, Laura Knight, Amélie Héliou, Ning Niu, Shane Gu, Chenxi Pang, Yeqing Li, Nir Levine, Ariel Stolovich, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Charlie Deck, Hyo Lee, Zonglin Li, Kyle Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem, Sho Arora, Christy Koh, Soheil Hassas Yeganeh, Siim Põder, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, Vinod Koverkathu, Christopher A. Choquette-Choo, Yunjie Li, TJ Lu, Abe Ittycheriah, Prakash Shroff, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Guillaume Desjardins, Marco Cornero, Brona
Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Rivière, Alanna Walton, Clément Crepy, Alicia Parrish, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan, Claudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-Plucińska, David Bridson, Dario de Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex Morris, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Lynette Webb, Sahil Dua, Dong Li, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas,
Martin Wicke, Xiao Ma, Evgenii Eltyshev, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Christof Angermueller, Xiaowei Li, Anoop Sinha, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Goldenson, Parashar Shah, MK Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki, Chrisantha Fernando, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Denny Zhou, Komal Jalan, Dinghua Li, Blake Hechtman, Parker Schuh, Milad Nasr, Kieran Milan, Vladimir Mikulik, Juliana Franco, Tim Green, Nam Nguyen, Joe Kelley, Aroma Mahendru, Andrea Hu, Joshua
Howland, Ben Vargas, Jeffrey Hui, Kshitij Bansal, Vikram Rao, Rakesh Ghiya, Emma Wang, Ke Ye, Jean Michel Sarr, Melanie Moranski Preston, Madeleine Elish, Steve Li, Aakash Kaku, Jigar Gupta, Ice Pasupat, Da-Cheng Juan, Milan Someswar, Tejvi M., Xinyun Chen, Aida Amini, Alex Fabrikant, Eric Chu, Xuanyi Dong, Amruta Muthal, Senaka Buthpitiya, Sarthak Jauhari, Nan Hua, Urvashi Khandelwal, Ayal Hitron, Jie Ren, Larissa Rinaldi, Shahar Drath, Avigail Dabush, Nan-Jiang Jiang, Harshal Godhia, Uli Sachs, Anthony Chen, Yicheng Fan, Hagai Taitelbaum, Hila Noga, Zhuyun Dai, James Wang, Chen Liang, Jenny Hamer, Chun-Sung Ferng, Chenel Elkind, Aviel Atias, Paulina Lee, Vít Listík, Mathias Carlen, Jan van de Kerkhof, Marcin Pikus, Krunoslav Zaher, Paul Müller, Sasha Zykova, Richard Stefanec, Vitaly Gatsko, Christoph Hirnschall, Ashwin Sethi, Xingyu Federico Xu, Chetan Ahuja, Beth Tsai, Anca Stefanoiu, Bo Feng, Keshav Dhandhania, Manish Katyal, Akshay Gupta, Atharva Parulekar, Divya Pitta, Jing Zhao, Vivaan Bhatia,
Yashodha Bhavnani, Omar Alhadlaq, Xiaolin Li, Peter Danenberg, Dennis Tu, Alex Pine, Vera Filippova, Abhipso Ghosh, Ben Limonchik, Bhargava Urala, Chaitanya Krishna Lanka, Derik Clive, Yi Sun, Edward Li, Hao Wu, Kevin Hongtongsak, Ianna Li, Kalind Thakkar, Kuanysh Omarov, Kushal Majmundar, Michael Alverson, Michael Kucharski, Mohak Patel, Mudit Jain, Maksim Zabelin, Paolo Pelagatti, Rohan Kohli, Saurabh Kumar, Joseph Kim, Swetha Sankar, Vineet Shah, Lakshmi Ramachandruni, Xiangkai Zeng, Ben Bariach, Laura Weidinger, Tu Vu, Alek Andreev, Antoine He, Kevin Hui, Sheleem Kashem, Amar Subramanya, Sissie Hsiao, Demis Hassabis, Koray Kavukcuoglu, Adam Sadovsky, Quoc Le, Trevor Strohman, Yonghui Wu, Slav Petrov, Jeffrey Dean, and Oriol Vinyals.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">Gemini: A family of highly capable multimodal models.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx31">
<span class="ltx_tag ltx_tag_bibitem">[Xu et al., 2023] </span>
<span class="ltx_bibblock">
Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">Open-vocabulary panoptic segmentation with text-to-image diffusion models.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx32">
<span class="ltx_tag ltx_tag_bibitem">[Yue et al., 2024] </span>
<span class="ltx_bibblock">
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx32.1.1">Proceedings of CVPR</span>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Oct  6 19:52:33 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
