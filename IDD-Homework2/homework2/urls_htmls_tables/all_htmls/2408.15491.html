<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression</title>
<!--Generated on Wed Aug 28 02:26:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.15491v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S1" title="In Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S2" title="In Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S2.SS1" title="In 2 Related Work ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Retrieval-Augmented Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S2.SS2" title="In 2 Related Work ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Long Context Large Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S2.SS3" title="In 2 Related Work ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Prompt Engineering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S2.SS4" title="In 2 Related Work ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Context Compression</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S3" title="In Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S3.SS1" title="In 3 Method ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Model Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S3.SS2" title="In 3 Method ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Training Objectives</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S3.SS3" title="In 3 Method ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Instruction-Aware Contextual Compression by Ranking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S3.SS4" title="In 3 Method ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Instruction-Aware Contextual Compression by Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S3.SS5" title="In 3 Method ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Ensemble the two methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S4" title="In Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S4.SS1" title="In 4 Experiments ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S4.SS1.SSS1" title="In 4.1 Datasets ‣ 4 Experiments ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Ranking datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S4.SS1.SSS2" title="In 4.1 Datasets ‣ 4 Experiments ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Generation Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S4.SS2" title="In 4 Experiments ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Large Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S4.SS3" title="In 4 Experiments ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Experimental Settings</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S5" title="In Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results and Discussions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S5.SS1" title="In 5 Results and Discussions ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Comparison to Original Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S5.SS2" title="In 5 Results and Discussions ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Comparison to Baseline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S5.SS3" title="In 5 Results and Discussions ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>The Impact of Generation Steps</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S5.SS4" title="In 5 Results and Discussions ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Speed Up and Memory Saving</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S5.SS5" title="In 5 Results and Discussions ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Re-ranking Performance</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S6" title="In Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">[1]<span class="ltx_ERROR undefined" id="p1.1.1">\fnm</span>Haowen <span class="ltx_ERROR undefined" id="p1.1.2">\sur</span>Hou
<span class="ltx_ERROR undefined" id="p1.1.3">\equalcont</span>These authors contributed equally to this work.</p>
</div>
<div class="ltx_para" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\equalcont</span>
<p class="ltx_p" id="p2.2">These authors contributed equally to this work.</p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1">[1]<span class="ltx_ERROR undefined" id="p3.1.1">\orgname</span>Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), <span class="ltx_ERROR undefined" id="p3.1.2">\orgaddress</span><span class="ltx_ERROR undefined" id="p3.1.3">\street</span>Yutang, <span class="ltx_ERROR undefined" id="p3.1.4">\city</span>Shenzhen, <span class="ltx_ERROR undefined" id="p3.1.5">\postcode</span>518000, <span class="ltx_ERROR undefined" id="p3.1.6">\state</span>Guangdong, <span class="ltx_ERROR undefined" id="p3.1.7">\country</span>China</p>
</div>
<h1 class="ltx_title ltx_title_document">Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:houhaowen@gml.ac.cn">houhaowen@gml.ac.cn</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id1.1.id1">\fnm</span>Fei <span class="ltx_ERROR undefined" id="id2.2.id2">\sur</span>Ma
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id3.1.id1">\fnm</span>Binwen <span class="ltx_ERROR undefined" id="id4.2.id2">\sur</span>Bai
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id5.1.id1">\fnm</span>Xinxin <span class="ltx_ERROR undefined" id="id6.2.id2">\sur</span>Zhu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id7.1.id1">\fnm</span>Fei <span class="ltx_ERROR undefined" id="id8.2.id2">\sur</span>Yu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">*
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id9.id1">Large Language Models (LLMs) have garnered widespread attention due to their remarkable performance across various tasks. However, to mitigate the issue of hallucinations, LLMs often incorporate retrieval-augmented pipeline to provide them with rich external knowledge and context. Nevertheless, challenges stem from inaccurate and coarse-grained context retrieved from the retriever. Supplying irrelevant context to the LLMs can result in poorer responses, increased inference latency, and higher costs. This paper introduces a method called Instruction-Aware Contextual Compression, which filters out less informative content, thereby accelerating and enhancing the use of LLMs. The experimental results demonstrate that Instruction-Aware Contextual Compression notably reduces memory consumption and minimizes generation latency while maintaining performance levels comparable to those achieved with the use of the full context. Specifically, we achieved a 50% reduction in context-related costs, resulting in a 5% reduction in inference memory usage and a 2.2-fold increase in inference speed, with only a minor drop of 0.047 in Rouge-1. These findings suggest that our method strikes an effective balance between efficiency and performance.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Large Language Models, Context Compression, Retrieval Augmented Generation
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="138" id="S0.F1.g1" src="extracted/5817813/teaser.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Retrieval Augmented Generation(RAG) pipeline with Instruction-Aware Contextual Compression.</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large language models (LLMs) have exhibited impressive capabilities in terms of both their robust performance and generalization across a diverse spectrum of natural language processing tasks, as well as practical real-world applications <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib31" title="">31</a>]</cite>.
To address certain issues with Large Language Models (LLMs), such as long-context <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib39" title="">39</a>]</cite> or hallucination <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib29" title="">29</a>]</cite> problems, retrieval-augmented generation (RAG) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib18" title="">18</a>]</cite> has emerged. RAG has become an important approach to enhance Large Language Models.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, when using RAG, there can still be problems with irrelevant information. On one hand, inaccurate recall may lead to the retrieval of irrelevant documents. On the other hand, even within relevant documents, there might be irrelevant content that could distract the Large Language Model (LLM) from the relevant information. Passing the full document to the LLMs can lead to poor responses, large inference latency, and high costs.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Contextual compression aims to address this issue. The concept is straightforward: rather than directly presenting retrieved documents in their original form, they can be compressed, ensuring that only relevant information is conveyed.
Some research efforts <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib19" title="">19</a>]</cite> have been committed to effectively compressing the context or prompt for large language models, with the aim of utilizing the most concise input while simultaneously preserving the robust performance of these models.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we introduce <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">Instruction-Aware Contextual Compressor</span> (IACC), a novel approach that harnesses both ranking and generation information to eliminate extraneous context, thereby mitigating the computational overhead associated with the given context. Consequently, this leads to a reduction in both inference memory usage and inference time. The Instruction-Aware Contextual Compressor is adept at preserving finely detailed content directly related to instructions while compactly representing context, resulting in an efficient and streamlined input for Large Language Models (LLMs) without compromising their performance.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The main contributions of our paper are as follows:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce Instruction-Aware Contextual Compressor, an innovative model aimed at enhancing the context efficiency of LLMs, which is able to reduce memory usage and inference latency without sacrificing LLMs performance.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We found that Instruction-aware contextual compression by generation is more effective than Instruction-aware contextual compression by ranking, even though the former utilizes training data that is only one-tenth of the latter.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We developed the WikiQA-LongForm Dataset, a long-form open-domain question answering dataset based on Wikipedia entries, which can be used for training and evaluating models’ context compression capabilities. This dataset is now publicly available at <a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/howard-hou/WikiQA-LongForm" title="">WikiQA-LongForm</a> for use in other research projects.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The remaining sections of the paper are structured as follows: In Section 2, we delve into the related work. Section 3 outlines the method and model architecture. Section 4 describes experimental setup. Results are detailed in Section 5, and we provide conclusion in Section 6.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we review existing approaches <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib36" title="">36</a>]</cite> aimed at addressing the limitations imposed by context length in Large Language Models (LLMs). These limitations have motivated the development of various techniques to extend the context window of LLMs and enhance their performance.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Retrieval-Augmented Generation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Retrieval has been integrated into language models for years to enhance various aspects such as perplexity <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib32" title="">32</a>]</cite>, factual accuracy <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib24" title="">24</a>]</cite>, downstream task accuracy <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib18" title="">18</a>]</cite>, and in-context learning capability <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib11" title="">11</a>]</cite>. Combined with a standalone retriever <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib20" title="">20</a>]</cite>, retrieval-augmented LLM is a well-established approach for addressing question answering with long documents in an open-domain context.
In previous studies, language models have been augmented with retrieval during inference <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib41" title="">41</a>]</cite>, fine-tuning <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib8" title="">8</a>]</cite>, and pretraining <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib32" title="">32</a>]</cite>.
There are also some methods that aim to integrate LLM and retriever into a single model, creating an end-to-end solution <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib28" title="">28</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Long Context Large Language Models</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Many approaches have sought to improve the handling of longer contexts in Large Language Models (LLMs) through modifications to their underlying architectures. Notably, the Longformer <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib3" title="">3</a>]</cite> employs a linear attention mechanism that scales with sequence length, allowing it to accommodate longer contexts effectively. CoLT5 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib1" title="">1</a>]</cite> introduces conditional computation techniques that enable the model to focus more on crucial tokens in both feedforward and attention layers. However, it’s worth noting that many existing works have not yet adopted such architectural modifications, mainly due to the high computational cost associated with training LLMs.
Another category of approaches addresses the context length limitation by employing context chunking strategies. The Parallel Context Windows <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib26" title="">26</a>]</cite> proposes a parallel context window method, which calculates attention within individual chunks and incorporates positional embeddings between these chunks.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Prompt Engineering</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Prompt engineering is a relatively emerging discipline focused on crafting and refining prompts to harness the power of language models (LMs) for diverse applications and research endeavors. Proficiency in prompt engineering aids in gaining a deeper insight into the capabilities and constraints of large language models (LLMs).
Researchers employ prompt engineering to enhance the performance of LLMs across an array of common and intricate tasks, including question answering and arithmetic reasoning. Developers leverage prompt engineering to devise resilient and efficient prompting strategies that interact seamlessly with LLMs and other associated tools.
Prompt engineering encompasses two key directions: text-to-text and text-to-image <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib25" title="">25</a>]</cite> interactions. This area of research has witnessed significant manual efforts, exemplified by A Prompt Pattern Catalog <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib37" title="">37</a>]</cite>, where a comprehensive collection of handcrafted prompt techniques is meticulously documented. On the other hand, there are automated approaches to prompt generation, such as the work on ”Automatic Prompt Engineer” by Zhou and ”LM-BFF” by Gao <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib7" title="">7</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Context Compression</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.2">Context compression can be considered a form of prompt engineering, although their emphases are slightly different. Several techniques aim to compress prompts effectively while maintaining context relevance. The Selective Context <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib19" title="">19</a>]</cite> approach leverages concepts from information theory, specifically self-information, to compress the context. Another approach, Learning to Compress Prompts with Gist Tokens <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib23" title="">23</a>]</cite>, trains Gist models to compress prompt words into ”gist” tokens before inputting them into the LLMs. Additionally, LeanContext <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib2" title="">2</a>]</cite> extracts a dynamic number <math alttext="k" class="ltx_Math" display="inline" id="S2.SS4.p1.1.m1.1"><semantics id="S2.SS4.p1.1.m1.1a"><mi id="S2.SS4.p1.1.m1.1.1" xref="S2.SS4.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.1.m1.1b"><ci id="S2.SS4.p1.1.m1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.1.m1.1d">italic_k</annotation></semantics></math> of key sentences from prompts and uses a reinforcement learning mechanism to determine the optimal value of <math alttext="k" class="ltx_Math" display="inline" id="S2.SS4.p1.2.m2.1"><semantics id="S2.SS4.p1.2.m2.1a"><mi id="S2.SS4.p1.2.m2.1.1" xref="S2.SS4.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.2.m2.1b"><ci id="S2.SS4.p1.2.m2.1.1.cmml" xref="S2.SS4.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.2.m2.1d">italic_k</annotation></semantics></math> for compression.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We introduce Instruction-Aware Contextual Compression <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib19" title="">19</a>]</cite>, an innovative approach for context compression that leverages both ranking and generation information.
In contrast to the instruction-agnostic context compression methods used previously, Instruction-Aware Contextual Compression is a method that relies on instructions to perform context compression. Depending on the specific instruction provided, the model produces different compression outcomes, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S3.F2" title="Figure 2 ‣ 3 Method ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_tag">2</span></a>, removing irrelevant portions of the context, ultimately achieving improved context compression results.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">To achieve Instruction-Aware Contextual Compression, we propose a two-stage pre-training methodology comprising the following stages:
(1) Ranking-Based Learning Stage.
(2) Generative Learning Stage. This section commences with an exposition of the model architecture employed in Instruction-Aware Contextual Compressor, and then introduces how we trained it in two different stages.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S3.F2.1" style="width:433.6pt;height:67.4pt;vertical-align:-30.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.7pt,0.1pt) scale(0.992371431755214,0.992371431755214) ;">
<p class="ltx_p" id="S3.F2.1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S3.F2.1.1.1" style="width:433.6pt;">
<span class="ltx_p" id="S3.F2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.F2.1.1.1.1.1">Original: </span></span>
<span class="ltx_p" id="S3.F2.1.1.1.2">The 2018 Chinese Professional Baseball League (CPBL) Red vs White All-Star Game was a special event held during the 29th CPBL season. It took place on July 7th and July 8th, 2018, at the Taipei Tianmu Baseball Stadium in Taipei, Taiwan. The main game was held on the first day, while the second day featured five skill competitions. The main game concluded with the Three-Family Mart Brothers White Team defeating the Taiwan Cooperative Bank Red Team by a one-run difference.</span>
</span></p>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S3.F2.2" style="width:433.6pt;height:45.5pt;vertical-align:-19.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.7pt,0.1pt) scale(0.992371431755214,0.992371431755214) ;">
<p class="ltx_p" id="S3.F2.2.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S3.F2.2.1.1" style="width:433.6pt;">
<span class="ltx_p" id="S3.F2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.F2.2.1.1.1.1">Compressed by instruction ”When was the 2018 CPBL Red vs. White All-Star Game held?”: </span></span>
<span class="ltx_p" id="S3.F2.2.1.1.2"><span class="ltx_text" id="S3.F2.2.1.1.2.1" style="background-color:#FFFFFF;">The</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.3"><span class="ltx_text" id="S3.F2.2.1.1.3.1" style="background-color:#FFEBEB;"></span></span>
<span class="ltx_p" id="S3.F2.2.1.1.4"><span class="ltx_text" id="S3.F2.2.1.1.4.1" style="background-color:#FFF8F8;">2</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.5"><span class="ltx_text" id="S3.F2.2.1.1.5.1" style="background-color:#FFF7F7;">0</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.6"><span class="ltx_text" id="S3.F2.2.1.1.6.1" style="background-color:#FFFBFB;">1</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.7"><span class="ltx_text" id="S3.F2.2.1.1.7.1" style="background-color:#FFDBDB;">8</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.8"><span class="ltx_text" id="S3.F2.2.1.1.8.1" style="background-color:#FFFFFF;">Chinese</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.9"><span class="ltx_text" id="S3.F2.2.1.1.9.1" style="background-color:#FFF9F9;">Professional</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.10"><span class="ltx_text" id="S3.F2.2.1.1.10.1" style="background-color:#FFF6F6;">Baseball</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.11"><span class="ltx_text" id="S3.F2.2.1.1.11.1" style="background-color:#FFF2F2;">League</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.12"><span class="ltx_text" id="S3.F2.2.1.1.12.1" style="background-color:#FFDFDF;">(</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.13"><span class="ltx_text" id="S3.F2.2.1.1.13.1" style="background-color:#FFA3A3;">CP</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.14"><span class="ltx_text" id="S3.F2.2.1.1.14.1" style="background-color:#FFAAAA;">BL</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.15"><span class="ltx_text" id="S3.F2.2.1.1.15.1" style="background-color:#FFEDED;">)</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.16"><span class="ltx_text" id="S3.F2.2.1.1.16.1" style="background-color:#FFFBFB;">Red</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.17"><span class="ltx_text" id="S3.F2.2.1.1.17.1" style="background-color:#FFDFDF;">vs</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.18"><span class="ltx_text" id="S3.F2.2.1.1.18.1" style="background-color:#FFB3B3;">White</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.19"><span class="ltx_text" id="S3.F2.2.1.1.19.1" style="background-color:#FF9898;">All</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.20"><span class="ltx_text" id="S3.F2.2.1.1.20.1" style="background-color:#FFECEC;">-</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.21"><span class="ltx_text" id="S3.F2.2.1.1.21.1" style="background-color:#FFA7A7;">Star</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.22"><span class="ltx_text" id="S3.F2.2.1.1.22.1" style="background-color:#FFB5B5;">Game</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.23"><span class="ltx_text" id="S3.F2.2.1.1.23.1" style="background-color:#FFF4F4;">was</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.24"><span class="ltx_text" id="S3.F2.2.1.1.24.1" style="background-color:#FFF8F8;">a</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.25"><span class="ltx_text" id="S3.F2.2.1.1.25.1" style="background-color:#FFE6E6;"> special</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.26"><span class="ltx_text" id="S3.F2.2.1.1.26.1" style="background-color:#FFFFFF;">event</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.27"><span class="ltx_text" id="S3.F2.2.1.1.27.1" style="background-color:#FFE5E5;">held</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.28"><span class="ltx_text" id="S3.F2.2.1.1.28.1" style="background-color:#FFFDFD;">during</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.29"><span class="ltx_text" id="S3.F2.2.1.1.29.1" style="background-color:#FFFFFF;">the</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.30"><span class="ltx_text" id="S3.F2.2.1.1.30.1" style="background-color:#FFEBEB;"></span></span>
<span class="ltx_p" id="S3.F2.2.1.1.31"><span class="ltx_text" id="S3.F2.2.1.1.31.1" style="background-color:#FFF7F7;">2</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.32"><span class="ltx_text" id="S3.F2.2.1.1.32.1" style="background-color:#FFEEEE;">9</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.33"><span class="ltx_text" id="S3.F2.2.1.1.33.1" style="background-color:#FFD9D9;">th</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.34"><span class="ltx_text" id="S3.F2.2.1.1.34.1" style="background-color:#FF7070;">CP</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.35"><span class="ltx_text" id="S3.F2.2.1.1.35.1" style="background-color:#FF8181;">BL</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.36"><span class="ltx_text" id="S3.F2.2.1.1.36.1" style="background-color:#FFEDED;">season</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.37"><span class="ltx_text" id="S3.F2.2.1.1.37.1" style="background-color:#FFDFDF;">.</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.38"><span class="ltx_text" id="S3.F2.2.1.1.38.1" style="background-color:#FFF5F5;">It</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.39"><span class="ltx_text" id="S3.F2.2.1.1.39.1" style="background-color:#FFCACA;">took</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.40"><span class="ltx_text" id="S3.F2.2.1.1.40.1" style="background-color:#FFFFFF;">place</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.41"><span class="ltx_text" id="S3.F2.2.1.1.41.1" style="background-color:#FFFFFF;">on</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.42"><span class="ltx_text" id="S3.F2.2.1.1.42.1" style="background-color:#FFE4E4;">July</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.43"><span class="ltx_text" id="S3.F2.2.1.1.43.1" style="background-color:#FFC7C7;"></span></span>
<span class="ltx_p" id="S3.F2.2.1.1.44"><span class="ltx_text" id="S3.F2.2.1.1.44.1" style="background-color:#FF0000;">7</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.45"><span class="ltx_text" id="S3.F2.2.1.1.45.1" style="background-color:#FF4343;">th</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.46"><span class="ltx_text" id="S3.F2.2.1.1.46.1" style="background-color:#FFC4C4;">and</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.47"><span class="ltx_text" id="S3.F2.2.1.1.47.1" style="background-color:#FFFCFC;">July</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.48"><span class="ltx_text" id="S3.F2.2.1.1.48.1" style="background-color:#FFDDDD;"></span></span>
<span class="ltx_p" id="S3.F2.2.1.1.49"><span class="ltx_text" id="S3.F2.2.1.1.49.1" style="background-color:#FF9292;">8</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.50"><span class="ltx_text" id="S3.F2.2.1.1.50.1" style="background-color:#FF9898;">th</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.51"><span class="ltx_text" id="S3.F2.2.1.1.51.1" style="background-color:#FFEDED;">,</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.52"><span class="ltx_text" id="S3.F2.2.1.1.52.1" style="background-color:#FFDFDF;"></span></span>
<span class="ltx_p" id="S3.F2.2.1.1.53"><span class="ltx_text" id="S3.F2.2.1.1.53.1" style="background-color:#FFFDFD;">2</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.54"><span class="ltx_text" id="S3.F2.2.1.1.54.1" style="background-color:#FFFFFF;">0</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.55"><span class="ltx_text" id="S3.F2.2.1.1.55.1" style="background-color:#FFFDFD;">1</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.56"><span class="ltx_text" id="S3.F2.2.1.1.56.1" style="background-color:#FFEEEE;">8</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.57"><span class="ltx_text" id="S3.F2.2.1.1.57.1" style="background-color:#FFFCFC;">,</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.58"><span class="ltx_text" id="S3.F2.2.1.1.58.1" style="background-color:#FFF9F9;">at</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.59"><span class="ltx_text" id="S3.F2.2.1.1.59.1" style="background-color:#FFFEFE;">the</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.60"><span class="ltx_text" id="S3.F2.2.1.1.60.1" style="background-color:#FFE4E4;">Taip</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.61"><span class="ltx_text" id="S3.F2.2.1.1.61.1" style="background-color:#FFF0F0;">ei</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.62"><span class="ltx_text" id="S3.F2.2.1.1.62.1" style="background-color:#FFD6D6;">Tian</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.63"><span class="ltx_text" id="S3.F2.2.1.1.63.1" style="background-color:#FFE7E7;">mu</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.64"><span class="ltx_text" id="S3.F2.2.1.1.64.1" style="background-color:#FFF9F9;">Baseball</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.65"><span class="ltx_text" id="S3.F2.2.1.1.65.1" style="background-color:#FFC8C8;">Stadium</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.66"><span class="ltx_text" id="S3.F2.2.1.1.66.1" style="background-color:#FFFFFF;">in</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.67"><span class="ltx_text" id="S3.F2.2.1.1.67.1" style="background-color:#FFEBEB;">Taip</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.68"><span class="ltx_text" id="S3.F2.2.1.1.68.1" style="background-color:#FFF9F9;">ei</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.69"><span class="ltx_text" id="S3.F2.2.1.1.69.1" style="background-color:#FFFBFB;">,</span></span>
<span class="ltx_p" id="S3.F2.2.1.1.70"><span class="ltx_text" id="S3.F2.2.1.1.70.1" style="background-color:#FFFAFA;">Taiwan</span></span>
</span></p>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S3.F2.3" style="width:433.6pt;height:57.4pt;vertical-align:-25.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.7pt,0.1pt) scale(0.992371431755214,0.992371431755214) ;">
<p class="ltx_p" id="S3.F2.3.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S3.F2.3.1.1" style="width:433.6pt;">
<span class="ltx_p" id="S3.F2.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.F2.3.1.1.1.1">Compressed by instruction ”Who won the 2018 CPBL Red vs White All-Star Game? ”:</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.2"><span class="ltx_text" id="S3.F2.3.1.1.2.1" style="background-color:#FFFFFF;">The</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.3"><span class="ltx_text" id="S3.F2.3.1.1.3.1" style="background-color:#FFEFEF;"></span></span>
<span class="ltx_p" id="S3.F2.3.1.1.4"><span class="ltx_text" id="S3.F2.3.1.1.4.1" style="background-color:#FFF5F5;">2</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.5"><span class="ltx_text" id="S3.F2.3.1.1.5.1" style="background-color:#FFF2F2;">0</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.6"><span class="ltx_text" id="S3.F2.3.1.1.6.1" style="background-color:#FFF8F8;">1</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.7"><span class="ltx_text" id="S3.F2.3.1.1.7.1" style="background-color:#FFE9E9;">8</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.8"><span class="ltx_text" id="S3.F2.3.1.1.8.1" style="background-color:#FFFFFF;">Chinese</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.9"><span class="ltx_text" id="S3.F2.3.1.1.9.1" style="background-color:#FFFAFA;">Professional</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.10"><span class="ltx_text" id="S3.F2.3.1.1.10.1" style="background-color:#FFFEFE;">Baseball</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.11"><span class="ltx_text" id="S3.F2.3.1.1.11.1" style="background-color:#FFFFFF;">League</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.12"><span class="ltx_text" id="S3.F2.3.1.1.12.1" style="background-color:#FFEEEE;">(</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.13"><span class="ltx_text" id="S3.F2.3.1.1.13.1" style="background-color:#FF7C7C;">CP</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.14"><span class="ltx_text" id="S3.F2.3.1.1.14.1" style="background-color:#FF8282;">BL</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.15"><span class="ltx_text" id="S3.F2.3.1.1.15.1" style="background-color:#FFDFDF;">)</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.16"><span class="ltx_text" id="S3.F2.3.1.1.16.1" style="background-color:#FFF7F7;">Red</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.17"><span class="ltx_text" id="S3.F2.3.1.1.17.1" style="background-color:#FFD2D2;">vs</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.18"><span class="ltx_text" id="S3.F2.3.1.1.18.1" style="background-color:#FF9C9C;">White</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.19"><span class="ltx_text" id="S3.F2.3.1.1.19.1" style="background-color:#FF7D7D;">All</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.20"><span class="ltx_text" id="S3.F2.3.1.1.20.1" style="background-color:#FFE7E7;">-</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.21"><span class="ltx_text" id="S3.F2.3.1.1.21.1" style="background-color:#FF7C7C;">Star</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.22"><span class="ltx_text" id="S3.F2.3.1.1.22.1" style="background-color:#FF8E8E;">Game</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.23"><span class="ltx_text" id="S3.F2.3.1.1.23.1" style="background-color:#FFF3F3;">was</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.24"><span class="ltx_text" id="S3.F2.3.1.1.24.1" style="background-color:#FFF3F3;">a</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.25"><span class="ltx_text" id="S3.F2.3.1.1.25.1" style="background-color:#FFE6E6;"> special</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.26"><span class="ltx_text" id="S3.F2.3.1.1.26.1" style="background-color:#FFFFFF;">event</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.27"><span class="ltx_text" id="S3.F2.3.1.1.27.1" style="background-color:#FFFFFF;">held</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.28"><span class="ltx_text" id="S3.F2.3.1.1.28.1" style="background-color:#FFFEFE;">during</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.29"><span class="ltx_text" id="S3.F2.3.1.1.29.1" style="background-color:#FFFFFF;">the</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.30"><span class="ltx_text" id="S3.F2.3.1.1.30.1" style="background-color:#FFD7D7;"></span></span>
<span class="ltx_p" id="S3.F2.3.1.1.31"><span class="ltx_text" id="S3.F2.3.1.1.31.1" style="background-color:#FFE8E8;">2</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.32"><span class="ltx_text" id="S3.F2.3.1.1.32.1" style="background-color:#FFEFEF;">9</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.33"><span class="ltx_text" id="S3.F2.3.1.1.33.1" style="background-color:#FFFBFB;">th</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.34"><span class="ltx_text" id="S3.F2.3.1.1.34.1" style="background-color:#FF0000;">CP</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.35"><span class="ltx_text" id="S3.F2.3.1.1.35.1" style="background-color:#FF4545;">BL</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.36"><span class="ltx_text" id="S3.F2.3.1.1.36.1" style="background-color:#FFD0D0;">season</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.37"><span class="ltx_text" id="S3.F2.3.1.1.37.1" style="background-color:#FFD3D3;">.</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.38"><span class="ltx_text" id="S3.F2.3.1.1.38.1" style="background-color:#FFFFFF;">The</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.39"><span class="ltx_text" id="S3.F2.3.1.1.39.1" style="background-color:#FFFFFF;">main</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.40"><span class="ltx_text" id="S3.F2.3.1.1.40.1" style="background-color:#FF8C8C;">game</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.41"><span class="ltx_text" id="S3.F2.3.1.1.41.1" style="background-color:#FFEFEF;">concluded</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.42"><span class="ltx_text" id="S3.F2.3.1.1.42.1" style="background-color:#FFA9A9;">with</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.43"><span class="ltx_text" id="S3.F2.3.1.1.43.1" style="background-color:#FFFEFE;">the</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.44"><span class="ltx_text" id="S3.F2.3.1.1.44.1" style="background-color:#FFDFDF;">Three</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.45"><span class="ltx_text" id="S3.F2.3.1.1.45.1" style="background-color:#FFDDDD;">-</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.46"><span class="ltx_text" id="S3.F2.3.1.1.46.1" style="background-color:#FFF0F0;">Family</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.47"><span class="ltx_text" id="S3.F2.3.1.1.47.1" style="background-color:#FFDFDF;">Mart</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.48"><span class="ltx_text" id="S3.F2.3.1.1.48.1" style="background-color:#FFF2F2;">Brothers</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.49"><span class="ltx_text" id="S3.F2.3.1.1.49.1" style="background-color:#FFDDDD;">White</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.50"><span class="ltx_text" id="S3.F2.3.1.1.50.1" style="background-color:#FFEFEF;"> Team</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.51"><span class="ltx_text" id="S3.F2.3.1.1.51.1" style="background-color:#FFDBDB;">defeat</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.52"><span class="ltx_text" id="S3.F2.3.1.1.52.1" style="background-color:#FFF7F7;">ing</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.53"><span class="ltx_text" id="S3.F2.3.1.1.53.1" style="background-color:#FFFFFF;">the</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.54"><span class="ltx_text" id="S3.F2.3.1.1.54.1" style="background-color:#FFEAEA;">Taiwan</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.55"><span class="ltx_text" id="S3.F2.3.1.1.55.1" style="background-color:#FFD7D7;">Cooperativ</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.56"><span class="ltx_text" id="S3.F2.3.1.1.56.1" style="background-color:#FFE9E9;">e</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.57"><span class="ltx_text" id="S3.F2.3.1.1.57.1" style="background-color:#FFEFEF;">Bank</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.58"><span class="ltx_text" id="S3.F2.3.1.1.58.1" style="background-color:#FFFBFB;">Red</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.59"><span class="ltx_text" id="S3.F2.3.1.1.59.1" style="background-color:#FFEBEB;">Team</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.60"><span class="ltx_text" id="S3.F2.3.1.1.60.1" style="background-color:#FFBABA;">by</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.61"><span class="ltx_text" id="S3.F2.3.1.1.61.1" style="background-color:#FFFFFF;">a</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.62"><span class="ltx_text" id="S3.F2.3.1.1.62.1" style="background-color:#FFF2F2;">one</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.63"><span class="ltx_text" id="S3.F2.3.1.1.63.1" style="background-color:#FFD8D8;">-</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.64"><span class="ltx_text" id="S3.F2.3.1.1.64.1" style="background-color:#FFDDDD;">run</span></span>
<span class="ltx_p" id="S3.F2.3.1.1.65"><span class="ltx_text" id="S3.F2.3.1.1.65.1" style="background-color:#FFF9F9;">difference</span></span>
</span></p>
</span></div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A visualisation of Instruction-Aware Contextual Compression. Deeper color indicates a stronger relevance to the instruction.</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Model Architecture</h3>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="325" id="S3.F3.g1" src="extracted/5817813/arch.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Model architecture of Instruction-Aware Contextual Compressor. We jointly optimize two objectives which enforce the model to extract contextual representation most relevant to the instruction.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We introduce Instruction-Aware Contextual Compressor as a trainable module to implement the Instruction-Aware Contextual Compression method. Instruction-Aware Contextual Compressor adopts an encoder-decoder architecture, consisting of both an encoder and a decoder, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S3.F3" title="Figure 3 ‣ 3.1 Model Architecture ‣ 3 Method ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The document encoder is a standard multi-layer transformer encoder and utilized to extract features from the input documents.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">The decoder is a standard multi-layer transformer decoder, which equipped with two distinct functionalities, which can be toggled by modifying the masking:</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">The ranking decoder performs re-ranking based on the output features obtained from the document encoder. In this mode, interaction occurs between instruction features and document features within cross-attention layers. The decoder employs bidirectional self-attention layers without any masking.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">The generation decoder, on the other hand, replaces the bidirectional self-attention layers in the decoder with causal self-attention layers. It uses a [BOS] token to denote the start of a sequence and an end-of-sequence token to signify its conclusion.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">The model consists of 8 encoder and 8 decoder layers, which affect its depth and ability to capture complex patterns. The primary model dimension is 512 and it uses a feed-forward dimension of 1024 for its inner layers. The model has 6 attention heads, allowing it to focus on different aspects of the input data.
The model has a total of 0.18 billion trainable parameters, which is significantly smaller than the current mainstream large language models like Llama  <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib31" title="">31</a>]</cite>, which have 7 billion, 13 billion, and 70 billion parameters, respectively.
We initialize Instruction-Aware Contextual Compressor with the pre-trained weights of umT5<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib6" title="">6</a>]</cite>, which has been pre-trained on a multilingual corpus, enabling it to handle multilingual tasks effectively.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1">The model’s architecture, in conjunction with its training objectives, empowers it to capture the intricate interplay between instructions and documents, facilitating the extraction of the most pertinent information from the documents.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training Objectives</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We jointly optimize two objectives during training,
with one ranking-based objective and one generation based objective. Each instruction-document pair only requires one forward pass through the document encoder, and two forward passes through the decoder, where different functionalities are activated to compute the two losses as delineated below</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Ranking Loss</span> activates the ranking decoder. It aims to learn instruction-document representation that captures the fine-grained alignment between instruction and document. In ranking task, the model uses a ranking head (a linear layer) to predict a instruction-document matching score given their instruction feature and document feature. All positive samples are placed in the first position, and an additional 19 hard negative samples are retrieved by the retriever. Then, the model is trained as a 20-class classification task. This approach effectively boosts the scores of positive samples while suppressing the scores of negative samples. The specific formula for calculating the loss is as follows:</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="L_{ranking}=-\sum_{i}^{c}t_{i}log(\frac{e^{s_{i}}}{\sum_{j}^{c}{e^{s_{j}}}})" class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.2" xref="S3.E1.m1.1.2.cmml"><msub id="S3.E1.m1.1.2.2" xref="S3.E1.m1.1.2.2.cmml"><mi id="S3.E1.m1.1.2.2.2" xref="S3.E1.m1.1.2.2.2.cmml">L</mi><mrow id="S3.E1.m1.1.2.2.3" xref="S3.E1.m1.1.2.2.3.cmml"><mi id="S3.E1.m1.1.2.2.3.2" xref="S3.E1.m1.1.2.2.3.2.cmml">r</mi><mo id="S3.E1.m1.1.2.2.3.1" xref="S3.E1.m1.1.2.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.2.2.3.3" xref="S3.E1.m1.1.2.2.3.3.cmml">a</mi><mo id="S3.E1.m1.1.2.2.3.1a" xref="S3.E1.m1.1.2.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.2.2.3.4" xref="S3.E1.m1.1.2.2.3.4.cmml">n</mi><mo id="S3.E1.m1.1.2.2.3.1b" xref="S3.E1.m1.1.2.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.2.2.3.5" xref="S3.E1.m1.1.2.2.3.5.cmml">k</mi><mo id="S3.E1.m1.1.2.2.3.1c" xref="S3.E1.m1.1.2.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.2.2.3.6" xref="S3.E1.m1.1.2.2.3.6.cmml">i</mi><mo id="S3.E1.m1.1.2.2.3.1d" xref="S3.E1.m1.1.2.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.2.2.3.7" xref="S3.E1.m1.1.2.2.3.7.cmml">n</mi><mo id="S3.E1.m1.1.2.2.3.1e" xref="S3.E1.m1.1.2.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.2.2.3.8" xref="S3.E1.m1.1.2.2.3.8.cmml">g</mi></mrow></msub><mo id="S3.E1.m1.1.2.1" xref="S3.E1.m1.1.2.1.cmml">=</mo><mrow id="S3.E1.m1.1.2.3" xref="S3.E1.m1.1.2.3.cmml"><mo id="S3.E1.m1.1.2.3a" xref="S3.E1.m1.1.2.3.cmml">−</mo><mrow id="S3.E1.m1.1.2.3.2" xref="S3.E1.m1.1.2.3.2.cmml"><munderover id="S3.E1.m1.1.2.3.2.1" xref="S3.E1.m1.1.2.3.2.1.cmml"><mo id="S3.E1.m1.1.2.3.2.1.2.2" movablelimits="false" xref="S3.E1.m1.1.2.3.2.1.2.2.cmml">∑</mo><mi id="S3.E1.m1.1.2.3.2.1.2.3" xref="S3.E1.m1.1.2.3.2.1.2.3.cmml">i</mi><mi id="S3.E1.m1.1.2.3.2.1.3" xref="S3.E1.m1.1.2.3.2.1.3.cmml">c</mi></munderover><mrow id="S3.E1.m1.1.2.3.2.2" xref="S3.E1.m1.1.2.3.2.2.cmml"><msub id="S3.E1.m1.1.2.3.2.2.2" xref="S3.E1.m1.1.2.3.2.2.2.cmml"><mi id="S3.E1.m1.1.2.3.2.2.2.2" xref="S3.E1.m1.1.2.3.2.2.2.2.cmml">t</mi><mi id="S3.E1.m1.1.2.3.2.2.2.3" xref="S3.E1.m1.1.2.3.2.2.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.2.3.2.2.1" xref="S3.E1.m1.1.2.3.2.2.1.cmml">⁢</mo><mi id="S3.E1.m1.1.2.3.2.2.3" xref="S3.E1.m1.1.2.3.2.2.3.cmml">l</mi><mo id="S3.E1.m1.1.2.3.2.2.1a" xref="S3.E1.m1.1.2.3.2.2.1.cmml">⁢</mo><mi id="S3.E1.m1.1.2.3.2.2.4" xref="S3.E1.m1.1.2.3.2.2.4.cmml">o</mi><mo id="S3.E1.m1.1.2.3.2.2.1b" xref="S3.E1.m1.1.2.3.2.2.1.cmml">⁢</mo><mi id="S3.E1.m1.1.2.3.2.2.5" xref="S3.E1.m1.1.2.3.2.2.5.cmml">g</mi><mo id="S3.E1.m1.1.2.3.2.2.1c" xref="S3.E1.m1.1.2.3.2.2.1.cmml">⁢</mo><mrow id="S3.E1.m1.1.2.3.2.2.6.2" xref="S3.E1.m1.1.1.cmml"><mo id="S3.E1.m1.1.2.3.2.2.6.2.1" stretchy="false" xref="S3.E1.m1.1.1.cmml">(</mo><mfrac id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msup id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.2.2" xref="S3.E1.m1.1.1.2.2.cmml">e</mi><msub id="S3.E1.m1.1.1.2.3" xref="S3.E1.m1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1.2.3.2" xref="S3.E1.m1.1.1.2.3.2.cmml">s</mi><mi id="S3.E1.m1.1.1.2.3.3" xref="S3.E1.m1.1.1.2.3.3.cmml">i</mi></msub></msup><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><msubsup id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml"><mo id="S3.E1.m1.1.1.3.1.2.2" xref="S3.E1.m1.1.1.3.1.2.2.cmml">∑</mo><mi id="S3.E1.m1.1.1.3.1.2.3" xref="S3.E1.m1.1.1.3.1.2.3.cmml">j</mi><mi id="S3.E1.m1.1.1.3.1.3" xref="S3.E1.m1.1.1.3.1.3.cmml">c</mi></msubsup><msup id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml">e</mi><msub id="S3.E1.m1.1.1.3.2.3" xref="S3.E1.m1.1.1.3.2.3.cmml"><mi id="S3.E1.m1.1.1.3.2.3.2" xref="S3.E1.m1.1.1.3.2.3.2.cmml">s</mi><mi id="S3.E1.m1.1.1.3.2.3.3" xref="S3.E1.m1.1.1.3.2.3.3.cmml">j</mi></msub></msup></mrow></mfrac><mo id="S3.E1.m1.1.2.3.2.2.6.2.2" stretchy="false" xref="S3.E1.m1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.2.cmml" xref="S3.E1.m1.1.2"><eq id="S3.E1.m1.1.2.1.cmml" xref="S3.E1.m1.1.2.1"></eq><apply id="S3.E1.m1.1.2.2.cmml" xref="S3.E1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.2.1.cmml" xref="S3.E1.m1.1.2.2">subscript</csymbol><ci id="S3.E1.m1.1.2.2.2.cmml" xref="S3.E1.m1.1.2.2.2">𝐿</ci><apply id="S3.E1.m1.1.2.2.3.cmml" xref="S3.E1.m1.1.2.2.3"><times id="S3.E1.m1.1.2.2.3.1.cmml" xref="S3.E1.m1.1.2.2.3.1"></times><ci id="S3.E1.m1.1.2.2.3.2.cmml" xref="S3.E1.m1.1.2.2.3.2">𝑟</ci><ci id="S3.E1.m1.1.2.2.3.3.cmml" xref="S3.E1.m1.1.2.2.3.3">𝑎</ci><ci id="S3.E1.m1.1.2.2.3.4.cmml" xref="S3.E1.m1.1.2.2.3.4">𝑛</ci><ci id="S3.E1.m1.1.2.2.3.5.cmml" xref="S3.E1.m1.1.2.2.3.5">𝑘</ci><ci id="S3.E1.m1.1.2.2.3.6.cmml" xref="S3.E1.m1.1.2.2.3.6">𝑖</ci><ci id="S3.E1.m1.1.2.2.3.7.cmml" xref="S3.E1.m1.1.2.2.3.7">𝑛</ci><ci id="S3.E1.m1.1.2.2.3.8.cmml" xref="S3.E1.m1.1.2.2.3.8">𝑔</ci></apply></apply><apply id="S3.E1.m1.1.2.3.cmml" xref="S3.E1.m1.1.2.3"><minus id="S3.E1.m1.1.2.3.1.cmml" xref="S3.E1.m1.1.2.3"></minus><apply id="S3.E1.m1.1.2.3.2.cmml" xref="S3.E1.m1.1.2.3.2"><apply id="S3.E1.m1.1.2.3.2.1.cmml" xref="S3.E1.m1.1.2.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.3.2.1.1.cmml" xref="S3.E1.m1.1.2.3.2.1">superscript</csymbol><apply id="S3.E1.m1.1.2.3.2.1.2.cmml" xref="S3.E1.m1.1.2.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.3.2.1.2.1.cmml" xref="S3.E1.m1.1.2.3.2.1">subscript</csymbol><sum id="S3.E1.m1.1.2.3.2.1.2.2.cmml" xref="S3.E1.m1.1.2.3.2.1.2.2"></sum><ci id="S3.E1.m1.1.2.3.2.1.2.3.cmml" xref="S3.E1.m1.1.2.3.2.1.2.3">𝑖</ci></apply><ci id="S3.E1.m1.1.2.3.2.1.3.cmml" xref="S3.E1.m1.1.2.3.2.1.3">𝑐</ci></apply><apply id="S3.E1.m1.1.2.3.2.2.cmml" xref="S3.E1.m1.1.2.3.2.2"><times id="S3.E1.m1.1.2.3.2.2.1.cmml" xref="S3.E1.m1.1.2.3.2.2.1"></times><apply id="S3.E1.m1.1.2.3.2.2.2.cmml" xref="S3.E1.m1.1.2.3.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.3.2.2.2.1.cmml" xref="S3.E1.m1.1.2.3.2.2.2">subscript</csymbol><ci id="S3.E1.m1.1.2.3.2.2.2.2.cmml" xref="S3.E1.m1.1.2.3.2.2.2.2">𝑡</ci><ci id="S3.E1.m1.1.2.3.2.2.2.3.cmml" xref="S3.E1.m1.1.2.3.2.2.2.3">𝑖</ci></apply><ci id="S3.E1.m1.1.2.3.2.2.3.cmml" xref="S3.E1.m1.1.2.3.2.2.3">𝑙</ci><ci id="S3.E1.m1.1.2.3.2.2.4.cmml" xref="S3.E1.m1.1.2.3.2.2.4">𝑜</ci><ci id="S3.E1.m1.1.2.3.2.2.5.cmml" xref="S3.E1.m1.1.2.3.2.2.5">𝑔</ci><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.2.3.2.2.6.2"><divide id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.2.3.2.2.6.2"></divide><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.2">superscript</csymbol><ci id="S3.E1.m1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.2.2">𝑒</ci><apply id="S3.E1.m1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.2.3.2">𝑠</ci><ci id="S3.E1.m1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.2.3.3">𝑖</ci></apply></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><apply id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.1.1.cmml" xref="S3.E1.m1.1.1.3.1">superscript</csymbol><apply id="S3.E1.m1.1.1.3.1.2.cmml" xref="S3.E1.m1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.1.2.1.cmml" xref="S3.E1.m1.1.1.3.1">subscript</csymbol><sum id="S3.E1.m1.1.1.3.1.2.2.cmml" xref="S3.E1.m1.1.1.3.1.2.2"></sum><ci id="S3.E1.m1.1.1.3.1.2.3.cmml" xref="S3.E1.m1.1.1.3.1.2.3">𝑗</ci></apply><ci id="S3.E1.m1.1.1.3.1.3.cmml" xref="S3.E1.m1.1.1.3.1.3">𝑐</ci></apply><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2">superscript</csymbol><ci id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2">𝑒</ci><apply id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.1.1.3.2.3.2">𝑠</ci><ci id="S3.E1.m1.1.1.3.2.3.3.cmml" xref="S3.E1.m1.1.1.3.2.3.3">𝑗</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">L_{ranking}=-\sum_{i}^{c}t_{i}log(\frac{e^{s_{i}}}{\sum_{j}^{c}{e^{s_{j}}}})</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_L start_POSTSUBSCRIPT italic_r italic_a italic_n italic_k italic_i italic_n italic_g end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_l italic_o italic_g ( divide start_ARG italic_e start_POSTSUPERSCRIPT italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT italic_s start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.5">where <math alttext="s_{i}" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><msub id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">s</mi><mi id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">𝑠</ci><ci id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">s_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> represents the ranking score of <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2.1"><semantics id="S3.SS2.p4.2.m2.1a"><mi id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><ci id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.2.m2.1d">italic_i</annotation></semantics></math>-th sample. <math alttext="t_{i}" class="ltx_Math" display="inline" id="S3.SS2.p4.3.m3.1"><semantics id="S3.SS2.p4.3.m3.1a"><msub id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml"><mi id="S3.SS2.p4.3.m3.1.1.2" xref="S3.SS2.p4.3.m3.1.1.2.cmml">t</mi><mi id="S3.SS2.p4.3.m3.1.1.3" xref="S3.SS2.p4.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2">𝑡</ci><ci id="S3.SS2.p4.3.m3.1.1.3.cmml" xref="S3.SS2.p4.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">t_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.3.m3.1d">italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p4.4.m4.1"><semantics id="S3.SS2.p4.4.m4.1a"><mi id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><ci id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.4.m4.1d">italic_i</annotation></semantics></math>-th target. <math alttext="c" class="ltx_Math" display="inline" id="S3.SS2.p4.5.m5.1"><semantics id="S3.SS2.p4.5.m5.1a"><mi id="S3.SS2.p4.5.m5.1.1" xref="S3.SS2.p4.5.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m5.1b"><ci id="S3.SS2.p4.5.m5.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m5.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.5.m5.1d">italic_c</annotation></semantics></math> represents the number of class.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p5.1.1">The Language Modeling Loss</span> activates the generation decoder, with the goal of generating useful response based on the provided context and instruction. It is optimized using a cross-entropy loss that trains the model to maximize the likelihood of the text in an autoregressive manner. We incorporate a label smoothing factor of 0.1 when calculating the loss. In comparison to the ranking loss, the Language Modeling loss equips the model with the ability to generalize for following instructions. This empowers the model to gain a deeper understanding of the potential correct context location and to effectively model fine-grained correlations. the language modeling loss can be written as:</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="L_{lm}=-\sum_{t=1}^{N}\log{p(y_{t}|y_{&lt;t})}" class="ltx_Math" display="block" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">L</mi><mrow id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.3.3.2" xref="S3.E2.m1.1.1.3.3.2.cmml">l</mi><mo id="S3.E2.m1.1.1.3.3.1" xref="S3.E2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.3.3.3" xref="S3.E2.m1.1.1.3.3.3.cmml">m</mi></mrow></msub><mo id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mo id="S3.E2.m1.1.1.1a" xref="S3.E2.m1.1.1.1.cmml">−</mo><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><munderover id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml"><mo id="S3.E2.m1.1.1.1.1.2.2.2" movablelimits="false" xref="S3.E2.m1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E2.m1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.3.2" xref="S3.E2.m1.1.1.1.1.2.2.3.2.cmml">t</mi><mo id="S3.E2.m1.1.1.1.1.2.2.3.1" xref="S3.E2.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E2.m1.1.1.1.1.2.2.3.3" xref="S3.E2.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E2.m1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.3.1.cmml">log</mi><mo id="S3.E2.m1.1.1.1.1.1.3a" lspace="0.167em" xref="S3.E2.m1.1.1.1.1.1.3.cmml">⁡</mo><mi id="S3.E2.m1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.3.2.cmml">p</mi></mrow><mo id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.cmml">y</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml">t</mi></msub><mo fence="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml">y</mi><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.2.cmml"></mi><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">t</mi></mrow></msub></mrow><mo id="S3.E2.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"></eq><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">𝐿</ci><apply id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3"><times id="S3.E2.m1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3.1"></times><ci id="S3.E2.m1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.2">𝑙</ci><ci id="S3.E2.m1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3">𝑚</ci></apply></apply><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><minus id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1"></minus><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1"><apply id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2">subscript</csymbol><sum id="S3.E2.m1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2"></sum><apply id="S3.E2.m1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.3"><eq id="S3.E2.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.3.2">𝑡</ci><cn id="S3.E2.m1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.E2.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.3">𝑁</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3"><log id="S3.E2.m1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.3.1"></log><ci id="S3.E2.m1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.3.2">𝑝</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.2">𝑦</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2">𝑦</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.3"><lt id="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.2">absent</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.3">𝑡</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">L_{lm}=-\sum_{t=1}^{N}\log{p(y_{t}|y_{&lt;t})}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">italic_L start_POSTSUBSCRIPT italic_l italic_m end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_log italic_p ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_y start_POSTSUBSCRIPT &lt; italic_t end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p6.4">where <math alttext="p(y_{t}|y_{&lt;t})" class="ltx_Math" display="inline" id="S3.SS2.p6.1.m1.1"><semantics id="S3.SS2.p6.1.m1.1a"><mrow id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml"><mi id="S3.SS2.p6.1.m1.1.1.3" xref="S3.SS2.p6.1.m1.1.1.3.cmml">p</mi><mo id="S3.SS2.p6.1.m1.1.1.2" xref="S3.SS2.p6.1.m1.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p6.1.m1.1.1.1.1" xref="S3.SS2.p6.1.m1.1.1.1.1.1.cmml"><mo id="S3.SS2.p6.1.m1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p6.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p6.1.m1.1.1.1.1.1" xref="S3.SS2.p6.1.m1.1.1.1.1.1.cmml"><msub id="S3.SS2.p6.1.m1.1.1.1.1.1.2" xref="S3.SS2.p6.1.m1.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p6.1.m1.1.1.1.1.1.2.2" xref="S3.SS2.p6.1.m1.1.1.1.1.1.2.2.cmml">y</mi><mi id="S3.SS2.p6.1.m1.1.1.1.1.1.2.3" xref="S3.SS2.p6.1.m1.1.1.1.1.1.2.3.cmml">t</mi></msub><mo fence="false" id="S3.SS2.p6.1.m1.1.1.1.1.1.1" xref="S3.SS2.p6.1.m1.1.1.1.1.1.1.cmml">|</mo><msub id="S3.SS2.p6.1.m1.1.1.1.1.1.3" xref="S3.SS2.p6.1.m1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p6.1.m1.1.1.1.1.1.3.2" xref="S3.SS2.p6.1.m1.1.1.1.1.1.3.2.cmml">y</mi><mrow id="S3.SS2.p6.1.m1.1.1.1.1.1.3.3" xref="S3.SS2.p6.1.m1.1.1.1.1.1.3.3.cmml"><mi id="S3.SS2.p6.1.m1.1.1.1.1.1.3.3.2" xref="S3.SS2.p6.1.m1.1.1.1.1.1.3.3.2.cmml"></mi><mo id="S3.SS2.p6.1.m1.1.1.1.1.1.3.3.1" xref="S3.SS2.p6.1.m1.1.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S3.SS2.p6.1.m1.1.1.1.1.1.3.3.3" xref="S3.SS2.p6.1.m1.1.1.1.1.1.3.3.3.cmml">t</mi></mrow></msub></mrow><mo id="S3.SS2.p6.1.m1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p6.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><apply id="S3.SS2.p6.1.m1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1"><times id="S3.SS2.p6.1.m1.1.1.2.cmml" xref="S3.SS2.p6.1.m1.1.1.2"></times><ci id="S3.SS2.p6.1.m1.1.1.3.cmml" xref="S3.SS2.p6.1.m1.1.1.3">𝑝</ci><apply id="S3.SS2.p6.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.p6.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.SS2.p6.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS2.p6.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.1.2.2">𝑦</ci><ci id="S3.SS2.p6.1.m1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S3.SS2.p6.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p6.1.m1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.1.3.2">𝑦</ci><apply id="S3.SS2.p6.1.m1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.1.3.3"><lt id="S3.SS2.p6.1.m1.1.1.1.1.1.3.3.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S3.SS2.p6.1.m1.1.1.1.1.1.3.3.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.1.3.3.2">absent</csymbol><ci id="S3.SS2.p6.1.m1.1.1.1.1.1.3.3.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.1.3.3.3">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">p(y_{t}|y_{&lt;t})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.1.m1.1d">italic_p ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_y start_POSTSUBSCRIPT &lt; italic_t end_POSTSUBSCRIPT )</annotation></semantics></math> denotes the output probability for the correct token <math alttext="y_{t}" class="ltx_Math" display="inline" id="S3.SS2.p6.2.m2.1"><semantics id="S3.SS2.p6.2.m2.1a"><msub id="S3.SS2.p6.2.m2.1.1" xref="S3.SS2.p6.2.m2.1.1.cmml"><mi id="S3.SS2.p6.2.m2.1.1.2" xref="S3.SS2.p6.2.m2.1.1.2.cmml">y</mi><mi id="S3.SS2.p6.2.m2.1.1.3" xref="S3.SS2.p6.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.2.m2.1b"><apply id="S3.SS2.p6.2.m2.1.1.cmml" xref="S3.SS2.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.2.m2.1.1.1.cmml" xref="S3.SS2.p6.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p6.2.m2.1.1.2.cmml" xref="S3.SS2.p6.2.m2.1.1.2">𝑦</ci><ci id="S3.SS2.p6.2.m2.1.1.3.cmml" xref="S3.SS2.p6.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.2.m2.1c">y_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.2.m2.1d">italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> given the previous context <math alttext="y_{&lt;t}" class="ltx_Math" display="inline" id="S3.SS2.p6.3.m3.1"><semantics id="S3.SS2.p6.3.m3.1a"><msub id="S3.SS2.p6.3.m3.1.1" xref="S3.SS2.p6.3.m3.1.1.cmml"><mi id="S3.SS2.p6.3.m3.1.1.2" xref="S3.SS2.p6.3.m3.1.1.2.cmml">y</mi><mrow id="S3.SS2.p6.3.m3.1.1.3" xref="S3.SS2.p6.3.m3.1.1.3.cmml"><mi id="S3.SS2.p6.3.m3.1.1.3.2" xref="S3.SS2.p6.3.m3.1.1.3.2.cmml"></mi><mo id="S3.SS2.p6.3.m3.1.1.3.1" xref="S3.SS2.p6.3.m3.1.1.3.1.cmml">&lt;</mo><mi id="S3.SS2.p6.3.m3.1.1.3.3" xref="S3.SS2.p6.3.m3.1.1.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.3.m3.1b"><apply id="S3.SS2.p6.3.m3.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.3.m3.1.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p6.3.m3.1.1.2.cmml" xref="S3.SS2.p6.3.m3.1.1.2">𝑦</ci><apply id="S3.SS2.p6.3.m3.1.1.3.cmml" xref="S3.SS2.p6.3.m3.1.1.3"><lt id="S3.SS2.p6.3.m3.1.1.3.1.cmml" xref="S3.SS2.p6.3.m3.1.1.3.1"></lt><csymbol cd="latexml" id="S3.SS2.p6.3.m3.1.1.3.2.cmml" xref="S3.SS2.p6.3.m3.1.1.3.2">absent</csymbol><ci id="S3.SS2.p6.3.m3.1.1.3.3.cmml" xref="S3.SS2.p6.3.m3.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.3.m3.1c">y_{&lt;t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.3.m3.1d">italic_y start_POSTSUBSCRIPT &lt; italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. <math alttext="N" class="ltx_Math" display="inline" id="S3.SS2.p6.4.m4.1"><semantics id="S3.SS2.p6.4.m4.1a"><mi id="S3.SS2.p6.4.m4.1.1" xref="S3.SS2.p6.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.4.m4.1b"><ci id="S3.SS2.p6.4.m4.1.1.cmml" xref="S3.SS2.p6.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.4.m4.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.4.m4.1d">italic_N</annotation></semantics></math> denotes the sequence length.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Instruction-Aware Contextual Compression by Ranking</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Instruction-Aware contextual compression by ranking is a fairly straightforward process. First, an appropriate text-splitting strategy is employed, which can involve splitting based on specific character or by length, effectively converting the document into multiple chunks. Next, the model’s ranking capability is applied to score chunks with instruction, followed by sorting them. Chunks are then retained based on a specified percentage. Throughout the compression process, care is taken to maintain the order of the chunks.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Instruction-Aware Contextual Compression by Generation</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Compressing context by generation leverages the ability of Grad-CAM <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib27" title="">27</a>]</cite> to capture fine-grained relevance.
The calculation process of Grad-CAM can be summarized as follows: Firstly, perform a forward pass to obtain the final classification probabilities. Calculate the gradients of the token with respect to the target class. Average the gradients for each token and extract an attention map within a specific cross-attention layer. Grad-CAM scores can be obtained by multiplying the attention map and the gradient weight vector. These Grad-CAM scores can be considered as the contribution of each token to the classification result.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">There’s no need to pre-split the context; instead, the entire context is input into the model. Following that, k-step responses are generated based on the context and instruction. In the Instruction-Aware Contextual Compressor, attention maps and gradients are recorded for specific cross-attention layers, which are used to compute token-level Grad-CAM scores. These token-level Grad-CAM scores are then averaged to obtain chunk/sentence-level Grad-CAM scores. The chunks are subsequently sorted based on their Grad-CAM scores, and a specified percentage of the highest-scoring chunks is retained. Similarly, throughout the compression process, care is taken to maintain the order of the chunks.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Ensemble the two methods</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Effectively ensemble these two methods of Instruction-Aware contextual compression can yield better compression results. The magnitude difference between the ranking score and Grad-CAM score makes it challenging to determine a suitable weighting parameter for fusion. Therefore, in the end, we opted for a non-parametric approach. Specifically, we individually rank the two types of information and then use the average of the two rankings to compress the context.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In this study, we utilized two types of datasets. The first are ranking datasets, designed to empower the model with robust re-ranking capabilities. The second are generation datasets, intended to equip the model with generative abilities. The ranking datasets comprise 15 million samples, while the generation datasets consist of 1.63 million samples.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Ranking datasets</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1"><math alttext="\mathbf{T^{2}}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.1.m1.1"><semantics id="S4.SS1.SSS1.p1.1.m1.1a"><msup id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS1.p1.1.m1.1.1.2" xref="S4.SS1.SSS1.p1.1.m1.1.1.2.cmml">𝐓</mi><mn id="S4.SS1.SSS1.p1.1.m1.1.1.3" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.cmml">𝟐</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.1b"><apply id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1">superscript</csymbol><ci id="S4.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.2">𝐓</ci><cn id="S4.SS1.SSS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.SSS1.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.1c">\mathbf{T^{2}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.1.m1.1d">bold_T start_POSTSUPERSCRIPT bold_2 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.1">Ranking <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib38" title="">38</a>]</cite></span> is a large-scale Chinese passage ranking dataset published in April 2023, which comprises 307K queries and 2.3M unique passages from real-world search engines. To constructing more accurate ranking algorithms, each query-passage pair has 4-level fine-grained annotations. For the retrieval task, it classifies Level-2 and Level-3 passages as relevant passages, while categorizing all remaining passages as irrelevant.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.1">M3E Dataset</span> <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib35" title="">35</a>]</cite> comprises a total of 22M sentence pair samples from a diverse range of topics, including Chinese encyclopedia, finance, healthcare, law, news, academia, etc.
This dataset mainly consists of datasets used for other tasks, among which over 3M of data is instruction fine-tuning data, while some datasets comes from tasks such as Q&amp;A, parallel semantics, machine reading comprehension, corpus, NL2SQL, text classification, text summarization, natural language processing, etc.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Generation Datasets</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.1">Dureader Dataset <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib9" title="">9</a>]</cite></span> is a extensive open-domain Chinese machine reading comprehension dataset, encompassing 200,000 questions, 420,000 answers, and 1 million documents. The questions and documents are sourced from Baidu Search and Baidu Zhidao, while the answers are manually crafted. In this study, we only use its robust subset, which contains 14,500 samples for training and 1.42k samples for validation.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p2.1.1">WikiQA-LongForm Dataset</span> is a long-form open-domain question answering dataset based on Wikipedia entries. We employed a heuristic approach and our proprietary NLU model to filter out lower-quality and sensitive or controversial entries, retaining 254,547 high-quality entries. These entries were transformed into multi-turn dialogue data using ChatGPT, resulting in high-quality Long Form QA data after further heuristic filtering. The WikiQA-LongForm Dataset is a contribution of this study and is publicly available at <a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/howard-hou/WikiQA-LongForm" title="">WikiQA-LongForm</a> for use in other research projects.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Large Language Models</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">During our experimentation, we conducted tests using Instruction-Aware Contextual Compressor on ChatGPT, which is based on the GPT-3.5-turbo-0613 architecture. ChatGPT represents an Instruct-tuned language model that has undergone further enhancement through Reinforcement Learning from Human Feedback (RLHF) and boasts an impressive 175 billion parameters. The foundational language model of ChatGPT appears to be code-davinci-0022, and previously, davinci, as outlined in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib5" title="">5</a>]</cite>. Our objective was to compare the performance of ChatGPT with and without the application of Instruction-Aware Contextual Compressor to gain insights into its impact on the model’s efficiency and accuracy.
The settings of ChatGPT are all set to their default values, except for the <math alttext="top\_p" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">t</mi><mo id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">o</mi><mo id="S4.SS2.p1.1.m1.1.1.1a" xref="S4.SS2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.p1.1.m1.1.1.4" xref="S4.SS2.p1.1.m1.1.1.4.cmml">p</mi><mo id="S4.SS2.p1.1.m1.1.1.1b" xref="S4.SS2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.p1.1.m1.1.1.5" mathvariant="normal" xref="S4.SS2.p1.1.m1.1.1.5.cmml">_</mi><mo id="S4.SS2.p1.1.m1.1.1.1c" xref="S4.SS2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.p1.1.m1.1.1.6" xref="S4.SS2.p1.1.m1.1.1.6.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝑡</ci><ci id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">𝑜</ci><ci id="S4.SS2.p1.1.m1.1.1.4.cmml" xref="S4.SS2.p1.1.m1.1.1.4">𝑝</ci><ci id="S4.SS2.p1.1.m1.1.1.5.cmml" xref="S4.SS2.p1.1.m1.1.1.5">_</ci><ci id="S4.SS2.p1.1.m1.1.1.6.cmml" xref="S4.SS2.p1.1.m1.1.1.6">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">top\_p</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_t italic_o italic_p _ italic_p</annotation></semantics></math> parameter, which is set to 0.1. This adjustment is made to reduce the impact of randomness on the evaluation results.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Experimental Settings</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We conduct a comparative analysis to assess the effectiveness of Instruction-Aware Contextual Compressor and analyze the associated trade-offs.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">RAG Baseline</span>: The table <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S4.T1" title="Table 1 ‣ 4.3 Experimental Settings ‣ 4 Experiments ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_tag">1</span></a> displays several RAG baselines, including scenarios where RAG is not used, direct feeding of the correct context to the large language model, the scenario where only a retriever is used in pipeline, and the scenario where a retriever is used in combination with Instruction-Aware Contextual Compressor for re-ranking.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The LLM performance in different scenarios</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.2">Rouge-1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.3">Rouge-2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.4">Rouge-L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.5">Recall@1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.1.1">Ground Truth</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.2">0.683</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.3">0.539</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.4">0.631</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.5">1.0</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.3.2.1">Recalled Top1</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.2">0.656</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.3">0.507</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.4">0.605</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.5">0.86</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.4.3.1">Recall + Rerank Top1</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.4.3.2">0.675</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.4.3.3">0.529</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.4.3.4">0.623</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.4.3.5">0.962</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">Compression Baseline</span>: Our evaluation involves a comparison between Instruction-Aware Contextual Compressor and Selective Context <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib19" title="">19</a>]</cite>, which employs a basic approach to filter out an equivalent amount of data based on self-information. Selective Context utilizes the GPT-2 model with 124 million parameters.
Some readers may doubt whether the GPT-2 124M model is too small to be considered a sufficiently robust baseline for comparison.
To this end, we used Baichuan-7B with 7 billion parameters to run the Selective Context, which has parameters 56 times larger than GPT-2.
The results at a retention ratio of 0.5 are presented in the table below:</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of ROUGE-L scores for the Selective Context method using Baichuan-7B and GPT-2 models.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.2">Parameters</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.3">R (rouge-l)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.4">P (rouge-l)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.5">F (rouge-l)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.2.1.1">Baichuan-7B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.2">7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.3">0.5448</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.4">0.5419</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.5">0.5024</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T2.1.3.2.1">GPT-2</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.3.2.2">124M</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.3.2.3">0.5758</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.3.2.4">0.5506</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.3.2.5">0.5205</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">We found that Baichuan-7B did not perform better than GPT-2.
This also indicates that the selective context method is not scalable.
This therefore indicates that the baseline we used is sufficiently robust.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p5.1.1">Retention Ratios</span>: In our experiments, we explore various content retention ratios: 0.2, 0.35, 0.5, 0.65, and 0.8. These ratios determine the proportion of content to be retained. This exploration allows us to examine the trade-off between efficiency and performance as the amount of retained information varies.</p>
</div>
<div class="ltx_para" id="S4.SS3.p6">
<p class="ltx_p" id="S4.SS3.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p6.1.1">Setting for Inference Measure:</span> To measure the inference acceleration and memory savings brought about by context compression, we conducted practical measurements using an NVIDIA 4090 GPU with 24GB of VRAM. The LLM model used was Baichuan-7B <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#bib.bib40" title="">40</a>]</cite>, and the data format employed was bfloat16.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussions</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Comparison to Original Context</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We initially compare the performance of Instruction-Aware Contextual Compressor with varying context retention ratios to the reranked original context, which utilizes the original context after reranking but no compression at all. All results are shown in table <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S5.T3" title="Table 3 ‣ 5.1 Comparison to Original Context ‣ 5 Results and Discussions ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_tag">3</span></a>, and the ”diff” column represents the difference in performance compared to uncompressed text.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">As shown in the table <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S5.T3" title="Table 3 ‣ 5.1 Comparison to Original Context ‣ 5 Results and Discussions ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_tag">3</span></a>, at retention rate of 0.8, the performance loss is minimal, with Rouge-1 showing only a marginal decrease in the range of 0.003 to 0.008. This demonstrates a high level of consistency between answers provided in compressed contexts and those in original contexts. Surprisingly, the Rouge-2 and Rouge-L score with the generation method is even higher than the original text, which was unexpected. This indicates that our method successfully filtered unrelated or even noisy content, improving the LLM’s performance.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">As the retention ratio decreases, the effectiveness of all methods declines, which is expected since there is less valuable information provided to the LLM. Overall, the generation method outperforms the ranking method, with a slower rate of performance decline from 0.8 to 0.35 compared to the ranking method. However, it was unexpected that at a retention ratio of 0.2, there was a sudden significant drop in performance, indicating a rapid loss of effectiveness for the generation method at low retention ratios.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">In traditional machine learning, ensemble learning is widely regarded as a robust and effective method for improving model performance. Therefore, we propose using the ”average rank” to combine the generation method and the ranking method. This involves taking the average of the ranks assigned by the generation method and the ranking method, resulting in a new ranking score for a given text. Overall, the ”average rank” method outperforms both the generation and ranking methods. It shows significant improvement from 0.2 to 0.65 retention ratios, with fewer losses compared to the original context.
At a retention ratio of 0.8, while it may not outperform the generation method, it still surpasses the ranking method.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparing Instruction-Aware Contextual Compressor with different context retention ratio to the original context</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.1" style="width:433.6pt;height:306pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.00001900258928,1.00001900258928) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T3.1.1.1.1.1">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.1.2">Rouge-1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.1.3">Rouge-2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.1.4">Rouge-L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.1.5">Rouge-1 Diff</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.1.6">Rouge-2 Diff</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.1.7">Rouge-L Diff</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.1.2.1.1">origin</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.2.1.2">0.675</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.2.1.3">0.529</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.2.1.4">0.623</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.2.1.5">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.2.1.6">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.2.1.7">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.1.3.2.1">ranking-0.8</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.3.2.2">0.667</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.3.2.3">0.522</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.3.2.4">0.617</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.3.2.5">-0.008</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.3.2.6">-0.007</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.3.2.7">-0.006</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.1.4.3.1">ranking-0.65</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.3.2">0.643</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.3.3">0.493</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.3.4">0.594</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.3.5">-0.032</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.3.6">-0.036</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.3.7">-0.029</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.1.5.4.1">ranking-0.5</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.5.4.2">0.633</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.5.4.3">0.481</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.5.4.4">0.584</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.5.4.5">-0.042</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.5.4.6">-0.048</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.5.4.7">-0.039</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.1.6.5.1">ranking-0.35</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.6.5.2">0.608</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.6.5.3">0.454</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.6.5.4">0.56</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.6.5.5">-0.067</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.6.5.6">-0.075</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.6.5.7">-0.063</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.1.7.6.1">ranking-0.2</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.7.6.2">0.587</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.7.6.3">0.429</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.7.6.4">0.539</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.7.6.5">-0.088</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.7.6.6">-0.1</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.7.6.7">-0.084</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.1.8.7.1">generation-0.8</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.8.7.2">0.672</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.8.7.3">0.53</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.8.7.4">0.624</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.8.7.5">-0.003</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.8.7.6">0.001</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.8.7.7">0.001</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.1.9.8.1">generation-0.65</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.9.8.2">0.66</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.9.8.3">0.515</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.9.8.4">0.611</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.9.8.5">-0.015</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.9.8.6">-0.014</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.9.8.7">-0.012</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.1.10.9.1">generation-0.5</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.10.9.2">0.642</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.10.9.3">0.495</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.10.9.4">0.596</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.10.9.5">-0.033</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.10.9.6">-0.034</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.10.9.7">-0.027</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.1.11.10.1">generation-0.35</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.11.10.2">0.62</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.11.10.3">0.471</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.11.10.4">0.574</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.11.10.5">-0.055</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.11.10.6">-0.058</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.11.10.7">-0.049</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.1.12.11.1">generation-0.2</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.12.11.2">0.559</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.12.11.3">0.4</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.12.11.4">0.509</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.12.11.5">-0.116</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.12.11.6">-0.129</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.12.11.7">-0.114</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.1.13.12.1">ensembled-0.8</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.13.12.2">0.669</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.13.12.3">0.523</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.13.12.4">0.619</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.13.12.5">-0.006</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.13.12.6">-0.006</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.13.12.7">-0.004</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.1.14.13.1">ensembled-0.65</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.14.13.2">0.66</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.14.13.3">0.515</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.14.13.4">0.611</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.14.13.5">-0.015</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.14.13.6">-0.014</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.14.13.7">-0.012</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.1.15.14.1">ensembled-0.5</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.15.14.2">0.649</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.15.14.3">0.504</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.15.14.4">0.601</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.15.14.5">-0.026</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.15.14.6">-0.025</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.15.14.7">-0.022</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.1.16.15.1">ensembled-0.35</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.16.15.2">0.628</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.16.15.3">0.479</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.16.15.4">0.582</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.16.15.5">-0.047</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.16.15.6">-0.05</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.16.15.7">-0.041</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T3.1.1.17.16.1">ensembled-0.2</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.1.17.16.2">0.599</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.1.17.16.3">0.442</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.1.17.16.4">0.553</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.1.17.16.5">-0.076</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.1.17.16.6">-0.087</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.1.17.16.7">-0.07</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Comparison to Baseline</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">In this section, we compare our method to Selective Context baseline, and the results are presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S5.F4" title="Figure 4 ‣ 5.2 Comparison to Baseline ‣ 5 Results and Discussions ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_tag">4</span></a>.
Selective Context is a context compression method based on text self-information and represents the state-of-the-art as of the time of writing this paper. Comparing our method to Selective Context, which serves as a baseline, can effectively demonstrate the validity of our approach.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S5.F4" title="Figure 4 ‣ 5.2 Comparison to Baseline ‣ 5 Results and Discussions ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_tag">4</span></a>, our proposed method, Instruction-Aware Contextual Compressor, is even more effective compared to Selective Context. Both methods, whether purely ranking-based or generation-based, outperform Selective Context, and the lead becomes more significant as the retention ratio decreases. This indicates that our proposed method excels in selecting more informative content even when only a limited amount of information can be retained.</p>
</div>
<figure class="ltx_figure" id="S5.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="218" id="S5.F4.g1" src="extracted/5817813/retention-vs-rouge1.png" width="359"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="221" id="S5.F4.g2" src="extracted/5817813/retention-vs-rouge2.png" width="359"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="221" id="S5.F4.g3" src="extracted/5817813/retention-vs-rougel.png" width="359"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Performance of Instruction-Aware Contextual Compression compared to the Selective Context baseline</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>The Impact of Generation Steps</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">For context compression using generative information, intuitively, if the number of generation steps is too few, it might not have generated a complete response. Consequently, the effectiveness at this stage may be suboptimal. As the number of generation steps increases, the effectiveness of compression is expected to improve. To explore this, we conducted experiments with a fixed retention ratio of 0.5, testing a series of generation step values ranging from 4 to 64. The results, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S5.F5" title="Figure 5 ‣ 5.3 The Impact of Generation Steps ‣ 5 Results and Discussions ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_tag">5</span></a>, indeed demonstrate that for generation-based context compression, the performance gradually improves with an increase in the number of steps. However, after reaching 32 steps, it reaches a plateau, indicating a diminishing marginal return with further increases in the number of generation steps.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="303" id="S5.F5.g1" src="extracted/5817813/generation_step-vs-performance.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The Impact of Generation Steps on Context Compression Effectiveness</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Speed Up and Memory Saving</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">We also measured the impact of context compression on the Large Language Model (LLM). As demonstrated in the table <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S5.T4" title="Table 4 ‣ 5.4 Speed Up and Memory Saving ‣ 5 Results and Discussions ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_tag">4</span></a>, when the retention ratio is set to 0.5, the inference speed per token increases by a factor of 2.2, while memory usage decreases by 5.05%.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Speed up after context compression</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.1">Retention Ratio</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.2">Ranking</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.3">Generation</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.4">Ensemble</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.1">0.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.2">1.38</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.3">1.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.4">0.94</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3.2">
<td class="ltx_td ltx_align_center" id="S5.T4.1.3.2.1">0.65</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.3.2.2">1.73</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.3.2.3">1.17</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.3.2.4">1.08</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.4.3">
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.1">0.5</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.2">2.05</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.3">1.31</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.4">1.20</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.5.4">
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.1">0.35</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.2">2.41</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.3">1.45</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.4">1.32</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.6.5.1">0.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.6.5.2">2.81</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.6.5.3">1.58</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.6.5.4">1.43</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Re-ranking Performance</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">The ability to filter out irrelevant documents through re-ranking is also a crucial capability of Instruction-Aware Contextual Compression. Following a hierarchical design approach, a large number of initially retrieved documents are first re-ordered using Instruction-Aware Contextual Compressor to select the top-k documents, which are then further compressed. Therefore, we present the retrieval performance of Instruction-Aware Contextual Compressor on extensive datasets, measured by Recall@1, 5, and 10, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.15491v1#S5.T5" title="Table 5 ‣ 5.5 Re-ranking Performance ‣ 5 Results and Discussions ‣ Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Re-ranking Performance of Instruction-Aware Contextual Compressor</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T5.1.2.1.1">Dataset</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.2.1.2">Recall@1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.2.1.3">Recall@5</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.2.1.4">Recall@10</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.3.1.1">wikipedia-cn-20230720-dataset</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.1.2">0.975</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.1.3">0.998</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T5.1.3.1.4">0.999</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.4.2.1">wiki_atomic_edits</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.2.2">0.968</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.2.3">0.997</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.4.2.4">0.999</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.5.3.1">alpaca_gpt4</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.3.2">0.789</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.3.3">0.933</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.5.3.4">0.971</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.6.4.1">bq</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.4.2">0.718</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.4.3">0.833</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.6.4.4">0.903</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.7.5.1">firefly</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.7.5.2">0.632</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.7.5.3">0.849</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.7.5.4">0.947</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.8.6.1">webqa</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.8.6.2">0.667</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.8.6.3">0.913</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.8.6.4">0.974</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.9.7.1">dureader_dataset</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.7.2">0.913</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.7.3">0.988</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.9.7.4">0.996</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.10.8.1">cmrc2018</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.10.8.2">0.972</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.10.8.3">0.986</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.10.8.4">0.993</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.11.9.1">csl</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.11.9.2">0.846</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.11.9.3">0.961</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.11.9.4">0.986</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.12.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.12.10.1">pawsx</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.12.10.2">0.606</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.12.10.3">0.996</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.12.10.4">1.000</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.13.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.13.11.1">dureader_robust</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.13.11.2">0.627</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.13.11.3">0.859</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.13.11.4">0.921</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.1.1">
<math alttext="\rm T^{2}" class="ltx_Math" display="inline" id="S5.T5.1.1.1.m1.1"><semantics id="S5.T5.1.1.1.m1.1a"><msup id="S5.T5.1.1.1.m1.1.1" xref="S5.T5.1.1.1.m1.1.1.cmml"><mi id="S5.T5.1.1.1.m1.1.1.2" mathvariant="normal" xref="S5.T5.1.1.1.m1.1.1.2.cmml">T</mi><mn id="S5.T5.1.1.1.m1.1.1.3" xref="S5.T5.1.1.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.m1.1b"><apply id="S5.T5.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T5.1.1.1.m1.1.1.1.cmml" xref="S5.T5.1.1.1.m1.1.1">superscript</csymbol><ci id="S5.T5.1.1.1.m1.1.1.2.cmml" xref="S5.T5.1.1.1.m1.1.1.2">T</ci><cn id="S5.T5.1.1.1.m1.1.1.3.cmml" type="integer" xref="S5.T5.1.1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.m1.1c">\rm T^{2}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.1.1.1.m1.1d">roman_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>Ranking_train_dataset</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.2">0.405</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3">0.756</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.1.4">0.901</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.14.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.14.12.1">tiracl</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.14.12.2">0.727</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.14.12.3">0.891</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.14.12.4">0.956</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.15.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.15.13.1">belle_2m</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.15.13.2">0.749</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.15.13.3">0.921</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.15.13.4">0.972</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.16.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.16.14.1">mlqa</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.16.14.2">0.718</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.16.14.3">0.908</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.16.14.4">0.960</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.17.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.17.15.1">lcqmc</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.17.15.2">0.435</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.17.15.3">0.876</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.17.15.4">0.960</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.18.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.18.16.1">hc3_chinese</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.18.16.2">0.270</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.18.16.3">0.751</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.18.16.4">0.897</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.19.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.19.17.1">zhihu_kol</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.19.17.2">0.238</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.19.17.3">0.557</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.19.17.4">0.787</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.20.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.20.18.1">xlsum</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.20.18.2">0.395</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.20.18.3">0.833</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.20.18.4">0.933</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.21.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.21.19.1">ocnli</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.21.19.2">0.371</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.21.19.3">0.876</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.1.21.19.4">0.961</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.22.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T5.1.22.20.1">chatmed_consult</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.22.20.2">0.594</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.22.20.3">0.803</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T5.1.22.20.4">0.899</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we introduced Instruction-Aware Contextual Compression to filter out less relevant content, providing a more concise and efficient context representation for LLMs, all without compromising their performance.
An important discovery we made is that generation-based Instruction-Aware Contextual Compression is more effective than ranking-based Instruction-Aware Contextual Compression methods. With generation-based Instruction-Aware Contextual Compression, using only 1/10 of the data, the results can surpass those of the Instruction-Aware Contextual Compression method.
With only 50% of the context retained, we achieved a 2.2x inference speedup for the LLM and saved 5% of GPU VRAM, while the Rouge-1 metric only dropped by 0.047.
According to our evaluations, the results show that Instruction-Aware Contextual Compressor significantly improves the efficiency of LLMs and serves as a valuable component in the Retrieval-augmented Generation pipeline.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Data availability and access</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">The code and data used in this project are open-sourced and available at <a class="ltx_ref ltx_href" href="https://github.com/howard-hou/instruction-aware-contextual-compressor" title="">https://github.com/howard-hou/instruction-aware-contextual-compressor</a>. The provided information is sufficient to reproduce the results of this paper. Additional data supporting the findings of this study can be obtained from the corresponding author upon reasonable request.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.1.1">
<span class="ltx_bibblock"><span class="ltx_ERROR undefined" id="bib.1.1.1.1">\bibcommenthead</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ainslie et al [2023]</span>
<span class="ltx_bibblock">
Ainslie J, Lei T, de Jong M, et al (2023) Colt5: Faster long-range transformers with conditional computation. ArXiv preprint abs/2303.09752. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.09752" title="">https://arxiv.org/abs/2303.09752</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arefeen et al [2023]</span>
<span class="ltx_bibblock">
Arefeen MA, Debnath B, Chakradhar S (2023) Leancontext: Cost-efficient domain-specific question answering using llms. ArXiv preprint abs/2309.00841. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2309.00841" title="">https://arxiv.org/abs/2309.00841</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beltagy et al [2020]</span>
<span class="ltx_bibblock">
Beltagy I, Peters ME, Cohan A (2020) Longformer: The long-document transformer. ArXiv preprint abs/2004.05150. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2004.05150" title="">https://arxiv.org/abs/2004.05150</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et al [2022]</span>
<span class="ltx_bibblock">
Borgeaud S, Mensch A, Hoffmann J, et al (2022) Improving language models by retrieving from trillions of tokens. In: ICML

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al [2020]</span>
<span class="ltx_bibblock">
Brown T, Mann B, Ryder N, et al (2020) Language models are few-shot learners. NeurIPS

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al [2023]</span>
<span class="ltx_bibblock">
Chung HW, Constant N, García X, et al (2023) Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. ArXiv abs/2304.09151. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:258187051" title="">https://api.semanticscholar.org/CorpusID:258187051</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al [2021]</span>
<span class="ltx_bibblock">
Gao T, Fisch A, Chen D (2021) Making pre-trained language models better few-shot learners. In: Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL-IJCNLP 2021, Association for Computational Linguistics (ACL), pp 3816–3830

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et al [2020]</span>
<span class="ltx_bibblock">
Guu K, Lee K, Tung Z, et al (2020) REALM: Retrieval augmented language model pre-training. In: ICML

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al [2017]</span>
<span class="ltx_bibblock">
He W, Liu K, Liu J, et al (2017) Dureader: a chinese machine reading comprehension dataset from real-world applications. ArXiv abs/1711.05073. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:3662564" title="">https://api.semanticscholar.org/CorpusID:3662564</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honovich et al [2022]</span>
<span class="ltx_bibblock">
Honovich O, Shaham U, Bowman SR, et al (2022) Instruction induction: From few examples to natural language task descriptions. ArXiv preprint abs/2205.10782. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2205.10782" title="">https://arxiv.org/abs/2205.10782</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al [2023]</span>
<span class="ltx_bibblock">
Huang J, Ping W, Xu P, et al (2023) Raven: In-context learning with retrieval augmented encoder-decoder language models. arXiv preprint arXiv:230807922

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard and Grave [2021]</span>
<span class="ltx_bibblock">
Izacard G, Grave É (2021) Leveraging passage retrieval with generative models for open domain question answering. In: EACL

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et al [2022]</span>
<span class="ltx_bibblock">
Izacard G, Lewis P, Lomeli M, et al (2022) Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:220803299

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al [2022]</span>
<span class="ltx_bibblock">
Ji Z, Lee N, Frieske R, et al (2022) Survey of hallucination in natural language generation. ACM Computing Surveys 55:1 – 38. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:246652372" title="">https://api.semanticscholar.org/CorpusID:246652372</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al [2022]</span>
<span class="ltx_bibblock">
Jiang Z, Gao L, Araki J, et al (2022) Retrieval as attention: End-to-end learning of retrieval and reading within a single transformer. In: EMNLP

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al [2020]</span>
<span class="ltx_bibblock">
Karpukhin V, Oguz B, Min S, et al (2020) Dense passage retrieval for open-domain question answering. In: EMNLP

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal et al [2019]</span>
<span class="ltx_bibblock">
Khandelwal U, Levy O, Jurafsky D, et al (2019) Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:191100172

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al [2020]</span>
<span class="ltx_bibblock">
Lewis P, Perez E, Piktus A, et al (2020) Retrieval-augmented generation for knowledge-intensive nlp tasks. NeurIPS

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al [2023]</span>
<span class="ltx_bibblock">
Li Y, Dong B, Lin C, et al (2023) Compressing context to enhance inference efficiency of large language models. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:263830231" title="">https://api.semanticscholar.org/CorpusID:263830231</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al [2023]</span>
<span class="ltx_bibblock">
Lin SC, Asai A, Li M, et al (2023) How to train your dragon: Diverse augmentation towards generalizable dense retrieval. arXiv preprint arXiv:230207452

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al [2022]</span>
<span class="ltx_bibblock">
Liu J, Shen D, Zhang Y, et al (2022) What makes good in-context examples for GPT-3? In: Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. Association for Computational Linguistics, Dublin, Ireland and Online, pp 100–114, <a class="ltx_ref" href="https:/doi.org/10.18653/v1/2022.deelio-1.10" title="">10.18653/v1/2022.deelio-1.10</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.deelio-1.10" title="">https://aclanthology.org/2022.deelio-1.10</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al [2022]</span>
<span class="ltx_bibblock">
Lu Y, Bartolo M, Moore A, et al (2022) Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Dublin, Ireland, pp 8086–8098, <a class="ltx_ref" href="https:/doi.org/10.18653/v1/2022.acl-long.556" title="">10.18653/v1/2022.acl-long.556</a>, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.acl-long.556" title="">https://aclanthology.org/2022.acl-long.556</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mu et al [2023]</span>
<span class="ltx_bibblock">
Mu J, Li XL, Goodman N (2023) Learning to compress prompts with gist tokens. ArXiv preprint abs/2304.08467. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2304.08467" title="">https://arxiv.org/abs/2304.08467</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano et al [2021]</span>
<span class="ltx_bibblock">
Nakano R, Hilton J, Balaji S, et al (2021) WebGPT: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:211209332

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oppenlaender [2022]</span>
<span class="ltx_bibblock">
Oppenlaender J (2022) Prompt engineering for text-based generative art. arXiv preprint arXiv:220413988 abs/2204.13988. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.13988" title="">https://arxiv.org/abs/2204.13988</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ratner et al [2023]</span>
<span class="ltx_bibblock">
Ratner N, Levine Y, Belinkov Y, et al (2023) Parallel context windows for large language models. In: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp 6383–6402

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Selvaraju et al [2016]</span>
<span class="ltx_bibblock">
Selvaraju RR, Das A, Vedantam R, et al (2016) Grad-cam: Visual explanations from deep networks via gradient-based localization. International Journal of Computer Vision 128:336–359. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:15019293" title="">https://api.semanticscholar.org/CorpusID:15019293</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al [2023]</span>
<span class="ltx_bibblock">
Shi W, Min S, Yasunaga M, et al (2023) RePlug: Retrieval-augmented black-box language models. arXiv preprint arXiv:230112652

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shuster et al [2021]</span>
<span class="ltx_bibblock">
Shuster K, Poff S, Chen M, et al (2021) Retrieval augmentation reduces hallucination in conversation. In: Conference on Empirical Methods in Natural Language Processing, URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:233240939" title="">https://api.semanticscholar.org/CorpusID:233240939</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al [2023a]</span>
<span class="ltx_bibblock">
Touvron H, Lavril T, Izacard G, et al (2023a) Llama: Open and efficient foundation language models. arXiv preprint arXiv:230213971

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al [2023b]</span>
<span class="ltx_bibblock">
Touvron H, Martin L, Stone K, et al (2023b) Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:230709288

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al [2023a]</span>
<span class="ltx_bibblock">
Wang B, Ping W, Xu P, et al (2023a) Shall we pretrain autoregressive language models with retrieval? a comprehensive study. arXiv preprint arXiv:230406762

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al [2023b]</span>
<span class="ltx_bibblock">
Wang J, Liu Z, Zhao L, et al (2023b) Review of large vision models and visual prompt engineering. arXiv preprint arXiv:230700855 abs/2307.00855. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.00855" title="">https://arxiv.org/abs/2307.00855</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al [2022]</span>
<span class="ltx_bibblock">
Wang L, Yang N, Huang X, et al (2022) Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:221203533

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang Yuxin [2023]</span>
<span class="ltx_bibblock">
Wang Yuxin HsSun Qingxuan (2023) M3e: Moka massive mixed embedding model

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al [2022]</span>
<span class="ltx_bibblock">
Wei J, Wang X, Schuurmans D, et al (2022) Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of Neural Information Processing Systems 35:24824–24837

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">White et al [2023]</span>
<span class="ltx_bibblock">
White J, Fu Q, Hays S, et al (2023) A prompt pattern catalog to enhance prompt engineering with chatgpt. ArXiv preprint abs/2302.11382. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2302.11382" title="">https://arxiv.org/abs/2302.11382</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al [2023]</span>
<span class="ltx_bibblock">
Xie X, Dong Q, Wang B, et al (2023) T2ranking: A large-scale chinese benchmark for passage ranking. Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:258041275" title="">https://api.semanticscholar.org/CorpusID:258041275</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al [2023]</span>
<span class="ltx_bibblock">
Xu P, Ping W, Wu X, et al (2023) Retrieval meets long context large language models. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:263620134" title="">https://api.semanticscholar.org/CorpusID:263620134</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al [2023]</span>
<span class="ltx_bibblock">
Yang AM, Xiao B, Wang B, et al (2023) Baichuan 2: Open large-scale language models. ArXiv abs/2309.10305. URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:261951743" title="">https://api.semanticscholar.org/CorpusID:261951743</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yogatama et al [2021]</span>
<span class="ltx_bibblock">
Yogatama D, de Masson d’Autume C, Kong L (2021) Adaptive semiparametric language models. Transactions of the Association for Computational Linguistics 9:362–373

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al [2022]</span>
<span class="ltx_bibblock">
Zhou Y, Muresanu AI, Han Z, et al (2022) Large language models are human-level prompt engineers. In: The Eleventh International Conference on Learning Representations

</span>
</li>
</ul>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">This project was supported by the Industrial Metaverse project of Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), with the project number 000015; it was also supported by the Guangming District Government GPT Service project, with the project number 23210016.</p>
</div>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Author information</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">Haowen Hou and Fei Ma are contributed equally to this work.</p>
</div>
<section class="ltx_subsection" id="Sx3.SSx1">
<h3 class="ltx_title ltx_title_subsection">Authors and Affiliations</h3>
<div class="ltx_para" id="Sx3.SSx1.p1">
<p class="ltx_p" id="Sx3.SSx1.p1.1">Guangdong Laboratory of Artificial Intelligence and Digital Economy(SZ), Yutang, Shenzhen, 518000, Guangdong, China.</p>
</div>
<div class="ltx_para" id="Sx3.SSx1.p2">
<p class="ltx_p" id="Sx3.SSx1.p2.1">Haowen Hou, Fei Ma, Binwen Bai, Xinxin Zhu &amp; Fei Yu</p>
</div>
</section>
<section class="ltx_subsection" id="Sx3.SSx2">
<h3 class="ltx_title ltx_title_subsection">Contributions</h3>
<div class="ltx_para" id="Sx3.SSx2.p1">
<p class="ltx_p" id="Sx3.SSx2.p1.1">All authors have contributed to the ideas and design of this research.
Haowen Hou was in charge of developing and training the model, and also wrote this manuscript.
Fei Ma carried out the experiments and the analysis of the outcomes.
Binwen Bai and Xinxin Zhu managed the data collection, arrangement, cleansing, and its ultimate transformation into the training format.
Fei Yu offered feedback on the preliminary drafts of the manuscript.
All authors have reviewed and approved the final manuscript.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx3.SSx3">
<h3 class="ltx_title ltx_title_subsection">Corresponding author</h3>
<div class="ltx_para" id="Sx3.SSx3.p1">
<p class="ltx_p" id="Sx3.SSx3.p1.1">Correspondence to Haowen Hou.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx4">
<h2 class="ltx_title ltx_title_section">Ethics declarations</h2>
<section class="ltx_subsection" id="Sx4.SSx1">
<h3 class="ltx_title ltx_title_subsection">Competing interests</h3>
<div class="ltx_para" id="Sx4.SSx1.p1">
<p class="ltx_p" id="Sx4.SSx1.p1.1">The authors confirm that there are no financial conflicts or personal connections that might be perceived as affecting the findings presented in this study.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx4.SSx2">
<h3 class="ltx_title ltx_title_subsection">Ethical and informed consent for data used</h3>
<div class="ltx_para" id="Sx4.SSx2.p1">
<p class="ltx_p" id="Sx4.SSx2.p1.1">There are no concerns regarding the ethical use of the data or the informed consent process. All necessary ethical guidelines and protocols have been followed, and informed consent was obtained from all participants involved in the study.</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Aug 28 02:26:09 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
