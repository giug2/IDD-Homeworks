<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2305.17006] Zero-shot Visual Question Answering with Language Model Feedback</title><meta property="og:description" content="In this paper, we propose a novel language model guided captioning approach, Lamoc, for knowledge-based visual question answeringÂ (VQA). Our approach employs the generated captions by a captioning model as the context â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Zero-shot Visual Question Answering with Language Model Feedback">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Zero-shot Visual Question Answering with Language Model Feedback">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2305.17006">

<!--Generated on Thu Feb 29 05:11:37 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Zero-shot Visual Question Answering with Language Model Feedback</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Yifan Du<sup id="id1.1.id1" class="ltx_sup">1,4</sup>,
Junyi Li<sup id="id2.2.id2" class="ltx_sup">1,3</sup>,
Tianyi Tang<sup id="id3.3.id3" class="ltx_sup">1</sup>,
<span id="id4.4.id4" class="ltx_text ltx_font_bold">Wayne Xin Zhao<sup id="id4.4.id4.1" class="ltx_sup"><span id="id4.4.id4.1.1" class="ltx_text ltx_font_medium">1,4 ğŸ–‚</span></sup></span>
</span><span class="ltx_author_notes">ğŸ–‚Â Corresponding author.</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id5.1.id1" class="ltx_text ltx_font_bold">Ji-Rong Wen<sup id="id5.1.id1.1" class="ltx_sup"><span id="id5.1.id1.1.1" class="ltx_text ltx_font_medium">1, 2,4</span></sup></span> 
<br class="ltx_break"><sup id="id6.2.id2" class="ltx_sup">1</sup>Gaoling School of Artificial Intelligence, Renmin University of China 
<br class="ltx_break"><sup id="id7.3.id3" class="ltx_sup">2</sup>School of Information, Renmin University of China 
<br class="ltx_break"><sup id="id8.4.id4" class="ltx_sup">3</sup>DIRO, UniversitÃ© de MontrÃ©al 
<br class="ltx_break"><sup id="id9.5.id5" class="ltx_sup">4</sup>Beijing Key Laboratory of Big Data Management and Analysis Methods
<br class="ltx_break"><span id="id10.6.id6" class="ltx_text ltx_font_typewriter">{yifandu1999,
batmanfly}@gmail.com
<br class="ltx_break">lijunyi@ruc.edu.cn</span>,
<span id="id11.7.id7" class="ltx_text ltx_font_typewriter">steventianyitang@outlook.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id12.id1" class="ltx_p">In this paper, we propose a novel language model guided captioning approach, <span id="id12.id1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Lamoc</span>, for knowledge-based visual question answeringÂ (VQA). Our approach employs the generated captions by a captioning model as the context of an answer prediction model, which is a Pre-trained Language modelÂ (PLM).
As the major contribution, we leverage the guidance and feedback of the prediction model to improve the capability of the captioning model. In this way, the captioning model can become aware of the task goal and information need from the PLM.
To develop our approach, we design two specific training stages, where the first stage adapts the captioning model to the prediction model (selecting more suitable caption propositions for training) and the second stage tunes the captioning model according to the task goal (learning from feedback of the PLM).
Extensive experiments demonstrate the effectiveness of the proposed approach on the knowledge-based VQA task. Specifically, on the challenging A-OKVQA dataset, <span id="id12.id1.2" class="ltx_text ltx_font_smallcaps">Lamoc</span> outperforms several competitive zero-shot methods and even achieves comparable results to a fine-tuned VLP model. Our code is publicly available atÂ <a target="_blank" href="https://github.com/RUCAIBox/LAMOC" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/RUCAIBox/LAMOC</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recently, pre-trained language modelsÂ (PLMs)Â <cite class="ltx_cite ltx_citemacro_citep">(Devlin etÂ al., <a href="#bib.bib8" title="" class="ltx_ref">2019</a>; Brown etÂ al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>, especially large language modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al., <a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite> have demonstrated excellent capabilities in solving tasks that require background knowledge or complex reasoning, such as commonsense reasoningÂ <cite class="ltx_cite ltx_citemacro_citep">(Sap etÂ al., <a href="#bib.bib29" title="" class="ltx_ref">2019</a>; Rajani etÂ al., <a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite>
and logical reasoningÂ <cite class="ltx_cite ltx_citemacro_citep">(Wei etÂ al., <a href="#bib.bib38" title="" class="ltx_ref">2022</a>; Kojima etÂ al., <a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2305.17006/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="113" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An example that a captioning model (BLIP) fails to provide suitable descriptions for a prediction model (FLAN-T5) of a question in A-OKVQA dataset.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Inspired by these successes, recent studies have proposed utilizing PLMs<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>In this paper, PLMs refer to the models trained on text-only corpus, instead of the text encoder/decoder in vision-language pre-trainedÂ (VLP) models, which typically have a weaker reasoning capacity in linguistic content.</span></span></span> to solve complex vision-language tasks, exemplified by the task of knowledge-based <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">visual question answering</em>Â (VQA) that aims to answer open-ended questions given an image based on outside knowledgeÂ <cite class="ltx_cite ltx_citemacro_cite">Schwenk etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2022</a>)</cite>. It has been shown that PLM-enhanced approachesÂ <cite class="ltx_cite ltx_citemacro_citep">(Gui etÂ al., <a href="#bib.bib11" title="" class="ltx_ref">2022</a>; Lin etÂ al., <a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite> typically lead to an improved performance on the knowledge-based VQA task than pure vision-language pre-trained (VLP) modelsÂ <cite class="ltx_cite ltx_citemacro_cite">Schwenk etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In the literature, existing PLM-enhanced VQA approaches can be roughly categorized into two lines. The first line of research focuses on adapting PLMs to the vision modality by introducing specific modular networks or training objectivesÂ <cite class="ltx_cite ltx_citemacro_citep">(Tsimpoukelli etÂ al., <a href="#bib.bib33" title="" class="ltx_ref">2021</a>; Liang etÂ al., <a href="#bib.bib21" title="" class="ltx_ref">2022</a>; Alayrac etÂ al., <a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite>.
However, they usually incur a high computational cost during pre-training in order to effectively integrate a vision encoder into the PLM.
As another line of research, several studies aim to reduce the cost of tuning PLMs in vision-language tasks by utilizing PLMs in a zero-shot or few-shot manner.
They typically generate a caption for an image using a captioning modelÂ (<em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> a fine-tuned VLP model), and employ the generated caption as the context (<em id="S1.p3.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> prompt) to assist PLMs in question answeringÂ <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al., <a href="#bib.bib39" title="" class="ltx_ref">2022</a>; Tiong etÂ al., <a href="#bib.bib32" title="" class="ltx_ref">2022</a>; Guo etÂ al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>.
Such an approach is training-free and can be generally applied with various PLMs.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">However, in these existing zero-shot or few-shot methods, the captioning model is unaware of both <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">task goal</em> and <em id="S1.p4.1.2" class="ltx_emph ltx_font_italic">information need</em> for the integrated PLM. They directly reuse the captioning model fine-tuned on caption datasets. As a result, the generated captions tend to be less informative for the VQA task, even irrelevant to the question. FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents an example that an inappropriate caption leads to an incorrect answer generated by the PLM. As we can see, the question is highly related to keywords â€œ<em id="S1.p4.1.3" class="ltx_emph ltx_font_italic">icing</em>â€ or â€œ<em id="S1.p4.1.4" class="ltx_emph ltx_font_italic">frosting</em>â€, while the captioning model misses these information and generates a general description.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To address this issue, we propose <span id="S1.p5.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Lamoc</span>: a novel <span id="S1.p5.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">LA</span>nguage <span id="S1.p5.1.3" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">MO</span>del guided <span id="S1.p5.1.4" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">C</span>aptioning approach for the VQA task. The key idea is to leverage the guidance and feedback of the prediction model (<em id="S1.p5.1.5" class="ltx_emph ltx_font_italic">i.e.,</em> the PLM) to improve the capability of the captioning model, so that it can be aware of the task goal and information need, and assist the prediction model in answer prediction. Our approach is specially designed with two gradual training stages. At the first stage, the captioning model is trained to align to the prediction model, in which the prediction model selects captions that are more pertinent to a given question from multiple propositions generated by the captioning model. These selected captions are informative and can be used to fine-tune the captioning model to generate informative captions. At the second stage, since the generated caption is used by the PLM as direct evidence for VQA, we employ the feedback from the PLM as reward signals to train the captioning model via reinforcement learning.
During training, only the captioning model is tuned while the PLM is fixed, which significantly reduces the computational costs. Meanwhile, since the feedback is from PLM, both training stages do not require any labeled data.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our contributions can be summarized as follows: (1) We propose <span id="S1.p6.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Lamoc</span>, a
novel approach for training captioning models to generate informative captions that can assist PLMs in VQA tasks;
(2) Using a small number of randomly sampled unlabeled (image, question) pairs, <span id="S1.p6.1.2" class="ltx_text ltx_font_bold ltx_font_smallcaps">Lamoc</span> consistently outperforms several competitive zero/few-shot baselines without PLM feedback on two knowledge-based VQA datasets: OK-VQA and A-OKVQA; (3) We have demonstrated the effectiveness of our method on PLMs of varying scales, from 223M to 11B. This not only confirms the robustness of our approach but also demonstrates its potential for generalization to Large Language ModelsÂ (LLMs).</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">PLMs for VQA.</h5>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">After training on large corpora, PLMs exhibit surprising abilities, such as chain-of-thought reasoningÂ <cite class="ltx_cite ltx_citemacro_citep">(Wei etÂ al., <a href="#bib.bib38" title="" class="ltx_ref">2022</a>)</cite>, in-context learningÂ <cite class="ltx_cite ltx_citemacro_citep">(Brown etÂ al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>, and instruction followingÂ <cite class="ltx_cite ltx_citemacro_citep">(Chung etÂ al., <a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite>, which cannot be obtained by vision-language pre-training. Thus, some works adopt PLM to perform VQA and obtain promising results. One line of research combines a PLM and a vision encoder and trains them end-to-end. FrozenÂ <cite class="ltx_cite ltx_citemacro_citep">(Tsimpoukelli etÂ al., <a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Liang etÂ al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite> train a visual encoder or a modular network and keep the PLM frozen to retain its powerful abilities. FlamingoÂ <cite class="ltx_cite ltx_citemacro_citep">(Alayrac etÂ al., <a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite> elaborates the model architecture to combine the vision and language models and scales the model size to 80B. Another line of research tries to deploy PLMs on VQA tasks in a few-shot/zero-shot manner. PICaÂ <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al., <a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite> and Img2PromptÂ <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite> translate the image to captions or tags and employ GPT-3 to answer a question by in-context learning. PNP-VQAÂ <cite class="ltx_cite ltx_citemacro_citep">(Tiong etÂ al., <a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite> generates question-related captions and utilizes a QA modelÂ <cite class="ltx_cite ltx_citemacro_citep">(Khashabi etÂ al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite> for answer prediction. This type of work does not require extra training and can be adapted to new PLMs.  Our work follows the second paradigm and is an extension of these works.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Learning from Feedback.</h5>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">A regular paradigm to train a model is defining a loss function and optimizing it. However, certain objectives, such as coherence, diversity, and toxicity in text generation, may not be easily incorporated into the loss function and learned in an end-to-end mannerÂ <cite class="ltx_cite ltx_citemacro_citep">(Paulus etÂ al., <a href="#bib.bib27" title="" class="ltx_ref">2018</a>; Pang and He, <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>. Thus, explicit feedback on model output is regarded as a learning signal to assist in training. <cite class="ltx_cite ltx_citemacro_citet">Campos and Shern (<a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite> utilize a PLMâ€™s refinement and human feedback to fine-tune a summary model. <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a href="#bib.bib36" title="" class="ltx_ref">2022c</a>)</cite> leverage compiler feedback to improve the compilability of programs generated by the language model. <cite class="ltx_cite ltx_citemacro_citet">Ouyang etÂ al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite> align a language model with the userâ€™s intention through reinforcement learning from human feedback. We borrow idea from these works, but our feedback comes from a PLM instead of humans, thus saving the annotation cost.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we present the proposed <span id="S3.p1.1.1" class="ltx_text ltx_font_smallcaps">Lamoc</span>: <span id="S3.p1.1.2" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">LA</span>nguage <span id="S3.p1.1.3" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">MO</span>del guided <span id="S3.p1.1.4" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">C</span>aptioning method for VQA.
The overall architecture of <span id="S3.p1.1.5" class="ltx_text ltx_font_smallcaps">Lamoc</span> is depicted in FigureÂ <a href="#S3.F2" title="Figure 2 â€£ 3 Method â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2305.17006/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="256" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of our proposed approach <span id="S3.F2.2.1" class="ltx_text ltx_font_smallcaps">Lamoc</span>. In captioning adaption, we utilize a PLM to select informative captions and fine-tune the captioning model on them. When learning from PLM feedback, we regard the feedback from the PLM as reward signals and perform reinforcement learning on the captioning model. </figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview of Our Approach</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.4" class="ltx_p">In this work, we study the task of <em id="S3.SS1.p1.4.1" class="ltx_emph ltx_font_italic">visual question answeringÂ (VQA)</em>. Given
an image-question pair <math id="S3.SS1.p1.1.m1.2" class="ltx_Math" alttext="x:\langle x_{i},x_{q}\rangle" display="inline"><semantics id="S3.SS1.p1.1.m1.2a"><mrow id="S3.SS1.p1.1.m1.2.2" xref="S3.SS1.p1.1.m1.2.2.cmml"><mi id="S3.SS1.p1.1.m1.2.2.4" xref="S3.SS1.p1.1.m1.2.2.4.cmml">x</mi><mo lspace="0.278em" rspace="0.278em" id="S3.SS1.p1.1.m1.2.2.3" xref="S3.SS1.p1.1.m1.2.2.3.cmml">:</mo><mrow id="S3.SS1.p1.1.m1.2.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p1.1.m1.2.2.2.2.3" xref="S3.SS1.p1.1.m1.2.2.2.3.cmml">âŸ¨</mo><msub id="S3.SS1.p1.1.m1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p1.1.m1.2.2.2.2.4" xref="S3.SS1.p1.1.m1.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p1.1.m1.2.2.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.2.2.cmml"><mi id="S3.SS1.p1.1.m1.2.2.2.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.cmml">x</mi><mi id="S3.SS1.p1.1.m1.2.2.2.2.2.3" xref="S3.SS1.p1.1.m1.2.2.2.2.2.3.cmml">q</mi></msub><mo stretchy="false" id="S3.SS1.p1.1.m1.2.2.2.2.5" xref="S3.SS1.p1.1.m1.2.2.2.3.cmml">âŸ©</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.2b"><apply id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2"><ci id="S3.SS1.p1.1.m1.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.3">:</ci><ci id="S3.SS1.p1.1.m1.2.2.4.cmml" xref="S3.SS1.p1.1.m1.2.2.4">ğ‘¥</ci><list id="S3.SS1.p1.1.m1.2.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2"><apply id="S3.SS1.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2">ğ‘¥</ci><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2">ğ‘¥</ci><ci id="S3.SS1.p1.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2.3">ğ‘</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.2c">x:\langle x_{i},x_{q}\rangle</annotation></semantics></math>, the task goal is to predict a correct answer <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">y</annotation></semantics></math> to the question <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="x_{q}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">x</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">ğ‘¥</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">x_{q}</annotation></semantics></math> given the image <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">x</mi><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">ğ‘¥</ci><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">x_{i}</annotation></semantics></math>. Following prior studiesÂ <cite class="ltx_cite ltx_citemacro_cite">Yang etÂ al. (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>); Tiong etÂ al. (<a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite>, we adopt a captioning-based approach for VQA, in which a captioning model generates auxiliary captions for helping answer prediction.
Formally, we represent the above idea in a probabilistic way:</p>
<table id="A2.EGx1" class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table">

<tbody id="S3.Ex1">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1.m2.1" class="ltx_Math" alttext="\displaystyle p(y|x_{i},x_{q})" display="inline"><semantics id="S3.E1.m2.1a"><mrow id="S3.E1.m2.1.1" xref="S3.E1.m2.1.1.cmml"><mi id="S3.E1.m2.1.1.3" xref="S3.E1.m2.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E1.m2.1.1.2" xref="S3.E1.m2.1.1.2.cmml">â€‹</mo><mrow id="S3.E1.m2.1.1.1.1" xref="S3.E1.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m2.1.1.1.1.2" xref="S3.E1.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m2.1.1.1.1.1" xref="S3.E1.m2.1.1.1.1.1.cmml"><mi id="S3.E1.m2.1.1.1.1.1.4" xref="S3.E1.m2.1.1.1.1.1.4.cmml">y</mi><mo fence="false" id="S3.E1.m2.1.1.1.1.1.3" xref="S3.E1.m2.1.1.1.1.1.3.cmml">|</mo><mrow id="S3.E1.m2.1.1.1.1.1.2.2" xref="S3.E1.m2.1.1.1.1.1.2.3.cmml"><msub id="S3.E1.m2.1.1.1.1.1.1.1.1" xref="S3.E1.m2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m2.1.1.1.1.1.1.1.1.2" xref="S3.E1.m2.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E1.m2.1.1.1.1.1.1.1.1.3" xref="S3.E1.m2.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m2.1.1.1.1.1.2.2.3" xref="S3.E1.m2.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E1.m2.1.1.1.1.1.2.2.2" xref="S3.E1.m2.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E1.m2.1.1.1.1.1.2.2.2.2" xref="S3.E1.m2.1.1.1.1.1.2.2.2.2.cmml">x</mi><mi id="S3.E1.m2.1.1.1.1.1.2.2.2.3" xref="S3.E1.m2.1.1.1.1.1.2.2.2.3.cmml">q</mi></msub></mrow></mrow><mo stretchy="false" id="S3.E1.m2.1.1.1.1.3" xref="S3.E1.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m2.1b"><apply id="S3.E1.m2.1.1.cmml" xref="S3.E1.m2.1.1"><times id="S3.E1.m2.1.1.2.cmml" xref="S3.E1.m2.1.1.2"></times><ci id="S3.E1.m2.1.1.3.cmml" xref="S3.E1.m2.1.1.3">ğ‘</ci><apply id="S3.E1.m2.1.1.1.1.1.cmml" xref="S3.E1.m2.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m2.1.1.1.1.1.3.cmml" xref="S3.E1.m2.1.1.1.1.1.3">conditional</csymbol><ci id="S3.E1.m2.1.1.1.1.1.4.cmml" xref="S3.E1.m2.1.1.1.1.1.4">ğ‘¦</ci><list id="S3.E1.m2.1.1.1.1.1.2.3.cmml" xref="S3.E1.m2.1.1.1.1.1.2.2"><apply id="S3.E1.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m2.1.1.1.1.1.1.1.1.2">ğ‘¥</ci><ci id="S3.E1.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m2.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E1.m2.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m2.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m2.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m2.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E1.m2.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m2.1.1.1.1.1.2.2.2.2">ğ‘¥</ci><ci id="S3.E1.m2.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m2.1.1.1.1.1.2.2.2.3">ğ‘</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m2.1c">\displaystyle p(y|x_{i},x_{q})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S3.Ex1.m1.1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics id="S3.Ex1.m1.1a"><mo id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.1b"><eq id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex1.m2.4" class="ltx_Math" alttext="\displaystyle\sum_{z\in\mathcal{Z}}\underbrace{p(z|x_{i},x_{q};\Theta_{C})}_{\text{caption generation}}\cdot\underbrace{p(y|x_{q},z;\Theta_{P})}_{\text{answer prediction}}," display="inline"><semantics id="S3.Ex1.m2.4a"><mrow id="S3.Ex1.m2.4.4.1" xref="S3.Ex1.m2.4.4.1.1.cmml"><mrow id="S3.Ex1.m2.4.4.1.1" xref="S3.Ex1.m2.4.4.1.1.cmml"><mstyle displaystyle="true" id="S3.Ex1.m2.4.4.1.1.1" xref="S3.Ex1.m2.4.4.1.1.1.cmml"><munder id="S3.Ex1.m2.4.4.1.1.1a" xref="S3.Ex1.m2.4.4.1.1.1.cmml"><mo movablelimits="false" id="S3.Ex1.m2.4.4.1.1.1.2" xref="S3.Ex1.m2.4.4.1.1.1.2.cmml">âˆ‘</mo><mrow id="S3.Ex1.m2.4.4.1.1.1.3" xref="S3.Ex1.m2.4.4.1.1.1.3.cmml"><mi id="S3.Ex1.m2.4.4.1.1.1.3.2" xref="S3.Ex1.m2.4.4.1.1.1.3.2.cmml">z</mi><mo id="S3.Ex1.m2.4.4.1.1.1.3.1" xref="S3.Ex1.m2.4.4.1.1.1.3.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m2.4.4.1.1.1.3.3" xref="S3.Ex1.m2.4.4.1.1.1.3.3.cmml">ğ’µ</mi></mrow></munder></mstyle><mrow id="S3.Ex1.m2.4.4.1.1.2" xref="S3.Ex1.m2.4.4.1.1.2.cmml"><munder id="S3.Ex1.m2.4.4.1.1.2.2" xref="S3.Ex1.m2.4.4.1.1.2.2.cmml"><munder accentunder="true" id="S3.Ex1.m2.1.1" xref="S3.Ex1.m2.1.1.cmml"><mrow id="S3.Ex1.m2.1.1.1" xref="S3.Ex1.m2.1.1.1.cmml"><mi id="S3.Ex1.m2.1.1.1.3" xref="S3.Ex1.m2.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.1.1.1.2" xref="S3.Ex1.m2.1.1.1.2.cmml">â€‹</mo><mrow id="S3.Ex1.m2.1.1.1.1.1" xref="S3.Ex1.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex1.m2.1.1.1.1.1.2" xref="S3.Ex1.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m2.1.1.1.1.1.1" xref="S3.Ex1.m2.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m2.1.1.1.1.1.1.5" xref="S3.Ex1.m2.1.1.1.1.1.1.5.cmml">z</mi><mo fence="false" id="S3.Ex1.m2.1.1.1.1.1.1.4" xref="S3.Ex1.m2.1.1.1.1.1.1.4.cmml">|</mo><mrow id="S3.Ex1.m2.1.1.1.1.1.1.3.3" xref="S3.Ex1.m2.1.1.1.1.1.1.3.4.cmml"><msub id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.Ex1.m2.1.1.1.1.1.1.3.3.4" xref="S3.Ex1.m2.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S3.Ex1.m2.1.1.1.1.1.1.2.2.2" xref="S3.Ex1.m2.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.Ex1.m2.1.1.1.1.1.1.2.2.2.2" xref="S3.Ex1.m2.1.1.1.1.1.1.2.2.2.2.cmml">x</mi><mi id="S3.Ex1.m2.1.1.1.1.1.1.2.2.2.3" xref="S3.Ex1.m2.1.1.1.1.1.1.2.2.2.3.cmml">q</mi></msub><mo id="S3.Ex1.m2.1.1.1.1.1.1.3.3.5" xref="S3.Ex1.m2.1.1.1.1.1.1.3.4.cmml">;</mo><msub id="S3.Ex1.m2.1.1.1.1.1.1.3.3.3" xref="S3.Ex1.m2.1.1.1.1.1.1.3.3.3.cmml"><mi mathvariant="normal" id="S3.Ex1.m2.1.1.1.1.1.1.3.3.3.2" xref="S3.Ex1.m2.1.1.1.1.1.1.3.3.3.2.cmml">Î˜</mi><mi id="S3.Ex1.m2.1.1.1.1.1.1.3.3.3.3" xref="S3.Ex1.m2.1.1.1.1.1.1.3.3.3.3.cmml">C</mi></msub></mrow></mrow><mo rspace="0.055em" stretchy="false" id="S3.Ex1.m2.1.1.1.1.1.3" xref="S3.Ex1.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m2.1.1.2" xref="S3.Ex1.m2.1.1.2.cmml">âŸ</mo></munder><mtext id="S3.Ex1.m2.4.4.1.1.2.2.2" xref="S3.Ex1.m2.4.4.1.1.2.2.2a.cmml">caption generation</mtext></munder><mo rspace="0.222em" id="S3.Ex1.m2.4.4.1.1.2.1" xref="S3.Ex1.m2.4.4.1.1.2.1.cmml">â‹…</mo><munder id="S3.Ex1.m2.4.4.1.1.2.3" xref="S3.Ex1.m2.4.4.1.1.2.3.cmml"><munder accentunder="true" id="S3.Ex1.m2.3.3" xref="S3.Ex1.m2.3.3.cmml"><mrow id="S3.Ex1.m2.3.3.2" xref="S3.Ex1.m2.3.3.2.cmml"><mi id="S3.Ex1.m2.3.3.2.4" xref="S3.Ex1.m2.3.3.2.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.3.3.2.3" xref="S3.Ex1.m2.3.3.2.3.cmml">â€‹</mo><mrow id="S3.Ex1.m2.3.3.2.2.1" xref="S3.Ex1.m2.3.3.2.2.1.1.cmml"><mo stretchy="false" id="S3.Ex1.m2.3.3.2.2.1.2" xref="S3.Ex1.m2.3.3.2.2.1.1.cmml">(</mo><mrow id="S3.Ex1.m2.3.3.2.2.1.1" xref="S3.Ex1.m2.3.3.2.2.1.1.cmml"><mi id="S3.Ex1.m2.3.3.2.2.1.1.4" xref="S3.Ex1.m2.3.3.2.2.1.1.4.cmml">y</mi><mo fence="false" id="S3.Ex1.m2.3.3.2.2.1.1.3" xref="S3.Ex1.m2.3.3.2.2.1.1.3.cmml">|</mo><mrow id="S3.Ex1.m2.3.3.2.2.1.1.2.2" xref="S3.Ex1.m2.3.3.2.2.1.1.2.3.cmml"><msub id="S3.Ex1.m2.3.3.2.2.1.1.1.1.1" xref="S3.Ex1.m2.3.3.2.2.1.1.1.1.1.cmml"><mi id="S3.Ex1.m2.3.3.2.2.1.1.1.1.1.2" xref="S3.Ex1.m2.3.3.2.2.1.1.1.1.1.2.cmml">x</mi><mi id="S3.Ex1.m2.3.3.2.2.1.1.1.1.1.3" xref="S3.Ex1.m2.3.3.2.2.1.1.1.1.1.3.cmml">q</mi></msub><mo id="S3.Ex1.m2.3.3.2.2.1.1.2.2.3" xref="S3.Ex1.m2.3.3.2.2.1.1.2.3.cmml">,</mo><mi id="S3.Ex1.m2.2.2.1.1" xref="S3.Ex1.m2.2.2.1.1.cmml">z</mi><mo id="S3.Ex1.m2.3.3.2.2.1.1.2.2.4" xref="S3.Ex1.m2.3.3.2.2.1.1.2.3.cmml">;</mo><msub id="S3.Ex1.m2.3.3.2.2.1.1.2.2.2" xref="S3.Ex1.m2.3.3.2.2.1.1.2.2.2.cmml"><mi mathvariant="normal" id="S3.Ex1.m2.3.3.2.2.1.1.2.2.2.2" xref="S3.Ex1.m2.3.3.2.2.1.1.2.2.2.2.cmml">Î˜</mi><mi id="S3.Ex1.m2.3.3.2.2.1.1.2.2.2.3" xref="S3.Ex1.m2.3.3.2.2.1.1.2.2.2.3.cmml">P</mi></msub></mrow></mrow><mo stretchy="false" id="S3.Ex1.m2.3.3.2.2.1.3" xref="S3.Ex1.m2.3.3.2.2.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m2.3.3.3" xref="S3.Ex1.m2.3.3.3.cmml">âŸ</mo></munder><mtext id="S3.Ex1.m2.4.4.1.1.2.3.2" xref="S3.Ex1.m2.4.4.1.1.2.3.2a.cmml">answer prediction</mtext></munder></mrow></mrow><mo id="S3.Ex1.m2.4.4.1.2" xref="S3.Ex1.m2.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m2.4b"><apply id="S3.Ex1.m2.4.4.1.1.cmml" xref="S3.Ex1.m2.4.4.1"><apply id="S3.Ex1.m2.4.4.1.1.1.cmml" xref="S3.Ex1.m2.4.4.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m2.4.4.1.1.1.1.cmml" xref="S3.Ex1.m2.4.4.1.1.1">subscript</csymbol><sum id="S3.Ex1.m2.4.4.1.1.1.2.cmml" xref="S3.Ex1.m2.4.4.1.1.1.2"></sum><apply id="S3.Ex1.m2.4.4.1.1.1.3.cmml" xref="S3.Ex1.m2.4.4.1.1.1.3"><in id="S3.Ex1.m2.4.4.1.1.1.3.1.cmml" xref="S3.Ex1.m2.4.4.1.1.1.3.1"></in><ci id="S3.Ex1.m2.4.4.1.1.1.3.2.cmml" xref="S3.Ex1.m2.4.4.1.1.1.3.2">ğ‘§</ci><ci id="S3.Ex1.m2.4.4.1.1.1.3.3.cmml" xref="S3.Ex1.m2.4.4.1.1.1.3.3">ğ’µ</ci></apply></apply><apply id="S3.Ex1.m2.4.4.1.1.2.cmml" xref="S3.Ex1.m2.4.4.1.1.2"><ci id="S3.Ex1.m2.4.4.1.1.2.1.cmml" xref="S3.Ex1.m2.4.4.1.1.2.1">â‹…</ci><apply id="S3.Ex1.m2.4.4.1.1.2.2.cmml" xref="S3.Ex1.m2.4.4.1.1.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m2.4.4.1.1.2.2.1.cmml" xref="S3.Ex1.m2.4.4.1.1.2.2">subscript</csymbol><apply id="S3.Ex1.m2.1.1.cmml" xref="S3.Ex1.m2.1.1"><ci id="S3.Ex1.m2.1.1.2.cmml" xref="S3.Ex1.m2.1.1.2">âŸ</ci><apply id="S3.Ex1.m2.1.1.1.cmml" xref="S3.Ex1.m2.1.1.1"><times id="S3.Ex1.m2.1.1.1.2.cmml" xref="S3.Ex1.m2.1.1.1.2"></times><ci id="S3.Ex1.m2.1.1.1.3.cmml" xref="S3.Ex1.m2.1.1.1.3">ğ‘</ci><apply id="S3.Ex1.m2.1.1.1.1.1.1.cmml" xref="S3.Ex1.m2.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex1.m2.1.1.1.1.1.1.4.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.4">conditional</csymbol><ci id="S3.Ex1.m2.1.1.1.1.1.1.5.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.5">ğ‘§</ci><list id="S3.Ex1.m2.1.1.1.1.1.1.3.4.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.3.3"><apply id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.2">ğ‘¥</ci><ci id="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.Ex1.m2.1.1.1.1.1.1.2.2.2.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m2.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.Ex1.m2.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.2.2.2.2">ğ‘¥</ci><ci id="S3.Ex1.m2.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.2.2.2.3">ğ‘</ci></apply><apply id="S3.Ex1.m2.1.1.1.1.1.1.3.3.3.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m2.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.Ex1.m2.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.3.3.3.2">Î˜</ci><ci id="S3.Ex1.m2.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.3.3.3.3">ğ¶</ci></apply></list></apply></apply></apply><ci id="S3.Ex1.m2.4.4.1.1.2.2.2a.cmml" xref="S3.Ex1.m2.4.4.1.1.2.2.2"><mtext mathsize="70%" id="S3.Ex1.m2.4.4.1.1.2.2.2.cmml" xref="S3.Ex1.m2.4.4.1.1.2.2.2">caption generation</mtext></ci></apply><apply id="S3.Ex1.m2.4.4.1.1.2.3.cmml" xref="S3.Ex1.m2.4.4.1.1.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m2.4.4.1.1.2.3.1.cmml" xref="S3.Ex1.m2.4.4.1.1.2.3">subscript</csymbol><apply id="S3.Ex1.m2.3.3.cmml" xref="S3.Ex1.m2.3.3"><ci id="S3.Ex1.m2.3.3.3.cmml" xref="S3.Ex1.m2.3.3.3">âŸ</ci><apply id="S3.Ex1.m2.3.3.2.cmml" xref="S3.Ex1.m2.3.3.2"><times id="S3.Ex1.m2.3.3.2.3.cmml" xref="S3.Ex1.m2.3.3.2.3"></times><ci id="S3.Ex1.m2.3.3.2.4.cmml" xref="S3.Ex1.m2.3.3.2.4">ğ‘</ci><apply id="S3.Ex1.m2.3.3.2.2.1.1.cmml" xref="S3.Ex1.m2.3.3.2.2.1"><csymbol cd="latexml" id="S3.Ex1.m2.3.3.2.2.1.1.3.cmml" xref="S3.Ex1.m2.3.3.2.2.1.1.3">conditional</csymbol><ci id="S3.Ex1.m2.3.3.2.2.1.1.4.cmml" xref="S3.Ex1.m2.3.3.2.2.1.1.4">ğ‘¦</ci><list id="S3.Ex1.m2.3.3.2.2.1.1.2.3.cmml" xref="S3.Ex1.m2.3.3.2.2.1.1.2.2"><apply id="S3.Ex1.m2.3.3.2.2.1.1.1.1.1.cmml" xref="S3.Ex1.m2.3.3.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m2.3.3.2.2.1.1.1.1.1.1.cmml" xref="S3.Ex1.m2.3.3.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m2.3.3.2.2.1.1.1.1.1.2.cmml" xref="S3.Ex1.m2.3.3.2.2.1.1.1.1.1.2">ğ‘¥</ci><ci id="S3.Ex1.m2.3.3.2.2.1.1.1.1.1.3.cmml" xref="S3.Ex1.m2.3.3.2.2.1.1.1.1.1.3">ğ‘</ci></apply><ci id="S3.Ex1.m2.2.2.1.1.cmml" xref="S3.Ex1.m2.2.2.1.1">ğ‘§</ci><apply id="S3.Ex1.m2.3.3.2.2.1.1.2.2.2.cmml" xref="S3.Ex1.m2.3.3.2.2.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m2.3.3.2.2.1.1.2.2.2.1.cmml" xref="S3.Ex1.m2.3.3.2.2.1.1.2.2.2">subscript</csymbol><ci id="S3.Ex1.m2.3.3.2.2.1.1.2.2.2.2.cmml" xref="S3.Ex1.m2.3.3.2.2.1.1.2.2.2.2">Î˜</ci><ci id="S3.Ex1.m2.3.3.2.2.1.1.2.2.2.3.cmml" xref="S3.Ex1.m2.3.3.2.2.1.1.2.2.2.3">ğ‘ƒ</ci></apply></list></apply></apply></apply><ci id="S3.Ex1.m2.4.4.1.1.2.3.2a.cmml" xref="S3.Ex1.m2.4.4.1.1.2.3.2"><mtext mathsize="70%" id="S3.Ex1.m2.4.4.1.1.2.3.2.cmml" xref="S3.Ex1.m2.4.4.1.1.2.3.2">answer prediction</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m2.4c">\displaystyle\sum_{z\in\mathcal{Z}}\underbrace{p(z|x_{i},x_{q};\Theta_{C})}_{\text{caption generation}}\cdot\underbrace{p(y|x_{q},z;\Theta_{P})}_{\text{answer prediction}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p id="S3.SS1.p1.10" class="ltx_p">where a captioning model <math id="S3.SS1.p1.5.m1.1" class="ltx_Math" alttext="\Theta_{C}" display="inline"><semantics id="S3.SS1.p1.5.m1.1a"><msub id="S3.SS1.p1.5.m1.1.1" xref="S3.SS1.p1.5.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p1.5.m1.1.1.2" xref="S3.SS1.p1.5.m1.1.1.2.cmml">Î˜</mi><mi id="S3.SS1.p1.5.m1.1.1.3" xref="S3.SS1.p1.5.m1.1.1.3.cmml">C</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m1.1b"><apply id="S3.SS1.p1.5.m1.1.1.cmml" xref="S3.SS1.p1.5.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m1.1.1.1.cmml" xref="S3.SS1.p1.5.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m1.1.1.2.cmml" xref="S3.SS1.p1.5.m1.1.1.2">Î˜</ci><ci id="S3.SS1.p1.5.m1.1.1.3.cmml" xref="S3.SS1.p1.5.m1.1.1.3">ğ¶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m1.1c">\Theta_{C}</annotation></semantics></math> firstly generates an auxiliary captions <math id="S3.SS1.p1.6.m2.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS1.p1.6.m2.1a"><mi id="S3.SS1.p1.6.m2.1.1" xref="S3.SS1.p1.6.m2.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m2.1b"><ci id="S3.SS1.p1.6.m2.1.1.cmml" xref="S3.SS1.p1.6.m2.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m2.1c">z</annotation></semantics></math>, and then
a prediction model <math id="S3.SS1.p1.7.m3.1" class="ltx_Math" alttext="\Theta_{P}" display="inline"><semantics id="S3.SS1.p1.7.m3.1a"><msub id="S3.SS1.p1.7.m3.1.1" xref="S3.SS1.p1.7.m3.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p1.7.m3.1.1.2" xref="S3.SS1.p1.7.m3.1.1.2.cmml">Î˜</mi><mi id="S3.SS1.p1.7.m3.1.1.3" xref="S3.SS1.p1.7.m3.1.1.3.cmml">P</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m3.1b"><apply id="S3.SS1.p1.7.m3.1.1.cmml" xref="S3.SS1.p1.7.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m3.1.1.1.cmml" xref="S3.SS1.p1.7.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.7.m3.1.1.2.cmml" xref="S3.SS1.p1.7.m3.1.1.2">Î˜</ci><ci id="S3.SS1.p1.7.m3.1.1.3.cmml" xref="S3.SS1.p1.7.m3.1.1.3">ğ‘ƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m3.1c">\Theta_{P}</annotation></semantics></math> predicts an answer candidate <math id="S3.SS1.p1.8.m4.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS1.p1.8.m4.1a"><mi id="S3.SS1.p1.8.m4.1.1" xref="S3.SS1.p1.8.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m4.1b"><ci id="S3.SS1.p1.8.m4.1.1.cmml" xref="S3.SS1.p1.8.m4.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m4.1c">y</annotation></semantics></math> based on the caption <math id="S3.SS1.p1.9.m5.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS1.p1.9.m5.1a"><mi id="S3.SS1.p1.9.m5.1.1" xref="S3.SS1.p1.9.m5.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m5.1b"><ci id="S3.SS1.p1.9.m5.1.1.cmml" xref="S3.SS1.p1.9.m5.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m5.1c">z</annotation></semantics></math> and the question <math id="S3.SS1.p1.10.m6.1" class="ltx_Math" alttext="x_{q}" display="inline"><semantics id="S3.SS1.p1.10.m6.1a"><msub id="S3.SS1.p1.10.m6.1.1" xref="S3.SS1.p1.10.m6.1.1.cmml"><mi id="S3.SS1.p1.10.m6.1.1.2" xref="S3.SS1.p1.10.m6.1.1.2.cmml">x</mi><mi id="S3.SS1.p1.10.m6.1.1.3" xref="S3.SS1.p1.10.m6.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m6.1b"><apply id="S3.SS1.p1.10.m6.1.1.cmml" xref="S3.SS1.p1.10.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m6.1.1.1.cmml" xref="S3.SS1.p1.10.m6.1.1">subscript</csymbol><ci id="S3.SS1.p1.10.m6.1.1.2.cmml" xref="S3.SS1.p1.10.m6.1.1.2">ğ‘¥</ci><ci id="S3.SS1.p1.10.m6.1.1.3.cmml" xref="S3.SS1.p1.10.m6.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m6.1c">x_{q}</annotation></semantics></math>. We evaluate this probability by iterating over a set of generated captions.
Here, we consider an unsupervised setting: no labeled answer data is available. Although there is no labeled answers, we assume that a small number of
image-question pairs can be obtained for training (no overlapping with the task dataset).</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.5" class="ltx_p">To instantiate this probabilistic approach, we adopt a vision-language pre-trainedÂ (VLP) model, <em id="S3.SS1.p2.5.1" class="ltx_emph ltx_font_italic">i.e.,</em> BLIPÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2022b</a>)</cite>, as the captioning model, and a pre-trained language modelÂ (PLM), <em id="S3.SS1.p2.5.2" class="ltx_emph ltx_font_italic">i.e.,</em> FLAN-T5-XXLÂ <cite class="ltx_cite ltx_citemacro_cite">Chung etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite>, as the prediction model.
The prediction model <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\Theta_{P}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">Î˜</mi><mi id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">P</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">Î˜</ci><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">ğ‘ƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\Theta_{P}</annotation></semantics></math> is expected to fulfill the task by accurately predicting the answer, while the captioning model <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\Theta_{C}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><msub id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">Î˜</mi><mi id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">C</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">Î˜</ci><ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">ğ¶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\Theta_{C}</annotation></semantics></math> plays an assisted role by providing informative evidence for <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="\Theta_{P}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><msub id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">Î˜</mi><mi id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">P</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">Î˜</ci><ci id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">ğ‘ƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\Theta_{P}</annotation></semantics></math>.
In our approach, the captioning model <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="\Theta_{C}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><msub id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">Î˜</mi><mi id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">C</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">Î˜</ci><ci id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">ğ¶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">\Theta_{C}</annotation></semantics></math> can be tuned while the prediction model <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="\Theta_{P}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><msub id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">Î˜</mi><mi id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml">P</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">Î˜</ci><ci id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3">ğ‘ƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">\Theta_{P}</annotation></semantics></math> is fixed during optimization. By leveraging the unlabeled image-question pairs (without the labeled answers), we let the two models cooperate with each other: the captioning model generates informative evidence for helping answer prediction, and the prediction model provides task-specific guidance and feedback to improve the captioning model.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.4" class="ltx_p">To optimize our approach, we design a gradual training process including two stages:
(1) <em id="S3.SS1.p3.4.1" class="ltx_emph ltx_font_italic">captioning adaptation</em> aims to adjust <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="\Theta_{C}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">Î˜</mi><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">C</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">Î˜</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">ğ¶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\Theta_{C}</annotation></semantics></math> to produce informative captions that are suitable for <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="\Theta_{P}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><msub id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">Î˜</mi><mi id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">P</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">Î˜</ci><ci id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">ğ‘ƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\Theta_{P}</annotation></semantics></math>Â (Â§<a href="#S3.SS2.SSS1" title="3.2.1 Captioning Adaptation â€£ 3.2 Language Model Guided Captioning â€£ 3 Method â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>), and (2) <em id="S3.SS1.p3.4.2" class="ltx_emph ltx_font_italic">feedback-based learning</em> aims to optimize <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="\Theta_{C}" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><msub id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">Î˜</mi><mi id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml">C</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">Î˜</ci><ci id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3">ğ¶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\Theta_{C}</annotation></semantics></math> according to task-specific feedback from <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="\Theta_{P}" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><msub id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p3.4.m4.1.1.2" xref="S3.SS1.p3.4.m4.1.1.2.cmml">Î˜</mi><mi id="S3.SS1.p3.4.m4.1.1.3" xref="S3.SS1.p3.4.m4.1.1.3.cmml">P</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2">Î˜</ci><ci id="S3.SS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3">ğ‘ƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">\Theta_{P}</annotation></semantics></math>Â (Â§<a href="#S3.SS2.SSS2" title="3.2.2 Feedback-based Learning â€£ 3.2 Language Model Guided Captioning â€£ 3 Method â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>). Once the captioning model is well trained, we employ the prediction model for predicting the final answer as in Eq.Â (<a href="#S3.Ex1" title="3.1 Overview of Our Approach â€£ 3 Method â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), based on the captions provided by the captioning modelÂ (Â§<a href="#S3.SS3" title="3.3 Answer Prediction â€£ 3 Method â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>). Next, we introduce these parts in details.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Language Model Guided Captioning</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.2" class="ltx_p">The key of our approach (Eq.Â (<a href="#S3.Ex1" title="3.1 Overview of Our Approach â€£ 3 Method â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>)) is to train an effective captioning model <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\Theta_{C}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">Î˜</mi><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">C</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">Î˜</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">ğ¶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\Theta_{C}</annotation></semantics></math> for improving the capability of the prediction model <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\Theta_{P}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi mathvariant="normal" id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">Î˜</mi><mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">P</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">Î˜</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">ğ‘ƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\Theta_{P}</annotation></semantics></math> on VQA. Considering that there are no labeled answers, we employ the prediction model to provide guidance and feedback to optimize the captioning model.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Captioning Adaptation</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Since the captioning model is originally intended to describe the given image, it may not be in suited form to assist the prediction model. Thus, we propose a captioning adaptation strategy that tunes the captioning model to fit the prediction model.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS1.p2.3" class="ltx_p"><span id="S3.SS2.SSS1.p2.3.1" class="ltx_text ltx_font_bold">Caption Propositions.</span> We first sample <math id="S3.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.SSS1.p2.1.m1.1a"><mi id="S3.SS2.SSS1.p2.1.m1.1.1" xref="S3.SS2.SSS1.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.1.m1.1b"><ci id="S3.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.1.m1.1c">n</annotation></semantics></math> image-question pairs from VQAv2Â <cite class="ltx_cite ltx_citemacro_citep">(Goyal etÂ al., <a href="#bib.bib10" title="" class="ltx_ref">2017</a>)</cite>, which is a large VQA dataset containing more than <math id="S3.SS2.SSS1.p2.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS2.SSS1.p2.2.m2.1a"><mn id="S3.SS2.SSS1.p2.2.m2.1.1" xref="S3.SS2.SSS1.p2.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.2.m2.1b"><cn type="integer" id="S3.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.2.m2.1c">1</annotation></semantics></math>M questions and does not overlap with our task dataset. Then we employ the captioning model to propose <math id="S3.SS2.SSS1.p2.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.SSS1.p2.3.m3.1a"><mi id="S3.SS2.SSS1.p2.3.m3.1.1" xref="S3.SS2.SSS1.p2.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.3.m3.1b"><ci id="S3.SS2.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.3.m3.1c">k</annotation></semantics></math> captions for each image by nucleus samplingÂ <cite class="ltx_cite ltx_citemacro_cite">Holtzman etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite>. Among these captions, some may be better suited for the prediction model than the rest. We would like to identify such captions and use them to refine the captioning model.</p>
</div>
<div id="S3.SS2.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS1.p3.1" class="ltx_p"><span id="S3.SS2.SSS1.p3.1.1" class="ltx_text ltx_font_bold">Instruction-based Captions Selection.</span>
Since the prediction model is developed based on the FLAN-T5-XXL, it has encoded a large amount of knowledge in a massive number of parameters.
We design the following instruction to prompt FLAN-T5-XXL to identify more informative captions:</p>
</div>
<div id="S3.SS2.SSS1.p4" class="ltx_para">
<p id="S3.SS2.SSS1.p4.1" class="ltx_p"><em id="S3.SS2.SSS1.p4.1.1" class="ltx_emph ltx_font_italic">â€œQuestion: <span id="S3.SS2.SSS1.p4.1.1.1" class="ltx_text ltx_font_typewriter">[QUESTION]</span> Caption: <span id="S3.SS2.SSS1.p4.1.1.2" class="ltx_text ltx_font_typewriter">[CAPTION]</span>\n To what degree does the caption relate to the question:\n A: 0%\n B: 25%\n C: 50%\n D:75%â€.</em></p>
</div>
<div id="S3.SS2.SSS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS1.p5.1" class="ltx_p">Given the above prompt, FLAN-T5-XXL will generate a corresponding option among the set <math id="S3.SS2.SSS1.p5.1.m1.4" class="ltx_Math" alttext="\{A,B,C,D\}" display="inline"><semantics id="S3.SS2.SSS1.p5.1.m1.4a"><mrow id="S3.SS2.SSS1.p5.1.m1.4.5.2" xref="S3.SS2.SSS1.p5.1.m1.4.5.1.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p5.1.m1.4.5.2.1" xref="S3.SS2.SSS1.p5.1.m1.4.5.1.cmml">{</mo><mi id="S3.SS2.SSS1.p5.1.m1.1.1" xref="S3.SS2.SSS1.p5.1.m1.1.1.cmml">A</mi><mo id="S3.SS2.SSS1.p5.1.m1.4.5.2.2" xref="S3.SS2.SSS1.p5.1.m1.4.5.1.cmml">,</mo><mi id="S3.SS2.SSS1.p5.1.m1.2.2" xref="S3.SS2.SSS1.p5.1.m1.2.2.cmml">B</mi><mo id="S3.SS2.SSS1.p5.1.m1.4.5.2.3" xref="S3.SS2.SSS1.p5.1.m1.4.5.1.cmml">,</mo><mi id="S3.SS2.SSS1.p5.1.m1.3.3" xref="S3.SS2.SSS1.p5.1.m1.3.3.cmml">C</mi><mo id="S3.SS2.SSS1.p5.1.m1.4.5.2.4" xref="S3.SS2.SSS1.p5.1.m1.4.5.1.cmml">,</mo><mi id="S3.SS2.SSS1.p5.1.m1.4.4" xref="S3.SS2.SSS1.p5.1.m1.4.4.cmml">D</mi><mo stretchy="false" id="S3.SS2.SSS1.p5.1.m1.4.5.2.5" xref="S3.SS2.SSS1.p5.1.m1.4.5.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p5.1.m1.4b"><set id="S3.SS2.SSS1.p5.1.m1.4.5.1.cmml" xref="S3.SS2.SSS1.p5.1.m1.4.5.2"><ci id="S3.SS2.SSS1.p5.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p5.1.m1.1.1">ğ´</ci><ci id="S3.SS2.SSS1.p5.1.m1.2.2.cmml" xref="S3.SS2.SSS1.p5.1.m1.2.2">ğµ</ci><ci id="S3.SS2.SSS1.p5.1.m1.3.3.cmml" xref="S3.SS2.SSS1.p5.1.m1.3.3">ğ¶</ci><ci id="S3.SS2.SSS1.p5.1.m1.4.4.cmml" xref="S3.SS2.SSS1.p5.1.m1.4.4">ğ·</ci></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p5.1.m1.4c">\{A,B,C,D\}</annotation></semantics></math>. Such an option reflects the correlation between the caption and question, and the captions with the predicted option
â€œ<em id="S3.SS2.SSS1.p5.1.1" class="ltx_emph ltx_font_italic">D:75%</em>â€ are more relevant to the question.
Since the options are made by the prediction model itself, they tend to be more useful for answer prediction. Thus, we keep the captions with the predicted option â€œ<em id="S3.SS2.SSS1.p5.1.2" class="ltx_emph ltx_font_italic">D:75%</em>â€ and discard the rest.</p>
</div>
<div id="S3.SS2.SSS1.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS1.p6.1" class="ltx_p"><span id="S3.SS2.SSS1.p6.1.1" class="ltx_text ltx_font_bold">Captioning Model Fine-tuning.</span> Via the above caption selection, we can obtain a set of more informative captions, which are judged by the prediction model. Further, we use them to fine-tune the captioning model by optimizing the following cross-entropy loss:</p>
</div>
<div id="S3.SS2.SSS1.p7" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\mathcal{L}_{FT}=-\frac{1}{T}\sum\limits_{t=1}^{T}\log p(z_{t}|x_{i},z_{&lt;t})," display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml">â„’</mi><mrow id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.3.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.3.1" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.1.3.3.3.cmml">T</mi></mrow></msub><mo id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.1.1.1.1.1a" xref="S3.E2.m1.1.1.1.1.1.cmml">âˆ’</mo><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mfrac id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml"><mn id="S3.E2.m1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.3.2.cmml">1</mn><mi id="S3.E2.m1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.3.3.cmml">T</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><munderover id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E2.m1.1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.2.2.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.3.2.cmml">t</mi><mo id="S3.E2.m1.1.1.1.1.1.1.1.2.2.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E2.m1.1.1.1.1.1.1.1.2.2.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E2.m1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.2.3.cmml">T</mi></munderover><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E2.m1.1.1.1.1.1.1.1.1.3a" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml">â¡</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.4.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.4.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.4.2.cmml">z</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.4.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.4.3.cmml">t</mi></msub><mo fence="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">|</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">z</mi><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml"></mi><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml">&lt;</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml">t</mi></mrow></msub></mrow></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"></eq><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2">â„’</ci><apply id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3"><times id="S3.E2.m1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1"></times><ci id="S3.E2.m1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2">ğ¹</ci><ci id="S3.E2.m1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3">ğ‘‡</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><minus id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1"></minus><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3"><divide id="S3.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3"></divide><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.2">1</cn><ci id="S3.E2.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.3">ğ‘‡</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><apply id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E2.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.2"></sum><apply id="S3.E2.m1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.3"><eq id="S3.E2.m1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.3.2">ğ‘¡</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2.3">ğ‘‡</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3"><log id="S3.E2.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.1"></log><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2">ğ‘</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3">conditional</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.4.2">ğ‘§</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.4.3">ğ‘¡</ci></apply><list id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2"><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">ğ‘¥</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2">ğ‘§</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3"><lt id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.1"></lt><csymbol cd="latexml" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.2">absent</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.3">ğ‘¡</ci></apply></apply></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\mathcal{L}_{FT}=-\frac{1}{T}\sum\limits_{t=1}^{T}\log p(z_{t}|x_{i},z_{&lt;t}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS1.p7.5" class="ltx_p">where <math id="S3.SS2.SSS1.p7.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.SSS1.p7.1.m1.1a"><mi id="S3.SS2.SSS1.p7.1.m1.1.1" xref="S3.SS2.SSS1.p7.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p7.1.m1.1b"><ci id="S3.SS2.SSS1.p7.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p7.1.m1.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p7.1.m1.1c">T</annotation></semantics></math> is the length of caption, <math id="S3.SS2.SSS1.p7.2.m2.1" class="ltx_Math" alttext="z_{t}" display="inline"><semantics id="S3.SS2.SSS1.p7.2.m2.1a"><msub id="S3.SS2.SSS1.p7.2.m2.1.1" xref="S3.SS2.SSS1.p7.2.m2.1.1.cmml"><mi id="S3.SS2.SSS1.p7.2.m2.1.1.2" xref="S3.SS2.SSS1.p7.2.m2.1.1.2.cmml">z</mi><mi id="S3.SS2.SSS1.p7.2.m2.1.1.3" xref="S3.SS2.SSS1.p7.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p7.2.m2.1b"><apply id="S3.SS2.SSS1.p7.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p7.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p7.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p7.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p7.2.m2.1.1.2">ğ‘§</ci><ci id="S3.SS2.SSS1.p7.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p7.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p7.2.m2.1c">z_{t}</annotation></semantics></math> denotes the <math id="S3.SS2.SSS1.p7.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS2.SSS1.p7.3.m3.1a"><mi id="S3.SS2.SSS1.p7.3.m3.1.1" xref="S3.SS2.SSS1.p7.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p7.3.m3.1b"><ci id="S3.SS2.SSS1.p7.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p7.3.m3.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p7.3.m3.1c">t</annotation></semantics></math>-th token of the informative caption selected by FLAN-T5-XXL, <math id="S3.SS2.SSS1.p7.4.m4.1" class="ltx_Math" alttext="z_{&lt;t}" display="inline"><semantics id="S3.SS2.SSS1.p7.4.m4.1a"><msub id="S3.SS2.SSS1.p7.4.m4.1.1" xref="S3.SS2.SSS1.p7.4.m4.1.1.cmml"><mi id="S3.SS2.SSS1.p7.4.m4.1.1.2" xref="S3.SS2.SSS1.p7.4.m4.1.1.2.cmml">z</mi><mrow id="S3.SS2.SSS1.p7.4.m4.1.1.3" xref="S3.SS2.SSS1.p7.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS1.p7.4.m4.1.1.3.2" xref="S3.SS2.SSS1.p7.4.m4.1.1.3.2.cmml"></mi><mo id="S3.SS2.SSS1.p7.4.m4.1.1.3.1" xref="S3.SS2.SSS1.p7.4.m4.1.1.3.1.cmml">&lt;</mo><mi id="S3.SS2.SSS1.p7.4.m4.1.1.3.3" xref="S3.SS2.SSS1.p7.4.m4.1.1.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p7.4.m4.1b"><apply id="S3.SS2.SSS1.p7.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p7.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p7.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.p7.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p7.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.p7.4.m4.1.1.2">ğ‘§</ci><apply id="S3.SS2.SSS1.p7.4.m4.1.1.3.cmml" xref="S3.SS2.SSS1.p7.4.m4.1.1.3"><lt id="S3.SS2.SSS1.p7.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS1.p7.4.m4.1.1.3.1"></lt><csymbol cd="latexml" id="S3.SS2.SSS1.p7.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS1.p7.4.m4.1.1.3.2">absent</csymbol><ci id="S3.SS2.SSS1.p7.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS1.p7.4.m4.1.1.3.3">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p7.4.m4.1c">z_{&lt;t}</annotation></semantics></math> represents the generated token up to the <math id="S3.SS2.SSS1.p7.5.m5.1" class="ltx_Math" alttext="(t-1)" display="inline"><semantics id="S3.SS2.SSS1.p7.5.m5.1a"><mrow id="S3.SS2.SSS1.p7.5.m5.1.1.1" xref="S3.SS2.SSS1.p7.5.m5.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p7.5.m5.1.1.1.2" xref="S3.SS2.SSS1.p7.5.m5.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS1.p7.5.m5.1.1.1.1" xref="S3.SS2.SSS1.p7.5.m5.1.1.1.1.cmml"><mi id="S3.SS2.SSS1.p7.5.m5.1.1.1.1.2" xref="S3.SS2.SSS1.p7.5.m5.1.1.1.1.2.cmml">t</mi><mo id="S3.SS2.SSS1.p7.5.m5.1.1.1.1.1" xref="S3.SS2.SSS1.p7.5.m5.1.1.1.1.1.cmml">âˆ’</mo><mn id="S3.SS2.SSS1.p7.5.m5.1.1.1.1.3" xref="S3.SS2.SSS1.p7.5.m5.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.SS2.SSS1.p7.5.m5.1.1.1.3" xref="S3.SS2.SSS1.p7.5.m5.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p7.5.m5.1b"><apply id="S3.SS2.SSS1.p7.5.m5.1.1.1.1.cmml" xref="S3.SS2.SSS1.p7.5.m5.1.1.1"><minus id="S3.SS2.SSS1.p7.5.m5.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p7.5.m5.1.1.1.1.1"></minus><ci id="S3.SS2.SSS1.p7.5.m5.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p7.5.m5.1.1.1.1.2">ğ‘¡</ci><cn type="integer" id="S3.SS2.SSS1.p7.5.m5.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p7.5.m5.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p7.5.m5.1c">(t-1)</annotation></semantics></math>-th step. After fine-tuning, the captioning model can be better suited for the prediction model.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Feedback-based Learning</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Though adapting to the prediction model, the captioning model is still unaware of the answer prediction task for VQA. Thus, we further propose construct pseudo supervision signals based on the PLM feedback from the prediction model. Since the captioning model is only involved as an intermediate component for answer prediction, we design a reinforcement learning method for optimizing it.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p"><span id="S3.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Reward From PLM Feedback.</span> A key design consideration of reinforcement learning is the definition of the reward function. In our approach, instead of only generating relevant captions for the images, the effectiveness of the captioning model should be measured by how well it helps find the correct answer. To achieve this goal, we design the following two kinds of reward signals.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p">â€¢Â <em id="S3.SS2.SSS2.p3.1.1" class="ltx_emph ltx_font_italic">Prompt-based Reward:</em> A heuristic method is utilizing the prompt in Â§<a href="#S3.SS2.SSS1" title="3.2.1 Captioning Adaptation â€£ 3.2 Language Model Guided Captioning â€£ 3 Method â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a> to instruct FLAN-T5-XXL to obtain a relevance score, and regard this relevance score as the reward signal:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.7" class="ltx_Math" alttext="r(x_{q},z)=\mathop{\arg\max}\limits_{s\in\{0,0.25,0.5,0.75\}}p(s|x_{q},z;\Theta_{P})," display="block"><semantics id="S3.E3.m1.7a"><mrow id="S3.E3.m1.7.7.1" xref="S3.E3.m1.7.7.1.1.cmml"><mrow id="S3.E3.m1.7.7.1.1" xref="S3.E3.m1.7.7.1.1.cmml"><mrow id="S3.E3.m1.7.7.1.1.1" xref="S3.E3.m1.7.7.1.1.1.cmml"><mi id="S3.E3.m1.7.7.1.1.1.3" xref="S3.E3.m1.7.7.1.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.1.1.1.2" xref="S3.E3.m1.7.7.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E3.m1.7.7.1.1.1.1.1" xref="S3.E3.m1.7.7.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m1.7.7.1.1.1.1.1.2" xref="S3.E3.m1.7.7.1.1.1.1.2.cmml">(</mo><msub id="S3.E3.m1.7.7.1.1.1.1.1.1" xref="S3.E3.m1.7.7.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.7.7.1.1.1.1.1.1.2" xref="S3.E3.m1.7.7.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E3.m1.7.7.1.1.1.1.1.1.3" xref="S3.E3.m1.7.7.1.1.1.1.1.1.3.cmml">q</mi></msub><mo id="S3.E3.m1.7.7.1.1.1.1.1.3" xref="S3.E3.m1.7.7.1.1.1.1.2.cmml">,</mo><mi id="S3.E3.m1.5.5" xref="S3.E3.m1.5.5.cmml">z</mi><mo stretchy="false" id="S3.E3.m1.7.7.1.1.1.1.1.4" xref="S3.E3.m1.7.7.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.7.7.1.1.3" xref="S3.E3.m1.7.7.1.1.3.cmml">=</mo><mrow id="S3.E3.m1.7.7.1.1.2" xref="S3.E3.m1.7.7.1.1.2.cmml"><munder id="S3.E3.m1.7.7.1.1.2.2" xref="S3.E3.m1.7.7.1.1.2.2.cmml"><mrow id="S3.E3.m1.7.7.1.1.2.2.2" xref="S3.E3.m1.7.7.1.1.2.2.2.cmml"><mi id="S3.E3.m1.7.7.1.1.2.2.2.1" xref="S3.E3.m1.7.7.1.1.2.2.2.1.cmml">arg</mi><mo lspace="0.167em" id="S3.E3.m1.7.7.1.1.2.2.2a" xref="S3.E3.m1.7.7.1.1.2.2.2.cmml">â¡</mo><mi id="S3.E3.m1.7.7.1.1.2.2.2.2" xref="S3.E3.m1.7.7.1.1.2.2.2.2.cmml">max</mi></mrow><mrow id="S3.E3.m1.4.4.4" xref="S3.E3.m1.4.4.4.cmml"><mi id="S3.E3.m1.4.4.4.6" xref="S3.E3.m1.4.4.4.6.cmml">s</mi><mo id="S3.E3.m1.4.4.4.5" xref="S3.E3.m1.4.4.4.5.cmml">âˆˆ</mo><mrow id="S3.E3.m1.4.4.4.7.2" xref="S3.E3.m1.4.4.4.7.1.cmml"><mo stretchy="false" id="S3.E3.m1.4.4.4.7.2.1" xref="S3.E3.m1.4.4.4.7.1.cmml">{</mo><mn id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">0</mn><mo id="S3.E3.m1.4.4.4.7.2.2" xref="S3.E3.m1.4.4.4.7.1.cmml">,</mo><mn id="S3.E3.m1.2.2.2.2" xref="S3.E3.m1.2.2.2.2.cmml">0.25</mn><mo id="S3.E3.m1.4.4.4.7.2.3" xref="S3.E3.m1.4.4.4.7.1.cmml">,</mo><mn id="S3.E3.m1.3.3.3.3" xref="S3.E3.m1.3.3.3.3.cmml">0.5</mn><mo id="S3.E3.m1.4.4.4.7.2.4" xref="S3.E3.m1.4.4.4.7.1.cmml">,</mo><mn id="S3.E3.m1.4.4.4.4" xref="S3.E3.m1.4.4.4.4.cmml">0.75</mn><mo stretchy="false" id="S3.E3.m1.4.4.4.7.2.5" xref="S3.E3.m1.4.4.4.7.1.cmml">}</mo></mrow></mrow></munder><mrow id="S3.E3.m1.7.7.1.1.2.1" xref="S3.E3.m1.7.7.1.1.2.1.cmml"><mi id="S3.E3.m1.7.7.1.1.2.1.3" xref="S3.E3.m1.7.7.1.1.2.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.1.1.2.1.2" xref="S3.E3.m1.7.7.1.1.2.1.2.cmml">â€‹</mo><mrow id="S3.E3.m1.7.7.1.1.2.1.1.1" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.7.7.1.1.2.1.1.1.2" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.7.7.1.1.2.1.1.1.1" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.cmml"><mi id="S3.E3.m1.7.7.1.1.2.1.1.1.1.4" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.4.cmml">s</mi><mo fence="false" id="S3.E3.m1.7.7.1.1.2.1.1.1.1.3" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.3.cmml">|</mo><mrow id="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.2" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.3.cmml"><msub id="S3.E3.m1.7.7.1.1.2.1.1.1.1.1.1.1" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.7.7.1.1.2.1.1.1.1.1.1.1.2" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E3.m1.7.7.1.1.2.1.1.1.1.1.1.1.3" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.1.1.1.3.cmml">q</mi></msub><mo id="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.2.3" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.3.cmml">,</mo><mi id="S3.E3.m1.6.6" xref="S3.E3.m1.6.6.cmml">z</mi><mo id="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.2.4" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.3.cmml">;</mo><msub id="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.2.2" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.2.2.cmml"><mi mathvariant="normal" id="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.2.2.2" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.2.2.2.cmml">Î˜</mi><mi id="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.2.2.3" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.2.2.3.cmml">P</mi></msub></mrow></mrow><mo stretchy="false" id="S3.E3.m1.7.7.1.1.2.1.1.1.3" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E3.m1.7.7.1.2" xref="S3.E3.m1.7.7.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.7b"><apply id="S3.E3.m1.7.7.1.1.cmml" xref="S3.E3.m1.7.7.1"><eq id="S3.E3.m1.7.7.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.3"></eq><apply id="S3.E3.m1.7.7.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1"><times id="S3.E3.m1.7.7.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.2"></times><ci id="S3.E3.m1.7.7.1.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.1.3">ğ‘Ÿ</ci><interval closure="open" id="S3.E3.m1.7.7.1.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1"><apply id="S3.E3.m1.7.7.1.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.7.7.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.2">ğ‘¥</ci><ci id="S3.E3.m1.7.7.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.3">ğ‘</ci></apply><ci id="S3.E3.m1.5.5.cmml" xref="S3.E3.m1.5.5">ğ‘§</ci></interval></apply><apply id="S3.E3.m1.7.7.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.2"><apply id="S3.E3.m1.7.7.1.1.2.2.cmml" xref="S3.E3.m1.7.7.1.1.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.2.2.1.cmml" xref="S3.E3.m1.7.7.1.1.2.2">subscript</csymbol><apply id="S3.E3.m1.7.7.1.1.2.2.2.cmml" xref="S3.E3.m1.7.7.1.1.2.2.2"><arg id="S3.E3.m1.7.7.1.1.2.2.2.1.cmml" xref="S3.E3.m1.7.7.1.1.2.2.2.1"></arg><max id="S3.E3.m1.7.7.1.1.2.2.2.2.cmml" xref="S3.E3.m1.7.7.1.1.2.2.2.2"></max></apply><apply id="S3.E3.m1.4.4.4.cmml" xref="S3.E3.m1.4.4.4"><in id="S3.E3.m1.4.4.4.5.cmml" xref="S3.E3.m1.4.4.4.5"></in><ci id="S3.E3.m1.4.4.4.6.cmml" xref="S3.E3.m1.4.4.4.6">ğ‘ </ci><set id="S3.E3.m1.4.4.4.7.1.cmml" xref="S3.E3.m1.4.4.4.7.2"><cn type="integer" id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">0</cn><cn type="float" id="S3.E3.m1.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2">0.25</cn><cn type="float" id="S3.E3.m1.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3">0.5</cn><cn type="float" id="S3.E3.m1.4.4.4.4.cmml" xref="S3.E3.m1.4.4.4.4">0.75</cn></set></apply></apply><apply id="S3.E3.m1.7.7.1.1.2.1.cmml" xref="S3.E3.m1.7.7.1.1.2.1"><times id="S3.E3.m1.7.7.1.1.2.1.2.cmml" xref="S3.E3.m1.7.7.1.1.2.1.2"></times><ci id="S3.E3.m1.7.7.1.1.2.1.3.cmml" xref="S3.E3.m1.7.7.1.1.2.1.3">ğ‘</ci><apply id="S3.E3.m1.7.7.1.1.2.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.2.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.7.7.1.1.2.1.1.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.3">conditional</csymbol><ci id="S3.E3.m1.7.7.1.1.2.1.1.1.1.4.cmml" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.4">ğ‘ </ci><list id="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.3.cmml" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.2"><apply id="S3.E3.m1.7.7.1.1.2.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.7.7.1.1.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.1.1.1.2">ğ‘¥</ci><ci id="S3.E3.m1.7.7.1.1.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.1.1.1.3">ğ‘</ci></apply><ci id="S3.E3.m1.6.6.cmml" xref="S3.E3.m1.6.6">ğ‘§</ci><apply id="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.2.2.1.cmml" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.2.2.2.cmml" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.2.2.2">Î˜</ci><ci id="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.2.2.3.cmml" xref="S3.E3.m1.7.7.1.1.2.1.1.1.1.2.2.2.3">ğ‘ƒ</ci></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.7c">r(x_{q},z)=\mathop{\arg\max}\limits_{s\in\{0,0.25,0.5,0.75\}}p(s|x_{q},z;\Theta_{P}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS2.p3.2" class="ltx_p">A higher score indicates a more informative caption, which is encouraged.</p>
</div>
<div id="S3.SS2.SSS2.p4" class="ltx_para">
<p id="S3.SS2.SSS2.p4.3" class="ltx_p">â€¢Â <em id="S3.SS2.SSS2.p4.3.1" class="ltx_emph ltx_font_italic">Confidence-based Reward:</em> Since there is no ground-truth answer during training, following Eq.(<a href="#S3.Ex1" title="3.1 Overview of Our Approach â€£ 3 Method â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), we employ the probability score of the predicted answer (the most confident candidate) given by the prediction model as the reward:</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.3" class="ltx_Math" alttext="r(x_{q},z)=p(\hat{y}|x_{q},z;\Theta_{P})," display="block"><semantics id="S3.E4.m1.3a"><mrow id="S3.E4.m1.3.3.1" xref="S3.E4.m1.3.3.1.1.cmml"><mrow id="S3.E4.m1.3.3.1.1" xref="S3.E4.m1.3.3.1.1.cmml"><mrow id="S3.E4.m1.3.3.1.1.1" xref="S3.E4.m1.3.3.1.1.1.cmml"><mi id="S3.E4.m1.3.3.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E4.m1.3.3.1.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.2.cmml">(</mo><msub id="S3.E4.m1.3.3.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.3.cmml">q</mi></msub><mo id="S3.E4.m1.3.3.1.1.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.1.2.cmml">,</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">z</mi><mo stretchy="false" id="S3.E4.m1.3.3.1.1.1.1.1.4" xref="S3.E4.m1.3.3.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.3.3.1.1.3" xref="S3.E4.m1.3.3.1.1.3.cmml">=</mo><mrow id="S3.E4.m1.3.3.1.1.2" xref="S3.E4.m1.3.3.1.1.2.cmml"><mi id="S3.E4.m1.3.3.1.1.2.3" xref="S3.E4.m1.3.3.1.1.2.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.1.2.2" xref="S3.E4.m1.3.3.1.1.2.2.cmml">â€‹</mo><mrow id="S3.E4.m1.3.3.1.1.2.1.1" xref="S3.E4.m1.3.3.1.1.2.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.3.3.1.1.2.1.1.2" xref="S3.E4.m1.3.3.1.1.2.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.3.3.1.1.2.1.1.1" xref="S3.E4.m1.3.3.1.1.2.1.1.1.cmml"><mover accent="true" id="S3.E4.m1.3.3.1.1.2.1.1.1.4" xref="S3.E4.m1.3.3.1.1.2.1.1.1.4.cmml"><mi id="S3.E4.m1.3.3.1.1.2.1.1.1.4.2" xref="S3.E4.m1.3.3.1.1.2.1.1.1.4.2.cmml">y</mi><mo id="S3.E4.m1.3.3.1.1.2.1.1.1.4.1" xref="S3.E4.m1.3.3.1.1.2.1.1.1.4.1.cmml">^</mo></mover><mo fence="false" id="S3.E4.m1.3.3.1.1.2.1.1.1.3" xref="S3.E4.m1.3.3.1.1.2.1.1.1.3.cmml">|</mo><mrow id="S3.E4.m1.3.3.1.1.2.1.1.1.2.2" xref="S3.E4.m1.3.3.1.1.2.1.1.1.2.3.cmml"><msub id="S3.E4.m1.3.3.1.1.2.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.2.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.3.3.1.1.2.1.1.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.2.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E4.m1.3.3.1.1.2.1.1.1.1.1.1.3" xref="S3.E4.m1.3.3.1.1.2.1.1.1.1.1.1.3.cmml">q</mi></msub><mo id="S3.E4.m1.3.3.1.1.2.1.1.1.2.2.3" xref="S3.E4.m1.3.3.1.1.2.1.1.1.2.3.cmml">,</mo><mi id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml">z</mi><mo id="S3.E4.m1.3.3.1.1.2.1.1.1.2.2.4" xref="S3.E4.m1.3.3.1.1.2.1.1.1.2.3.cmml">;</mo><msub id="S3.E4.m1.3.3.1.1.2.1.1.1.2.2.2" xref="S3.E4.m1.3.3.1.1.2.1.1.1.2.2.2.cmml"><mi mathvariant="normal" id="S3.E4.m1.3.3.1.1.2.1.1.1.2.2.2.2" xref="S3.E4.m1.3.3.1.1.2.1.1.1.2.2.2.2.cmml">Î˜</mi><mi id="S3.E4.m1.3.3.1.1.2.1.1.1.2.2.2.3" xref="S3.E4.m1.3.3.1.1.2.1.1.1.2.2.2.3.cmml">P</mi></msub></mrow></mrow><mo stretchy="false" id="S3.E4.m1.3.3.1.1.2.1.1.3" xref="S3.E4.m1.3.3.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4.m1.3.3.1.2" xref="S3.E4.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.3b"><apply id="S3.E4.m1.3.3.1.1.cmml" xref="S3.E4.m1.3.3.1"><eq id="S3.E4.m1.3.3.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.3"></eq><apply id="S3.E4.m1.3.3.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1"><times id="S3.E4.m1.3.3.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.2"></times><ci id="S3.E4.m1.3.3.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.1.3">ğ‘Ÿ</ci><interval closure="open" id="S3.E4.m1.3.3.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1"><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2">ğ‘¥</ci><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.3">ğ‘</ci></apply><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">ğ‘§</ci></interval></apply><apply id="S3.E4.m1.3.3.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.2"><times id="S3.E4.m1.3.3.1.1.2.2.cmml" xref="S3.E4.m1.3.3.1.1.2.2"></times><ci id="S3.E4.m1.3.3.1.1.2.3.cmml" xref="S3.E4.m1.3.3.1.1.2.3">ğ‘</ci><apply id="S3.E4.m1.3.3.1.1.2.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1"><csymbol cd="latexml" id="S3.E4.m1.3.3.1.1.2.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1.1.3">conditional</csymbol><apply id="S3.E4.m1.3.3.1.1.2.1.1.1.4.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1.1.4"><ci id="S3.E4.m1.3.3.1.1.2.1.1.1.4.1.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1.1.4.1">^</ci><ci id="S3.E4.m1.3.3.1.1.2.1.1.1.4.2.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1.1.4.2">ğ‘¦</ci></apply><list id="S3.E4.m1.3.3.1.1.2.1.1.1.2.3.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1.1.2.2"><apply id="S3.E4.m1.3.3.1.1.2.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.2.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.2.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1.1.1.1.1.2">ğ‘¥</ci><ci id="S3.E4.m1.3.3.1.1.2.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1.1.1.1.1.3">ğ‘</ci></apply><ci id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2">ğ‘§</ci><apply id="S3.E4.m1.3.3.1.1.2.1.1.1.2.2.2.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.2.1.1.1.2.2.2.1.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.2.1.1.1.2.2.2.2.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1.1.2.2.2.2">Î˜</ci><ci id="S3.E4.m1.3.3.1.1.2.1.1.1.2.2.2.3.cmml" xref="S3.E4.m1.3.3.1.1.2.1.1.1.2.2.2.3">ğ‘ƒ</ci></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.3c">r(x_{q},z)=p(\hat{y}|x_{q},z;\Theta_{P}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS2.p4.2" class="ltx_p">where <math id="S3.SS2.SSS2.p4.1.m1.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS2.SSS2.p4.1.m1.1a"><mi id="S3.SS2.SSS2.p4.1.m1.1.1" xref="S3.SS2.SSS2.p4.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.1.m1.1b"><ci id="S3.SS2.SSS2.p4.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p4.1.m1.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.1.m1.1c">z</annotation></semantics></math> is the generated caption by the captioning model and <math id="S3.SS2.SSS2.p4.2.m2.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="S3.SS2.SSS2.p4.2.m2.1a"><mover accent="true" id="S3.SS2.SSS2.p4.2.m2.1.1" xref="S3.SS2.SSS2.p4.2.m2.1.1.cmml"><mi id="S3.SS2.SSS2.p4.2.m2.1.1.2" xref="S3.SS2.SSS2.p4.2.m2.1.1.2.cmml">y</mi><mo id="S3.SS2.SSS2.p4.2.m2.1.1.1" xref="S3.SS2.SSS2.p4.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.2.m2.1b"><apply id="S3.SS2.SSS2.p4.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p4.2.m2.1.1"><ci id="S3.SS2.SSS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p4.2.m2.1.1.1">^</ci><ci id="S3.SS2.SSS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p4.2.m2.1.1.2">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.2.m2.1c">\hat{y}</annotation></semantics></math> is the predicted answer from the prediction model.
In this way, the PLM (<em id="S3.SS2.SSS2.p4.2.1" class="ltx_emph ltx_font_italic">i.e.,</em> the prediction model) can inform the captioning model about the informativeness of the generated caption: the larger probability score, the more informative a caption is, and vice versa. We will verify the reliability of these reward designs in Â§<a href="#S5.SS1" title="5.1 The Reliability of Feedback From PLM â€£ 5 Analysis â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>.</p>
</div>
<div id="S3.SS2.SSS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS2.p5.7" class="ltx_p"><span id="S3.SS2.SSS2.p5.7.1" class="ltx_text ltx_font_bold">Policy Gradient.</span> In the framework of reinforcement learning, caption generation can be viewed as a sequential decision-making process over the whole vocabulary space. Each generated caption with <math id="S3.SS2.SSS2.p5.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.SSS2.p5.1.m1.1a"><mi id="S3.SS2.SSS2.p5.1.m1.1.1" xref="S3.SS2.SSS2.p5.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p5.1.m1.1b"><ci id="S3.SS2.SSS2.p5.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p5.1.m1.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p5.1.m1.1c">T</annotation></semantics></math> tokens is treated as an individual episode of length <math id="S3.SS2.SSS2.p5.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.SSS2.p5.2.m2.1a"><mi id="S3.SS2.SSS2.p5.2.m2.1.1" xref="S3.SS2.SSS2.p5.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p5.2.m2.1b"><ci id="S3.SS2.SSS2.p5.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p5.2.m2.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p5.2.m2.1c">T</annotation></semantics></math> in this process. At the <math id="S3.SS2.SSS2.p5.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS2.SSS2.p5.3.m3.1a"><mi id="S3.SS2.SSS2.p5.3.m3.1.1" xref="S3.SS2.SSS2.p5.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p5.3.m3.1b"><ci id="S3.SS2.SSS2.p5.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p5.3.m3.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p5.3.m3.1c">t</annotation></semantics></math>-th time step, the state <math id="S3.SS2.SSS2.p5.4.m4.2" class="ltx_Math" alttext="(x_{i},z_{&lt;t})" display="inline"><semantics id="S3.SS2.SSS2.p5.4.m4.2a"><mrow id="S3.SS2.SSS2.p5.4.m4.2.2.2" xref="S3.SS2.SSS2.p5.4.m4.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p5.4.m4.2.2.2.3" xref="S3.SS2.SSS2.p5.4.m4.2.2.3.cmml">(</mo><msub id="S3.SS2.SSS2.p5.4.m4.1.1.1.1" xref="S3.SS2.SSS2.p5.4.m4.1.1.1.1.cmml"><mi id="S3.SS2.SSS2.p5.4.m4.1.1.1.1.2" xref="S3.SS2.SSS2.p5.4.m4.1.1.1.1.2.cmml">x</mi><mi id="S3.SS2.SSS2.p5.4.m4.1.1.1.1.3" xref="S3.SS2.SSS2.p5.4.m4.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.SSS2.p5.4.m4.2.2.2.4" xref="S3.SS2.SSS2.p5.4.m4.2.2.3.cmml">,</mo><msub id="S3.SS2.SSS2.p5.4.m4.2.2.2.2" xref="S3.SS2.SSS2.p5.4.m4.2.2.2.2.cmml"><mi id="S3.SS2.SSS2.p5.4.m4.2.2.2.2.2" xref="S3.SS2.SSS2.p5.4.m4.2.2.2.2.2.cmml">z</mi><mrow id="S3.SS2.SSS2.p5.4.m4.2.2.2.2.3" xref="S3.SS2.SSS2.p5.4.m4.2.2.2.2.3.cmml"><mi id="S3.SS2.SSS2.p5.4.m4.2.2.2.2.3.2" xref="S3.SS2.SSS2.p5.4.m4.2.2.2.2.3.2.cmml"></mi><mo id="S3.SS2.SSS2.p5.4.m4.2.2.2.2.3.1" xref="S3.SS2.SSS2.p5.4.m4.2.2.2.2.3.1.cmml">&lt;</mo><mi id="S3.SS2.SSS2.p5.4.m4.2.2.2.2.3.3" xref="S3.SS2.SSS2.p5.4.m4.2.2.2.2.3.3.cmml">t</mi></mrow></msub><mo stretchy="false" id="S3.SS2.SSS2.p5.4.m4.2.2.2.5" xref="S3.SS2.SSS2.p5.4.m4.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p5.4.m4.2b"><interval closure="open" id="S3.SS2.SSS2.p5.4.m4.2.2.3.cmml" xref="S3.SS2.SSS2.p5.4.m4.2.2.2"><apply id="S3.SS2.SSS2.p5.4.m4.1.1.1.1.cmml" xref="S3.SS2.SSS2.p5.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p5.4.m4.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p5.4.m4.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p5.4.m4.1.1.1.1.2.cmml" xref="S3.SS2.SSS2.p5.4.m4.1.1.1.1.2">ğ‘¥</ci><ci id="S3.SS2.SSS2.p5.4.m4.1.1.1.1.3.cmml" xref="S3.SS2.SSS2.p5.4.m4.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.SS2.SSS2.p5.4.m4.2.2.2.2.cmml" xref="S3.SS2.SSS2.p5.4.m4.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p5.4.m4.2.2.2.2.1.cmml" xref="S3.SS2.SSS2.p5.4.m4.2.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS2.p5.4.m4.2.2.2.2.2.cmml" xref="S3.SS2.SSS2.p5.4.m4.2.2.2.2.2">ğ‘§</ci><apply id="S3.SS2.SSS2.p5.4.m4.2.2.2.2.3.cmml" xref="S3.SS2.SSS2.p5.4.m4.2.2.2.2.3"><lt id="S3.SS2.SSS2.p5.4.m4.2.2.2.2.3.1.cmml" xref="S3.SS2.SSS2.p5.4.m4.2.2.2.2.3.1"></lt><csymbol cd="latexml" id="S3.SS2.SSS2.p5.4.m4.2.2.2.2.3.2.cmml" xref="S3.SS2.SSS2.p5.4.m4.2.2.2.2.3.2">absent</csymbol><ci id="S3.SS2.SSS2.p5.4.m4.2.2.2.2.3.3.cmml" xref="S3.SS2.SSS2.p5.4.m4.2.2.2.2.3.3">ğ‘¡</ci></apply></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p5.4.m4.2c">(x_{i},z_{&lt;t})</annotation></semantics></math> is the combination of the image and caption generated up to the <math id="S3.SS2.SSS2.p5.5.m5.1" class="ltx_Math" alttext="(t-1)" display="inline"><semantics id="S3.SS2.SSS2.p5.5.m5.1a"><mrow id="S3.SS2.SSS2.p5.5.m5.1.1.1" xref="S3.SS2.SSS2.p5.5.m5.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p5.5.m5.1.1.1.2" xref="S3.SS2.SSS2.p5.5.m5.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS2.p5.5.m5.1.1.1.1" xref="S3.SS2.SSS2.p5.5.m5.1.1.1.1.cmml"><mi id="S3.SS2.SSS2.p5.5.m5.1.1.1.1.2" xref="S3.SS2.SSS2.p5.5.m5.1.1.1.1.2.cmml">t</mi><mo id="S3.SS2.SSS2.p5.5.m5.1.1.1.1.1" xref="S3.SS2.SSS2.p5.5.m5.1.1.1.1.1.cmml">âˆ’</mo><mn id="S3.SS2.SSS2.p5.5.m5.1.1.1.1.3" xref="S3.SS2.SSS2.p5.5.m5.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.SS2.SSS2.p5.5.m5.1.1.1.3" xref="S3.SS2.SSS2.p5.5.m5.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p5.5.m5.1b"><apply id="S3.SS2.SSS2.p5.5.m5.1.1.1.1.cmml" xref="S3.SS2.SSS2.p5.5.m5.1.1.1"><minus id="S3.SS2.SSS2.p5.5.m5.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p5.5.m5.1.1.1.1.1"></minus><ci id="S3.SS2.SSS2.p5.5.m5.1.1.1.1.2.cmml" xref="S3.SS2.SSS2.p5.5.m5.1.1.1.1.2">ğ‘¡</ci><cn type="integer" id="S3.SS2.SSS2.p5.5.m5.1.1.1.1.3.cmml" xref="S3.SS2.SSS2.p5.5.m5.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p5.5.m5.1c">(t-1)</annotation></semantics></math>-th token, and the action <math id="S3.SS2.SSS2.p5.6.m6.1" class="ltx_Math" alttext="z_{t}" display="inline"><semantics id="S3.SS2.SSS2.p5.6.m6.1a"><msub id="S3.SS2.SSS2.p5.6.m6.1.1" xref="S3.SS2.SSS2.p5.6.m6.1.1.cmml"><mi id="S3.SS2.SSS2.p5.6.m6.1.1.2" xref="S3.SS2.SSS2.p5.6.m6.1.1.2.cmml">z</mi><mi id="S3.SS2.SSS2.p5.6.m6.1.1.3" xref="S3.SS2.SSS2.p5.6.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p5.6.m6.1b"><apply id="S3.SS2.SSS2.p5.6.m6.1.1.cmml" xref="S3.SS2.SSS2.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p5.6.m6.1.1.1.cmml" xref="S3.SS2.SSS2.p5.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p5.6.m6.1.1.2.cmml" xref="S3.SS2.SSS2.p5.6.m6.1.1.2">ğ‘§</ci><ci id="S3.SS2.SSS2.p5.6.m6.1.1.3.cmml" xref="S3.SS2.SSS2.p5.6.m6.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p5.6.m6.1c">z_{t}</annotation></semantics></math> is the <math id="S3.SS2.SSS2.p5.7.m7.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS2.SSS2.p5.7.m7.1a"><mi id="S3.SS2.SSS2.p5.7.m7.1.1" xref="S3.SS2.SSS2.p5.7.m7.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p5.7.m7.1b"><ci id="S3.SS2.SSS2.p5.7.m7.1.1.cmml" xref="S3.SS2.SSS2.p5.7.m7.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p5.7.m7.1c">t</annotation></semantics></math>-th token to be generated. We employ the policy gradient algorithmÂ <cite class="ltx_cite ltx_citemacro_citep">(Sutton and Barto, <a href="#bib.bib31" title="" class="ltx_ref">2018</a>)</cite> and perform gradient descent to optimize the following objective function:</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.2" class="ltx_Math" alttext="\mathcal{L}_{RL}=-\sum\limits_{t=1}^{T}r(x_{q},z)\log p(z_{t}|x_{i},z_{&lt;t};\Theta_{cap})," display="block"><semantics id="S3.E5.m1.2a"><mrow id="S3.E5.m1.2.2.1" xref="S3.E5.m1.2.2.1.1.cmml"><mrow id="S3.E5.m1.2.2.1.1" xref="S3.E5.m1.2.2.1.1.cmml"><msub id="S3.E5.m1.2.2.1.1.4" xref="S3.E5.m1.2.2.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.2.2.1.1.4.2" xref="S3.E5.m1.2.2.1.1.4.2.cmml">â„’</mi><mrow id="S3.E5.m1.2.2.1.1.4.3" xref="S3.E5.m1.2.2.1.1.4.3.cmml"><mi id="S3.E5.m1.2.2.1.1.4.3.2" xref="S3.E5.m1.2.2.1.1.4.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.2.2.1.1.4.3.1" xref="S3.E5.m1.2.2.1.1.4.3.1.cmml">â€‹</mo><mi id="S3.E5.m1.2.2.1.1.4.3.3" xref="S3.E5.m1.2.2.1.1.4.3.3.cmml">L</mi></mrow></msub><mo id="S3.E5.m1.2.2.1.1.3" xref="S3.E5.m1.2.2.1.1.3.cmml">=</mo><mrow id="S3.E5.m1.2.2.1.1.2" xref="S3.E5.m1.2.2.1.1.2.cmml"><mo id="S3.E5.m1.2.2.1.1.2a" xref="S3.E5.m1.2.2.1.1.2.cmml">âˆ’</mo><mrow id="S3.E5.m1.2.2.1.1.2.2" xref="S3.E5.m1.2.2.1.1.2.2.cmml"><munderover id="S3.E5.m1.2.2.1.1.2.2.3" xref="S3.E5.m1.2.2.1.1.2.2.3.cmml"><mo movablelimits="false" id="S3.E5.m1.2.2.1.1.2.2.3.2.2" xref="S3.E5.m1.2.2.1.1.2.2.3.2.2.cmml">âˆ‘</mo><mrow id="S3.E5.m1.2.2.1.1.2.2.3.2.3" xref="S3.E5.m1.2.2.1.1.2.2.3.2.3.cmml"><mi id="S3.E5.m1.2.2.1.1.2.2.3.2.3.2" xref="S3.E5.m1.2.2.1.1.2.2.3.2.3.2.cmml">t</mi><mo id="S3.E5.m1.2.2.1.1.2.2.3.2.3.1" xref="S3.E5.m1.2.2.1.1.2.2.3.2.3.1.cmml">=</mo><mn id="S3.E5.m1.2.2.1.1.2.2.3.2.3.3" xref="S3.E5.m1.2.2.1.1.2.2.3.2.3.3.cmml">1</mn></mrow><mi id="S3.E5.m1.2.2.1.1.2.2.3.3" xref="S3.E5.m1.2.2.1.1.2.2.3.3.cmml">T</mi></munderover><mrow id="S3.E5.m1.2.2.1.1.2.2.2" xref="S3.E5.m1.2.2.1.1.2.2.2.cmml"><mi id="S3.E5.m1.2.2.1.1.2.2.2.4" xref="S3.E5.m1.2.2.1.1.2.2.2.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.2.2.1.1.2.2.2.3" xref="S3.E5.m1.2.2.1.1.2.2.2.3.cmml">â€‹</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.2.cmml">(</mo><msub id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">q</mi></msub><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">z</mi><mo stretchy="false" id="S3.E5.m1.2.2.1.1.1.1.1.1.1.4" xref="S3.E5.m1.2.2.1.1.1.1.1.1.2.cmml">)</mo></mrow><mo lspace="0.167em" rspace="0em" id="S3.E5.m1.2.2.1.1.2.2.2.3a" xref="S3.E5.m1.2.2.1.1.2.2.2.3.cmml">â€‹</mo><mrow id="S3.E5.m1.2.2.1.1.2.2.2.5" xref="S3.E5.m1.2.2.1.1.2.2.2.5.cmml"><mi id="S3.E5.m1.2.2.1.1.2.2.2.5.1" xref="S3.E5.m1.2.2.1.1.2.2.2.5.1.cmml">log</mi><mo lspace="0.167em" id="S3.E5.m1.2.2.1.1.2.2.2.5a" xref="S3.E5.m1.2.2.1.1.2.2.2.5.cmml">â¡</mo><mi id="S3.E5.m1.2.2.1.1.2.2.2.5.2" xref="S3.E5.m1.2.2.1.1.2.2.2.5.2.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E5.m1.2.2.1.1.2.2.2.3b" xref="S3.E5.m1.2.2.1.1.2.2.2.3.cmml">â€‹</mo><mrow id="S3.E5.m1.2.2.1.1.2.2.2.2.1" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S3.E5.m1.2.2.1.1.2.2.2.2.1.2" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.cmml">(</mo><mrow id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.cmml"><msub id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.5" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.5.cmml"><mi id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.5.2" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.5.2.cmml">z</mi><mi id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.5.3" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.5.3.cmml">t</mi></msub><mo fence="false" id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.4" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.4.cmml">|</mo><mrow id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.4.cmml"><msub id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.1.1.1.cmml"><mi id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.4" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.4.cmml">,</mo><msub id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.cmml"><mi id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.2" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.2.cmml">z</mi><mrow id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.3" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.3.cmml"><mi id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.3.2" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.3.2.cmml"></mi><mo id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.3.1" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.3.1.cmml">&lt;</mo><mi id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.3.3" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.3.3.cmml">t</mi></mrow></msub><mo id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.5" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.4.cmml">;</mo><msub id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.cmml"><mi mathvariant="normal" id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.2" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.2.cmml">Î˜</mi><mrow id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.cmml"><mi id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.2" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.1" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.1.cmml">â€‹</mo><mi id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.3" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.1a" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.1.cmml">â€‹</mo><mi id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.4" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.4.cmml">p</mi></mrow></msub></mrow></mrow><mo stretchy="false" id="S3.E5.m1.2.2.1.1.2.2.2.2.1.3" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E5.m1.2.2.1.2" xref="S3.E5.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.2b"><apply id="S3.E5.m1.2.2.1.1.cmml" xref="S3.E5.m1.2.2.1"><eq id="S3.E5.m1.2.2.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.3"></eq><apply id="S3.E5.m1.2.2.1.1.4.cmml" xref="S3.E5.m1.2.2.1.1.4"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.4.1.cmml" xref="S3.E5.m1.2.2.1.1.4">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.4.2.cmml" xref="S3.E5.m1.2.2.1.1.4.2">â„’</ci><apply id="S3.E5.m1.2.2.1.1.4.3.cmml" xref="S3.E5.m1.2.2.1.1.4.3"><times id="S3.E5.m1.2.2.1.1.4.3.1.cmml" xref="S3.E5.m1.2.2.1.1.4.3.1"></times><ci id="S3.E5.m1.2.2.1.1.4.3.2.cmml" xref="S3.E5.m1.2.2.1.1.4.3.2">ğ‘…</ci><ci id="S3.E5.m1.2.2.1.1.4.3.3.cmml" xref="S3.E5.m1.2.2.1.1.4.3.3">ğ¿</ci></apply></apply><apply id="S3.E5.m1.2.2.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.2"><minus id="S3.E5.m1.2.2.1.1.2.3.cmml" xref="S3.E5.m1.2.2.1.1.2"></minus><apply id="S3.E5.m1.2.2.1.1.2.2.cmml" xref="S3.E5.m1.2.2.1.1.2.2"><apply id="S3.E5.m1.2.2.1.1.2.2.3.cmml" xref="S3.E5.m1.2.2.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.2.2.3.1.cmml" xref="S3.E5.m1.2.2.1.1.2.2.3">superscript</csymbol><apply id="S3.E5.m1.2.2.1.1.2.2.3.2.cmml" xref="S3.E5.m1.2.2.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.2.2.3.2.1.cmml" xref="S3.E5.m1.2.2.1.1.2.2.3">subscript</csymbol><sum id="S3.E5.m1.2.2.1.1.2.2.3.2.2.cmml" xref="S3.E5.m1.2.2.1.1.2.2.3.2.2"></sum><apply id="S3.E5.m1.2.2.1.1.2.2.3.2.3.cmml" xref="S3.E5.m1.2.2.1.1.2.2.3.2.3"><eq id="S3.E5.m1.2.2.1.1.2.2.3.2.3.1.cmml" xref="S3.E5.m1.2.2.1.1.2.2.3.2.3.1"></eq><ci id="S3.E5.m1.2.2.1.1.2.2.3.2.3.2.cmml" xref="S3.E5.m1.2.2.1.1.2.2.3.2.3.2">ğ‘¡</ci><cn type="integer" id="S3.E5.m1.2.2.1.1.2.2.3.2.3.3.cmml" xref="S3.E5.m1.2.2.1.1.2.2.3.2.3.3">1</cn></apply></apply><ci id="S3.E5.m1.2.2.1.1.2.2.3.3.cmml" xref="S3.E5.m1.2.2.1.1.2.2.3.3">ğ‘‡</ci></apply><apply id="S3.E5.m1.2.2.1.1.2.2.2.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2"><times id="S3.E5.m1.2.2.1.1.2.2.2.3.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.3"></times><ci id="S3.E5.m1.2.2.1.1.2.2.2.4.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.4">ğ‘Ÿ</ci><interval closure="open" id="S3.E5.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1"><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.2">ğ‘¥</ci><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.3">ğ‘</ci></apply><ci id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1">ğ‘§</ci></interval><apply id="S3.E5.m1.2.2.1.1.2.2.2.5.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.5"><log id="S3.E5.m1.2.2.1.1.2.2.2.5.1.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.5.1"></log><ci id="S3.E5.m1.2.2.1.1.2.2.2.5.2.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.5.2">ğ‘</ci></apply><apply id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1"><csymbol cd="latexml" id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.4.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.4">conditional</csymbol><apply id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.5.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.5"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.5.1.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.5">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.5.2.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.5.2">ğ‘§</ci><ci id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.5.3.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.5.3">ğ‘¡</ci></apply><list id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.4.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3"><apply id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.1.1.1.2">ğ‘¥</ci><ci id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.1.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.2.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.2">ğ‘§</ci><apply id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.3.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.3"><lt id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.3.1.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.3.1"></lt><csymbol cd="latexml" id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.3.2.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.3.2">absent</csymbol><ci id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.3.3.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.2.2.2.3.3">ğ‘¡</ci></apply></apply><apply id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.1.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.2.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.2">Î˜</ci><apply id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3"><times id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.1.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.1"></times><ci id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.2.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.2">ğ‘</ci><ci id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.3.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.3">ğ‘</ci><ci id="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.4.cmml" xref="S3.E5.m1.2.2.1.1.2.2.2.2.1.1.3.3.3.3.4">ğ‘</ci></apply></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.2c">\mathcal{L}_{RL}=-\sum\limits_{t=1}^{T}r(x_{q},z)\log p(z_{t}|x_{i},z_{&lt;t};\Theta_{cap}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS2.p5.9" class="ltx_p">where <math id="S3.SS2.SSS2.p5.8.m1.5" class="ltx_Math" alttext="z=\langle z_{1},...,z_{t},...,z_{T}\rangle" display="inline"><semantics id="S3.SS2.SSS2.p5.8.m1.5a"><mrow id="S3.SS2.SSS2.p5.8.m1.5.5" xref="S3.SS2.SSS2.p5.8.m1.5.5.cmml"><mi id="S3.SS2.SSS2.p5.8.m1.5.5.5" xref="S3.SS2.SSS2.p5.8.m1.5.5.5.cmml">z</mi><mo id="S3.SS2.SSS2.p5.8.m1.5.5.4" xref="S3.SS2.SSS2.p5.8.m1.5.5.4.cmml">=</mo><mrow id="S3.SS2.SSS2.p5.8.m1.5.5.3.3" xref="S3.SS2.SSS2.p5.8.m1.5.5.3.4.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p5.8.m1.5.5.3.3.4" xref="S3.SS2.SSS2.p5.8.m1.5.5.3.4.cmml">âŸ¨</mo><msub id="S3.SS2.SSS2.p5.8.m1.3.3.1.1.1" xref="S3.SS2.SSS2.p5.8.m1.3.3.1.1.1.cmml"><mi id="S3.SS2.SSS2.p5.8.m1.3.3.1.1.1.2" xref="S3.SS2.SSS2.p5.8.m1.3.3.1.1.1.2.cmml">z</mi><mn id="S3.SS2.SSS2.p5.8.m1.3.3.1.1.1.3" xref="S3.SS2.SSS2.p5.8.m1.3.3.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.SSS2.p5.8.m1.5.5.3.3.5" xref="S3.SS2.SSS2.p5.8.m1.5.5.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.SSS2.p5.8.m1.1.1" xref="S3.SS2.SSS2.p5.8.m1.1.1.cmml">â€¦</mi><mo id="S3.SS2.SSS2.p5.8.m1.5.5.3.3.6" xref="S3.SS2.SSS2.p5.8.m1.5.5.3.4.cmml">,</mo><msub id="S3.SS2.SSS2.p5.8.m1.4.4.2.2.2" xref="S3.SS2.SSS2.p5.8.m1.4.4.2.2.2.cmml"><mi id="S3.SS2.SSS2.p5.8.m1.4.4.2.2.2.2" xref="S3.SS2.SSS2.p5.8.m1.4.4.2.2.2.2.cmml">z</mi><mi id="S3.SS2.SSS2.p5.8.m1.4.4.2.2.2.3" xref="S3.SS2.SSS2.p5.8.m1.4.4.2.2.2.3.cmml">t</mi></msub><mo id="S3.SS2.SSS2.p5.8.m1.5.5.3.3.7" xref="S3.SS2.SSS2.p5.8.m1.5.5.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.SSS2.p5.8.m1.2.2" xref="S3.SS2.SSS2.p5.8.m1.2.2.cmml">â€¦</mi><mo id="S3.SS2.SSS2.p5.8.m1.5.5.3.3.8" xref="S3.SS2.SSS2.p5.8.m1.5.5.3.4.cmml">,</mo><msub id="S3.SS2.SSS2.p5.8.m1.5.5.3.3.3" xref="S3.SS2.SSS2.p5.8.m1.5.5.3.3.3.cmml"><mi id="S3.SS2.SSS2.p5.8.m1.5.5.3.3.3.2" xref="S3.SS2.SSS2.p5.8.m1.5.5.3.3.3.2.cmml">z</mi><mi id="S3.SS2.SSS2.p5.8.m1.5.5.3.3.3.3" xref="S3.SS2.SSS2.p5.8.m1.5.5.3.3.3.3.cmml">T</mi></msub><mo stretchy="false" id="S3.SS2.SSS2.p5.8.m1.5.5.3.3.9" xref="S3.SS2.SSS2.p5.8.m1.5.5.3.4.cmml">âŸ©</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p5.8.m1.5b"><apply id="S3.SS2.SSS2.p5.8.m1.5.5.cmml" xref="S3.SS2.SSS2.p5.8.m1.5.5"><eq id="S3.SS2.SSS2.p5.8.m1.5.5.4.cmml" xref="S3.SS2.SSS2.p5.8.m1.5.5.4"></eq><ci id="S3.SS2.SSS2.p5.8.m1.5.5.5.cmml" xref="S3.SS2.SSS2.p5.8.m1.5.5.5">ğ‘§</ci><list id="S3.SS2.SSS2.p5.8.m1.5.5.3.4.cmml" xref="S3.SS2.SSS2.p5.8.m1.5.5.3.3"><apply id="S3.SS2.SSS2.p5.8.m1.3.3.1.1.1.cmml" xref="S3.SS2.SSS2.p5.8.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p5.8.m1.3.3.1.1.1.1.cmml" xref="S3.SS2.SSS2.p5.8.m1.3.3.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p5.8.m1.3.3.1.1.1.2.cmml" xref="S3.SS2.SSS2.p5.8.m1.3.3.1.1.1.2">ğ‘§</ci><cn type="integer" id="S3.SS2.SSS2.p5.8.m1.3.3.1.1.1.3.cmml" xref="S3.SS2.SSS2.p5.8.m1.3.3.1.1.1.3">1</cn></apply><ci id="S3.SS2.SSS2.p5.8.m1.1.1.cmml" xref="S3.SS2.SSS2.p5.8.m1.1.1">â€¦</ci><apply id="S3.SS2.SSS2.p5.8.m1.4.4.2.2.2.cmml" xref="S3.SS2.SSS2.p5.8.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p5.8.m1.4.4.2.2.2.1.cmml" xref="S3.SS2.SSS2.p5.8.m1.4.4.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS2.p5.8.m1.4.4.2.2.2.2.cmml" xref="S3.SS2.SSS2.p5.8.m1.4.4.2.2.2.2">ğ‘§</ci><ci id="S3.SS2.SSS2.p5.8.m1.4.4.2.2.2.3.cmml" xref="S3.SS2.SSS2.p5.8.m1.4.4.2.2.2.3">ğ‘¡</ci></apply><ci id="S3.SS2.SSS2.p5.8.m1.2.2.cmml" xref="S3.SS2.SSS2.p5.8.m1.2.2">â€¦</ci><apply id="S3.SS2.SSS2.p5.8.m1.5.5.3.3.3.cmml" xref="S3.SS2.SSS2.p5.8.m1.5.5.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p5.8.m1.5.5.3.3.3.1.cmml" xref="S3.SS2.SSS2.p5.8.m1.5.5.3.3.3">subscript</csymbol><ci id="S3.SS2.SSS2.p5.8.m1.5.5.3.3.3.2.cmml" xref="S3.SS2.SSS2.p5.8.m1.5.5.3.3.3.2">ğ‘§</ci><ci id="S3.SS2.SSS2.p5.8.m1.5.5.3.3.3.3.cmml" xref="S3.SS2.SSS2.p5.8.m1.5.5.3.3.3.3">ğ‘‡</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p5.8.m1.5c">z=\langle z_{1},...,z_{t},...,z_{T}\rangle</annotation></semantics></math> is the caption, and <math id="S3.SS2.SSS2.p5.9.m2.2" class="ltx_Math" alttext="r(x_{q},z)" display="inline"><semantics id="S3.SS2.SSS2.p5.9.m2.2a"><mrow id="S3.SS2.SSS2.p5.9.m2.2.2" xref="S3.SS2.SSS2.p5.9.m2.2.2.cmml"><mi id="S3.SS2.SSS2.p5.9.m2.2.2.3" xref="S3.SS2.SSS2.p5.9.m2.2.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p5.9.m2.2.2.2" xref="S3.SS2.SSS2.p5.9.m2.2.2.2.cmml">â€‹</mo><mrow id="S3.SS2.SSS2.p5.9.m2.2.2.1.1" xref="S3.SS2.SSS2.p5.9.m2.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p5.9.m2.2.2.1.1.2" xref="S3.SS2.SSS2.p5.9.m2.2.2.1.2.cmml">(</mo><msub id="S3.SS2.SSS2.p5.9.m2.2.2.1.1.1" xref="S3.SS2.SSS2.p5.9.m2.2.2.1.1.1.cmml"><mi id="S3.SS2.SSS2.p5.9.m2.2.2.1.1.1.2" xref="S3.SS2.SSS2.p5.9.m2.2.2.1.1.1.2.cmml">x</mi><mi id="S3.SS2.SSS2.p5.9.m2.2.2.1.1.1.3" xref="S3.SS2.SSS2.p5.9.m2.2.2.1.1.1.3.cmml">q</mi></msub><mo id="S3.SS2.SSS2.p5.9.m2.2.2.1.1.3" xref="S3.SS2.SSS2.p5.9.m2.2.2.1.2.cmml">,</mo><mi id="S3.SS2.SSS2.p5.9.m2.1.1" xref="S3.SS2.SSS2.p5.9.m2.1.1.cmml">z</mi><mo stretchy="false" id="S3.SS2.SSS2.p5.9.m2.2.2.1.1.4" xref="S3.SS2.SSS2.p5.9.m2.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p5.9.m2.2b"><apply id="S3.SS2.SSS2.p5.9.m2.2.2.cmml" xref="S3.SS2.SSS2.p5.9.m2.2.2"><times id="S3.SS2.SSS2.p5.9.m2.2.2.2.cmml" xref="S3.SS2.SSS2.p5.9.m2.2.2.2"></times><ci id="S3.SS2.SSS2.p5.9.m2.2.2.3.cmml" xref="S3.SS2.SSS2.p5.9.m2.2.2.3">ğ‘Ÿ</ci><interval closure="open" id="S3.SS2.SSS2.p5.9.m2.2.2.1.2.cmml" xref="S3.SS2.SSS2.p5.9.m2.2.2.1.1"><apply id="S3.SS2.SSS2.p5.9.m2.2.2.1.1.1.cmml" xref="S3.SS2.SSS2.p5.9.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p5.9.m2.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS2.p5.9.m2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p5.9.m2.2.2.1.1.1.2.cmml" xref="S3.SS2.SSS2.p5.9.m2.2.2.1.1.1.2">ğ‘¥</ci><ci id="S3.SS2.SSS2.p5.9.m2.2.2.1.1.1.3.cmml" xref="S3.SS2.SSS2.p5.9.m2.2.2.1.1.1.3">ğ‘</ci></apply><ci id="S3.SS2.SSS2.p5.9.m2.1.1.cmml" xref="S3.SS2.SSS2.p5.9.m2.1.1">ğ‘§</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p5.9.m2.2c">r(x_{q},z)</annotation></semantics></math> is the reward given by the PLM. 
Finally, we jointly optimize the two loss functions:</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.1" class="ltx_Math" alttext="\mathcal{L}=(1-\alpha)\cdot\mathcal{L}_{FT}+\alpha\cdot\mathcal{L}_{RL}," display="block"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><mrow id="S3.E6.m1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.3.cmml">â„’</mi><mo id="S3.E6.m1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E6.m1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.cmml"><mrow id="S3.E6.m1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E6.m1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E6.m1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.cmml"><mn id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mi id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.cmml">Î±</mi></mrow><mo rspace="0.055em" stretchy="false" id="S3.E6.m1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.E6.m1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.2.cmml">â‹…</mo><msub id="S3.E6.m1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.1.1.1.1.1.1.3.2" xref="S3.E6.m1.1.1.1.1.1.1.3.2.cmml">â„’</mi><mrow id="S3.E6.m1.1.1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.3.3.2" xref="S3.E6.m1.1.1.1.1.1.1.3.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.1.3.3.1" xref="S3.E6.m1.1.1.1.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.1.1.3.3.3" xref="S3.E6.m1.1.1.1.1.1.1.3.3.3.cmml">T</mi></mrow></msub></mrow><mo id="S3.E6.m1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.2.cmml">+</mo><mrow id="S3.E6.m1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.3.cmml"><mi id="S3.E6.m1.1.1.1.1.1.3.2" xref="S3.E6.m1.1.1.1.1.1.3.2.cmml">Î±</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E6.m1.1.1.1.1.1.3.1" xref="S3.E6.m1.1.1.1.1.1.3.1.cmml">â‹…</mo><msub id="S3.E6.m1.1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.1.1.1.1.1.3.3.2" xref="S3.E6.m1.1.1.1.1.1.3.3.2.cmml">â„’</mi><mrow id="S3.E6.m1.1.1.1.1.1.3.3.3" xref="S3.E6.m1.1.1.1.1.1.3.3.3.cmml"><mi id="S3.E6.m1.1.1.1.1.1.3.3.3.2" xref="S3.E6.m1.1.1.1.1.1.3.3.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.1.3.3.3.1" xref="S3.E6.m1.1.1.1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.1.1.1.1.1.3.3.3.3" xref="S3.E6.m1.1.1.1.1.1.3.3.3.3.cmml">L</mi></mrow></msub></mrow></mrow></mrow><mo id="S3.E6.m1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"><eq id="S3.E6.m1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.2"></eq><ci id="S3.E6.m1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.3">â„’</ci><apply id="S3.E6.m1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1"><plus id="S3.E6.m1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.2"></plus><apply id="S3.E6.m1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1"><ci id="S3.E6.m1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2">â‹…</ci><apply id="S3.E6.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1"><minus id="S3.E6.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E6.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.2">1</cn><ci id="S3.E6.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1.1.3">ğ›¼</ci></apply><apply id="S3.E6.m1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3.2">â„’</ci><apply id="S3.E6.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3.3"><times id="S3.E6.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3.3.1"></times><ci id="S3.E6.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3.3.2">ğ¹</ci><ci id="S3.E6.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3.3.3">ğ‘‡</ci></apply></apply></apply><apply id="S3.E6.m1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.3"><ci id="S3.E6.m1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.3.1">â‹…</ci><ci id="S3.E6.m1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.3.2">ğ›¼</ci><apply id="S3.E6.m1.1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.3.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.3.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.3.3.2">â„’</ci><apply id="S3.E6.m1.1.1.1.1.1.3.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.3.3.3"><times id="S3.E6.m1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.3.3.3.1"></times><ci id="S3.E6.m1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.3.3.3.2">ğ‘…</ci><ci id="S3.E6.m1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.E6.m1.1.1.1.1.1.3.3.3.3">ğ¿</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">\mathcal{L}=(1-\alpha)\cdot\mathcal{L}_{FT}+\alpha\cdot\mathcal{L}_{RL},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS2.p5.10" class="ltx_p">where <math id="S3.SS2.SSS2.p5.10.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.SSS2.p5.10.m1.1a"><mi id="S3.SS2.SSS2.p5.10.m1.1.1" xref="S3.SS2.SSS2.p5.10.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p5.10.m1.1b"><ci id="S3.SS2.SSS2.p5.10.m1.1.1.cmml" xref="S3.SS2.SSS2.p5.10.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p5.10.m1.1c">\alpha</annotation></semantics></math> is a weight factor to balance the two parts.</p>
</div>
<div id="S3.SS2.SSS2.p6" class="ltx_para">
<p id="S3.SS2.SSS2.p6.2" class="ltx_p">To fully exploit the online feedback provided by FLAN-T5-XXL, we only optimize the captioning adaptation loss function <math id="S3.SS2.SSS2.p6.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{FT}" display="inline"><semantics id="S3.SS2.SSS2.p6.1.m1.1a"><msub id="S3.SS2.SSS2.p6.1.m1.1.1" xref="S3.SS2.SSS2.p6.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p6.1.m1.1.1.2" xref="S3.SS2.SSS2.p6.1.m1.1.1.2.cmml">â„’</mi><mrow id="S3.SS2.SSS2.p6.1.m1.1.1.3" xref="S3.SS2.SSS2.p6.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS2.p6.1.m1.1.1.3.2" xref="S3.SS2.SSS2.p6.1.m1.1.1.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p6.1.m1.1.1.3.1" xref="S3.SS2.SSS2.p6.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p6.1.m1.1.1.3.3" xref="S3.SS2.SSS2.p6.1.m1.1.1.3.3.cmml">T</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p6.1.m1.1b"><apply id="S3.SS2.SSS2.p6.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p6.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p6.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.1.2">â„’</ci><apply id="S3.SS2.SSS2.p6.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.1.3"><times id="S3.SS2.SSS2.p6.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.1.3.1"></times><ci id="S3.SS2.SSS2.p6.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.1.3.2">ğ¹</ci><ci id="S3.SS2.SSS2.p6.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.1.3.3">ğ‘‡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p6.1.m1.1c">\mathcal{L}_{FT}</annotation></semantics></math> in the initial epoch, while the reinforcement learning loss function <math id="S3.SS2.SSS2.p6.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{RL}" display="inline"><semantics id="S3.SS2.SSS2.p6.2.m2.1a"><msub id="S3.SS2.SSS2.p6.2.m2.1.1" xref="S3.SS2.SSS2.p6.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p6.2.m2.1.1.2" xref="S3.SS2.SSS2.p6.2.m2.1.1.2.cmml">â„’</mi><mrow id="S3.SS2.SSS2.p6.2.m2.1.1.3" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS2.p6.2.m2.1.1.3.2" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p6.2.m2.1.1.3.1" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS2.p6.2.m2.1.1.3.3" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.3.cmml">L</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p6.2.m2.1b"><apply id="S3.SS2.SSS2.p6.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p6.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p6.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p6.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p6.2.m2.1.1.2">â„’</ci><apply id="S3.SS2.SSS2.p6.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p6.2.m2.1.1.3"><times id="S3.SS2.SSS2.p6.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.1"></times><ci id="S3.SS2.SSS2.p6.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.2">ğ‘…</ci><ci id="S3.SS2.SSS2.p6.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS2.p6.2.m2.1.1.3.3">ğ¿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p6.2.m2.1c">\mathcal{L}_{RL}</annotation></semantics></math> is optimized throughout the training process.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Answer Prediction</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.5" class="ltx_p">At inference time, we utilize the updated captioning model to assist the prediction model in answering questions, by calculating the probability <math id="S3.SS3.p1.1.m1.2" class="ltx_Math" alttext="p(y|x_{q},z;\Theta_{P})" display="inline"><semantics id="S3.SS3.p1.1.m1.2a"><mrow id="S3.SS3.p1.1.m1.2.2" xref="S3.SS3.p1.1.m1.2.2.cmml"><mi id="S3.SS3.p1.1.m1.2.2.3" xref="S3.SS3.p1.1.m1.2.2.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.2.2.2" xref="S3.SS3.p1.1.m1.2.2.2.cmml">â€‹</mo><mrow id="S3.SS3.p1.1.m1.2.2.1.1" xref="S3.SS3.p1.1.m1.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p1.1.m1.2.2.1.1.2" xref="S3.SS3.p1.1.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS3.p1.1.m1.2.2.1.1.1" xref="S3.SS3.p1.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.2.2.1.1.1.4" xref="S3.SS3.p1.1.m1.2.2.1.1.1.4.cmml">y</mi><mo fence="false" id="S3.SS3.p1.1.m1.2.2.1.1.1.3" xref="S3.SS3.p1.1.m1.2.2.1.1.1.3.cmml">|</mo><mrow id="S3.SS3.p1.1.m1.2.2.1.1.1.2.2" xref="S3.SS3.p1.1.m1.2.2.1.1.1.2.3.cmml"><msub id="S3.SS3.p1.1.m1.2.2.1.1.1.1.1.1" xref="S3.SS3.p1.1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.2.2.1.1.1.1.1.1.2" xref="S3.SS3.p1.1.m1.2.2.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS3.p1.1.m1.2.2.1.1.1.1.1.1.3" xref="S3.SS3.p1.1.m1.2.2.1.1.1.1.1.1.3.cmml">q</mi></msub><mo id="S3.SS3.p1.1.m1.2.2.1.1.1.2.2.3" xref="S3.SS3.p1.1.m1.2.2.1.1.1.2.3.cmml">,</mo><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">z</mi><mo id="S3.SS3.p1.1.m1.2.2.1.1.1.2.2.4" xref="S3.SS3.p1.1.m1.2.2.1.1.1.2.3.cmml">;</mo><msub id="S3.SS3.p1.1.m1.2.2.1.1.1.2.2.2" xref="S3.SS3.p1.1.m1.2.2.1.1.1.2.2.2.cmml"><mi mathvariant="normal" id="S3.SS3.p1.1.m1.2.2.1.1.1.2.2.2.2" xref="S3.SS3.p1.1.m1.2.2.1.1.1.2.2.2.2.cmml">Î˜</mi><mi id="S3.SS3.p1.1.m1.2.2.1.1.1.2.2.2.3" xref="S3.SS3.p1.1.m1.2.2.1.1.1.2.2.2.3.cmml">P</mi></msub></mrow></mrow><mo stretchy="false" id="S3.SS3.p1.1.m1.2.2.1.1.3" xref="S3.SS3.p1.1.m1.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.2b"><apply id="S3.SS3.p1.1.m1.2.2.cmml" xref="S3.SS3.p1.1.m1.2.2"><times id="S3.SS3.p1.1.m1.2.2.2.cmml" xref="S3.SS3.p1.1.m1.2.2.2"></times><ci id="S3.SS3.p1.1.m1.2.2.3.cmml" xref="S3.SS3.p1.1.m1.2.2.3">ğ‘</ci><apply id="S3.SS3.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS3.p1.1.m1.2.2.1.1"><csymbol cd="latexml" id="S3.SS3.p1.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.2.2.1.1.1.3">conditional</csymbol><ci id="S3.SS3.p1.1.m1.2.2.1.1.1.4.cmml" xref="S3.SS3.p1.1.m1.2.2.1.1.1.4">ğ‘¦</ci><list id="S3.SS3.p1.1.m1.2.2.1.1.1.2.3.cmml" xref="S3.SS3.p1.1.m1.2.2.1.1.1.2.2"><apply id="S3.SS3.p1.1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.2.2.1.1.1.1.1.1.2">ğ‘¥</ci><ci id="S3.SS3.p1.1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.2.2.1.1.1.1.1.1.3">ğ‘</ci></apply><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">ğ‘§</ci><apply id="S3.SS3.p1.1.m1.2.2.1.1.1.2.2.2.cmml" xref="S3.SS3.p1.1.m1.2.2.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.2.2.1.1.1.2.2.2.1.cmml" xref="S3.SS3.p1.1.m1.2.2.1.1.1.2.2.2">subscript</csymbol><ci id="S3.SS3.p1.1.m1.2.2.1.1.1.2.2.2.2.cmml" xref="S3.SS3.p1.1.m1.2.2.1.1.1.2.2.2.2">Î˜</ci><ci id="S3.SS3.p1.1.m1.2.2.1.1.1.2.2.2.3.cmml" xref="S3.SS3.p1.1.m1.2.2.1.1.1.2.2.2.3">ğ‘ƒ</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.2c">p(y|x_{q},z;\Theta_{P})</annotation></semantics></math>.
To increase the diversity of captions and the coverage of answers, we first randomly sample <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mn id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">20</mn><mo id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">20\%</annotation></semantics></math> patches from the whole image at each time and apply top-<math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">k</annotation></semantics></math> samplingÂ <cite class="ltx_cite ltx_citemacro_citep">(Fan etÂ al., <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> to generate a caption for these patches with the updated captioning model. We repeat this process <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mi id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><ci id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">m</annotation></semantics></math> times to generate <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><mi id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><ci id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">m</annotation></semantics></math> diverse captions. Then we concatenate each of them with the corresponding question to construct the following prompt:</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p"><em id="S3.SS3.p2.1.1" class="ltx_emph ltx_font_italic">â€œPlease answer the following question.\n<span id="S3.SS3.p2.1.1.1" class="ltx_text ltx_font_typewriter">[CAPTION]</span>. <span id="S3.SS3.p2.1.1.2" class="ltx_text ltx_font_typewriter">[QUESTION]</span>â€.</em></p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p">Based on this prompt, the FLAN-T5-XXL is instructed to propose an answer with greedy decoding. We can take the max-voting strategy over all the generated answers.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">Different from previous work on learning from feedbackÂ <cite class="ltx_cite ltx_citemacro_cite">Campos and Shern (<a href="#bib.bib3" title="" class="ltx_ref">2022</a>); Wang etÂ al. (<a href="#bib.bib36" title="" class="ltx_ref">2022c</a>); Ouyang etÂ al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>, our proposed approach explores the guidance and feedback from the prediction model instead of human annotations. As we will see in Â§<a href="#S5.SS1" title="5.1 The Reliability of Feedback From PLM â€£ 5 Analysis â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>, our empirical study shows that there exists a negative correlation between the negative log likelihood assigned by a PLM and the VQA score of a generated answer. This finding suggests that the reward <math id="S3.SS3.p4.1.m1.2" class="ltx_Math" alttext="r(x_{q},z)" display="inline"><semantics id="S3.SS3.p4.1.m1.2a"><mrow id="S3.SS3.p4.1.m1.2.2" xref="S3.SS3.p4.1.m1.2.2.cmml"><mi id="S3.SS3.p4.1.m1.2.2.3" xref="S3.SS3.p4.1.m1.2.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.1.m1.2.2.2" xref="S3.SS3.p4.1.m1.2.2.2.cmml">â€‹</mo><mrow id="S3.SS3.p4.1.m1.2.2.1.1" xref="S3.SS3.p4.1.m1.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS3.p4.1.m1.2.2.1.1.2" xref="S3.SS3.p4.1.m1.2.2.1.2.cmml">(</mo><msub id="S3.SS3.p4.1.m1.2.2.1.1.1" xref="S3.SS3.p4.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS3.p4.1.m1.2.2.1.1.1.2" xref="S3.SS3.p4.1.m1.2.2.1.1.1.2.cmml">x</mi><mi id="S3.SS3.p4.1.m1.2.2.1.1.1.3" xref="S3.SS3.p4.1.m1.2.2.1.1.1.3.cmml">q</mi></msub><mo id="S3.SS3.p4.1.m1.2.2.1.1.3" xref="S3.SS3.p4.1.m1.2.2.1.2.cmml">,</mo><mi id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">z</mi><mo stretchy="false" id="S3.SS3.p4.1.m1.2.2.1.1.4" xref="S3.SS3.p4.1.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.2b"><apply id="S3.SS3.p4.1.m1.2.2.cmml" xref="S3.SS3.p4.1.m1.2.2"><times id="S3.SS3.p4.1.m1.2.2.2.cmml" xref="S3.SS3.p4.1.m1.2.2.2"></times><ci id="S3.SS3.p4.1.m1.2.2.3.cmml" xref="S3.SS3.p4.1.m1.2.2.3">ğ‘Ÿ</ci><interval closure="open" id="S3.SS3.p4.1.m1.2.2.1.2.cmml" xref="S3.SS3.p4.1.m1.2.2.1.1"><apply id="S3.SS3.p4.1.m1.2.2.1.1.1.cmml" xref="S3.SS3.p4.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS3.p4.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS3.p4.1.m1.2.2.1.1.1.2">ğ‘¥</ci><ci id="S3.SS3.p4.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS3.p4.1.m1.2.2.1.1.1.3">ğ‘</ci></apply><ci id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">ğ‘§</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.2c">r(x_{q},z)</annotation></semantics></math> given by PLM can potentially serve as a substitute for labeled data to improve the captioning model for the VQA task.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T1.1.2" class="ltx_tr">
<td id="S4.T1.1.2.1" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T1.1.2.1.1" class="ltx_text"></span> <span id="S4.T1.1.2.1.2" class="ltx_text">
<span id="S4.T1.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.1.2.1.2.1.1" class="ltx_tr">
<span id="S4.T1.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Evaluation</span></span>
<span id="S4.T1.1.2.1.2.1.2" class="ltx_tr">
<span id="S4.T1.1.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Setting</span></span>
</span></span><span id="S4.T1.1.2.1.3" class="ltx_text"></span></td>
<td id="S4.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_tt">Method</td>
<td id="S4.T1.1.2.3" class="ltx_td ltx_align_center ltx_border_tt">Parameters</td>
<td id="S4.T1.1.2.4" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T1.1.2.4.1" class="ltx_text"></span> <span id="S4.T1.1.2.4.2" class="ltx_text">
<span id="S4.T1.1.2.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.1.2.4.2.1.1" class="ltx_tr">
<span id="S4.T1.1.2.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Use</span></span>
<span id="S4.T1.1.2.4.2.1.2" class="ltx_tr">
<span id="S4.T1.1.2.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Extra PLM?</span></span>
</span></span><span id="S4.T1.1.2.4.3" class="ltx_text"></span></td>
<td id="S4.T1.1.2.5" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T1.1.2.5.1" class="ltx_text"></span> <span id="S4.T1.1.2.5.2" class="ltx_text">
<span id="S4.T1.1.2.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.1.2.5.2.1.1" class="ltx_tr">
<span id="S4.T1.1.2.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">With extra V-L</span></span>
<span id="S4.T1.1.2.5.2.1.2" class="ltx_tr">
<span id="S4.T1.1.2.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Pre-training?</span></span>
</span></span><span id="S4.T1.1.2.5.3" class="ltx_text"></span></td>
<td id="S4.T1.1.2.6" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T1.1.2.6.1" class="ltx_text"></span> <span id="S4.T1.1.2.6.2" class="ltx_text">
<span id="S4.T1.1.2.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.1.2.6.2.1.1" class="ltx_tr">
<span id="S4.T1.1.2.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">OK-VQA</span></span>
<span id="S4.T1.1.2.6.2.1.2" class="ltx_tr">
<span id="S4.T1.1.2.6.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">test</span></span>
</span></span><span id="S4.T1.1.2.6.3" class="ltx_text"></span></td>
<td id="S4.T1.1.2.7" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T1.1.2.7.1" class="ltx_text"></span> <span id="S4.T1.1.2.7.2" class="ltx_text">
<span id="S4.T1.1.2.7.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.1.2.7.2.1.1" class="ltx_tr">
<span id="S4.T1.1.2.7.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">A-OKVQA</span></span>
<span id="S4.T1.1.2.7.2.1.2" class="ltx_tr">
<span id="S4.T1.1.2.7.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">val</span></span>
</span></span><span id="S4.T1.1.2.7.3" class="ltx_text"></span></td>
</tr>
<tr id="S4.T1.1.3" class="ltx_tr">
<td id="S4.T1.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-bottom:4.30554pt;" colspan="7"><span id="S4.T1.1.3.1.1" class="ltx_text ltx_font_bold">Models fine-tuned on training set.</span></td>
</tr>
<tr id="S4.T1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.2" class="ltx_td ltx_align_center" rowspan="2"><span id="S4.T1.1.1.2.1" class="ltx_text"><span id="S4.T1.1.1.2.1.1" class="ltx_text"></span> <span id="S4.T1.1.1.2.1.2" class="ltx_text">
<span id="S4.T1.1.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.1.1.2.1.2.1.1" class="ltx_tr">
<span id="S4.T1.1.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Supervised</span></span>
<span id="S4.T1.1.1.2.1.2.1.2" class="ltx_tr">
<span id="S4.T1.1.1.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">learning</span></span>
</span></span> <span id="S4.T1.1.1.2.1.3" class="ltx_text"></span></span></td>
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_center">BLIP<sup id="S4.T1.1.1.1.1" class="ltx_sup"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_italic">â€ </span></sup>
</td>
<td id="S4.T1.1.1.3" class="ltx_td ltx_align_center">226M</td>
<td id="S4.T1.1.1.4" class="ltx_td ltx_align_center">âœ—</td>
<td id="S4.T1.1.1.5" class="ltx_td ltx_align_center">âœ—</td>
<td id="S4.T1.1.1.6" class="ltx_td ltx_align_center">37.6</td>
<td id="S4.T1.1.1.7" class="ltx_td ltx_align_center">38.5</td>
</tr>
<tr id="S4.T1.1.4" class="ltx_tr">
<td id="S4.T1.1.4.1" class="ltx_td ltx_align_center">
<span id="S4.T1.1.4.1.1" class="ltx_text"></span> <span id="S4.T1.1.4.1.2" class="ltx_text">
<span id="S4.T1.1.4.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.1.4.1.2.1.1" class="ltx_tr">
<span id="S4.T1.1.4.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">PromptCap</span></span>
</span></span><span id="S4.T1.1.4.1.3" class="ltx_text"></span></td>
<td id="S4.T1.1.4.2" class="ltx_td ltx_align_center">175B</td>
<td id="S4.T1.1.4.3" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.4.4" class="ltx_td ltx_align_center">âœ—</td>
<td id="S4.T1.1.4.5" class="ltx_td ltx_align_center">58.8</td>
<td id="S4.T1.1.4.6" class="ltx_td ltx_align_center">58.0</td>
</tr>
<tr id="S4.T1.1.5" class="ltx_tr">
<td id="S4.T1.1.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-bottom:4.30554pt;" colspan="7"><span id="S4.T1.1.5.1.1" class="ltx_text ltx_font_bold">Models without fine-tuning.</span></td>
</tr>
<tr id="S4.T1.1.6" class="ltx_tr">
<td id="S4.T1.1.6.1" class="ltx_td ltx_align_center" rowspan="3"><span id="S4.T1.1.6.1.1" class="ltx_text">Few-shot</span></td>
<td id="S4.T1.1.6.2" class="ltx_td ltx_align_center">FewVLM<sub id="S4.T1.1.6.2.1" class="ltx_sub">base</sub>
</td>
<td id="S4.T1.1.6.3" class="ltx_td ltx_align_center">288M</td>
<td id="S4.T1.1.6.4" class="ltx_td ltx_align_center">âœ—</td>
<td id="S4.T1.1.6.5" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.6.6" class="ltx_td ltx_align_center">15.0</td>
<td id="S4.T1.1.6.7" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T1.1.7" class="ltx_tr">
<td id="S4.T1.1.7.1" class="ltx_td ltx_align_center">FewVLM<sub id="S4.T1.1.7.1.1" class="ltx_sub">large</sub>
</td>
<td id="S4.T1.1.7.2" class="ltx_td ltx_align_center">804M</td>
<td id="S4.T1.1.7.3" class="ltx_td ltx_align_center">âœ—</td>
<td id="S4.T1.1.7.4" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.7.5" class="ltx_td ltx_align_center">23.1</td>
<td id="S4.T1.1.7.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T1.1.8" class="ltx_tr">
<td id="S4.T1.1.8.1" class="ltx_td ltx_align_center">PICa</td>
<td id="S4.T1.1.8.2" class="ltx_td ltx_align_center">175B</td>
<td id="S4.T1.1.8.3" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.8.4" class="ltx_td ltx_align_center">âœ—</td>
<td id="S4.T1.1.8.5" class="ltx_td ltx_align_center">48.0</td>
<td id="S4.T1.1.8.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T1.1.9" class="ltx_tr">
<td id="S4.T1.1.9.1" class="ltx_td ltx_align_center ltx_border_bb" rowspan="12"><span id="S4.T1.1.9.1.1" class="ltx_text">Zero-shot</span></td>
<td id="S4.T1.1.9.2" class="ltx_td ltx_align_center ltx_border_t">VL-T5<sub id="S4.T1.1.9.2.1" class="ltx_sub">no-VQA</sub>
</td>
<td id="S4.T1.1.9.3" class="ltx_td ltx_align_center ltx_border_t">288M</td>
<td id="S4.T1.1.9.4" class="ltx_td ltx_align_center ltx_border_t">âœ—</td>
<td id="S4.T1.1.9.5" class="ltx_td ltx_align_center ltx_border_t">âœ”</td>
<td id="S4.T1.1.9.6" class="ltx_td ltx_align_center ltx_border_t">5.8</td>
<td id="S4.T1.1.9.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T1.1.10" class="ltx_tr">
<td id="S4.T1.1.10.1" class="ltx_td ltx_align_center">VLKD<sub id="S4.T1.1.10.1.1" class="ltx_sub">ViT-B/16</sub>
</td>
<td id="S4.T1.1.10.2" class="ltx_td ltx_align_center">494M</td>
<td id="S4.T1.1.10.3" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.10.4" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.10.5" class="ltx_td ltx_align_center">10.5</td>
<td id="S4.T1.1.10.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T1.1.11" class="ltx_tr">
<td id="S4.T1.1.11.1" class="ltx_td ltx_align_center">VLKD<sub id="S4.T1.1.11.1.1" class="ltx_sub">ViT-B-L/14</sub>
</td>
<td id="S4.T1.1.11.2" class="ltx_td ltx_align_center">713M</td>
<td id="S4.T1.1.11.3" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.11.4" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.11.5" class="ltx_td ltx_align_center">13.3</td>
<td id="S4.T1.1.11.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T1.1.12" class="ltx_tr">
<td id="S4.T1.1.12.1" class="ltx_td ltx_align_center">Flamingo<sub id="S4.T1.1.12.1.1" class="ltx_sub">3B</sub>
</td>
<td id="S4.T1.1.12.2" class="ltx_td ltx_align_center">3B</td>
<td id="S4.T1.1.12.3" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.12.4" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.12.5" class="ltx_td ltx_align_center">41.2</td>
<td id="S4.T1.1.12.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T1.1.13" class="ltx_tr">
<td id="S4.T1.1.13.1" class="ltx_td ltx_align_center">Flamingo<sub id="S4.T1.1.13.1.1" class="ltx_sub">9B</sub>
</td>
<td id="S4.T1.1.13.2" class="ltx_td ltx_align_center">9B</td>
<td id="S4.T1.1.13.3" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.13.4" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.13.5" class="ltx_td ltx_align_center">44.7</td>
<td id="S4.T1.1.13.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T1.1.14" class="ltx_tr">
<td id="S4.T1.1.14.1" class="ltx_td ltx_align_center">Flamingo<sub id="S4.T1.1.14.1.1" class="ltx_sub">80B</sub>
</td>
<td id="S4.T1.1.14.2" class="ltx_td ltx_align_center">80B</td>
<td id="S4.T1.1.14.3" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.14.4" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.14.5" class="ltx_td ltx_align_center">50.6</td>
<td id="S4.T1.1.14.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T1.1.15" class="ltx_tr">
<td id="S4.T1.1.15.1" class="ltx_td ltx_align_center">Frozen</td>
<td id="S4.T1.1.15.2" class="ltx_td ltx_align_center">7B</td>
<td id="S4.T1.1.15.3" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.15.4" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.15.5" class="ltx_td ltx_align_center">5.9</td>
<td id="S4.T1.1.15.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T1.1.16" class="ltx_tr">
<td id="S4.T1.1.16.1" class="ltx_td ltx_align_center">PNP-VQA<sub id="S4.T1.1.16.1.1" class="ltx_sub">3B</sub>
</td>
<td id="S4.T1.1.16.2" class="ltx_td ltx_align_center">3.9B</td>
<td id="S4.T1.1.16.3" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.16.4" class="ltx_td ltx_align_center">âœ—</td>
<td id="S4.T1.1.16.5" class="ltx_td ltx_align_center">34.1</td>
<td id="S4.T1.1.16.6" class="ltx_td ltx_align_center">33.4</td>
</tr>
<tr id="S4.T1.1.17" class="ltx_tr">
<td id="S4.T1.1.17.1" class="ltx_td ltx_align_center">PNP-VQA<sub id="S4.T1.1.17.1.1" class="ltx_sub">11B</sub>
</td>
<td id="S4.T1.1.17.2" class="ltx_td ltx_align_center">11.9B</td>
<td id="S4.T1.1.17.3" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.17.4" class="ltx_td ltx_align_center">âœ—</td>
<td id="S4.T1.1.17.5" class="ltx_td ltx_align_center">35.9</td>
<td id="S4.T1.1.17.6" class="ltx_td ltx_align_center">36.0</td>
</tr>
<tr id="S4.T1.1.18" class="ltx_tr">
<td id="S4.T1.1.18.1" class="ltx_td ltx_align_center">Img2Prompt<sub id="S4.T1.1.18.1.1" class="ltx_sub">6.7B</sub>
</td>
<td id="S4.T1.1.18.2" class="ltx_td ltx_align_center">8.3B</td>
<td id="S4.T1.1.18.3" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.18.4" class="ltx_td ltx_align_center">âœ—</td>
<td id="S4.T1.1.18.5" class="ltx_td ltx_align_center">38.2</td>
<td id="S4.T1.1.18.6" class="ltx_td ltx_align_center">33.3</td>
</tr>
<tr id="S4.T1.1.19" class="ltx_tr">
<td id="S4.T1.1.19.1" class="ltx_td ltx_align_center">Img2Prompt<sub id="S4.T1.1.19.1.1" class="ltx_sub">13B</sub>
</td>
<td id="S4.T1.1.19.2" class="ltx_td ltx_align_center">14.6B</td>
<td id="S4.T1.1.19.3" class="ltx_td ltx_align_center">âœ”</td>
<td id="S4.T1.1.19.4" class="ltx_td ltx_align_center">âœ—</td>
<td id="S4.T1.1.19.5" class="ltx_td ltx_align_center">39.9</td>
<td id="S4.T1.1.19.6" class="ltx_td ltx_align_center">33.3</td>
</tr>
<tr id="S4.T1.1.20" class="ltx_tr">
<td id="S4.T1.1.20.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.20.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Lamoc<sub id="S4.T1.1.20.1.1.1" class="ltx_sub"><span id="S4.T1.1.20.1.1.1.1" class="ltx_text ltx_font_upright">11B</span></sub><span id="S4.T1.1.20.1.1.2" class="ltx_text ltx_font_upright">Â (Ours)</span></span></td>
<td id="S4.T1.1.20.2" class="ltx_td ltx_align_center ltx_border_bb">11.4B</td>
<td id="S4.T1.1.20.3" class="ltx_td ltx_align_center ltx_border_bb">âœ”</td>
<td id="S4.T1.1.20.4" class="ltx_td ltx_align_center ltx_border_bb">âœ—</td>
<td id="S4.T1.1.20.5" class="ltx_td ltx_align_center ltx_border_bb">40.3</td>
<td id="S4.T1.1.20.6" class="ltx_td ltx_align_center ltx_border_bb">37.9</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>
Results on OK-VQA and A-OKVQA. The methods are categorized by whether they use extra PLM and whether carry out V-L pre-training. The methods in the upper part have been fine-tuned on the training set, while those in the middle and bottom parts have not. All methods using extra PLM keep it frozen. <sup id="S4.T1.5.1" class="ltx_sup"><span id="S4.T1.5.1.1" class="ltx_text ltx_font_italic">â€ </span></sup> Instead of first fine-tuning BLIP on VQAv2 and then performing task-specific fine-tuning, we directly fine-tune BLIP on two target datasets for a fair comparison.
</figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section shows the experimental setup and then highlights the main conclusions of our results.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Task Datasets.</span>
Since our goal is to improve the performance of PLMs on visual commonsense tasks, we choose two knowledge-based VQA datasets to evaluate our method: (1) <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_bold">OK-VQA</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Marino etÂ al., <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite> contains 5,046 questions in the test set that require external knowledge resources to answer. (2) <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_bold">A-OKVQA</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Schwenk etÂ al., <a href="#bib.bib30" title="" class="ltx_ref">2022</a>)</cite> is an augmented dataset based on OK-VQA, which requires additional types of world knowledge compared to OK-VQA. Since the test set of A-OKVQA is not public, we evaluate our method on the validation set. We do not test on VQAv2Â <cite class="ltx_cite ltx_citemacro_citep">(Goyal etÂ al., <a href="#bib.bib10" title="" class="ltx_ref">2017</a>)</cite> because the majority of questions in this dataset are largely focused on recognition and simple visual detection tasks, which can be done without much logical reasoning or external knowledge, and a fine-tuned VLP model could obtain surprising resultsÂ <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a href="#bib.bib35" title="" class="ltx_ref">2022b</a>, <a href="#bib.bib34" title="" class="ltx_ref">a</a>)</cite>.
We do not use training data to make a fair comparison with other methods.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Baselines.</span>
We divide previous methods into two categories: (1) <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_bold">Methods without extra large-scale Vision-LanguageÂ (V-L) pre-training</span>, which means the models have not been pre-trained on large-scale V-L datasets, including PICaÂ <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al., <a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite>, PNP-VQAÂ <cite class="ltx_cite ltx_citemacro_citep">(Tiong etÂ al., <a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite>, Img2PromptÂ <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>. <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_smallcaps">Lamoc</span> also belongs to this category. (2) <span id="S4.SS1.p2.1.4" class="ltx_text ltx_font_bold">Methods with extra large-scale V-L pre-training</span>, which means that the PLM and the vision encoder are jointly trained on V-L datasetsÂ (although the PLM may be fixed, it obtains the ability to understand images), including VL-T5Â <cite class="ltx_cite ltx_citemacro_citep">(Cho etÂ al., <a href="#bib.bib4" title="" class="ltx_ref">2021</a>)</cite>, FewVLMÂ <cite class="ltx_cite ltx_citemacro_citep">(Jin etÂ al., <a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite>, VLKDÂ <cite class="ltx_cite ltx_citemacro_citep">(Dai etÂ al., <a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>, FrozenÂ <cite class="ltx_cite ltx_citemacro_citep">(Tsimpoukelli etÂ al., <a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite>, and FlamingoÂ <cite class="ltx_cite ltx_citemacro_citep">(Alayrac etÂ al., <a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite>.
The above methods do not use or use few labeled data (zero-shot/few-shot).
Besides, we include two methods, <em id="S4.SS1.p2.1.5" class="ltx_emph ltx_font_italic">i.e.,</em> BLIPÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2022b</a>)</cite> and PromptCapÂ <cite class="ltx_cite ltx_citemacro_cite">Hu etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite>, which are fine-tuned on large amounts of labeled data.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Implementation details.</span> For image captioning, we adopt BLIPÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a href="#bib.bib19" title="" class="ltx_ref">2022b</a>)</cite> with 446M parameters and load the released checkpoint that has been fine-tuned on the COCO 2014 training setÂ <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a href="#bib.bib22" title="" class="ltx_ref">2014</a>)</cite>, which has no overlap with both the OK-VQA and A-OKVQA evaluation datasets. For the PLM, we utilize FLAN-T5-XXLÂ <cite class="ltx_cite ltx_citemacro_citep">(Wei etÂ al., <a href="#bib.bib38" title="" class="ltx_ref">2022</a>)</cite>, which has been fine-tuned on more than 1,800 tasks through instructions and stores considerable world knowledge. We also carry out experiments on PLMs with other sizes, from 223M to 11B parameters, to demonstrate the robustness and generalizability of our approach across PLMs with different sizes. It is noteworthy that the informative caption dataset used in the captioning adaptation stage is selected by FLAN-T5-XXL, because the relevance score given by smaller models is not reliable, as will be illustrated in Â§<a href="#S5.SS1" title="5.1 The Reliability of Feedback From PLM â€£ 5 Analysis â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>. When training the captioning model, we select 1,000 (image, question) pairs without labels from VQAv2 (about <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mn id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">10</mn><mo id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">10\%</annotation></semantics></math> of the amount of training data for our target datasets), which has no overlap with the OK-VQA and A-OKVQA. It is worth noting that these 1,000 image-question pairs can be sampled from any datasets or even be generated, we sample from VQAv2 for the sake of reproducibility. The answers are generated by the PLM auto-regressively, without access to the pre-defined answer list. We conduct experiments with 5 random seeds and report the average VQA score according to official evaluation protocols.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">TableÂ <a href="#S4.T1" title="Table 1 â€£ 4 Experiment â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> displays the results of our methods and baselines on OK-VQA and A-OKVQA.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">First, <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_smallcaps">Lamoc</span> outperforms PNP-VQA on both datasets, a strong zero-shot baseline without large-scale V-L pre-training. Compared to PNP-VQA and Img2Prompt with similar model sizes, <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_smallcaps">Lamoc</span> achieves prominent gains on the challenging A-OKVQA dataset (38.2 vs 36.0 and 38.2 vs 33.3). These significant achievements clearly show the effectiveness of our method. However, the improvements on OK-VQA are marginal. This is because questions in OK-VQA do not require too much reasoningÂ <cite class="ltx_cite ltx_citemacro_cite">Schwenk etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2022</a>)</cite>, and the answers are likely to be contained in general captions. Therefore, some baselines without training from feedback also achieve comparable or even better results.

While, since Flamingo has been trained on a massive V-L dataset, it achieves the best performance among zero-shot methods. It has been reported that large-scale V-L pretraining can develop a mapping between images and knowledge concepts that can aid in knowledge-based VQAÂ <cite class="ltx_cite ltx_citemacro_citep">(Tiong etÂ al., <a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite>. Compared to these baselines, our approach does not require additional image-question matching or question generation modules, thus speeding up the inference speed.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Second, <span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_smallcaps">Lamoc</span> narrows the gap between methods with and without fine-tuning, and even achieves comparable results with the fine-tuned VLP model, <em id="S4.SS2.p3.1.2" class="ltx_emph ltx_font_italic">i.e.,</em> BLIP. For example, the performance gap between PNP-VQA<sub id="S4.SS2.p3.1.3" class="ltx_sub">11B</sub> and BLIP is 2.5, and has been decreased to 0.3 by <span id="S4.SS2.p3.1.4" class="ltx_text ltx_font_smallcaps">Lamoc</span>, which implies the importance of language model feedback.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Finally, we report the results of our methods with different model sizes in TableÂ <a href="#S5.T2" title="Table 2 â€£ 5.2 The Effectiveness of Two-stage Training â€£ 5 Analysis â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. When increasing the model scale from 223M to 11B, we observe a 1-4 point improvement in VQA scores on the challenging A-OKVQA dataset. This indicates that a larger PLM can not only store more world knowledge to assist with question answering, but also provide more accurate feedback to refine the captioning model. This is further supported by the ablation study in Â§<a href="#S5.SS1" title="5.1 The Reliability of Feedback From PLM â€£ 5 Analysis â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>The Reliability of Feedback From PLM</h3>

<figure id="S5.F3" class="ltx_figure"><img src="/html/2305.17006/assets/x3.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="92" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The relationship between the captionâ€™s reward and the corresponding answerâ€™s VQA score on A-OKVQA validation set. Figure (a) reflects the reliability of prompt-based reward while figure (b) reflects the reliability of confidence-based reward.</figcaption>
</figure>
<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The main idea of our work is leveraging the feedback of a PLM to guide caption generation, so a critical aspect is the reliability of the feedback. <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">Lamoc</span> involves two types of feedback: (1) prompt-based reward and (2) confidence-based reward, which will be evaluated independently.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">To evaluate the reliability of the first type of feedback, we analyze the relation between the VQA score and the relevance score provided by the PLM on A-OKVQA validation setÂ (FigureÂ <a href="#S5.F3" title="Figure 3 â€£ 5.1 The Reliability of Feedback From PLM â€£ 5 Analysis â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(a)). We can observe that as the relevance score provided by FLAN-T5-XXL increases, the VQA score also increases, indicating that FLAN-T5-XXL is a suitable prediction model for providing accurate feedback and the relevance scores can be regarded as reward signals. However, this trend is not observed for the other three models, implying that their feedback is unreliable. As a result, we only use FLAN-T5-XXL to select informative captions during captioning adaptation.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">To evaluate the reliability of the second type of feedback, we prompt FLAN-T5 to answer the question conditioned on the captions and plot the relationship between the negative log-likelihood (NLL) of the generated answer and its corresponding VQA score. As FigureÂ <a href="#S5.F3" title="Figure 3 â€£ 5.1 The Reliability of Feedback From PLM â€£ 5 Analysis â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(b) shows, there is a negative correlation between the NLL of the generated answers and their VQA scores, suggesting that captions with lower NLL are more informative and relevant to the questions. Therefore, the probability of the generated answer is a reliable feedback and can be used as the reward signal during reinforcement learning.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>The Effectiveness of Two-stage Training</h3>

<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T2.1.1" class="ltx_tr">
<td id="S5.T2.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding:0.5pt 4.0pt;" rowspan="2"><span id="S5.T2.1.1.1.1" class="ltx_text">Method</span></td>
<td id="S5.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 4.0pt;" colspan="2">FLAN-T5-base (223M)</td>
<td id="S5.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 4.0pt;" colspan="2">FLAN-T5-large (738M)</td>
<td id="S5.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 4.0pt;" colspan="2">FLAN-T5-XL (3B)</td>
<td id="S5.T2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.5pt 4.0pt;" colspan="2">FLAN-T5-XXL (11B)</td>
</tr>
<tr id="S5.T2.1.2" class="ltx_tr">
<td id="S5.T2.1.2.1" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">OK-VQA</td>
<td id="S5.T2.1.2.2" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">A-OKVQA</td>
<td id="S5.T2.1.2.3" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">OK-VQA</td>
<td id="S5.T2.1.2.4" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">A-OKVQA</td>
<td id="S5.T2.1.2.5" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">OK-VQA</td>
<td id="S5.T2.1.2.6" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">A-OKVQA</td>
<td id="S5.T2.1.2.7" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">OK-VQA</td>
<td id="S5.T2.1.2.8" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">A-OKVQA</td>
</tr>
<tr id="S5.T2.1.3" class="ltx_tr">
<td id="S5.T2.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.5pt 4.0pt;">BLIP caption</td>
<td id="S5.T2.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 4.0pt;">20.42</td>
<td id="S5.T2.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 4.0pt;">19.46</td>
<td id="S5.T2.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 4.0pt;">23.86</td>
<td id="S5.T2.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 4.0pt;">28.86</td>
<td id="S5.T2.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 4.0pt;">32.36</td>
<td id="S5.T2.1.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 4.0pt;">31.21</td>
<td id="S5.T2.1.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 4.0pt;">38.48</td>
<td id="S5.T2.1.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 4.0pt;">35.06</td>
</tr>
<tr id="S5.T2.1.4" class="ltx_tr">
<td id="S5.T2.1.4.1" class="ltx_td ltx_align_left" style="padding:0.5pt 4.0pt;">+ adaptation</td>
<td id="S5.T2.1.4.2" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">19.72</td>
<td id="S5.T2.1.4.3" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">18.71</td>
<td id="S5.T2.1.4.4" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;"><span id="S5.T2.1.4.4.1" class="ltx_text ltx_font_bold">27.43</span></td>
<td id="S5.T2.1.4.5" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">29.19</td>
<td id="S5.T2.1.4.6" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">32.22</td>
<td id="S5.T2.1.4.7" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">31.07</td>
<td id="S5.T2.1.4.8" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">38.35</td>
<td id="S5.T2.1.4.9" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">35.30</td>
</tr>
<tr id="S5.T2.1.5" class="ltx_tr">
<td id="S5.T2.1.5.1" class="ltx_td ltx_align_left" style="padding:0.5pt 4.0pt;">+ RL (prompt)</td>
<td id="S5.T2.1.5.2" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;"><span id="S5.T2.1.5.2.1" class="ltx_text ltx_font_bold">21.24</span></td>
<td id="S5.T2.1.5.3" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">19.25</td>
<td id="S5.T2.1.5.4" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">27.29</td>
<td id="S5.T2.1.5.5" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">29.73</td>
<td id="S5.T2.1.5.6" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">32.28</td>
<td id="S5.T2.1.5.7" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">30.63</td>
<td id="S5.T2.1.5.8" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">38.74</td>
<td id="S5.T2.1.5.9" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">37.62</td>
</tr>
<tr id="S5.T2.1.6" class="ltx_tr">
<td id="S5.T2.1.6.1" class="ltx_td ltx_align_left" style="padding:0.5pt 4.0pt;">+ RL (confidence)</td>
<td id="S5.T2.1.6.2" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">21.14</td>
<td id="S5.T2.1.6.3" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">19.74</td>
<td id="S5.T2.1.6.4" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">25.09</td>
<td id="S5.T2.1.6.5" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">28.98</td>
<td id="S5.T2.1.6.6" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;">32.02</td>
<td id="S5.T2.1.6.7" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;"><span id="S5.T2.1.6.7.1" class="ltx_text ltx_font_bold">32.10</span></td>
<td id="S5.T2.1.6.8" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;"><span id="S5.T2.1.6.8.1" class="ltx_text ltx_font_bold">40.31</span></td>
<td id="S5.T2.1.6.9" class="ltx_td ltx_align_center" style="padding:0.5pt 4.0pt;"><span id="S5.T2.1.6.9.1" class="ltx_text ltx_font_bold">37.85</span></td>
</tr>
<tr id="S5.T2.1.7" class="ltx_tr">
<td id="S5.T2.1.7.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding:0.5pt 4.0pt;">+ adaptation + RL</td>
<td id="S5.T2.1.7.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 4.0pt;">19.72</td>
<td id="S5.T2.1.7.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 4.0pt;"><span id="S5.T2.1.7.3.1" class="ltx_text ltx_font_bold">20.63</span></td>
<td id="S5.T2.1.7.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 4.0pt;">24.82</td>
<td id="S5.T2.1.7.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 4.0pt;"><span id="S5.T2.1.7.5.1" class="ltx_text ltx_font_bold">29.84</span></td>
<td id="S5.T2.1.7.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 4.0pt;"><span id="S5.T2.1.7.6.1" class="ltx_text ltx_font_bold">32.77</span></td>
<td id="S5.T2.1.7.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 4.0pt;">32.00</td>
<td id="S5.T2.1.7.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 4.0pt;">39.72</td>
<td id="S5.T2.1.7.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.5pt 4.0pt;">37.09</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>
Results of different model sizes and different training objectives. â€œBLIP captionâ€ means feeding captions generated by BLIP to PLM without captioning adaptation and feedback-based learning. â€œadaptationâ€ means captioning adaptation, while â€œRLÂ (prompt)â€ means RL with prompt-based reward, and â€œRLÂ (confidence)â€ means RL with confidence-based reward.
</figcaption>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">When training the captioning model, we adopt two gradual training stages: captioning adaptation and feedback-based learning. In this part, we study the effectiveness of this training strategy and explore whether one training stage is more effective than the other. As illustrated in TableÂ <a href="#S5.T2" title="Table 2 â€£ 5.2 The Effectiveness of Two-stage Training â€£ 5 Analysis â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, different models benefit from different training objectives. For example, the captioning adaptation stage is more beneficial for FLAN-T5-large, leading to an improvement of about 4 points on OK-VQA. On the other hand, FLAN-T5-XXL benefits the most from reinforcement learning with prompt-based rewards and obtains more than 4 points improvement on A-OKVQA. Moreover, the results show that jointly training the two objectives further boosts performance, highlighting the effectiveness of the proposed two-stage training approach.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Case Study</h3>

<figure id="S5.F4" class="ltx_figure"><img src="/html/2305.17006/assets/x4.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="233" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Example captions and predictions generated by BLIP and <span id="S5.F4.2.1" class="ltx_text ltx_font_smallcaps">Lamoc</span>.</figcaption>
</figure>
<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">FigureÂ <a href="#S5.F4" title="Figure 4 â€£ 5.3 Case Study â€£ 5 Analysis â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> displays three instances of the captions generated by BLIP and <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_smallcaps">Lamoc</span>, along with the corresponding answers generated by FLAN-T5-XXL. Since <span id="S5.SS3.p1.1.2" class="ltx_text ltx_font_smallcaps">Lamoc</span> is trained on the basis of BLIP, the difference can reflect the effect of our method. As can be observed, the captions generated by <span id="S5.SS3.p1.1.3" class="ltx_text ltx_font_smallcaps">Lamoc</span> are longer and more comprehensive, containing key information relevant to the question. For example, in FigureÂ <a href="#S5.F4" title="Figure 4 â€£ 5.3 Case Study â€£ 5 Analysis â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(a), <span id="S5.SS3.p1.1.4" class="ltx_text ltx_font_smallcaps">Lamoc</span> generates captions that include specific details such as â€œ<span id="S5.SS3.p1.1.5" class="ltx_text ltx_font_italic">frosting</span>â€ and â€œ<span id="S5.SS3.p1.1.6" class="ltx_text ltx_font_italic">chocolate</span>â€, while BLIP only generates general captions about â€œ<span id="S5.SS3.p1.1.7" class="ltx_text ltx_font_italic">donuts</span>â€ and â€œ<span id="S5.SS3.p1.1.8" class="ltx_text ltx_font_italic">box</span>â€, without sufficient information to help answer the question. These results highlight the importance of training the captioning model under the guidance of PLMs.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">One concern is that the PLM may generate correct answers due to the language bias, not attributing to the relevant information contained in the captions. For example, in FigureÂ <a href="#S5.F4" title="Figure 4 â€£ 5.3 Case Study â€£ 5 Analysis â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(a), the PLM may generate the answer â€œ<span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_italic">chocolate</span>â€, even if the captions do not mention chocolateÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite>. However, since chocolate often co-occurs with donuts in the training corpora, the PLM may associate chocolate with donuts and generate it as the answer. In order to check how often such a situation happens, we randomly sample 100 questions where the prediction model gives correct answers. For each question, we manually assess whether their answer is derived from the caption. Our analysis reveals that only 6 out of 100 captions are irrelevant to the questions, indicating the reliability of the captions.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">Another interesting phenomenon is that the sentences generated by <span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_smallcaps">Lamoc</span> can be grammatically incoherent and sometimes incomplete. This indicates that PLM prompting may not always conform to human language patterns, which is consistent with previous studiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Webson and Pavlick, <a href="#bib.bib37" title="" class="ltx_ref">2022</a>; Deng etÂ al., <a href="#bib.bib7" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p">The ablation study of the level of relevance, the number of captions, and the influence of different prompt designs can be found in appendixÂ <a href="#A2" title="Appendix B Additional Ablation Study â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we propose <span id="S6.p1.1.1" class="ltx_text ltx_font_smallcaps">Lamoc</span>, a language model guided captioning method that improves a captioning model to generate comprehensive captions for an image to help answer the question. In order to train such a model, we first perform captioning adaptation on a self-generated dataset filtered by FLAN-T5-XXL, and then fine-tune the updated captioning model through reinforcement learning from PLM feedback. Our method, <span id="S6.p1.1.2" class="ltx_text ltx_font_smallcaps">Lamoc</span>, generates captions that are both informative and able to assist PLMs in VQA tasks, as demonstrated through experiments on two knowledge-based VQA datasets. On the challenging A-OKVQA dataset, <span id="S6.p1.1.3" class="ltx_text ltx_font_smallcaps">Lamoc</span> substantially outperforms previous zero-shot methods and even achieves comparable results to a fine-tuned VLP model. Additionally, we show that <span id="S6.p1.1.4" class="ltx_text ltx_font_smallcaps">Lamoc</span> is generalizable to PLMs of varying sizes, from 223M to 11B parameters, demonstrating its potential to be applied to LLMs, which we leave as future work.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitations</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In our study, we have demonstrated the effectiveness of our proposed method on FLAN-T5 with different sizes. However, we have not yet evaluated its performance on LLMs, which possess an even greater number of parameters and have been pre-trained on larger corpora, thus potentially providing more accurate feedback for both caption adaptation and reinforcement learning. Meanwhile, it is worth noting that PLMs may contain certain biases, and training based on their feedback may amplify these biases. As future work, we aim to investigate the scalability of our method to LLMs, as well as strategies to mitigate the potential negative effects of biases present in PLMs.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Ackownledgement</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was partially supported by National Natural Science Foundation of China under Grant No. 62222215, Beijing Natural Science Foundation under Grant No. 4222027, and Beijing Outstanding Young Scientist Program under Grant No. BJJWZYJH012019100020098. Xin Zhao is the corresponding author.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds,
etÂ al. 2022.

</span>
<span class="ltx_bibblock">Flamingo: a visual language model for few-shot learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.14198</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. (2020)</span>
<span class="ltx_bibblock">
TomÂ B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, DanielÂ M. Ziegler, Jeffrey Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="" class="ltx_ref ltx_href">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Campos and Shern (2022)</span>
<span class="ltx_bibblock">
JonÂ Ander Campos and Jun Shern. 2022.

</span>
<span class="ltx_bibblock">Training language models with language feedback.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">ACL Workshop on Learning with Natural Language Supervision.
2022.</em>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho etÂ al. (2021)</span>
<span class="ltx_bibblock">
Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021.

</span>
<span class="ltx_bibblock">Unifying vision-and-language tasks via text generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
1931â€“1942. PMLR.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung etÂ al. (2022)</span>
<span class="ltx_bibblock">
HyungÂ Won Chung, LeÂ Hou, Shayne Longpre, Barret Zoph, YiÂ Tay, William Fedus,
Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.11416</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai etÂ al. (2022)</span>
<span class="ltx_bibblock">
Wenliang Dai, LuÂ Hou, Lifeng Shang, Xin Jiang, Qun Liu, and Pascale Fung. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.findings-acl.187" title="" class="ltx_ref ltx_href">Enabling
multimodal generation on CLIP via vision-language knowledge distillation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, pages 2383â€“2395. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng etÂ al. (2022)</span>
<span class="ltx_bibblock">
Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu,
Meng Song, EricÂ P Xing, and Zhiting Hu. 2022.

</span>
<span class="ltx_bibblock">Rlprompt: Optimizing discrete text prompts with reinforcement
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.12548</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin etÂ al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/n19-1423" title="" class="ltx_ref ltx_href">BERT: pre-training of
deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume
1 (Long and Short Papers)</em>, pages 4171â€“4186. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan etÂ al. (2018)</span>
<span class="ltx_bibblock">
Angela Fan, Mike Lewis, and YannÂ N. Dauphin. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P18-1082" title="" class="ltx_ref ltx_href">Hierarchical neural
story generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20,
2018, Volume 1: Long Papers</em>, pages 889â€“898. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal etÂ al. (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
2017.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 6904â€“6913.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gui etÂ al. (2022)</span>
<span class="ltx_bibblock">
Liangke Gui, Borui Wang, Qiuyuan Huang, Alexander Hauptmann, Yonatan Bisk, and
Jianfeng Gao. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.naacl-main.70" title="" class="ltx_ref ltx_href">KAT: A
knowledge augmented transformer for vision-and-language</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022</em>,
pages 956â€“968. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jiaxian Guo, Junnan Li, Dongxu Li, Anthony MengÂ Huat Tiong, Boyang Li, Dacheng
Tao, and StevenÂ CH Hoi. 2022.

</span>
<span class="ltx_bibblock">From images to textual prompts: Zero-shot vqa with frozen large
language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.10846</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holtzman etÂ al. (2019)</span>
<span class="ltx_bibblock">
Ari Holtzman, Jan Buys, LiÂ Du, Maxwell Forbes, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock">The curious case of neural text degeneration.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.09751</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, NoahÂ A Smith, and Jiebo Luo.
2022.

</span>
<span class="ltx_bibblock">Promptcap: Prompt-guided task-aware image captioning.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.09699</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin etÂ al. (2022)</span>
<span class="ltx_bibblock">
Woojeong Jin, YuÂ Cheng, Yelong Shen, Weizhu Chen, and Xiang Ren. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-long.197" title="" class="ltx_ref ltx_href">A good prompt
is worth millions of parameters: Low-resource prompt-based learning for
vision-language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin,
Ireland, May 22-27, 2022</em>, pages 2763â€“2775. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khashabi etÂ al. (2022)</span>
<span class="ltx_bibblock">
Daniel Khashabi, Yeganeh Kordi, and Hannaneh Hajishirzi. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2202.12359" title="" class="ltx_ref ltx_href">Unifiedqa-v2: Stronger
generalization via broader cross-format training</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2202.12359.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kojima etÂ al. (2022)</span>
<span class="ltx_bibblock">
Takeshi Kojima, ShixiangÂ Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
Iwasawa. 2022.

</span>
<span class="ltx_bibblock">Large language models are zero-shot reasoners.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.11916</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, and Steven C.Â H.
Hoi. 2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2209.09019" title="" class="ltx_ref ltx_href">LAVIS: A
library for language-vision intelligence</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2209.09019.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven C.Â H. Hoi. 2022b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v162/li22n.html" title="" class="ltx_ref ltx_href">BLIP:
bootstrapping language-image pre-training for unified vision-language
understanding and generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning, ICML 2022,
17-23 July 2022, Baltimore, Maryland, USA</em>, volume 162 of <em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic">Proceedings
of Machine Learning Research</em>, pages 12888â€“12900. PMLR.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, WayneÂ Xin Zhao, and Ji-Rong Wen.
2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2305.10355" title="" class="ltx_ref ltx_href">Evaluating object
hallucination in large vision-language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.10355.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Sheng Liang, Mengjie Zhao, and Hinrich SchÃ¼tze. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.findings-acl.234" title="" class="ltx_ref ltx_href">Modular
and parameter-efficient multimodal fusion with prompting</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, pages 2976â€“2985. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr DollÃ¡r, and CÂ Lawrence Zitnick. 2014.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pages 740â€“755.
Springer.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2022)</span>
<span class="ltx_bibblock">
Yuanze Lin, Yujia Xie, Dongdong Chen, Yichong Xu, Chenguang Zhu, and LuÂ Yuan.
2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2206.01201" title="" class="ltx_ref ltx_href">REVIVE: regional
visual representation matters in knowledge-based visual question answering</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2206.01201.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino etÂ al. (2019)</span>
<span class="ltx_bibblock">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019.

</span>
<span class="ltx_bibblock">Ok-vqa: A visual question answering benchmark requiring external
knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/cvf conference on computer vision
and pattern recognition</em>, pages 3195â€“3204.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeff Wu, XuÂ Jiang, Diogo Almeida, CarrollÂ L Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, etÂ al.
2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.02155</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pang and He (2021)</span>
<span class="ltx_bibblock">
RichardÂ Yuanzhe Pang and HeÂ He. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=RovX-uQ1Hua" title="" class="ltx_ref ltx_href">Text generation
by learning from demonstrations</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paulus etÂ al. (2018)</span>
<span class="ltx_bibblock">
Romain Paulus, Caiming Xiong, and Richard Socher. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=HkAClQgA-" title="" class="ltx_ref ltx_href">A deep reinforced
model for abstractive summarization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
Proceedings</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajani etÂ al. (2019)</span>
<span class="ltx_bibblock">
NazneenÂ Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/p19-1487" title="" class="ltx_ref ltx_href">Explain yourself!
leveraging language models for commonsense reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Conference of the Association for
Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2,
2019, Volume 1: Long Papers</em>, pages 4932â€“4942. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sap etÂ al. (2019)</span>
<span class="ltx_bibblock">
Maarten Sap, Ronan LeÂ Bras, Emily Allaway, Chandra Bhagavatula, Nicholas
Lourie, Hannah Rashkin, Brendan Roof, NoahÂ A Smith, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock">Atomic: An atlas of machine commonsense for if-then reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial
intelligence</em>, volumeÂ 33, pages 3027â€“3035.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwenk etÂ al. (2022)</span>
<span class="ltx_bibblock">
Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and
Roozbeh Mottaghi. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-031-20074-8_9" title="" class="ltx_ref ltx_href">A-OKVQA: A
benchmark for visual question answering using world knowledge</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Computer Vision - ECCV 2022 - 17th European Conference,
Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VIII</em>, volume
13668 of <em id="bib.bib30.2.2" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em>, pages 146â€“162. Springer.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sutton and Barto (2018)</span>
<span class="ltx_bibblock">
RichardÂ S Sutton and AndrewÂ G Barto. 2018.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Reinforcement learning: An introduction</em>.

</span>
<span class="ltx_bibblock">MIT press.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiong etÂ al. (2022)</span>
<span class="ltx_bibblock">
Anthony MengÂ Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, and StevenÂ CH
Hoi. 2022.

</span>
<span class="ltx_bibblock">Plug-and-play vqa: Zero-shot vqa by conjoining large pretrained
models with zero training.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.08773</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsimpoukelli etÂ al. (2021)</span>
<span class="ltx_bibblock">
Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S.Â M.Â Ali Eslami, Oriol Vinyals,
and Felix Hill. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html" title="" class="ltx_ref ltx_href">Multimodal few-shot learning with frozen language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 34: Annual
Conference on Neural Information Processing Systems 2021, NeurIPS 2021,
December 6-14, 2021, virtual</em>, pages 200â€“212.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Peng Wang, AnÂ Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma,
Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v162/wang22al.html" title="" class="ltx_ref ltx_href">OFA:
unifying architectures, tasks, and modalities through a simple
sequence-to-sequence learning framework</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning, ICML 2022,
17-23 July 2022, Baltimore, Maryland, USA</em>, volume 162 of <em id="bib.bib34.2.2" class="ltx_emph ltx_font_italic">Proceedings
of Machine Learning Research</em>, pages 23318â€“23340. PMLR.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Wenhui Wang, Hangbo Bao, LiÂ Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti
Aggarwal, OwaisÂ Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei.
2022b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2208.10442" title="" class="ltx_ref ltx_href">Image as a foreign
language: Beit pretraining for all vision and vision-language tasks</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2208.10442.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022c)</span>
<span class="ltx_bibblock">
Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao
Wu, Xin Jiang, and Qun Liu. 2022c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.findings-acl.2" title="" class="ltx_ref ltx_href">Compilable
neural code generation with compiler feedback</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, pages 9â€“19. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Webson and Pavlick (2022)</span>
<span class="ltx_bibblock">
Albert Webson and Ellie Pavlick. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.naacl-main.167" title="" class="ltx_ref ltx_href">Do
prompt-based models really understand the meaning of their prompts?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022</em>,
pages 2300â€“2344. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, EdÂ Chi, Quoc Le, and
Denny Zhou. 2022.

</span>
<span class="ltx_bibblock">Chain of thought prompting elicits reasoning in large language
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.11903</em>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and
Lijuan Wang. 2022.

</span>
<span class="ltx_bibblock">An empirical study of gpt-3 for few-shot knowledge-based vqa.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volumeÂ 36, pages 3081â€“3089.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2023)</span>
<span class="ltx_bibblock">
WayneÂ Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,
Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,
Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2303.18223" title="" class="ltx_ref ltx_href">A survey of large
language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.18223.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Ax1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">Appendix</h2>

</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Training Details and Artifacts</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.4" class="ltx_p">For <span id="A1.p1.4.1" class="ltx_text ltx_font_smallcaps">LAMOC</span> training, we adopt the officially released BLIP captioning checkpoint<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth</a></span></span></span> for model initialization. For both captioning adaptation and reinforcement learning, we adopt the following hyper-parameters: learning rate <math id="A1.p1.1.m1.1" class="ltx_Math" alttext="2e-6" display="inline"><semantics id="A1.p1.1.m1.1a"><mrow id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml"><mrow id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2.cmml"><mn id="A1.p1.1.m1.1.1.2.2" xref="A1.p1.1.m1.1.1.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="A1.p1.1.m1.1.1.2.1" xref="A1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="A1.p1.1.m1.1.1.2.3" xref="A1.p1.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="A1.p1.1.m1.1.1.1" xref="A1.p1.1.m1.1.1.1.cmml">âˆ’</mo><mn id="A1.p1.1.m1.1.1.3" xref="A1.p1.1.m1.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1"><minus id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1.1"></minus><apply id="A1.p1.1.m1.1.1.2.cmml" xref="A1.p1.1.m1.1.1.2"><times id="A1.p1.1.m1.1.1.2.1.cmml" xref="A1.p1.1.m1.1.1.2.1"></times><cn type="integer" id="A1.p1.1.m1.1.1.2.2.cmml" xref="A1.p1.1.m1.1.1.2.2">2</cn><ci id="A1.p1.1.m1.1.1.2.3.cmml" xref="A1.p1.1.m1.1.1.2.3">ğ‘’</ci></apply><cn type="integer" id="A1.p1.1.m1.1.1.3.cmml" xref="A1.p1.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">2e-6</annotation></semantics></math>, warmup 600 steps, weight decay <math id="A1.p1.2.m2.1" class="ltx_Math" alttext="0.05" display="inline"><semantics id="A1.p1.2.m2.1a"><mn id="A1.p1.2.m2.1.1" xref="A1.p1.2.m2.1.1.cmml">0.05</mn><annotation-xml encoding="MathML-Content" id="A1.p1.2.m2.1b"><cn type="float" id="A1.p1.2.m2.1.1.cmml" xref="A1.p1.2.m2.1.1">0.05</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.2.m2.1c">0.05</annotation></semantics></math>, batch size 8. The balance factor <math id="A1.p1.3.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="A1.p1.3.m3.1a"><mi id="A1.p1.3.m3.1.1" xref="A1.p1.3.m3.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="A1.p1.3.m3.1b"><ci id="A1.p1.3.m3.1.1.cmml" xref="A1.p1.3.m3.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.3.m3.1c">\alpha</annotation></semantics></math> is set to <math id="A1.p1.4.m4.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="A1.p1.4.m4.1a"><mn id="A1.p1.4.m4.1.1" xref="A1.p1.4.m4.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="A1.p1.4.m4.1b"><cn type="float" id="A1.p1.4.m4.1.1.cmml" xref="A1.p1.4.m4.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.4.m4.1c">0.9</annotation></semantics></math>. We train the model for 10 epochs and choose the one with the highest rewardÂ (without labels from the validation set). All the experiments are conducted based on LAVISÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a href="#bib.bib18" title="" class="ltx_ref">2022a</a>)</cite> under BSD 3-Clause License. The A-OKVQA is under the Apache License 2.0.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Ablation Study</h2>

<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Level of Relevance</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p id="A2.SS1.p1.1" class="ltx_p">When prompting the PLM to give a correlation score for the caption, the level of relevance is part of the prompt, thus can influence the result. We try different levels for the prompt-based reward and the results are in TableÂ <a href="#A2.T3" title="Table 3 â€£ B.1 Level of Relevance â€£ Appendix B Additional Ablation Study â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Since four levels gives the highest vqa score, we use four levels in our prompt-based reinforcement learning.</p>
</div>
<figure id="A2.T3" class="ltx_table">
<table id="A2.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A2.T3.1.1" class="ltx_tr">
<td id="A2.T3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">Level</td>
<td id="A2.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">A-OKVQA</td>
</tr>
<tr id="A2.T3.1.2" class="ltx_tr">
<td id="A2.T3.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="A2.T3.1.2.1.1" class="ltx_text"></span><span id="A2.T3.1.2.1.2" class="ltx_text">
<span id="A2.T3.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T3.1.2.1.2.1.1" class="ltx_tr">
<span id="A2.T3.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">A: 0%; B: 100%</span></span>
</span></span><span id="A2.T3.1.2.1.3" class="ltx_text"></span></td>
<td id="A2.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">27.25</td>
</tr>
<tr id="A2.T3.1.3" class="ltx_tr">
<td id="A2.T3.1.3.1" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="A2.T3.1.3.1.1" class="ltx_text"></span><span id="A2.T3.1.3.1.2" class="ltx_text">
<span id="A2.T3.1.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T3.1.3.1.2.1.1" class="ltx_tr">
<span id="A2.T3.1.3.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">A: 0%; B: 50%; C: 100%</span></span>
</span></span><span id="A2.T3.1.3.1.3" class="ltx_text"></span></td>
<td id="A2.T3.1.3.2" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">28.29</td>
</tr>
<tr id="A2.T3.1.4" class="ltx_tr">
<td id="A2.T3.1.4.1" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="A2.T3.1.4.1.1" class="ltx_text"></span><span id="A2.T3.1.4.1.2" class="ltx_text">
<span id="A2.T3.1.4.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T3.1.4.1.2.1.1" class="ltx_tr">
<span id="A2.T3.1.4.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">A: 0%; B: 25%; C: 50%; D: 75%</span></span>
</span></span><span id="A2.T3.1.4.1.3" class="ltx_text"></span></td>
<td id="A2.T3.1.4.2" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">28.98</td>
</tr>
<tr id="A2.T3.1.5" class="ltx_tr">
<td id="A2.T3.1.5.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="A2.T3.1.5.1.1" class="ltx_text"></span><span id="A2.T3.1.5.1.2" class="ltx_text">
<span id="A2.T3.1.5.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T3.1.5.1.2.1.1" class="ltx_tr">
<span id="A2.T3.1.5.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">A: 0%; B: 25%; C: 50%; D: 75%; E: 100%</span></span>
</span></span><span id="A2.T3.1.5.1.3" class="ltx_text"></span></td>
<td id="A2.T3.1.5.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">27.96</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>
VQA score of models trained with different levels of prompt-based rewards.
</figcaption>
</figure>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Number of Captions</h3>

<div id="A2.SS2.p1" class="ltx_para">
<p id="A2.SS2.p1.1" class="ltx_p">Since the PLM is "blind," all visual information is carried by the captions. Thus, the number of captions is critical for the PLM to answer the question. In Figure <a href="#A2.F5" title="Figure 5 â€£ B.2 Number of Captions â€£ Appendix B Additional Ablation Study â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we explore the influence of the number of captions. Our results indicate that utilizing a larger number of captions leads to improved performance across various model sizes. Performance gains continue to accumulate even when utilizing 10 captions, leading us to posit that incorporating an even greater number of captions would result in further improvements.</p>
</div>
<figure id="A2.F5" class="ltx_figure"><img src="/html/2305.17006/assets/x5.png" id="A2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>VQA score with different number of captions in the A-OKVQA validation set.</figcaption>
</figure>
</section>
<section id="A2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Prompt Design</h3>

<div id="A2.SS3.p1" class="ltx_para">
<p id="A2.SS3.p1.1" class="ltx_p">Another critical design of our method is instructing the FLAN-T5 to provide feedback and answer questions, so we explore the effects of different formats of instruction in Table <a href="#A2.T4" title="Table 4 â€£ B.3 Prompt Design â€£ Appendix B Additional Ablation Study â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. We can observe that prompt design has a great impact on the results
TableÂ <a href="#A2.T4" title="Table 4 â€£ B.3 Prompt Design â€£ Appendix B Additional Ablation Study â€£ Zero-shot Visual Question Answering with Language Model Feedback" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, which is in line with the conclusion of previous worksÂ <cite class="ltx_cite ltx_citemacro_citep">(Wei etÂ al., <a href="#bib.bib38" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<figure id="A2.T4" class="ltx_table">
<table id="A2.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A2.T4.1.1" class="ltx_tr">
<td id="A2.T4.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">Prompt</td>
<td id="A2.T4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">OK-VQA</td>
<td id="A2.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">A-OKVQA</td>
</tr>
<tr id="A2.T4.1.2" class="ltx_tr">
<td id="A2.T4.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="A2.T4.1.2.1.1" class="ltx_text"></span><span id="A2.T4.1.2.1.2" class="ltx_text">
<span id="A2.T4.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T4.1.2.1.2.1.1" class="ltx_tr">
<span id="A2.T4.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">Answer the following question in</span></span>
<span id="A2.T4.1.2.1.2.1.2" class="ltx_tr">
<span id="A2.T4.1.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">one word. Q: [caption]. [question]</span></span>
</span></span><span id="A2.T4.1.2.1.3" class="ltx_text"></span></td>
<td id="A2.T4.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">29.53</td>
<td id="A2.T4.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">29.84</td>
</tr>
<tr id="A2.T4.1.3" class="ltx_tr">
<td id="A2.T4.1.3.1" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="A2.T4.1.3.1.1" class="ltx_text"></span><span id="A2.T4.1.3.1.2" class="ltx_text">
<span id="A2.T4.1.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T4.1.3.1.2.1.1" class="ltx_tr">
<span id="A2.T4.1.3.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">Please answer the following</span></span>
<span id="A2.T4.1.3.1.2.1.2" class="ltx_tr">
<span id="A2.T4.1.3.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">question. [caption]. [question]</span></span>
</span></span><span id="A2.T4.1.3.1.3" class="ltx_text"></span></td>
<td id="A2.T4.1.3.2" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">28.22</td>
<td id="A2.T4.1.3.3" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">29.73</td>
</tr>
<tr id="A2.T4.1.4" class="ltx_tr">
<td id="A2.T4.1.4.1" class="ltx_td ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">[caption]. [question]</td>
<td id="A2.T4.1.4.2" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">27.59</td>
<td id="A2.T4.1.4.3" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">27.99</td>
</tr>
<tr id="A2.T4.1.5" class="ltx_tr">
<td id="A2.T4.1.5.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">
<span id="A2.T4.1.5.1.1" class="ltx_text"></span><span id="A2.T4.1.5.1.2" class="ltx_text">
<span id="A2.T4.1.5.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T4.1.5.1.2.1.1" class="ltx_tr">
<span id="A2.T4.1.5.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">[caption]. [question] Letâ€™s think</span></span>
<span id="A2.T4.1.5.1.2.1.2" class="ltx_tr">
<span id="A2.T4.1.5.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:2.8pt;padding-right:2.8pt;">step by step.</span></span>
</span></span><span id="A2.T4.1.5.1.3" class="ltx_text"></span></td>
<td id="A2.T4.1.5.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">18.08</td>
<td id="A2.T4.1.5.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">28.72</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>
VQA score of the answers generated by FLAN-T5-large conditioned on different prompts.
</figcaption>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2305.17005" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2305.17006" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2305.17006">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2305.17006" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2305.17007" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 05:11:37 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
