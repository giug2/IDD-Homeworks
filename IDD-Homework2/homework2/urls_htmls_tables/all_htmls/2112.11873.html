<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2112.11873] FLoBC: A Decentralized Blockchain-Based Federated Learning Framework</title><meta property="og:description" content="The rapid expansion of data worldwide invites the need for more distributed solutions in order to apply machine learning on a much wider scale. The resultant distributed learning systems can have various degrees of cen…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FLoBC: A Decentralized Blockchain-Based Federated Learning Framework">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FLoBC: A Decentralized Blockchain-Based Federated Learning Framework">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2112.11873">

<!--Generated on Fri Mar  1 15:44:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Byzantine Fault-Tolerance, 
Federated Learning, 
Blockchain, 
Decentralized Systems, 
Distributed Machine Learning, 
Privacy Preserving
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">FLoBC: A Decentralized Blockchain-Based Federated Learning Framework</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Mohamed Ghanem
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">The American University in Cairo
<br class="ltx_break"></span>oscar@aucegypt.edu
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fadi Dawoud
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.1.id1" class="ltx_text ltx_font_italic">The American University in Cairo
<br class="ltx_break"></span>fadiadel@aucegypt.edu
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Habiba Gamal


 <span id="id3.1.id1" class="ltx_text"></span><span id="id4.2.id2" class="ltx_text"></span> <span id="id5.3.id3" class="ltx_ERROR undefined">{@IEEEauthorhalign}</span>
Eslam Soliman
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id6.4.id1" class="ltx_text ltx_font_italic">The American University in Cairo
<br class="ltx_break"></span>habibabassem@aucegypt.edu
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id7.5.id1" class="ltx_text ltx_font_italic">The American University in Cairo
<br class="ltx_break"></span>eslam98@aucegypt.edu
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tamer El-Batt
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id8.1.id1" class="ltx_text ltx_font_italic">The American University in Cairo
<br class="ltx_break"></span>tamer.elbatt@aucegypt.edu
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hossam Sharara
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id9.1.id1" class="ltx_text ltx_font_italic">The American University in Cairo
<br class="ltx_break"></span>hossam.sharara@aucegypt.edu
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id10.id1" class="ltx_p">The rapid expansion of data worldwide invites the need for more distributed solutions in order to apply machine learning on a much wider scale. The resultant distributed learning systems can have various degrees of centralization. In this work, we demonstrate our solution FLoBC for building a generic decentralized federated learning system using the blockchain technology, accommodating any machine learning model that is compatible with gradient descent optimization. We present our system design comprising the two decentralized actors: <em id="id10.id1.1" class="ltx_emph ltx_font_italic">trainer</em> and <em id="id10.id1.2" class="ltx_emph ltx_font_italic">validator</em>, alongside our methodology for ensuring reliable and efficient operation of said system. Finally, we utilize FLoBC as an experimental sandbox to compare and contrast the effects of trainer-to-validator ratio, reward-penalty policy, and model synchronization schemes on the overall system performance, ultimately showing by example that a decentralized federated learning system is indeed a feasible alternative to more centralized architectures.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Byzantine Fault-Tolerance,
Federated Learning,
Blockchain,
Decentralized Systems,
Distributed Machine Learning,
Privacy Preserving

</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Graphical Abstract</h2>

<figure id="Sx1.1" class="ltx_figure"><img src="/html/2112.11873/assets/figs/graphical_abstract.png" id="Sx1.1.g1" class="ltx_graphics ltx_img_landscape" width="180" height="135" alt="[Uncaptioned image]">
</figure><span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">0</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">0</sup><span class="ltx_note_type">footnotetext: </span>This paper is the result of BSc in Computer Engineering thesis at the American University in Cairo (Spring 2021)</span></span></span>
</section>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, there has been an enormous
corpus of research and development dedicated to
and focused on accelerating the processes of
machine learning in every conceivable shape or
form. Long gone are the days when computer
hardware was not adequate enough to handle
already existing machine learning algorithms, yet
it seems the rapidly growing demand for machine
learning applications has put us in a situation that
yet again undermines the capabilities of individual
machines. Here, an intuitive question comes to
mind: why not just have multiple machines
collaborate on training the model at hand? That is,
in fact, the idea that Google realized in their
conception of Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. The idea of federated learning solves another major problem: the scarcity of data at a single machine. This is solved through the collaboration of different nodes in the training, each using its local data then sharing its model updates, realizing a single model trained on the sum of their local data.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this paper, we present an experimental framework FLoBC for building federated learning systems over blockchain. We further illustrate how it offers feasible solutions to common challenges in decentralized federated learning, namely, <span id="S1.p2.1.1" class="ltx_text ltx_font_bold">privacy</span>, <span id="S1.p2.1.2" class="ltx_text ltx_font_bold">efficiency</span>, and <span id="S1.p2.1.3" class="ltx_text ltx_font_bold">Byzantine fault-tolerance</span>. FLoBC is a proof-of-concept used to spawn mini-systems that demonstrate several aspects of our solution on a relatively small scale, on which we, in later sections, conduct several experiments in order to verify the extent of its efficacy by comparing its performance to a control group for which the features in question are disabled.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background and Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Prior to the conception of <em id="S2.p1.1.1" class="ltx_emph ltx_font_italic">federated learning</em>, <span id="S2.p1.1.2" class="ltx_text ltx_font_italic">blockchain</span> has been a prominent technology that aimed at realizing decentralized consensus. It is not hard to see the potential merit of combining blockchain with federated learning in order to achieve much more decentralization and privacy. There is a number of contributions in the field of
decentralized federated learning, some of which involve blockchain while others employ other decentralized protocols. The core objective is to eliminate the need for a central server to gather user data and perform model training. The motivation for that is two-fold: on one hand, it provides more privacy, and on the other hand, it relaxes the minimum required amount of computational power by distributing computation across the network. Some of the existing contributions rely on an All-Reduce scheme in which each training node shares its updates with all the nodes in the system resulting in a communication cost of <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="O(n^{2})" display="inline"><semantics id="S2.p1.1.m1.1a"><mrow id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mi id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml">​</mo><mrow id="S2.p1.1.m1.1.1.1.1" xref="S2.p1.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.p1.1.m1.1.1.1.1.2" xref="S2.p1.1.m1.1.1.1.1.1.cmml">(</mo><msup id="S2.p1.1.m1.1.1.1.1.1" xref="S2.p1.1.m1.1.1.1.1.1.cmml"><mi id="S2.p1.1.m1.1.1.1.1.1.2" xref="S2.p1.1.m1.1.1.1.1.1.2.cmml">n</mi><mn id="S2.p1.1.m1.1.1.1.1.1.3" xref="S2.p1.1.m1.1.1.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S2.p1.1.m1.1.1.1.1.3" xref="S2.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><times id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2"></times><ci id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3">𝑂</ci><apply id="S2.p1.1.m1.1.1.1.1.1.cmml" xref="S2.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.1.1.1.1.cmml" xref="S2.p1.1.m1.1.1.1.1">superscript</csymbol><ci id="S2.p1.1.m1.1.1.1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.1.1.1.2">𝑛</ci><cn type="integer" id="S2.p1.1.m1.1.1.1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">O(n^{2})</annotation></semantics></math> for n training nodes. One important aspect to leverage (wherever applicable) is network topology which can be
exploited to reduce the communication cost. There are proposals that use the ring topology in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, the tree
topology in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and graph topology in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> to share
the model updates of the node with its one-hop
neighbors. However, all these approaches require
multi-hops for the updates to reach all the nodes in
the system, thus leading to slow convergence.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Following the same objective of reducing communication overhead, other contributions, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, use the gossip protocol which relies on the fact that each peer
sends its updates to another peer in the network, thus propagating the information to the whole network. This arguably puts too much responsibility on the trainer side to validate updates by other peers. To remedy this, our framework introduces validator nodes (similar to cryptocurrency miners) in order to alleviate the computational load of trainers and to have a clearer and easier-to-trace trust system.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Last but not least, and most relevant to our work, some existing work uses blockchain as the communication infrastructure of decentralized federated learning. Blockchain is used to
facilitate uploading and tracking updates, to reward
the users for participating in the training or the
validation of the model, and to make the updates
immutable and secure. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, the role of the
central server in centralized federated learning is
fulfilled by smart contracts. The node performs
the local training, then the local model updates
are sent to an elected consensus committee that
verifies the updates and assigns scores to them. Subsequently, the updated
global model is added to the blockchain. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, each training node is assigned a validator that validates said trainer’s updates, computes proof of work (PoW) and rewards the trainer accordingly. After the updates are added to the blockchain, the miner is rewarded. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> follows a similar algorithm but the trainer can send its updates to any validator and the rewards are given by the model owner, not the validator or the blockchain. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> make use of a distributed peer-to-peer file sharing system to hold the model checkpoints. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> are specially tailored for automotive in-vehicle training. 
<br class="ltx_break">By contrast, our framework uses <span id="S2.p3.1.1" class="ltx_text ltx_font_italic">proof of stake</span> (PoS) as a much more lightweight alternative to cryptographic PoW. In addition, our methodology is a generic one, hence, it is not bound to a specific task or purpose as long as the required consensus assumptions are met by the underlying network (refer to section <a href="#S3.SS2" title="III-B Degree of Centralization ‣ III Methodology ‣ FLoBC: A Decentralized Blockchain-Based Federated Learning Framework" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>).</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we introduce our approach for designing a system that closely realizes a decentralized version of <em id="S3.p1.1.1" class="ltx_emph ltx_font_italic">Federated Learning</em>. In pursuit of that, a few principal standards were set forth to serve as a basis for features and solutions employed in the system. These standards are mainly:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><em id="S3.I1.i1.p1.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">Generality</em>: The system design needs to be agnostic of the model specifics apart from it being compatible with gradient descent.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><em id="S3.I1.i2.p1.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">Decentralization</em>: System politics shall be undertaken through decentralized consensus.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><em id="S3.I1.i3.p1.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">No Data Sharing</em>: Nodes shall only communicate insights, never having to share their data.</p>
</div>
</li>
</ul>
<p id="S3.p1.2" class="ltx_p">We cover challenges as well as
solutions pertaining to the performance of
said distributed learning system in the context of
supervised learning where we have a loss function
that we aim to minimize, most prominently
through gradient descent. Though it is mainly
focused on computational aspects, the system also takes into
account communicational efficiency. We
shall discuss the three main dimensions of a
distributed learning system: Parallelism,
Degree of Centralization, and Synchronization.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Parallelism</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Training an ML model is, in the great majority of
cases, the heaviest part of the
whole machine learning process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, so how could
distributed learning help break this down? For
that, distributed learning generally offers two
paradigms to parallelize the process:
Model-Parallelism and Data-Parallelism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, the latter of which was chosen for our system.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Model Parallelism:</span>
In model-parallelism, the model structure itself is
split or distributed across multiple nodes, a
paradigm often referred to as Split Learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
For instance, in the case of a neural network, the
network would be split at certain layers (called cut
layers), then each trainer would be assigned a group
of consecutive layers, and all layer groups are stitched together based on the layer order in the model architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
While Split Learning has the advantages of data
privacy and enabling nodes with low computation
power to participate in the training, it has some
inherent sequentiality that ultimately limits its
scalability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. For our purposes, the primary downside of
model-parallelism is that it is model and algorithm
specific, which makes it difficult to utilize for a
general-purpose system for training.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Data Parallelism</span>:
In data-parallelism, each node trains the model in
its entirety, but only on a subset of the training
data, with each node doing this in parallel, then all
generated model updates are aggregated into the global model. This approach has the merit of better
parallel processing on stochastic processes such as stochastic gradient descent (SGD) which is widely used in distributed learning.
Moreover, it can be readily generalized to any
process that lends itself to parallelism such as
gradient computation and approximation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. As such, we opted for this model of parallelism because it greatly simplifies the general model structure to a flat array of weights, from which the full model can be reconstructed.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Degree of Centralization</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">It is not hard to see that the two ends of this
spectrum are: fully centralized to fully
decentralized. Accordingly, a strictly fully
centralized system would simply be a single device
doing all of the training process from start to
finish. A more distributed and less centralized
version of that would be the classical
federated learning with a central server
or cluster of servers supervising the training done by other nodes. Here, we present a system on a much higher degree of decentralization. Similar to most cryptocurrency networks, our system comprises two types of nodes: <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">trainer</span> and <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_bold">validator</span>, the latter of which is analogous to a crypto-miner. Validators are responsible for maintaining models on the underlying blockchain network through decentralized consensus based on voting. Ideally, a validator’s vote, on whether some update to the model (made by a <em id="S3.SS2.p1.1.3" class="ltx_emph ltx_font_italic">trainer</em> node) is acceptable, should be decided by validation on data presumably unrevealed to the trainer. That, combined with the voting-based nature of consensus, stipulates the following requirements and assumptions in order for the system to operate soundly:</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">Strictly more than 2/3 of validators are non-Byzantine.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">Different validators’ datasets should be sufficiently positively correlated in order to reach a meaningful consensus.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p">The underlying network is at least partially synchronous.</p>
</div>
</li>
<li id="S3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i4.p1" class="ltx_para">
<p id="S3.I2.i4.p1.1" class="ltx_p">The quality of models produced by the system is limited by the amount of collective useful data used during training and validation.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">One
serious implication of the system’s centralization
degree is the choice of the gradient descent (or any
optimization technique) variant that suits how the
system is structured. Since the scope of this study
is focused on distributed learning, we are only
going to consider the distributed centralized
version and its fully decentralized nemesis. As
such, we won’t be concerned with traditional
gradient descent (as it is impossible to use for such
a distributed system), but instead, a stochastic
variant of it should be employed to approximate
the true gradient bearing in mind the rate at which
this approximation converges to the true value
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Much like most federated learning systems, our system uses <em id="S3.SS2.p2.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">Federated Averaging</em> to perform parallel stochastic gradient descent (P-SGD). At its core, it follows a very simple assumption: different nodes training on different datasets
whose updates get aggregated or averaged onto a
single model would eventually converge to a valid
global model.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Synchronization</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">One crucial aspect of the distributed learning
process is that distributed model updates happen in
tandem to prevent model degradation due to out of
sync parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Like many other parallel and
massively parallel paradigms, the concept of a
synchronization barrier applies to the case of a
distributed learning system. In this context, each
barrier would signal an update-sharing round in
which updates are aggregated into an updated
model. The more frequently synchronization
barriers occur, the faster the model converges, and
the less it degrades <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. However, here lies an
important tradeoff between rate of convergence
and communicational cost. That is, each barrier
incurs a price on the network and node bandwidth.
The main scheme here is that the system has two
types of phases: a computation phase, and
communication phase for sharing the
computational results. For that purpose, there are
many techniques to handle node update
synchronization, and the following subsections
introduce the most common ones.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Bulk Synchronous Parallel (BSP):</span>
Abbreviated as BSP, it is known to be the simplest
approach to preserve consistency by alternating
between rounds of computation followed by
communication. The main pro of this model is that
it has the best consistency (hence, fastest
convergence), yet its main bottleneck is that
finished nodes have to wait for all worker nodes to
finish their computation at each synchronization
barrier <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. In our variant of BSP, each training round is limited by a fixed period that ends with a strict deadline after which no further trainer updates are considered for that round.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Stale Synchronous Parallel (SSP):</span>
This model is similar to BSP but with more relaxed
synchronization barriers that allow a certain
number of iterations beyond the preset number.
After that leeway is reached, all workers are
paused <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Given its nature as a compromise to
BSP, it has good convergence insurances in low to
moderate staleness, beyond which convergence
rates start to decay <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. In our design, SSP is modelled as BSP with the possibility of a deadline extension which is a fraction of the original round period determined in proportion to the ratio of trainers that have not yet finished training for that round, denoted by <em id="S3.SS3.p3.1.2" class="ltx_emph ltx_font_italic">slack ratio</em>. Meanwhile, trainers that already finished by the deadline are allowed to continue training for at most N more steps before the deadline extension finishes.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p"><span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_bold">Barrierless Asynchronous Parallel (BAP):</span>
This model represents the opposite end of the
spectrum to BSP as it almost totally eliminates the
cost of synchronization by allowing waitless,
asynchronous communication between nodes,
which achieves a very low overhead (hence, higher
speedups), but it suffers from potentially slow and
even incorrect convergence with increased delays
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. In our variant of BAP, trainers are allowed to train for as long as a round may extend; that is, until a new model is released, trainers can keep advancing local training, periodically sharing it with a validator. A new model is released when a minimum ratio of labour has been submitted, and that’s when trainers should pull in the new model.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p"><span id="S3.SS3.p5.1.1" class="ltx_text ltx_font_bold">Approximate Synchronous Parallel (ASP):</span>
Although our system does not support ASP, the scheme remains note-worthy. Unlike the SSP model, ASP is concerned with limiting parameter accuracy instead of staleness. It does so by ignoring updates that are not significant to spare the synchronization costs of them <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. However, one downside of that approach is that it is not easy to decide which parameters are insignificant, along with increased complexity of implementation.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Byzantine Fault-Tolerance</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.2" class="ltx_p">Possible cases and scenarios of Byzantine behavior are countless, and decentralized systems are arguably most prone to it. In this work, we focus on lazy<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Lazy, in this context, includes nodes that are not contributing useful updates to models.</span></span></span> and malicious trainer behavior given that malicious validator behavior is mostly taken care of by the blockchain’s decentralized consensus. To remedy this, we employ a reward-penalty policy, under which each trainer <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">i</annotation></semantics></math> is assigned a trust score (or reputation) <math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="\phi_{i}" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><msub id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml">ϕ</mi><mi id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">italic-ϕ</ci><ci id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">\phi_{i}</annotation></semantics></math> such that:</p>
<p id="S3.SS4.p1.3.1" class="ltx_p ltx_align_center"><math id="S3.SS4.p1.3.1.m1.1" class="ltx_Math" alttext="0\leq\phi_{i}\leq 1" display="inline"><semantics id="S3.SS4.p1.3.1.m1.1a"><mrow id="S3.SS4.p1.3.1.m1.1.1" xref="S3.SS4.p1.3.1.m1.1.1.cmml"><mn id="S3.SS4.p1.3.1.m1.1.1.2" xref="S3.SS4.p1.3.1.m1.1.1.2.cmml">0</mn><mo id="S3.SS4.p1.3.1.m1.1.1.3" xref="S3.SS4.p1.3.1.m1.1.1.3.cmml">≤</mo><msub id="S3.SS4.p1.3.1.m1.1.1.4" xref="S3.SS4.p1.3.1.m1.1.1.4.cmml"><mi id="S3.SS4.p1.3.1.m1.1.1.4.2" xref="S3.SS4.p1.3.1.m1.1.1.4.2.cmml">ϕ</mi><mi id="S3.SS4.p1.3.1.m1.1.1.4.3" xref="S3.SS4.p1.3.1.m1.1.1.4.3.cmml">i</mi></msub><mo id="S3.SS4.p1.3.1.m1.1.1.5" xref="S3.SS4.p1.3.1.m1.1.1.5.cmml">≤</mo><mn id="S3.SS4.p1.3.1.m1.1.1.6" xref="S3.SS4.p1.3.1.m1.1.1.6.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.1.m1.1b"><apply id="S3.SS4.p1.3.1.m1.1.1.cmml" xref="S3.SS4.p1.3.1.m1.1.1"><and id="S3.SS4.p1.3.1.m1.1.1a.cmml" xref="S3.SS4.p1.3.1.m1.1.1"></and><apply id="S3.SS4.p1.3.1.m1.1.1b.cmml" xref="S3.SS4.p1.3.1.m1.1.1"><leq id="S3.SS4.p1.3.1.m1.1.1.3.cmml" xref="S3.SS4.p1.3.1.m1.1.1.3"></leq><cn type="integer" id="S3.SS4.p1.3.1.m1.1.1.2.cmml" xref="S3.SS4.p1.3.1.m1.1.1.2">0</cn><apply id="S3.SS4.p1.3.1.m1.1.1.4.cmml" xref="S3.SS4.p1.3.1.m1.1.1.4"><csymbol cd="ambiguous" id="S3.SS4.p1.3.1.m1.1.1.4.1.cmml" xref="S3.SS4.p1.3.1.m1.1.1.4">subscript</csymbol><ci id="S3.SS4.p1.3.1.m1.1.1.4.2.cmml" xref="S3.SS4.p1.3.1.m1.1.1.4.2">italic-ϕ</ci><ci id="S3.SS4.p1.3.1.m1.1.1.4.3.cmml" xref="S3.SS4.p1.3.1.m1.1.1.4.3">𝑖</ci></apply></apply><apply id="S3.SS4.p1.3.1.m1.1.1c.cmml" xref="S3.SS4.p1.3.1.m1.1.1"><leq id="S3.SS4.p1.3.1.m1.1.1.5.cmml" xref="S3.SS4.p1.3.1.m1.1.1.5"></leq><share href="#S3.SS4.p1.3.1.m1.1.1.4.cmml" id="S3.SS4.p1.3.1.m1.1.1d.cmml" xref="S3.SS4.p1.3.1.m1.1.1"></share><cn type="integer" id="S3.SS4.p1.3.1.m1.1.1.6.cmml" xref="S3.SS4.p1.3.1.m1.1.1.6">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.1.m1.1c">0\leq\phi_{i}\leq 1</annotation></semantics></math></p>
<p id="S3.SS4.p1.4.1" class="ltx_p ltx_align_center"><math id="S3.SS4.p1.4.1.m1.1" class="ltx_Math" alttext="\sum_{i=1}^{n}\phi_{i}=1" display="inline"><semantics id="S3.SS4.p1.4.1.m1.1a"><mrow id="S3.SS4.p1.4.1.m1.1.1" xref="S3.SS4.p1.4.1.m1.1.1.cmml"><mrow id="S3.SS4.p1.4.1.m1.1.1.2" xref="S3.SS4.p1.4.1.m1.1.1.2.cmml"><msubsup id="S3.SS4.p1.4.1.m1.1.1.2.1" xref="S3.SS4.p1.4.1.m1.1.1.2.1.cmml"><mo id="S3.SS4.p1.4.1.m1.1.1.2.1.2.2" xref="S3.SS4.p1.4.1.m1.1.1.2.1.2.2.cmml">∑</mo><mrow id="S3.SS4.p1.4.1.m1.1.1.2.1.2.3" xref="S3.SS4.p1.4.1.m1.1.1.2.1.2.3.cmml"><mi id="S3.SS4.p1.4.1.m1.1.1.2.1.2.3.2" xref="S3.SS4.p1.4.1.m1.1.1.2.1.2.3.2.cmml">i</mi><mo id="S3.SS4.p1.4.1.m1.1.1.2.1.2.3.1" xref="S3.SS4.p1.4.1.m1.1.1.2.1.2.3.1.cmml">=</mo><mn id="S3.SS4.p1.4.1.m1.1.1.2.1.2.3.3" xref="S3.SS4.p1.4.1.m1.1.1.2.1.2.3.3.cmml">1</mn></mrow><mi id="S3.SS4.p1.4.1.m1.1.1.2.1.3" xref="S3.SS4.p1.4.1.m1.1.1.2.1.3.cmml">n</mi></msubsup><msub id="S3.SS4.p1.4.1.m1.1.1.2.2" xref="S3.SS4.p1.4.1.m1.1.1.2.2.cmml"><mi id="S3.SS4.p1.4.1.m1.1.1.2.2.2" xref="S3.SS4.p1.4.1.m1.1.1.2.2.2.cmml">ϕ</mi><mi id="S3.SS4.p1.4.1.m1.1.1.2.2.3" xref="S3.SS4.p1.4.1.m1.1.1.2.2.3.cmml">i</mi></msub></mrow><mo id="S3.SS4.p1.4.1.m1.1.1.1" xref="S3.SS4.p1.4.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS4.p1.4.1.m1.1.1.3" xref="S3.SS4.p1.4.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.1.m1.1b"><apply id="S3.SS4.p1.4.1.m1.1.1.cmml" xref="S3.SS4.p1.4.1.m1.1.1"><eq id="S3.SS4.p1.4.1.m1.1.1.1.cmml" xref="S3.SS4.p1.4.1.m1.1.1.1"></eq><apply id="S3.SS4.p1.4.1.m1.1.1.2.cmml" xref="S3.SS4.p1.4.1.m1.1.1.2"><apply id="S3.SS4.p1.4.1.m1.1.1.2.1.cmml" xref="S3.SS4.p1.4.1.m1.1.1.2.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.1.m1.1.1.2.1.1.cmml" xref="S3.SS4.p1.4.1.m1.1.1.2.1">superscript</csymbol><apply id="S3.SS4.p1.4.1.m1.1.1.2.1.2.cmml" xref="S3.SS4.p1.4.1.m1.1.1.2.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.1.m1.1.1.2.1.2.1.cmml" xref="S3.SS4.p1.4.1.m1.1.1.2.1">subscript</csymbol><sum id="S3.SS4.p1.4.1.m1.1.1.2.1.2.2.cmml" xref="S3.SS4.p1.4.1.m1.1.1.2.1.2.2"></sum><apply id="S3.SS4.p1.4.1.m1.1.1.2.1.2.3.cmml" xref="S3.SS4.p1.4.1.m1.1.1.2.1.2.3"><eq id="S3.SS4.p1.4.1.m1.1.1.2.1.2.3.1.cmml" xref="S3.SS4.p1.4.1.m1.1.1.2.1.2.3.1"></eq><ci id="S3.SS4.p1.4.1.m1.1.1.2.1.2.3.2.cmml" xref="S3.SS4.p1.4.1.m1.1.1.2.1.2.3.2">𝑖</ci><cn type="integer" id="S3.SS4.p1.4.1.m1.1.1.2.1.2.3.3.cmml" xref="S3.SS4.p1.4.1.m1.1.1.2.1.2.3.3">1</cn></apply></apply><ci id="S3.SS4.p1.4.1.m1.1.1.2.1.3.cmml" xref="S3.SS4.p1.4.1.m1.1.1.2.1.3">𝑛</ci></apply><apply id="S3.SS4.p1.4.1.m1.1.1.2.2.cmml" xref="S3.SS4.p1.4.1.m1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS4.p1.4.1.m1.1.1.2.2.1.cmml" xref="S3.SS4.p1.4.1.m1.1.1.2.2">subscript</csymbol><ci id="S3.SS4.p1.4.1.m1.1.1.2.2.2.cmml" xref="S3.SS4.p1.4.1.m1.1.1.2.2.2">italic-ϕ</ci><ci id="S3.SS4.p1.4.1.m1.1.1.2.2.3.cmml" xref="S3.SS4.p1.4.1.m1.1.1.2.2.3">𝑖</ci></apply></apply><cn type="integer" id="S3.SS4.p1.4.1.m1.1.1.3.cmml" xref="S3.SS4.p1.4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.1.m1.1c">\sum_{i=1}^{n}\phi_{i}=1</annotation></semantics></math></p>
<p id="S3.SS4.p1.5" class="ltx_p">A trainer’s trust score is used as the weight factor for that trainer updates in the federated learning algorithm. The effect of that is two-fold. First, it enables validators to control the impact of said trainer’s updates on the model based on its trust level. Secondly, it creates intrinsic competition in the system as any rise in one trainer’s score effectively results in a relative decline in other trainers’ scores. Trust scores are adjusted based on validation results. Upon receiving a trainer update, a validator adds its gradients to the latest model weights, and computes its validation score. Subsequently, it updates the respective trainer’s trust score based on whether it leads to improvement or decline.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Architecture</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The purpose of this section is to present an overall description of a typical FLoBC<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The word FLoBC is sometimes used to refer to a system spawned through the framework rather than the framework itself.</span></span></span> system such as ones we built to conduct our experiments. Said system was not designed with full deployment in mind, hence some details have been simplified. As such, FLoBC serves as a minimal proof-of-concept.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">System Overview</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">FLoBC is a distributed system driven by two main actors: trainers and validators, the latter of which has more involved responsibilities. On a lower level, the system is composed of six main services, each providing a very specific functionality and has a strictly defined interface with other services and layers.</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Blockchain service</span>: Constitutes the main fabric of communication and data storage across all validator nodes. To ensure data consistency across all validators, this subsystem utilizes an elaborate schema for decentralized consensus, namely a variant of the Practical Byzantine Fault Tolerance (pBFT) family of consensus algorithms that are mainly based on voting and elections.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Storage schema service</span>: Facilitates and enables structured access and modification of the local storage database on which the blockchain state is stored.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Machine learning service</span>: Represents the machine learning layer on top of the blockchain. It is built as a service that handles the execution of machine learning transactions such as gradients sharing by interfacing with the local storage schema to reflect the transaction execution such as model creation and model aggregation.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><span id="S4.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Training validation service</span>: Is a hybrid layer between the blockchain and the training layer. It is responsible for validating incoming updates and returning a verdict of whether a gradient transaction should be accepted or rejected.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p"><span id="S4.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Reputation management service</span>: Handles the accounting aspects of the system, realizing a reward-punishment mechanism to ensure that only honest trainers get to stay in the network while Byzantine ones are dismissed to maintain the performance and quality of created models and ensure fairness.</p>
</div>
</li>
<li id="S4.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S4.I1.i6.p1" class="ltx_para">
<p id="S4.I1.i6.p1.1" class="ltx_p"><span id="S4.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">Model training service</span>: Performs model training including all needed intermediate transformations such as model flattening and rebuilding at the trainer side. This layer sends and receives flattened models and flattened gradients.</p>
</div>
</li>
<li id="S4.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="S4.I1.i7.p1" class="ltx_para">
<p id="S4.I1.i7.p1.1" class="ltx_p"><span id="S4.I1.i7.p1.1.1" class="ltx_text ltx_font_bold">Training flow management service</span>: Manages the communication between trainers and validators including model and gradients exchanging, along with synchronizing training iterations.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Implementation Technology</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">FLoBC is built on top of an Exonum Blockchain that uses a variant of practical Byzantine Fault Tolerance (pBFT) consensus algorithm. The primary reason behind this choice is the voting-based consensus of Exonum being much more lightweight than the typical cryptographic Proof-of-Work (PoW) consensus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. This lightweight consensus becomes even more crucial when the already computationally heavy nature of model training is taken into consideration. The majority of system services are implemented using the Rust programming language while lightweight clients are implemented using JavaScript. The training and validation are carried out using easily pluggable Python scripts.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">System Views</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">There are multiple ways to view FLoBC depending on the scope level, the most important of which are presented in the following.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS1.4.1.1" class="ltx_text">IV-C</span>1 </span>Actor-level View</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">This view is the most high-level depiction of the system in terms of its main acting entities and how they are connected.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p">In our system, we have two types of actors: trainer and validator. While trainers constitute the main work labor in the system by performing gradient calculations using their data, validators are even heavier in composition as they are the ones undertaking elections to achieve consensus and validation to ensure model quality.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para">
<p id="S4.SS3.SSS1.p3.1" class="ltx_p">One important property of our architecture is that trainers are not fully connected to the validators while validators are fully interconnected. It is worth noting that while the roles (i.e., trainer or validator) are not fixed or dedicated, a node is not allowed to perform both roles simultaneously for the same subnetwork, yet a node can be a validator on a subnetwork while being a trainer for another. Further, each model subnetwork requires at least one validator to function.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS2.4.1.1" class="ltx_text">IV-C</span>2 </span>Modular &amp; Service-level Views</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">The modular view in Figure <a href="#S4.F1" title="Figure 1 ‣ IV-C2 Modular &amp; Service-level Views ‣ IV-C System Views ‣ IV Architecture ‣ FLoBC: A Decentralized Blockchain-Based Federated Learning Framework" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> gives a somewhat closer look on a validator’s insides, showing the data flow from each high-level module to the others. It first shows that trainers communicate with validators over HTTP whose endpoint then passes the transactions to the validation module that uses the validation dataset to validate the model-update transactions in order to ultimately decide whether to accept of reject the update onto the blockchain. Validators also expose a Wire API for answering queries about the blockchain state (e.g., what’s the latest model version?), which is helpful for system diagnosis/monitoring. On the other hand, the service-level view emphasizes the separation of concerns/responsibilities across the system into multiple services illustrated in Figure <a href="#S4.F2" title="Figure 2 ‣ IV-C2 Modular &amp; Service-level Views ‣ IV-C System Views ‣ IV Architecture ‣ FLoBC: A Decentralized Blockchain-Based Federated Learning Framework" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The storage schema service is central to all services in the validator side as it represents the main interface for making persistent changes. The ML service is responsible for consolidating trainer updates and handling the trainer flow (e.g., synchronization) while the validation service is responsible for assessing trainer work and providing the results to the reputation service which can adjust trust scores accordingly. The Wire APIs on the sides represent auxiliary querying interfaces for miscellaneous purposes (e.g., for getting the latest released model version) besides the main blockchain interfaces used for sending transactions.</p>
</div>
<figure id="S4.F1" class="ltx_figure"><img src="/html/2112.11873/assets/figs/Byzantine_Modular_View.png" id="S4.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="275" height="253" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Modular View</figcaption>
</figure>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2112.11873/assets/figs/Service-level.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="180" height="194" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Service-level View</figcaption>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we describe the experiments we have carried out using FLoBC. All the experiments carried out in this section were training and validating a convolutional neural network (CNN) predicting handwritten MNIST digits on the regular MNIST dataset with images of size <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="28\times 28" display="inline"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mn id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">28</mn><mo lspace="0.222em" rspace="0.222em" id="S5.p1.1.m1.1.1.1" xref="S5.p1.1.m1.1.1.1.cmml">×</mo><mn id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml">28</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><times id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1"></times><cn type="integer" id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">28</cn><cn type="integer" id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3">28</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">28\times 28</annotation></semantics></math> pixels. Each trainer in the system uses a random 30% sample of the MNIST training dataset. The metric used to evaluate training in these experiments is accuracy, but the system will work equivalently using any machine learning model metric. The metric is easily pluggable in the Python machine learning validation script.</p>
</div>
<section id="S5.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">Benchmark: Decentralized vs. Centralized Performance</h3>

<div id="S5.SSx1.p1" class="ltx_para">
<p id="S5.SSx1.p1.1" class="ltx_p">In this experiment, we compare the performance of the centralized model with that of the decentralized model utilizing the best performing configuration (as demonstrated later by experiment 1) comprising 7 trainers and 3 validators with an active reward-penalty policy. The centralized model is the golden benchmark of our system. The aim of this experiment is to test whether the added benefits of decentralization would result in a significant cost of model quality.</p>
</div>
<section id="S5.SSx1.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Setup</h4>

<div id="S5.SSx1.SSSx1.p1" class="ltx_para">
<p id="S5.SSx1.SSSx1.p1.1" class="ltx_p">In this experiment, the centralized model uses the whole data available in the MNIST training data-set. Since in the best performing decentralized configuration there are 7 trainers affecting the model each iteration, in our centralized benchmark, each training iteration is 7 epochs. The training lasts for 30 iterations for both the centralized and decentralized runs.</p>
</div>
</section>
<section id="S5.SSx1.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Results and Discussion</h4>

<figure id="S5.F3" class="ltx_figure"><img src="/html/2112.11873/assets/figs/exp4.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Accuracy against iterations for the centralized and decentralized runs</figcaption>
</figure>
<div id="S5.SSx1.SSSx2.p1" class="ltx_para">
<p id="S5.SSx1.SSSx2.p1.1" class="ltx_p">As it can be seen in Figure <a href="#S5.F3" title="Figure 3 ‣ Results and Discussion ‣ Benchmark: Decentralized vs. Centralized Performance ‣ V Experiments ‣ FLoBC: A Decentralized Blockchain-Based Federated Learning Framework" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the centralized model had a higher accuracy than the decentralized alternative for 10 iterations, after which the decentralized model trained by 7 trainers and validated by 3 validators marginally outperformed the centralized model. The difference in accuracy is below 0.5%. This experiment shows that the added benefits of decentralization and privacy preserving training do not degrade the model performance. Note that this is not meant to demonstrate that the decentralized version is superior in performance to its centralized counterpart. Rather, it shows that the two are closely comparable in that regard.</p>
</div>
</section>
</section>
<section id="S5.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">Experiment 1: Trainers-to-Validators Ratio</h3>

<div id="S5.SSx2.p1" class="ltx_para">
<p id="S5.SSx2.p1.1" class="ltx_p">In this experiment, we have maximum N participating nodes. Each run we split N into different numbers of trainers and validators. Through this experiment we want to determine the best trainers-to-validators split that makes use of the most computing recources and achieves the best model performance while still relying on multiple validators to generate a more trustworthy model. This experiment will also show the speed of convergence to the highest accuracy by pointing out the iteration at which the highest accuracy was achieved. There is a trade-off between the number of trainers and validators that this experiment aims to balance out. Note that increasing the number of trainers with good quality data should increase the model performance. Greedily, we would want all the nodes in the system to be trainers, however, this has a major drawback. Relying on a single validator means that the validator is a trusted entity whereas relying on multiple validators increases the trustworthiness of the model because of consensus. However, we want to refrain from having too many validators because this wastes computing resources that could have been utilized in training and adds on unnecessary communication costs. A trainer shares its updates with one validator that validates those updates then shares them with the rest of the validators for further validation and finally consensus. This being said, as we increase the number of the validators, we increase the communication cost associated with voting consensus. For these reasons, for the highest efficiency and the best performance, it is important to determine the best trainers-to-validators ratio that achieves the best model quality without compromising decentralization.</p>
</div>
<section id="S5.SSx2.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Setup</h4>

<div id="S5.SSx2.SSSx1.p1" class="ltx_para">
<p id="S5.SSx2.SSSx1.p1.1" class="ltx_p">In this experiment, the N number of agents in the system is set to 10. For instance, with N = 10, one system run has 9 trainers and 1 validator while another run has 8 trainers and 2 validators, etc. Each run, we change the number of trainers and validators to try all their combinations that add up to 10 agents. Each run lasts for 30 iterations. We record the iteration and its accuracy for each run of the system.
The new model version creation waits for all the trainers to share their updates with the validators since it is important in this experiment that all trainers submit their work for validation each model update iteration since we are studying the effect of the split of the validators and trainers on the model performance.</p>
</div>
</section>
<section id="S5.SSx2.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Results and Discussion</h4>

<figure id="S5.F4" class="ltx_figure"><img src="/html/2112.11873/assets/figs/exp1.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="153" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Maximum accuracy and its iteration index for different splits of trainers and validators</figcaption>
</figure>
<div id="S5.SSx2.SSSx2.p1" class="ltx_para">
<p id="S5.SSx2.SSSx2.p1.1" class="ltx_p">As it can be seen from Figure <a href="#S5.F4" title="Figure 4 ‣ Results and Discussion ‣ Experiment 1: Trainers-to-Validators Ratio ‣ V Experiments ‣ FLoBC: A Decentralized Blockchain-Based Federated Learning Framework" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the best configuration for this model is having 3 validators and 7 trainers. Using this configuration, the maximum accuracy of 0.9874 was reached at iteration 26. Note that the least accuracy was that for the single trainer configuration since much less computing resources and data have been dedicated for training.</p>
</div>
</section>
</section>
<section id="S5.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">Experiment 2: Reward-Penalty Policy</h3>

<div id="S5.SSx3.p1" class="ltx_para">
<p id="S5.SSx3.p1.1" class="ltx_p">The purpose of this experiment is to evaluate the usefulness of computing a score for each trainer, where a trainer’s score affects the extent to which this trainer’s updates affect the overall model.</p>
</div>
<section id="S5.SSx3.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Setup</h4>

<div id="S5.SSx3.SSSx1.p1" class="ltx_para">
<p id="S5.SSx3.SSSx1.p1.1" class="ltx_p">This experiment was performed using processes representing 6 trainers (indexed from 0 to 5) and 3 validators. Training ran for 30 iterations. The BSP synchoronization scheme was employed with a sufficiently large period to allow all trainers to submit updates every synchronization period.</p>
</div>
<div id="S5.SSx3.SSSx1.p2" class="ltx_para">
<p id="S5.SSx3.SSSx1.p2.1" class="ltx_p">The updates of each of the trainers were offset using an approximately normal noise. The mean of the noise is 0 and the standard deviation of the noise is linearly proportional to the index of the trainer, with proportionality constant k = 0.0545. Thus, the updates of trainer 0 received zero noise, and the updates of trainer 5 were significantly offset by the noise.</p>
</div>
<div id="S5.SSx3.SSSx1.p3" class="ltx_para">
<p id="S5.SSx3.SSSx1.p3.1" class="ltx_p">For purposes of this experiment, the minimum acceptance threshold for updates was reduced substantially to allow low-performing updates to affect the model generation and be affected by scoring.
The control group had scoring disabled; that is, the scores of all 6 trainers were constant and equal throughout the training process. The scoring group allowed scoring for each trainer to change and to adapt to the relative qualities of the provided updates by each trainer.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Scores of all trainers across training rounds</figcaption>
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.1.1.1" class="ltx_tr">
<td id="S5.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Round</td>
<td id="S5.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5</td>
<td id="S5.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10</td>
<td id="S5.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15</td>
<td id="S5.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20</td>
<td id="S5.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">25</td>
<td id="S5.T1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30</td>
</tr>
<tr id="S5.T1.1.2.2" class="ltx_tr">
<td id="S5.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Trainer 0</td>
<td id="S5.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.16667</td>
<td id="S5.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.764622</td>
<td id="S5.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.823812</td>
<td id="S5.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.866141</td>
<td id="S5.T1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.907603</td>
<td id="S5.T1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.964613</td>
<td id="S5.T1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
</tr>
<tr id="S5.T1.1.3.3" class="ltx_tr">
<td id="S5.T1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Trainer 1</td>
<td id="S5.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.16667</td>
<td id="S5.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.235378</td>
<td id="S5.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.176188</td>
<td id="S5.T1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.133859</td>
<td id="S5.T1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.092397</td>
<td id="S5.T1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.035387</td>
<td id="S5.T1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
</tr>
<tr id="S5.T1.1.4.4" class="ltx_tr">
<td id="S5.T1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Trainer 2</td>
<td id="S5.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.16667</td>
<td id="S5.T1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
</tr>
<tr id="S5.T1.1.5.5" class="ltx_tr">
<td id="S5.T1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Trainer 3</td>
<td id="S5.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.16667</td>
<td id="S5.T1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
</tr>
<tr id="S5.T1.1.6.6" class="ltx_tr">
<td id="S5.T1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Trainer 4</td>
<td id="S5.T1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.16667</td>
<td id="S5.T1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
</tr>
<tr id="S5.T1.1.7.7" class="ltx_tr">
<td id="S5.T1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Trainer 5</td>
<td id="S5.T1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.16667</td>
<td id="S5.T1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0</td>
<td id="S5.T1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SSx3.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Results and Discussion</h4>

<div id="S5.SSx3.SSSx2.p1" class="ltx_para">
<p id="S5.SSx3.SSSx2.p1.1" class="ltx_p">During the operation of the scoring group, the scores of each trainer were updated after every synchronization period. A sample of the scores of all trainers over the 30 training iterations is displayed in Table <a href="#S5.T1" title="TABLE I ‣ Setup ‣ Experiment 2: Reward-Penalty Policy ‣ V Experiments ‣ FLoBC: A Decentralized Blockchain-Based Federated Learning Framework" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. We note that, shortly after training started, the validators realized that most trainers were providing low-quality updates, and thus, the scores of four trainers dropped to zero almost immediately. Over the remaining rounds, the score of trainer 0, whose updates were not affected by added noise, steadily increased against the score of the second-best trainer, trainer 1, whose updates were affected by a minimal amount of noise. Hence, we conclude that scoring enables the validators to accurately estimate the relative performance of each of the trainers.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2112.11873/assets/figs/exp2.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="213" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Accuracy performance of scoring group vs control (uniform) group over 30 training rounds</figcaption>
</figure>
<div id="S5.SSx3.SSSx2.p2" class="ltx_para">
<p id="S5.SSx3.SSSx2.p2.1" class="ltx_p">For the two groups, after each training round, the current model accuracy is computed. The accuracy performance of the two groups is shown in Figure <a href="#S5.F5" title="Figure 5 ‣ Results and Discussion ‣ Experiment 2: Reward-Penalty Policy ‣ V Experiments ‣ FLoBC: A Decentralized Blockchain-Based Federated Learning Framework" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We note that trainer scoring improved the performance, stability, and convergence speed of the model, as only the updates from high-performing trainers were taken into consideration and contributed to improving the model.</p>
</div>
</section>
</section>
<section id="S5.SSx4" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">Experiment 3: Synchronization Schemes</h3>

<div id="S5.SSx4.p1" class="ltx_para">
<p id="S5.SSx4.p1.1" class="ltx_p">In this experiment, we compare some of the common synchronization schemes used in parallel computing after adapting our own variations of them. The schemes implemented in FLoBC and those tested in the experiment are Bulk Synchronous Parallel (BSP), Stale Synchronous Parallel (SSP), and Barrierless Asynchronous Parallel (BAP), as previously described in the methodology. The purpose of the experiment is to compare the different schemes in terms of two main metrics: model growth relative to the number of training rounds, and the average time per training round. The former indicates convergence while the latter indicates progression speed.</p>
</div>
<section id="S5.SSx4.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Setup</h4>

<div id="S5.SSx4.SSSx1.p1" class="ltx_para">
<p id="S5.SSx4.SSSx1.p1.1" class="ltx_p">For the experiment setup, a system of three validators and six trainers is used in all runs with a uniform trust policy; that is, all trainers have the same trust scores. For both BSP and SSP, the same base synchronization barrier period is used. Different trainers are running at different paces to emulate real-world computational differences, with the synchronization barrier period set to be longer than the time most (but not all) trainers will take to finish one training job. To measure the model growth metric, the system will run for a fixed N=30 training rounds, comparing how long each scheme takes to converge on the highest model accuracy while also taking the three highest accuracies into consideration. As for the average training iteration delay, the system will run for a fixed period of 20 minutes. For BAP, we use two configurations: one with a slack ratio threshold of 0% (fully relaxed), and another with 40%. That is, in the first one, a sync barrier is thrown when all trainers have submitted at least one update in the current training round while the second one is contingent on only 60% of trainers having done so.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2112.11873/assets/figs/exp3_30iterations_4schemes.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Accuracy performance for 4 different schemes across 30 training iterations</figcaption>
</figure>
</section>
<section id="S5.SSx4.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Results and Discussion</h4>

<div id="S5.SSx4.SSSx2.p1" class="ltx_para">
<p id="S5.SSx4.SSSx2.p1.1" class="ltx_p">As depicted in Figure <a href="#S5.F6" title="Figure 6 ‣ Setup ‣ Experiment 3: Synchronization Schemes ‣ V Experiments ‣ FLoBC: A Decentralized Blockchain-Based Federated Learning Framework" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, results seemingly meet their theoretical expectations, confirming the trade-offs for the three different schemes. BSP has the lowest performance as it is the most strict in terms of synchronization, leading to lower utilization of training power. However, it is quite stable as it offers more predictable limits on speed of progression (at a fixed period). As for the two BAP runs<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Labelled as BAP<math id="footnote3.m1.1" class="ltx_Math" alttext="\_&lt;" display="inline"><semantics id="footnote3.m1.1b"><mrow id="footnote3.m1.1.1" xref="footnote3.m1.1.1.cmml"><mi mathvariant="normal" id="footnote3.m1.1.1.2" xref="footnote3.m1.1.1.2.cmml">_</mi><mo id="footnote3.m1.1.1.1" xref="footnote3.m1.1.1.1.cmml">&lt;</mo><mi id="footnote3.m1.1.1.3" xref="footnote3.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="footnote3.m1.1c"><apply id="footnote3.m1.1.1.cmml" xref="footnote3.m1.1.1"><lt id="footnote3.m1.1.1.1.cmml" xref="footnote3.m1.1.1.1"></lt><ci id="footnote3.m1.1.1.2.cmml" xref="footnote3.m1.1.1.2">_</ci><csymbol cd="latexml" id="footnote3.m1.1.1.3.cmml" xref="footnote3.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m1.1d">\_&lt;</annotation></semantics></math>majority ratio<math id="footnote3.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="footnote3.m2.1b"><mo id="footnote3.m2.1.1" xref="footnote3.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="footnote3.m2.1c"><gt id="footnote3.m2.1.1.cmml" xref="footnote3.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m2.1d">&gt;</annotation></semantics></math></span></span></span>, BAP_1 with a majority ratio of 100% was expected to perform best since it has the best utilization of trainer work, beating its BAP_0.6 counterpart, which confirms the expected effect of decreasing the majority ratio in our variant of BAP. On the other hand, SSP appeared to strike a balance between BAP and BSP. This is expected since SSP is regarded as a compromise between strict synchronization (BSP) and full relaxation (BAP).</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2112.11873/assets/figs/exp3_20minutes_4schemes.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Accuracy performance for 4 different schemes across 20 minutes of training time</figcaption>
</figure>
<div id="S5.SSx4.SSSx2.p2" class="ltx_para">
<p id="S5.SSx4.SSSx2.p2.1" class="ltx_p">Looking at Figure <a href="#S5.F7" title="Figure 7 ‣ Results and Discussion ‣ Experiment 3: Synchronization Schemes ‣ V Experiments ‣ FLoBC: A Decentralized Blockchain-Based Federated Learning Framework" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, fully relaxed BAP performed the least number of iterations (i.e., training rounds) by far due to its relaxed sync barriers. Naturally, partially relaxed BAP managed to perform significantly more rounds, eventually leading it to score the highest accuracy of all schemes. On the other hand, BSP had the furthest progression, managing to score very close to the top accuracy due to its faster pace. SSP performed fewer training iterations than BSP since it applies more relaxation on the sync barrier. However, SSP still outperforms BSP in terms of accuracy due to its better utilization of the training power. By and large, each one of these schemes suits certain system configurations (and purposes), depending on several factors, but the primary deciders are trainer pace and quality.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In summary, FLoBC is a proof-of-concept that is intended to show that its components can come together to form a coherent system with the desired levels of generality, decentralization, efficiency, privacy, and Byzantine fault-tolerance. Through our benchmark experiments, our decentralized system has shown itself to be a viable match to pure centralized training, marginally outperforming the centralized benchmark by a factor of 0.5%. Using toy systems spawned by FLoBC, we first showed that there is a quasi-optimal balance to strike on trainer-to-validator partitioning, empirically determining that a 7:3 trainer-to-validator ratio works best. Secondly, we demonstrated that even a simple reward-penalty policy can have a notable positive effect on the quality of produced models. Finally, we compared three major synchronization schemes (BSP, SSP, &amp; BAP), highlighting and contrasting their different trade-offs.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Future Work</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Despite the SGD-compatibility of a model being an assumption embedded in the system, FLoBC can be feasibly extended to handle other computational models.
Further improvements can be applied to synchronization schemes by allowing more adaptability in the sync periods based on the expected time that most trainers take to submit their updates in one round. Furthermore, to ensure a higher level of data privacy, a layer of differential privacy could be incorporated into transaction communication to limit the extent to which gradients can be inverted to extract trainer data.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Alekh Agarwal, Olivier Chapelle, Miroslav Dudík, and John Langford.

</span>
<span class="ltx_bibblock">A reliable effective terascale linear learning system.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">The Journal of Machine Learning Research</span>, 15(1):1111–1133,
2014.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau,
Tom Schaul, Brendan Shillingford, and Nando De Freitas.

</span>
<span class="ltx_bibblock">Learning to learn by gradient descent by gradient descent.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.04474</span>, 2016.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Léon Bottou.

</span>
<span class="ltx_bibblock">Large-scale machine learning with stochastic gradient descent.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of COMPSTAT’2010</span>, pages 177–186. Springer,
2010.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Amin Fadaeddini, Babak Majidi, and Mohammad Eshghi.

</span>
<span class="ltx_bibblock">Secure decentralized peer-to-peer training of deep neural networks
based on distributed ledger technology.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">The Journal of Supercomputing</span>, pages 1–15, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Andrew Gibiansky.

</span>
<span class="ltx_bibblock">Bringing hpc techniques to deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Baidu Research, Tech. Rep.</span>, 2017.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Otkrist Gupta and Ramesh Raskar.

</span>
<span class="ltx_bibblock">Distributed learning of deep neural network over multiple agents.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Journal of Network and Computer Applications</span>, 116:1–8, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
István Hegedűs, Gábor Danner, and Márk Jelasity.

</span>
<span class="ltx_bibblock">Gossip learning as a decentralized alternative to federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">IFIP International Conference on Distributed Applications and
Interoperable Systems</span>, pages 74–90. Springer, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, et al.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.04977</span>, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Hyesung Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim.

</span>
<span class="ltx_bibblock">Blockchained on-device federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">IEEE Communications Letters</span>, 24(6):1279–1283, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Anusha Lalitha, Osman Cihan Kilinc, Tara Javidi, and Farinaz Koushanfar.

</span>
<span class="ltx_bibblock">Peer-to-peer federated learning on graphs.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1901.11173</span>, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Hao Li, Asim Kadav, Erik Kruus, and Cristian Ungureanu.

</span>
<span class="ltx_bibblock">Malt: distributed data-parallelism for existing ml applications.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proceedings of the Tenth European Conference on Computer
Systems</span>, pages 1–16, 2015.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Yuzheng Li, Chuan Chen, Nan Liu, Huawei Huang, Zibin Zheng, and Qiang Yan.

</span>
<span class="ltx_bibblock">A blockchain-based decentralized federated learning framework with
committee consensus.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">IEEE Network</span>, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Ismael Martinez, Sreya Francis, and Abdelhakim Senhaji Hafid.

</span>
<span class="ltx_bibblock">Record and reward federated learning contributions with blockchain.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">2019 International Conference on Cyber-Enabled Distributed
Computing and Knowledge Discovery (CyberC)</span>, pages 50–57. IEEE, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Artificial Intelligence and Statistics</span>, pages 1273–1282.
PMLR, 2017.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Shiva Raj Pokhrel and Jinho Choi.

</span>
<span class="ltx_bibblock">A decentralized federated learning approach for connected autonomous
vehicles.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">2020 IEEE Wireless Communications and Networking Conference
Workshops (WCNCW)</span>, pages 1–6. IEEE, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Shiva Raj Pokhrel and Jinho Choi.

</span>
<span class="ltx_bibblock">Federated learning with blockchain for autonomous vehicles: Analysis
and design challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Communications</span>, 68(8):4734–4746, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Chandra Thapa, Mahawaga Arachchige Pathum Chamikara, and Seyit Camtepe.

</span>
<span class="ltx_bibblock">Splitfed: When federated learning meets split learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.12088</span>, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Joost Verbraeken, Matthijs Wolting, Jonathan Katzy, Jeroen Kloppenburg, Tim
Verbelen, and Jan S. Rellermeyer.

</span>
<span class="ltx_bibblock">A survey on distributed machine learning, 2019.

</span>
<span class="ltx_bibblock">cite arxiv:1912.09789.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Yury Yanovich, Ivan Ivashchenko, Alex Ostrovsky, Aleksandr Shevchenko, and
Aleksei Sidorov.

</span>
<span class="ltx_bibblock">Exonum: Byzantine fault tolerant protocol for blockchains.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">bitfury. com</span>, pages 1–36, 2018.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2112.11872" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2112.11873" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2112.11873">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2112.11873" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2112.11875" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 15:44:31 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
