<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2104.11776] UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments</title><meta property="og:description" content="Synthetic data generation has become essential in last years for feeding data-driven algorithms, which surpassed traditional techniques performance in almost every computer vision problem. Gathering and labelling the a…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2104.11776">

<!--Generated on Sun Mar 17 04:34:06 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Synthetic Data,  Data Generation,  Simulation,  Deep Learning,  Computer Vision
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments
<br class="ltx_break"><span id="id1.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span></span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pablo Martinez-Gonzalez1,
Sergiu Oprea1,
John Alejandro Castro-Vargas1,
<br class="ltx_break">Alberto Garcia-Garcia3,
Sergio Orts-Escolano2,
<br class="ltx_break">Jose Garcia-Rodriguez1 and
Markus Vincze4
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">1 Department of Computer Technology, University of Alicante, Spain 
<br class="ltx_break">Email: {pmartinez, soprea, jacastro, jgarcia}@dtic.ua.es
</span>
<span class="ltx_contact ltx_role_affiliation">2 Department of Computer Science and Artificial Intelligence, University of Alicante, Spain 
<br class="ltx_break">Email: sorts@ua.es
</span>
<span class="ltx_contact ltx_role_affiliation">3 Institute of Space Sciences (ICE-CSIC), Spain 
<br class="ltx_break">Email: garciagarcia@ice.csic.es
</span>
<span class="ltx_contact ltx_role_affiliation">4 Vision for Robotics Laboratory, TU Wien, Austria 
<br class="ltx_break">Email: vincze@acin.tuwien.ac.at
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Synthetic data generation has become essential in last years for feeding data-driven algorithms, which surpassed traditional techniques performance in almost every computer vision problem. Gathering and labelling the amount of data needed for these data-hungry models in the real world may become unfeasible and error-prone, while synthetic data give us the possibility of generating huge amounts of data with pixel-perfect annotations. However, most synthetic datasets lack from enough realism in their rendered images. In that context UnrealROX generation tool was presented in 2019, allowing to generate highly realistic data, at high resolutions and framerates, with an efficient pipeline based on Unreal Engine, a cutting-edge videogame engine. UnrealROX enabled robotic vision researchers to generate realistic and visually plausible data with full ground truth for a wide variety of problems such as class and instance semantic segmentation, object detection, depth estimation, visual grasping, and navigation. Nevertheless, its workflow was very tied to generate image sequences from a robotic on-board camera, making hard to generate data for other purposes. In this work, we present UnrealROX+, an improved version of UnrealROX where its decoupled and easy-to-use data acquisition system allows to quickly design and generate data in a much more flexible and customizable way. Moreover, it is packaged as an Unreal plug-in, which makes it more comfortable to use with already existing Unreal projects, and it also includes new features such as generating albedo or a <span id="id2.id1.1" class="ltx_text ltx_font_italic">Python API</span> for interacting with the virtual environment from <span id="id2.id1.2" class="ltx_text ltx_font_italic">Deep Learning</span> frameworks.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Synthetic Data, Data Generation, Simulation, Deep Learning, Computer Vision

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the past few years, the evolution of vision-based deep learning architectures have confirmed its support on synthetically generated data in order to save the lack of huge amounts of labeled image data. It is well known that gathering relevant amounts of images with ground truth in the real world is an expensive and tedious task, if not impossible in some cases. Virtual environments with powerful rendering technologies and engines allow generating huge amounts of synthetic images that can be automatically labeled.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In 2018, the Robotrix dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> was presented as a synthetically generated multi-purpose dataset for robotics. The main contribution of this dataset was the quality and variety of the data that it presented, including RGB images, depth and normal maps, and instance and semantic segmentation masks at 1080p resolution and 60 <span title="" class="ltx_glossaryref">frames per second (FPS)</span>. The tool that was developed to generate it, UnrealROX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, is based on <span title="" class="ltx_glossaryref">Unreal Engine 4 (UE4)</span>, one of the most widespread video game engines and, therefore, it is one of the most cutting-edge and evolving platforms for realistic 3D rendering. The tool was developed specifically for generating that robotic-oriented dataset, and due to that, it was not engineered for getting data in some other ways that could be useful for different purposes, such us reinforcement learning, or simply for random or scripted data gathering. In this way, in order to make UnrealROX a truly data generator for a wider range of applications, we decoupled the main functionalities from it, so that it would be easy to isolate the data acquisition tool, and to add any custom behaviour that any developer could need for generating its own data. In this paper, we present this new and more flexible workflow, and all the new functionalities that we added to the tool, as well as several qualitative experiments that prove the usefulness of synthetic data from Unreal Engine in a variety of vision-based deep learning architectures.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">This paper is organized as follows. First, Section <a href="#S2" title="II Related Works ‣ UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> analyzes already existing environments for synthetic data generation and puts our proposal in context. Next, Section <a href="#S3" title="III System ‣ UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> describes new features on the system that make it a flexible tool for generating synthetic data from <span title="" class="ltx_glossaryref">UE4</span> environments. Section <a href="#S4" title="IV Applications ‣ UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> presents different areas inside machine learning where data generated through this tool could be useful, and Section <a href="#S5" title="V Experiments ‣ UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> includes a set of qualitative experiments carried out for testing the tool usefulness. At last, Section <a href="#S6" title="VI Conclusion ‣ UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> summarizes the paper and draws conclusions about this work, and Section <a href="#S7" title="VII Limitations and Future Works ‣ UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> points out the limitations and possible future extensions for the tool.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Works</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Synthetic environments have been used for a long time to benchmark vision and robotic algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Recently, their importance has been highlighted for training and evaluating machine learning models, not only for robotic vision problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, but many others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Due to the increasing need for samples to train such data-driven architectures, there exists an increasing number of synthetic datasets.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In the context of generating data for indoor robotic tasks (the original proposal of UnrealROX), we already reviewed <span title="" class="ltx_glossaryref">Cornell House Agent Learning Environment (CHALET)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, a 3D house simulator for manipulation and navigation learning built in Unity 3D, <span title="" class="ltx_glossaryref">Household Multimodal Environment (HoME)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, a multimodal household environment for AI learning from visual, auditive, and physical information within realistic synthetic environments sourced from SUNCG, AI2-<span title="" class="ltx_glossaryref">THe House of inteRactions (THOR)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, a framework for visual AI research which consists of near-photorealistic synthetic 3D indoor scenes in which agents can navigate and change the state of actionable objects, and <span title="" class="ltx_glossaryref">Multimodal Indoor Simulator (MINOS)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, a simulator for navigation in complex indoor environments. Their main shortcomings range from lack of realism, full 3D robot meshes, or first, third or multi camera support.
Few more recent generators or environments are VirtualHome, Habitat or ElderSim.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">VirtualHome</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> is a multi-agent platform to simulate activities in a household. Agents are represented as humanoid avatars, which can move and interact with the environment through high-level instructions. It can be used to render videos of human activities, or train agents to perform complex tasks. Its strongest point is being able to generate whole sequences from just some high-level instructions, and its weakest one is probably the render and animation realism.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Habitat</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> enables the training of embodied agents (virtual robots) in a highly efficient photorealistic 3D simulation. It uses its own fast and optimized 3D simulator, and also offers an <span title="" class="ltx_glossaryref">Application Program Interface (API)</span> for end-to-end development of embodied AI algorithms: defining tasks (e.g. navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents.</p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">ElderSim</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> is a synthetic action simulation platform that can generate synthetic data on elders’ daily activities. It can generate realistic motions of synthetic characters for 55 kinds of activities, with several customizable data-generating options and output modalities. It is based on Unreal Engine, and provides an user interface for making data generation easy. ElderSim is focused on generating data from multiple and fixed points of view, and oriented to action and human pose detection.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">When presenting <span id="S2.p6.1.1" class="ltx_text ltx_font_italic">UnrealROX</span>, we already stated that few already existent tools and environments served as inspirations for the project. They were UnrealCV, Gazebo, and NVIDIA’s Isaac Sim. UnrealCV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> is a project that extends <span title="" class="ltx_glossaryref">UE4</span> to create virtual worlds and ease communication with computer vision applications. UnrealCV consists of two parts: server and client. The server is a plugin that runs embedded into an <span title="" class="ltx_glossaryref">UE4</span> game. We took the main concept and design behind UnrealCV and implemented the whole pipeline inside <span title="" class="ltx_glossaryref">UE4</span> itself to be more efficient and customizable. Gazebo <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>http://http://gazebosim.org/</span></span></span>, on the other hand, is a well-known robot simulator that enables accurate and efficient simulation of robots in indoor and outdoor environments. It integrates a robust physics engine (Bullet, ODE, Simbody, and DART), advanced 3D graphics (using OGRE), and sensors and noise modelling. Lastly, NVIDIA’s Isaac Sim<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://developer.nvidia.com/isaac-sim</span></span></span> is a virtual simulator for robotics that lets developers train and test their software using highly realistic virtual simulation environments. When UnrealROX was presented, this project was in an early development phase, and ran under <span title="" class="ltx_glossaryref">UE4</span>. Currently, the project has been integrated with <span id="S2.p6.1.2" class="ltx_text ltx_font_italic">NVIDIA Omniverse</span>, an open platform built for virtual collaboration and real-time photorealistic simulation.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Our Proposal in Context</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">UnrealROX focused on simulating a wide range of common indoor robot actions, both in terms of poses and object interactions, by leveraging a human operator to generate plausible trajectories and grasps in virtual reality. Now, we propose to take advantage of all this data-generation potential, making it easier to use it for a wider variety of goals. Unreal Engine is a widespread platform in continuous development, and any new hardware, such as a virtual or augmented reality headsets, motion capture devices, or similar, will presumably provide support for it. So, we find highly interesting to have the possibility of generating any kind of data, in a fast, easy and customizable way, with this versatile and powerful engine.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Along with the generation of raw data (RGB-D/3D/Stereo) and ground truth (2D/3D class and instance segmentation, 6D poses, and 2D/3D bounding boxes), we now provide albedo, shading maps, interaction information, and a Python <span title="" class="ltx_glossaryref">API</span>. Moreover, the pose of virtual agents with skeleton (human meshes, robots or hands, for example) can also be projected over RGB or mask images, since joint 6D pose is provided in a frame-by-frame basis.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">System</span>
</h2>

<figure id="S3.F1" class="ltx_figure">
<p id="S3.F1.1" class="ltx_p ltx_align_center"><span id="S3.F1.1.1" class="ltx_text"><img src="/html/2104.11776/assets/x1.png" id="S3.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="392" height="197" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>UnrealROX+ system diagram. In green, decoupled data acquiring system that can be used with more flexibility. In red, adapted logic from UnrealROX for using new decoupled data acquiring system, not essential for generating data. Custom classes (actors or components) are underlined.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">As we already stated when we first presented our tool in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, the rendering engine we chose to generate photorealistic RGB images was <span title="" class="ltx_glossaryref">UE4</span>. The reasons for this choice are the following ones: (1) it is arguably one of the best game engines able to produce extremely realistic renderings, (2) beyond gaming, it has become widely adopted by Virtual Reality developers and indoor/architectural visualization experts so a whole lot of tools, examples, documentation, and assets are available; (3) due to its impact across various communities, many hardware solutions offer plugins for <span title="" class="ltx_glossaryref">UE4</span> that make them work out-of-the-box; and (4) Epic Games provides the full C++ source code and updates to it, so the full suite can be used and easily modified for free. Arguably, the most attractive feature of <span title="" class="ltx_glossaryref">UE4</span> that made us take that decision is its capability to render extremely photorealistic imagery of synthetic environments. Some <span title="" class="ltx_glossaryref">UE4</span> features that enable this realism are: physically-based materials, pre-calculated bounce light via Lightmass, stationary lights, post-processing, and reflections. In addition, real-time ray tracing rendering was introduced as beta feature in Unreal Engine’s 4.22 version on April 2019 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, and nowadays it is completely available <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. This allowed us to generate ray-tracing-rendered images, as the one showed in Figure <a href="#S3.F2" title="Figure 2 ‣ III System ‣ UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, both in real time and offline, without modifying the tool.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<p id="S3.F2.1" class="ltx_p ltx_align_center"><span id="S3.F2.1.1" class="ltx_text"><img src="/html/2104.11776/assets/figures/archviz.jpg" id="S3.F2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="314" height="192" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Real time ray-tracing snapshot on <span title="" class="ltx_glossaryref">UE4</span> from the Archviz Interior Rendering sample project.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">UnrealROX was initially developed as a tool for generating synthetic data from virtual photorealistic 3D environments, as we already know. However, its workflow was totally focused on immersing a human agent in these environments through virtual reality, <span id="S3.p2.1.1" class="ltx_text ltx_font_italic">recording</span> all its movements and interactions, and later <span id="S3.p2.1.2" class="ltx_text ltx_font_italic">rebuilding</span> the sequence frame by frame in order to have the time needed to retrieve all kind of image data, at high frame rates, high resolutions, and high realism. After releasing RobotriX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and UnrealROX, we continued using the tool for generating data for different kinds of problems, and several modifications were introduced to adapt its workflow. We end up deciding to decouple the data acquisition feature of the tool from the main workflow that was used for generating RobotriX (<span id="S3.p2.1.3" class="ltx_text ltx_font_italic">record</span>, <span id="S3.p2.1.4" class="ltx_text ltx_font_italic">rebuild</span> and <span id="S3.p2.1.5" class="ltx_text ltx_font_italic">acquire</span>). In this way, the tool now allows to acquire image and other data from the virtual environment very easily, directly from the <span title="" class="ltx_glossaryref">UE4</span> visual scripting language, which is also known as <span id="S3.p2.1.6" class="ltx_text ltx_font_italic">blueprints</span>. That means that preparing scripts for generating images of objects from different point of views, or moving a camera (randomly or not) across the scene is truly fast. We also kept the possibility of generating data with the original workflow, but adapting the code for calling the new decoupled data acquiring system.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">The other big improvement for the tool was packaging it as a Unreal Engine plug-in, which is remarkable because it allows to incorporate all these interesting features to any existing project very easily. These two main changes in the tool implied many other little ones that we are going to review in this chapter, as well as other additions that the tool received during this period. For example, along with RGB, depth maps, normal maps, and instance segmentation masks, we also provide albedo directly retrieved from <span title="" class="ltx_glossaryref">UE4</span>, and its corresponding shading maps that can be post processed later. We can see the new system diagram for UnrealROX+ in Figure <a href="#S3.F1" title="Figure 1 ‣ III System ‣ UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The tool itself, as well as more extensive documentation is available as open-source software<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/3dperceptionlab/unrealrox-plus</span></span></span>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Data acquiring subsystem</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The motivation for this tool revision was decoupling the ground-truth image acquisition in order to make it faster and more flexible to use for any scenario. We decided to encapsulate all these functionalities under a custom class inheriting from the basic Unreal class <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">Camera</span>, that we called <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">ROXCamera</span>. In this way, each <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_italic">ROXCamera</span> in the scene has the possibility to be configured and requested to retrieve some concrete data, separately. Automating this process for more than one camera is as simple as keeping a data structure with their references and using them when needed. Moreover, these cameras can be referenced and called from Blueprints very easily.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2104.11776/assets/figures/gt_rgb.jpg" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="192" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2104.11776/assets/figures/gt_albedo.png" id="S3.F3.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="192" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2104.11776/assets/figures/gt_shading.png" id="S3.F3.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="192" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2104.11776/assets/figures/gt_normal.png" id="S3.F3.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="192" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2104.11776/assets/figures/gt_depth.png" id="S3.F3.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="192" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2104.11776/assets/figures/gt_mask.png" id="S3.F3.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="192" height="108" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>UnrealROX+ image data. From left to right and top down: RGB, albedo, shading map, normal map, depth map and instance segmentation map. Shading map is not generated directly from <span title="" class="ltx_glossaryref">UE4</span>, it is computed later from RGB and albedo.</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">This encapsulation simplifies usage, but we also reinterpreted the data-acquiring logic itself to make it more elegant. Original UnrealROX retrieved all image data directly from the main viewport, which is the point of view that is being rendered in <span title="" class="ltx_glossaryref">UE4</span> user’s screen. Using main viewport when generating data from several cameras implies continuously switching the viewport source camera, as well as the rendering mode of the whole scene (for generating RGB, depth, normal maps, etc). This strategy was rough, especially knowing that <span title="" class="ltx_glossaryref">UE4</span> provides a dedicated and flexible entity for capturing image data from the scene without having to use a standard camera and the viewport: the <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">SceneCapture2D</span> actor, or more precisely, the <span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_italic">CaptureComponent2D</span>, which is the component that contains its functionality.
It can be used for rendering (with its own custom render mode) a concrete point of view to a <span id="S3.SS1.p2.1.3" class="ltx_text ltx_font_italic">RenderTarget</span>, instead of using the viewport.
<span id="S3.SS1.p2.1.4" class="ltx_text ltx_font_italic">RenderTarget</span> is another entity, which encapsulates the data that is rendered by <span id="S3.SS1.p2.1.5" class="ltx_text ltx_font_italic">CaptureComponent2D</span> and can dump it to a <span id="S3.SS1.p2.1.6" class="ltx_text ltx_font_italic">Texture</span>, or to a file, for example.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Our <span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_italic">ROXCamera</span> class is designed to create one <span id="S3.SS1.p3.1.2" class="ltx_text ltx_font_italic">CaptureComponent2D</span> entity for each type of data that must be generated from that point of view, and everything is configured automatically. As we have one <span id="S3.SS1.p3.1.3" class="ltx_text ltx_font_italic">CaptureComponent2D</span> for each render mode, we don’t have to worry about changing the general render mode. The file saving logic can be easily managed from the <span id="S3.SS1.p3.1.4" class="ltx_text ltx_font_italic">RenderTarget</span>, where we can select the color codification or the file format (which differs among different image data).</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Segmentation masks</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Semantic segmentation masks are one of the most expensive data to provide in any real dataset. For any real-world image, manually discriminating at pixel level for generating a reliable mask (for semantic or instance segmentation) is a very tedious task. So, obtaining this kind of information in a pixel-perfect and automatic way is one of the most relevant reasons to use synthetic generators for creating huge image datasets.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">In our first version of UnrealROX, inspired by a preliminary implementation of UnrealCV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, we modified the engine’s source code (simply changing a flag) in order to be able to modify at will the vertex color as a post process operation, and thus being able to render and capture these kind of images. The fact of recompiling the whole engine shouldn’t be a problem, since <span title="" class="ltx_glossaryref">UE4</span> is open source, but both the development and the use by other teams become harder and slower in this way, so we developed alternatives for generating these data. At the beginning of each generating execution, every mesh on the scene is gathered, and a reference to it is stored. Then, we developed two approaches:</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">A plain-color material instance is automatically created and associated with each of those meshes. A distinct color is assigned to each mesh, in such way that, when rendering the scene in <span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">base color</span> mode (i.e. <span id="S3.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">unlit</span>, without lighting computations) with these new materials applied, we get pixel-perfect segmentation masks. Obviously, we have to keep a data structure with references to both original and plain-color materials in order to be able to swap between these two rendering modes at any time.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">A <span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">post process material</span> is applied depending on the value stored at the <span id="S3.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">custom stencil buffer</span>. This is a special GPU buffer which we have the possibility to modify. For each mesh in the scene, we can assign a custom stencil value, resulting in the buffer storing that value for each pixel that belongs to that mesh. Then, it can be used in a post process material, which act as shaders in <span title="" class="ltx_glossaryref">UE4</span>, performing operations at pixel level. Post process materials can be applied directly to a <span id="S3.I1.i2.p1.1.3" class="ltx_text ltx_font_italic">CaptureComponent2D</span> entity, which is great for our workflow.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Acquiring and storing these mask images is a delicate task, since their stored pixel values must match exactly the ones that we set numerically in the engine. In order to avoid little color variances inside the same region, tone mapping is disabled (or at least it is applied before the post process), and linear color read from <span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_italic">RenderTarget</span> must be transformed to <span id="S3.SS2.p4.1.2" class="ltx_text ltx_font_italic">sRGB</span> color space with gamma equal to 1.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">It is worth mentioning that the plain-color material approach has a remarkable disadvantage. Swapping materials for every mesh on the scene is way more computationally demanding than the <span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_italic">render mode</span> switch that we had in the first UnrealROX version, or the post process material alternative. Nevertheless, we still consider that it is viable option because we can still generate segmentation masks separately from the rest of ground truth information (as long as we are generating data offline). In this way, we will only have to perform the material swap once at the beginning of the generation, and once more at the end. On the other hand, the post process material approach has the limitation of just being able to codify stencil 256 different values, so it only should be used if the the scene has less than that number of different meshes.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Reflectance and Shading maps</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_bold">Reflectance</span> is another image data which is interesting for several problems, such as Intrinsic Image Decomposition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> or Inverse rendering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. It represents the inherent color from surfaces, the color they naturally reflect, without any lightning influence or computation (shadows, specularities, etc).
In computer graphics research area is common to use the term <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_italic">albedo</span>, which technically represents the percentage of light that is reflected by a surface <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, but can be used as well as <span id="S3.SS3.p1.1.3" class="ltx_text ltx_font_italic">reflectance</span> for referring this kind of images with only the diffuse reflection component from an illumination model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. We will use reflectance or albedo indistinctly.
This data can be directly obtained in <span title="" class="ltx_glossaryref">UE4</span> through the render mode <span id="S3.SS3.p1.1.4" class="ltx_text ltx_font_italic">base color</span>, that can be chosen in <span id="S3.SS3.p1.1.5" class="ltx_text ltx_font_italic">CaptureComponent2D</span> structure. As we already verified when working on generating segmentation masks, pixel color information obtained through images captured in base color are perfectly accurate when retrieved as described.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Shading maps</span>, on the other side, provide us information about lightning for each pixel, mainly how much it is darkened or lightened starting from the base color provided by reflectance. Image decomposition is defined as follows <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>:</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.1" class="ltx_Math" alttext="I=R\odot S" display="block"><semantics id="S3.Ex1.m1.1a"><mrow id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.2" xref="S3.Ex1.m1.1.1.2.cmml">I</mi><mo id="S3.Ex1.m1.1.1.1" xref="S3.Ex1.m1.1.1.1.cmml">=</mo><mrow id="S3.Ex1.m1.1.1.3" xref="S3.Ex1.m1.1.1.3.cmml"><mi id="S3.Ex1.m1.1.1.3.2" xref="S3.Ex1.m1.1.1.3.2.cmml">R</mi><mo lspace="0.222em" rspace="0.222em" id="S3.Ex1.m1.1.1.3.1" xref="S3.Ex1.m1.1.1.3.1.cmml">⊙</mo><mi id="S3.Ex1.m1.1.1.3.3" xref="S3.Ex1.m1.1.1.3.3.cmml">S</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.1b"><apply id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1"><eq id="S3.Ex1.m1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1"></eq><ci id="S3.Ex1.m1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.2">𝐼</ci><apply id="S3.Ex1.m1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.3"><csymbol cd="latexml" id="S3.Ex1.m1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.3.1">direct-product</csymbol><ci id="S3.Ex1.m1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.3.2">𝑅</ci><ci id="S3.Ex1.m1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.3.3">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">I=R\odot S</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">being <span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_italic">I</span> the composed image, <span id="S3.SS3.p4.1.2" class="ltx_text ltx_font_italic">R</span> the reflectance layer, and <span id="S3.SS3.p4.1.3" class="ltx_text ltx_font_italic">S</span> the shading layer. So <span id="S3.SS3.p4.1.4" class="ltx_text ltx_font_italic">I</span> is defined as the element-wise multiplication (<math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="\odot" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><mo id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">⊙</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><csymbol cd="latexml" id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">\odot</annotation></semantics></math>) between reflectance and shading. In this case, we don’t acquire this data directly from the engine. Instead, we compute it from RGB and albedo images. Starting from the equation defined previously, we can infer that:</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<table id="S3.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex2.m1.1" class="ltx_Math" alttext="S=I\varoslash(R+\epsilon)" display="block"><semantics id="S3.Ex2.m1.1a"><mrow id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml"><mi id="S3.Ex2.m1.1.1.3" xref="S3.Ex2.m1.1.1.3.cmml">S</mi><mo id="S3.Ex2.m1.1.1.4" xref="S3.Ex2.m1.1.1.4.cmml">=</mo><mi id="S3.Ex2.m1.1.1.5" xref="S3.Ex2.m1.1.1.5.cmml">I</mi><mo lspace="0.278em" rspace="0.278em" id="S3.Ex2.m1.1.1.6" xref="S3.Ex2.m1.1.1.6.cmml">⊘</mo><mrow id="S3.Ex2.m1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex2.m1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.1.1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.1.1.2.cmml">R</mi><mo id="S3.Ex2.m1.1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.Ex2.m1.1.1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.1.1.3.cmml">ϵ</mi></mrow><mo stretchy="false" id="S3.Ex2.m1.1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.1b"><apply id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1"><and id="S3.Ex2.m1.1.1a.cmml" xref="S3.Ex2.m1.1.1"></and><apply id="S3.Ex2.m1.1.1b.cmml" xref="S3.Ex2.m1.1.1"><eq id="S3.Ex2.m1.1.1.4.cmml" xref="S3.Ex2.m1.1.1.4"></eq><ci id="S3.Ex2.m1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.3">𝑆</ci><ci id="S3.Ex2.m1.1.1.5.cmml" xref="S3.Ex2.m1.1.1.5">𝐼</ci></apply><apply id="S3.Ex2.m1.1.1c.cmml" xref="S3.Ex2.m1.1.1"><ci id="S3.Ex2.m1.1.1.6.cmml" xref="S3.Ex2.m1.1.1.6">⊘</ci><share href="#S3.Ex2.m1.1.1.5.cmml" id="S3.Ex2.m1.1.1d.cmml" xref="S3.Ex2.m1.1.1"></share><apply id="S3.Ex2.m1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1"><plus id="S3.Ex2.m1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1"></plus><ci id="S3.Ex2.m1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.2">𝑅</ci><ci id="S3.Ex2.m1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.3">italic-ϵ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.1c">S=I\varoslash(R+\epsilon)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.2" class="ltx_p">which means that shading map can be computed as the matrix element-wise division (<math id="S3.SS3.p6.1.m1.1" class="ltx_Math" alttext="\varoslash" display="inline"><semantics id="S3.SS3.p6.1.m1.1a"><mo id="S3.SS3.p6.1.m1.1.1" xref="S3.SS3.p6.1.m1.1.1.cmml">⊘</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.1.m1.1b"><ci id="S3.SS3.p6.1.m1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1">⊘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.1.m1.1c">\varoslash</annotation></semantics></math>) between the image and its correspondent albedo. Since a division must be performed, we can avoid dividing by zero adding a very little value (<math id="S3.SS3.p6.2.m2.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS3.p6.2.m2.1a"><mi id="S3.SS3.p6.2.m2.1.1" xref="S3.SS3.p6.2.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.2.m2.1b"><ci id="S3.SS3.p6.2.m2.1.1.cmml" xref="S3.SS3.p6.2.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.2.m2.1c">\epsilon</annotation></semantics></math>) to albedo pixel values. For the vast majority of scenarios, white lighting can be assumed, so shading maps are widely represented as gray scale. That means that light affects to all three RGB channels equally. An example of albedo and shading map images, along with its correspondent RGB and other images, can be seen at Figure <a href="#S3.F3" title="Figure 3 ‣ III-A Data acquiring subsystem ‣ III System ‣ UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Skeletal Meshes</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">One of the key parts in the development of the original UnrealROX was storing every joint (bone) 6D transformation of the controlled pawn, which is essentially an skeleton (i.e. <span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_italic">SkeletalMeshActor</span> in <span title="" class="ltx_glossaryref">UE4</span>). We needed to keep all that information frame by frame in order to later be able to retrieve the exact pose of the skeleton for acquiring data offline. This goal was reached by creating our own class (<span id="S3.SS4.p1.1.2" class="ltx_text ltx_font_italic">ROXBasePawn</span>) that inherits from the basic <span id="S3.SS4.p1.1.3" class="ltx_text ltx_font_italic">Pawn</span> class, and adding to it all the logic needed to save and later recover joint transformations. However, this approach had a huge disadvantage: it was mandatory to use our <span id="S3.SS4.p1.1.4" class="ltx_text ltx_font_italic">ROXBasePawn</span> class in order to be able to use the offline data-acquiring workflow with an skeletal mesh. This may imply migrate code and assets from an already existing class, to ours. The alternative we developed to better align this with <span title="" class="ltx_glossaryref">UE4</span> programming standards and the fact of delivering a plug-in, consisted of encapsulating this joint-retrieving logic as a component, so that it can be added to any existing actor with a <span id="S3.SS4.p1.1.5" class="ltx_text ltx_font_italic">SkeletalMeshComponent</span>.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.4.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.5.2" class="ltx_text ltx_font_italic">Python communication for Reinforcement Learning</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">One of the biggest future works mentioned on the original UnrealROX paper was a <span id="S3.SS5.p1.1.1" class="ltx_text ltx_font_italic">Python <span title="" class="ltx_glossaryref">API</span></span> in order to be able to modify Unreal Engine’s scene from deep learning frameworks. This workflow is specially useful for reinforcement learning models, since they are systems that successively improve the actions they take in an environment by maximizing a reward function. So, these models need to modify the scene, see what happens (retrieving data from it) and check how the reward function behaves. In order to make our tool useful for these applications, and therefore, make <span title="" class="ltx_glossaryref">UE4</span> usable for reinforcement learning, we developed several commands that can be thrown from Python programming language, and caught by Unreal Engine, performing the correspondent actions in the virtual scene. The system is defined as a client-server architecture, where Python API acts as client, and our <span title="" class="ltx_glossaryref">UE4</span> plug-in as server, and it works both locally and remotely. We defined the following groups of commands (further documentation is available at GitHub repository):</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Lists</span>: In order to interact with actors and meshes in the scene, we must know which they are, so we implemented commands to list them depending of their type, such as <span id="S3.I2.i1.p1.1.2" class="ltx_text ltx_font_typewriter">actor_list</span>, <span id="S3.I2.i1.p1.1.3" class="ltx_text ltx_font_typewriter">object_list</span>, <span id="S3.I2.i1.p1.1.4" class="ltx_text ltx_font_typewriter">camera_list</span>, <span id="S3.I2.i1.p1.1.5" class="ltx_text ltx_font_typewriter">skeletal_list</span>, among others.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Transformations</span>: To apply transformations over actors in the scene we have <span id="S3.I2.i2.p1.1.2" class="ltx_text ltx_font_typewriter">move</span>, <span id="S3.I2.i2.p1.1.3" class="ltx_text ltx_font_typewriter">rotate</span> and <span id="S3.I2.i2.p1.1.4" class="ltx_text ltx_font_typewriter">scale</span>, as well as their correspondent commands for retrieving the current location, rotation and scale.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p"><span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Cameras</span>: We can, for example, spawn and orientate cameras with <span id="S3.I2.i3.p1.1.2" class="ltx_text ltx_font_typewriter">spawn_camera</span> and <span id="S3.I2.i3.p1.1.3" class="ltx_text ltx_font_typewriter">camera_look_at</span>, respectively.</p>
</div>
</li>
<li id="S3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i4.p1" class="ltx_para">
<p id="S3.I2.i4.p1.1" class="ltx_p"><span id="S3.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">Data acquiring</span>: We can acquire image data from a concrete camera with <span id="S3.I2.i4.p1.1.2" class="ltx_text ltx_font_typewriter">get_rgb</span>, <span id="S3.I2.i4.p1.1.3" class="ltx_text ltx_font_typewriter">get_depth</span>, <span id="S3.I2.i4.p1.1.4" class="ltx_text ltx_font_typewriter">get_normal</span>, <span id="S3.I2.i4.p1.1.5" class="ltx_text ltx_font_typewriter">get_instance_mask</span> and <span id="S3.I2.i4.p1.1.6" class="ltx_text ltx_font_typewriter">get_albedo</span>. Other commands can configure this operations, and also acquire other kind of data, such as bounding boxes with <span id="S3.I2.i4.p1.1.7" class="ltx_text ltx_font_typewriter">get_3d_bounding_box</span>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS6.4.1.1" class="ltx_text">III-F</span> </span><span id="S3.SS6.5.2" class="ltx_text ltx_font_italic">Grasping subsystem and interaction information</span>
</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">The grasping subsystem that we used for the robot-object interactions in RobotriX, which was included on the original version of UnrealROX, was completely decoupled and evolved on its own project with <span id="S3.SS6.p1.1.1" class="ltx_text ltx_font_italic">UnrealGrasp</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, which is still being developed in parallel. It will also be released as a plug-in, completely compatible with UnrealROX+. This also decoupled interaction information that was added to the rest of frame-by-frame data stored for movable <span id="S3.SS6.p1.1.2" class="ltx_text ltx_font_italic">StaticMeshActor</span> actors. Data gathered for each frame was: if that object was being touched (overlapped), grasped, or none, by the controlled actor. We left this information to be given separately by UnrealGrasp, so UnrealROX+ just provide a list of overlapping actors. This is, for each actor, the ones that are being overlapped in that concrete frame, not distinguishing static from skeletal meshes.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Applications</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">UnrealROX+ provides a great variety of data. The set of tasks and problems that can be addressed using such data ranges from low to high-level ones. Some of the most relevant low-level tasks include:</p>
</div>
<div id="S4.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Depth Estimation: Both monocular (deep learning models estimating depth from RGB) and stereo (estimate 3D information from a pair of displaced cameras) are enabled by our system. Stereo may be reached manually by placing two cameras, but we implemented it natively so that it was more comfortable to work with camera pairs.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Object detection and pose estimation: We provide rich information about objects in the scene (instance and class segmentation masks, 2D and 3D bounding boxes, 6D position) to work in these problems.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Instance/Class segmentation: Instance masks are directly provided, and they can be post-processed for enabling class segmentation as well.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Normal estimation: Estimating the normal map of a given surface is an important previous step for many other tasks. For instance, certain algorithms require normal information in a point cloud to extract possible grasping points. UnrealROX+ provides per-pixel normal information.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p">Intrinsic image decomposition: Image decomposition in its intrinsic parts, reflectance and shading.</p>
</div>
</li>
</ul>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">That low-level data enables other higher-level tasks that either make use of the output of those systems, or take the low-level data as input, or both:</p>
</div>
<div id="S4.p4" class="ltx_para">
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">Hand pose estimation: UnrealROX+ provides the 6D pose of every skeleton joint, so hand pose can be estimated. It is useful for gesture detection, for example.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">Human pose estimation: Again, 6D pose of every skeleton joint is provided, so skeleton pose estimation can be trained with data generated by our tool.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p">Obstacle avoidance and navigation: By leveraging various types of low-level information such as RGB images, depth maps, bounding boxes, and semantic segmentation, robots can learn to avoid obstacles (by detecting objects and estimating their distance) and even navigate in indoor environments.</p>
</div>
</li>
</ul>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">Data generated by UnrealROX+ can be useful for other applications, although in this case they may need further development or combination with other tools:</p>
<ul id="S4.I3" class="ltx_itemize">
<li id="S4.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i1.p1" class="ltx_para">
<p id="S4.I3.i1.p1.1" class="ltx_p">Reinforcement learning: Python <span title="" class="ltx_glossaryref">API</span> enables using the tool for generating data on demand directly from a reinforcement learning model in training process.</p>
</div>
</li>
<li id="S4.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i2.p1" class="ltx_para">
<p id="S4.I3.i2.p1.1" class="ltx_p">Visual grasping: In combination with <span id="S4.I3.i2.p1.1.1" class="ltx_text ltx_font_italic">UnrealGrasp</span>, agents in the virtual scene can perform plausible grasps and record RGB and hand pose data.</p>
</div>
</li>
<li id="S4.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i3.p1" class="ltx_para">
<p id="S4.I3.i3.p1.1" class="ltx_p">Action detection and prediction: RGB and skeleton pose data are provided directly, and further labelling for actions at multiple granularity levels can be made afterwards with proper tools. Even some kind of automatic labelling can be developed if pre-designed animations are being used for performing those actions.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In the previous section we showed multiple potential applications to which our data generator and the corresponding ground truth could be applied to train machine learning systems. Besides tasks such us depth estimation and visual grasping, which relied on data generated via the previous workflow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, we selected two more applications in which the new UnrealROX+ is exploited.
We show the usefulness of the generated data in the aforementioned tasks, complementing the already available datasets from the real domain.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Hand joint position estimation</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">3D hand joint estimation is a computer vision challenge that have been addressed through deep learning models in the last years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. However, datasets that provide hand pose or joints precise position in 3D are not very common and extensive. The problem consist of estimating hand joint positions in 3D from an RGB image, so data such as segmentation masks, depth (to compute point clouds) and skeleton pose are needed. In this context, UnrealROX+ is pretty useful for generating this kind of data easily from <span title="" class="ltx_glossaryref">UE4</span>. That has been done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, where a hand pose dataset has been developed using an early version of UnrealROX+ (see Figure <a href="#S5.F4" title="Figure 4 ‣ V-A Hand joint position estimation ‣ V Experiments ‣ UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). Moreover, a graph convolutional network fed with point cloud data, GraphHands, is presented to validate the generated data.</p>
</div>
<figure id="S5.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2104.11776/assets/figures/grasp2.jpg" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="293" height="220" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2104.11776/assets/figures/grasp3.jpg" id="S5.F4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="293" height="220" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2104.11776/assets/figures/grasp4.jpg" id="S5.F4.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="293" height="220" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2104.11776/assets/x2.png" id="S5.F4.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="325" height="224" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>This figure shows a subset of samples images with 3 of the objects available on the UnrealROX-generated dataset presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, as well as a point-cloud example. Point clouds are computed from the dataset and used for feeding GraphHands.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">6D pose estimation</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Another widely used technique for which data generated with our tool can be helpful is 6D pose estimation of objects from 2D RGB images. This approach takes the object location problem one step further since it infers 3D rotation of the detected objects besides its location in an image (traditionally represented with a 2D bounding box). As a result, this estimation gives back a 3D bounding box that will estimate both 3D location (centroid) and rotation of the object.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Pix2Pose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> is a recent model that addresses this problem. It implements an auto-encoder architecture designed to estimate 3D position of each pixel from a 2D RGB image. In addition, it includes some interesting particularities, such its robustness to occlusions by leveraging recent achievements in generative adversarial training to precisely recover occluded parts. To prove the usefulness of our generator in this problem, we trained Pix2Pose with our simulated data for single object pose estimation. The objects chosen for the experimentation have been taken from YCB model set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, since it provides both real objects, that can be ordered for physically having them, and high-precise registered models from that objects, that can be imported to <span title="" class="ltx_glossaryref">UE4</span>. The real data we are going to use is the one provided from the YCBv dataset, from the BOP 6D pose Benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, which contains image sequences with real objects from YCB model set. So, we can generate fully annotated synthetic data to train the network, and real data to test it. Some qualitative results can be appreciated in Figure <a href="#S5.F5" title="Figure 5 ‣ V-B 6D pose estimation ‣ V Experiments ‣ UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual 3D Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, where we can see how 3D bounding boxes for this two objects are pretty well estimated over real data.</p>
</div>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2104.11776/assets/figures/ps_mustard.jpg" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="293" height="165" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2104.11776/assets/figures/ps_banana.jpg" id="S5.F5.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="293" height="165" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2104.11776/assets/figures/pr_mustard.png" id="S5.F5.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="293" height="220" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2104.11776/assets/figures/pr_banana.png" id="S5.F5.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="293" height="220" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Above, example of training synthetic data with two different objects generated with UnrealROX+. Below, qualitative evaluation of 6D pose estimation with Pix2Pose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> over real data from YCBv dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> (in blue estimated pose, in green ground truth).</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper presented major improvements on the original UnrealROX, our previously presented tool for generating synthetic data from realistic 3D environments in Unreal Engine 4. It already demonstrated its potential for simulating robotic interactions and generating huge amounts of data that facilitates training data-driven methods for various applications such as semantic segmentation, depth estimation, or object recognition. However, it lacked from the flexibility and modularity for making the tool profitable on a wider range of scenarios, and for a bigger number of researches. So, we worked on decoupling the main data-retrieval system from the main workflow in order to make it much easier and faster to set up and script custom behaviours with Unreal Engine’s visual scripting language.
Moreover, new kinds of data, such as albedo or shading maps, or more efficient ways of acquire and managing data, such as segmentation masks or skeleton pose retrieval, were also added, along with useful features such as the Python <span title="" class="ltx_glossaryref">API</span>, useful for Reinforcement Learning.
All UnrealROX original features (multicamera, bounding boxes, …), and also the original workflow that was used for generating RobotriX, are included or migrated to this new version of the tool, this time as an easy-to-use Unreal Engine plug-in. Grasping system, that has been moved to a separate project that will have its own plug-in, which will be completely compatible with UnrealROX+.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Limitations and Future Works</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Most of the limitations and future works that we presented in our previous paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> have been addressed in this work, and many others are left outside for having decoupled grasping from the data-acquiring tool.
One that remains unsolved is the possibility of managing non-rigid objects and deformations offline, this is, being able to recreate these behaviours in a frame-by-frame basis for acquiring data at a slower and more precise regime.
Another possible improvement points are creating a system able to process <span title="" class="ltx_glossaryref">Unified Robot Description File (URDF)</span> to automatically import robots,
or providing an additional segmentation layer for objects behind transparent materials.
Camera distortion and noise are aspects that may distinguish real from synthetic images. They can be addressed as a post-process, but it could be something interesting to parameterize in the tool itself. Other phenomenons, such as camera shake, very common on robotic applications, are not provided or guided by our tool, although they could be simulated through <span title="" class="ltx_glossaryref">UE4</span>.
Talking about new functionalities, the presented Python <span title="" class="ltx_glossaryref">API</span> for Reinforcement Learning is somewhat preliminary and many more commands and more efficient or comfortable ways of working may be added. Lastly, the main future work is definitely the improved and encapsulated version of <span id="S7.p1.1.1" class="ltx_text ltx_font_italic">UnrealGrasp</span>, including detailed interaction information.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work has been funded by the Spanish Government PID2019-104818RB-I00 grant for the MoDeaAS project, supported with Feder funds. This work has also been supported by Spanish national grants for PhD studies FPU17/00166, ACIF/2018/197 and UAFPU2019-13. Experiments were made possible by a generous hardware donation from NVIDIA.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Garcia-Garcia, P. Martinez-Gonzalez, S. Oprea, J. A. Castro-Vargas,
S. Orts-Escolano, J. Garcia-Rodriguez, and A. Jover-Alvarez, “The robotrix:
An extremely photorealistic and very-large-scale indoor dataset of sequences
with robot trajectories and interactions,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IROS</em>, 2018, pp.
6790–6797.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
P. Martinez-Gonzalez, S. Oprea, A. Garcia-Garcia, A. Jover-Alvarez,
S. Orts-Escolano, and J. Garcia-Rodriguez, “UnrealROX: An extremely
photorealistic virtual reality environment for robotics simulations and
synthetic data generation,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Virtual Reality</em>, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black, “A naturalistic open
source movie for optical flow evaluation,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">European Conf. on
Computer Vision (ECCV)</em>, ser. Part IV, LNCS 7577, A. Fitzgibbon et al.
(Eds.), Ed.   Springer-Verlag, Oct.
2012, pp. 611–625.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. Brodeur, E. Perez, A. Anand, F. Golemo, L. Celotti, F. Strub, J. Rouat,
H. Larochelle, and A. Courville, “Home: A household multimodal
environment,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.11017</em>, 2017.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez, “The synthia
dataset: A large collection of synthetic images for semantic segmentation of
urban scenes,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition</em>, 2016, pp. 3234–3243.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and
K. Goldberg, “Dex-net 2.0: Deep learning to plan robust grasps with
synthetic point clouds and analytic grasp metrics,” 2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J. Mu, W. Qiu, G. D. Hager, and A. L. Yuille, “Learning from synthetic
animals,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, June 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Q. Wang, J. Gao, W. Lin, and Y. Yuan, “Learning from synthetic data for crowd
counting in the wild,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, June 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
E. Richardson, M. Sela, and R. Kimmel, “3d face reconstruction by
learning from synthetic data,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2016 Fourth International Conference
on 3D Vision (3DV)</em>, 2016, pp. 460–469.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
N. Garnett, R. Uziel, N. Efrat, and D. Levi, “Synthetic-to-real domain
adaptation for lane detection,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Asian Conference
on Computer Vision (ACCV)</em>, November 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
D. Ludl, T. Gulde, and C. Curio, “Enhancing data-driven algorithms for
human pose estimation and action recognition through simulation,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Intelligent Transportation Systems</em>, vol. 21, no. 9, pp.
3990–3999, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
C. Yan, D. Misra, A. Bennnett, A. Walsman, Y. Bisk, and Y. Artzi, “Chalet:
Cornell house agent learning environment,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1801.07357</em>, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi, “Ai2-thor:
An interactive 3d environment for visual ai,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1712.05474</em>, 2017.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. Savva, A. X. Chang, A. Dosovitskiy, T. Funkhouser, and V. Koltun, “Minos:
Multimodal indoor simulator for navigation in complex environments,”
<em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1712.03931</em>, 2017.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba,
“Virtualhome: Simulating household activities via programs,” in
<em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.   IEEE Computer
Society, 2018, pp. 8494–8502.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
M. Savva, J. Malik, D. Parikh, D. Batra, A. Kadian, O. Maksymets, Y. Zhao,
E. Wijmans, B. Jain, J. Straub, J. Liu, and V. Koltun, “Habitat: A
platform for embodied AI research,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ICCV</em>.   IEEE, 2019, pp. 9338–9346.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
H. Hwang, C. Jang, G. Park, J. Cho, and I. Kim, “Eldersim: A synthetic data
generation platform for human action recognition in eldercare applications,”
<em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2010.14742, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
W. Qiu and A. Yuille, “Unrealcv: Connecting computer vision to unreal
engine,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>.   Springer, 2016, pp. 909–916.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
W. Qiu, F. Zhong, Y. Zhang, S. Qiao, Z. Xiao, T. S. Kim, and Y. Wang,
“Unrealcv: Virtual worlds for computer vision,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
2017 ACM on Multimedia Conference</em>.   ACM, 2017, pp. 1221–1224.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
J. Cañada. Real-time ray tracing in unreal engine. [Online]. Available:
https://www.unrealengine.com/en-US/blog/real-time-ray-tracing-in-unreal-engine-part-1—the-evolution

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
R. Cowgill. Introducing ray tracing in ue4. [Online]. Available:
https://developer.nvidia.com/blog/introducing-ray-tracing-in-unreal-engine-4/

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
S. Bell, K. Bala, and N. Snavely, “Intrinsic images in the wild,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">ACM
Trans. Graph.</em>, vol. 33, no. 4, pp. 159:1–159:12, 2014.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Y. Yu and W. A. P. Smith, “Inverserendernet: Learning single image inverse
rendering,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.   Computer Vision Foundation / IEEE, 2019, pp. 3155–3164.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J. Coakley, “Reflectance and albedo, surface,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Encyclopedia of the
Atmosphere</em>, pp. 1914–1923, 2003.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Y. Sato, M. D. Wheeler, and K. Ikeuchi, “Object shape and reflectance modeling
from observation,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 24th annual conference on
Computer graphics and interactive techniques</em>, 1997, pp. 379–387.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
S. Oprea, P. Martinez-Gonzalez, A. Garcia-Garcia, J. A. Castro-Vargas,
S. Orts-Escolano, and J. G. Rodríguez, “A visually realistic
grasping system for object manipulation and interaction in virtual reality
environments,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Comput. Graph.</em>, vol. 83, pp. 77–86, 2019.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
M. Asadi-Aghbolaghi, A. Clapés, M. Bellantonio, H. J. Escalante,
V. Ponce-López, X. Baró, I. Guyon, S. Kasaei, and S. Escalera,
“A survey on deep learning based approaches for action and gesture
recognition in image sequences,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">2017 12th IEEE International
Conference on Automatic Face Gesture Recognition (FG 2017)</em>, 2017, pp.
476–483.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
J.-A. Castro-Vargas, A. Garcia-Garcia, S. Oprea, P. Martinez-Gonzalez, and
J. Garcia-Rodriguez, “3d hand joints position estimation with graph
convolutional networks: A graphhands baseline,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Robot 2019: Fourth
Iberian Robotics Conference</em>, M. F. Silva, J. Luís Lima, L. P. Reis,
A. Sanfeliu, and D. Tardioli, Eds., 2020, pp. 551–562.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
K. Park, T. Patten, and M. Vincze, “Pix2pose: Pix2pose: Pixel-wise coordinate
regression of objects for 6d pose estimation,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">The IEEE
International Conference on Computer Vision (ICCV)</em>, Oct 2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
B. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel, and A. M.
Dollar, “Benchmarking in manipulation research: Using the
yale-cmu-berkeley object and model set,” <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics Automation
Magazine</em>, vol. 22, no. 3, pp. 36–52, 2015.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
T. Hodaň, F. Michel, E. Brachmann, W. Kehl, A. Glent Buch, D. Kraft,
B. Drost, J. Vidal, S. Ihrke, X. Zabulis, C. Sahin, F. Manhardt, F. Tombari,
T.-K. Kim, J. Matas, and C. Rother, “BOP: Benchmark for 6D object pose
estimation,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision (ECCV)</em>, 2018.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2104.11775" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2104.11776" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2104.11776">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2104.11776" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2104.11777" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 04:34:06 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
