<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.04906] Model-Agnostic Federated Learning</title><meta property="og:description" content="Since its debut in 2016, Federated Learning (FL) has been tied to the inner workings of Deep Neural Networks (DNNs); this allowed its development as DNNs proliferated but neglected those scenarios in which using DNNs i…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Model-Agnostic Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Model-Agnostic Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.04906">

<!--Generated on Thu Feb 29 21:04:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Machine Learning Federated Learning Federated AdaBoost Software Engineering">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>University of Turin, Turin, Italy 
<br class="ltx_break"><span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{gianluca.mittone, iacopo.colonnelli, robert.birke, marco.aldinucci}@unito.it</span></span></span>
</span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>University of Verona, Verona, Italy 
<br class="ltx_break"><span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>walter.riviera@univr.it</span></span></span></span></span></span>
<div id="id1a" class="ltx_logical-block">
<div id="id1.p1" class="ltx_para">
<p id="id1.p1.1" class="ltx_p ltx_align_center">The following paper is the accepted version of Springer copyrighted material</p>
<p id="id1.p1.2" class="ltx_p ltx_align_center"><span id="id1.p1.2.1" class="ltx_text ltx_font_italic">Mittone, G., Riviera, W., Colonnelli, I., Birke, R., Aldinucci, M. (2023). Model-Agnostic Federated Learning. In: Cano, J., Dikaiakos, M.D., Papadopoulos, G.A., Pericàs, M., Sakellariou, R. (eds) Euro-Par 2023: Parallel Processing. Euro-Par 2023. Lecture Notes in Computer Science, vol 14100. Springer, Cham</span></p>
<p id="id1.p1.3" class="ltx_p ltx_align_center">presented at the EuroPar’23 conference in Limassol, Cyprus.</p>
<p id="id1.p1.4" class="ltx_p ltx_align_center">DOI: <a target="_blank" href="https://doi.org/10.1007/978-3-031-39698-4_26" title="" class="ltx_ref ltx_href">10.1007/978-3-031-39698-4_26</a></p>
</div>
</div>
<h1 class="ltx_title ltx_title_document">Model-Agnostic Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gianluca Mittone
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-1887-6911" title="ORCID identifier" class="ltx_ref">0000-0002-1887-6911</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Walter Riviera
</span><span class="ltx_author_notes">22
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-5292-7594" title="ORCID identifier" class="ltx_ref">0000-0001-5292-7594</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Iacopo Colonnelli
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-9290-2017" title="ORCID identifier" class="ltx_ref">0000-0001-9290-2017</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Robert Birke
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-1144-3707" title="ORCID identifier" class="ltx_ref">0000-0003-1144-3707</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marco Aldinucci
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-8788-0829" title="ORCID identifier" class="ltx_ref">0000-0001-8788-0829</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Since its debut in 2016, Federated Learning (FL) has been tied to the inner workings of Deep Neural Networks (DNNs); this allowed its development as DNNs proliferated but neglected those scenarios in which using DNNs is not possible or advantageous.
The fact that most current FL frameworks only support DNNs reinforces this problem.
To address the lack of non-DNN-based FL solutions, we propose MAFL (Model-Agnostic Federated Learning).
MAFL merges a model-agnostic FL algorithm, AdaBoost.F, with an open industry-grade FL framework: Intel<sup id="id2.id1.1" class="ltx_sup">®</sup> OpenFL.
MAFL is the first FL system not tied to any machine learning model, allowing exploration of FL beyond DNNs.
We test MAFL from multiple points of view, assessing its correctness, flexibility, and scaling properties up to 64 nodes of an HPC cluster.
We also show how we optimised OpenFL achieving a 5.5x speedup over a standard FL scenario.
MAFL is compatible with x86-64, ARM-v8, Power and RISC-V.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Machine Learning Federated Learning Federated AdaBoost Software Engineering
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated Learning (FL) is a Machine Learning (ML) technique that has gained tremendous popularity in the last years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>: a shared ML model is trained without ever exchanging the data owned by each party or requiring it to be gathered in one common computational infrastructure.
The popularity of FL caused the development of a plethora of FL frameworks, e.g., Flower <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, FedML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and HPE Swarm Learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> to cite a few.
These frameworks only support one ML model type: Deep Neural Networks (DNNs).
While DNNs have shown unprecedented results across a wide range of applications, from image recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> to natural language processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, from drug discovery <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> to fraud detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, they are not the best model for every use case.
DNNs require massive amounts of data, which collecting and eventually labelling is often prohibitive; furthermore, DNNs are not well-suited for all types of data.
For example, traditional ML models can offer a better performance-to-complexity ratio on tabular data than DNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
DNNs also behave as black-box, making them undesirable when the model’s output has to be explained <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
Lastly, DNNs require high computational resources, and modern security-preserving approaches, e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, only exacerbate this issues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">We propose the open-source <span id="S1.p2.1.1" class="ltx_text ltx_font_bold">MAFL<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1.1.1.1" class="ltx_text ltx_font_medium">1</span></span><a target="_blank" href="https://github.com/alpha-unito/Model-Agnostic-FL" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_medium">https://github.com/alpha-unito/Model-Agnostic-FL</a></span></span></span></span> (<em id="S1.p2.1.2" class="ltx_emph ltx_font_italic">Model-Agnostic Federated Learning</em>) framework to alleviate these problems.
MAFL leverages <em id="S1.p2.1.3" class="ltx_emph ltx_font_italic">Ensemble Learning</em> to support and aggregate ML models independently from their type.
Ensemble Learning exploits the combination of multiple <span id="S1.p2.1.4" class="ltx_text ltx_font_italic">weak learners</span> to obtain a single <span id="S1.p2.1.5" class="ltx_text ltx_font_italic">strong learner</span>.
A weak learner is a learning algorithm that only guarantees performance better than a random guessing model; in contrast, a strong learner provides a very high learning performance (at least on the training set).
Since weak learners are not bound to be a specific ML model, Ensemble Learning techniques can be considered <em id="S1.p2.1.6" class="ltx_emph ltx_font_italic">model-agnostic</em>.
We adopt the AdaBoost.F algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, which leverages the AdaBoost algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and adapts it to the FL setting, and we marry it with an open-source industry-grade FL platform, i.e., Intel<sup id="S1.p2.1.7" class="ltx_sup">®</sup> OpenFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
To our knowledge, MAFL is the first and only model-agnostic FL framework available to researchers and industry at publication.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The rest of the paper introduces the basic concepts behind MAFL.
We provide implementation details underlying its development, highlight the challenges we overcame, and empirically assess our approach from the computational performances and learning metrics points of view.
To summarise, the contributions of this paper are the following:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">we introduce MAFL, the first FL software able to work with any supervised ML model, from heavy DNNs to lightweight trees;</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">we describe the architectural challenges posed by a model-agnostic FL framework in detail;</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">we describe how Intel<sup id="S1.I1.i3.p1.1.1" class="ltx_sup">®</sup> OpenFL can be improved to boost computational performances;</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">we provide an extensive empirical evaluation of MAFL to showcase its correctness, flexibility, and performance.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><em id="S2.p1.1.1" class="ltx_emph ltx_font_italic">FL</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> usually refers to a centralised structure in which two types of entities, a single <em id="S2.p1.1.2" class="ltx_emph ltx_font_italic">aggregator</em> and multiple <em id="S2.p1.1.3" class="ltx_emph ltx_font_italic">collaborators</em>, work together to solve a common ML problem.
A FL framework orchestrate the federation by distributing initial models, collecting the model updates, merging them according to an aggregation strategy, and broadcasting back the updated model.
FL requires a <em id="S2.p1.1.4" class="ltx_emph ltx_font_italic">higher-level software infrastructure</em> than traditional ML flows due to the necessity of exchanging model parameters quickly and securely.
Model training is typically delegated to de-facto standard (deep) ML frameworks, e.g., PyTorch and TensorFlow.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Different <em id="S2.p2.1.1" class="ltx_emph ltx_font_italic">FL frameworks</em> are emerging.
Riviera <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> provides a compelling list of 36 open-source tools ranked by community adoption, popularity growth, and feature maturity, and Beltrán <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> reviews 16 FL frameworks, identifying only six as mature.
All of the surveyed frameworks support supervised training of DNNs, but only FATE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, IBM-Federated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, and NVIDIA FLARE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> offer support for a few different ML models, mainly implementing federated K-means or Extreme Gradient Boosting (XGBoost): this is due to the problem of defining a model-agnostic aggregation strategy.
DNNs’ client updates consist of tensors (mainly weights or gradients) that can be easily serialised and mathematically combined (e.g., averaged), as are also the updates provided by federated K-means and XGBoost.
This assumption does not hold in a model-agnostic scenario, where the serialisation infrastructure and the aggregation mechanism have to be powerful enough to accommodate different update types.
A truly model-agnostic aggregation strategy should be able to aggregate not only tensors, but also complex objects like entire ML model.
AdaBoost.F is capable of doing that.
Section <a href="#S3" title="3 Model-agnostic Federated Algorithms ‣ Model-Agnostic Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> delves deeper into the state-of-the-art of federated ensemble algorithms.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">As a base for developing MAFL, we chose a mature, open-source framework supporting only DNNs: Intel<sup id="S2.p3.1.1" class="ltx_sup">®</sup> OpenFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
The reason for this choice is twofold: (i) its structure and community support; and (ii) the possibility of leveraging the existing ecosystem by maintaining the same use and feel.
Section <a href="#S4" title="4 MAFL Architecture ‣ Model-Agnostic Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> delves into the differences between plain OpenFL and its MAFL extension, showing how much DNN-centric a representative modern FL framework can be.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Model-agnostic Federated Algorithms</h2>

<figure id="S3.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04906/assets/x1.png" id="S3.F1.sf1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="691" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F1.sf1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">DistBoost.F</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04906/assets/x2.png" id="S3.F1.sf2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="691" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F1.sf2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">PreWeak.F</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04906/assets/x3.png" id="S3.F1.sf3.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="691" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.sf3.3.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F1.sf3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">AdaBoost.F</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.21.10.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.18.9" class="ltx_text ltx_font_bold" style="font-size:90%;">The three protocols implied by DistBoost.F, PreWeak.F, and AdaBoost.F<span id="S3.F1.18.9.9" class="ltx_text ltx_font_medium">. <math id="S3.F1.10.1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.F1.10.1.1.m1.1b"><mi id="S3.F1.10.1.1.m1.1.1" xref="S3.F1.10.1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.F1.10.1.1.m1.1c"><ci id="S3.F1.10.1.1.m1.1.1.cmml" xref="S3.F1.10.1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.10.1.1.m1.1d">N</annotation></semantics></math> is the dataset size, <math id="S3.F1.11.2.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.F1.11.2.2.m2.1b"><mi id="S3.F1.11.2.2.m2.1.1" xref="S3.F1.11.2.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.F1.11.2.2.m2.1c"><ci id="S3.F1.11.2.2.m2.1.1.cmml" xref="S3.F1.11.2.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.11.2.2.m2.1d">T</annotation></semantics></math> is the number of training rounds, <math id="S3.F1.12.3.3.m3.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.F1.12.3.3.m3.1b"><mi id="S3.F1.12.3.3.m3.1.1" xref="S3.F1.12.3.3.m3.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.F1.12.3.3.m3.1c"><ci id="S3.F1.12.3.3.m3.1.1.cmml" xref="S3.F1.12.3.3.m3.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.12.3.3.m3.1d">h</annotation></semantics></math> the weak hypothesis, <math id="S3.F1.13.4.4.m4.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.F1.13.4.4.m4.1b"><mi id="S3.F1.13.4.4.m4.1.1" xref="S3.F1.13.4.4.m4.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.F1.13.4.4.m4.1c"><ci id="S3.F1.13.4.4.m4.1.1.cmml" xref="S3.F1.13.4.4.m4.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.13.4.4.m4.1d">\epsilon</annotation></semantics></math> the classification error, <math id="S3.F1.14.5.5.m5.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.F1.14.5.5.m5.1b"><mi id="S3.F1.14.5.5.m5.1.1" xref="S3.F1.14.5.5.m5.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.F1.14.5.5.m5.1c"><ci id="S3.F1.14.5.5.m5.1.1.cmml" xref="S3.F1.14.5.5.m5.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.14.5.5.m5.1d">\alpha</annotation></semantics></math> the AdaBoost coefficient. The subscript <math id="S3.F1.15.6.6.m6.2" class="ltx_Math" alttext="i\in[1,n]" display="inline"><semantics id="S3.F1.15.6.6.m6.2b"><mrow id="S3.F1.15.6.6.m6.2.3" xref="S3.F1.15.6.6.m6.2.3.cmml"><mi id="S3.F1.15.6.6.m6.2.3.2" xref="S3.F1.15.6.6.m6.2.3.2.cmml">i</mi><mo id="S3.F1.15.6.6.m6.2.3.1" xref="S3.F1.15.6.6.m6.2.3.1.cmml">∈</mo><mrow id="S3.F1.15.6.6.m6.2.3.3.2" xref="S3.F1.15.6.6.m6.2.3.3.1.cmml"><mo stretchy="false" id="S3.F1.15.6.6.m6.2.3.3.2.1" xref="S3.F1.15.6.6.m6.2.3.3.1.cmml">[</mo><mn id="S3.F1.15.6.6.m6.1.1" xref="S3.F1.15.6.6.m6.1.1.cmml">1</mn><mo id="S3.F1.15.6.6.m6.2.3.3.2.2" xref="S3.F1.15.6.6.m6.2.3.3.1.cmml">,</mo><mi id="S3.F1.15.6.6.m6.2.2" xref="S3.F1.15.6.6.m6.2.2.cmml">n</mi><mo stretchy="false" id="S3.F1.15.6.6.m6.2.3.3.2.3" xref="S3.F1.15.6.6.m6.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F1.15.6.6.m6.2c"><apply id="S3.F1.15.6.6.m6.2.3.cmml" xref="S3.F1.15.6.6.m6.2.3"><in id="S3.F1.15.6.6.m6.2.3.1.cmml" xref="S3.F1.15.6.6.m6.2.3.1"></in><ci id="S3.F1.15.6.6.m6.2.3.2.cmml" xref="S3.F1.15.6.6.m6.2.3.2">𝑖</ci><interval closure="closed" id="S3.F1.15.6.6.m6.2.3.3.1.cmml" xref="S3.F1.15.6.6.m6.2.3.3.2"><cn type="integer" id="S3.F1.15.6.6.m6.1.1.cmml" xref="S3.F1.15.6.6.m6.1.1">1</cn><ci id="S3.F1.15.6.6.m6.2.2.cmml" xref="S3.F1.15.6.6.m6.2.2">𝑛</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.15.6.6.m6.2d">i\in[1,n]</annotation></semantics></math> indices the collaborators and the superscript <math id="S3.F1.16.7.7.m7.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.F1.16.7.7.m7.1b"><mi id="S3.F1.16.7.7.m7.1.1" xref="S3.F1.16.7.7.m7.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.F1.16.7.7.m7.1c"><ci id="S3.F1.16.7.7.m7.1.1.cmml" xref="S3.F1.16.7.7.m7.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.16.7.7.m7.1d">t</annotation></semantics></math> the training rounds (with <math id="S3.F1.17.8.8.m8.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.F1.17.8.8.m8.1b"><mn id="S3.F1.17.8.8.m8.1.1" xref="S3.F1.17.8.8.m8.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.F1.17.8.8.m8.1c"><cn type="integer" id="S3.F1.17.8.8.m8.1.1.cmml" xref="S3.F1.17.8.8.m8.1.1">0</cn></annotation-xml></semantics></math> standing for an untrained weak hypothesis). <math id="S3.F1.18.9.9.m9.2" class="ltx_Math" alttext="c\in[1,n]" display="inline"><semantics id="S3.F1.18.9.9.m9.2b"><mrow id="S3.F1.18.9.9.m9.2.3" xref="S3.F1.18.9.9.m9.2.3.cmml"><mi id="S3.F1.18.9.9.m9.2.3.2" xref="S3.F1.18.9.9.m9.2.3.2.cmml">c</mi><mo id="S3.F1.18.9.9.m9.2.3.1" xref="S3.F1.18.9.9.m9.2.3.1.cmml">∈</mo><mrow id="S3.F1.18.9.9.m9.2.3.3.2" xref="S3.F1.18.9.9.m9.2.3.3.1.cmml"><mo stretchy="false" id="S3.F1.18.9.9.m9.2.3.3.2.1" xref="S3.F1.18.9.9.m9.2.3.3.1.cmml">[</mo><mn id="S3.F1.18.9.9.m9.1.1" xref="S3.F1.18.9.9.m9.1.1.cmml">1</mn><mo id="S3.F1.18.9.9.m9.2.3.3.2.2" xref="S3.F1.18.9.9.m9.2.3.3.1.cmml">,</mo><mi id="S3.F1.18.9.9.m9.2.2" xref="S3.F1.18.9.9.m9.2.2.cmml">n</mi><mo stretchy="false" id="S3.F1.18.9.9.m9.2.3.3.2.3" xref="S3.F1.18.9.9.m9.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F1.18.9.9.m9.2c"><apply id="S3.F1.18.9.9.m9.2.3.cmml" xref="S3.F1.18.9.9.m9.2.3"><in id="S3.F1.18.9.9.m9.2.3.1.cmml" xref="S3.F1.18.9.9.m9.2.3.1"></in><ci id="S3.F1.18.9.9.m9.2.3.2.cmml" xref="S3.F1.18.9.9.m9.2.3.2">𝑐</ci><interval closure="closed" id="S3.F1.18.9.9.m9.2.3.3.1.cmml" xref="S3.F1.18.9.9.m9.2.3.3.2"><cn type="integer" id="S3.F1.18.9.9.m9.1.1.cmml" xref="S3.F1.18.9.9.m9.1.1">1</cn><ci id="S3.F1.18.9.9.m9.2.2.cmml" xref="S3.F1.18.9.9.m9.2.2">𝑛</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.18.9.9.m9.2d">c\in[1,n]</annotation></semantics></math> is the index of the best weak hypothesis in the hypothesis space. The red dotted line in PreWeak.F indicates the absence of communication.
</span></span></figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">None of the frameworks mentioned in Sec. <a href="#S2" title="2 Related Works ‣ Model-Agnostic Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> supports model-agnostic FL algorithms, i.e., they cannot handle different ML models seamlessly.
The reason is twofold.
On the one hand, modern FL frameworks still try to achieve sufficient technical maturity, rather than adding new functionalities.
On the other hand, model-agnostic federated algorithms are still new and little investigated.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.3" class="ltx_p">Recently, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> proposed three federated versions of AdaBoost: <em id="S3.p2.3.1" class="ltx_emph ltx_font_italic">DistBoost.F</em>, <em id="S3.p2.3.2" class="ltx_emph ltx_font_italic">PreWeak.F</em>, and <em id="S3.p2.3.3" class="ltx_emph ltx_font_italic">AdaBoost.F</em>.
All three algorithms are model-agnostic due to their inherent roots in AdaBoost.
Following the terminology commonly used in ensemble learning literature, we call <em id="S3.p2.3.4" class="ltx_emph ltx_font_italic">weak hypothesis</em> a model learned at each federated round and <em id="S3.p2.3.5" class="ltx_emph ltx_font_italic">strong hypothesis</em> the final global model produced by the algorithms.
The general steps of an AdaBoost-based FL algorithm are the following:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">The aggregator receives the dataset size <math id="S3.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.I1.i1.p1.1.m1.1a"><mi id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><ci id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">N</annotation></semantics></math> from each collaborator and sends them an initial version of the weak hypothesis.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">The aggregator receives the weak hypothesis <math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="h_{i}" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><msub id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.2" xref="S3.I1.i2.p1.1.m1.1.1.2.cmml">h</mi><mi id="S3.I1.i2.p1.1.m1.1.1.3" xref="S3.I1.i2.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><apply id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.1.m1.1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2">ℎ</ci><ci id="S3.I1.i2.p1.1.m1.1.1.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">h_{i}</annotation></semantics></math> from each collaborator and broadcasts the entire hypothesis space to every collaborator.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">The errors <math id="S3.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.I1.i3.p1.1.m1.1a"><mi id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><ci id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">\epsilon</annotation></semantics></math> committed by the global weak hypothesis on the local data are calculated by each client and sent to the aggregator.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.2" class="ltx_p">The aggregator exploits the error information to select the best weak hypothesis <math id="S3.I1.i4.p1.1.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.I1.i4.p1.1.m1.1a"><mi id="S3.I1.i4.p1.1.m1.1.1" xref="S3.I1.i4.p1.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.1.m1.1b"><ci id="S3.I1.i4.p1.1.m1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.1.m1.1c">c</annotation></semantics></math>, adds it to the global strong hypothesis and sends the calculated AdaBoost coefficient <math id="S3.I1.i4.p1.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.I1.i4.p1.2.m2.1a"><mi id="S3.I1.i4.p1.2.m2.1.1" xref="S3.I1.i4.p1.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.2.m2.1b"><ci id="S3.I1.i4.p1.2.m2.1.1.cmml" xref="S3.I1.i4.p1.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.2.m2.1c">\alpha</annotation></semantics></math> to the collaborators.</p>
</div>
</li>
</ol>
<p id="S3.p2.2" class="ltx_p">Note that <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">N</annotation></semantics></math> is needed to adequately weight the errors committed by the global weak hypothesis on the local data, thus allowing to compute <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.p2.2.m2.1a"><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">\alpha</annotation></semantics></math> correctly.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.3" class="ltx_p">Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Model-agnostic Federated Algorithms ‣ Model-Agnostic Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> depicts the protocol specialisations for the three algorithms described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
They are similar once abstracted from their low-level details.
While step 1 is inherently a setup step, steps 2-4 are repeated cyclically by DistBoost.F and AdaBoost.F.
PreWeak.F instead fuses steps 1 and 2 at setup time, receiving from each collaborator <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">T</annotation></semantics></math> instances of already trained weak hypotheses (one for each training round) and broadcasting <math id="S3.p3.2.m2.1" class="ltx_Math" alttext="n\times T" display="inline"><semantics id="S3.p3.2.m2.1a"><mrow id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml"><mi id="S3.p3.2.m2.1.1.2" xref="S3.p3.2.m2.1.1.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.p3.2.m2.1.1.1" xref="S3.p3.2.m2.1.1.1.cmml">×</mo><mi id="S3.p3.2.m2.1.1.3" xref="S3.p3.2.m2.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><apply id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1"><times id="S3.p3.2.m2.1.1.1.cmml" xref="S3.p3.2.m2.1.1.1"></times><ci id="S3.p3.2.m2.1.1.2.cmml" xref="S3.p3.2.m2.1.1.2">𝑛</ci><ci id="S3.p3.2.m2.1.1.3.cmml" xref="S3.p3.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">n\times T</annotation></semantics></math> models to the federation.
Then, each federated round <math id="S3.p3.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.p3.3.m3.1a"><mi id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><ci id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">t</annotation></semantics></math> loops only on steps 3 and 4 due to the different <em id="S3.p3.3.1" class="ltx_emph ltx_font_italic">hypothesis space</em> the algorithms explore.
While DistBoost.F and AdaBoost.F create a weak hypothesis during each federated round, PreWeak.F creates the whole hypothesis space during step 2 and then searches for the best solution in it.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">All three algorithms produce the same strong hypothesis and AdaBoost model,
but they differ in the selection of the best weak hypothesis at each round:</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">DistBoost.F uses a committee of weak hypotheses;</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">PreWeak.F uses the weak hypotheses from a fully trained AdaBoost model;</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p">AdaBoost.F uses the best weak hypothesis trained in the current round.</p>
</div>
</li>
</ul>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">The generic model-agnostic federated protocol is more complex than the standard FL one.
It requires one more communication for each round and the exchange of complex objects across the network (the weak hypotheses), impacting performance.
Note that each arrow going from collaborator <math id="S3.p5.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.p5.1.m1.1a"><mi id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><ci id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">i</annotation></semantics></math> to the aggregator in Fig. <a href="#S3.F1" title="Figure 1 ‣ 3 Model-agnostic Federated Algorithms ‣ Model-Agnostic Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> implies a synchronisation barrier among all the collaborators in the federation.
Increasing the number of global synchronisation points reduces concurrency and increases the sensitivity to stragglers.
It is worth noting that once an FL framework can handle the common protocol structure, implementing any of the three algorithms requires the same effort.
For this study, we implemented AdaBoost.F for two main reasons.
First, its protocol covers the whole set of messages (like DistBoost.F), making it computationally more interesting to analyse than PreWeak.F.
Besides, AdaBoost.F achieves the best learning results out of the three, also when data is heavily non-IID across the collaborators.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>MAFL Architecture</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Redesigning OpenFL comprises two main goals: allowing more flexible protocol management and making the whole infrastructure model agnostic.
During this process, we aimed to make the changes the least invasive and respect the original design principles whenever possible (see Fig.<a href="#S4.F2" title="Figure 2 ‣ 4 MAFL Architecture ‣ Model-Agnostic Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2303.04906/assets/Figures/openfl.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="231" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.3.2" class="ltx_text" style="font-size:90%;">OpenFL architecture from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The proposed extension targets only the inner components (coloured in blue).</span></figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>The Plan Generalization</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The <em id="S4.SS1.p1.1.1" class="ltx_emph ltx_font_italic">Plan</em> guides the software components’ run time.
It is a YAML file containing all the directives handling the FL learning task, such as which software components to use, where to save the produced models, how many rounds to train, which tasks compose a federated round, and so on.
The original OpenFL Plan is rather primitive in its functions.
It is not entirely customisable by the user, and many of its fields are overwritten at run time with the default values.
Due to its unused power, the parsing of the plan file has been extended and empowered, making it capable of handling new types of tasks, along with a higher range of arguments (and also making it evaluate <em id="S4.SS1.p1.1.2" class="ltx_emph ltx_font_italic">every</em> parameter in the file).</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The new model-agnostic workflow can be triggered by specifying the <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_typewriter">nn: False</span> argument under the <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_typewriter">Aggregator</span> and <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_typewriter">Collaborator</span> keywords.
The specific steps of the protocol can then be specified in the <span id="S4.SS1.p2.1.4" class="ltx_text ltx_font_typewriter">tasks</span> section.
In the Intel<sup id="S4.SS1.p2.1.5" class="ltx_sup">®</sup> OpenFL framework, there are only three possible tasks:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_typewriter">aggregated_model_validation</span>: test set validation of aggregated model;</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_typewriter">train</span>: local training of the model;</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_typewriter">locally_tuned_model_validation</span>: test set validation of local model.</p>
</div>
</li>
</ul>
<p id="S4.SS1.p2.2" class="ltx_p">The three tasks are executed cyclically, with the Aggregator broadcasting the aggregated model before the first task and gathering the local models after the training step.
In MAFL, the tasks vocabulary comprises three additional tasks:</p>
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p"><span id="S4.I2.i1.p1.1.1" class="ltx_text ltx_font_typewriter">weak_learners_validate</span>: test set validation of the weak learners;</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p"><span id="S4.I2.i2.p1.1.1" class="ltx_text ltx_font_typewriter">adaboost_update</span>: update of the global parameters of AdaBoost.F on the Collaborators and the ensemble model on the Aggregator;</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p"><span id="S4.I2.i3.p1.1.1" class="ltx_text ltx_font_typewriter">adaboost_validate</span>: local test set validation of the aggregated AdaBoost.F model.</p>
</div>
</li>
</ul>
<p id="S4.SS1.p2.3" class="ltx_p">The <span id="S4.SS1.p2.3.1" class="ltx_text ltx_font_typewriter">weak_learners_validate</span> task is similar to <span id="S4.SS1.p2.3.2" class="ltx_text ltx_font_typewriter">aggregated_model_validation</span>. However, it returns additional information for AdaBoost.F, such as which samples are correctly predicted/mispredicted and the norm of the samples’ weights.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">The extended set of tasks allows users to use new FL algorithms, such as AdaBoost.F.
Additionally, if the <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_typewriter">adaboost_update</span> task is omitted, it is possible to obtain a simple <em id="S4.SS1.p3.1.2" class="ltx_emph ltx_font_italic">Federated Bagging</em> behaviour.
Switching behaviour requires small actions other than changing the Plan; however, both functionalities are documented with tutorials in the code repository.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Expanded Communication Protocol</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">New messages have been implemented into the original <em id="S4.SS2.p1.1.1" class="ltx_emph ltx_font_italic">communication protocol</em>, allowing the exchange of values other than ML models and performance metrics since AdaBoost.F relies on exchanging locally calculated parameters.
Furthermore, Intel<sup id="S4.SS2.p1.1.2" class="ltx_sup">®</sup> OpenFL only implements two synchronisation points in its original workflow: one at the end of the federation round and one when the Collaborator asks the Aggregator for the aggregated model.
These synchronisation points are hard-coded into the software and cannot be generalised for other uses.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">For the AdaBoost.F workflow, a more general synchronisation point is needed: not two consecutive steps can be executed before each Collaborator has concluded the previous one.
Thus a new <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_typewriter">synch</span> message has been added to the <em id="S4.SS2.p2.1.2" class="ltx_emph ltx_font_italic">gRPC</em> protocol.
The working mechanism of this synchronisation point is straightforward: the collaborators ask for a <span id="S4.SS2.p2.1.3" class="ltx_text ltx_font_typewriter">synch</span> at the end of each task, and if not all collaborators have finished the current task, it is put to sleep; otherwise, it is allowed to continue to the next task.
This solution, even if not the most efficient, respects the Intel<sup id="S4.SS2.p2.1.4" class="ltx_sup">®</sup> OpenFL internal synchronisation mechanisms and thus does not require any different structure or new dependency.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Core Classes Extension</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The following core classes of the framework have been modified to allow the standard and model-agnostic workflows to coexist (see Fig. <a href="#S4.F2" title="Figure 2 ‣ 4 MAFL Architecture ‣ Model-Agnostic Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for an overview).</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The <span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_typewriter">Collaborator</span> class can now offer different behaviours according to the ML model used in the computation.
Suppose the Plan specifies that the training will not involve DNNs.
In that case, the Collaborator will actively keep track of the parameters necessary to the AdaBoost.F algorithm, like the mispredicted examples, the weight associated with each data sample, and the weighted error committed by the models.
Additionally, the handling of the internal database used for storage will change behaviour, changing tags and names associated with the entries to make possible finer requests to it.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">The <span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_typewriter">Aggregator</span> can now generate any ML models (instead of only DNNs weights), handle aggregation functions instantiated dynamically from the plan file, and handle the synchronisation needed at the end of each step.
New methods allow the Aggregator to query the internal database more finely, thus allowing it to read and write ML models with the same tags and name as the Collaborator.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_typewriter">TensorDB</span>, the internal class used for storage, has been modified to accommodate the new behaviours described above.
This class implements a simple <em id="S4.SS3.p4.1.2" class="ltx_emph ltx_font_italic">Pandas</em> data frame responsible for all model storage and retrieving done by the Aggregator and Collaborators.
Furthermore, its <span id="S4.SS3.p4.1.3" class="ltx_text ltx_font_typewriter">clean_up</span> method has been revised, making it possible to maintain a fixed amount of data in memory.
This fix has an important effect on the computational performance since the query time to this object is directly proportional to the amount of data it contains.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.1" class="ltx_p">Finally, the more high-level and interactive classes, namely <span id="S4.SS3.p5.1.1" class="ltx_text ltx_font_typewriter">Director</span> and <span id="S4.SS3.p5.1.2" class="ltx_text ltx_font_typewriter">Envoy</span>, and the serialization library have been updated to work correctly with the new underlying code base.
These software components are supposed to be long-lived: they should constantly be running on the server and clients’ hosts.
When a new experiment starts, they will instantiate the necessary <span id="S4.SS3.p5.1.3" class="ltx_text ltx_font_typewriter">Aggregator</span> and <span id="S4.SS3.p5.1.4" class="ltx_text ltx_font_typewriter">Collaborators</span> objects with the parameters for the specified workflow.</p>
</div>
<div id="S4.SS3.p6" class="ltx_para">
<p id="S4.SS3.p6.1" class="ltx_p">This effort results in a model-agnostic FL framework that supports the standard DNNs-based FL workflow and the new AdaBoost.F algorithm.
Using the software in one mode or another does not require any additional programming effort from the user: a few simple configuration instructions are enough.
Additionally, the installation procedure has been updated to incorporate all new module dependencies of the software.
Finally, a complete set of tutorials has been added to the repository: this way, it should be easy for any developer to get started with this experimental software.

</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The complete set of tutorials replicating the experiments from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> are used to assess MAFL’s correctness and efficiency.
We run them on a cloud and HPC infrastructure, both x86-64 based, and Monte Cimone, the first RISC-V based HPC system; however, MAFL runs also on ARM-v8 and Power systems.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Performance Optimizations</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Using weak learners instead of DNNs drastically reduces the computational load. As an example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> reports 18.5 vs 419.3 seconds to train a 10-leaves decision tree or a DNN model, respectively, on the PRAISE training set (with comparable prediction performance). Moreover, AdaBoost.F requires one additional communication phase per round.
This exacerbates the impact of time spent in communication and synchronisation on the overall system performance. To reduce this impact, we propose and evaluate different optimisations to reduce this overhead. Applying all proposed optimisations, we achieve a 5.5x speedup on a representative FL task (see Fig. <a href="#S5.F3" title="Figure 3 ‣ 5.1 Performance Optimizations ‣ 5 Evaluation ‣ Model-Agnostic Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
As a baseline workload, we train a 10-leaves decision tree on the Adult dataset over 100 rounds using 9 nodes (1 aggregator plus 8 collaborators).
We use physical machines to obtain stable and reliable computing times, as execution times on bare-metal nodes are more deterministic than cloud infrastructures.
Each HPC node is equipped with two 18-core Intel<sup id="S5.SS1.p1.1.1" class="ltx_sup">®</sup> Xeon E5-2697 v4 @2.30 GHz and 126 GB of RAM.
A 100Gb/s Intel<sup id="S5.SS1.p1.1.2" class="ltx_sup">®</sup> Omni-Path network interface (in IPoFabric mode) is used as interconnection network.
Reported times are average of five runs <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mo id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><csymbol cd="latexml" id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\pm</annotation></semantics></math> the 95% CI.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.7" class="ltx_p">We start by measuring the execution time given by the baseline: 484.13<math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mo id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><csymbol cd="latexml" id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">\pm</annotation></semantics></math>15.80 seconds.
The first optimisation is to adapt the buffer sizes used by gRPC to accommodate larger models and avoid resizing operations. Increasing the buffer from 2MB to 32MB using decision trees reduced the execution time to 477.0<math id="S5.SS1.p2.2.m2.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS1.p2.2.m2.1a"><mo id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><csymbol cd="latexml" id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">\pm</annotation></semantics></math>17.5 seconds, an improvement of <math id="S5.SS1.p2.3.m3.1" class="ltx_Math" alttext="\sim 1.5\%" display="inline"><semantics id="S5.SS1.p2.3.m3.1a"><mrow id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml"><mi id="S5.SS1.p2.3.m3.1.1.2" xref="S5.SS1.p2.3.m3.1.1.2.cmml"></mi><mo id="S5.SS1.p2.3.m3.1.1.1" xref="S5.SS1.p2.3.m3.1.1.1.cmml">∼</mo><mrow id="S5.SS1.p2.3.m3.1.1.3" xref="S5.SS1.p2.3.m3.1.1.3.cmml"><mn id="S5.SS1.p2.3.m3.1.1.3.2" xref="S5.SS1.p2.3.m3.1.1.3.2.cmml">1.5</mn><mo id="S5.SS1.p2.3.m3.1.1.3.1" xref="S5.SS1.p2.3.m3.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><apply id="S5.SS1.p2.3.m3.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1"><csymbol cd="latexml" id="S5.SS1.p2.3.m3.1.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S5.SS1.p2.3.m3.1.1.2.cmml" xref="S5.SS1.p2.3.m3.1.1.2">absent</csymbol><apply id="S5.SS1.p2.3.m3.1.1.3.cmml" xref="S5.SS1.p2.3.m3.1.1.3"><csymbol cd="latexml" id="S5.SS1.p2.3.m3.1.1.3.1.cmml" xref="S5.SS1.p2.3.m3.1.1.3.1">percent</csymbol><cn type="float" id="S5.SS1.p2.3.m3.1.1.3.2.cmml" xref="S5.SS1.p2.3.m3.1.1.3.2">1.5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">\sim 1.5\%</annotation></semantics></math>. While this seems small, the larger the models, the bigger the impact of this optimisation.
The second optimisation is the choice of the serialisation framework: by using <span id="S5.SS1.p2.7.1" class="ltx_text ltx_font_typewriter">Cloudpickle</span>, we reduce the execution time to 471.4<math id="S5.SS1.p2.4.m4.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS1.p2.4.m4.1a"><mo id="S5.SS1.p2.4.m4.1.1" xref="S5.SS1.p2.4.m4.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.4.m4.1b"><csymbol cd="latexml" id="S5.SS1.p2.4.m4.1.1.cmml" xref="S5.SS1.p2.4.m4.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.4.m4.1c">\pm</annotation></semantics></math>6.1 seconds, an improvement of <math id="S5.SS1.p2.5.m5.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.SS1.p2.5.m5.1a"><mo id="S5.SS1.p2.5.m5.1.1" xref="S5.SS1.p2.5.m5.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.5.m5.1b"><csymbol cd="latexml" id="S5.SS1.p2.5.m5.1.1.cmml" xref="S5.SS1.p2.5.m5.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.5.m5.1c">\sim</annotation></semantics></math>2.6%.
Next, we examine <span id="S5.SS1.p2.7.2" class="ltx_text ltx_font_typewriter">TensorDB</span>, which grows linearly in the number of federated rounds, thus slowing down access time linearly.
We modified the <span id="S5.SS1.p2.7.3" class="ltx_text ltx_font_typewriter">TensorDB</span> to store only the essential information of the last two federation rounds: this results in a stable memory occupation and access time.
With this change, the execution time drops to 414.8<math id="S5.SS1.p2.6.m6.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS1.p2.6.m6.1a"><mo id="S5.SS1.p2.6.m6.1.1" xref="S5.SS1.p2.6.m6.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.6.m6.1b"><csymbol cd="latexml" id="S5.SS1.p2.6.m6.1.1.cmml" xref="S5.SS1.p2.6.m6.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.6.m6.1c">\pm</annotation></semantics></math>0.9 seconds, an improvement of <math id="S5.SS1.p2.7.m7.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.SS1.p2.7.m7.1a"><mo id="S5.SS1.p2.7.m7.1.1" xref="S5.SS1.p2.7.m7.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.7.m7.1b"><csymbol cd="latexml" id="S5.SS1.p2.7.m7.1.1.cmml" xref="S5.SS1.p2.7.m7.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.7.m7.1c">\sim</annotation></semantics></math>14.4% over the baseline.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.3" class="ltx_p">Lastly, two <span id="S5.SS1.p3.3.1" class="ltx_text ltx_font_typewriter">sleep</span> are present in the MAFL code: one for the end-round synchronisation and another for the <span id="S5.SS1.p3.3.2" class="ltx_text ltx_font_typewriter">synch</span> general synchronisation point, fixed respectively at 10 and 1 seconds.
Both have been lowered to 0.01 seconds since we assessed empirically that this is the lowest waiting time still improving the global execution time.
This choice has also been made possible due to the computational infrastructures exploited in this work; it may not be suitable for wide-scale implementations in which servers and clients are geographically distant or compute and energy-constrained.
With this sleep calibration, we obtained a global execution time of 250.8<math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mo id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><csymbol cd="latexml" id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">\pm</annotation></semantics></math>9.6 seconds, a <math id="S5.SS1.p3.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.SS1.p3.2.m2.1a"><mo id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><csymbol cd="latexml" id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">\sim</annotation></semantics></math>48.2% less than the baseline. Overall, with all the optimisations applied together, we can achieve a final mean execution time of 88.6<math id="S5.SS1.p3.3.m3.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS1.p3.3.m3.1a"><mo id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><csymbol cd="latexml" id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">\pm</annotation></semantics></math> seconds, i.e. a 5.46x speedup over the baseline.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2303.04906/assets/Figures/ablation.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="260" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S5.F3.3.2" class="ltx_text" style="font-size:90%;">Ablation study of the proposed software optimisations; the 95% CI has been obtained over five executions.</span></figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Correctness</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We replicate the experiments from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and compare the ML results.
These experiments involve ten different datasets: <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">adult</span>, <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_typewriter">forestcover</span>, <span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_typewriter">kr-vs-kp</span>, <span id="S5.SS2.p1.1.4" class="ltx_text ltx_font_typewriter">splice</span>, <span id="S5.SS2.p1.1.5" class="ltx_text ltx_font_typewriter">vehicle</span>, <span id="S5.SS2.p1.1.6" class="ltx_text ltx_font_typewriter">segmentation</span>, <span id="S5.SS2.p1.1.7" class="ltx_text ltx_font_typewriter">sat</span>, <span id="S5.SS2.p1.1.8" class="ltx_text ltx_font_typewriter">pendigits</span>, <span id="S5.SS2.p1.1.9" class="ltx_text ltx_font_typewriter">vowel</span>, and <span id="S5.SS2.p1.1.10" class="ltx_text ltx_font_typewriter">letter</span>.
These are standard ML datasets targeting classification tasks, both binary (<span id="S5.SS2.p1.1.11" class="ltx_text ltx_font_typewriter">adult</span>, <span id="S5.SS2.p1.1.12" class="ltx_text ltx_font_typewriter">forestcover</span>, <span id="S5.SS2.p1.1.13" class="ltx_text ltx_font_typewriter">kr-vs-kp</span>) and multi-class (all the others), with a varying number of features (from the 14 of <span id="S5.SS2.p1.1.14" class="ltx_text ltx_font_typewriter">adult</span> up to the 61 of <span id="S5.SS2.p1.1.15" class="ltx_text ltx_font_typewriter">splice</span>), and a different number of samples (from the 846 of <span id="S5.SS2.p1.1.16" class="ltx_text ltx_font_typewriter">vehicle</span> up to the 495.141 of <span id="S5.SS2.p1.1.17" class="ltx_text ltx_font_typewriter">forestcover</span>).
Each training set has been split in an IID way across all the Collaborators, while the testing has been done on the entire test set.
A simple Decision Tree from SciKit-Learn with ten leaves is used as a weak learner; instead, the AdaBoost class has been created manually.
We set the number of federated rounds to 300 and use 10 nodes: 1 aggregator plus 9 collaborators. We note that these optimizations can also benefit the original OpenFL.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.4.2.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S5.T1.2.1" class="ltx_text" style="font-size:90%;">Mean F1 scores <math id="S5.T1.2.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.2.1.m1.1b"><mo id="S5.T1.2.1.m1.1.1" xref="S5.T1.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.1.m1.1c"><csymbol cd="latexml" id="S5.T1.2.1.m1.1.1.cmml" xref="S5.T1.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.1.m1.1d">\pm</annotation></semantics></math> standard deviation over 5 runs.</span></figcaption>
<table id="S5.T1.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.5.1.1" class="ltx_tr">
<th id="S5.T1.5.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S5.T1.5.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<td id="S5.T1.5.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.5.1.1.2.1" class="ltx_text ltx_font_bold">Classes</span></td>
<td id="S5.T1.5.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.5.1.1.3.1" class="ltx_text ltx_font_bold">Reference</span></td>
<td id="S5.T1.5.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.5.1.1.4.1" class="ltx_text ltx_font_bold">MAFL</span></td>
</tr>
<tr id="S5.T1.5.2.2" class="ltx_tr">
<th id="S5.T1.5.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Adult</th>
<td id="S5.T1.5.2.2.2" class="ltx_td ltx_align_center ltx_border_t">2</td>
<td id="S5.T1.5.2.2.3" class="ltx_td ltx_align_center ltx_border_t">85.58 ± 0.06</td>
<td id="S5.T1.5.2.2.4" class="ltx_td ltx_align_right ltx_border_t">85.60 ± 0.05</td>
</tr>
<tr id="S5.T1.5.3.3" class="ltx_tr">
<th id="S5.T1.5.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ForestCover</th>
<td id="S5.T1.5.3.3.2" class="ltx_td ltx_align_center">2</td>
<td id="S5.T1.5.3.3.3" class="ltx_td ltx_align_center">83.67 ± 0.21</td>
<td id="S5.T1.5.3.3.4" class="ltx_td ltx_align_right">83.94 ± 0.14</td>
</tr>
<tr id="S5.T1.5.4.4" class="ltx_tr">
<th id="S5.T1.5.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Kr-vs-kp</th>
<td id="S5.T1.5.4.4.2" class="ltx_td ltx_align_center">2</td>
<td id="S5.T1.5.4.4.3" class="ltx_td ltx_align_center">99.38 ± 0.29</td>
<td id="S5.T1.5.4.4.4" class="ltx_td ltx_align_right">99.50 ± 0.21</td>
</tr>
<tr id="S5.T1.5.5.5" class="ltx_tr">
<th id="S5.T1.5.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Splice</th>
<td id="S5.T1.5.5.5.2" class="ltx_td ltx_align_center ltx_border_t">3</td>
<td id="S5.T1.5.5.5.3" class="ltx_td ltx_align_center ltx_border_t">95.61 ± 0.62</td>
<td id="S5.T1.5.5.5.4" class="ltx_td ltx_align_right ltx_border_t">96.97 ± 0.65</td>
</tr>
<tr id="S5.T1.5.6.6" class="ltx_tr">
<th id="S5.T1.5.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vehicle</th>
<td id="S5.T1.5.6.6.2" class="ltx_td ltx_align_center">4</td>
<td id="S5.T1.5.6.6.3" class="ltx_td ltx_align_center">72.94 ± 3.40</td>
<td id="S5.T1.5.6.6.4" class="ltx_td ltx_align_right">80.04 ± 3.30</td>
</tr>
<tr id="S5.T1.5.7.7" class="ltx_tr">
<th id="S5.T1.5.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Segmentation</th>
<td id="S5.T1.5.7.7.2" class="ltx_td ltx_align_center">7</td>
<td id="S5.T1.5.7.7.3" class="ltx_td ltx_align_center">86.07 ± 2.86</td>
<td id="S5.T1.5.7.7.4" class="ltx_td ltx_align_right">85.58 ± 0.06</td>
</tr>
<tr id="S5.T1.5.8.8" class="ltx_tr">
<th id="S5.T1.5.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Sat</th>
<td id="S5.T1.5.8.8.2" class="ltx_td ltx_align_center">8</td>
<td id="S5.T1.5.8.8.3" class="ltx_td ltx_align_center">83.52 ± 0.58</td>
<td id="S5.T1.5.8.8.4" class="ltx_td ltx_align_right">84.89 ± 0.57</td>
</tr>
<tr id="S5.T1.5.9.9" class="ltx_tr">
<th id="S5.T1.5.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Pendigits</th>
<td id="S5.T1.5.9.9.2" class="ltx_td ltx_align_center">10</td>
<td id="S5.T1.5.9.9.3" class="ltx_td ltx_align_center">93.21 ± 0.80</td>
<td id="S5.T1.5.9.9.4" class="ltx_td ltx_align_right">92.06 ± 0.44</td>
</tr>
<tr id="S5.T1.5.10.10" class="ltx_tr">
<th id="S5.T1.5.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vowel</th>
<td id="S5.T1.5.10.10.2" class="ltx_td ltx_align_center">11</td>
<td id="S5.T1.5.10.10.3" class="ltx_td ltx_align_center">79.80 ± 1.47</td>
<td id="S5.T1.5.10.10.4" class="ltx_td ltx_align_right">79.34 ± 3.31</td>
</tr>
<tr id="S5.T1.5.11.11" class="ltx_tr">
<th id="S5.T1.5.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Letter</th>
<td id="S5.T1.5.11.11.2" class="ltx_td ltx_align_center ltx_border_bb">26</td>
<td id="S5.T1.5.11.11.3" class="ltx_td ltx_align_center ltx_border_bb">68.32 ± 1.63</td>
<td id="S5.T1.5.11.11.4" class="ltx_td ltx_align_right ltx_border_bb">71.13 ± 2.02</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Table <a href="#S5.T1" title="Table 1 ‣ 5.2 Correctness ‣ 5 Evaluation ‣ Model-Agnostic Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> reports each dataset’s reference and calculated F1 scores (mean value <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mo id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><csymbol cd="latexml" id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">\pm</annotation></semantics></math> the standard deviation over five runs).
The values reported are fully compatible with the results reported in the original study, thus assessing the correctness of the implementation.
In particular, it can be observed that the standard deviation intervals are particularly high for the <span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_typewriter">vehicle</span>, <span id="S5.SS2.p2.1.2" class="ltx_text ltx_font_typewriter">segmentation</span>, and <span id="S5.SS2.p2.1.3" class="ltx_text ltx_font_typewriter">vowel</span>.
This fact can be due to the small size of the training set of these datasets, respectively 677, 209, and 792 samples, which, when split up across ten Collaborators, results in an even smaller quantity of data per client: this can thus determine the creation of low-performance weak learners.
Furthermore, also <span id="S5.SS2.p2.1.4" class="ltx_text ltx_font_typewriter">letter</span> reported a high standard deviation: this could be due to the difference between the classification capabilities of the employed weak learner (a 10-leaves Decision Tree) compared to the high number of labels present in this dataset (26 classes), thus making it hard to obtain high-performance weak learners.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">The mean F1 score curve for each dataset can be observed in Figure <a href="#S5.F4.sf1" title="In Figure 4 ‣ 5.3 Flexibility ‣ 5 Evaluation ‣ Model-Agnostic Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(a)</span></a>.
As can be seen, after an initial dip in performance, almost each learning curve continues to grow monotonically to higher values.
This fact is expected since the AdaBoost.F is supposed to improve its classification performance with more weak learners.
It has to be observed that, at each federated round, a new weak learner will be added to the aggregated model: the AdaBoost.F grows linearly in size with the number of federated rounds.
This characteristic of the algorithm has many consequences, like the increasingly longer time needed for inference and for moving the aggregated model over the network.
From Figure <a href="#S5.F4.sf1" title="In Figure 4 ‣ 5.3 Flexibility ‣ 5 Evaluation ‣ Model-Agnostic Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(a)</span></a>, we can observe that, in the vast majority of cases, a few tens of federated rounds are more than enough to obtain a decent level of F1 scores; this is interesting since it is possible to obtain a small and efficient AdaBoost.F model in little training effort.
Instead, for the more complex datasets like <span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_typewriter">letter</span> and <span id="S5.SS2.p3.1.2" class="ltx_text ltx_font_typewriter">vowel</span>, we can observe that it is possible to obtain better performance with longer training efforts.
This means that is possible to use AdaBoost.F to produce bigger and heavier models at need, according to the desired performance and inference complexity.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Flexibility</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">To demonstrate the model-agnostic property of MAFL, we choose the <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">vowel</span> dataset and train different ML model types on it.
In particular, one representative ML model has been chosen from each multi-label classifier family available on SciKit-Learn: Extremely Randomized Tree (Trees), Ridge Linear Regression (Linear models), Multi-Layer Perceptron (Neural Networks), K-Nearest Neighbors (Neighbors), Gaussian Naive Bayes (Naive Bayes), and simple 10-leaves Decision Trees as baselines.
Fig. <a href="#S5.F4.sf2" title="In Figure 4 ‣ 5.3 Flexibility ‣ 5 Evaluation ‣ Model-Agnostic Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a> summarises the F1 curves for the different ML models used as weak learners.
Each model has been used out-of-the-box, without hyper-parameter tuning using the default parameters set by SciKit-Learn v1.1.2.
All ML models work straightforwardly in the proposed software without needing to code anything manually: it is sufficient to replace the class name in the experiment file.
This proves the ease with which data scientists can leverage MAFL to experiment with different model types.</p>
</div>
<figure id="S5.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04906/assets/Figures/adaboostf.png" id="S5.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="437" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F4.sf1.3.2" class="ltx_text" style="font-size:90%;">Aggregated AdaBoost.F model F1 score on each test set.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04906/assets/Figures/agnostic.png" id="S5.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="436" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F4.sf2.3.2" class="ltx_text" style="font-size:90%;">Example of the effect of using different ML models as weak learners.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S5.F4.3.2" class="ltx_text" style="font-size:90%;">ML properties of MAFL </span></figcaption>
</figure>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Scalability Analysis</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">We perform this scalability study using the HPC nodes from Sec. <a href="#S5.SS1" title="5.1 Performance Optimizations ‣ 5 Evaluation ‣ Model-Agnostic Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a> and Monte Cimone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, the first available HPC-class cluster based on RISC-V processors.
It comprises eight computing nodes equipped with a U740 SoC from SiFive integrating four U74 RV64GCB cores @ 1.2 GHz, 16GB RAM and a 1 Gb/s interconnection network.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">We select the <span id="S5.SS4.p2.1.1" class="ltx_text ltx_font_typewriter">forestcover</span> dataset for running these experiments, being the largest dataset used in this study, split into a 485K training samples and 10K testing samples.
The weak learner is the same 10-leaves SciKit-Learn Decision Tree from Sec.<a href="#S5.SS2" title="5.2 Correctness ‣ 5 Evaluation ‣ Model-Agnostic Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>.
We lowered the number of federated training rounds to 100 since they are enough to provide an acceptable and stable result (10 on the RISC-V system due to the longer computational times required).
Different federations have been tested, varying numbers of Collaborators from 2 to 64 by powers of 2.
We went no further since OpenFL is designed to suit a cross-silo FL scenario, which means a few tens of clients.
We investigated two different scenarios: <em id="S5.SS4.p2.1.2" class="ltx_emph ltx_font_italic">strong scaling</em>, where we increase the collaborators while keeping the same problem size by spitting the dataset samples in uniform chunks across collaborators; and <em id="S5.SS4.p2.1.3" class="ltx_emph ltx_font_italic">weak scaling</em>, where we scale the problem size with the number collaborators by assigning each collaborator the entire dataset.
In both cases, the baseline reference time is the time taken by a federation comprising the aggregator and a single collaborator.
We report the mean over 5 runs for each experiment.</p>
</div>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04906/assets/Figures/strong_scaling.png" id="S5.F5.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="437" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">Strong scaling</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.04906/assets/Figures/weak_scaling.png" id="S5.F5.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="434" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">Weak scaling</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.3.2" class="ltx_text" style="font-size:90%;">Strong and weak scaling properties of MAFL.</span></figcaption>
</figure>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">Fig. <a href="#S5.F5" title="Figure 5 ‣ 5.4 Scalability Analysis ‣ 5 Evaluation ‣ Model-Agnostic Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the strong and weak scaling properties of MAFL.
The RISC-V plot stops at 7 because we have just 8 nodes in the cluster, and we want to avoid sharing a node between the aggregator and collaborator to maintain the same experiment system setting.
In the strong scaling scenario, the software does not scale efficiently beyond 8 HPC nodes, as the execution becomes increasingly communication-bound.
The same also affects the weak scaling.
Nevertheless, the degradation is sublinear (each point on the x/axis doubles the number of nodes).
This is important because the main benefit in the FL scenario is the additional training data brought in by each contributor node.
The RISC-V cluster exhibits better strong scalability when comparing the two clusters.
This is justified by the slower compute speed of the RISC-V cores leading to higher training times, making the execution more compute-bound, especially for a low number of nodes.
The weak scalability on the RISC-V cluster suffers from the lower network speed.
Since real-world cross-silo federations rarely count more than a dozen participants, it can be assessed that MAFL is suitable for experimenting with such real-world scenarios.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The implementation experience of MAFL and the subsequent experimentation made it evident that current FL frameworks are not designed to be as flexible as the current research environment needs them to be.
The fact that the standard workflow of OpenFL was not customisable in any possible way without modifying the code and that the serialisation structure is DNN-specific led the authors to the idea that a new, workflow-based FL framework is needed.
Such a framework should not implement a fixed workflow but allow the user to express any number of workflow steps, entities, the relations between them, and the objects that must be exchanged.
This property implies the generalisation of the serialisation infrastructure, which cannot be limited to tensors only.
Such an approach would lead to a much more straightforward implementation of newer and experimental approaches to FL, both from the architectural and ML perspective.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Furthermore, the use of asynchronous communication can help better manage the concurrent architecture of the federation.
These systems are usually slowed down by stragglers that, since the whole system is supposed to wait for them, will slow down the entire computation.
In our experience implementing MAFL, a significant part of the scalability issues is determined by the waiting time between the different collaborators taking part in the training.
While such an approach would improve the scalability performance of any FL framework, it also underlies the investigation of how to simultaneously handle newer and older updates.
This capability would improve the computational performance of gradient and non-gradient-based systems: the relative aggregation algorithms must be revised to accommodate this new logic.
This matter is not trivial and deserves research interest.
Lastly, due to the possibility of exploiting less computationally requiring models, MAFL can easily be used to implement FL on low-power devices, such as systems based on the new RISC-V.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusions</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">A model-agnostic modified version of Intel<sup id="S7.p1.1.1" class="ltx_sup">®</sup> OpenFL implementing the AdaBoost.F federated boosting algorithm, named MAFL, has been proposed.
The proposed software has been proven to implement the AdaBoost.F algorithm correctly and can scale sufficiently to experiment efficiently with small cross-silo federations.
MAFL is open-source, freely available online, easily installable, and has a complete set of already implemented examples.
To our knowledge, MAFL is the first FL framework to implement a model-agnostic, non-gradient-based algorithm.
This effort will allow researchers to experiment with this new conception of FL more freely, pushing the concept of model-agnostic FL even further.
Furthermore, this work aims to contribute directly to the RISC-V community, enabling FL research on this innovative platform.</p>
</div>
<section id="S7.SS0.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.0.1 </span>Acknowledgments</h4>

<div id="S7.SS0.SSS1.p1" class="ltx_para">
<p id="S7.SS0.SSS1.p1.1" class="ltx_p">This work has been supported by the Spoke “FutureHPC &amp; BigData” of the ICSC – Centro Nazionale di Ricerca in “High Performance Computing, Big Data and Quantum Computing”, funded by European Union – NextGenerationEU and the EuPilot project funded by EuroHPC JU under G.A. n. 101034126.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Arfat, Y., Mittone, G., Colonnelli, I., D’Ascenzo, F., Esposito, R., Aldinucci,
M.: Pooling critical datasets with federated learning. In: IEEE PDP (2023)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Bartolini, A., Ficarelli, F., Parisi, E., Beneventi, F., Barchi, F., Gregori,
D., et al.: Monte cimone: Paving the road for the first generation of risc-v
high-performance computers. In: IEEE SOCC. pp. 1–6 (2022)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Beltrán, E.T.M., Pérez, M.Q., Sánchez, P.M.S., Bernal, S.L., Bovet,
G., Pérez, M.G., et al.: Decentralized federated learning: Fundamentals,
state-of-the-art, frameworks, trends, and challenges. arXiv preprint
arXiv:2211.08413 (2022)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Beutel, D.J., Topal, T., Mathur, A., Qiu, X., Parcollet, T., de Gusmão,
P.P., et al.: Flower: A friendly federated learning research framework. arXiv
preprint arXiv:2007.14390 (2020)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Foley, P., Sheller, M.J., Edwards, B., Pati, S., Riviera, W., Sharma, M.,
et al.: Openfl: the open federated learning library. Phys. Med. Biol.
<span id="bib.bib5.1.1" class="ltx_text ltx_font_bold">67</span>(21), 214001 (2022)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Freund, Y., Schapire, R.E.: A decision-theoretic generalization of on-line
learning and an application to boosting. J. Comput. Syst. Sci.
<span id="bib.bib6.1.1" class="ltx_text ltx_font_bold">55</span>(1), 119–139 (1997)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
He, C., Li, S., So, J., Zhang, M., Wang, H., Wang, X., et al.: Fedml: A
research library and benchmark for federated machine learning. arXiv preprint
arXiv:2007.13518 (July 2020)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Holzinger, A., Langs, G., Denk, H., Zatloukal, K., Müller, H.: Causability
and explainability of artificial intelligence in medicine. Wiley
Interdisciplinary Reviews: Data Mining and Knowledge Discovery
<span id="bib.bib8.1.1" class="ltx_text ltx_font_bold">9</span>(4),  1312 (2019)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Kairouz, P., McMahan, H.B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.N.,
Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al.: Advances and
open problems in federated learning. Found. Trends Mach. Learn.
<span id="bib.bib9.1.1" class="ltx_text ltx_font_bold">14</span>(1-2), 1–210 (2021)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Kleanthous, C., Chatzis, S.: Gated mixture variational autoencoders for value
added tax audit case selection. Knowl. Based Syst. <span id="bib.bib10.1.1" class="ltx_text ltx_font_bold">188</span>, 105048
(2020)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep
convolutional neural networks. Commun. ACM <span id="bib.bib11.1.1" class="ltx_text ltx_font_bold">60</span>(6), 84–90 (2017)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Liu, Y., Fan, T., Chen, T., Xu, Q., Yang, Q.: Fate: An industrial grade
platform for collaborative learning with data protection. J. Mach. Learn.
Res. <span id="bib.bib12.1.1" class="ltx_text ltx_font_bold">22</span>(1), 10320–10325 (2021)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Ludwig, H., Baracaldo, N., Thomas, G., Zhou, Y., Anwar, A., Rajamoni, S.,
et al.: IBM federated learning: an enterprise framework white paper v0. 1.
arXiv preprint arXiv:2007.10987 (2020)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Lyu, L., Yu, H., Ma, X., Chen, C., Sun, L., Zhao, J., et al.: Privacy and
robustness in federated learning: Attacks and defenses. IEEE Trans. Neural.
Netw. Learn. Syst. pp. 1–21 (2022)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
McMahan, B., Moore, E., Ramage, D., Hampson, S., Agüera y Arcas, B.:
Communication-efficient learning of deep networks from decentralized data.
In: Proceedings of the 20th International Conference on Artificial
Intelligence and Statistics AISTATS. vol. 54, pp. 1273–1282. PMLR, Fort
Lauderdale, FL, USA (2017)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Meese, C., Chen, H., Asif, S.A., Li, W., Shen, C.C., Nejad, M.: BFRT:
Blockchained federated learning for real-time traffic flow prediction. In:
IEEE CCGrid. pp. 317–326 (2022)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
O’Mahony, N., Campbell, S., Carvalho, A., Harapanahalli, S., Hernandez, G.V.,
Krpalkova, L., et al.: Deep learning vs. traditional computer vision. In:
SAI. vol. 943, pp. 128–144. Springer, Cham (2019)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Polato, M., Esposito, R., Aldinucci, M.: Boosting the federation: Cross-silo
federated learning without gradient descent. In: IEEE IJCNN). pp. 1–10
(2022)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Riviera, W., Menegaz, G., Boscolo Galazzo, I.: FeLebrities: a user-centric
assessment of federated learning frameworks. TechRxiv (2022)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Roth, H.R., Cheng, Y., Wen, Y., Yang, I., Xu, Z., Hsieh, Y.T., et al.: Nvidia
flare: Federated learning from simulation to real-world. arXiv preprint
arXiv:2210.13291 (2022)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Sotthiwat, E., Zhen, L., Li, Z., Zhang, C.: Partially encrypted multi-party
computation for federated learning. In: IEEE CCGrid. pp. 828–835 (2021)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural
networks. In: NeurIPS. pp. 3104–3112 (2014)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Warnat-Herresthal, S., Schultze, H., Shastry, K.L., Manamohan, S., Mukherjee,
S., Garg, V., et al.: Swarm learning for decentralized and confidential
clinical machine learning. Nature <span id="bib.bib23.1.1" class="ltx_text ltx_font_bold">594</span>(7862), 265–270 (2021)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Zhavoronkov, A., Ivanenkov, Y.A., Aliper, A., Veselov, M.S., Aladinskiy, V.A.,
Aladinskaya, A.V., et al.: Deep learning enables rapid identification of
potent ddr1 kinase inhibitors. Nature biotechnology <span id="bib.bib24.1.1" class="ltx_text ltx_font_bold">37</span>(9),
1038–1040 (2019)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.04905" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.04906" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2303.04906">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.04906" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.04907" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 21:04:51 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
