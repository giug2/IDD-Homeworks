<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2308.11891] Bridging the Gap: Deciphering Tabular Data Using Large Language Model</title><meta property="og:description" content="In the realm of natural language processing, the understanding of tabular data has perpetually stood as a focal point of scholarly inquiry. The emergence of expansive language models, exemplified by the likes of ChatGP…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bridging the Gap: Deciphering Tabular Data Using Large Language Model">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Bridging the Gap: Deciphering Tabular Data Using Large Language Model">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2308.11891">

<!--Generated on Wed Feb 28 11:20:11 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Bridging the Gap: Deciphering Tabular Data Using Large Language Model</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Hengyuan Zhang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Peng Chang
</span><span class="ltx_author_notes">Corresponding author. PAII Inc., Palo Alto, CA 94306, USA. Email: pengchang@gmail.com</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zongcheng Ji
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text">In the realm of natural language processing, the understanding of tabular data has perpetually stood as a focal point of scholarly inquiry. The emergence of expansive language models, exemplified by the likes of ChatGPT, has ushered in a wave of endeavors wherein researchers aim to harness these models for tasks related to table-based question answering. Central to our investigative pursuits is the elucidation of methodologies that amplify the aptitude of such large language models in discerning both the structural intricacies and inherent content of tables, ultimately facilitating their capacity to provide informed responses to pertinent queries. To this end, we have architected a distinctive module dedicated to the serialization of tables for seamless integration with expansive language models. Additionally, we’ve instituted a corrective mechanism within the model to rectify potential inaccuracies. Experimental results indicate that, although our proposed method trails the SOTA by approximately 11.7</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Tabular data forms the cornerstone of numerous sectors, ranging from healthcare and finance to marketing and machine learning. Its pervasiveness underscores the importance of efficiently querying and interpreting this form of structured information. However, the intricacies of multi-dimensional queries and the vastness of data they cover often call for considerable human intervention, primarily in the construction and refinement of SQL statements <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">RBE<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>20</a>]</cite>. Consequently, the data management landscape is fraught with challenges that render the interpretation and utilization of such data a daunting task<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">NKP<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>18</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The rapidly advancing field of artificial intelligence offers potential solutions to these challenges. More specifically, recent advances in large language models present an intriguing opportunity to apply their understanding and generation capabilities to this field of research. These models, trained on diverse and massive text corpora, have showcased the capability to generate human-like text and understand the context and intricacies of the information contained within the text<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">RSR<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>20</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this context, the current research provides an innovative take on the application of LLMs for decoding and querying tabular data. Our work aims to mitigate the need for human intervention by employing LLMs to decipher table structures, comprehend the complexities of the presented problems, and subsequently, formulate SQL queries. In contrast to traditional methodologies, this approach provides a scalable, efficient, and robust solution for handling large-scale, multi-dimensional data.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our strategy expands on the iterative nature of LLMs, deploying them not just to generate SQL queries but also to learn from their errors and improve in an evolutionary fashion. This adaptation facilitates the handling of queries of varying complexity levels, effectively expanding the range of manageable data queries. Consequently, this approach greatly enhances the efficiency and accessibility of data management systems, providing a path to reduce the time-consuming and error-prone process of manual SQL statement construction.Moreover, we propose an innovative deployment of LLMs, effectively redefining their role in the field of data analysis and management. The potential of this pioneering methodology is confirmed through its promising performance across various datasets, exemplified by its outstanding performance on the benchmark Spider dataset.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This paper is divided into several sections detailing our research. We begin by reviewing the related work in the field, underscoring the gap that our research seeks to fill. This is followed by a detailed explanation of our methodology and how LLMs can be employed to handle tabular data. Subsequent sections detail our experimental setup, followed by an in-depth analysis of the results. Finally, we conclude with a discussion of the implications of our research, its limitations, and potential avenues for future work.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The scope of the current research spans and contributes to several seminal works and key areas in the field, namely Large Language Models and SQL Query Generation from Natural Language, which not only serve as the foundational bedrock of our study but also constitute the backdrop against which our innovations are delineated.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Large Language Models</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Our research is significantly founded on advancements in large language models. Specifically, the groundbreaking research on GPT-4 by OpenAI et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">Ope23</a>]</cite> , a language model with trillions of autoregressive parameters, has set a precedent for the capabilities of such models. They demonstrated that scaling up language models significantly improves performance on a variety of tasks, including translation, question-answering, and cloze tasks, among others. Our work extends the capabilities of these large language models, specifically LLaMA-2<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">TMS<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>23</a>]</cite>, towards understanding and generating SQL queries from table structures and problem statements, pushing the boundary of what these models can achieve.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>SQL Query Generation from Natural Language</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">SQL query generation has been a focal point for many researchers<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">RSR<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>22</a>]</cite>. A notable contribution was made by Yu et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">YZY<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>18</a>]</cite>.with the Spider dataset and the Text-to-SQL model, significantly advancing handling of complex, cross-domain SQL queries. Yet, handling the most complex queries remained a challenge<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">KSHL20</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">To address this, Pourreza and Rafiei<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">PR23a</a>]</cite> introduced Din-SQL, applying in-context learning of Text-to-SQL through a decomposed approach with self-correction. Concurrently, Li et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">PR23b</a>]</cite> proposed GraphiX-T5, combining pre-trained Transformers and graph-aware layers for Text-to-SQL parsing, capitalizing on the strengths of both methodologies.Additionally, at the AAAI Conference on Artificial Intelligence in 2023, Li, Zhang, Li, and others<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">LZLC23</a>]</cite> unveiled ResdSQL, which decoupled schema linking and skeleton parsing for Text-to-SQL, demonstrating a new approach to the task.While these works have made substantial progress, complex query handling remains challenging. Our work uses a large language model with vast pretraining to handle intricate queries, promoting generalization across different domains.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our approach utilizes LLaMA-2<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">TMS<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>23</a>]</cite> as the LLM, serializing table structures and questions as inputs to the model, which then produces SQL statements for querying on the tables. Throughout this process, we employ an iterative optimization procedure and fine-tuning techniques to ensure the model’s accuracy and efficiency.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Input Construction</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Firstly, we devise a mechanism for input construction that comprises both the problem statement and the schema of the table. By concatenating these elements, we generate a robust instructional input for the model. This concatenation is not simply an arbitrary combination, but rather a well-curated blending of both the data (table schema) and the desired output (problem statement). The reasoning behind this is twofold: the schema offers the model the requisite understanding of the data structure, while the problem statement provides the goal that the query should aim to achieve. The construction process of the entire input is shown in <a href="#S3.F1" title="Figure 1 ‣ 3.1 Input Construction ‣ 3 Methodology ‣ Bridging the Gap: Deciphering Tabular Data Using Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2308.11891/assets/1.jpg" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="509" height="239" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Input Construction</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>SQL Generation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">After the input is constructed, it is used to prompt the LLaMA-2 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">TMS<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>23</a>]</cite>. Leveraging its massive 70-billion parameter structure, the model is then tasked with generating a SQL statement as its output. This is not just a simple recitation of the input but an active interpretation of the input’s complexity and its conversion into an appropriate SQL query. The aim is for the model to take in a problem and schema as input and generate a SQL statement that accurately and efficiently answers the problem, all the while adhering to the structure of the data as represented by the schema. The generation process of the entire input is shown in <a href="#S3.F2" title="Figure 2 ‣ 3.2 SQL Generation ‣ 3 Methodology ‣ Bridging the Gap: Deciphering Tabular Data Using Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2308.11891/assets/2.jpg" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="509" height="148" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>SQL Generation</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Iterative Refinement</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">This input-output relationship forms the core of our methodology. However, to bolster the accuracy and efficiency of the SQL statements, we further incorporate an iterative refinement process. This involves the generated SQL statement being evaluated for its accuracy, with the model using this evaluation to improve its future outputs. This iterative process is not simply a loop, but rather a learning cycle where the model uses past experiences to improve future performances, effectively embodying an artificial form of evolution. The iterative refinement process of the entire input is shown in <a href="#S3.F3" title="Figure 3 ‣ 3.3 Iterative Refinement ‣ 3 Methodology ‣ Bridging the Gap: Deciphering Tabular Data Using Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2308.11891/assets/3.jpg" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="509" height="189" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>SQL Generation</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In order to evaluate our methodology, we conducted an empirical analysis using the Spider benchmark dataset, which provides a rich and diverse ground for testing and comparison. The experiment was run on a robust hardware configuration featuring four A100 80GB GPUs, to effectively utilize the computational power offered by the LLaMA-2 70b version.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Performance Metrics</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We compare our work with BRIDGE<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">LZLC23</a>]</cite> and RESDSQL<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">LSX20</a>]</cite>, with the results shown in <a href="#S4.T2" title="Table 2 ‣ 4.3 Comparative Analysis ‣ 4 Experiments ‣ Bridging the Gap: Deciphering Tabular Data Using Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Our model’s performance was evaluated on two primary metrics used in the Spider dataset: Execution Accuracy and Exact Set Match<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">ZYK20</a>]</cite>. The results demonstrate that our methodology exhibits robust performance with an Execution Accuracy of 70.5, while achieving an Exact Set Match score of 59.3. Our method significantly outperforms the method proposed by BRIDGE, but lags slightly behind RESDSQL.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Methods</th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Executation Accuracy</th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Exact Match Score</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<td id="S4.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">BRIDGE</td>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">59.9</td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">65.2</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<td id="S4.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r">RESDQL</td>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.3.2.2.1" class="ltx_text ltx_font_bold">79.9</span></td>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.3.2.3.1" class="ltx_text ltx_font_bold">72.0</span></td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<td id="S4.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.4.3.1.1" class="ltx_text ltx_font_bold">Ours</span></td>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">70.5</td>
<td id="S4.T1.1.4.3.3" class="ltx_td ltx_align_center">59.3</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance Metrics</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparative Analysis</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We divided the Spider dataset into three categories based on difficulty level: easy, medium, and hard, with data ratios of 3:2:1. As shown in <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:2</span>, we evaluated the EX and EM scores of RESDSQL, BRIDGE, and our proposed method on these three types of problems, where the data format is ’EX/EM’. By analyzing the results from <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:2</span>, we find that our method performs very well on easy problems, but does not surpass the current best solution when dealing with harder problems.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r">Methods</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r">Easy</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Medium</th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Hard</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">BRIDGE</th>
<th id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">72.4/75.7</th>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61.3/64.1</td>
<td id="S4.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">42.9/43.5</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<th id="S4.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">RESDQL</th>
<th id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">84.8/<span id="S4.T2.1.3.2.2.1" class="ltx_text ltx_font_bold">87.0</span>
</th>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.3.2.3.1" class="ltx_text ltx_font_bold">81.2/83.5</span></td>
<td id="S4.T2.1.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.3.2.4.1" class="ltx_text ltx_font_bold">68.1/63.6</span></td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<th id="S4.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.1.4.3.1.1" class="ltx_text ltx_font_bold">Ours</span></th>
<th id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.4.3.2.1" class="ltx_text ltx_font_bold">85.2</span>/76.9</th>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">74.0/48.9</td>
<td id="S4.T2.1.4.3.4" class="ltx_td ltx_align_center">74.0/48.9</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance Metrics</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In summary, our study highlights the flexibility of large language models, emphasizing their ability to explore various solution paths, leveraging their extensive parameter space. Despite sometimes leading to lower Exact Set Match scores, these models maintain high Execution Accuracy, making them particularly valuable in real-world scenarios where deriving the correct answer is more critical than following a predefined path<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">DZG<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>23</a>]</cite>. Our research underscores the significant capacity of large models for executing SQL queries accurately and their impressive generalizability across datasets. This underlines their potential for transformative roles in data management tasks. Looking forward, the future research direction is to enhance the ability of Large Language Models<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">ZZL<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">+</span></sup>23</a>]</cite> to understand tabular data and increase the accuracy of generating SQL queries. Our findings mark a significant stride toward leveraging the full potential of large language models.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[DZG<sup id="bib.bibx1.4.4.1" class="ltx_sup"><span id="bib.bibx1.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>23]</span>
<span class="ltx_bibblock">
Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, Jinshu Lin, Dongfang
Lou, et al.

</span>
<span class="ltx_bibblock">C3: Zero-shot text-to-sql with chatgpt.

</span>
<span class="ltx_bibblock"><span id="bib.bibx1.7.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.07306</span>, 2023.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[KSHL20]</span>
<span class="ltx_bibblock">
Hyeonji Kim, Byeong-Hoon So, Wook-Shin Han, and Hongrae Lee.

</span>
<span class="ltx_bibblock">Natural language to sql: Where are we today?

</span>
<span class="ltx_bibblock"><span id="bib.bibx2.1.1" class="ltx_text ltx_font_italic">Proceedings of the VLDB Endowment</span>, 13(10):1737–1750, 2020.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[LSX20]</span>
<span class="ltx_bibblock">
Xi Victoria Lin, Richard Socher, and Caiming Xiong.

</span>
<span class="ltx_bibblock">Bridging textual and tabular data for cross-domain text-to-sql
semantic parsing.

</span>
<span class="ltx_bibblock"><span id="bib.bibx3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2012.12627</span>, 2020.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[LZLC23]</span>
<span class="ltx_bibblock">
Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen.

</span>
<span class="ltx_bibblock">Resdsql: Decoupling schema linking and skeleton parsing for
text-to-sql.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx4.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, volume 37, pages 13067–13075, 2023.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[NKP<sup id="bib.bibx5.4.4.1" class="ltx_sup"><span id="bib.bibx5.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>18]</span>
<span class="ltx_bibblock">
Fatemeh Nargesian, Udayan Khurana, Tejaswini Pedapati, Horst Samulowitz, and
Deepak Turaga.

</span>
<span class="ltx_bibblock">Dataset evolver: An interactive feature engineering notebook.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx5.7.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, volume 32, 2018.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ope23]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2023.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[PR23a]</span>
<span class="ltx_bibblock">
Mohammadreza Pourreza and Davood Rafiei.

</span>
<span class="ltx_bibblock">Din-sql: Decomposed in-context learning of text-to-sql with
self-correction.

</span>
<span class="ltx_bibblock"><span id="bib.bibx7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.11015</span>, 2023.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[PR23b]</span>
<span class="ltx_bibblock">
Mohammadreza Pourreza and Davood Rafiei.

</span>
<span class="ltx_bibblock">Din-sql: Decomposed in-context learning of text-to-sql with
self-correction.

</span>
<span class="ltx_bibblock"><span id="bib.bibx8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.11015</span>, 2023.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[RBE<sup id="bib.bibx9.4.4.1" class="ltx_sup"><span id="bib.bibx9.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>20]</span>
<span class="ltx_bibblock">
Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and
Christopher Ré.

</span>
<span class="ltx_bibblock">Snorkel: Rapid training data creation with weak supervision.

</span>
<span class="ltx_bibblock"><span id="bib.bibx9.7.1" class="ltx_text ltx_font_italic">The VLDB Journal</span>, 29(2-3):709–730, 2020.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[RSR<sup id="bib.bibx10.4.4.1" class="ltx_sup"><span id="bib.bibx10.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>20]</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><span id="bib.bibx10.7.1" class="ltx_text ltx_font_italic">The Journal of Machine Learning Research</span>, 21(1):5485–5551,
2020.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[RSR<sup id="bib.bibx11.4.4.1" class="ltx_sup"><span id="bib.bibx11.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>22]</span>
<span class="ltx_bibblock">
TJ Revanth, K Venkat Sai, R Ramya, Renusree Chava, V Sushma, and BS Ramya.

</span>
<span class="ltx_bibblock">Nl2sql: Natural language to sql query translator.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx11.7.1" class="ltx_text ltx_font_italic">Emerging Research in Computing, Information, Communication
and Applications: ERCICA 2020, Volume 2</span>, pages 267–278. Springer, 2022.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[TMS<sup id="bib.bibx12.4.4.1" class="ltx_sup"><span id="bib.bibx12.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>23]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
et al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><span id="bib.bibx12.7.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.09288</span>, 2023.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[YZY<sup id="bib.bibx13.4.4.1" class="ltx_sup"><span id="bib.bibx13.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>18]</span>
<span class="ltx_bibblock">
Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James
Ma, Irene Li, Qingning Yao, Shanelle Roman, et al.

</span>
<span class="ltx_bibblock">Spider: A large-scale human-labeled dataset for complex and
cross-domain semantic parsing and text-to-sql task.

</span>
<span class="ltx_bibblock"><span id="bib.bibx13.7.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1809.08887</span>, 2018.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[ZYK20]</span>
<span class="ltx_bibblock">
Ruiqi Zhong, Tao Yu, and Dan Klein.

</span>
<span class="ltx_bibblock">Semantic evaluation for text-to-sql with distilled test suites.

</span>
<span class="ltx_bibblock"><span id="bib.bibx14.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.02840</span>, 2020.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[ZZL<sup id="bib.bibx15.4.4.1" class="ltx_sup"><span id="bib.bibx15.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup>23]</span>
<span class="ltx_bibblock">
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.

</span>
<span class="ltx_bibblock">A survey of large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bibx15.7.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.18223</span>, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2308.11890" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2308.11891" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2308.11891">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2308.11891" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2308.11892" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 11:20:11 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
