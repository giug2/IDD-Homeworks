<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1511.07394] Where To Look: Focus Regions for Visual Question Answering</title><meta property="og:description" content="We present a method that learns to answer visual questions by
selecting image regions relevant to the text-based query. Our method
maps textual queries and visual features from various regions into a shared space where…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Where To Look: Focus Regions for Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Where To Look: Focus Regions for Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1511.07394">

<!--Generated on Fri Mar  1 17:46:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Where To Look: Focus Regions for Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kevin J. Shih
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of Illinois at Urbana-Champaign
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Saurabh Singh
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of Illinois at Urbana-Champaign
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Derek Hoiem
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of Illinois at Urbana-Champaign
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">We present a method that learns to answer visual questions by
selecting image regions relevant to the text-based query. Our method
maps textual queries and visual features from various regions into a shared space where they are
compared for relevance with an inner product. Our method
exhibits significant improvements in answering questions such as ‘‘what
color,’’ where it is necessary to evaluate a specific location,
and ‘‘what room,’’ where it selectively identifies informative image
regions. Our model is tested on the recently released VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
dataset, which features free-form human-annotated questions and
answers.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_font_typewriter ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section"><span id="S1.1.1.1" class="ltx_text ltx_font_serif">1</span> </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Visual question answering (VQA) is the task of answering a natural language
question about an image. VQA includes many challenges
in language representation and grounding, recognition, common sense
reasoning, and specialized tasks like counting and reading. In this
paper, we focus on a key problem for VQA and other visual reasoning
tasks: knowing where to look. Consider Figure </span><a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Where To Look: Focus Regions for Visual Question Answering" class="ltx_ref ltx_font_typewriter" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S1.p1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">. It’s
easy to answer ‘‘What color is the walk light?’’ if the light bulb is localized,
while answering whether it’s raining may be dealt with by identifying
umbrellas, puddles, or cloudy skies. We want to learn where to look to
answer questions supervised by only images and question/answer pairs.
For example, if we have several training examples for ‘‘What time of
day is it?’’ or similar questions, the system should learn what kind
of answer is expected and where in the image it should base its
response.</span></p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/1511.07394/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="230" height="198" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text ltx_font_serif">Figure 1</span>: </span>Our goal is to identify the correct answer for a natural
language question, such as ‘‘What color is the walk light?’’ or
‘‘Is it raining?’’ We particularly focus on the problem of
learning where to look. This is a challenging problem as it
requires grounding language with vision and learning to recognize
objects, use relations, and determine relevance. For example,
whether it is raining may be determined by detecting the presence of puddles
gray skies, or umbrellas in the scene, whereas the color of the walk light
requires focused attention on the light alone. The above figure
shows example attention regions produced by our proposed model.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p"><span id="S1.p2.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Learning where to look from question-image pairs has many
challenges. Questions such as ‘‘What sport is this?’’ might be
best answered using the full image. Other questions such as ‘‘What
is on the sofa?’’ or ‘‘What color is the woman’s shirt?’’ require
focusing on particular regions. Still others such as ‘‘What does the
sign say?’’ or ‘‘Are the man and woman dating?’’ require specialized
knowledge or reasoning that we do not expect to achieve. The system
needs to learn to recognize objects, infer spatial
relations, determine relevance, and find correspondence between
natural language and visual features. Our key idea is to learn a
non-linear mapping of language and visual region features into a
common latent space to determine relevance. The relevant regions are
then used to score a specific question-answer pairing. The latent embedding and
the scoring function are learned jointly using a margin-based loss
supervised solely by question-answer pairings. We perform
experiments on the VQA dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S1.p2.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S1.p2.1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> because it features
open-ended language, with a wide variety of questions (see
Fig. </span><a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Where To Look: Focus Regions for Visual Question Answering" class="ltx_ref ltx_font_typewriter" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S1.p2.1.5" class="ltx_text ltx_font_typewriter" style="font-size:90%;">). Specifically, we focus on its
multiple-choice format because its evaluation is much less ambiguous
than open-ended answer verification.</span></p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/1511.07394/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="140" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.6.1.1" class="ltx_text ltx_font_serif">Figure 2</span>: </span>Examples from VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. From left to right, the above examples require focused region information to pinpoint the dots, whole image information to determine the weather, and abstract knowledge regarding relationships between children and stuffed animals.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p"><span id="S1.p3.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">We focus on learning where to look but also provide useful baselines and analysis for the task as a whole. Our contributions are as follows:</span></p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">We present an image-region selection mechanism that learns
to identify image regions relevant to questions.</span></p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">We present a learning framework for solving multiple-choice
visual QA with a margin-based loss that significantly outperforms
provided baselines from </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.I1.i2.p1.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S1.I1.i2.p1.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S1.I1.i2.p1.1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">.</span></p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">We compare with baselines that answer questions without the
image, use the whole image, and use all image regions with uniform
weighting, providing a detailed analysis for when selective regions improve VQA performance.</span></p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_font_typewriter ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section"><span id="S2.1.1.1" class="ltx_text ltx_font_serif">2</span> </span>Related Works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Many recent works in tying text to images have
explored the task of automated image captioning </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S2.p1.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S2.p1.1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">. While VQA can be considered as a type of directed
captioning task, our work relates to some </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S2.p1.1.6.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S2.p1.1.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> in that we learn to employ an attention mechanism
for region focus, though our formulation makes determining region
relevance a more explicit part of the learning process. In Fang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.8.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S2.p1.1.9.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S2.p1.1.10" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, words
are detected in various portions of the image and combined together
with a language model to generate captions. Similarly, Xu et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.11.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S2.p1.1.12.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S2.p1.1.13" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
uses a recurrent network model to detect salient objects and generate
caption words one by one. Our model works in
the opposite direction of these caption models at test time by determining the relevant image
region given a textual query as input. This allows our model to
determine whether a question-answer pair is a good match given evidence from
the image.</span></p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Partly due to the difficulty of evaluating image captioning, several
visual question answering datasets have been proposed along with
applied approaches. We choose to experiment on VQA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S2.p2.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S2.p2.1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> due to
the open ended nature of its question and answer
annotations. Questions are collected by asking annotators to pose a difficult question for a smart robot, and multiple
answers are collected for each question. We experiment on the
multiple-choice setting as its evaluation is less ambiguous than that
of open-ended response evaluation. Most other visual question answering
datasets </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S2.p2.1.6.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S2.p2.1.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> are based on reformulating
existing object annotations into questions, which provides an
interesting visual task but limits the scope of visual and abstract
knowledge required.</span></p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Our model is inspired by End-to-End Memory Networks </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S2.p3.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S2.p3.1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> proposed
for answering questions based on a series of sentences. The regions
in our model are analogous to the sentences in theirs, and, similarly
to them, we learn an embedding to project question and potential
features into a shared subspace to determine relevance with an inner product. Our method
differs in many details such as the language model and more broadly in
that we are answering questions based on an image, rather than a text
document. Ba et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S2.p3.1.6.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S2.p3.1.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> also uses a similar architecture, but in a zero-shot
learning framework to predict classifiers for novel categories. They project language and vision features into a shared subspace to perform similarity computations with inner products like us, though the score is used to guide the generation of object classifiers rather than to rank image regions.</span></p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Existing approaches in VQA tend to use recurrent
networks to model language and predict
answers </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p4.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S2.p4.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S2.p4.1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, though simpler Bag-Of-Words (BOW) and averaging models have been
shown to perform roughly as well if not better than sequence-based
LSTM </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p4.1.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S2.p4.1.6.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S2.p4.1.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;">. Yu et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p4.1.8.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S2.p4.1.9.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S2.p4.1.10" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, which
proposes a Visual Madlibs dataset for fill-in-the-blank and question
answering, focuses their approach on learning latent embeddings and
finds normalized CCA on averaged word2vec representations </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p4.1.11.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S2.p4.1.12.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S2.p4.1.13" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> to outperform recurrent networks for embedding. Similarly in our work, we find a fixed-length averaged representation of word2vec vectors for language to be
highly effective and much simpler to train, and our approach differs at a high level in our
focus on learning where to look.</span></p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_font_typewriter ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section"><span id="S3.1.1.1" class="ltx_text ltx_font_serif">3</span> </span>Approach</h2>

<figure id="S3.F3" class="ltx_figure"><img src="/html/1511.07394/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="140" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.3.1.1" class="ltx_text ltx_font_serif">Figure 3</span>: </span>Overview of our network for the
example question-answer pairing: ‘‘What color is the fire hydrant? Yellow.’’ Question
and answer representations are concatenated, fed through the network, then
combined with selectively weighted image region features to
produce a score.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p"><span id="S3.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Our method learns to embed the textual question and the
set of visual image regions into a latent space where the inner product
yields a relevance weighting for each region. See
Figure </span><a href="#S3.F3" title="Figure 3 ‣ 3 Approach ‣ Where To Look: Focus Regions for Visual Question Answering" class="ltx_ref ltx_font_typewriter" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.p1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> for an overview. The input is a question,
potential answer, and image features from a set of automatically selected candidate regions. We encode the parsed question and answer using word2vec </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p1.1.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S3.p1.1.4.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S3.p1.1.5" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> and a two-layer network. Visual features for each region are encoded using the top two layers (including the output layer) of a CNN trained on ImageNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p1.1.6.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S3.p1.1.7.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S3.p1.1.8" class="ltx_text ltx_font_typewriter" style="font-size:90%;">. The language and vision features are then embedded and compared with a dot product, which is soft-maxed to produce a per-region relevance weighting. Using these weights, a weighted average of concatenated vision and language features is the input to a 2-layer network that outputs a score for whether the answer is correct.</span></p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_typewriter ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.1.1.1" class="ltx_text ltx_font_serif">3.1</span> </span>QA Objective</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Our model is trained for the multiple choice task of the VQA
dateset. For a given question and its corresponding choices, the
objective of our network aims to maximize a margin between correct and
incorrect choices in a structured-learning fashion. We achieve this
by using a hinge loss over predicted confidences </span><math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi mathsize="90%" id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">y</annotation></semantics></math><span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">.</span></p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">In our setting, multiple answers could be acceptable to varying
degrees, as correctness is determined by the consensus of 10
annotators. For example, most may say that the color of a scarf is
‘‘blue’’ while a few others say ‘‘purple’’. To take this into
account, we scale the margin by the gap in number of annotators returning the specific answer:</span></p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="{\mathcal{L}}(y)=\max_{\forall n\neq p}(0,y_{n}+(a_{p}-a_{n})-y_{p})." display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1.4" xref="S3.E1.m1.3.3.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S3.E1.m1.3.3.1.1.4.2" xref="S3.E1.m1.3.3.1.1.4.2.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.4.1" xref="S3.E1.m1.3.3.1.1.4.1.cmml">​</mo><mrow id="S3.E1.m1.3.3.1.1.4.3.2" xref="S3.E1.m1.3.3.1.1.4.cmml"><mo maxsize="90%" minsize="90%" id="S3.E1.m1.3.3.1.1.4.3.2.1" xref="S3.E1.m1.3.3.1.1.4.cmml">(</mo><mi mathsize="90%" id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">y</mi><mo maxsize="90%" minsize="90%" id="S3.E1.m1.3.3.1.1.4.3.2.2" xref="S3.E1.m1.3.3.1.1.4.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E1.m1.3.3.1.1.3" xref="S3.E1.m1.3.3.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.2.2" xref="S3.E1.m1.3.3.1.1.2.3.cmml"><munder id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml"><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.2.cmml">max</mi><mrow id="S3.E1.m1.3.3.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.3.cmml"><mrow id="S3.E1.m1.3.3.1.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.1.1.3.2.cmml"><mo mathsize="90%" rspace="0.167em" id="S3.E1.m1.3.3.1.1.1.1.1.3.2.1" xref="S3.E1.m1.3.3.1.1.1.1.1.3.2.1.cmml">∀</mo><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.1.1.1.3.2.2" xref="S3.E1.m1.3.3.1.1.1.1.1.3.2.2.cmml">n</mi></mrow><mo mathsize="90%" id="S3.E1.m1.3.3.1.1.1.1.1.3.1" xref="S3.E1.m1.3.3.1.1.1.1.1.3.1.cmml">≠</mo><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.1.1.1.3.3.cmml">p</mi></mrow></munder><mo id="S3.E1.m1.3.3.1.1.2.2a" xref="S3.E1.m1.3.3.1.1.2.3.cmml">⁡</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.2" xref="S3.E1.m1.3.3.1.1.2.3.cmml"><mo maxsize="90%" minsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.2" xref="S3.E1.m1.3.3.1.1.2.3.cmml">(</mo><mn mathsize="90%" id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">0</mn><mo mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.3" xref="S3.E1.m1.3.3.1.1.2.3.cmml">,</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.2.1" xref="S3.E1.m1.3.3.1.1.2.2.2.1.cmml"><mrow id="S3.E1.m1.3.3.1.1.2.2.2.1.1" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.2.2.2.1.1.3" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.3.cmml"><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.1.1.3.2" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.3.2.cmml">y</mi><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.1.1.3.3" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.3.3.cmml">n</mi></msub><mo mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.1.1.2" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.2.cmml">+</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.cmml"><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.2" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.2.cmml">a</mi><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.3" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.3.cmml">p</mi></msub><mo mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.1.cmml">−</mo><msub id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.3.2.cmml">a</mi><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.3.3.cmml">n</mi></msub></mrow><mo maxsize="90%" minsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.1.2" xref="S3.E1.m1.3.3.1.1.2.2.2.1.2.cmml">−</mo><msub id="S3.E1.m1.3.3.1.1.2.2.2.1.3" xref="S3.E1.m1.3.3.1.1.2.2.2.1.3.cmml"><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.1.3.2" xref="S3.E1.m1.3.3.1.1.2.2.2.1.3.2.cmml">y</mi><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.1.3.3" xref="S3.E1.m1.3.3.1.1.2.2.2.1.3.3.cmml">p</mi></msub></mrow><mo maxsize="90%" minsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.4" xref="S3.E1.m1.3.3.1.1.2.3.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" mathsize="90%" id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1"><eq id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.3"></eq><apply id="S3.E1.m1.3.3.1.1.4.cmml" xref="S3.E1.m1.3.3.1.1.4"><times id="S3.E1.m1.3.3.1.1.4.1.cmml" xref="S3.E1.m1.3.3.1.1.4.1"></times><ci id="S3.E1.m1.3.3.1.1.4.2.cmml" xref="S3.E1.m1.3.3.1.1.4.2">ℒ</ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑦</ci></apply><apply id="S3.E1.m1.3.3.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2"><apply id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1">subscript</csymbol><max id="S3.E1.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.2"></max><apply id="S3.E1.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.3"><neq id="S3.E1.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.3.1"></neq><apply id="S3.E1.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.3.2"><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.3.2.1">for-all</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.3.2.2">𝑛</ci></apply><ci id="S3.E1.m1.3.3.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.3.3">𝑝</ci></apply></apply><cn type="integer" id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">0</cn><apply id="S3.E1.m1.3.3.1.1.2.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1"><minus id="S3.E1.m1.3.3.1.1.2.2.2.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.2"></minus><apply id="S3.E1.m1.3.3.1.1.2.2.2.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1"><plus id="S3.E1.m1.3.3.1.1.2.2.2.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.2"></plus><apply id="S3.E1.m1.3.3.1.1.2.2.2.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.2.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.2.2.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.3.2">𝑦</ci><ci id="S3.E1.m1.3.3.1.1.2.2.2.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.3.3">𝑛</ci></apply><apply id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1"><minus id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.2">𝑎</ci><ci id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.3">𝑝</ci></apply><apply id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.3.2">𝑎</ci><ci id="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.1.1.1.1.3.3">𝑛</ci></apply></apply></apply><apply id="S3.E1.m1.3.3.1.1.2.2.2.1.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.2.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.2.2.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.3.2">𝑦</ci><ci id="S3.E1.m1.3.3.1.1.2.2.2.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1.3.3">𝑝</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">{\mathcal{L}}(y)=\max_{\forall n\neq p}(0,y_{n}+(a_{p}-a_{n})-y_{p}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.9" class="ltx_p"><span id="S3.SS1.p3.9.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">The above objective requires that the score of the correct answer (</span><math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="y_{p}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi mathsize="90%" id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">y</mi><mi mathsize="90%" id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">𝑦</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">y_{p}</annotation></semantics></math><span id="S3.SS1.p3.9.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">) is at
least some margin above the score of the highest-scoring incorrect answer
(</span><math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="y_{n}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><msub id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi mathsize="90%" id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">y</mi><mi mathsize="90%" id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">𝑦</ci><ci id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">y_{n}</annotation></semantics></math><span id="S3.SS1.p3.9.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">) selected from among the set of incorrect choices (</span><math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="n\neq p" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mrow id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mi mathsize="90%" id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">n</mi><mo mathsize="90%" id="S3.SS1.p3.3.m3.1.1.1" xref="S3.SS1.p3.3.m3.1.1.1.cmml">≠</mo><mi mathsize="90%" id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><neq id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1"></neq><ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">𝑛</ci><ci id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">n\neq p</annotation></semantics></math><span id="S3.SS1.p3.9.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">).
For example, if 6/10 of the annotators answer </span><math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><mi mathsize="90%" id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><ci id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">p</annotation></semantics></math><span id="S3.SS1.p3.9.5" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> (</span><math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="a_{p}=0.6" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><mrow id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml"><msub id="S3.SS1.p3.5.m5.1.1.2" xref="S3.SS1.p3.5.m5.1.1.2.cmml"><mi mathsize="90%" id="S3.SS1.p3.5.m5.1.1.2.2" xref="S3.SS1.p3.5.m5.1.1.2.2.cmml">a</mi><mi mathsize="90%" id="S3.SS1.p3.5.m5.1.1.2.3" xref="S3.SS1.p3.5.m5.1.1.2.3.cmml">p</mi></msub><mo mathsize="90%" id="S3.SS1.p3.5.m5.1.1.1" xref="S3.SS1.p3.5.m5.1.1.1.cmml">=</mo><mn mathsize="90%" id="S3.SS1.p3.5.m5.1.1.3" xref="S3.SS1.p3.5.m5.1.1.3.cmml">0.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><apply id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1"><eq id="S3.SS1.p3.5.m5.1.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1.1"></eq><apply id="S3.SS1.p3.5.m5.1.1.2.cmml" xref="S3.SS1.p3.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.1.1.2.1.cmml" xref="S3.SS1.p3.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS1.p3.5.m5.1.1.2.2.cmml" xref="S3.SS1.p3.5.m5.1.1.2.2">𝑎</ci><ci id="S3.SS1.p3.5.m5.1.1.2.3.cmml" xref="S3.SS1.p3.5.m5.1.1.2.3">𝑝</ci></apply><cn type="float" id="S3.SS1.p3.5.m5.1.1.3.cmml" xref="S3.SS1.p3.5.m5.1.1.3">0.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">a_{p}=0.6</annotation></semantics></math><span id="S3.SS1.p3.9.6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">) and
2 annotators answer </span><math id="S3.SS1.p3.6.m6.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS1.p3.6.m6.1a"><mi mathsize="90%" id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.1b"><ci id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.1c">n</annotation></semantics></math><span id="S3.SS1.p3.9.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> (</span><math id="S3.SS1.p3.7.m7.1" class="ltx_Math" alttext="a_{n}=0.2" display="inline"><semantics id="S3.SS1.p3.7.m7.1a"><mrow id="S3.SS1.p3.7.m7.1.1" xref="S3.SS1.p3.7.m7.1.1.cmml"><msub id="S3.SS1.p3.7.m7.1.1.2" xref="S3.SS1.p3.7.m7.1.1.2.cmml"><mi mathsize="90%" id="S3.SS1.p3.7.m7.1.1.2.2" xref="S3.SS1.p3.7.m7.1.1.2.2.cmml">a</mi><mi mathsize="90%" id="S3.SS1.p3.7.m7.1.1.2.3" xref="S3.SS1.p3.7.m7.1.1.2.3.cmml">n</mi></msub><mo mathsize="90%" id="S3.SS1.p3.7.m7.1.1.1" xref="S3.SS1.p3.7.m7.1.1.1.cmml">=</mo><mn mathsize="90%" id="S3.SS1.p3.7.m7.1.1.3" xref="S3.SS1.p3.7.m7.1.1.3.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.7.m7.1b"><apply id="S3.SS1.p3.7.m7.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1"><eq id="S3.SS1.p3.7.m7.1.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1.1"></eq><apply id="S3.SS1.p3.7.m7.1.1.2.cmml" xref="S3.SS1.p3.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.7.m7.1.1.2.1.cmml" xref="S3.SS1.p3.7.m7.1.1.2">subscript</csymbol><ci id="S3.SS1.p3.7.m7.1.1.2.2.cmml" xref="S3.SS1.p3.7.m7.1.1.2.2">𝑎</ci><ci id="S3.SS1.p3.7.m7.1.1.2.3.cmml" xref="S3.SS1.p3.7.m7.1.1.2.3">𝑛</ci></apply><cn type="float" id="S3.SS1.p3.7.m7.1.1.3.cmml" xref="S3.SS1.p3.7.m7.1.1.3">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.7.m7.1c">a_{n}=0.2</annotation></semantics></math><span id="S3.SS1.p3.9.8" class="ltx_text ltx_font_typewriter" style="font-size:90%;">), then </span><math id="S3.SS1.p3.8.m8.1" class="ltx_Math" alttext="y_{p}" display="inline"><semantics id="S3.SS1.p3.8.m8.1a"><msub id="S3.SS1.p3.8.m8.1.1" xref="S3.SS1.p3.8.m8.1.1.cmml"><mi mathsize="90%" id="S3.SS1.p3.8.m8.1.1.2" xref="S3.SS1.p3.8.m8.1.1.2.cmml">y</mi><mi mathsize="90%" id="S3.SS1.p3.8.m8.1.1.3" xref="S3.SS1.p3.8.m8.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.8.m8.1b"><apply id="S3.SS1.p3.8.m8.1.1.cmml" xref="S3.SS1.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.8.m8.1.1.1.cmml" xref="S3.SS1.p3.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p3.8.m8.1.1.2.cmml" xref="S3.SS1.p3.8.m8.1.1.2">𝑦</ci><ci id="S3.SS1.p3.8.m8.1.1.3.cmml" xref="S3.SS1.p3.8.m8.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.8.m8.1c">y_{p}</annotation></semantics></math><span id="S3.SS1.p3.9.9" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> should outscore </span><math id="S3.SS1.p3.9.m9.1" class="ltx_Math" alttext="y_{n}" display="inline"><semantics id="S3.SS1.p3.9.m9.1a"><msub id="S3.SS1.p3.9.m9.1.1" xref="S3.SS1.p3.9.m9.1.1.cmml"><mi mathsize="90%" id="S3.SS1.p3.9.m9.1.1.2" xref="S3.SS1.p3.9.m9.1.1.2.cmml">y</mi><mi mathsize="90%" id="S3.SS1.p3.9.m9.1.1.3" xref="S3.SS1.p3.9.m9.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.9.m9.1b"><apply id="S3.SS1.p3.9.m9.1.1.cmml" xref="S3.SS1.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.9.m9.1.1.1.cmml" xref="S3.SS1.p3.9.m9.1.1">subscript</csymbol><ci id="S3.SS1.p3.9.m9.1.1.2.cmml" xref="S3.SS1.p3.9.m9.1.1.2">𝑦</ci><ci id="S3.SS1.p3.9.m9.1.1.3.cmml" xref="S3.SS1.p3.9.m9.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.9.m9.1c">y_{n}</annotation></semantics></math><span id="S3.SS1.p3.9.10" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> by a margin of at least 0.4.</span></p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_typewriter ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.1.1.1" class="ltx_text ltx_font_serif">3.2</span> </span>Region Selection Layer</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Our region selection layer selectively combines
incoming text features with image features from relevant regions of
the image. To determine relevance, the layer first projects the image
features and the text features into a shared
N-dimensional space, after which an inner product is computed for each
question-answer pair and all available regions.</span></p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.3" class="ltx_p"><span id="S3.SS2.p2.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Let </span><math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="G_{r}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi mathsize="90%" id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">G</mi><mi mathsize="90%" id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝐺</ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">G_{r}</annotation></semantics></math><span id="S3.SS2.p2.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> be the projection of all region features in column vectors
of </span><math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="X_{r}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><msub id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi mathsize="90%" id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">X</mi><mi mathsize="90%" id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝑋</ci><ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">X_{r}</annotation></semantics></math><span id="S3.SS2.p2.3.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, and </span><math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\vec{g}_{l}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><msub id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mover accent="true" id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml"><mi mathsize="90%" id="S3.SS2.p2.3.m3.1.1.2.2" xref="S3.SS2.p2.3.m3.1.1.2.2.cmml">g</mi><mo mathsize="90%" stretchy="false" id="S3.SS2.p2.3.m3.1.1.2.1" xref="S3.SS2.p2.3.m3.1.1.2.1.cmml">→</mo></mover><mi mathsize="90%" id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">subscript</csymbol><apply id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2"><ci id="S3.SS2.p2.3.m3.1.1.2.1.cmml" xref="S3.SS2.p2.3.m3.1.1.2.1">→</ci><ci id="S3.SS2.p2.3.m3.1.1.2.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2.2">𝑔</ci></apply><ci id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\vec{g}_{l}</annotation></semantics></math><span id="S3.SS2.p2.3.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> be the projection of a single embedded question-answer
pair. The feedforward pass to compute the relevance weightings is
computed as follows:</span></p>
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle G_{r}=" display="inline"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml"><mi mathsize="90%" id="S3.E2.m1.1.1.2.2" xref="S3.E2.m1.1.1.2.2.cmml">G</mi><mi mathsize="90%" id="S3.E2.m1.1.1.2.3" xref="S3.E2.m1.1.1.2.3.cmml">r</mi></msub><mo mathsize="90%" id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">=</mo><mi id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"></eq><apply id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.2.2">𝐺</ci><ci id="S3.E2.m1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.2.3">𝑟</ci></apply><csymbol cd="latexml" id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle G_{r}=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2.m2.1" class="ltx_Math" alttext="\displaystyle AX_{r}+\vec{b}_{r}" display="inline"><semantics id="S3.E2.m2.1a"><mrow id="S3.E2.m2.1.1" xref="S3.E2.m2.1.1.cmml"><mrow id="S3.E2.m2.1.1.2" xref="S3.E2.m2.1.1.2.cmml"><mi mathsize="90%" id="S3.E2.m2.1.1.2.2" xref="S3.E2.m2.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E2.m2.1.1.2.1" xref="S3.E2.m2.1.1.2.1.cmml">​</mo><msub id="S3.E2.m2.1.1.2.3" xref="S3.E2.m2.1.1.2.3.cmml"><mi mathsize="90%" id="S3.E2.m2.1.1.2.3.2" xref="S3.E2.m2.1.1.2.3.2.cmml">X</mi><mi mathsize="90%" id="S3.E2.m2.1.1.2.3.3" xref="S3.E2.m2.1.1.2.3.3.cmml">r</mi></msub></mrow><mo mathsize="90%" id="S3.E2.m2.1.1.1" xref="S3.E2.m2.1.1.1.cmml">+</mo><msub id="S3.E2.m2.1.1.3" xref="S3.E2.m2.1.1.3.cmml"><mover accent="true" id="S3.E2.m2.1.1.3.2" xref="S3.E2.m2.1.1.3.2.cmml"><mi mathsize="90%" id="S3.E2.m2.1.1.3.2.2" xref="S3.E2.m2.1.1.3.2.2.cmml">b</mi><mo mathsize="90%" stretchy="false" id="S3.E2.m2.1.1.3.2.1" xref="S3.E2.m2.1.1.3.2.1.cmml">→</mo></mover><mi mathsize="90%" id="S3.E2.m2.1.1.3.3" xref="S3.E2.m2.1.1.3.3.cmml">r</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m2.1b"><apply id="S3.E2.m2.1.1.cmml" xref="S3.E2.m2.1.1"><plus id="S3.E2.m2.1.1.1.cmml" xref="S3.E2.m2.1.1.1"></plus><apply id="S3.E2.m2.1.1.2.cmml" xref="S3.E2.m2.1.1.2"><times id="S3.E2.m2.1.1.2.1.cmml" xref="S3.E2.m2.1.1.2.1"></times><ci id="S3.E2.m2.1.1.2.2.cmml" xref="S3.E2.m2.1.1.2.2">𝐴</ci><apply id="S3.E2.m2.1.1.2.3.cmml" xref="S3.E2.m2.1.1.2.3"><csymbol cd="ambiguous" id="S3.E2.m2.1.1.2.3.1.cmml" xref="S3.E2.m2.1.1.2.3">subscript</csymbol><ci id="S3.E2.m2.1.1.2.3.2.cmml" xref="S3.E2.m2.1.1.2.3.2">𝑋</ci><ci id="S3.E2.m2.1.1.2.3.3.cmml" xref="S3.E2.m2.1.1.2.3.3">𝑟</ci></apply></apply><apply id="S3.E2.m2.1.1.3.cmml" xref="S3.E2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m2.1.1.3.1.cmml" xref="S3.E2.m2.1.1.3">subscript</csymbol><apply id="S3.E2.m2.1.1.3.2.cmml" xref="S3.E2.m2.1.1.3.2"><ci id="S3.E2.m2.1.1.3.2.1.cmml" xref="S3.E2.m2.1.1.3.2.1">→</ci><ci id="S3.E2.m2.1.1.3.2.2.cmml" xref="S3.E2.m2.1.1.3.2.2">𝑏</ci></apply><ci id="S3.E2.m2.1.1.3.3.cmml" xref="S3.E2.m2.1.1.3.3">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m2.1c">\displaystyle AX_{r}+\vec{b}_{r}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\displaystyle\vec{g}_{l}=" display="inline"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><msub id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml"><mover accent="true" id="S3.E3.m1.1.1.2.2" xref="S3.E3.m1.1.1.2.2.cmml"><mi mathsize="90%" id="S3.E3.m1.1.1.2.2.2" xref="S3.E3.m1.1.1.2.2.2.cmml">g</mi><mo mathsize="90%" stretchy="false" id="S3.E3.m1.1.1.2.2.1" xref="S3.E3.m1.1.1.2.2.1.cmml">→</mo></mover><mi mathsize="90%" id="S3.E3.m1.1.1.2.3" xref="S3.E3.m1.1.1.2.3.cmml">l</mi></msub><mo mathsize="90%" id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml">=</mo><mi id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"></eq><apply id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.2">subscript</csymbol><apply id="S3.E3.m1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.2.2"><ci id="S3.E3.m1.1.1.2.2.1.cmml" xref="S3.E3.m1.1.1.2.2.1">→</ci><ci id="S3.E3.m1.1.1.2.2.2.cmml" xref="S3.E3.m1.1.1.2.2.2">𝑔</ci></apply><ci id="S3.E3.m1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.2.3">𝑙</ci></apply><csymbol cd="latexml" id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\displaystyle\vec{g}_{l}=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3.m2.1" class="ltx_Math" alttext="\displaystyle B\vec{x}_{l}+\vec{b}_{l}" display="inline"><semantics id="S3.E3.m2.1a"><mrow id="S3.E3.m2.1.1" xref="S3.E3.m2.1.1.cmml"><mrow id="S3.E3.m2.1.1.2" xref="S3.E3.m2.1.1.2.cmml"><mi mathsize="90%" id="S3.E3.m2.1.1.2.2" xref="S3.E3.m2.1.1.2.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S3.E3.m2.1.1.2.1" xref="S3.E3.m2.1.1.2.1.cmml">​</mo><msub id="S3.E3.m2.1.1.2.3" xref="S3.E3.m2.1.1.2.3.cmml"><mover accent="true" id="S3.E3.m2.1.1.2.3.2" xref="S3.E3.m2.1.1.2.3.2.cmml"><mi mathsize="90%" id="S3.E3.m2.1.1.2.3.2.2" xref="S3.E3.m2.1.1.2.3.2.2.cmml">x</mi><mo mathsize="90%" stretchy="false" id="S3.E3.m2.1.1.2.3.2.1" xref="S3.E3.m2.1.1.2.3.2.1.cmml">→</mo></mover><mi mathsize="90%" id="S3.E3.m2.1.1.2.3.3" xref="S3.E3.m2.1.1.2.3.3.cmml">l</mi></msub></mrow><mo mathsize="90%" id="S3.E3.m2.1.1.1" xref="S3.E3.m2.1.1.1.cmml">+</mo><msub id="S3.E3.m2.1.1.3" xref="S3.E3.m2.1.1.3.cmml"><mover accent="true" id="S3.E3.m2.1.1.3.2" xref="S3.E3.m2.1.1.3.2.cmml"><mi mathsize="90%" id="S3.E3.m2.1.1.3.2.2" xref="S3.E3.m2.1.1.3.2.2.cmml">b</mi><mo mathsize="90%" stretchy="false" id="S3.E3.m2.1.1.3.2.1" xref="S3.E3.m2.1.1.3.2.1.cmml">→</mo></mover><mi mathsize="90%" id="S3.E3.m2.1.1.3.3" xref="S3.E3.m2.1.1.3.3.cmml">l</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m2.1b"><apply id="S3.E3.m2.1.1.cmml" xref="S3.E3.m2.1.1"><plus id="S3.E3.m2.1.1.1.cmml" xref="S3.E3.m2.1.1.1"></plus><apply id="S3.E3.m2.1.1.2.cmml" xref="S3.E3.m2.1.1.2"><times id="S3.E3.m2.1.1.2.1.cmml" xref="S3.E3.m2.1.1.2.1"></times><ci id="S3.E3.m2.1.1.2.2.cmml" xref="S3.E3.m2.1.1.2.2">𝐵</ci><apply id="S3.E3.m2.1.1.2.3.cmml" xref="S3.E3.m2.1.1.2.3"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.2.3.1.cmml" xref="S3.E3.m2.1.1.2.3">subscript</csymbol><apply id="S3.E3.m2.1.1.2.3.2.cmml" xref="S3.E3.m2.1.1.2.3.2"><ci id="S3.E3.m2.1.1.2.3.2.1.cmml" xref="S3.E3.m2.1.1.2.3.2.1">→</ci><ci id="S3.E3.m2.1.1.2.3.2.2.cmml" xref="S3.E3.m2.1.1.2.3.2.2">𝑥</ci></apply><ci id="S3.E3.m2.1.1.2.3.3.cmml" xref="S3.E3.m2.1.1.2.3.3">𝑙</ci></apply></apply><apply id="S3.E3.m2.1.1.3.cmml" xref="S3.E3.m2.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.3.1.cmml" xref="S3.E3.m2.1.1.3">subscript</csymbol><apply id="S3.E3.m2.1.1.3.2.cmml" xref="S3.E3.m2.1.1.3.2"><ci id="S3.E3.m2.1.1.3.2.1.cmml" xref="S3.E3.m2.1.1.3.2.1">→</ci><ci id="S3.E3.m2.1.1.3.2.2.cmml" xref="S3.E3.m2.1.1.3.2.2">𝑏</ci></apply><ci id="S3.E3.m2.1.1.3.3.cmml" xref="S3.E3.m2.1.1.3.3">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m2.1c">\displaystyle B\vec{x}_{l}+\vec{b}_{l}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4.m1.2" class="ltx_Math" alttext="\displaystyle\vec{s}_{l,r}=" display="inline"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.3" xref="S3.E4.m1.2.3.cmml"><msub id="S3.E4.m1.2.3.2" xref="S3.E4.m1.2.3.2.cmml"><mover accent="true" id="S3.E4.m1.2.3.2.2" xref="S3.E4.m1.2.3.2.2.cmml"><mi mathsize="90%" id="S3.E4.m1.2.3.2.2.2" xref="S3.E4.m1.2.3.2.2.2.cmml">s</mi><mo mathsize="90%" stretchy="false" id="S3.E4.m1.2.3.2.2.1" xref="S3.E4.m1.2.3.2.2.1.cmml">→</mo></mover><mrow id="S3.E4.m1.2.2.2.4" xref="S3.E4.m1.2.2.2.3.cmml"><mi mathsize="90%" id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml">l</mi><mo mathsize="90%" id="S3.E4.m1.2.2.2.4.1" xref="S3.E4.m1.2.2.2.3.cmml">,</mo><mi mathsize="90%" id="S3.E4.m1.2.2.2.2" xref="S3.E4.m1.2.2.2.2.cmml">r</mi></mrow></msub><mo mathsize="90%" id="S3.E4.m1.2.3.1" xref="S3.E4.m1.2.3.1.cmml">=</mo><mi id="S3.E4.m1.2.3.3" xref="S3.E4.m1.2.3.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.3.cmml" xref="S3.E4.m1.2.3"><eq id="S3.E4.m1.2.3.1.cmml" xref="S3.E4.m1.2.3.1"></eq><apply id="S3.E4.m1.2.3.2.cmml" xref="S3.E4.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.3.2.1.cmml" xref="S3.E4.m1.2.3.2">subscript</csymbol><apply id="S3.E4.m1.2.3.2.2.cmml" xref="S3.E4.m1.2.3.2.2"><ci id="S3.E4.m1.2.3.2.2.1.cmml" xref="S3.E4.m1.2.3.2.2.1">→</ci><ci id="S3.E4.m1.2.3.2.2.2.cmml" xref="S3.E4.m1.2.3.2.2.2">𝑠</ci></apply><list id="S3.E4.m1.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.4"><ci id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1">𝑙</ci><ci id="S3.E4.m1.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2">𝑟</ci></list></apply><csymbol cd="latexml" id="S3.E4.m1.2.3.3.cmml" xref="S3.E4.m1.2.3.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">\displaystyle\vec{s}_{l,r}=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E4.m2.1" class="ltx_Math" alttext="\displaystyle\sigma(G_{r}^{T}\vec{g}_{l})" display="inline"><semantics id="S3.E4.m2.1a"><mrow id="S3.E4.m2.1.1" xref="S3.E4.m2.1.1.cmml"><mi mathsize="90%" id="S3.E4.m2.1.1.3" xref="S3.E4.m2.1.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.E4.m2.1.1.2" xref="S3.E4.m2.1.1.2.cmml">​</mo><mrow id="S3.E4.m2.1.1.1.1" xref="S3.E4.m2.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E4.m2.1.1.1.1.2" xref="S3.E4.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m2.1.1.1.1.1" xref="S3.E4.m2.1.1.1.1.1.cmml"><msubsup id="S3.E4.m2.1.1.1.1.1.2" xref="S3.E4.m2.1.1.1.1.1.2.cmml"><mi mathsize="90%" id="S3.E4.m2.1.1.1.1.1.2.2.2" xref="S3.E4.m2.1.1.1.1.1.2.2.2.cmml">G</mi><mi mathsize="90%" id="S3.E4.m2.1.1.1.1.1.2.2.3" xref="S3.E4.m2.1.1.1.1.1.2.2.3.cmml">r</mi><mi mathsize="90%" id="S3.E4.m2.1.1.1.1.1.2.3" xref="S3.E4.m2.1.1.1.1.1.2.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S3.E4.m2.1.1.1.1.1.1" xref="S3.E4.m2.1.1.1.1.1.1.cmml">​</mo><msub id="S3.E4.m2.1.1.1.1.1.3" xref="S3.E4.m2.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E4.m2.1.1.1.1.1.3.2" xref="S3.E4.m2.1.1.1.1.1.3.2.cmml"><mi mathsize="90%" id="S3.E4.m2.1.1.1.1.1.3.2.2" xref="S3.E4.m2.1.1.1.1.1.3.2.2.cmml">g</mi><mo mathsize="90%" stretchy="false" id="S3.E4.m2.1.1.1.1.1.3.2.1" xref="S3.E4.m2.1.1.1.1.1.3.2.1.cmml">→</mo></mover><mi mathsize="90%" id="S3.E4.m2.1.1.1.1.1.3.3" xref="S3.E4.m2.1.1.1.1.1.3.3.cmml">l</mi></msub></mrow><mo maxsize="90%" minsize="90%" id="S3.E4.m2.1.1.1.1.3" xref="S3.E4.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m2.1b"><apply id="S3.E4.m2.1.1.cmml" xref="S3.E4.m2.1.1"><times id="S3.E4.m2.1.1.2.cmml" xref="S3.E4.m2.1.1.2"></times><ci id="S3.E4.m2.1.1.3.cmml" xref="S3.E4.m2.1.1.3">𝜎</ci><apply id="S3.E4.m2.1.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1"><times id="S3.E4.m2.1.1.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1.1.1"></times><apply id="S3.E4.m2.1.1.1.1.1.2.cmml" xref="S3.E4.m2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.1.2.1.cmml" xref="S3.E4.m2.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E4.m2.1.1.1.1.1.2.2.cmml" xref="S3.E4.m2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.1.2.2.1.cmml" xref="S3.E4.m2.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E4.m2.1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m2.1.1.1.1.1.2.2.2">𝐺</ci><ci id="S3.E4.m2.1.1.1.1.1.2.2.3.cmml" xref="S3.E4.m2.1.1.1.1.1.2.2.3">𝑟</ci></apply><ci id="S3.E4.m2.1.1.1.1.1.2.3.cmml" xref="S3.E4.m2.1.1.1.1.1.2.3">𝑇</ci></apply><apply id="S3.E4.m2.1.1.1.1.1.3.cmml" xref="S3.E4.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.1.3.1.cmml" xref="S3.E4.m2.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E4.m2.1.1.1.1.1.3.2.cmml" xref="S3.E4.m2.1.1.1.1.1.3.2"><ci id="S3.E4.m2.1.1.1.1.1.3.2.1.cmml" xref="S3.E4.m2.1.1.1.1.1.3.2.1">→</ci><ci id="S3.E4.m2.1.1.1.1.1.3.2.2.cmml" xref="S3.E4.m2.1.1.1.1.1.3.2.2">𝑔</ci></apply><ci id="S3.E4.m2.1.1.1.1.1.3.3.cmml" xref="S3.E4.m2.1.1.1.1.1.3.3">𝑙</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m2.1c">\displaystyle\sigma(G_{r}^{T}\vec{g}_{l})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E5.m1.1" class="ltx_Math" alttext="\displaystyle\sigma(\vec{z})=" display="inline"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.2" xref="S3.E5.m1.1.2.cmml"><mrow id="S3.E5.m1.1.2.2" xref="S3.E5.m1.1.2.2.cmml"><mi mathsize="90%" id="S3.E5.m1.1.2.2.2" xref="S3.E5.m1.1.2.2.2.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.2.2.1" xref="S3.E5.m1.1.2.2.1.cmml">​</mo><mrow id="S3.E5.m1.1.2.2.3.2" xref="S3.E5.m1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E5.m1.1.2.2.3.2.1" xref="S3.E5.m1.1.1.cmml">(</mo><mover accent="true" id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><mi mathsize="90%" id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml">z</mi><mo mathsize="90%" stretchy="false" id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.cmml">→</mo></mover><mo maxsize="90%" minsize="90%" id="S3.E5.m1.1.2.2.3.2.2" xref="S3.E5.m1.1.1.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E5.m1.1.2.1" xref="S3.E5.m1.1.2.1.cmml">=</mo><mi id="S3.E5.m1.1.2.3" xref="S3.E5.m1.1.2.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.2.cmml" xref="S3.E5.m1.1.2"><eq id="S3.E5.m1.1.2.1.cmml" xref="S3.E5.m1.1.2.1"></eq><apply id="S3.E5.m1.1.2.2.cmml" xref="S3.E5.m1.1.2.2"><times id="S3.E5.m1.1.2.2.1.cmml" xref="S3.E5.m1.1.2.2.1"></times><ci id="S3.E5.m1.1.2.2.2.cmml" xref="S3.E5.m1.1.2.2.2">𝜎</ci><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.2.2.3.2"><ci id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1.1">→</ci><ci id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2">𝑧</ci></apply></apply><csymbol cd="latexml" id="S3.E5.m1.1.2.3.cmml" xref="S3.E5.m1.1.2.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\displaystyle\sigma(\vec{z})=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E5.m2.2" class="ltx_Math" alttext="\displaystyle\frac{e^{z_{j}}}{\sum_{k=1}^{K}e^{z_{k}}}\mbox{ for }j=1,...K" display="inline"><semantics id="S3.E5.m2.2a"><mrow id="S3.E5.m2.2.2" xref="S3.E5.m2.2.2.cmml"><mrow id="S3.E5.m2.2.2.3" xref="S3.E5.m2.2.2.3.cmml"><mstyle displaystyle="true" id="S3.E5.m2.2.2.3.2" xref="S3.E5.m2.2.2.3.2.cmml"><mfrac id="S3.E5.m2.2.2.3.2a" xref="S3.E5.m2.2.2.3.2.cmml"><msup id="S3.E5.m2.2.2.3.2.2" xref="S3.E5.m2.2.2.3.2.2.cmml"><mi mathsize="90%" id="S3.E5.m2.2.2.3.2.2.2" xref="S3.E5.m2.2.2.3.2.2.2.cmml">e</mi><msub id="S3.E5.m2.2.2.3.2.2.3" xref="S3.E5.m2.2.2.3.2.2.3.cmml"><mi mathsize="90%" id="S3.E5.m2.2.2.3.2.2.3.2" xref="S3.E5.m2.2.2.3.2.2.3.2.cmml">z</mi><mi mathsize="90%" id="S3.E5.m2.2.2.3.2.2.3.3" xref="S3.E5.m2.2.2.3.2.2.3.3.cmml">j</mi></msub></msup><mrow id="S3.E5.m2.2.2.3.2.3" xref="S3.E5.m2.2.2.3.2.3.cmml"><msubsup id="S3.E5.m2.2.2.3.2.3.1" xref="S3.E5.m2.2.2.3.2.3.1.cmml"><mo maxsize="90%" minsize="90%" stretchy="true" id="S3.E5.m2.2.2.3.2.3.1.2.2" xref="S3.E5.m2.2.2.3.2.3.1.2.2.cmml">∑</mo><mrow id="S3.E5.m2.2.2.3.2.3.1.2.3" xref="S3.E5.m2.2.2.3.2.3.1.2.3.cmml"><mi mathsize="90%" id="S3.E5.m2.2.2.3.2.3.1.2.3.2" xref="S3.E5.m2.2.2.3.2.3.1.2.3.2.cmml">k</mi><mo mathsize="90%" id="S3.E5.m2.2.2.3.2.3.1.2.3.1" xref="S3.E5.m2.2.2.3.2.3.1.2.3.1.cmml">=</mo><mn mathsize="90%" id="S3.E5.m2.2.2.3.2.3.1.2.3.3" xref="S3.E5.m2.2.2.3.2.3.1.2.3.3.cmml">1</mn></mrow><mi mathsize="90%" id="S3.E5.m2.2.2.3.2.3.1.3" xref="S3.E5.m2.2.2.3.2.3.1.3.cmml">K</mi></msubsup><msup id="S3.E5.m2.2.2.3.2.3.2" xref="S3.E5.m2.2.2.3.2.3.2.cmml"><mi mathsize="90%" id="S3.E5.m2.2.2.3.2.3.2.2" xref="S3.E5.m2.2.2.3.2.3.2.2.cmml">e</mi><msub id="S3.E5.m2.2.2.3.2.3.2.3" xref="S3.E5.m2.2.2.3.2.3.2.3.cmml"><mi mathsize="90%" id="S3.E5.m2.2.2.3.2.3.2.3.2" xref="S3.E5.m2.2.2.3.2.3.2.3.2.cmml">z</mi><mi mathsize="90%" id="S3.E5.m2.2.2.3.2.3.2.3.3" xref="S3.E5.m2.2.2.3.2.3.2.3.3.cmml">k</mi></msub></msup></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.E5.m2.2.2.3.1" xref="S3.E5.m2.2.2.3.1.cmml">​</mo><mtext class="ltx_mathvariant_monospace" mathsize="90%" id="S3.E5.m2.2.2.3.3" xref="S3.E5.m2.2.2.3.3a.cmml"> for </mtext><mo lspace="0em" rspace="0em" id="S3.E5.m2.2.2.3.1a" xref="S3.E5.m2.2.2.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E5.m2.2.2.3.4" xref="S3.E5.m2.2.2.3.4.cmml">j</mi></mrow><mo mathsize="90%" id="S3.E5.m2.2.2.2" xref="S3.E5.m2.2.2.2.cmml">=</mo><mrow id="S3.E5.m2.2.2.1.1" xref="S3.E5.m2.2.2.1.2.cmml"><mn mathsize="90%" id="S3.E5.m2.1.1" xref="S3.E5.m2.1.1.cmml">1</mn><mo mathsize="90%" id="S3.E5.m2.2.2.1.1.2" xref="S3.E5.m2.2.2.1.2.cmml">,</mo><mrow id="S3.E5.m2.2.2.1.1.1" xref="S3.E5.m2.2.2.1.1.1.cmml"><mi mathsize="90%" mathvariant="normal" id="S3.E5.m2.2.2.1.1.1.2" xref="S3.E5.m2.2.2.1.1.1.2.cmml">…</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.2.2.1.1.1.1" xref="S3.E5.m2.2.2.1.1.1.1.cmml">​</mo><mi mathsize="90%" id="S3.E5.m2.2.2.1.1.1.3" xref="S3.E5.m2.2.2.1.1.1.3.cmml">K</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m2.2b"><apply id="S3.E5.m2.2.2.cmml" xref="S3.E5.m2.2.2"><eq id="S3.E5.m2.2.2.2.cmml" xref="S3.E5.m2.2.2.2"></eq><apply id="S3.E5.m2.2.2.3.cmml" xref="S3.E5.m2.2.2.3"><times id="S3.E5.m2.2.2.3.1.cmml" xref="S3.E5.m2.2.2.3.1"></times><apply id="S3.E5.m2.2.2.3.2.cmml" xref="S3.E5.m2.2.2.3.2"><divide id="S3.E5.m2.2.2.3.2.1.cmml" xref="S3.E5.m2.2.2.3.2"></divide><apply id="S3.E5.m2.2.2.3.2.2.cmml" xref="S3.E5.m2.2.2.3.2.2"><csymbol cd="ambiguous" id="S3.E5.m2.2.2.3.2.2.1.cmml" xref="S3.E5.m2.2.2.3.2.2">superscript</csymbol><ci id="S3.E5.m2.2.2.3.2.2.2.cmml" xref="S3.E5.m2.2.2.3.2.2.2">𝑒</ci><apply id="S3.E5.m2.2.2.3.2.2.3.cmml" xref="S3.E5.m2.2.2.3.2.2.3"><csymbol cd="ambiguous" id="S3.E5.m2.2.2.3.2.2.3.1.cmml" xref="S3.E5.m2.2.2.3.2.2.3">subscript</csymbol><ci id="S3.E5.m2.2.2.3.2.2.3.2.cmml" xref="S3.E5.m2.2.2.3.2.2.3.2">𝑧</ci><ci id="S3.E5.m2.2.2.3.2.2.3.3.cmml" xref="S3.E5.m2.2.2.3.2.2.3.3">𝑗</ci></apply></apply><apply id="S3.E5.m2.2.2.3.2.3.cmml" xref="S3.E5.m2.2.2.3.2.3"><apply id="S3.E5.m2.2.2.3.2.3.1.cmml" xref="S3.E5.m2.2.2.3.2.3.1"><csymbol cd="ambiguous" id="S3.E5.m2.2.2.3.2.3.1.1.cmml" xref="S3.E5.m2.2.2.3.2.3.1">superscript</csymbol><apply id="S3.E5.m2.2.2.3.2.3.1.2.cmml" xref="S3.E5.m2.2.2.3.2.3.1"><csymbol cd="ambiguous" id="S3.E5.m2.2.2.3.2.3.1.2.1.cmml" xref="S3.E5.m2.2.2.3.2.3.1">subscript</csymbol><sum id="S3.E5.m2.2.2.3.2.3.1.2.2.cmml" xref="S3.E5.m2.2.2.3.2.3.1.2.2"></sum><apply id="S3.E5.m2.2.2.3.2.3.1.2.3.cmml" xref="S3.E5.m2.2.2.3.2.3.1.2.3"><eq id="S3.E5.m2.2.2.3.2.3.1.2.3.1.cmml" xref="S3.E5.m2.2.2.3.2.3.1.2.3.1"></eq><ci id="S3.E5.m2.2.2.3.2.3.1.2.3.2.cmml" xref="S3.E5.m2.2.2.3.2.3.1.2.3.2">𝑘</ci><cn type="integer" id="S3.E5.m2.2.2.3.2.3.1.2.3.3.cmml" xref="S3.E5.m2.2.2.3.2.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E5.m2.2.2.3.2.3.1.3.cmml" xref="S3.E5.m2.2.2.3.2.3.1.3">𝐾</ci></apply><apply id="S3.E5.m2.2.2.3.2.3.2.cmml" xref="S3.E5.m2.2.2.3.2.3.2"><csymbol cd="ambiguous" id="S3.E5.m2.2.2.3.2.3.2.1.cmml" xref="S3.E5.m2.2.2.3.2.3.2">superscript</csymbol><ci id="S3.E5.m2.2.2.3.2.3.2.2.cmml" xref="S3.E5.m2.2.2.3.2.3.2.2">𝑒</ci><apply id="S3.E5.m2.2.2.3.2.3.2.3.cmml" xref="S3.E5.m2.2.2.3.2.3.2.3"><csymbol cd="ambiguous" id="S3.E5.m2.2.2.3.2.3.2.3.1.cmml" xref="S3.E5.m2.2.2.3.2.3.2.3">subscript</csymbol><ci id="S3.E5.m2.2.2.3.2.3.2.3.2.cmml" xref="S3.E5.m2.2.2.3.2.3.2.3.2">𝑧</ci><ci id="S3.E5.m2.2.2.3.2.3.2.3.3.cmml" xref="S3.E5.m2.2.2.3.2.3.2.3.3">𝑘</ci></apply></apply></apply></apply><ci id="S3.E5.m2.2.2.3.3a.cmml" xref="S3.E5.m2.2.2.3.3"><mtext class="ltx_mathvariant_monospace" mathsize="90%" id="S3.E5.m2.2.2.3.3.cmml" xref="S3.E5.m2.2.2.3.3"> for </mtext></ci><ci id="S3.E5.m2.2.2.3.4.cmml" xref="S3.E5.m2.2.2.3.4">𝑗</ci></apply><list id="S3.E5.m2.2.2.1.2.cmml" xref="S3.E5.m2.2.2.1.1"><cn type="integer" id="S3.E5.m2.1.1.cmml" xref="S3.E5.m2.1.1">1</cn><apply id="S3.E5.m2.2.2.1.1.1.cmml" xref="S3.E5.m2.2.2.1.1.1"><times id="S3.E5.m2.2.2.1.1.1.1.cmml" xref="S3.E5.m2.2.2.1.1.1.1"></times><ci id="S3.E5.m2.2.2.1.1.1.2.cmml" xref="S3.E5.m2.2.2.1.1.1.2">…</ci><ci id="S3.E5.m2.2.2.1.1.1.3.cmml" xref="S3.E5.m2.2.2.1.1.1.3">𝐾</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m2.2c">\displaystyle\frac{e^{z_{j}}}{\sum_{k=1}^{K}e^{z_{k}}}\mbox{ for }j=1,...K</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.8" class="ltx_p"><span id="S3.SS2.p2.8.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Here, the output </span><math id="S3.SS2.p2.4.m1.2" class="ltx_Math" alttext="\vec{s}_{l,r}" display="inline"><semantics id="S3.SS2.p2.4.m1.2a"><msub id="S3.SS2.p2.4.m1.2.3" xref="S3.SS2.p2.4.m1.2.3.cmml"><mover accent="true" id="S3.SS2.p2.4.m1.2.3.2" xref="S3.SS2.p2.4.m1.2.3.2.cmml"><mi mathsize="90%" id="S3.SS2.p2.4.m1.2.3.2.2" xref="S3.SS2.p2.4.m1.2.3.2.2.cmml">s</mi><mo mathsize="90%" stretchy="false" id="S3.SS2.p2.4.m1.2.3.2.1" xref="S3.SS2.p2.4.m1.2.3.2.1.cmml">→</mo></mover><mrow id="S3.SS2.p2.4.m1.2.2.2.4" xref="S3.SS2.p2.4.m1.2.2.2.3.cmml"><mi mathsize="90%" id="S3.SS2.p2.4.m1.1.1.1.1" xref="S3.SS2.p2.4.m1.1.1.1.1.cmml">l</mi><mo mathsize="90%" id="S3.SS2.p2.4.m1.2.2.2.4.1" xref="S3.SS2.p2.4.m1.2.2.2.3.cmml">,</mo><mi mathsize="90%" id="S3.SS2.p2.4.m1.2.2.2.2" xref="S3.SS2.p2.4.m1.2.2.2.2.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m1.2b"><apply id="S3.SS2.p2.4.m1.2.3.cmml" xref="S3.SS2.p2.4.m1.2.3"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m1.2.3.1.cmml" xref="S3.SS2.p2.4.m1.2.3">subscript</csymbol><apply id="S3.SS2.p2.4.m1.2.3.2.cmml" xref="S3.SS2.p2.4.m1.2.3.2"><ci id="S3.SS2.p2.4.m1.2.3.2.1.cmml" xref="S3.SS2.p2.4.m1.2.3.2.1">→</ci><ci id="S3.SS2.p2.4.m1.2.3.2.2.cmml" xref="S3.SS2.p2.4.m1.2.3.2.2">𝑠</ci></apply><list id="S3.SS2.p2.4.m1.2.2.2.3.cmml" xref="S3.SS2.p2.4.m1.2.2.2.4"><ci id="S3.SS2.p2.4.m1.1.1.1.1.cmml" xref="S3.SS2.p2.4.m1.1.1.1.1">𝑙</ci><ci id="S3.SS2.p2.4.m1.2.2.2.2.cmml" xref="S3.SS2.p2.4.m1.2.2.2.2">𝑟</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m1.2c">\vec{s}_{l,r}</annotation></semantics></math><span id="S3.SS2.p2.8.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> is the softmax normalized weighting
(</span><math id="S3.SS2.p2.5.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS2.p2.5.m2.1a"><mi mathsize="90%" id="S3.SS2.p2.5.m2.1.1" xref="S3.SS2.p2.5.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m2.1b"><ci id="S3.SS2.p2.5.m2.1.1.cmml" xref="S3.SS2.p2.5.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m2.1c">\sigma</annotation></semantics></math><span id="S3.SS2.p2.8.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">) of the inner products of </span><math id="S3.SS2.p2.6.m3.1" class="ltx_Math" alttext="\vec{g}_{l}" display="inline"><semantics id="S3.SS2.p2.6.m3.1a"><msub id="S3.SS2.p2.6.m3.1.1" xref="S3.SS2.p2.6.m3.1.1.cmml"><mover accent="true" id="S3.SS2.p2.6.m3.1.1.2" xref="S3.SS2.p2.6.m3.1.1.2.cmml"><mi mathsize="90%" id="S3.SS2.p2.6.m3.1.1.2.2" xref="S3.SS2.p2.6.m3.1.1.2.2.cmml">g</mi><mo mathsize="90%" stretchy="false" id="S3.SS2.p2.6.m3.1.1.2.1" xref="S3.SS2.p2.6.m3.1.1.2.1.cmml">→</mo></mover><mi mathsize="90%" id="S3.SS2.p2.6.m3.1.1.3" xref="S3.SS2.p2.6.m3.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m3.1b"><apply id="S3.SS2.p2.6.m3.1.1.cmml" xref="S3.SS2.p2.6.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m3.1.1.1.cmml" xref="S3.SS2.p2.6.m3.1.1">subscript</csymbol><apply id="S3.SS2.p2.6.m3.1.1.2.cmml" xref="S3.SS2.p2.6.m3.1.1.2"><ci id="S3.SS2.p2.6.m3.1.1.2.1.cmml" xref="S3.SS2.p2.6.m3.1.1.2.1">→</ci><ci id="S3.SS2.p2.6.m3.1.1.2.2.cmml" xref="S3.SS2.p2.6.m3.1.1.2.2">𝑔</ci></apply><ci id="S3.SS2.p2.6.m3.1.1.3.cmml" xref="S3.SS2.p2.6.m3.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m3.1c">\vec{g}_{l}</annotation></semantics></math><span id="S3.SS2.p2.8.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> with each projected
region feature in </span><math id="S3.SS2.p2.7.m4.1" class="ltx_Math" alttext="G_{r}" display="inline"><semantics id="S3.SS2.p2.7.m4.1a"><msub id="S3.SS2.p2.7.m4.1.1" xref="S3.SS2.p2.7.m4.1.1.cmml"><mi mathsize="90%" id="S3.SS2.p2.7.m4.1.1.2" xref="S3.SS2.p2.7.m4.1.1.2.cmml">G</mi><mi mathsize="90%" id="S3.SS2.p2.7.m4.1.1.3" xref="S3.SS2.p2.7.m4.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m4.1b"><apply id="S3.SS2.p2.7.m4.1.1.cmml" xref="S3.SS2.p2.7.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m4.1.1.1.cmml" xref="S3.SS2.p2.7.m4.1.1">subscript</csymbol><ci id="S3.SS2.p2.7.m4.1.1.2.cmml" xref="S3.SS2.p2.7.m4.1.1.2">𝐺</ci><ci id="S3.SS2.p2.7.m4.1.1.3.cmml" xref="S3.SS2.p2.7.m4.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m4.1c">G_{r}</annotation></semantics></math><span id="S3.SS2.p2.8.5" class="ltx_text ltx_font_typewriter" style="font-size:90%;">. Vectors </span><math id="S3.SS2.p2.8.m5.1" class="ltx_Math" alttext="\vec{b}" display="inline"><semantics id="S3.SS2.p2.8.m5.1a"><mover accent="true" id="S3.SS2.p2.8.m5.1.1" xref="S3.SS2.p2.8.m5.1.1.cmml"><mi mathsize="90%" id="S3.SS2.p2.8.m5.1.1.2" xref="S3.SS2.p2.8.m5.1.1.2.cmml">b</mi><mo mathsize="90%" stretchy="false" id="S3.SS2.p2.8.m5.1.1.1" xref="S3.SS2.p2.8.m5.1.1.1.cmml">→</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m5.1b"><apply id="S3.SS2.p2.8.m5.1.1.cmml" xref="S3.SS2.p2.8.m5.1.1"><ci id="S3.SS2.p2.8.m5.1.1.1.cmml" xref="S3.SS2.p2.8.m5.1.1.1">→</ci><ci id="S3.SS2.p2.8.m5.1.1.2.cmml" xref="S3.SS2.p2.8.m5.1.1.2">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m5.1c">\vec{b}</annotation></semantics></math><span id="S3.SS2.p2.8.6" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> represent biases. The
purpose of the inner product is to force the model to determine region
relevance in a vector similarity fashion.</span></p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.5" class="ltx_p"><span id="S3.SS2.p3.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Using 100 regions per image, this gives us 100 region weights for a question-answer pair. Next, the text features are concatenated directly with image features for each region to produce
100 different feature vectors. This is shown in the horizontal
stacking of </span><math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="X_{r}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><msub id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi mathsize="90%" id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">X</mi><mi mathsize="90%" id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝑋</ci><ci id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">X_{r}</annotation></semantics></math><span id="S3.SS2.p3.5.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> and repetitions of </span><math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="\vec{x}_{l}" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mover accent="true" id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml"><mi mathsize="90%" id="S3.SS2.p3.2.m2.1.1.2.2" xref="S3.SS2.p3.2.m2.1.1.2.2.cmml">x</mi><mo mathsize="90%" stretchy="false" id="S3.SS2.p3.2.m2.1.1.2.1" xref="S3.SS2.p3.2.m2.1.1.2.1.cmml">→</mo></mover><mi mathsize="90%" id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><apply id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2"><ci id="S3.SS2.p3.2.m2.1.1.2.1.cmml" xref="S3.SS2.p3.2.m2.1.1.2.1">→</ci><ci id="S3.SS2.p3.2.m2.1.1.2.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2.2">𝑥</ci></apply><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">\vec{x}_{l}</annotation></semantics></math><span id="S3.SS2.p3.5.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> below. Each feature vector is linearly projected with </span><math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mi mathsize="90%" id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><ci id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">W</annotation></semantics></math><span id="S3.SS2.p3.5.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, and the weighted average is computed using </span><math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="\vec{s}_{r}" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><msub id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml"><mover accent="true" id="S3.SS2.p3.4.m4.1.1.2" xref="S3.SS2.p3.4.m4.1.1.2.cmml"><mi mathsize="90%" id="S3.SS2.p3.4.m4.1.1.2.2" xref="S3.SS2.p3.4.m4.1.1.2.2.cmml">s</mi><mo mathsize="90%" stretchy="false" id="S3.SS2.p3.4.m4.1.1.2.1" xref="S3.SS2.p3.4.m4.1.1.2.1.cmml">→</mo></mover><mi mathsize="90%" id="S3.SS2.p3.4.m4.1.1.3" xref="S3.SS2.p3.4.m4.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><apply id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">subscript</csymbol><apply id="S3.SS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1.2"><ci id="S3.SS2.p3.4.m4.1.1.2.1.cmml" xref="S3.SS2.p3.4.m4.1.1.2.1">→</ci><ci id="S3.SS2.p3.4.m4.1.1.2.2.cmml" xref="S3.SS2.p3.4.m4.1.1.2.2">𝑠</ci></apply><ci id="S3.SS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">\vec{s}_{r}</annotation></semantics></math><span id="S3.SS2.p3.5.5" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> to attain feature vector </span><math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="\vec{a}_{l}" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><msub id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml"><mover accent="true" id="S3.SS2.p3.5.m5.1.1.2" xref="S3.SS2.p3.5.m5.1.1.2.cmml"><mi mathsize="90%" id="S3.SS2.p3.5.m5.1.1.2.2" xref="S3.SS2.p3.5.m5.1.1.2.2.cmml">a</mi><mo mathsize="90%" stretchy="false" id="S3.SS2.p3.5.m5.1.1.2.1" xref="S3.SS2.p3.5.m5.1.1.2.1.cmml">→</mo></mover><mi mathsize="90%" id="S3.SS2.p3.5.m5.1.1.3" xref="S3.SS2.p3.5.m5.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><apply id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">subscript</csymbol><apply id="S3.SS2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2"><ci id="S3.SS2.p3.5.m5.1.1.2.1.cmml" xref="S3.SS2.p3.5.m5.1.1.2.1">→</ci><ci id="S3.SS2.p3.5.m5.1.1.2.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2.2">𝑎</ci></apply><ci id="S3.SS2.p3.5.m5.1.1.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">\vec{a}_{l}</annotation></semantics></math><span id="S3.SS2.p3.5.6" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> for each question and answer pair, which is then fed through relu and batch-normalization layers.</span></p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<table id="S5.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E9.m1.2" class="ltx_Math" alttext="\displaystyle P_{l,r}=" display="inline"><semantics id="S3.E9.m1.2a"><mrow id="S3.E9.m1.2.3" xref="S3.E9.m1.2.3.cmml"><msub id="S3.E9.m1.2.3.2" xref="S3.E9.m1.2.3.2.cmml"><mi mathsize="90%" id="S3.E9.m1.2.3.2.2" xref="S3.E9.m1.2.3.2.2.cmml">P</mi><mrow id="S3.E9.m1.2.2.2.4" xref="S3.E9.m1.2.2.2.3.cmml"><mi mathsize="90%" id="S3.E9.m1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.cmml">l</mi><mo mathsize="90%" id="S3.E9.m1.2.2.2.4.1" xref="S3.E9.m1.2.2.2.3.cmml">,</mo><mi mathsize="90%" id="S3.E9.m1.2.2.2.2" xref="S3.E9.m1.2.2.2.2.cmml">r</mi></mrow></msub><mo mathsize="90%" id="S3.E9.m1.2.3.1" xref="S3.E9.m1.2.3.1.cmml">=</mo><mi id="S3.E9.m1.2.3.3" xref="S3.E9.m1.2.3.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m1.2b"><apply id="S3.E9.m1.2.3.cmml" xref="S3.E9.m1.2.3"><eq id="S3.E9.m1.2.3.1.cmml" xref="S3.E9.m1.2.3.1"></eq><apply id="S3.E9.m1.2.3.2.cmml" xref="S3.E9.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E9.m1.2.3.2.1.cmml" xref="S3.E9.m1.2.3.2">subscript</csymbol><ci id="S3.E9.m1.2.3.2.2.cmml" xref="S3.E9.m1.2.3.2.2">𝑃</ci><list id="S3.E9.m1.2.2.2.3.cmml" xref="S3.E9.m1.2.2.2.4"><ci id="S3.E9.m1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1">𝑙</ci><ci id="S3.E9.m1.2.2.2.2.cmml" xref="S3.E9.m1.2.2.2.2">𝑟</ci></list></apply><csymbol cd="latexml" id="S3.E9.m1.2.3.3.cmml" xref="S3.E9.m1.2.3.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m1.2c">\displaystyle P_{l,r}=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E9.m2.1" class="ltx_Math" alttext="\displaystyle W\left[{\begin{array}[]{c}X_{r}\\
\begin{array}[]{ccc}-&amp;\vec{x}_{l}&amp;-\end{array}\end{array}}\right]+\vec{b}_{o}" display="inline"><semantics id="S3.E9.m2.1a"><mrow id="S3.E9.m2.1.2" xref="S3.E9.m2.1.2.cmml"><mrow id="S3.E9.m2.1.2.2" xref="S3.E9.m2.1.2.2.cmml"><mi mathsize="90%" id="S3.E9.m2.1.2.2.2" xref="S3.E9.m2.1.2.2.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.E9.m2.1.2.2.1" xref="S3.E9.m2.1.2.2.1.cmml">​</mo><mrow id="S3.E9.m2.1.2.2.3.2" xref="S3.E9.m2.1.2.2.3.1.cmml"><mo id="S3.E9.m2.1.2.2.3.2.1" xref="S3.E9.m2.1.2.2.3.1.1.cmml">[</mo><mtable rowspacing="0pt" id="S3.E9.m2.1.1" xref="S3.E9.m2.1.1.cmml"><mtr id="S3.E9.m2.1.1a" xref="S3.E9.m2.1.1.cmml"><mtd id="S3.E9.m2.1.1b" xref="S3.E9.m2.1.1.cmml"><msub id="S3.E6.1.1" xref="S3.E6.1.1.cmml"><mi mathsize="90%" id="S3.E6.1.1.2" xref="S3.E6.1.1.2.cmml">X</mi><mi mathsize="90%" id="S3.E6.1.1.3" xref="S3.E6.1.1.3.cmml">r</mi></msub></mtd></mtr><mtr id="S3.E9.m2.1.1c" xref="S3.E9.m2.1.1.cmml"><mtd id="S3.E9.m2.1.1d" xref="S3.E9.m2.1.1.cmml"><mtable columnspacing="5pt" id="S3.E8.1.1" xref="S3.E8.1.1.cmml"><mtr id="S3.E8.1.1a" xref="S3.E8.1.1.cmml"><mtd id="S3.E8.1.1b" xref="S3.E8.1.1.cmml"><mo mathsize="90%" id="S3.E7.1.1" xref="S3.E7.1.1.cmml">−</mo></mtd><mtd id="S3.E8.1.1c" xref="S3.E8.1.1.cmml"><msub id="S3.E7.2.1" xref="S3.E7.2.1.cmml"><mover accent="true" id="S3.E7.2.1.2" xref="S3.E7.2.1.2.cmml"><mi mathsize="90%" id="S3.E7.2.1.2.2" xref="S3.E7.2.1.2.2.cmml">x</mi><mo mathsize="90%" stretchy="false" id="S3.E7.2.1.2.1" xref="S3.E7.2.1.2.1.cmml">→</mo></mover><mi mathsize="90%" id="S3.E7.2.1.3" xref="S3.E7.2.1.3.cmml">l</mi></msub></mtd><mtd id="S3.E8.1.1d" xref="S3.E8.1.1.cmml"><mo mathsize="90%" id="S3.E7.3.1" xref="S3.E7.3.1.cmml">−</mo></mtd></mtr></mtable></mtd></mtr></mtable><mo id="S3.E9.m2.1.2.2.3.2.2" xref="S3.E9.m2.1.2.2.3.1.1.cmml">]</mo></mrow></mrow><mo mathsize="90%" id="S3.E9.m2.1.2.1" xref="S3.E9.m2.1.2.1.cmml">+</mo><msub id="S3.E9.m2.1.2.3" xref="S3.E9.m2.1.2.3.cmml"><mover accent="true" id="S3.E9.m2.1.2.3.2" xref="S3.E9.m2.1.2.3.2.cmml"><mi mathsize="90%" id="S3.E9.m2.1.2.3.2.2" xref="S3.E9.m2.1.2.3.2.2.cmml">b</mi><mo mathsize="90%" stretchy="false" id="S3.E9.m2.1.2.3.2.1" xref="S3.E9.m2.1.2.3.2.1.cmml">→</mo></mover><mi mathsize="90%" id="S3.E9.m2.1.2.3.3" xref="S3.E9.m2.1.2.3.3.cmml">o</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m2.1b"><apply id="S3.E9.m2.1.2.cmml" xref="S3.E9.m2.1.2"><plus id="S3.E9.m2.1.2.1.cmml" xref="S3.E9.m2.1.2.1"></plus><apply id="S3.E9.m2.1.2.2.cmml" xref="S3.E9.m2.1.2.2"><times id="S3.E9.m2.1.2.2.1.cmml" xref="S3.E9.m2.1.2.2.1"></times><ci id="S3.E9.m2.1.2.2.2.cmml" xref="S3.E9.m2.1.2.2.2">𝑊</ci><apply id="S3.E9.m2.1.2.2.3.1.cmml" xref="S3.E9.m2.1.2.2.3.2"><csymbol cd="latexml" id="S3.E9.m2.1.2.2.3.1.1.cmml" xref="S3.E9.m2.1.2.2.3.2.1">delimited-[]</csymbol><matrix id="S3.E9.m2.1.1.cmml" xref="S3.E9.m2.1.1"><matrixrow id="S3.E9.m2.1.1a.cmml" xref="S3.E9.m2.1.1"><apply id="S3.E6.1.1.cmml" xref="S3.E6.1.1"><csymbol cd="ambiguous" id="S3.E6.1.1.1.cmml" xref="S3.E6.1.1">subscript</csymbol><ci id="S3.E6.1.1.2.cmml" xref="S3.E6.1.1.2">𝑋</ci><ci id="S3.E6.1.1.3.cmml" xref="S3.E6.1.1.3">𝑟</ci></apply></matrixrow><matrixrow id="S3.E9.m2.1.1b.cmml" xref="S3.E9.m2.1.1"><matrix id="S3.E8.1.1.cmml" xref="S3.E8.1.1"><matrixrow id="S3.E8.1.1a.cmml" xref="S3.E8.1.1"><minus id="S3.E7.1.1.cmml" xref="S3.E7.1.1"></minus><apply id="S3.E7.2.1.cmml" xref="S3.E7.2.1"><csymbol cd="ambiguous" id="S3.E7.2.1.1.cmml" xref="S3.E7.2.1">subscript</csymbol><apply id="S3.E7.2.1.2.cmml" xref="S3.E7.2.1.2"><ci id="S3.E7.2.1.2.1.cmml" xref="S3.E7.2.1.2.1">→</ci><ci id="S3.E7.2.1.2.2.cmml" xref="S3.E7.2.1.2.2">𝑥</ci></apply><ci id="S3.E7.2.1.3.cmml" xref="S3.E7.2.1.3">𝑙</ci></apply><minus id="S3.E7.3.1.cmml" xref="S3.E7.3.1"></minus></matrixrow></matrix></matrixrow></matrix></apply></apply><apply id="S3.E9.m2.1.2.3.cmml" xref="S3.E9.m2.1.2.3"><csymbol cd="ambiguous" id="S3.E9.m2.1.2.3.1.cmml" xref="S3.E9.m2.1.2.3">subscript</csymbol><apply id="S3.E9.m2.1.2.3.2.cmml" xref="S3.E9.m2.1.2.3.2"><ci id="S3.E9.m2.1.2.3.2.1.cmml" xref="S3.E9.m2.1.2.3.2.1">→</ci><ci id="S3.E9.m2.1.2.3.2.2.cmml" xref="S3.E9.m2.1.2.3.2.2">𝑏</ci></apply><ci id="S3.E9.m2.1.2.3.3.cmml" xref="S3.E9.m2.1.2.3.3">𝑜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m2.1c">\displaystyle W\left[{\begin{array}[]{c}X_{r}\\
\begin{array}[]{ccc}-&amp;\vec{x}_{l}&amp;-\end{array}\end{array}}\right]+\vec{b}_{o}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
<tbody id="S3.E10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E10.m1.1" class="ltx_Math" alttext="\displaystyle\vec{a}_{l}=" display="inline"><semantics id="S3.E10.m1.1a"><mrow id="S3.E10.m1.1.1" xref="S3.E10.m1.1.1.cmml"><msub id="S3.E10.m1.1.1.2" xref="S3.E10.m1.1.1.2.cmml"><mover accent="true" id="S3.E10.m1.1.1.2.2" xref="S3.E10.m1.1.1.2.2.cmml"><mi mathsize="90%" id="S3.E10.m1.1.1.2.2.2" xref="S3.E10.m1.1.1.2.2.2.cmml">a</mi><mo mathsize="90%" stretchy="false" id="S3.E10.m1.1.1.2.2.1" xref="S3.E10.m1.1.1.2.2.1.cmml">→</mo></mover><mi mathsize="90%" id="S3.E10.m1.1.1.2.3" xref="S3.E10.m1.1.1.2.3.cmml">l</mi></msub><mo mathsize="90%" id="S3.E10.m1.1.1.1" xref="S3.E10.m1.1.1.1.cmml">=</mo><mi id="S3.E10.m1.1.1.3" xref="S3.E10.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E10.m1.1b"><apply id="S3.E10.m1.1.1.cmml" xref="S3.E10.m1.1.1"><eq id="S3.E10.m1.1.1.1.cmml" xref="S3.E10.m1.1.1.1"></eq><apply id="S3.E10.m1.1.1.2.cmml" xref="S3.E10.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E10.m1.1.1.2.1.cmml" xref="S3.E10.m1.1.1.2">subscript</csymbol><apply id="S3.E10.m1.1.1.2.2.cmml" xref="S3.E10.m1.1.1.2.2"><ci id="S3.E10.m1.1.1.2.2.1.cmml" xref="S3.E10.m1.1.1.2.2.1">→</ci><ci id="S3.E10.m1.1.1.2.2.2.cmml" xref="S3.E10.m1.1.1.2.2.2">𝑎</ci></apply><ci id="S3.E10.m1.1.1.2.3.cmml" xref="S3.E10.m1.1.1.2.3">𝑙</ci></apply><csymbol cd="latexml" id="S3.E10.m1.1.1.3.cmml" xref="S3.E10.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E10.m1.1c">\displaystyle\vec{a}_{l}=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E10.m2.2" class="ltx_Math" alttext="\displaystyle P\vec{s}_{l,r}" display="inline"><semantics id="S3.E10.m2.2a"><mrow id="S3.E10.m2.2.3" xref="S3.E10.m2.2.3.cmml"><mi mathsize="90%" id="S3.E10.m2.2.3.2" xref="S3.E10.m2.2.3.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E10.m2.2.3.1" xref="S3.E10.m2.2.3.1.cmml">​</mo><msub id="S3.E10.m2.2.3.3" xref="S3.E10.m2.2.3.3.cmml"><mover accent="true" id="S3.E10.m2.2.3.3.2" xref="S3.E10.m2.2.3.3.2.cmml"><mi mathsize="90%" id="S3.E10.m2.2.3.3.2.2" xref="S3.E10.m2.2.3.3.2.2.cmml">s</mi><mo mathsize="90%" stretchy="false" id="S3.E10.m2.2.3.3.2.1" xref="S3.E10.m2.2.3.3.2.1.cmml">→</mo></mover><mrow id="S3.E10.m2.2.2.2.4" xref="S3.E10.m2.2.2.2.3.cmml"><mi mathsize="90%" id="S3.E10.m2.1.1.1.1" xref="S3.E10.m2.1.1.1.1.cmml">l</mi><mo mathsize="90%" id="S3.E10.m2.2.2.2.4.1" xref="S3.E10.m2.2.2.2.3.cmml">,</mo><mi mathsize="90%" id="S3.E10.m2.2.2.2.2" xref="S3.E10.m2.2.2.2.2.cmml">r</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.E10.m2.2b"><apply id="S3.E10.m2.2.3.cmml" xref="S3.E10.m2.2.3"><times id="S3.E10.m2.2.3.1.cmml" xref="S3.E10.m2.2.3.1"></times><ci id="S3.E10.m2.2.3.2.cmml" xref="S3.E10.m2.2.3.2">𝑃</ci><apply id="S3.E10.m2.2.3.3.cmml" xref="S3.E10.m2.2.3.3"><csymbol cd="ambiguous" id="S3.E10.m2.2.3.3.1.cmml" xref="S3.E10.m2.2.3.3">subscript</csymbol><apply id="S3.E10.m2.2.3.3.2.cmml" xref="S3.E10.m2.2.3.3.2"><ci id="S3.E10.m2.2.3.3.2.1.cmml" xref="S3.E10.m2.2.3.3.2.1">→</ci><ci id="S3.E10.m2.2.3.3.2.2.cmml" xref="S3.E10.m2.2.3.3.2.2">𝑠</ci></apply><list id="S3.E10.m2.2.2.2.3.cmml" xref="S3.E10.m2.2.2.2.4"><ci id="S3.E10.m2.1.1.1.1.cmml" xref="S3.E10.m2.1.1.1.1">𝑙</ci><ci id="S3.E10.m2.2.2.2.2.cmml" xref="S3.E10.m2.2.2.2.2">𝑟</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E10.m2.2c">\displaystyle P\vec{s}_{l,r}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">We also tried learning to predict a relevance score directly from concatenated vision and language features, rather than computing the dot product of the features in a latent embedded space. However,
the resulting model appeared to learn a salient region weighting
scheme that varied little with the language component. The
inner-product based relevance was the only formulation we tried that successfully takes account of both the query and the region information.</span></p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_typewriter ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.1.1.1" class="ltx_text ltx_font_serif">3.3</span> </span>Language Representation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">We represent our words with 300-dimensional word2vec
vectors </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.p1.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S3.SS3.p1.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S3.SS3.p1.1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> for their simplicity
and compact representation. We are also motivated by the ability of vector-based language representations to encode similar words with similar vectors, which may aid answering open-ended questions.
Using averages across word2vec vectors, we construct fixed-length
vectors for each question-answer pair, which our model then learns to
score. In our results section, we show that our vector-averaging language
model noticeably outperforms a more complex LSTM-based model from
 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.p1.1.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S3.SS3.p1.1.6.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S3.SS3.p1.1.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, demonstrating that BOW-like models provide very effective
and simple language representations for VQA tasks.</span></p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">We first tried separately averaging vectors for each word with the
question and answer, concatenating them to yield a 600-dimensional
vector, but since the word2vec representation is not sparse, averaging
several words may muddle the representation. We improve the
representation using the Stanford Parser </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.p2.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S3.SS3.p2.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S3.SS3.p2.1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> to bin the question into
additional separate semantic bins. The bins are defined as follows:</span></p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Bin 1</span><span id="S3.SS3.p3.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> captures the type of question by averaging
the word2vec representation of the first two words. For example, ‘‘How many’’ tends to require a numerical answer,
while ‘‘Is there’’ requires a yes or no answer.</span>
<br class="ltx_break"><span id="S3.SS3.p3.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">Bin 2</span><span id="S3.SS3.p3.1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> contains the nominal subject to encode subject of question.</span>
<br class="ltx_break"><span id="S3.SS3.p3.1.5" class="ltx_text ltx_font_bold" style="font-size:90%;">Bin 3</span><span id="S3.SS3.p3.1.6" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> contains the average of all other noun words.</span>
<br class="ltx_break"><span id="S3.SS3.p3.1.7" class="ltx_text ltx_font_bold" style="font-size:90%;">Bin 4</span><span id="S3.SS3.p3.1.8" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> contains the average of all remaining words,
excluding determiners such as ‘‘a,’’ ‘‘the,’’ and ‘‘few.’’</span></p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p"><span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Each bin then contains a 300-dimensional representation, which are
concatenated with a bin for the words in the candidate answer to yield
a 1500-dimensional question/answer representation. Figure </span><a href="#S3.F4" title="Figure 4 ‣ 3.3 Language Representation ‣ 3 Approach ‣ Where To Look: Focus Regions for Visual Question Answering" class="ltx_ref ltx_font_typewriter" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S3.SS3.p4.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> shows examples of binning for the parsed question. This representation separates out important components of a variable-length question while maintaining a fixed-length representation that simplifies the network architecture.</span></p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/1511.07394/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="161" height="101" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.3.1.1" class="ltx_text ltx_font_serif">Figure 4</span>: </span>Example parse-based binning of questions. Each bin is represented with the average of the word2vec vectors of its members. Empty bins are represented with a zero-vector.</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_font_typewriter ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.1.1.1" class="ltx_text ltx_font_serif">3.4</span> </span>Image Features</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p"><span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">The image features are fed directly into the region-selection
layer from a pre-trained network. We first select candidate regions
by extracting the top-ranked 99 Edge Boxes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS4.p1.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S3.SS4.p1.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S3.SS4.p1.1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> from
the image after performing non-max suppression with a 0.2 intersection
over union overlap criterion. We found this aggressive non-max
suppression to be important for selecting smaller regions that may be
important for some questions, as the top-ranked regions tend to be
highly overlapping large regions. Finally, a whole-image region is also added to
ensure that the model at least has the spatial support of the full
frame if necessary, bringing the total number of candidate regions to
100 per image. While we have not experimented with the number of
regions, it is possible that the improved recall from additional regions
may improve performance.</span></p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p"><span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">We extract features using the VGG-s network </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS4.p2.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S3.SS4.p2.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S3.SS4.p2.1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, concatenating the output
from the last fully connected layer (4096 dimensions) and the
pre-softmax layer (1000 dimensions) to get a 5096 dimensional feature
per region. The pre-softmax classification layer was included to
provide a more direct signal for objects from the Imagenet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS4.p2.1.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S3.SS4.p2.1.6.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S3.SS4.p2.1.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> classification task.</span></p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_font_typewriter ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.1.1.1" class="ltx_text ltx_font_serif">3.5</span> </span>Training</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p"><span id="S3.SS5.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Our overall network architecture is multi-layer feed-forward network
as seen in Fig. </span><a href="#S3.F3" title="Figure 3 ‣ 3 Approach ‣ Where To Look: Focus Regions for Visual Question Answering" class="ltx_ref ltx_font_typewriter" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.SS5.p1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, implemented in
MatConvNet</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS5.p1.1.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S3.SS5.p1.1.4.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S3.SS5.p1.1.5" class="ltx_text ltx_font_typewriter" style="font-size:90%;">. Our fully connected layers are
initialized with Xavier initialization (</span><math id="S3.SS5.p1.1.m1.1" class="ltx_Math" alttext="\frac{1}{\sqrt{n_{in}}}" display="inline"><semantics id="S3.SS5.p1.1.m1.1a"><mfrac id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml"><mn mathsize="90%" id="S3.SS5.p1.1.m1.1.1.2" xref="S3.SS5.p1.1.m1.1.1.2.cmml">1</mn><msqrt id="S3.SS5.p1.1.m1.1.1.3" xref="S3.SS5.p1.1.m1.1.1.3.cmml"><msub id="S3.SS5.p1.1.m1.1.1.3.2" xref="S3.SS5.p1.1.m1.1.1.3.2.cmml"><mi mathsize="90%" id="S3.SS5.p1.1.m1.1.1.3.2.2" xref="S3.SS5.p1.1.m1.1.1.3.2.2.cmml">n</mi><mrow id="S3.SS5.p1.1.m1.1.1.3.2.3" xref="S3.SS5.p1.1.m1.1.1.3.2.3.cmml"><mi mathsize="90%" id="S3.SS5.p1.1.m1.1.1.3.2.3.2" xref="S3.SS5.p1.1.m1.1.1.3.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.1.m1.1.1.3.2.3.1" xref="S3.SS5.p1.1.m1.1.1.3.2.3.1.cmml">​</mo><mi mathsize="90%" id="S3.SS5.p1.1.m1.1.1.3.2.3.3" xref="S3.SS5.p1.1.m1.1.1.3.2.3.3.cmml">n</mi></mrow></msub></msqrt></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><apply id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1"><divide id="S3.SS5.p1.1.m1.1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1"></divide><cn type="integer" id="S3.SS5.p1.1.m1.1.1.2.cmml" xref="S3.SS5.p1.1.m1.1.1.2">1</cn><apply id="S3.SS5.p1.1.m1.1.1.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3"><root id="S3.SS5.p1.1.m1.1.1.3a.cmml" xref="S3.SS5.p1.1.m1.1.1.3"></root><apply id="S3.SS5.p1.1.m1.1.1.3.2.cmml" xref="S3.SS5.p1.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS5.p1.1.m1.1.1.3.2.1.cmml" xref="S3.SS5.p1.1.m1.1.1.3.2">subscript</csymbol><ci id="S3.SS5.p1.1.m1.1.1.3.2.2.cmml" xref="S3.SS5.p1.1.m1.1.1.3.2.2">𝑛</ci><apply id="S3.SS5.p1.1.m1.1.1.3.2.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3.2.3"><times id="S3.SS5.p1.1.m1.1.1.3.2.3.1.cmml" xref="S3.SS5.p1.1.m1.1.1.3.2.3.1"></times><ci id="S3.SS5.p1.1.m1.1.1.3.2.3.2.cmml" xref="S3.SS5.p1.1.m1.1.1.3.2.3.2">𝑖</ci><ci id="S3.SS5.p1.1.m1.1.1.3.2.3.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3.2.3.3">𝑛</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">\frac{1}{\sqrt{n_{in}}}</annotation></semantics></math><span id="S3.SS5.p1.1.6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS5.p1.1.7.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S3.SS5.p1.1.8.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S3.SS5.p1.1.9" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> and
separated with a batch-normalization </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS5.p1.1.10.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S3.SS5.p1.1.11.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S3.SS5.p1.1.12" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> and relu
layer </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS5.p1.1.13.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S3.SS5.p1.1.14.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S3.SS5.p1.1.15" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> between each. The word2vec text features
are fed into the network’s input layer, whereas the image region features
feed in through the region selection layer.</span></p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.3" class="ltx_p"><span id="S3.SS5.p2.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Our network sizes are set as follows. The 1500 dimensional language
features first pass through 3 fully connected layers with output
dimensions 2048, 1500, and 1024 respectively. The embedded language features are then passed through the region selection layer to be combined with the vision
features. Inside the region selection layer, projections </span><math id="S3.SS5.p2.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS5.p2.1.m1.1a"><mi mathsize="90%" id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><ci id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">A</annotation></semantics></math><span id="S3.SS5.p2.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> and </span><math id="S3.SS5.p2.2.m2.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS5.p2.2.m2.1a"><mi mathsize="90%" id="S3.SS5.p2.2.m2.1.1" xref="S3.SS5.p2.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><ci id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">B</annotation></semantics></math><span id="S3.SS5.p2.3.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
project both vision and language representations down to 900
dimensions before computing their inner product. The exiting feature
representation passes through </span><math id="S3.SS5.p2.3.m3.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS5.p2.3.m3.1a"><mi mathsize="90%" id="S3.SS5.p2.3.m3.1.1" xref="S3.SS5.p2.3.m3.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.3.m3.1b"><ci id="S3.SS5.p2.3.m3.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.3.m3.1c">W</annotation></semantics></math><span id="S3.SS5.p2.3.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> with an output dimension of
2048. then finally through two more fully connected layers with output
dimensions of 900 and 1 where the output scalar is the
question-answer pair score.</span></p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.4" class="ltx_p"><span id="S3.SS5.p3.4.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">It is necessary to pay extra attention to the initialization of the
region-selection layer. The magnitude of the projection matrices </span><math id="S3.SS5.p3.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS5.p3.1.m1.1a"><mi mathsize="90%" id="S3.SS5.p3.1.m1.1.1" xref="S3.SS5.p3.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.1.m1.1b"><ci id="S3.SS5.p3.1.m1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.1.m1.1c">A</annotation></semantics></math><span id="S3.SS5.p3.4.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">,
</span><math id="S3.SS5.p3.2.m2.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS5.p3.2.m2.1a"><mi mathsize="90%" id="S3.SS5.p3.2.m2.1.1" xref="S3.SS5.p3.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.2.m2.1b"><ci id="S3.SS5.p3.2.m2.1.1.cmml" xref="S3.SS5.p3.2.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.2.m2.1c">B</annotation></semantics></math><span id="S3.SS5.p3.4.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> and </span><math id="S3.SS5.p3.3.m3.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS5.p3.3.m3.1a"><mi mathsize="90%" id="S3.SS5.p3.3.m3.1.1" xref="S3.SS5.p3.3.m3.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.3.m3.1b"><ci id="S3.SS5.p3.3.m3.1.1.cmml" xref="S3.SS5.p3.3.m3.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.3.m3.1c">W</annotation></semantics></math><span id="S3.SS5.p3.4.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> are initialized to </span><math id="S3.SS5.p3.4.m4.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S3.SS5.p3.4.m4.1a"><mn mathsize="90%" id="S3.SS5.p3.4.m4.1.1" xref="S3.SS5.p3.4.m4.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.4.m4.1b"><cn type="float" id="S3.SS5.p3.4.m4.1.1.cmml" xref="S3.SS5.p3.4.m4.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.4.m4.1c">0.001</annotation></semantics></math><span id="S3.SS5.p3.4.5" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> times the standard normal
distribution. We found that low initial values were important to
prevent the softmax in selection from spiking too early and to prevent
the higher-dimensional vision component from dominating early in the training.</span></p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_font_typewriter ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section"><span id="S4.1.1.1" class="ltx_text ltx_font_serif">4</span> </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">We evaluate the effects of our region-selection layer on the
multiple-choice format of the MS COCO Visual Question Answering (VQA)
dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.p1.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S4.p1.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S4.p1.1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">. This dataset contains 82,783 images for training,
40,504 for validation, and 81,434 for testing. Each image has 3
corresponding questions with recorded free-response answers from
10 annotators. Any response that comes from at least 3 annotators is
considered correct. We use the 18-way multiple choice task because its
evaluation is much less ambiguous than the open-ended response task,
though our method could be applied to the latter by treating the most
common or likely K responses as a large K-way multiple
choice task. We trained using only the training set, with 10% set
aside for model selection and parameter tuning. We perform detailed
evaluation on the validation set and further comparison on the test
set using the provided submission tools.</span></p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">We evaluate and analyze how much our region-weighting improves accuracy compared to using the whole image or only language (Tables </span><a href="#S4.T1" title="Table 1 ‣ 4.1 Comparisons between region, image, and language-only models ‣ 4 Experiments ‣ Where To Look: Focus Regions for Visual Question Answering" class="ltx_ref ltx_font_typewriter" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.p2.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, </span><a href="#S4.T2" title="Table 2 ‣ 4.1 Comparisons between region, image, and language-only models ‣ 4 Experiments ‣ Where To Look: Focus Regions for Visual Question Answering" class="ltx_ref ltx_font_typewriter" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S4.p2.1.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, </span><a href="#S4.T3" title="Table 3 ‣ 4.1 Comparisons between region, image, and language-only models ‣ 4 Experiments ‣ Where To Look: Focus Regions for Visual Question Answering" class="ltx_ref ltx_font_typewriter" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S4.p2.1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">) and show examples in Figure </span><a href="#S4.F8" title="Figure 8 ‣ 4.1 Comparisons between region, image, and language-only models ‣ 4 Experiments ‣ Where To Look: Focus Regions for Visual Question Answering" class="ltx_ref ltx_font_typewriter" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">8</span></a><span id="S4.p2.1.5" class="ltx_text ltx_font_typewriter" style="font-size:90%;">. We also perform a simple evaluation on a subset of images showing that relevant regions tend to have higher than average weights (Fig. </span><a href="#S4.F6" title="Figure 6 ‣ 4.1 Comparisons between region, image, and language-only models ‣ 4 Experiments ‣ Where To Look: Focus Regions for Visual Question Answering" class="ltx_ref ltx_font_typewriter" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">6</span></a><span id="S4.p2.1.6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">). We also show the advantage of our language model over other schemes (Table </span><a href="#S4.T4" title="Table 4 ‣ 4.2 Region Evaluation ‣ 4 Experiments ‣ Where To Look: Focus Regions for Visual Question Answering" class="ltx_ref ltx_font_typewriter" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S4.p2.1.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;">).</span></p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_typewriter ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.1.1.1" class="ltx_text ltx_font_serif">4.1</span> </span>Comparisons between region, image, and language-only models</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">We compare our region selection model with several baseline methods, described below.</span></p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Language-only</span><span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">: We train a network to score each answer purely from the language representation. This provides a baseline to demonstrate improvement due to image features, rather than just good guesses.</span></p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Word+Whole image</span><span id="S4.SS1.p3.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">: We concatenate CNN features computed over the entire image with the language features and score them using a 3-layer neural network, essentially replacing the region-selection layer with features computed over the whole image.</span></p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Word+Uniform averaged region features</span><span id="S4.SS1.p4.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">: To test that region weighting is important, we also try uniformly averaging features across all regions as the image representation and train as above.</span></p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Model</span></th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T1.1.1.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Overall (%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.2.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Language Only</span></th>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.2.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">53.98</span></td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<th id="S4.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.1.3.2.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Word+Whole Image</span></th>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.3.2.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">57.83</span></td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<th id="S4.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.1.4.3.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Word+ave. reg.</span></th>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.4.3.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">57.88</span></td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<th id="S4.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.1.5.4.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Word+Region Sel.</span></th>
<td id="S4.T1.1.5.4.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.5.4.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">58.94</span></td>
</tr>
<tr id="S4.T1.1.6.5" class="ltx_tr">
<th id="S4.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S4.T1.1.6.5.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">LSTM Q+I </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.6.5.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S4.T1.1.6.5.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.6.5.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">53.96</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.4.1.1" class="ltx_text ltx_font_serif">Table 1</span>: </span>Overall accuracy comparison on Validation. Our region
selection model outperforms our own baselines, demonstrating the
benefits of selective region weighting.</figcaption>
</figure>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p"><span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Table </span><a href="#S4.T1" title="Table 1 ‣ 4.1 Comparisons between region, image, and language-only models ‣ 4 Experiments ‣ Where To Look: Focus Regions for Visual Question Answering" class="ltx_ref ltx_font_typewriter" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.SS1.p5.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> shows the comparison of overall accuracy
on the validation set. Our proposed region-selection model outperforms all other models. Also, we can see that uniform weighting of regions is not helpful. We also include the
best-performing LSTM question+image model from the authors of the VQA
dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p5.1.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S4.SS1.p5.1.4.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S4.SS1.p5.1.5" class="ltx_text ltx_font_typewriter" style="font-size:90%;">. This model significantly underperforms even our much simpler baselines, which could be partly because the model was designed for open-ended answering and adapted for multiple choice.</span></p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p"><span id="S4.SS1.p6.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">We also evaluate our model on the test-dev and test-standard partitions in order to compare with
additional models from </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p6.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S4.SS1.p6.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S4.SS1.p6.1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">. In Table </span><a href="#S4.T2" title="Table 2 ‣ 4.1 Comparisons between region, image, and language-only models ‣ 4 Experiments ‣ Where To Look: Focus Regions for Visual Question Answering" class="ltx_ref ltx_font_typewriter" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S4.SS1.p6.1.5" class="ltx_text ltx_font_typewriter" style="font-size:90%;">,
we include comparisons to the best-performing question+image based
models from the VQA dataset paper </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p6.1.6.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S4.SS1.p6.1.7.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S4.SS1.p6.1.8" class="ltx_text ltx_font_typewriter" style="font-size:90%;">. Our model was retrained on train+val data, using a 10% held-out set from the train set for model selection. Note that our model significantly outperforms the baselines in the ‘‘others’’ category, which contains the majority of the question types that our model excels at.</span></p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p"><span id="S4.SS1.p7.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Table </span><a href="#S4.T3" title="Table 3 ‣ 4.1 Comparisons between region, image, and language-only models ‣ 4 Experiments ‣ Where To Look: Focus Regions for Visual Question Answering" class="ltx_ref ltx_font_typewriter" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S4.SS1.p7.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> offers a more detailed performance summary
across various question types, with discussion in the caption.
Figure </span><a href="#S4.F8" title="Figure 8 ‣ 4.1 Comparisons between region, image, and language-only models ‣ 4 Experiments ‣ Where To Look: Focus Regions for Visual Question Answering" class="ltx_ref ltx_font_typewriter" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">8</span></a><span id="S4.SS1.p7.1.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> shows a qualitative comparison of
results, highlighting some of the strengths and remaining problems of
our approach. These visualizations are created by soft masking the
image by with a mask created by summing the weights of each region and
normalizing to a max of one. A small blurring filter is applied to
the soft mask before normalization to remove distracting artifacts that occur from multiple overlapping rectangles.
On color questions, localization of the mentioned object tends to be very good, which leads to much more accurate answering. On some questions, such as ‘‘How many birds are in the sky?’’ the system cannot produce the correct answer but does focus on the relevant objects. The third row shows examples of how different questions lead to different focus regions. Notice how the model identifies the room as a bathroom in the third row by focusing on the toilet, and, when confirming that ‘‘kite’’ is the answer to ‘‘What is the woman flying over the beach?’’ focuses on the kite, not the woman or the beach.</span></p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/1511.07394/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_img_landscape" width="438" height="226" alt="Refer to caption">
<figcaption class="ltx_caption ltx_font_typewriter" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.3.1.1" class="ltx_text ltx_font_serif">Figure 5</span>: </span>Comparison of attention regions generated by various
question-answer pairings for the same question. Each visualization
is labeled with its corresponding answer choice and returned
confidence. We show the highlighted regions for the top multiple
choice answers and some unrelated ones. Notice that in the first
example, while the model clearly identified a green region within
the image to match the ‘‘green’’ option, the corresponding
confidence was significantly lower than that of the correct
options, showing that the model does more than just match answer
choices with image regions.</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Model</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">All</span></th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S4.T2.1.1.1.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Y/N</span></th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S4.T2.1.1.1.4.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Num.</span></th>
<th id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.1.1.1.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Others</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T2.1.2.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Word+Region Sel.</span></th>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">62.44</span></td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.2.1.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">77.62</span></td>
<td id="S4.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.2.1.4.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">34.28</span></td>
<td id="S4.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">55.84</span></td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<th id="S4.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.3.2.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">LSTM Q+I </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.3.2.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S4.T2.1.3.2.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.3.2.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">57.17</span></td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.3.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">78.95</span></td>
<td id="S4.T2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.3.2.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">35.80</span></td>
<td id="S4.T2.1.3.2.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.3.2.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">43.41</span></td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<th id="S4.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.4.3.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Q+I </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.4.3.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S4.T2.1.4.3.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.4.3.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">58.97</span></td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.4.3.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">75.97</span></td>
<td id="S4.T2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.4.3.4.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">34.35</span></td>
<td id="S4.T2.1.4.3.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.4.3.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">50.33</span></td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<th id="S4.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T2.1.5.4.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Word+Region Sel.</span></th>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.5.4.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">62.43</span></td>
<td id="S4.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.5.4.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">77.18</span></td>
<td id="S4.T2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.5.4.4.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">33.52</span></td>
<td id="S4.T2.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.5.4.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">56.09</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.7.1.1" class="ltx_text ltx_font_serif">Table 2</span>: </span>Accuracy comparison on Test-dev (top) and Test-standard (bottom). Our model
outperforms the best performing image and text models from  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</figcaption>
</figure>
<div id="S4.SS1.p8" class="ltx_para">
<p id="S4.SS1.p8.1" class="ltx_p"><span id="S4.SS1.p8.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">In Figure </span><a href="#S4.F5" title="Figure 5 ‣ 4.1 Comparisons between region, image, and language-only models ‣ 4 Experiments ‣ Where To Look: Focus Regions for Visual Question Answering" class="ltx_ref ltx_font_typewriter" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S4.SS1.p8.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, we show additional qualitative examples
of how the region selection varies with question-answer pairs. In the
first row, we see the model does more than simply match answer choices
to regions. While it does find a matching green region, the
corresponding confidence is still low. In addition, we see that
irrelevant answer choices tend to have less-focused attention
weightings. For example, the kitchen recognition question has most of
its weighting on what appears to be a discriminative kitchen patch for
the correct choice, whereas the ‘‘blue’’ choice appears to have a more
evenly spread out weighting.</span></p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/1511.07394/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="82" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.3.1.1" class="ltx_text ltx_font_serif">Figure 6</span>: </span>Example image with corresponding region weighting. Red
boxes correspond to manual annotation of regions relevant to the
question: ‘‘Are the people real?’’</figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure"><svg id="S4.F7.pic1" class="ltx_picture ltx_centering" height="218.87" overflow="visible" version="1.1" width="244.19"><g transform="translate(0,218.87) matrix(1 0 0 -1 0 0) translate(21.03,0) translate(0,38.87) matrix(1.0 0.0 0.0 1.0 -21.03 -38.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(38.67,0) translate(0,38.87)"><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M 0 -5.91 L 0 0 M 32.06 -5.91 L 32.06 0 M 64.13 -5.91 L 64.13 0 M 96.19 -5.91 L 96.19 0 M 128.25 -5.91 L 128.25 0 M 160.32 -5.91 L 160.32 0 M 192.38 -5.91 L 192.38 0 M 0 179.87 L 0 173.96 M 32.06 179.87 L 32.06 173.96 M 64.13 179.87 L 64.13 173.96 M 96.19 179.87 L 96.19 173.96 M 128.25 179.87 L 128.25 173.96 M 160.32 179.87 L 160.32 173.96 M 192.38 179.87 L 192.38 173.96" style="fill:none"></path></g><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M -17.64 0 L -11.73 0 M -17.64 46.51 L -11.73 46.51 M -17.64 93.03 L -11.73 93.03 M -17.64 139.54 L -11.73 139.54 M 193.98 0 L 188.08 0 M 193.98 46.51 L 188.08 46.51 M 193.98 93.03 L 188.08 93.03 M 193.98 139.54 L 188.08 139.54" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="0.4pt"><path d="M -17.64 0 L -17.64 173.96 L 193.98 173.96 L 193.98 0 L -17.64 0 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -12.45 -18.36)" fill="#000000" stroke="#000000"><foreignObject width="24.91" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="-0.4" display="inline"><semantics id="S4.F7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mrow id="S4.F7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mo mathsize="90%" id="S4.F7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1a" xref="S4.F7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">−</mo><mn mathsize="90%" id="S4.F7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S4.F7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">0.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S4.F7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1"><minus id="S4.F7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.F7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1"></minus><cn type="float" id="S4.F7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.F7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">-0.4</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 19.61 -18.36)" fill="#000000" stroke="#000000"><foreignObject width="24.91" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F7.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="-0.2" display="inline"><semantics id="S4.F7.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mrow id="S4.F7.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F7.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mo mathsize="90%" id="S4.F7.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1a" xref="S4.F7.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">−</mo><mn mathsize="90%" id="S4.F7.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S4.F7.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F7.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S4.F7.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F7.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1"><minus id="S4.F7.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.F7.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1"></minus><cn type="float" id="S4.F7.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.F7.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.2">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">-0.2</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 61.01 -18.36)" fill="#000000" stroke="#000000"><foreignObject width="6.23" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F7.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S4.F7.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="90%" id="S4.F7.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F7.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.F7.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F7.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F7.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">0</cn></annotation-xml></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 87.2 -18.36)" fill="#000000" stroke="#000000"><foreignObject width="17.99" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F7.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0.2" display="inline"><semantics id="S4.F7.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="90%" id="S4.F7.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F7.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.2</mn><annotation-xml encoding="MathML-Content" id="S4.F7.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F7.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F7.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">0.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">0.2</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 119.26 -18.36)" fill="#000000" stroke="#000000"><foreignObject width="17.99" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F7.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0.4" display="inline"><semantics id="S4.F7.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="90%" id="S4.F7.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F7.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.4</mn><annotation-xml encoding="MathML-Content" id="S4.F7.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F7.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F7.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1">0.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1c">0.4</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 151.32 -18.36)" fill="#000000" stroke="#000000"><foreignObject width="17.99" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F7.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0.6" display="inline"><semantics id="S4.F7.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="90%" id="S4.F7.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F7.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.6</mn><annotation-xml encoding="MathML-Content" id="S4.F7.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F7.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F7.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1">0.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1c">0.6</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 183.38 -18.36)" fill="#000000" stroke="#000000"><foreignObject width="17.99" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F7.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0.8" display="inline"><semantics id="S4.F7.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="90%" id="S4.F7.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F7.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S4.F7.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S4.F7.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F7.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1c">0.8</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -28.29 -4.01)" fill="#000000" stroke="#000000"><foreignObject width="6.23" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F7.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S4.F7.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="90%" id="S4.F7.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F7.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.F7.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F7.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F7.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1">0</cn></annotation-xml></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -34.52 42.5)" fill="#000000" stroke="#000000"><foreignObject width="12.45" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F7.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.F7.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="90%" id="S4.F7.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F7.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.F7.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F7.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F7.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1c">20</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -34.52 89.01)" fill="#000000" stroke="#000000"><foreignObject width="12.45" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F7.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S4.F7.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="90%" id="S4.F7.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F7.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S4.F7.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F7.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F7.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1c">40</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -34.52 135.53)" fill="#000000" stroke="#000000"><foreignObject width="12.45" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F7.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S4.F7.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="90%" id="S4.F7.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F7.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S4.F7.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F7.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F7.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1c">60</annotation></semantics></math></foreignObject></g><clipPath id="pgfcp1"><path d="M -17.64 0 L 193.98 0 L 193.98 173.96 L -17.64 173.96 Z"></path></clipPath><g clip-path="url(#pgfcp1)"><g stroke="#0000FF" fill="#B3B3FF" color="#0000FF"><path d="M 0 0 h 16.03 v 0 h -16.03 Z M 16.03 0 h 16.03 v 6.98 h -16.03 Z M 32.06 0 h 16.03 v 18.61 h -16.03 Z M 48.09 0 h 16.03 v 106.98 h -16.03 Z M 64.13 0 h 16.03 v 158.14 h -16.03 Z M 80.16 0 h 16.03 v 109.31 h -16.03 Z M 96.19 0 h 16.03 v 39.54 h -16.03 Z M 112.22 0 h 16.03 v 13.95 h -16.03 Z M 128.25 0 h 16.03 v 20.93 h -16.03 Z M 144.28 0 h 16.03 v 2.33 h -16.03 Z M 160.32 0 h 16.03 v 0 h -16.03 Z"></path></g><g></g></g><g transform="matrix(1.0 0.0 0.0 1.0 74.3 -32.3)" fill="#000000" stroke="#000000"><foreignObject width="27.74" height="7.78" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mu_{\mbox{in}}-\mu" display="inline"><semantics id="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1a"><mrow id="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><msub id="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml"><mi mathsize="90%" id="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.2" xref="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.2.cmml">μ</mi><mtext mathsize="90%" id="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.3" xref="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.3a.cmml">in</mtext></msub><mo mathsize="90%" id="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.1" xref="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml">−</mo><mi mathsize="90%" id="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">μ</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1"><minus id="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.1"></minus><apply id="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.1.cmml" xref="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2">subscript</csymbol><ci id="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.2.cmml" xref="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.2">𝜇</ci><ci id="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.3a.cmml" xref="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.3"><mtext mathsize="63%" id="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.3.cmml" xref="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.3">in</mtext></ci></apply><ci id="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.3">𝜇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1c">\mu_{\mbox{in}}-\mu</annotation></semantics></math></foreignObject></g></g></g></g></svg>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.9.1.1" class="ltx_text ltx_font_serif">Figure 7</span>: </span>Histogram of differences between mean pixel weight within
(<math id="S4.F7.3.m1.1" class="ltx_Math" alttext="\mu_{\mbox{in}}" display="inline"><semantics id="S4.F7.3.m1.1b"><msub id="S4.F7.3.m1.1.1" xref="S4.F7.3.m1.1.1.cmml"><mi id="S4.F7.3.m1.1.1.2" xref="S4.F7.3.m1.1.1.2.cmml">μ</mi><mtext class="ltx_mathvariant_monospace" id="S4.F7.3.m1.1.1.3" xref="S4.F7.3.m1.1.1.3a.cmml">in</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.F7.3.m1.1c"><apply id="S4.F7.3.m1.1.1.cmml" xref="S4.F7.3.m1.1.1"><csymbol cd="ambiguous" id="S4.F7.3.m1.1.1.1.cmml" xref="S4.F7.3.m1.1.1">subscript</csymbol><ci id="S4.F7.3.m1.1.1.2.cmml" xref="S4.F7.3.m1.1.1.2">𝜇</ci><ci id="S4.F7.3.m1.1.1.3a.cmml" xref="S4.F7.3.m1.1.1.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.F7.3.m1.1.1.3.cmml" xref="S4.F7.3.m1.1.1.3">in</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.3.m1.1d">\mu_{\mbox{in}}</annotation></semantics></math>) annotated regions and across the whole image (<math id="S4.F7.4.m2.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S4.F7.4.m2.1b"><mi id="S4.F7.4.m2.1.1" xref="S4.F7.4.m2.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S4.F7.4.m2.1c"><ci id="S4.F7.4.m2.1.1.cmml" xref="S4.F7.4.m2.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.4.m2.1d">\mu</annotation></semantics></math>). Pixel
weights are normalized by the maximum pixel weight. More weight is usually assigned to the relevant region: often much more and very rarely much less.
</figcaption>
</figure>
<figure id="S4.F8" class="ltx_figure"><img src="/html/1511.07394/assets/x7.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="397" height="542" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.3.1.1" class="ltx_text ltx_font_serif">Figure 8</span>: </span>Comparison of qualitative results from Val. The larger image shows the
selection weights overlayed on the original image (smaller). L:
Word only model; I: Word+Whole Image; R: Region Selection. The
scores shown are ground truth confidence - top incorrect. Note
that the first row shows successful examples in which tight region
localization allowed for an accurate color detection. In the third
row, we show examples of how weighting varies on the same image due
to differing language components. </figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.5.1.1" class="ltx_tr">
<th id="S4.T3.5.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S4.T3.5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S4.T3.5.1.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">region</span></th>
<th id="S4.T3.5.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S4.T3.5.1.1.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">image</span></th>
<th id="S4.T3.5.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S4.T3.5.1.1.4.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">text</span></th>
<th id="S4.T3.5.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T3.5.1.1.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">freq</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.5.2.1" class="ltx_tr">
<th id="S4.T3.5.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T3.5.2.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">overall</span></th>
<td id="S4.T3.5.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.5.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">58.94</span></td>
<td id="S4.T3.5.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.5.2.1.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">57.83</span></td>
<td id="S4.T3.5.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.5.2.1.4.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">53.98</span></td>
<td id="S4.T3.5.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.5.2.1.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">100.0%</span></td>
</tr>
<tr id="S4.T3.5.3.2" class="ltx_tr">
<th id="S4.T3.5.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.3.2.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">is/are/was</span></th>
<td id="S4.T3.5.3.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.3.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">75.42</span></td>
<td id="S4.T3.5.3.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.3.2.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">74.63</span></td>
<td id="S4.T3.5.3.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.3.2.4.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">75.00</span></td>
<td id="S4.T3.5.3.2.5" class="ltx_td ltx_align_center"><span id="S4.T3.5.3.2.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">33.3%</span></td>
</tr>
<tr id="S4.T3.5.4.3" class="ltx_tr">
<th id="S4.T3.5.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.4.3.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">identify: what</span></th>
<td id="S4.T3.5.4.3.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.4.3.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">52.89</span></td>
<td id="S4.T3.5.4.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.4.3.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">52.10</span></td>
<td id="S4.T3.5.4.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.4.3.4.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">45.11</span></td>
<td id="S4.T3.5.4.3.5" class="ltx_td ltx_align_center"><span id="S4.T3.5.4.3.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">23.8%</span></td>
</tr>
<tr id="S4.T3.5.5.4" class="ltx_tr">
<th id="S4.T3.5.5.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.5.4.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">kind/type/animal</span></th>
<td id="S4.T3.5.5.4.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.5.5.4.3" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.5.5.4.4" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.5.5.4.5" class="ltx_td"></td>
</tr>
<tr id="S4.T3.5.6.5" class="ltx_tr">
<th id="S4.T3.5.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.6.5.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">how many</span></th>
<td id="S4.T3.5.6.5.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.6.5.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">33.38</span></td>
<td id="S4.T3.5.6.5.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.6.5.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">36.84</span></td>
<td id="S4.T3.5.6.5.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.6.5.4.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">34.05</span></td>
<td id="S4.T3.5.6.5.5" class="ltx_td ltx_align_center"><span id="S4.T3.5.6.5.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">10.3%</span></td>
</tr>
<tr id="S4.T3.5.7.6" class="ltx_tr">
<th id="S4.T3.5.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.7.6.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">what color</span></th>
<td id="S4.T3.5.7.6.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.7.6.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">53.96</span></td>
<td id="S4.T3.5.7.6.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.7.6.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">43.52</span></td>
<td id="S4.T3.5.7.6.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.7.6.4.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">32.59</span></td>
<td id="S4.T3.5.7.6.5" class="ltx_td ltx_align_center"><span id="S4.T3.5.7.6.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">9.8%</span></td>
</tr>
<tr id="S4.T3.5.8.7" class="ltx_tr">
<th id="S4.T3.5.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.8.7.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">interpret:</span></th>
<td id="S4.T3.5.8.7.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.8.7.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">75.73</span></td>
<td id="S4.T3.5.8.7.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.8.7.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">74.43</span></td>
<td id="S4.T3.5.8.7.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.8.7.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">75.73</span></td>
<td id="S4.T3.5.8.7.5" class="ltx_td ltx_align_center"><span id="S4.T3.5.8.7.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">4.6%</span></td>
</tr>
<tr id="S4.T3.5.9.8" class="ltx_tr">
<th id="S4.T3.5.9.8.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.9.8.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">can/could/does/has</span></th>
<td id="S4.T3.5.9.8.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.5.9.8.3" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.5.9.8.4" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.5.9.8.5" class="ltx_td"></td>
</tr>
<tr id="S4.T3.5.10.9" class="ltx_tr">
<th id="S4.T3.5.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.10.9.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">none of the above</span></th>
<td id="S4.T3.5.10.9.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.10.9.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">45.40</span></td>
<td id="S4.T3.5.10.9.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.10.9.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">44.04</span></td>
<td id="S4.T3.5.10.9.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.10.9.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">48.23</span></td>
<td id="S4.T3.5.10.9.5" class="ltx_td ltx_align_center"><span id="S4.T3.5.10.9.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">4.1%</span></td>
</tr>
<tr id="S4.T3.5.11.10" class="ltx_tr">
<th id="S4.T3.5.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.11.10.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">where</span></th>
<td id="S4.T3.5.11.10.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.11.10.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">42.11</span></td>
<td id="S4.T3.5.11.10.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.11.10.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">42.43</span></td>
<td id="S4.T3.5.11.10.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.11.10.4.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">37.61</span></td>
<td id="S4.T3.5.11.10.5" class="ltx_td ltx_align_center"><span id="S4.T3.5.11.10.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">2.5%</span></td>
</tr>
<tr id="S4.T3.5.12.11" class="ltx_tr">
<th id="S4.T3.5.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.12.11.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">why/how</span></th>
<td id="S4.T3.5.12.11.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.12.11.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">26.31</span></td>
<td id="S4.T3.5.12.11.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.12.11.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">28.18</span></td>
<td id="S4.T3.5.12.11.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.12.11.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">29.24</span></td>
<td id="S4.T3.5.12.11.5" class="ltx_td ltx_align_center"><span id="S4.T3.5.12.11.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">2.2%</span></td>
</tr>
<tr id="S4.T3.5.13.12" class="ltx_tr">
<th id="S4.T3.5.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.13.12.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">relational: what is</span></th>
<td id="S4.T3.5.13.12.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.13.12.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">70.15</span></td>
<td id="S4.T3.5.13.12.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.13.12.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">67.48</span></td>
<td id="S4.T3.5.13.12.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.13.12.4.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">56.64</span></td>
<td id="S4.T3.5.13.12.5" class="ltx_td ltx_align_center"><span id="S4.T3.5.13.12.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">2.0%</span></td>
</tr>
<tr id="S4.T3.5.14.13" class="ltx_tr">
<th id="S4.T3.5.14.13.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.14.13.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">the man/woman</span></th>
<td id="S4.T3.5.14.13.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.5.14.13.3" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.5.14.13.4" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.5.14.13.5" class="ltx_td"></td>
</tr>
<tr id="S4.T3.5.15.14" class="ltx_tr">
<th id="S4.T3.5.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.15.14.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">relational: what is</span></th>
<td id="S4.T3.5.15.14.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.15.14.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">54.78</span></td>
<td id="S4.T3.5.15.14.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.15.14.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">54.80</span></td>
<td id="S4.T3.5.15.14.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.15.14.4.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">45.41</span></td>
<td id="S4.T3.5.15.14.5" class="ltx_td ltx_align_center"><span id="S4.T3.5.15.14.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">1.8%</span></td>
</tr>
<tr id="S4.T3.5.16.15" class="ltx_tr">
<th id="S4.T3.5.16.15.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.16.15.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">in/on</span></th>
<td id="S4.T3.5.16.15.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.5.16.15.3" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.5.16.15.4" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.5.16.15.5" class="ltx_td"></td>
</tr>
<tr id="S4.T3.5.17.16" class="ltx_tr">
<th id="S4.T3.5.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.17.16.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">which/who</span></th>
<td id="S4.T3.5.17.16.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.17.16.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">43.97</span></td>
<td id="S4.T3.5.17.16.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.17.16.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">42.70</span></td>
<td id="S4.T3.5.17.16.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.17.16.4.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">38.62</span></td>
<td id="S4.T3.5.17.16.5" class="ltx_td ltx_align_center"><span id="S4.T3.5.17.16.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">1.7%</span></td>
</tr>
<tr id="S4.T3.5.18.17" class="ltx_tr">
<th id="S4.T3.5.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.18.17.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">reading: what</span></th>
<td id="S4.T3.5.18.17.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.18.17.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">33.31</span></td>
<td id="S4.T3.5.18.17.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.18.17.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">31.54</span></td>
<td id="S4.T3.5.18.17.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.18.17.4.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">30.84</span></td>
<td id="S4.T3.5.18.17.5" class="ltx_td ltx_align_center"><span id="S4.T3.5.18.17.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">1.6%</span></td>
</tr>
<tr id="S4.T3.5.19.18" class="ltx_tr">
<th id="S4.T3.5.19.18.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.19.18.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">does/number/name</span></th>
<td id="S4.T3.5.19.18.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.5.19.18.3" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.5.19.18.4" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.5.19.18.5" class="ltx_td"></td>
</tr>
<tr id="S4.T3.5.20.19" class="ltx_tr">
<th id="S4.T3.5.20.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.20.19.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">identify scene:</span></th>
<td id="S4.T3.5.20.19.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.20.19.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">86.21</span></td>
<td id="S4.T3.5.20.19.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.20.19.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">76.65</span></td>
<td id="S4.T3.5.20.19.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.20.19.4.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">61.26</span></td>
<td id="S4.T3.5.20.19.5" class="ltx_td ltx_align_center"><span id="S4.T3.5.20.19.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">0.9%</span></td>
</tr>
<tr id="S4.T3.5.21.20" class="ltx_tr">
<th id="S4.T3.5.21.20.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.21.20.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">what room/sport</span></th>
<td id="S4.T3.5.21.20.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.5.21.20.3" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.5.21.20.4" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.5.21.20.5" class="ltx_td"></td>
</tr>
<tr id="S4.T3.5.22.21" class="ltx_tr">
<th id="S4.T3.5.22.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.22.21.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">what time</span></th>
<td id="S4.T3.5.22.21.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.22.21.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">41.47</span></td>
<td id="S4.T3.5.22.21.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.22.21.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">37.74</span></td>
<td id="S4.T3.5.22.21.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.22.21.4.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">38.64</span></td>
<td id="S4.T3.5.22.21.5" class="ltx_td ltx_align_center"><span id="S4.T3.5.22.21.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">0.8%</span></td>
</tr>
<tr id="S4.T3.5.23.22" class="ltx_tr">
<th id="S4.T3.5.23.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.5.23.22.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">what brand</span></th>
<td id="S4.T3.5.23.22.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.23.22.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">45.40</span></td>
<td id="S4.T3.5.23.22.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.23.22.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">44.04</span></td>
<td id="S4.T3.5.23.22.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.5.23.22.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">48.23</span></td>
<td id="S4.T3.5.23.22.5" class="ltx_td ltx_align_center"><span id="S4.T3.5.23.22.5.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">0.4%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.10.1.1" class="ltx_text ltx_font_serif">Table 3</span>: </span>Accuracies by type of question on the validation set.
Percent accuracy is shown for each subset for our proposed
region-based approach, classification using the whole image and
question/answer text, and classification based only on the text.
We also show the frequency of each question type. Note that
since there are 121,512 questions used for testing, there
are hundreds or thousands of examples of even the rarest question
types, so small gains are statistically meaningful. Overall, our
region selection scheme outperforms use of whole images by <math id="S4.T3.3.m1.1" class="ltx_Math" alttext="2\%" display="inline"><semantics id="S4.T3.3.m1.1b"><mrow id="S4.T3.3.m1.1.1" xref="S4.T3.3.m1.1.1.cmml"><mn id="S4.T3.3.m1.1.1.2" xref="S4.T3.3.m1.1.1.2.cmml">2</mn><mo id="S4.T3.3.m1.1.1.1" xref="S4.T3.3.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.3.m1.1c"><apply id="S4.T3.3.m1.1.1.cmml" xref="S4.T3.3.m1.1.1"><csymbol cd="latexml" id="S4.T3.3.m1.1.1.1.cmml" xref="S4.T3.3.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.T3.3.m1.1.1.2.cmml" xref="S4.T3.3.m1.1.1.2">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.m1.1d">2\%</annotation></semantics></math>
and text-only features by <math id="S4.T3.4.m2.1" class="ltx_Math" alttext="5\%" display="inline"><semantics id="S4.T3.4.m2.1b"><mrow id="S4.T3.4.m2.1.1" xref="S4.T3.4.m2.1.1.cmml"><mn id="S4.T3.4.m2.1.1.2" xref="S4.T3.4.m2.1.1.2.cmml">5</mn><mo id="S4.T3.4.m2.1.1.1" xref="S4.T3.4.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.4.m2.1c"><apply id="S4.T3.4.m2.1.1.cmml" xref="S4.T3.4.m2.1.1"><csymbol cd="latexml" id="S4.T3.4.m2.1.1.1.cmml" xref="S4.T3.4.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.T3.4.m2.1.1.2.cmml" xref="S4.T3.4.m2.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.m2.1d">5\%</annotation></semantics></math>. There is substantial
improvement in particular types of questions. For example,
questions such as ‘‘What is the woman holding?’’ are answered
correctly 70% of the time vs. 67% for whole image and only 57%
for text. ‘‘What color,’’ ‘‘What room,’’ and ‘‘What sport’’ also
benefit greatly from use of image features and further from
region weighting. Question types that have yes/no answers tend
not to improve, in part because the prior is so reliable. E.g.,
someone is unlikely to ask ‘‘Does the girl have a lollipop?’’ if
she is not so endowed. So ‘‘no’’ answers are unlikely and also
more difficult to verify. We also note that reading questions
(‘‘What does the sign say?’’) and counting questions (‘‘How many
sheep?’’) are not greatly improved by visual features in our system because they require specialized processes.
</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_typewriter ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.1.1.1" class="ltx_text ltx_font_serif">4.2</span> </span>Region Evaluation</h3>

<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r"><span id="S4.T4.1.1.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Model</span></th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T4.1.1.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Accuracy (%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.2.1" class="ltx_tr">
<th id="S4.T4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T4.1.2.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Q+A (2-bin)</span></th>
<td id="S4.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.2.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">51.87</span></td>
</tr>
<tr id="S4.T4.1.3.2" class="ltx_tr">
<th id="S4.T4.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T4.1.3.2.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">parsed(Q)+A (5-bin)</span></th>
<td id="S4.T4.1.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.3.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">53.98</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.4.1.1" class="ltx_text ltx_font_serif">Table 4</span>: </span>Language model comparison. The 2-bin model is the
concatenation of the question and answer averages. The parsed model
uses the Stanford dependency parser to further split the question
into 4 bins.</figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">In order to evaluate the consistency of our region weightings with
respect to various types of questions, we set up an informal
experiment to directly evaluate them. To determine how well the region
weighting corresponded to regions a person would use to answer a
question, we manually annotated 205 images from the validation set with bounding
boxes considered important to answer the corresponding
question. An example of the annotation and predicted weights can be
seen in Fig. </span><a href="#S4.F6" title="Figure 6 ‣ 4.1 Comparisons between region, image, and language-only models ‣ 4 Experiments ‣ Where To Look: Focus Regions for Visual Question Answering" class="ltx_ref ltx_font_typewriter" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">6</span></a><span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">. To evaluate, we compare the average pixel weighting within the
annotated boxes with the average across all pixels. Pixel weighting
was determined by cumulatively adding each region’s selection weight to each of its
constituent pixels. We observe that the the mean weighting within the
annotated regions was greater than the global average in 148 of the
instances (72.2%), often much greater, and rarely much smaller (Fig. </span><a href="#S4.F7" title="Figure 7 ‣ 4.1 Comparisons between region, image, and language-only models ‣ 4 Experiments ‣ Where To Look: Focus Regions for Visual Question Answering" class="ltx_ref ltx_font_typewriter" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">7</span></a><span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">).</span></p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_typewriter ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.1.1.1" class="ltx_text ltx_font_serif">4.3</span> </span>Language Model</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">We also compare our parsed and binned language model with a simple two-binned
model (one bin averages word2vec of question words; the other averages answer words) to justify our more complex representation. Each
model is trained on the train set and evaluated on the validation set
of the VQA real-images subset. The comparison results are shown in
Table </span><a href="#S4.T4" title="Table 4 ‣ 4.2 Region Evaluation ‣ 4 Experiments ‣ Where To Look: Focus Regions for Visual Question Answering" class="ltx_ref ltx_font_typewriter" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;"> and depict a significant performance
improvement using the parsing.</span></p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_font_typewriter ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section"><span id="S5.1.1.1" class="ltx_text ltx_font_serif">5</span> </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">We presented a model that learns to select regions from the image to solve visual question answering problems. Our model outperforms all baselines and existing work on the MS COCO VQA multiple choice
task </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.p1.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S5.p1.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span></cite><span id="S5.p1.1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, with substantial gains for some questions such as identifying object colors that require focusing on particular regions. One direction for future work is to learn to perform specialized tasks such as counting or reading. Other directions are to incorporate and adapt pre-trained models for object and attribute detectors or geometric reasoning, or to use outside knowledge sources to help learn what is relevant to answer difficult questions. We are also interested in learning where to look to find small objects and recognize activities.</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_font_typewriter ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and
D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Vqa: Visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">International Conference on Computer Vision (ICCV)</span><span id="bib.bib1.5.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
J. Ba, K. Swersky, S. Fidler, and R. Salakhutdinov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Predicting deep zero-shot convolutional neural networks using textual
descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">International Conference on Computer Vision (ICCV)</span><span id="bib.bib2.5.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Return of the devil in the details: Delving deep into convolutional
nets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">British Machine Vision Conference</span><span id="bib.bib3.5.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
X. Chen and C. Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Mind’s eye: A recurrent visual representation for image caption
generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib4.5.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, June 2015.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
M.-C. De Marneffe, B. MacCartney, C. D. Manning, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Generating typed dependency parses from phrase structure parses.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">Proceedings of LREC</span><span id="bib.bib5.5.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, volume 6, pages 449--454, 2006.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan,
K. Saenko, and T. Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Long-term recurrent convolutional networks for visual recognition and
description.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1411.4389</span><span id="bib.bib6.4.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Dollár, J. Gao,
X. He, M. Mitchell, J. Platt, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">From captions to visual concepts and back.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1411.4952</span><span id="bib.bib7.4.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
X. Glorot and Y. Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Understanding the difficulty of training deep feedforward neural
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">International conference on artificial intelligence and
statistics</span><span id="bib.bib8.5.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, pages 249--256, 2010.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
X. Glorot, A. Bordes, and Y. Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Deep sparse rectifier neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">International Conference on Artificial Intelligence and
Statistics</span><span id="bib.bib9.5.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, pages 315--323, 2011.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
Y. Gong, L. Wang, M. Hodosh, J. Hockenmaier, and S. Lazebnik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Improving image-sentence embeddings using large weakly annotated
photo collections.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">Computer Vision--ECCV 2014</span><span id="bib.bib10.5.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, pages 529--545. Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
S. Ioffe and C. Szegedy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Batch normalization: Accelerating deep network training by reducing
internal covariate shift.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1502.03167</span><span id="bib.bib11.4.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
A. Karpathy and L. Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Deep visual-semantic alignments for generating image descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib12.5.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Deep captioning with multimodal recurrent neural networks (m-rnn).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib13.4.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
J. Mao, W. Xu, Y. Yang, J. Wang, and A. L. Yuille.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Explain images with multimodal recurrent neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">NIPS Deep Learning Workshop</span><span id="bib.bib14.4.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
M. F. Mateusz Malinowski, Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Ask your neurons: A neural-based approach to answering questions
about images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib15.5.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
T. Mikolov, K. Chen, G. Corrado, and J. Dean.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Efficient estimation of word representations in vector space.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1301.3781</span><span id="bib.bib16.4.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
M. Ren, R. Kiros, and R. Zemel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Exploring models and data for image question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1505.02074v3</span><span id="bib.bib17.4.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">ImageNet Large Scale Visual Recognition Challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision (IJCV)</span><span id="bib.bib18.4.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, pages 1--42,
April 2015.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Weakly supervised memory networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib19.4.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, abs/1503.08895, 2015.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
A. Vedaldi and K. Lenc.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Matconvnet -- convolutional neural networks for matlab.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">2015.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
O. Vinyals, A. Toshev, S. Bengio, and D. Erhan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Show and tell: A neural image caption generator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib21.5.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, June 2015.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel, and
Y. Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Show, attend and tell: Neural image caption generation with visual
attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1502.03044</span><span id="bib.bib22.4.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
L. Yu, E. Park, A. C. Berg, and T. L. Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Visual madlibs: Fill in the blank image generation and question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1506.00278</span><span id="bib.bib23.4.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">
C. L. Zitnick and P. Dollár.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Edge boxes: Locating object proposals from edges.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_typewriter ltx_font_italic" style="font-size:90%;">Computer Vision--ECCV 2014</span><span id="bib.bib24.5.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">, pages 391--405. Springer, 2014.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1511.07393" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1511.07394" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1511.07394">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1511.07394" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1511.07395" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 17:46:14 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
