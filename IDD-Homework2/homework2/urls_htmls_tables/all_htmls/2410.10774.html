<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention</title>
<!--Generated on Mon Oct 14 17:44:10 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.10774v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S1" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S2" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S2.SS1" title="In 2 Related Works ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Camera Controllable Video Diffusion Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S2.SS2" title="In 2 Related Works ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Multi-view Image Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S2.SS3" title="In 2 Related Works ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>4D Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.SS1" title="In 3 Method ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.SS2" title="In 3 Method ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Camera Controllable Video Diffusion Model</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.SS2.SSS0.Px1" title="In 3.2 Camera Controllable Video Diffusion Model ‣ 3 Method ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Preliminaries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.SS2.SSS0.Px2" title="In 3.2 Camera Controllable Video Diffusion Model ‣ 3 Method ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Camera Conditioning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.SS2.SSS0.Px3" title="In 3.2 Camera Controllable Video Diffusion Model ‣ 3 Method ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Cross-frame Attention for Temporal Consistency</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.SS3" title="In 3 Method ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Consistent Multi-view Video Diffusion Model</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.SS3.SSS0.Px1" title="In 3.3 Consistent Multi-view Video Diffusion Model ‣ 3 Method ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Cross-view Attention for Multi-view consistency</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S4" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Joint Training Strategy on Curated Data Mixtures</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S4.SS1" title="In 4 Joint Training Strategy on Curated Data Mixtures ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Joint Training on Data Mixtures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S4.SS2" title="In 4 Joint Training Strategy on Curated Data Mixtures ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Data Curation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.SS1" title="In 5 Experiments ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Quantitative Comparisons</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.SS1.SSS0.Px1" title="In 5.1 Quantitative Comparisons ‣ 5 Experiments ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">3D Consistency of Frames</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.SS1.SSS0.Px2" title="In 5.1 Quantitative Comparisons ‣ 5 Experiments ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Multi-view Consistency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.SS1.SSS0.Px3" title="In 5.1 Quantitative Comparisons ‣ 5 Experiments ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Visual Quality</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.SS2" title="In 5 Experiments ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Qualtitative Comparison</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.SS3" title="In 5 Experiments ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Ablation Studies and Applications</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S6" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A1" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Additional Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A2" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Additional Data Curation Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A2.SS0.SSS0.Px1" title="In Appendix B Additional Data Curation Details ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Static 3D Objects</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A2.SS0.SSS0.Px2" title="In Appendix B Additional Data Curation Details ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Static 3D Scenes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A2.SS0.SSS0.Px3" title="In Appendix B Additional Data Curation Details ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Dynamic 3D Objects</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A2.SS0.SSS0.Px4" title="In Appendix B Additional Data Curation Details ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Monocular Videos</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A3" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Evaluation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A4" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Ablation Studies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A5" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Applications</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A5.SS1" title="In Appendix E Applications ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.1 </span>Advancing to Four Views at Inference</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A5.SS2" title="In Appendix E Applications ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.2 </span>3D Reconstruction of Generated Frames</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A6" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Limitations</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="id1.1.id1">
Dejia Xu<sup class="ltx_sup" id="id1.1.id1.1">1</sup>  ,Yifan Jiang<sup class="ltx_sup" id="id1.1.id1.2">2</sup>, Chen Huang
<sup class="ltx_sup" id="id1.1.id1.3">2</sup>,
Liangchen Song<sup class="ltx_sup" id="id1.1.id1.4">2</sup>, Thorsten Gernoth<sup class="ltx_sup" id="id1.1.id1.5">2</sup>
<br class="ltx_break"/></span>  <span class="ltx_text ltx_font_bold" id="id2.2.id2">Liangliang Cao<sup class="ltx_sup" id="id2.2.id2.1">3</sup>†, Zhangyang Wang<sup class="ltx_sup" id="id2.2.id2.2">1</sup>, Hao Tang<sup class="ltx_sup" id="id2.2.id2.3">2</sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id2.2.id2.4"><span class="ltx_text ltx_font_medium" id="id2.2.id2.4.1">1</span></sup></span>University of Texas at Austin, <sup class="ltx_sup" id="id3.3.id3">2</sup>Apple, <sup class="ltx_sup" id="id4.4.id4">3</sup>Google
</span><span class="ltx_author_notes"><span class="ltx_text ltx_font_bold" id="id5.5.id1">This work was performed while Dejia Xu interned at Apple.</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id6.id1">In recent years there have been remarkable breakthroughs in image-to-video generation.
However, the 3D consistency and camera controllability of generated frames have remained unsolved. Recent studies have attempted to incorporate camera control into the generation process, but their results are often limited to simple trajectories or lack the ability to generate consistent videos from multiple distinct camera paths for the same scene. To address these limitations, we introduce <span class="ltx_text ltx_font_bold" id="id6.id1.1">Cavia</span>, a novel framework for camera-controllable, multi-view video generation, capable of converting an input image into multiple spatiotemporally consistent videos. Our framework extends the spatial and temporal attention modules into view-integrated attention modules, improving both viewpoint and temporal consistency. This flexible design allows for joint training with diverse curated data sources, including scene-level static videos, object-level synthetic multi-view dynamic videos, and real-world monocular dynamic videos. To our best knowledge, Cavia is the first of its kind that allows the user to precisely specify camera motion while obtaining object motion.
Extensive experiments demonstrate that Cavia surpasses state-of-the-art methods in terms of geometric consistency and perceptual quality. Project Page: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ir1d.github.io/Cavia/" title="">https://ir1d.github.io/Cavia/</a></p>
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotetext: </span>This work was performed while Liangliang Cao worked at Apple.</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The rapid development of diffusion models has enabled significant advancements in video generative models. Early efforts have explored various approaches, either training a video model from scratch or by fine-tuning pre-trained image generation models with additional temporal layers <cite class="ltx_cite ltx_citemacro_citep">(Stability, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib58" title="">2023</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib64" title="">2023a</a>; Ho et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib17" title="">2022b</a>; Singer et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib55" title="">2022</a>; Ho et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib16" title="">2022a</a>; Nan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib42" title="">2024</a>)</cite>. The training data of these video models typically consist of a curated mixture of image <cite class="ltx_cite ltx_citemacro_citep">(Schuhmann et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib52" title="">2022</a>)</cite> and video datasets <cite class="ltx_cite ltx_citemacro_citep">(Bain et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib3" title="">2021</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib65" title="">2023b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib64" title="">a</a>; Nan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib42" title="">2024</a>)</cite>.
While substantial progress has been made in improving model architectures and refining training data, relatively little research has been conducted on the 3D consistency and camera controllability of generated videos.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To tackle this issue, several recent works <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib66" title="">2023c</a>; He et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib14" title="">2024</a>; Bahmani et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib2" title="">2024</a>; Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>; Hou et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib18" title="">2024</a>)</cite> have attempted to introduce camera controllability in video generation, aiming to ensure that generated frames adhere to viewpoint instructions, thereby improving 3D consistency.
These works either enhance viewpoint control through better conditioning signals <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib66" title="">2023c</a>; He et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib14" title="">2024</a>; Bahmani et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib2" title="">2024</a>)</cite> or by utilizing geometric priors, such as epipolar constraints <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite> or explicit 3D representations <cite class="ltx_cite ltx_citemacro_citep">(Hou et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib18" title="">2024</a>)</cite>.
However, despite these efforts, the generated videos often lack precise 3D consistency or are restricted to static scenes with little to no object motion. Moreover, it remains challenging for monocular video generators to produce multi-view consistent videos of the same scene from different camera trajectories.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Since independently sampling multiple sequences often results in significantly inconsistent scenes, generating multiple video sequences simultaneously is desirable. However, this remains extremely challenging due to the scarcity of multi-view video data in the wild, leading to multi-view generations limited to inconsistent near-static scenes or synthetic objects.
A concurrent work, CVD <cite class="ltx_cite ltx_citemacro_citep">(Kuang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib27" title="">2024</a>)</cite>, builds on multi-view static videos <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib84" title="">2018</a>)</cite> and warping-augmented monocular videos <cite class="ltx_cite ltx_citemacro_citep">(Bain et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib3" title="">2021</a>)</cite>, but it can only generate videos with limited baselines, yielding inconsistent results when object motion is present. Another concurrent work, Vivid-ZOO <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib29" title="">2024a</a>)</cite>, leverages dynamic objects from Objaverse <cite class="ltx_cite ltx_citemacro_citep">(Deitke et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib10" title="">2023b</a>)</cite> dataset and renders multi-view videos to train a video generator.
However, due to limited data sources, their results are primarily object-centric frames from fixed viewpoints, lacking realistic backgrounds.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To address these challenges, we propose <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">Cavia</span>, a novel framework that extends a monocular video generator <cite class="ltx_cite ltx_citemacro_citep">(Stability, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib58" title="">2023</a>)</cite> to generate multi-view consistent videos with precise camera control. We enhance the spatial and temporal attention modules to cross-view and cross-frame 3D attentions respectively, improving consistency across both viewpoints and frames.
Our model architecture enables a novel joint training strategy that fully utilizes static, monocular, and multi-view dynamic videos. Static videos <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib84" title="">2018</a>; Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib76" title="">2023</a>; Xia et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib69" title="">2024</a>; Reizenstein et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib45" title="">2021</a>; Deitke et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib10" title="">2023b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib9" title="">a</a>)</cite> are converted to multi-view formats to ensure the geometric consistency in the generated frames. We then incorporate rendered synthetic multi-view videos of dynamic 3D objects <cite class="ltx_cite ltx_citemacro_citep">(Liang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib33" title="">2024</a>; Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib23" title="">2024</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib32" title="">2024c</a>)</cite> to teach the model to generate reasonable object motion. To prevent overfitting on synthetic data, we finetune the model on pose-annotated monocular videos <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib65" title="">2023b</a>; Nan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib42" title="">2024</a>)</cite> to enhance performance on complex scenes.
Our framework synthesizes cross-view and cross-frame consistent videos, and extensive evaluations on real and text-to-image generated images show its applicability across challenging indoor, outdoor, object-centric, and large-scene cases. We systematically measure the quality of the generated videos in terms of per-video and cross-view geometric consistency and perceptual quality. Our experiments demonstrate our superiority compared to previous works both qualitatively and quantitatively. Our experiments demonstrate superior performance compared to previous methods, both qualitatively and quantitatively. Additionally, we show that our method can extrapolate to generate four views during inference and enable 3D reconstruction of the generated frames.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our main contributions can be summarized as follows,</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose a novel framework, <span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Cavia</span>, for generating multi-view videos with camera controllability. We introduce view-integrated attentions, namely cross-view and cross-frame 3D attentions, to enhance consistency across viewpoints and frames.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We introduce an effective joint training strategy that leverages a curated mixture of static, monocular dynamic, and multi-view dynamic videos, ensuring geometric consistency, high-quality object motion, and background preservation in the generated results.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Our experiments demonstrate superior geometric and perceptual quality in both monocular video generation and cross-video consistency compared to baseline methods. Additionally, our flexible framework can operate on four views at inference, offering improved view consistency and enabling 3D reconstruction of the generated frames.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Camera Controllable Video Diffusion Models</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Recent advancements in video diffusion models have significantly benefited from scaling model architectures and leveraging extensive datasets <cite class="ltx_cite ltx_citemacro_citep">(Bain et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib3" title="">2021</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib65" title="">2023b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib64" title="">a</a>)</cite>, leading to impressive capabilities in generating high-quality videos <cite class="ltx_cite ltx_citemacro_citep">(Stability, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib58" title="">2023</a>; Ho et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib17" title="">2022b</a>; Singer et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib55" title="">2022</a>; Ho et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib16" title="">2022a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib43" title="">OpenAI, </a>)</cite>.
While large foundational video diffusion models exist, our work focuses on enhancing camera control over video diffusion processes, a rapidly growing area of research. AnimateDiff <cite class="ltx_cite ltx_citemacro_citep">(Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib13" title="">2023</a>)</cite> and Stable Video Diffusion (SVD) <cite class="ltx_cite ltx_citemacro_citep">(Stability, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib58" title="">2023</a>)</cite> employ individual camera LoRA <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib19" title="">2021</a>)</cite> models for specific camera motions. MotionCtrl <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib66" title="">2023c</a>)</cite> improves flexibility by introducing camera matrices, while CameraCtrl <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib14" title="">2024</a>)</cite>, CamCo <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite>, and VD3D <cite class="ltx_cite ltx_citemacro_citep">(Bahmani et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib2" title="">2024</a>)</cite> enhance the camera control accuracy by introducing Plücker coordinates to the video models via controlnet <cite class="ltx_cite ltx_citemacro_citep">(Zhang &amp; Agrawala, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib79" title="">2023</a>)</cite>. To further improve the geometric consistency, CamCo <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite> applies epipolar constraints and CamTrol <cite class="ltx_cite ltx_citemacro_citep">(Hou et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib18" title="">2024</a>)</cite> incorporates 3D Gaussians <cite class="ltx_cite ltx_citemacro_citep">(Kerbl et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib26" title="">2023</a>)</cite>. However, these methods focus on monocular video generation, limiting their ability to sample multiple consistent video sequences of the same scene from distinct camera paths. CVD <cite class="ltx_cite ltx_citemacro_citep">(Kuang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib27" title="">2024</a>)</cite> extends CameraCtrl <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib14" title="">2024</a>)</cite> for multi-view video generation, but their results are constrained to simple camera and object motion. ViVid-Zoo <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib29" title="">2024a</a>)</cite> extends MVDream <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib54" title="">2023b</a>)</cite> for multi-view purposes but is limited to object-centric results with fixed viewpoints. In contrast, our work explores view-integrated attentions for more precise camera control over arbitrary viewpoints and introduces a joint training strategy leveraging data mixtures to improve novel-view performance in complex scenes.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Multi-view Image Generation</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Early approaches such as MVDiffusion <cite class="ltx_cite ltx_citemacro_citep">(Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib59" title="">2023</a>)</cite> focused on generating multiview images in parallel by employing correspondence-aware attention mechanisms, enabling effective cross-view information interaction, particularly for textured scene meshes. Recent approaches like Zero123++ <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib53" title="">2023a</a>)</cite>, Direct2.5 <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib40" title="">2024</a>)</cite>, Instant3D <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib30" title="">2023</a>)</cite>, MVDream <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib54" title="">2023b</a>)</cite>, MVDiffusion++ <cite class="ltx_cite ltx_citemacro_citep">(Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib60" title="">2024</a>)</cite>, CAT3D <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib12" title="">2024</a>)</cite>, and Wonder3D <cite class="ltx_cite ltx_citemacro_citep">(Long et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib39" title="">2024</a>)</cite> have introduced single-pass frameworks for multiview generation, utilizing multiview self-attention to improve viewpoint consistency. Other works, such as SyncDreamer <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib38" title="">2023b</a>)</cite>, One-2-3-45 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib36" title="">2024</a>)</cite>, Cascade-Zero123 <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib7" title="">2023</a>)</cite> and ConsistNet <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib73" title="">2024a</a>)</cite>, incorporate multiview features into 3D volumes to facilitate 3D-aware diffusion models <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib37" title="">2023a</a>; Watson et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib67" title="">2022</a>)</cite>.
Meanwhile, techniques such as Pose-Guided Diffusion <cite class="ltx_cite ltx_citemacro_citep">(Tseng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib61" title="">2023</a>)</cite>, Era3D <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib31" title="">2024b</a>)</cite>, Epidiff <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib20" title="">2024</a>)</cite>, and SPAD <cite class="ltx_cite ltx_citemacro_citep">(Kant et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib24" title="">2024</a>)</cite> have integrated epipolar-based features to facilitate enhanced viewpoint fusion within diffusion models. Finally, approaches like V3D <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib8" title="">2024b</a>)</cite>, IM-3D <cite class="ltx_cite ltx_citemacro_citep">(Melas-Kyriazi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib41" title="">2024</a>)</cite>, SV3D <cite class="ltx_cite ltx_citemacro_citep">(Voleti et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib63" title="">2024</a>)</cite> and Vivid-1-to-3 <cite class="ltx_cite ltx_citemacro_citep">(Kwak et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib28" title="">2024</a>)</cite> leverage priors from video diffusion models to achieve multiview generation with improved consistency.
However, these methods focus on generating static 3D objects or scenes, while our work introduces vivid object motion into multiview dynamic video generation in complex scenes.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>4D Generation</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Recent efforts in 4D generation have explored various methods <cite class="ltx_cite ltx_citemacro_citep">(Singer et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib56" title="">2023</a>; Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib82" title="">2023</a>; Bahmani et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib1" title="">2023</a>; Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib83" title="">2023</a>; Ling et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib34" title="">2023a</a>)</cite> that use score distillation from video diffusion models to optimize dynamic NeRFs or 3D Gaussians for text- or image-conditioned scenes.
Follow-up works <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib22" title="">2023</a>; Ren et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib46" title="">2023</a>; Yin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib75" title="">2023</a>; Ren et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib47" title="">2024</a>; Zeng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib77" title="">2024</a>; Pan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib44" title="">2024</a>)</cite> investigate video-to-4D generation, enabling controllable 4D scene generation from monocular videos.
More recent methods <cite class="ltx_cite ltx_citemacro_citep">(Liang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib33" title="">2024</a>; Xie et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib70" title="">2024</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib78" title="">2024</a>)</cite> utilize video diffusion models to address the spatial-temporal consistency required for efficient 4D generation.
However, these approaches primarily focus on object-centric generation and face challenges in producing realistic results with complex backgrounds. In contrast, our work emphasizes generating multi-view, 3D-consistent videos for complex scenes.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.4">Image-to-video generation takes a single image <math alttext="I_{0}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">I</mi><mn id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝐼</ci><cn id="S3.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">I_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> as input and outputs a video sequence <math alttext="O_{1},\cdots,O_{n}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.3"><semantics id="S3.SS1.p1.2.m2.3a"><mrow id="S3.SS1.p1.2.m2.3.3.2" xref="S3.SS1.p1.2.m2.3.3.3.cmml"><msub id="S3.SS1.p1.2.m2.2.2.1.1" xref="S3.SS1.p1.2.m2.2.2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.2.2.1.1.2" xref="S3.SS1.p1.2.m2.2.2.1.1.2.cmml">O</mi><mn id="S3.SS1.p1.2.m2.2.2.1.1.3" xref="S3.SS1.p1.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.2.m2.3.3.2.3" xref="S3.SS1.p1.2.m2.3.3.3.cmml">,</mo><mi id="S3.SS1.p1.2.m2.1.1" mathvariant="normal" xref="S3.SS1.p1.2.m2.1.1.cmml">⋯</mi><mo id="S3.SS1.p1.2.m2.3.3.2.4" xref="S3.SS1.p1.2.m2.3.3.3.cmml">,</mo><msub id="S3.SS1.p1.2.m2.3.3.2.2" xref="S3.SS1.p1.2.m2.3.3.2.2.cmml"><mi id="S3.SS1.p1.2.m2.3.3.2.2.2" xref="S3.SS1.p1.2.m2.3.3.2.2.2.cmml">O</mi><mi id="S3.SS1.p1.2.m2.3.3.2.2.3" xref="S3.SS1.p1.2.m2.3.3.2.2.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.3b"><list id="S3.SS1.p1.2.m2.3.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3.2"><apply id="S3.SS1.p1.2.m2.2.2.1.1.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.2.2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.2.2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1.2">𝑂</ci><cn id="S3.SS1.p1.2.m2.2.2.1.1.3.cmml" type="integer" xref="S3.SS1.p1.2.m2.2.2.1.1.3">1</cn></apply><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">⋯</ci><apply id="S3.SS1.p1.2.m2.3.3.2.2.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.3.2.2.1.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p1.2.m2.3.3.2.2.2.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2.2">𝑂</ci><ci id="S3.SS1.p1.2.m2.3.3.2.2.3.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.3c">O_{1},\cdots,O_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.3d">italic_O start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , italic_O start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>. By introducing camera control, the model additionally takes in a sequence of camera information <math alttext="C_{1},\cdots,C_{n}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.3"><semantics id="S3.SS1.p1.3.m3.3a"><mrow id="S3.SS1.p1.3.m3.3.3.2" xref="S3.SS1.p1.3.m3.3.3.3.cmml"><msub id="S3.SS1.p1.3.m3.2.2.1.1" xref="S3.SS1.p1.3.m3.2.2.1.1.cmml"><mi id="S3.SS1.p1.3.m3.2.2.1.1.2" xref="S3.SS1.p1.3.m3.2.2.1.1.2.cmml">C</mi><mn id="S3.SS1.p1.3.m3.2.2.1.1.3" xref="S3.SS1.p1.3.m3.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.3.m3.3.3.2.3" xref="S3.SS1.p1.3.m3.3.3.3.cmml">,</mo><mi id="S3.SS1.p1.3.m3.1.1" mathvariant="normal" xref="S3.SS1.p1.3.m3.1.1.cmml">⋯</mi><mo id="S3.SS1.p1.3.m3.3.3.2.4" xref="S3.SS1.p1.3.m3.3.3.3.cmml">,</mo><msub id="S3.SS1.p1.3.m3.3.3.2.2" xref="S3.SS1.p1.3.m3.3.3.2.2.cmml"><mi id="S3.SS1.p1.3.m3.3.3.2.2.2" xref="S3.SS1.p1.3.m3.3.3.2.2.2.cmml">C</mi><mi id="S3.SS1.p1.3.m3.3.3.2.2.3" xref="S3.SS1.p1.3.m3.3.3.2.2.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.3b"><list id="S3.SS1.p1.3.m3.3.3.3.cmml" xref="S3.SS1.p1.3.m3.3.3.2"><apply id="S3.SS1.p1.3.m3.2.2.1.1.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.2.2.1.1.1.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.2.2.1.1.2.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.2">𝐶</ci><cn id="S3.SS1.p1.3.m3.2.2.1.1.3.cmml" type="integer" xref="S3.SS1.p1.3.m3.2.2.1.1.3">1</cn></apply><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">⋯</ci><apply id="S3.SS1.p1.3.m3.3.3.2.2.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.3.3.2.2.1.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p1.3.m3.3.3.2.2.2.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2.2">𝐶</ci><ci id="S3.SS1.p1.3.m3.3.3.2.2.3.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.3c">C_{1},\cdots,C_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.3d">italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , italic_C start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>, which dictates the desired viewpoint changes for the output sequence. In the multi-view scenario, we extend each batch of the camera control signal and output video sequence to <math alttext="V" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_V</annotation></semantics></math> sequences.
In the following paragraphs, we present our proposed <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.1">Cavia</span> framework in detail.
First, we outline the preliminaries of image-to-video diffusion and describe how camera controllability is introduced in monocular video generation. Then, we elaborate on the model design for multi-view consistent video generation.
An overview of our framework is provided in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.F1" title="Figure 1 ‣ 3.1 Overview ‣ 3 Method ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="325" id="S3.F1.g1" src="x1.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An overview of Cavia is shown in (a). We introduce view-integrated attention modules, namely cross-view attentions and cross-frame attentions, which enforce viewpoint and temporal consistency of the generated frames, respectively. As illustrated in (b) and (c), our view-integrated attention incorporates additional feature dimensions into the attention mechanism, enhancing consistency across views and frames.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Camera Controllable Video Diffusion Model</h3>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Preliminaries</h4>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.4">Our model builds on pre-trained Stable Video Diffusion (SVD) <cite class="ltx_cite ltx_citemacro_citep">(Stability, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib58" title="">2023</a>)</cite>.
SVD extends Stable Diffusion 2.1<cite class="ltx_cite ltx_citemacro_citep">(Rombach et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib48" title="">2022</a>)</cite> by adding temporal convolution and attention layers, following the VideoLDM architecture <cite class="ltx_cite ltx_citemacro_citep">(Blattmann et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib4" title="">2023</a>)</cite>.
SVD is trained with a continuous-time noise scheduler <cite class="ltx_cite ltx_citemacro_citep">(Karras et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib25" title="">2022</a>)</cite>. In each iteration, the training data is perturbed by Gaussian noise <math alttext="\mathbf{n}(t)\sim\mathcal{N}(0,\sigma^{2}(t)\mathbf{I})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.1.m1.4"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.4a"><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.4.4" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.2.cmml">𝐧</mi><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.1.cmml">⁢</mo><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.3.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.cmml"><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">t</mi><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.3.2.2" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.cmml">)</mo></mrow></mrow><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">∼</mo><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.3.cmml">𝒩</mi><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.2.cmml">⁢</mo><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml"><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.2" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml">(</mo><mn id="S3.SS2.SSS0.Px1.p1.1.m1.3.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.3.cmml">0</mn><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml">,</mo><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.cmml"><msup id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.2.cmml">σ</mi><mn id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.3.cmml">2</mn></msup><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.2.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2.cmml">t</mi><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.3.2.2" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.cmml">)</mo></mrow><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.1a" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.4" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.4.cmml">𝐈</mi></mrow><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.4" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.4b"><apply id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4"><csymbol cd="latexml" id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.2">similar-to</csymbol><apply id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3"><times id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.1"></times><ci id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.2">𝐧</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1">𝑡</ci></apply><apply id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1"><times id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.2"></times><ci id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.3">𝒩</ci><interval closure="open" id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1"><cn id="S3.SS2.SSS0.Px1.p1.1.m1.3.3.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.3">0</cn><apply id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1"><times id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.1"></times><apply id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2">superscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.2">𝜎</ci><cn id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.3.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.3">2</cn></apply><ci id="S3.SS2.SSS0.Px1.p1.1.m1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2">𝑡</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.4.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.4">𝐈</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.4c">\mathbf{n}(t)\sim\mathcal{N}(0,\sigma^{2}(t)\mathbf{I})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.1.m1.4d">bold_n ( italic_t ) ∼ caligraphic_N ( 0 , italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_t ) bold_I )</annotation></semantics></math> and the diffusion model is tasked with estimating the clean data <math alttext="x_{0}\sim p_{0}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a"><mrow id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><msub id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2.cmml">x</mi><mn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3.cmml">0</mn></msub><mo id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml">∼</mo><msub id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.cmml">p</mi><mn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3.cmml">0</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1">similar-to</csymbol><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2">𝑥</ci><cn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3">0</cn></apply><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2">𝑝</ci><cn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">x_{0}\sim p_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.2.m2.1d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∼ italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>. Let <math alttext="p(\mathbf{x};\sigma(t))" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.3.m3.3"><semantics id="S3.SS2.SSS0.Px1.p1.3.m3.3a"><mrow id="S3.SS2.SSS0.Px1.p1.3.m3.3.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.3.cmml">p</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.cmml">⁢</mo><mrow id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.2.cmml"><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.2" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.2.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.3.m3.2.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.2.cmml">𝐱</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.2.cmml">;</mo><mrow id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.2.cmml">σ</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.1.cmml">⁢</mo><mrow id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">t</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.3.2.2" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.4" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.3.m3.3b"><apply id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3"><times id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2"></times><ci id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.3">𝑝</ci><list id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1"><ci id="S3.SS2.SSS0.Px1.p1.3.m3.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.2">𝐱</ci><apply id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1"><times id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.1"></times><ci id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.2">𝜎</ci><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1">𝑡</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.3c">p(\mathbf{x};\sigma(t))</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.3.m3.3d">italic_p ( bold_x ; italic_σ ( italic_t ) )</annotation></semantics></math> denote the marginal probability of noisy data <math alttext="\mathbf{x}_{t}=\mathbf{x}_{0}+\mathbf{n}(t)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.4.m4.1"><semantics id="S3.SS2.SSS0.Px1.p1.4.m4.1a"><mrow id="S3.SS2.SSS0.Px1.p1.4.m4.1.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.cmml"><msub id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.2.cmml">𝐱</mi><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.3.cmml">t</mi></msub><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.1.cmml">=</mo><mrow id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.cmml"><msub id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.2.cmml">𝐱</mi><mn id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.3.cmml">0</mn></msub><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.1.cmml">+</mo><mrow id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.2.cmml">𝐧</mi><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.1.cmml">⁢</mo><mrow id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.3.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.cmml"><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml">t</mi><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.3.2.2" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.4.m4.1b"><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2"><eq id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.1"></eq><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.2">𝐱</ci><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.3">𝑡</ci></apply><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3"><plus id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.1"></plus><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.2">𝐱</ci><cn id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.3.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.3">0</cn></apply><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3"><times id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.1"></times><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.2">𝐧</ci><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.4.m4.1c">\mathbf{x}_{t}=\mathbf{x}_{0}+\mathbf{n}(t)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.4.m4.1d">bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + bold_n ( italic_t )</annotation></semantics></math>, the iterative refinement process of diffusion model corresponds to the probability flow ordinary differential equation (ODE):</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d\mathbf{x}=-\dot{\sigma}(t)\sigma(t)\nabla_{\mathbf{x}}\log p(\mathbf{x};%
\sigma(t))dt." class="ltx_Math" display="block" id="S3.E1.m1.5"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1.3" xref="S3.E1.m1.5.5.1.1.3.cmml"><mi id="S3.E1.m1.5.5.1.1.3.2" xref="S3.E1.m1.5.5.1.1.3.2.cmml">d</mi><mo id="S3.E1.m1.5.5.1.1.3.1" xref="S3.E1.m1.5.5.1.1.3.1.cmml">⁢</mo><mi id="S3.E1.m1.5.5.1.1.3.3" xref="S3.E1.m1.5.5.1.1.3.3.cmml">𝐱</mi></mrow><mo id="S3.E1.m1.5.5.1.1.2" xref="S3.E1.m1.5.5.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.5.5.1.1.1" xref="S3.E1.m1.5.5.1.1.1.cmml"><mo id="S3.E1.m1.5.5.1.1.1a" xref="S3.E1.m1.5.5.1.1.1.cmml">−</mo><mrow id="S3.E1.m1.5.5.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.cmml"><mover accent="true" id="S3.E1.m1.5.5.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.3.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.3.2" xref="S3.E1.m1.5.5.1.1.1.1.3.2.cmml">σ</mi><mo id="S3.E1.m1.5.5.1.1.1.1.3.1" xref="S3.E1.m1.5.5.1.1.1.1.3.1.cmml">˙</mo></mover><mo id="S3.E1.m1.5.5.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.4.2" xref="S3.E1.m1.5.5.1.1.1.1.cmml"><mo id="S3.E1.m1.5.5.1.1.1.1.4.2.1" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">t</mi><mo id="S3.E1.m1.5.5.1.1.1.1.4.2.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E1.m1.5.5.1.1.1.1.2a" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml">⁢</mo><mi id="S3.E1.m1.5.5.1.1.1.1.5" xref="S3.E1.m1.5.5.1.1.1.1.5.cmml">σ</mi><mo id="S3.E1.m1.5.5.1.1.1.1.2b" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.6.2" xref="S3.E1.m1.5.5.1.1.1.1.cmml"><mo id="S3.E1.m1.5.5.1.1.1.1.6.2.1" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.cmml">(</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">t</mi><mo id="S3.E1.m1.5.5.1.1.1.1.6.2.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E1.m1.5.5.1.1.1.1.2c" lspace="0.167em" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.7" xref="S3.E1.m1.5.5.1.1.1.1.7.cmml"><mrow id="S3.E1.m1.5.5.1.1.1.1.7.1" xref="S3.E1.m1.5.5.1.1.1.1.7.1.cmml"><msub id="S3.E1.m1.5.5.1.1.1.1.7.1.1" xref="S3.E1.m1.5.5.1.1.1.1.7.1.1.cmml"><mo id="S3.E1.m1.5.5.1.1.1.1.7.1.1.2" rspace="0.167em" xref="S3.E1.m1.5.5.1.1.1.1.7.1.1.2.cmml">∇</mo><mi id="S3.E1.m1.5.5.1.1.1.1.7.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.7.1.1.3.cmml">𝐱</mi></msub><mi id="S3.E1.m1.5.5.1.1.1.1.7.1.2" xref="S3.E1.m1.5.5.1.1.1.1.7.1.2.cmml">log</mi></mrow><mo id="S3.E1.m1.5.5.1.1.1.1.7a" lspace="0.167em" xref="S3.E1.m1.5.5.1.1.1.1.7.cmml">⁡</mo><mi id="S3.E1.m1.5.5.1.1.1.1.7.2" xref="S3.E1.m1.5.5.1.1.1.1.7.2.cmml">p</mi></mrow><mo id="S3.E1.m1.5.5.1.1.1.1.2d" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.2.cmml"><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.2.cmml">(</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">𝐱</mi><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.2.cmml">;</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.cmml">σ</mi><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">t</mi><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.3.2.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.4" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.2.cmml">)</mo></mrow><mo id="S3.E1.m1.5.5.1.1.1.1.2e" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml">⁢</mo><mi id="S3.E1.m1.5.5.1.1.1.1.8" xref="S3.E1.m1.5.5.1.1.1.1.8.cmml">d</mi><mo id="S3.E1.m1.5.5.1.1.1.1.2f" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml">⁢</mo><mi id="S3.E1.m1.5.5.1.1.1.1.9" xref="S3.E1.m1.5.5.1.1.1.1.9.cmml">t</mi></mrow></mrow></mrow><mo id="S3.E1.m1.5.5.1.2" lspace="0em" xref="S3.E1.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1"><eq id="S3.E1.m1.5.5.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.2"></eq><apply id="S3.E1.m1.5.5.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.3"><times id="S3.E1.m1.5.5.1.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.3.1"></times><ci id="S3.E1.m1.5.5.1.1.3.2.cmml" xref="S3.E1.m1.5.5.1.1.3.2">𝑑</ci><ci id="S3.E1.m1.5.5.1.1.3.3.cmml" xref="S3.E1.m1.5.5.1.1.3.3">𝐱</ci></apply><apply id="S3.E1.m1.5.5.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1"><minus id="S3.E1.m1.5.5.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1"></minus><apply id="S3.E1.m1.5.5.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1"><times id="S3.E1.m1.5.5.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2"></times><apply id="S3.E1.m1.5.5.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.3"><ci id="S3.E1.m1.5.5.1.1.1.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.3.1">˙</ci><ci id="S3.E1.m1.5.5.1.1.1.1.3.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.3.2">𝜎</ci></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑡</ci><ci id="S3.E1.m1.5.5.1.1.1.1.5.cmml" xref="S3.E1.m1.5.5.1.1.1.1.5">𝜎</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝑡</ci><apply id="S3.E1.m1.5.5.1.1.1.1.7.cmml" xref="S3.E1.m1.5.5.1.1.1.1.7"><apply id="S3.E1.m1.5.5.1.1.1.1.7.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.7.1"><apply id="S3.E1.m1.5.5.1.1.1.1.7.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.7.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.7.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.7.1.1">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.7.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.7.1.1.2">∇</ci><ci id="S3.E1.m1.5.5.1.1.1.1.7.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.7.1.1.3">𝐱</ci></apply><log id="S3.E1.m1.5.5.1.1.1.1.7.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.7.1.2"></log></apply><ci id="S3.E1.m1.5.5.1.1.1.1.7.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.7.2">𝑝</ci></apply><list id="S3.E1.m1.5.5.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1"><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">𝐱</ci><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1"><times id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1"></times><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.2">𝜎</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝑡</ci></apply></list><ci id="S3.E1.m1.5.5.1.1.1.1.8.cmml" xref="S3.E1.m1.5.5.1.1.1.1.8">𝑑</ci><ci id="S3.E1.m1.5.5.1.1.1.1.9.cmml" xref="S3.E1.m1.5.5.1.1.1.1.9">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">d\mathbf{x}=-\dot{\sigma}(t)\sigma(t)\nabla_{\mathbf{x}}\log p(\mathbf{x};%
\sigma(t))dt.</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.5d">italic_d bold_x = - over˙ start_ARG italic_σ end_ARG ( italic_t ) italic_σ ( italic_t ) ∇ start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT roman_log italic_p ( bold_x ; italic_σ ( italic_t ) ) italic_d italic_t .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.9"><math alttext="\nabla_{\mathbf{x}}\log p(\mathbf{x};\sigma(t))" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.5.m1.3"><semantics id="S3.SS2.SSS0.Px1.p1.5.m1.3a"><mrow id="S3.SS2.SSS0.Px1.p1.5.m1.3.3" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.cmml"><msub id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.2" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.2.cmml">∇</mo><mi id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.3" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.3.cmml">𝐱</mi></msub><mi id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.2" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.2.cmml">log</mi></mrow><mo id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3a" lspace="0.167em" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.cmml">⁡</mo><mi id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.2" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.2.cmml">p</mi></mrow><mo id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.2" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.2.cmml">⁢</mo><mrow id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.2.cmml"><mo id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.2" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.2.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.5.m1.2.2" xref="S3.SS2.SSS0.Px1.p1.5.m1.2.2.cmml">𝐱</mi><mo id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.3" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.2.cmml">;</mo><mrow id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.2.cmml">σ</mi><mo id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.1.cmml">⁢</mo><mrow id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.5.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m1.1.1.cmml">t</mi><mo id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.3.2.2" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.4" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.5.m1.3b"><apply id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3"><times id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.2"></times><apply id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3"><apply id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1"><apply id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.2">∇</ci><ci id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.3">𝐱</ci></apply><log id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.2"></log></apply><ci id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.2">𝑝</ci></apply><list id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1"><ci id="S3.SS2.SSS0.Px1.p1.5.m1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.2.2">𝐱</ci><apply id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1"><times id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.1"></times><ci id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.2">𝜎</ci><ci id="S3.SS2.SSS0.Px1.p1.5.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.1.1">𝑡</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.5.m1.3c">\nabla_{\mathbf{x}}\log p(\mathbf{x};\sigma(t))</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.5.m1.3d">∇ start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT roman_log italic_p ( bold_x ; italic_σ ( italic_t ) )</annotation></semantics></math> refers to the score function, which is parameterized by a denoiser <math alttext="D_{\bm{\theta}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.6.m2.1"><semantics id="S3.SS2.SSS0.Px1.p1.6.m2.1a"><msub id="S3.SS2.SSS0.Px1.p1.6.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.6.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.6.m2.1.1.2" xref="S3.SS2.SSS0.Px1.p1.6.m2.1.1.2.cmml">D</mi><mi id="S3.SS2.SSS0.Px1.p1.6.m2.1.1.3" xref="S3.SS2.SSS0.Px1.p1.6.m2.1.1.3.cmml">𝜽</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.6.m2.1b"><apply id="S3.SS2.SSS0.Px1.p1.6.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.6.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.6.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m2.1.1.2">𝐷</ci><ci id="S3.SS2.SSS0.Px1.p1.6.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m2.1.1.3">𝜽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.6.m2.1c">D_{\bm{\theta}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.6.m2.1d">italic_D start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> through
<math alttext="\nabla_{\mathbf{x}}\log p(\mathbf{x};\sigma)\approx\left(D_{\bm{\theta}}(%
\mathbf{x};\sigma)-\mathbf{x}\right)/\sigma^{2}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.7.m3.5"><semantics id="S3.SS2.SSS0.Px1.p1.7.m3.5a"><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.cmml"><msub id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.2.cmml">∇</mo><mi id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.3" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.3.cmml">𝐱</mi></msub><mi id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.2.cmml">log</mi></mrow><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2a" lspace="0.167em" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.cmml">⁡</mo><mi id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.2.cmml">p</mi></mrow><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.1" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.1.cmml">⁢</mo><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.7.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.7.m3.1.1.cmml">𝐱</mi><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.2.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.1.cmml">;</mo><mi id="S3.SS2.SSS0.Px1.p1.7.m3.2.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.2.2.cmml">σ</mi><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.2.3" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.2.cmml">≈</mo><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.cmml"><msub id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.2.cmml">D</mi><mi id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.3" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.3.cmml">𝜽</mi></msub><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.1" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.7.m3.3.3" xref="S3.SS2.SSS0.Px1.p1.7.m3.3.3.cmml">𝐱</mi><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.2.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.1.cmml">;</mo><mi id="S3.SS2.SSS0.Px1.p1.7.m3.4.4" xref="S3.SS2.SSS0.Px1.p1.7.m3.4.4.cmml">σ</mi><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.2.3" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.1.cmml">−</mo><mi id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.3.cmml">𝐱</mi></mrow><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.cmml">)</mo></mrow><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.2.cmml">/</mo><msup id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.2.cmml">σ</mi><mn id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.3" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.3.cmml">2</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.7.m3.5b"><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5"><approx id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.2"></approx><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3"><times id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.1"></times><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2"><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1"><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.2">∇</ci><ci id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.3">𝐱</ci></apply><log id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.2"></log></apply><ci id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.2">𝑝</ci></apply><list id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.2"><ci id="S3.SS2.SSS0.Px1.p1.7.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.1.1">𝐱</ci><ci id="S3.SS2.SSS0.Px1.p1.7.m3.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.2.2">𝜎</ci></list></apply><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1"><divide id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.2"></divide><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1"><minus id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.1"></minus><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2"><times id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.1"></times><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.2">𝐷</ci><ci id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.3">𝜽</ci></apply><list id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.2"><ci id="S3.SS2.SSS0.Px1.p1.7.m3.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.3.3">𝐱</ci><ci id="S3.SS2.SSS0.Px1.p1.7.m3.4.4.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.4.4">𝜎</ci></list></apply><ci id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.3">𝐱</ci></apply><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3">superscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.2">𝜎</ci><cn id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.3.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.7.m3.5c">\nabla_{\mathbf{x}}\log p(\mathbf{x};\sigma)\approx\left(D_{\bm{\theta}}(%
\mathbf{x};\sigma)-\mathbf{x}\right)/\sigma^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.7.m3.5d">∇ start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT roman_log italic_p ( bold_x ; italic_σ ) ≈ ( italic_D start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_x ; italic_σ ) - bold_x ) / italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>. We follow the EDM-preconditioning framework <cite class="ltx_cite ltx_citemacro_citep">(Karras et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib25" title="">2022</a>; Stability, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib58" title="">2023</a>)</cite> and parameterize <math alttext="D_{\bm{\theta}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.8.m4.1"><semantics id="S3.SS2.SSS0.Px1.p1.8.m4.1a"><msub id="S3.SS2.SSS0.Px1.p1.8.m4.1.1" xref="S3.SS2.SSS0.Px1.p1.8.m4.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.8.m4.1.1.2" xref="S3.SS2.SSS0.Px1.p1.8.m4.1.1.2.cmml">D</mi><mi id="S3.SS2.SSS0.Px1.p1.8.m4.1.1.3" xref="S3.SS2.SSS0.Px1.p1.8.m4.1.1.3.cmml">𝜽</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.8.m4.1b"><apply id="S3.SS2.SSS0.Px1.p1.8.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.8.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.8.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m4.1.1.2">𝐷</ci><ci id="S3.SS2.SSS0.Px1.p1.8.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m4.1.1.3">𝜽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.8.m4.1c">D_{\bm{\theta}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.8.m4.1d">italic_D start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> with a neural network <math alttext="F_{\bm{\theta}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.9.m5.1"><semantics id="S3.SS2.SSS0.Px1.p1.9.m5.1a"><msub id="S3.SS2.SSS0.Px1.p1.9.m5.1.1" xref="S3.SS2.SSS0.Px1.p1.9.m5.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.9.m5.1.1.2" xref="S3.SS2.SSS0.Px1.p1.9.m5.1.1.2.cmml">F</mi><mi id="S3.SS2.SSS0.Px1.p1.9.m5.1.1.3" xref="S3.SS2.SSS0.Px1.p1.9.m5.1.1.3.cmml">𝜽</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.9.m5.1b"><apply id="S3.SS2.SSS0.Px1.p1.9.m5.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.9.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.9.m5.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.9.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.9.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.9.m5.1.1.2">𝐹</ci><ci id="S3.SS2.SSS0.Px1.p1.9.m5.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.9.m5.1.1.3">𝜽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.9.m5.1c">F_{\bm{\theta}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.9.m5.1d">italic_F start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> as follows,</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="D_{\bm{\theta}}=c_{\text{skip}}\mathbf{x}+c_{\text{out}}F_{\bm{\theta}}(c_{%
\text{in}}\mathbf{x};c_{\text{noise}})." class="ltx_Math" display="block" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.4.cmml"><mi id="S3.E2.m1.1.1.1.1.4.2" xref="S3.E2.m1.1.1.1.1.4.2.cmml">D</mi><mi id="S3.E2.m1.1.1.1.1.4.3" xref="S3.E2.m1.1.1.1.1.4.3.cmml">𝜽</mi></msub><mo id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml"><mrow id="S3.E2.m1.1.1.1.1.2.4" xref="S3.E2.m1.1.1.1.1.2.4.cmml"><msub id="S3.E2.m1.1.1.1.1.2.4.2" xref="S3.E2.m1.1.1.1.1.2.4.2.cmml"><mi id="S3.E2.m1.1.1.1.1.2.4.2.2" xref="S3.E2.m1.1.1.1.1.2.4.2.2.cmml">c</mi><mtext id="S3.E2.m1.1.1.1.1.2.4.2.3" xref="S3.E2.m1.1.1.1.1.2.4.2.3a.cmml">skip</mtext></msub><mo id="S3.E2.m1.1.1.1.1.2.4.1" xref="S3.E2.m1.1.1.1.1.2.4.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.1.1.2.4.3" xref="S3.E2.m1.1.1.1.1.2.4.3.cmml">𝐱</mi></mrow><mo id="S3.E2.m1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.2.3.cmml">+</mo><mrow id="S3.E2.m1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.2.2.cmml"><msub id="S3.E2.m1.1.1.1.1.2.2.4" xref="S3.E2.m1.1.1.1.1.2.2.4.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.4.2" xref="S3.E2.m1.1.1.1.1.2.2.4.2.cmml">c</mi><mtext id="S3.E2.m1.1.1.1.1.2.2.4.3" xref="S3.E2.m1.1.1.1.1.2.2.4.3a.cmml">out</mtext></msub><mo id="S3.E2.m1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml">⁢</mo><msub id="S3.E2.m1.1.1.1.1.2.2.5" xref="S3.E2.m1.1.1.1.1.2.2.5.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.5.2" xref="S3.E2.m1.1.1.1.1.2.2.5.2.cmml">F</mi><mi id="S3.E2.m1.1.1.1.1.2.2.5.3" xref="S3.E2.m1.1.1.1.1.2.2.5.3.cmml">𝜽</mi></msub><mo id="S3.E2.m1.1.1.1.1.2.2.3a" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml">⁢</mo><mrow id="S3.E2.m1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.3.cmml"><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.3" stretchy="false" xref="S3.E2.m1.1.1.1.1.2.2.2.3.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">c</mi><mtext id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3a.cmml">in</mtext></msub><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml">𝐱</mi></mrow><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.4" xref="S3.E2.m1.1.1.1.1.2.2.2.3.cmml">;</mo><msub id="S3.E2.m1.1.1.1.1.2.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.cmml">c</mi><mtext id="S3.E2.m1.1.1.1.1.2.2.2.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.3a.cmml">noise</mtext></msub><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.5" stretchy="false" xref="S3.E2.m1.1.1.1.1.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" lspace="0em" xref="S3.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"></eq><apply id="S3.E2.m1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.4.1.cmml" xref="S3.E2.m1.1.1.1.1.4">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.4.2.cmml" xref="S3.E2.m1.1.1.1.1.4.2">𝐷</ci><ci id="S3.E2.m1.1.1.1.1.4.3.cmml" xref="S3.E2.m1.1.1.1.1.4.3">𝜽</ci></apply><apply id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><plus id="S3.E2.m1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.3"></plus><apply id="S3.E2.m1.1.1.1.1.2.4.cmml" xref="S3.E2.m1.1.1.1.1.2.4"><times id="S3.E2.m1.1.1.1.1.2.4.1.cmml" xref="S3.E2.m1.1.1.1.1.2.4.1"></times><apply id="S3.E2.m1.1.1.1.1.2.4.2.cmml" xref="S3.E2.m1.1.1.1.1.2.4.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.4.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.4.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.4.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.4.2.2">𝑐</ci><ci id="S3.E2.m1.1.1.1.1.2.4.2.3a.cmml" xref="S3.E2.m1.1.1.1.1.2.4.2.3"><mtext id="S3.E2.m1.1.1.1.1.2.4.2.3.cmml" mathsize="70%" xref="S3.E2.m1.1.1.1.1.2.4.2.3">skip</mtext></ci></apply><ci id="S3.E2.m1.1.1.1.1.2.4.3.cmml" xref="S3.E2.m1.1.1.1.1.2.4.3">𝐱</ci></apply><apply id="S3.E2.m1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2"><times id="S3.E2.m1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.3"></times><apply id="S3.E2.m1.1.1.1.1.2.2.4.cmml" xref="S3.E2.m1.1.1.1.1.2.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.4.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.4">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.4.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.4.2">𝑐</ci><ci id="S3.E2.m1.1.1.1.1.2.2.4.3a.cmml" xref="S3.E2.m1.1.1.1.1.2.2.4.3"><mtext id="S3.E2.m1.1.1.1.1.2.2.4.3.cmml" mathsize="70%" xref="S3.E2.m1.1.1.1.1.2.2.4.3">out</mtext></ci></apply><apply id="S3.E2.m1.1.1.1.1.2.2.5.cmml" xref="S3.E2.m1.1.1.1.1.2.2.5"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.5.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.5">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.5.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.5.2">𝐹</ci><ci id="S3.E2.m1.1.1.1.1.2.2.5.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.5.3">𝜽</ci></apply><list id="S3.E2.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2"><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1"></times><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2">𝑐</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3a.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3"><mtext id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" mathsize="70%" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3">in</mtext></ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3">𝐱</ci></apply><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2">𝑐</ci><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.2.3a.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.3"><mtext id="S3.E2.m1.1.1.1.1.2.2.2.2.2.3.cmml" mathsize="70%" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.3">noise</mtext></ci></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">D_{\bm{\theta}}=c_{\text{skip}}\mathbf{x}+c_{\text{out}}F_{\bm{\theta}}(c_{%
\text{in}}\mathbf{x};c_{\text{noise}}).</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">italic_D start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT = italic_c start_POSTSUBSCRIPT skip end_POSTSUBSCRIPT bold_x + italic_c start_POSTSUBSCRIPT out end_POSTSUBSCRIPT italic_F start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( italic_c start_POSTSUBSCRIPT in end_POSTSUBSCRIPT bold_x ; italic_c start_POSTSUBSCRIPT noise end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.11">During training, the network <math alttext="F_{\bm{\theta}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.10.m1.1"><semantics id="S3.SS2.SSS0.Px1.p1.10.m1.1a"><msub id="S3.SS2.SSS0.Px1.p1.10.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.10.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.10.m1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.10.m1.1.1.2.cmml">F</mi><mi id="S3.SS2.SSS0.Px1.p1.10.m1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.10.m1.1.1.3.cmml">𝜽</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.10.m1.1b"><apply id="S3.SS2.SSS0.Px1.p1.10.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.10.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.10.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.10.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.10.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.10.m1.1.1.2">𝐹</ci><ci id="S3.SS2.SSS0.Px1.p1.10.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.10.m1.1.1.3">𝜽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.10.m1.1c">F_{\bm{\theta}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.10.m1.1d">italic_F start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> is optimized using denoising score matching for <math alttext="D_{\bm{\theta}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.11.m2.1"><semantics id="S3.SS2.SSS0.Px1.p1.11.m2.1a"><msub id="S3.SS2.SSS0.Px1.p1.11.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.11.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.11.m2.1.1.2" xref="S3.SS2.SSS0.Px1.p1.11.m2.1.1.2.cmml">D</mi><mi id="S3.SS2.SSS0.Px1.p1.11.m2.1.1.3" xref="S3.SS2.SSS0.Px1.p1.11.m2.1.1.3.cmml">𝜽</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.11.m2.1b"><apply id="S3.SS2.SSS0.Px1.p1.11.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.11.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.11.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.11.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.11.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.11.m2.1.1.2">𝐷</ci><ci id="S3.SS2.SSS0.Px1.p1.11.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.11.m2.1.1.3">𝜽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.11.m2.1c">D_{\bm{\theta}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.11.m2.1d">italic_D start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{E}\left[\left\|D_{\bm{\theta}}\left(\mathbf{x}_{0}+\mathbf{n};\sigma,%
\text{cond}\right)-\mathbf{x}_{0}\right\|_{2}^{2}\right]." class="ltx_Math" display="block" id="S3.E3.m1.3"><semantics id="S3.E3.m1.3a"><mrow id="S3.E3.m1.3.3.1" xref="S3.E3.m1.3.3.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1" xref="S3.E3.m1.3.3.1.1.cmml"><mi id="S3.E3.m1.3.3.1.1.3" xref="S3.E3.m1.3.3.1.1.3.cmml">𝔼</mi><mo id="S3.E3.m1.3.3.1.1.2" xref="S3.E3.m1.3.3.1.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.3.3.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.2.cmml"><mo id="S3.E3.m1.3.3.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.2.1.cmml">[</mo><msubsup id="S3.E3.m1.3.3.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml">D</mi><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml">𝜽</mi></msub><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">𝐱</mi><mn id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">0</mn></msub><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">𝐧</mi></mrow><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">;</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">σ</mi><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.4" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><mtext id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2a.cmml">cond</mtext><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.5" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">−</mo><msub id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml">𝐱</mi><mn id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.cmml">0</mn></msub></mrow><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E3.m1.3.3.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.3.cmml">2</mn><mn id="S3.E3.m1.3.3.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.3.cmml">2</mn></msubsup><mo id="S3.E3.m1.3.3.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S3.E3.m1.3.3.1.2" lspace="0em" xref="S3.E3.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.3b"><apply id="S3.E3.m1.3.3.1.1.cmml" xref="S3.E3.m1.3.3.1"><times id="S3.E3.m1.3.3.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.2"></times><ci id="S3.E3.m1.3.3.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.3">𝔼</ci><apply id="S3.E3.m1.3.3.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.3.3.1.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E3.m1.3.3.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1">superscript</csymbol><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1"><minus id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2"></minus><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1"><times id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2">𝐷</ci><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3">𝜽</ci></apply><list id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1"><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1"><plus id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1"></plus><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝐱</ci><cn id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" type="integer" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">0</cn></apply><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝐧</ci></apply><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝜎</ci><ci id="S3.E3.m1.2.2a.cmml" xref="S3.E3.m1.2.2"><mtext id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">cond</mtext></ci></list></apply><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.2">𝐱</ci><cn id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3">0</cn></apply></apply></apply><cn id="S3.E3.m1.3.3.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.3.3.1.1.1.1.1.1.3">2</cn></apply><cn id="S3.E3.m1.3.3.1.1.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.3.3.1.1.1.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.3c">\mathbb{E}\left[\left\|D_{\bm{\theta}}\left(\mathbf{x}_{0}+\mathbf{n};\sigma,%
\text{cond}\right)-\mathbf{x}_{0}\right\|_{2}^{2}\right].</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.3d">blackboard_E [ ∥ italic_D start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + bold_n ; italic_σ , cond ) - bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Camera Conditioning</h4>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.10">Although SVD is pre-trained on various high-quality video and image data, it does not natively support precise camera control instructions directly.
To address this, we introduce camera conditioning to the model via Plücker coordinates  <cite class="ltx_cite ltx_citemacro_citep">(Jia, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib21" title="">2020</a>)</cite>, which is widely adopted as position
embeddings in 360<sup class="ltx_sup" id="S3.SS2.SSS0.Px2.p1.10.1">∘</sup> unbounded light fields<cite class="ltx_cite ltx_citemacro_citep">(Sitzmann et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib57" title="">2021</a>)</cite>. Plücker coordinates are defined as <math alttext="P=(d^{\prime},o\times d^{\prime})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.2.m2.2"><semantics id="S3.SS2.SSS0.Px2.p1.2.m2.2a"><mrow id="S3.SS2.SSS0.Px2.p1.2.m2.2.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.4" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.4.cmml">P</mi><mo id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.3.cmml">=</mo><mrow id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.3.cmml"><mo id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.3" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.3.cmml">(</mo><msup id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.2.cmml">d</mi><mo id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.3.cmml">′</mo></msup><mo id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.4" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.3.cmml">,</mo><mrow id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.2.cmml">o</mi><mo id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.1.cmml">×</mo><msup id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.2.cmml">d</mi><mo id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.3.cmml">′</mo></msup></mrow><mo id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.5" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.2.m2.2b"><apply id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2"><eq id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.3"></eq><ci id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.4.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.4">𝑃</ci><interval closure="open" id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2"><apply id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.2">𝑑</ci><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.3">′</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2"><times id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.1"></times><ci id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.2">𝑜</ci><apply id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.2">𝑑</ci><ci id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.3">′</ci></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.2.m2.2c">P=(d^{\prime},o\times d^{\prime})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.2.m2.2d">italic_P = ( italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_o × italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT )</annotation></semantics></math>, where <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.3.m3.1"><semantics id="S3.SS2.SSS0.Px2.p1.3.m3.1a"><mo id="S3.SS2.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.3.m3.1b"><times id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.3.m3.1d">×</annotation></semantics></math> is the cross product and <math alttext="d^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.4.m4.1"><semantics id="S3.SS2.SSS0.Px2.p1.4.m4.1a"><msup id="S3.SS2.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.cmml">d</mi><mo id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.4.m4.1b"><apply id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2">𝑑</ci><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.4.m4.1c">d^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.4.m4.1d">italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> refers to the normalized ray direction <math alttext="d^{\prime}=\frac{d}{||d||}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.5.m5.1"><semantics id="S3.SS2.SSS0.Px2.p1.5.m5.1a"><mrow id="S3.SS2.SSS0.Px2.p1.5.m5.1.2" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.cmml"><msup id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.2" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.2.cmml">d</mi><mo id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.3" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.3.cmml">′</mo></msup><mo id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.1.cmml">=</mo><mfrac id="S3.SS2.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3.cmml">d</mi><mrow id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.2.cmml"><mo id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.3.1" maxsize="142%" minsize="142%" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.2.1.cmml">‖</mo><mi id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.1.cmml">d</mi><mo id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.3.2" maxsize="142%" minsize="142%" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.2.1.cmml">‖</mo></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.5.m5.1b"><apply id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2"><eq id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.1"></eq><apply id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.2">𝑑</ci><ci id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.3">′</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1"><divide id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1"></divide><ci id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3">𝑑</ci><apply id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.3"><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.3.1">norm</csymbol><ci id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.1">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.5.m5.1c">d^{\prime}=\frac{d}{||d||}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.5.m5.1d">italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = divide start_ARG italic_d end_ARG start_ARG | | italic_d | | end_ARG</annotation></semantics></math>. Let camera extrinsic matrix be <math alttext="E=[\bf{R}|\bf{T}]" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.6.m6.1"><semantics id="S3.SS2.SSS0.Px2.p1.6.m6.1a"><mrow id="S3.SS2.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.cmml">E</mi><mo id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.cmml">=</mo><mrow id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.2.cmml"><mo id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.2" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.2.1.cmml">[</mo><mrow id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.2.cmml">𝐑</mi><mo fence="false" id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.3.cmml">𝐓</mi></mrow><mo id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.3" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.6.m6.1b"><apply id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1"><eq id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2"></eq><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3">𝐸</ci><apply id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.2">𝐑</ci><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.3">𝐓</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.6.m6.1c">E=[\bf{R}|\bf{T}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.6.m6.1d">italic_E = [ bold_R | bold_T ]</annotation></semantics></math> and intrinsic matrix be <math alttext="\bf{K}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.7.m7.1"><semantics id="S3.SS2.SSS0.Px2.p1.7.m7.1a"><mi id="S3.SS2.SSS0.Px2.p1.7.m7.1.1" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.cmml">𝐊</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.7.m7.1b"><ci id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1">𝐊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.7.m7.1c">\bf{K}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.7.m7.1d">bold_K</annotation></semantics></math>,
the ray direction <math alttext="d_{x,y}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.8.m8.2"><semantics id="S3.SS2.SSS0.Px2.p1.8.m8.2a"><msub id="S3.SS2.SSS0.Px2.p1.8.m8.2.3" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.8.m8.2.3.2" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.3.2.cmml">d</mi><mrow id="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.4" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.8.m8.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.8.m8.1.1.1.1.cmml">x</mi><mo id="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.4.1" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.3.cmml">,</mo><mi id="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.2.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.8.m8.2b"><apply id="S3.SS2.SSS0.Px2.p1.8.m8.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.8.m8.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.8.m8.2.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.3.2">𝑑</ci><list id="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.4"><ci id="S3.SS2.SSS0.Px2.p1.8.m8.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.1.1.1.1">𝑥</ci><ci id="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.2">𝑦</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.8.m8.2c">d_{x,y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.8.m8.2d">italic_d start_POSTSUBSCRIPT italic_x , italic_y end_POSTSUBSCRIPT</annotation></semantics></math> for 2D pixel located at <math alttext="(x,y)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.9.m9.2"><semantics id="S3.SS2.SSS0.Px2.p1.9.m9.2a"><mrow id="S3.SS2.SSS0.Px2.p1.9.m9.2.3.2" xref="S3.SS2.SSS0.Px2.p1.9.m9.2.3.1.cmml"><mo id="S3.SS2.SSS0.Px2.p1.9.m9.2.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.9.m9.2.3.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px2.p1.9.m9.1.1" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1.cmml">x</mi><mo id="S3.SS2.SSS0.Px2.p1.9.m9.2.3.2.2" xref="S3.SS2.SSS0.Px2.p1.9.m9.2.3.1.cmml">,</mo><mi id="S3.SS2.SSS0.Px2.p1.9.m9.2.2" xref="S3.SS2.SSS0.Px2.p1.9.m9.2.2.cmml">y</mi><mo id="S3.SS2.SSS0.Px2.p1.9.m9.2.3.2.3" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.9.m9.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.9.m9.2b"><interval closure="open" id="S3.SS2.SSS0.Px2.p1.9.m9.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m9.2.3.2"><ci id="S3.SS2.SSS0.Px2.p1.9.m9.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1">𝑥</ci><ci id="S3.SS2.SSS0.Px2.p1.9.m9.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m9.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.9.m9.2c">(x,y)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.9.m9.2d">( italic_x , italic_y )</annotation></semantics></math> is formulated as <math alttext="d=\bf{R}\bf{K}^{-1}(\begin{smallmatrix}x\\
y\\
1\end{smallmatrix})+\bf{T}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.10.m10.1"><semantics id="S3.SS2.SSS0.Px2.p1.10.m10.1a"><mrow id="S3.SS2.SSS0.Px2.p1.10.m10.1.2" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.2" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.2.cmml">d</mi><mo id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.1" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.1.cmml">=</mo><mrow id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.cmml"><mrow id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.cmml"><msup id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.2" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.2.cmml">𝐑𝐊</mi><mrow id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3.cmml"><mo id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3a" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3.cmml">−</mo><mn id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3.2" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3.2.cmml">𝟏</mn></mrow></msup><mo id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.1" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.1.cmml">⁢</mo><mrow id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.3.2" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml"><mo id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml">(</mo><mtable id="S3.SS2.SSS0.Px2.p1.10.m10.1.1" rowspacing="0pt" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml"><mtr id="S3.SS2.SSS0.Px2.p1.10.m10.1.1a" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml"><mtd id="S3.SS2.SSS0.Px2.p1.10.m10.1.1b" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.1.1.1.cmml">𝐱</mi></mtd></mtr><mtr id="S3.SS2.SSS0.Px2.p1.10.m10.1.1c" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml"><mtd id="S3.SS2.SSS0.Px2.p1.10.m10.1.1d" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2.1.1" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2.1.1.cmml">𝐲</mi></mtd></mtr><mtr id="S3.SS2.SSS0.Px2.p1.10.m10.1.1e" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml"><mtd id="S3.SS2.SSS0.Px2.p1.10.m10.1.1f" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml"><mn id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3.1.1" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3.1.1.cmml">𝟏</mn></mtd></mtr></mtable><mo id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.3.2.2" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.1" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.1.cmml">+</mo><mi id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.3" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.3.cmml">𝐓</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.10.m10.1b"><apply id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2"><eq id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.1"></eq><ci id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.2">𝑑</ci><apply id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3"><plus id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.1"></plus><apply id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2"><times id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.1"></times><apply id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.2">𝐑𝐊</ci><apply id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3"><minus id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3"></minus><cn id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3.2.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3.2">1</cn></apply></apply><matrix id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.3.2"><matrixrow id="S3.SS2.SSS0.Px2.p1.10.m10.1.1a.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.3.2"><ci id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.1.1.1">𝐱</ci></matrixrow><matrixrow id="S3.SS2.SSS0.Px2.p1.10.m10.1.1b.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.3.2"><ci id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2.1.1">𝐲</ci></matrixrow><matrixrow id="S3.SS2.SSS0.Px2.p1.10.m10.1.1c.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.3.2"><cn id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3.1.1.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3.1.1">1</cn></matrixrow></matrix></apply><ci id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.3">𝐓</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.10.m10.1c">d=\bf{R}\bf{K}^{-1}(\begin{smallmatrix}x\\
y\\
1\end{smallmatrix})+\bf{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.10.m10.1d">italic_d = bold_RK start_POSTSUPERSCRIPT - bold_1 end_POSTSUPERSCRIPT ( start_ROW start_CELL bold_x end_CELL end_ROW start_ROW start_CELL bold_y end_CELL end_ROW start_ROW start_CELL bold_1 end_CELL end_ROW ) + bold_T</annotation></semantics></math>.
These spatial Plücker coordinates are concatenated channel-wise with the original latent inputs of SVD. We enlarge the convolution kernel of the first layer accordingly. The newly introduced matrices are zero-initialized to ensure training stability.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p2.1">We utilize a relative camera coordinate system, where the first frame is positioned at the world origin with an identity matrix for rotation. The following frames are rotated accordingly. To stabilize training, we normalize the scale of the training sequences to a unit scale. This is implemented by resizing the maximum distance-to-origin in the multi-view camera sequence to 1.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Cross-frame Attention for Temporal Consistency</h4>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p1.1">Vanilla 1D temporal attention in the SVD backbone is insufficient for modeling large pixel displacements when the viewpoint changes <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib54" title="">2023b</a>; Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib74" title="">2024b</a>)</cite>. In vanilla 1D temporal attention, attention matrices are calculated over the frame number dimension, and latent features only interact with features from the same spatial location across frames. This limits information flow between different spatial-temporal locations. While this might not be a big issue for video generation with limited motion, viewpoint changes typically cause significant pixel displacements, which calls for better architecture for more efficient information propagation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px3.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p2.5">To overcome this issue, we inflate the original 1D temporal attention modules in the SVD network into 3D cross-frame temporal attention modules, allowing for joint modeling of spatial-temporal feature coherence.
The inflation operation can be achieved by rearranging the latent features before the attention matrix calculations.
Consider the latent features of shape <math alttext="(B\hskip 2.5ptV\hskip 2.5ptF\hskip 2.5ptC\hskip 2.5ptH\hskip 2.5ptW)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.1.m1.1"><semantics id="S3.SS2.SSS0.Px3.p2.1.m1.1a"><mrow id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.2" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.2" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.2.cmml">B</mi><mo id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.3" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.3.cmml">V</mi><mo id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1a" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.4" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.4.cmml">F</mi><mo id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1b" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.5" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.5.cmml">C</mi><mo id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1c" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.6" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.6.cmml">H</mi><mo id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1d" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.7" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.7.cmml">W</mi></mrow><mo id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.3" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p2.1.m1.1b"><apply id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1"><times id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1"></times><ci id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.2">𝐵</ci><ci id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.3">𝑉</ci><ci id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.4.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.4">𝐹</ci><ci id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.5.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.5">𝐶</ci><ci id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.6.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.6">𝐻</ci><ci id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.7.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.7">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p2.1.m1.1c">(B\hskip 2.5ptV\hskip 2.5ptF\hskip 2.5ptC\hskip 2.5ptH\hskip 2.5ptW)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p2.1.m1.1d">( italic_B italic_V italic_F italic_C italic_H italic_W )</annotation></semantics></math> where <math alttext="F" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.2.m2.1"><semantics id="S3.SS2.SSS0.Px3.p2.2.m2.1a"><mi id="S3.SS2.SSS0.Px3.p2.2.m2.1.1" xref="S3.SS2.SSS0.Px3.p2.2.m2.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p2.2.m2.1b"><ci id="S3.SS2.SSS0.Px3.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.2.m2.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p2.2.m2.1c">F</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p2.2.m2.1d">italic_F</annotation></semantics></math> refers to the length of frames and <math alttext="V" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.3.m3.1"><semantics id="S3.SS2.SSS0.Px3.p2.3.m3.1a"><mi id="S3.SS2.SSS0.Px3.p2.3.m3.1.1" xref="S3.SS2.SSS0.Px3.p2.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p2.3.m3.1b"><ci id="S3.SS2.SSS0.Px3.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.3.m3.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p2.3.m3.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p2.3.m3.1d">italic_V</annotation></semantics></math> is the number of views, instead of employing 1D attention mechanism on rearranged shape of <math alttext="((B\hskip 2.5ptV\hskip 2.5ptH\hskip 2.5ptW)\hskip 2.5ptF\hskip 2.5ptC)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.4.m4.1"><semantics id="S3.SS2.SSS0.Px3.p2.4.m4.1a"><mrow id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.2" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.cmml"><mrow id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.2.cmml">B</mi><mo id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.1" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.3.cmml">V</mi><mo id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.1a" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.4" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.4.cmml">H</mi><mo id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.1b" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.5" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.5.cmml">W</mi></mrow><mo id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.2" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.2.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.3" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.3.cmml">F</mi><mo id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.2a" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.2.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.4" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.4.cmml">C</mi></mrow><mo id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.3" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p2.4.m4.1b"><apply id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1"><times id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.2"></times><apply id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1"><times id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.1"></times><ci id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.2">𝐵</ci><ci id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.3">𝑉</ci><ci id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.4.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.4">𝐻</ci><ci id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.5.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.5">𝑊</ci></apply><ci id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.3">𝐹</ci><ci id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.4.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.4">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p2.4.m4.1c">((B\hskip 2.5ptV\hskip 2.5ptH\hskip 2.5ptW)\hskip 2.5ptF\hskip 2.5ptC)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p2.4.m4.1d">( ( italic_B italic_V italic_H italic_W ) italic_F italic_C )</annotation></semantics></math>, our inflated attention operates on the rearranged shape of <math alttext="((B\hskip 2.5ptV)\hskip 2.5pt(F\hskip 2.5ptH\hskip 2.5ptW)\hskip 2.5ptC)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.5.m5.1"><semantics id="S3.SS2.SSS0.Px3.p2.5.m5.1a"><mrow id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.2" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.cmml"><mrow id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.2.cmml">B</mi><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.1" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.3.cmml">V</mi></mrow><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.3" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.3.cmml">⁢</mo><mrow id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.cmml"><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.2" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.cmml">(</mo><mrow id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.2" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.2.cmml">F</mi><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.1" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.3" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.3.cmml">H</mi><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.1a" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.4" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.4.cmml">W</mi></mrow><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.3" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.cmml">)</mo></mrow><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.3a" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.3.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.4" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.4.cmml">C</mi></mrow><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.3" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p2.5.m5.1b"><apply id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1"><times id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.3"></times><apply id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1"><times id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.1"></times><ci id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.2">𝐵</ci><ci id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.3">𝑉</ci></apply><apply id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1"><times id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.1"></times><ci id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.2">𝐹</ci><ci id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.3">𝐻</ci><ci id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.4.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.4">𝑊</ci></apply><ci id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.4.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.4">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p2.5.m5.1c">((B\hskip 2.5ptV)\hskip 2.5pt(F\hskip 2.5ptH\hskip 2.5ptW)\hskip 2.5ptC)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p2.5.m5.1d">( ( italic_B italic_V ) ( italic_F italic_H italic_W ) italic_C )</annotation></semantics></math>, integrating spatial features into the attention matrices.
A visualization is provided in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.F1" title="Figure 1 ‣ 3.1 Overview ‣ 3 Method ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">1</span></a>(c).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px3.p3">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p3.1">Since our rearrange operation only alters the sequence length of the attention inputs without modifying the feature dimensions, we can seamlessly inherit the pre-trained weights from the SVD backbone for our purpose. Thanks to this rearrange operation, our inflated temporal attention now calculates the similarity of spatial-temporal features simultaneously, accommodating larger pixel displacements while maintaining temporal consistency.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Consistent Multi-view Video Diffusion Model</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Adding Plücker coordinates for camera control and introducing improved temporal attention allows the video diffusion model to generate reasonably consistent monocular videos. However, for multi-view generation, a monocular video diffusion model that generates samples independently cannot ensure view consistency across multiple sequences. To address this, we introduce novel design mechanisms and training strategies to extend the monocular video diffusion model to the multi-view generation task.</p>
</div>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Cross-view Attention for Multi-view consistency</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.3">To improve cross-view consistency in multi-view videos, we aim to encourage information exchange during the generation process.
Since our temporal cross-frame attention modules already handle intra-view feature connections within each video sequence, we focus on exchanging inter-view signals through the spatial cross-view modules.
Inspired by MVDream <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib54" title="">2023b</a>)</cite>, we introduce 3D cross-view attention modules, inflated from the spatial attention blocks of SVD <cite class="ltx_cite ltx_citemacro_citep">(Stability, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib58" title="">2023</a>)</cite>.
Specifically, we rearrange the <span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS0.Px1.p1.3.1">V</span> views such that frames at each corresponding timesteps are concatenated before being sent into the attention modules.
In detail, we rearrange the latent features from shape <math alttext="(B\hskip 2.5ptV\hskip 2.5ptF\hskip 2.5ptC\hskip 2.5ptH\hskip 2.5ptW)" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.cmml"><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.2" stretchy="false" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.2" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.2.cmml">B</mi><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1" lspace="0.250em" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.3" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.3.cmml">V</mi><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1a" lspace="0.250em" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.4" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.4.cmml">F</mi><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1b" lspace="0.250em" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.5" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.5.cmml">C</mi><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1c" lspace="0.250em" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.6" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.6.cmml">H</mi><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1d" lspace="0.250em" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.7" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.7.cmml">W</mi></mrow><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.3" stretchy="false" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1"><times id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1"></times><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.2">𝐵</ci><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.3">𝑉</ci><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.4.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.4">𝐹</ci><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.5.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.5">𝐶</ci><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.6.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.6">𝐻</ci><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.7.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.7">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.1.m1.1c">(B\hskip 2.5ptV\hskip 2.5ptF\hskip 2.5ptC\hskip 2.5ptH\hskip 2.5ptW)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.1.m1.1d">( italic_B italic_V italic_F italic_C italic_H italic_W )</annotation></semantics></math> to <math alttext="(((B\hskip 2.5ptF)(V\hskip 2.5ptH\hskip 2.5ptW)\hskip 2.5ptC)" class="ltx_math_unparsed" display="inline" id="S3.SS3.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS3.SSS0.Px1.p1.2.m2.1a"><mrow id="S3.SS3.SSS0.Px1.p1.2.m2.1b"><mo id="S3.SS3.SSS0.Px1.p1.2.m2.1.1" stretchy="false">(</mo><mrow id="S3.SS3.SSS0.Px1.p1.2.m2.1.2"><mo id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.1" stretchy="false">(</mo><mrow id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.2"><mo id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.2.1" stretchy="false">(</mo><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.2.2">B</mi><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.2.3">F</mi><mo id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.2.4" stretchy="false">)</mo></mrow><mrow id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.3"><mo id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.3.1" stretchy="false">(</mo><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.3.2">V</mi><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.3.3">H</mi><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.3.4">W</mi><mo id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.3.5" rspace="0.250em" stretchy="false">)</mo></mrow><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.4">C</mi><mo id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.2.m2.1c">(((B\hskip 2.5ptF)(V\hskip 2.5ptH\hskip 2.5ptW)\hskip 2.5ptC)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.2.m2.1d">( ( ( italic_B italic_F ) ( italic_V italic_H italic_W ) italic_C )</annotation></semantics></math> instead of <math alttext="(((B\hskip 2.5ptV\hskip 2.5ptF)(H\hskip 2.5ptW)\hskip 2.5ptC)" class="ltx_math_unparsed" display="inline" id="S3.SS3.SSS0.Px1.p1.3.m3.1"><semantics id="S3.SS3.SSS0.Px1.p1.3.m3.1a"><mrow id="S3.SS3.SSS0.Px1.p1.3.m3.1b"><mo id="S3.SS3.SSS0.Px1.p1.3.m3.1.1" stretchy="false">(</mo><mrow id="S3.SS3.SSS0.Px1.p1.3.m3.1.2"><mo id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.1" stretchy="false">(</mo><mrow id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.2"><mo id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.2.1" stretchy="false">(</mo><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.2.2">B</mi><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.2.3">V</mi><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.2.4">F</mi><mo id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.2.5" stretchy="false">)</mo></mrow><mrow id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.3"><mo id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.3.1" stretchy="false">(</mo><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.3.2">H</mi><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.3.3">W</mi><mo id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.3.4" rspace="0.250em" stretchy="false">)</mo></mrow><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.4">C</mi><mo id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.3.m3.1c">(((B\hskip 2.5ptV\hskip 2.5ptF)(H\hskip 2.5ptW)\hskip 2.5ptC)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.3.m3.1d">( ( ( italic_B italic_V italic_F ) ( italic_H italic_W ) italic_C )</annotation></semantics></math>. A visualization is provided in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.F1" title="Figure 1 ‣ 3.1 Overview ‣ 3 Method ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">1</span></a>(b).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p2.1">Since only the second-to-last dimension, representing token length, is extended while other dimensions remain unchanged, our inflated spatial attention can inherit the model weights from the monocular setting. This flexibility allows our model to leverage training data with varying numbers of views and facilitates extrapolation to additional views at inference.
To handle multi-view generation, we introduce an additional view dimension to the input data.
To maintain workflow simplicity, we absorb the view dimension into the batch dimension during processing of other blocks, ensuring flexibility in handling different numbers of views.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Joint Training Strategy on Curated Data Mixtures</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Thanks to the view-integrated attention mechanism, which allows for inheriting the module weights, our framework can leverage various data sources, including static, multi-view dynamic, and monocular videos. This is hard to achieve in previous methods. In this section, we first illustrate our joint training strategy, followed by details on the curated data mixtures.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Joint Training on Data Mixtures</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.4">For videos capturing static scenes <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib84" title="">2018</a>; Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib76" title="">2023</a>; Xia et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib69" title="">2024</a>; Reizenstein et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib45" title="">2021</a>; Deitke et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib10" title="">2023b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib9" title="">a</a>)</cite>, we consider all frames to be temporally synchronized.
An arbitrary subsequence of length <math alttext="(F-1)\times V+1" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mrow id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml"><mrow id="S4.SS1.p1.1.m1.1.1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.SS1.p1.1.m1.1.1.1.1.1.2" stretchy="false" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.p1.1.m1.1.1.1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.1.1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.2.cmml">F</mi><mo id="S4.SS1.p1.1.m1.1.1.1.1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml">−</mo><mn id="S4.SS1.p1.1.m1.1.1.1.1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S4.SS1.p1.1.m1.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S4.SS1.p1.1.m1.1.1.1.2" rspace="0.222em" xref="S4.SS1.p1.1.m1.1.1.1.2.cmml">×</mo><mi id="S4.SS1.p1.1.m1.1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.1.3.cmml">V</mi></mrow><mo id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">+</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><plus id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2"></plus><apply id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"><times id="S4.SS1.p1.1.m1.1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.1.2"></times><apply id="S4.SS1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1.1.1"><minus id="S4.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.1"></minus><ci id="S4.SS1.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.2">𝐹</ci><cn id="S4.SS1.p1.1.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.3">1</cn></apply><ci id="S4.SS1.p1.1.m1.1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.1.3">𝑉</ci></apply><cn id="S4.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">(F-1)\times V+1</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">( italic_F - 1 ) × italic_V + 1</annotation></semantics></math> from the original video can be reformatted into a <math alttext="V" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">V</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">italic_V</annotation></semantics></math>-view sequence with a shared starting frame and <math alttext="F" class="ltx_Math" display="inline" id="S4.SS1.p1.3.m3.1"><semantics id="S4.SS1.p1.3.m3.1a"><mi id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">F</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.3.m3.1d">italic_F</annotation></semantics></math> total frames per view. Static scenes also allow frame order reversal, providing additional augmentation opportunities.
We further prepare multi-view dynamic videos by rendering animatable objects from Objaverse <cite class="ltx_cite ltx_citemacro_citep">(Liang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib33" title="">2024</a>; Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib23" title="">2024</a>)</cite>. We design random smooth trajectories with diverse elevation and azimuth changes to avoid overfitting on simple camera movements. These trajectories start from a shared random forward-facing starting point and result in <math alttext="n\times v" class="ltx_Math" display="inline" id="S4.SS1.p1.4.m4.1"><semantics id="S4.SS1.p1.4.m4.1a"><mrow id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><mi id="S4.SS1.p1.4.m4.1.1.2" xref="S4.SS1.p1.4.m4.1.1.2.cmml">n</mi><mo id="S4.SS1.p1.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p1.4.m4.1.1.1.cmml">×</mo><mi id="S4.SS1.p1.4.m4.1.1.3" xref="S4.SS1.p1.4.m4.1.1.3.cmml">v</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><times id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1.1"></times><ci id="S4.SS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2">𝑛</ci><ci id="S4.SS1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">n\times v</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.4.m4.1d">italic_n × italic_v</annotation></semantics></math> frames in total.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">To avoid the model overfitting on synthetic images with simple backgrounds, we include a portion of data from monocular in-the-wild videos <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib65" title="">2023b</a>; Nan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib42" title="">2024</a>)</cite>. Training multi-view camera control from monocular videos is extremely challenging. Although CamCo <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite> and 4DiM <cite class="ltx_cite ltx_citemacro_citep">(Watson et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib68" title="">2024</a>)</cite> have explored joint training for monocular video generation, these approaches are unsuitable for multi-view scenarios. The concurrent work CVD <cite class="ltx_cite ltx_citemacro_citep">(Kuang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib27" title="">2024</a>)</cite> explored homography warping to augment the monocular videos into pseudo-multi-view videos, but the limited realism of these augmentations restricts their ability to generate complex camera and object motion.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.2">To overcome these issues, we choose to jointly train our model on monocular and multi-view videos to effectively utilize the abundant object motion information from all data sources. We annotate the monocular videos with camera poses using Particle-SfM <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib81" title="">2022</a>)</cite>.
Since in-the-wild monocular videos often contain noisy or unnatural content, we apply a rigorous filtering pipeline to remove unsuitable clips.
These curated video clips, sourced from InternVid <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib65" title="">2023b</a>)</cite> and OpenVid <cite class="ltx_cite ltx_citemacro_citep">(Nan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib42" title="">2024</a>)</cite> datasets, provide rich object motion as well as complex backgrounds that mitigate the gap between scene-level static data and object-level dynamic data.
We rearrange monocular videos as <math alttext="V=1" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">V</mi><mo id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><eq id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1"></eq><ci id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">𝑉</ci><cn id="S4.SS1.p3.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">V=1</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">italic_V = 1</annotation></semantics></math> samples so that all data items can be processed uniformly without bells and whistles. Thanks to our view-integrated attention modules, which accommodate varying token lengths, the varying view numbers <math alttext="V" class="ltx_Math" display="inline" id="S4.SS1.p3.2.m2.1"><semantics id="S4.SS1.p3.2.m2.1a"><mi id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><ci id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">V</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.2.m2.1d">italic_V</annotation></semantics></math> do not affect the training process.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="259" id="S4.F2.g1" src="x2.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Statistics of the (a) point cloud size, (b) aesthetic score, and (c) camera motion classification result for our monocular video dataset.</figcaption>
</figure>
<figure class="ltx_figure ltx_align_floatright" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="154" id="S4.F3.g1" src="extracted/5924086/figures/data.png" width="220"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Sources of our training videos.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Data Curation</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We begin by training our model extensively on static video data sourced from various publicly available datasets. Wild-RGBD <cite class="ltx_cite ltx_citemacro_citep">(Xia et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib69" title="">2024</a>)</cite> includes nearly 20,000 RGB-D videos across 46 common object categories.
MVImgNet <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib76" title="">2023</a>)</cite> comprises 219,188 videos featuring objects from 238 classes. DL3DV-10K <cite class="ltx_cite ltx_citemacro_citep">(Ling et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib35" title="">2023b</a>)</cite> provides 7,000 long-duration videos captured in both indoor and outdoor environments. CO3Dv2 <cite class="ltx_cite ltx_citemacro_citep">(Reizenstein et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib45" title="">2021</a>)</cite> contains 34,000 turntable-like videos of rigid objects, crowd-sourced by nonexperts using cellphone cameras. Objaverse <cite class="ltx_cite ltx_citemacro_citep">(Deitke et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib10" title="">2023b</a>)</cite> and Objaverse-XL <cite class="ltx_cite ltx_citemacro_citep">(Deitke et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib9" title="">2023a</a>)</cite> exhaustively crawl 10 million publicly available 3D assets. From these, we filtered out low-quality assets, such as those with incorrect textures or overly simplistic geometry, yielding a high-quality subset of 400,000 assets.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Similar to Diffusion4D <cite class="ltx_cite ltx_citemacro_citep">(Liang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib33" title="">2024</a>)</cite> and Animate3D <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib23" title="">2024</a>)</cite>, we filter the animatable objects from Objaverse’s Sketchfab subset. We exclude objects with excessive motion, which might result in partial observations, as well as nearly static objects with minimal motion. This curation process helps us obtain 19,000 high-quality dynamic assets that can be rendered from arbitrary viewpoints and timesteps, facilitating multi-view video generation. During each training iteration, we augment the frames with randomly selected background colors.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">To improve the model’s ability to generate object motion in the presence of complex backgrounds, we prepare monocular videos with camera pose annotations similar to CamCo <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite>. First, we use Particle-SfM <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib81" title="">2022</a>)</cite> to estimate the camera poses for randomly sampled frames from videos from InternVid <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib65" title="">2023b</a>)</cite> and OpenVid <cite class="ltx_cite ltx_citemacro_citep">(Nan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib42" title="">2024</a>)</cite>. Inspired by CO3D <cite class="ltx_cite ltx_citemacro_citep">(Reizenstein et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib45" title="">2021</a>)</cite> and CamCo <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite>, we remove the videos where SfM fails to register all available frames or produces a point cloud with too few points or too many points.
Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S4.F2" title="Figure 2 ‣ 4.1 Joint Training on Data Mixtures ‣ 4 Joint Training Strategy on Curated Data Mixtures ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">2</span></a>(a) shows the point count statistics. A point cloud with too few points indicates poor frame registration to a shared 3D representation, while too many points suggest a mostly static scene, which is undesirable as we focus on object motion. Additionally, non-registered frames may indicate potential scene changes.
We then apply a rigorous filtering pipeline to ensure the quality of the video samples used for training.
This includes filtering based on aesthetic scores, optical character recognition (OCR), and camera motion classification using optical flow. Videos containing detected character regions are aggressively removed. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S4.F2" title="Figure 2 ‣ 4.1 Joint Training on Data Mixtures ‣ 4 Joint Training Strategy on Curated Data Mixtures ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">2</span></a>(b) and (c) present statistics on aesthetic score and camera motion classification results. Videos with low aesthetic scores or those classified as having static camera motion are excluded from the training set. Ultimately, we construct a dataset of 393,000 monocular videos annotated with camera poses. We provide a summary of the data sources used in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S4.F3" title="Figure 3 ‣ 4.1 Joint Training on Data Mixtures ‣ 4 Joint Training Strategy on Curated Data Mixtures ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">3</span></a>. More details and analysis are provided in the appendix.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1051" id="S4.F4.g1" src="x3.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Per-video qualitative comparisons. The first frame in each reference set is the input image. Neither the image nor the camera trajectories were seen during model training. Video results are provided in supplementary for clearer qualitative comparisons.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we present experimental results and analysis. Video comparisons are included in the supplementary material for optimal visual evaluation.
It is important to note that for all qualitative and quantitative evaluations, neither the input images nor the camera trajectories were used during model training.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Quantitative Comparisons</h3>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">3D Consistency of Frames</h4>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.3">We evaluate the 3D consistency of the generated videos using COLMAP <cite class="ltx_cite ltx_citemacro_citep">(Schönberger &amp; Frahm, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib50" title="">2016</a>; Schönberger et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib51" title="">2016</a>)</cite>. COLMAP is widely adopted for 3D reconstruction methods where camera pose estimation is required for in-the-wild images. We configure the COLMAP following previous methods <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib11" title="">2022</a>; Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite> for best few-view performance.
A higher COLMAP error rate indicates poorer 3D consistency in the input images. Motivated by this, we report COLMAP errors as a measure of the 3D consistency of the frames. Each video is retried up to five times to reduce randomness.
We randomly sample 1,000 video sequences from RealEstate10K <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib84" title="">2018</a>)</cite> test set for evaluation.
Since we have ground truth 3D scenes, we use the ground truth camera pose sequences as the viewpoint instruction of the video model and compare the generated frames against the ground truth images.
Similar to prior works <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib14" title="">2024</a>; Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite>, we extract the estimated camera poses and calculate the relative translation and rotation differences.
Specifically, given two camera pose sequences, we convert them to relative poses and align the first frames to world origin.
We then measure the angular errors in translation and rotation. Unlike previous works <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib14" title="">2024</a>; Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite> that calculate the Euclidean distance of translation vectors, we use angular error measurements to ensure the camera pose scales are normalized, addressing scale ambiguity. As shown in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.T1" title="Table 1 ‣ 3D Consistency of Frames ‣ 5.1 Quantitative Comparisons ‣ 5 Experiments ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">1</span></a>, we calculate the area under the cumulative error curve (AUC) of frames whose rotation and translations are below certain thresholds (<math alttext="5^{\circ}" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S5.SS1.SSS0.Px1.p1.1.m1.1a"><msup id="S5.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">5</mn><mo id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1">superscript</csymbol><cn id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" type="integer" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2">5</cn><compose id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.1.m1.1c">5^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p1.1.m1.1d">5 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="10^{\circ}" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.2.m2.1"><semantics id="S5.SS1.SSS0.Px1.p1.2.m2.1a"><msup id="S5.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.cmml"><mn id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.2" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml">10</mn><mo id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.3" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.2.m2.1b"><apply id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1">superscript</csymbol><cn id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml" type="integer" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.2">10</cn><compose id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.2.m2.1c">10^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p1.2.m2.1d">10 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="20^{\circ}" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.3.m3.1"><semantics id="S5.SS1.SSS0.Px1.p1.3.m3.1a"><msup id="S5.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1.cmml"><mn id="S5.SS1.SSS0.Px1.p1.3.m3.1.1.2" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1.2.cmml">20</mn><mo id="S5.SS1.SSS0.Px1.p1.3.m3.1.1.3" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.3.m3.1b"><apply id="S5.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1">superscript</csymbol><cn id="S5.SS1.SSS0.Px1.p1.3.m3.1.1.2.cmml" type="integer" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1.2">20</cn><compose id="S5.SS1.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.3.m3.1c">20^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p1.3.m3.1d">20 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math>). Our method significantly outperforms existing baselines.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1064" id="S5.F5.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Qualitative comparisons for 2-view video generations. Each generation consists of two rows, where each row represents a sequence of generated frames, with columns showing frames at the same timestep. Neither the image nor the camera trajectories were used during model training. <span class="ltx_text" id="S5.F5.2.1" style="color:#FF0000;">Red</span> dotted lines are annotated to highlight object motion. Video results are included in the supplementary material for clearer comparisons.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Quantitative comparison for monocular geometry consistency on RealEstate10K test set.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.7" style="width:318.0pt;height:85.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-43.9pt,11.7pt) scale(0.783705904146909,0.783705904146909) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.7.7">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.5.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T1.5.5.5.6" rowspan="2"><span class="ltx_text" id="S5.T1.5.5.5.6.1">Methods</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.1.1.1" rowspan="2"><span class="ltx_text" id="S5.T1.1.1.1.1.1">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.1.1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.1.1.m1.1a"><mo id="S5.T1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T1.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.1.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T1.2.2.2.2" rowspan="2"><span class="ltx_text" id="S5.T1.2.2.2.2.1">FVD <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.2.2.2.2.1.m1.1"><semantics id="S5.T1.2.2.2.2.1.m1.1a"><mo id="S5.T1.2.2.2.2.1.m1.1.1" stretchy="false" xref="S5.T1.2.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.2.1.m1.1b"><ci id="S5.T1.2.2.2.2.1.m1.1.1.cmml" xref="S5.T1.2.2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.2.2.2.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T1.3.3.3.3" rowspan="2"><span class="ltx_text" id="S5.T1.3.3.3.3.1">COLMAP error<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.3.3.3.3.1.m1.1"><semantics id="S5.T1.3.3.3.3.1.m1.1a"><mo id="S5.T1.3.3.3.3.1.m1.1.1" stretchy="false" xref="S5.T1.3.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.3.1.m1.1b"><ci id="S5.T1.3.3.3.3.1.m1.1.1.cmml" xref="S5.T1.3.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.3.3.3.3.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T1.4.4.4.4">Rot. AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.4.4.4.4.m1.1"><semantics id="S5.T1.4.4.4.4.m1.1a"><mo id="S5.T1.4.4.4.4.m1.1.1" stretchy="false" xref="S5.T1.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.4.4.m1.1b"><ci id="S5.T1.4.4.4.4.m1.1.1.cmml" xref="S5.T1.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.4.4.4.4.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.5.5.5.5">Trans. AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.5.5.5.5.m1.1"><semantics id="S5.T1.5.5.5.5.m1.1a"><mo id="S5.T1.5.5.5.5.m1.1.1" stretchy="false" xref="S5.T1.5.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.5.5.m1.1b"><ci id="S5.T1.5.5.5.5.m1.1.1.cmml" xref="S5.T1.5.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.5.5.5.5.m1.1d">↑</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S5.T1.7.7.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.6.6.6.1">(@<math alttext="5^{\circ}/10^{\circ}/20^{\circ}" class="ltx_Math" display="inline" id="S5.T1.6.6.6.1.m1.1"><semantics id="S5.T1.6.6.6.1.m1.1a"><mrow id="S5.T1.6.6.6.1.m1.1.1" xref="S5.T1.6.6.6.1.m1.1.1.cmml"><msup id="S5.T1.6.6.6.1.m1.1.1.2" xref="S5.T1.6.6.6.1.m1.1.1.2.cmml"><mn id="S5.T1.6.6.6.1.m1.1.1.2.2" xref="S5.T1.6.6.6.1.m1.1.1.2.2.cmml">5</mn><mo id="S5.T1.6.6.6.1.m1.1.1.2.3" xref="S5.T1.6.6.6.1.m1.1.1.2.3.cmml">∘</mo></msup><mo id="S5.T1.6.6.6.1.m1.1.1.1" xref="S5.T1.6.6.6.1.m1.1.1.1.cmml">/</mo><msup id="S5.T1.6.6.6.1.m1.1.1.3" xref="S5.T1.6.6.6.1.m1.1.1.3.cmml"><mn id="S5.T1.6.6.6.1.m1.1.1.3.2" xref="S5.T1.6.6.6.1.m1.1.1.3.2.cmml">10</mn><mo id="S5.T1.6.6.6.1.m1.1.1.3.3" xref="S5.T1.6.6.6.1.m1.1.1.3.3.cmml">∘</mo></msup><mo id="S5.T1.6.6.6.1.m1.1.1.1a" xref="S5.T1.6.6.6.1.m1.1.1.1.cmml">/</mo><msup id="S5.T1.6.6.6.1.m1.1.1.4" xref="S5.T1.6.6.6.1.m1.1.1.4.cmml"><mn id="S5.T1.6.6.6.1.m1.1.1.4.2" xref="S5.T1.6.6.6.1.m1.1.1.4.2.cmml">20</mn><mo id="S5.T1.6.6.6.1.m1.1.1.4.3" xref="S5.T1.6.6.6.1.m1.1.1.4.3.cmml">∘</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.6.6.6.1.m1.1b"><apply id="S5.T1.6.6.6.1.m1.1.1.cmml" xref="S5.T1.6.6.6.1.m1.1.1"><divide id="S5.T1.6.6.6.1.m1.1.1.1.cmml" xref="S5.T1.6.6.6.1.m1.1.1.1"></divide><apply id="S5.T1.6.6.6.1.m1.1.1.2.cmml" xref="S5.T1.6.6.6.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.T1.6.6.6.1.m1.1.1.2.1.cmml" xref="S5.T1.6.6.6.1.m1.1.1.2">superscript</csymbol><cn id="S5.T1.6.6.6.1.m1.1.1.2.2.cmml" type="integer" xref="S5.T1.6.6.6.1.m1.1.1.2.2">5</cn><compose id="S5.T1.6.6.6.1.m1.1.1.2.3.cmml" xref="S5.T1.6.6.6.1.m1.1.1.2.3"></compose></apply><apply id="S5.T1.6.6.6.1.m1.1.1.3.cmml" xref="S5.T1.6.6.6.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T1.6.6.6.1.m1.1.1.3.1.cmml" xref="S5.T1.6.6.6.1.m1.1.1.3">superscript</csymbol><cn id="S5.T1.6.6.6.1.m1.1.1.3.2.cmml" type="integer" xref="S5.T1.6.6.6.1.m1.1.1.3.2">10</cn><compose id="S5.T1.6.6.6.1.m1.1.1.3.3.cmml" xref="S5.T1.6.6.6.1.m1.1.1.3.3"></compose></apply><apply id="S5.T1.6.6.6.1.m1.1.1.4.cmml" xref="S5.T1.6.6.6.1.m1.1.1.4"><csymbol cd="ambiguous" id="S5.T1.6.6.6.1.m1.1.1.4.1.cmml" xref="S5.T1.6.6.6.1.m1.1.1.4">superscript</csymbol><cn id="S5.T1.6.6.6.1.m1.1.1.4.2.cmml" type="integer" xref="S5.T1.6.6.6.1.m1.1.1.4.2">20</cn><compose id="S5.T1.6.6.6.1.m1.1.1.4.3.cmml" xref="S5.T1.6.6.6.1.m1.1.1.4.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.6.6.1.m1.1c">5^{\circ}/10^{\circ}/20^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.6.6.6.1.m1.1d">5 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT / 10 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT / 20 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center" id="S5.T1.7.7.7.2">(@<math alttext="5^{\circ}/10^{\circ}/20^{\circ}" class="ltx_Math" display="inline" id="S5.T1.7.7.7.2.m1.1"><semantics id="S5.T1.7.7.7.2.m1.1a"><mrow id="S5.T1.7.7.7.2.m1.1.1" xref="S5.T1.7.7.7.2.m1.1.1.cmml"><msup id="S5.T1.7.7.7.2.m1.1.1.2" xref="S5.T1.7.7.7.2.m1.1.1.2.cmml"><mn id="S5.T1.7.7.7.2.m1.1.1.2.2" xref="S5.T1.7.7.7.2.m1.1.1.2.2.cmml">5</mn><mo id="S5.T1.7.7.7.2.m1.1.1.2.3" xref="S5.T1.7.7.7.2.m1.1.1.2.3.cmml">∘</mo></msup><mo id="S5.T1.7.7.7.2.m1.1.1.1" xref="S5.T1.7.7.7.2.m1.1.1.1.cmml">/</mo><msup id="S5.T1.7.7.7.2.m1.1.1.3" xref="S5.T1.7.7.7.2.m1.1.1.3.cmml"><mn id="S5.T1.7.7.7.2.m1.1.1.3.2" xref="S5.T1.7.7.7.2.m1.1.1.3.2.cmml">10</mn><mo id="S5.T1.7.7.7.2.m1.1.1.3.3" xref="S5.T1.7.7.7.2.m1.1.1.3.3.cmml">∘</mo></msup><mo id="S5.T1.7.7.7.2.m1.1.1.1a" xref="S5.T1.7.7.7.2.m1.1.1.1.cmml">/</mo><msup id="S5.T1.7.7.7.2.m1.1.1.4" xref="S5.T1.7.7.7.2.m1.1.1.4.cmml"><mn id="S5.T1.7.7.7.2.m1.1.1.4.2" xref="S5.T1.7.7.7.2.m1.1.1.4.2.cmml">20</mn><mo id="S5.T1.7.7.7.2.m1.1.1.4.3" xref="S5.T1.7.7.7.2.m1.1.1.4.3.cmml">∘</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.7.2.m1.1b"><apply id="S5.T1.7.7.7.2.m1.1.1.cmml" xref="S5.T1.7.7.7.2.m1.1.1"><divide id="S5.T1.7.7.7.2.m1.1.1.1.cmml" xref="S5.T1.7.7.7.2.m1.1.1.1"></divide><apply id="S5.T1.7.7.7.2.m1.1.1.2.cmml" xref="S5.T1.7.7.7.2.m1.1.1.2"><csymbol cd="ambiguous" id="S5.T1.7.7.7.2.m1.1.1.2.1.cmml" xref="S5.T1.7.7.7.2.m1.1.1.2">superscript</csymbol><cn id="S5.T1.7.7.7.2.m1.1.1.2.2.cmml" type="integer" xref="S5.T1.7.7.7.2.m1.1.1.2.2">5</cn><compose id="S5.T1.7.7.7.2.m1.1.1.2.3.cmml" xref="S5.T1.7.7.7.2.m1.1.1.2.3"></compose></apply><apply id="S5.T1.7.7.7.2.m1.1.1.3.cmml" xref="S5.T1.7.7.7.2.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T1.7.7.7.2.m1.1.1.3.1.cmml" xref="S5.T1.7.7.7.2.m1.1.1.3">superscript</csymbol><cn id="S5.T1.7.7.7.2.m1.1.1.3.2.cmml" type="integer" xref="S5.T1.7.7.7.2.m1.1.1.3.2">10</cn><compose id="S5.T1.7.7.7.2.m1.1.1.3.3.cmml" xref="S5.T1.7.7.7.2.m1.1.1.3.3"></compose></apply><apply id="S5.T1.7.7.7.2.m1.1.1.4.cmml" xref="S5.T1.7.7.7.2.m1.1.1.4"><csymbol cd="ambiguous" id="S5.T1.7.7.7.2.m1.1.1.4.1.cmml" xref="S5.T1.7.7.7.2.m1.1.1.4">superscript</csymbol><cn id="S5.T1.7.7.7.2.m1.1.1.4.2.cmml" type="integer" xref="S5.T1.7.7.7.2.m1.1.1.4.2">20</cn><compose id="S5.T1.7.7.7.2.m1.1.1.4.3.cmml" xref="S5.T1.7.7.7.2.m1.1.1.4.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.7.2.m1.1c">5^{\circ}/10^{\circ}/20^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.7.7.7.2.m1.1d">5 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT / 10 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT / 20 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math>)</td>
</tr>
<tr class="ltx_tr" id="S5.T1.7.7.8.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.7.7.8.1.1">SVD</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.7.7.8.1.2">16.89</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.7.7.8.1.3">139.64</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.7.7.8.1.4">30.3%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.7.7.8.1.5">14.4 / 22.8 / 35.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.7.7.8.1.6">0.2 / 1.0 / 3.2</td>
</tr>
<tr class="ltx_tr" id="S5.T1.7.7.9.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.7.7.9.2.1">MotionCtrl</th>
<td class="ltx_td ltx_align_center" id="S5.T1.7.7.9.2.2">21.09</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.7.7.9.2.3">119.06</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.7.7.9.2.4">55.0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.7.7.9.2.5">8.6 / 13.9 / 22.2</td>
<td class="ltx_td ltx_align_center" id="S5.T1.7.7.9.2.6">0.6 / 2.1 / 5.7</td>
</tr>
<tr class="ltx_tr" id="S5.T1.7.7.10.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.7.7.10.3.1">CameraCtrl</th>
<td class="ltx_td ltx_align_center" id="S5.T1.7.7.10.3.2">14.69</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.7.7.10.3.3">105.41</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.7.7.10.3.4">19.3%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.7.7.10.3.5">21.4 / 32.9 / 48.4</td>
<td class="ltx_td ltx_align_center" id="S5.T1.7.7.10.3.6">0.3 / 1.3 / 4.4</td>
</tr>
<tr class="ltx_tr" id="S5.T1.7.7.11.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T1.7.7.11.4.1">Ours</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.7.7.11.4.2"><span class="ltx_text ltx_font_bold" id="S5.T1.7.7.11.4.2.1">11.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T1.7.7.11.4.3"><span class="ltx_text ltx_font_bold" id="S5.T1.7.7.11.4.3.1">55.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T1.7.7.11.4.4"><span class="ltx_text ltx_font_bold" id="S5.T1.7.7.11.4.4.1">14.4%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T1.7.7.11.4.5">
<span class="ltx_text ltx_font_bold" id="S5.T1.7.7.11.4.5.1">22.9 </span>/ <span class="ltx_text ltx_font_bold" id="S5.T1.7.7.11.4.5.2">34.5</span> /<span class="ltx_text ltx_font_bold" id="S5.T1.7.7.11.4.5.3"> 50.1</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.7.7.11.4.6">
<span class="ltx_text ltx_font_bold" id="S5.T1.7.7.11.4.6.1">5.1</span> /<span class="ltx_text ltx_font_bold" id="S5.T1.7.7.11.4.6.2"> 12.7</span> /<span class="ltx_text ltx_font_bold" id="S5.T1.7.7.11.4.6.3"> 24.6</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Quantitative comparison for 2-view video generation.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.8" style="width:368.6pt;height:133.7pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-40.4pt,14.6pt) scale(0.820127696935088,0.820127696935088) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T2.8.8">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.6.6.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.6.7" rowspan="2"><span class="ltx_text" id="S5.T2.6.6.6.7.1">Scenes</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.6.8" rowspan="2"><span class="ltx_text" id="S5.T2.6.6.6.8.1">Methods</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.1.1" rowspan="2"><span class="ltx_text" id="S5.T2.1.1.1.1.1">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.1.1.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T2.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.1.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.2.2.2.2" rowspan="2"><span class="ltx_text" id="S5.T2.2.2.2.2.1">FVD <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.2.2.2.2.1.m1.1"><semantics id="S5.T2.2.2.2.2.1.m1.1a"><mo id="S5.T2.2.2.2.2.1.m1.1.1" stretchy="false" xref="S5.T2.2.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.2.1.m1.1b"><ci id="S5.T2.2.2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.2.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.3.3.3.3">Rot. AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.3.3.3.3.m1.1"><semantics id="S5.T2.3.3.3.3.m1.1a"><mo id="S5.T2.3.3.3.3.m1.1.1" stretchy="false" xref="S5.T2.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.3.m1.1b"><ci id="S5.T2.3.3.3.3.m1.1.1.cmml" xref="S5.T2.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.3.3.3.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.4.4.4.4">Trans. AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.4.4.4.4.m1.1"><semantics id="S5.T2.4.4.4.4.m1.1a"><mo id="S5.T2.4.4.4.4.m1.1.1" stretchy="false" xref="S5.T2.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.4.m1.1b"><ci id="S5.T2.4.4.4.4.m1.1.1.cmml" xref="S5.T2.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.4.4.4.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.5.5.5.5" rowspan="2"><span class="ltx_text" id="S5.T2.5.5.5.5.1">Prec. <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.5.5.5.5.1.m1.1"><semantics id="S5.T2.5.5.5.5.1.m1.1a"><mo id="S5.T2.5.5.5.5.1.m1.1.1" stretchy="false" xref="S5.T2.5.5.5.5.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.5.5.1.m1.1b"><ci id="S5.T2.5.5.5.5.1.m1.1.1.cmml" xref="S5.T2.5.5.5.5.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.5.5.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.5.5.5.5.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.6.6" rowspan="2"><span class="ltx_text" id="S5.T2.6.6.6.6.1">MS. <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.6.6.6.6.1.m1.1"><semantics id="S5.T2.6.6.6.6.1.m1.1a"><mo id="S5.T2.6.6.6.6.1.m1.1.1" stretchy="false" xref="S5.T2.6.6.6.6.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.6.6.1.m1.1b"><ci id="S5.T2.6.6.6.6.1.m1.1.1.cmml" xref="S5.T2.6.6.6.6.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.6.6.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.6.6.6.6.1.m1.1d">↑</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.8.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.7.7.7.1">(@<math alttext="5^{\circ}/10^{\circ}/20^{\circ}" class="ltx_Math" display="inline" id="S5.T2.7.7.7.1.m1.1"><semantics id="S5.T2.7.7.7.1.m1.1a"><mrow id="S5.T2.7.7.7.1.m1.1.1" xref="S5.T2.7.7.7.1.m1.1.1.cmml"><msup id="S5.T2.7.7.7.1.m1.1.1.2" xref="S5.T2.7.7.7.1.m1.1.1.2.cmml"><mn id="S5.T2.7.7.7.1.m1.1.1.2.2" xref="S5.T2.7.7.7.1.m1.1.1.2.2.cmml">5</mn><mo id="S5.T2.7.7.7.1.m1.1.1.2.3" xref="S5.T2.7.7.7.1.m1.1.1.2.3.cmml">∘</mo></msup><mo id="S5.T2.7.7.7.1.m1.1.1.1" xref="S5.T2.7.7.7.1.m1.1.1.1.cmml">/</mo><msup id="S5.T2.7.7.7.1.m1.1.1.3" xref="S5.T2.7.7.7.1.m1.1.1.3.cmml"><mn id="S5.T2.7.7.7.1.m1.1.1.3.2" xref="S5.T2.7.7.7.1.m1.1.1.3.2.cmml">10</mn><mo id="S5.T2.7.7.7.1.m1.1.1.3.3" xref="S5.T2.7.7.7.1.m1.1.1.3.3.cmml">∘</mo></msup><mo id="S5.T2.7.7.7.1.m1.1.1.1a" xref="S5.T2.7.7.7.1.m1.1.1.1.cmml">/</mo><msup id="S5.T2.7.7.7.1.m1.1.1.4" xref="S5.T2.7.7.7.1.m1.1.1.4.cmml"><mn id="S5.T2.7.7.7.1.m1.1.1.4.2" xref="S5.T2.7.7.7.1.m1.1.1.4.2.cmml">20</mn><mo id="S5.T2.7.7.7.1.m1.1.1.4.3" xref="S5.T2.7.7.7.1.m1.1.1.4.3.cmml">∘</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.7.7.7.1.m1.1b"><apply id="S5.T2.7.7.7.1.m1.1.1.cmml" xref="S5.T2.7.7.7.1.m1.1.1"><divide id="S5.T2.7.7.7.1.m1.1.1.1.cmml" xref="S5.T2.7.7.7.1.m1.1.1.1"></divide><apply id="S5.T2.7.7.7.1.m1.1.1.2.cmml" xref="S5.T2.7.7.7.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.T2.7.7.7.1.m1.1.1.2.1.cmml" xref="S5.T2.7.7.7.1.m1.1.1.2">superscript</csymbol><cn id="S5.T2.7.7.7.1.m1.1.1.2.2.cmml" type="integer" xref="S5.T2.7.7.7.1.m1.1.1.2.2">5</cn><compose id="S5.T2.7.7.7.1.m1.1.1.2.3.cmml" xref="S5.T2.7.7.7.1.m1.1.1.2.3"></compose></apply><apply id="S5.T2.7.7.7.1.m1.1.1.3.cmml" xref="S5.T2.7.7.7.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T2.7.7.7.1.m1.1.1.3.1.cmml" xref="S5.T2.7.7.7.1.m1.1.1.3">superscript</csymbol><cn id="S5.T2.7.7.7.1.m1.1.1.3.2.cmml" type="integer" xref="S5.T2.7.7.7.1.m1.1.1.3.2">10</cn><compose id="S5.T2.7.7.7.1.m1.1.1.3.3.cmml" xref="S5.T2.7.7.7.1.m1.1.1.3.3"></compose></apply><apply id="S5.T2.7.7.7.1.m1.1.1.4.cmml" xref="S5.T2.7.7.7.1.m1.1.1.4"><csymbol cd="ambiguous" id="S5.T2.7.7.7.1.m1.1.1.4.1.cmml" xref="S5.T2.7.7.7.1.m1.1.1.4">superscript</csymbol><cn id="S5.T2.7.7.7.1.m1.1.1.4.2.cmml" type="integer" xref="S5.T2.7.7.7.1.m1.1.1.4.2">20</cn><compose id="S5.T2.7.7.7.1.m1.1.1.4.3.cmml" xref="S5.T2.7.7.7.1.m1.1.1.4.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.7.7.1.m1.1c">5^{\circ}/10^{\circ}/20^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.7.7.7.1.m1.1d">5 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT / 10 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT / 20 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.8.2">(@<math alttext="5^{\circ}/10^{\circ}/20^{\circ}" class="ltx_Math" display="inline" id="S5.T2.8.8.8.2.m1.1"><semantics id="S5.T2.8.8.8.2.m1.1a"><mrow id="S5.T2.8.8.8.2.m1.1.1" xref="S5.T2.8.8.8.2.m1.1.1.cmml"><msup id="S5.T2.8.8.8.2.m1.1.1.2" xref="S5.T2.8.8.8.2.m1.1.1.2.cmml"><mn id="S5.T2.8.8.8.2.m1.1.1.2.2" xref="S5.T2.8.8.8.2.m1.1.1.2.2.cmml">5</mn><mo id="S5.T2.8.8.8.2.m1.1.1.2.3" xref="S5.T2.8.8.8.2.m1.1.1.2.3.cmml">∘</mo></msup><mo id="S5.T2.8.8.8.2.m1.1.1.1" xref="S5.T2.8.8.8.2.m1.1.1.1.cmml">/</mo><msup id="S5.T2.8.8.8.2.m1.1.1.3" xref="S5.T2.8.8.8.2.m1.1.1.3.cmml"><mn id="S5.T2.8.8.8.2.m1.1.1.3.2" xref="S5.T2.8.8.8.2.m1.1.1.3.2.cmml">10</mn><mo id="S5.T2.8.8.8.2.m1.1.1.3.3" xref="S5.T2.8.8.8.2.m1.1.1.3.3.cmml">∘</mo></msup><mo id="S5.T2.8.8.8.2.m1.1.1.1a" xref="S5.T2.8.8.8.2.m1.1.1.1.cmml">/</mo><msup id="S5.T2.8.8.8.2.m1.1.1.4" xref="S5.T2.8.8.8.2.m1.1.1.4.cmml"><mn id="S5.T2.8.8.8.2.m1.1.1.4.2" xref="S5.T2.8.8.8.2.m1.1.1.4.2.cmml">20</mn><mo id="S5.T2.8.8.8.2.m1.1.1.4.3" xref="S5.T2.8.8.8.2.m1.1.1.4.3.cmml">∘</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.8.8.8.2.m1.1b"><apply id="S5.T2.8.8.8.2.m1.1.1.cmml" xref="S5.T2.8.8.8.2.m1.1.1"><divide id="S5.T2.8.8.8.2.m1.1.1.1.cmml" xref="S5.T2.8.8.8.2.m1.1.1.1"></divide><apply id="S5.T2.8.8.8.2.m1.1.1.2.cmml" xref="S5.T2.8.8.8.2.m1.1.1.2"><csymbol cd="ambiguous" id="S5.T2.8.8.8.2.m1.1.1.2.1.cmml" xref="S5.T2.8.8.8.2.m1.1.1.2">superscript</csymbol><cn id="S5.T2.8.8.8.2.m1.1.1.2.2.cmml" type="integer" xref="S5.T2.8.8.8.2.m1.1.1.2.2">5</cn><compose id="S5.T2.8.8.8.2.m1.1.1.2.3.cmml" xref="S5.T2.8.8.8.2.m1.1.1.2.3"></compose></apply><apply id="S5.T2.8.8.8.2.m1.1.1.3.cmml" xref="S5.T2.8.8.8.2.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T2.8.8.8.2.m1.1.1.3.1.cmml" xref="S5.T2.8.8.8.2.m1.1.1.3">superscript</csymbol><cn id="S5.T2.8.8.8.2.m1.1.1.3.2.cmml" type="integer" xref="S5.T2.8.8.8.2.m1.1.1.3.2">10</cn><compose id="S5.T2.8.8.8.2.m1.1.1.3.3.cmml" xref="S5.T2.8.8.8.2.m1.1.1.3.3"></compose></apply><apply id="S5.T2.8.8.8.2.m1.1.1.4.cmml" xref="S5.T2.8.8.8.2.m1.1.1.4"><csymbol cd="ambiguous" id="S5.T2.8.8.8.2.m1.1.1.4.1.cmml" xref="S5.T2.8.8.8.2.m1.1.1.4">superscript</csymbol><cn id="S5.T2.8.8.8.2.m1.1.1.4.2.cmml" type="integer" xref="S5.T2.8.8.8.2.m1.1.1.4.2">20</cn><compose id="S5.T2.8.8.8.2.m1.1.1.4.3.cmml" xref="S5.T2.8.8.8.2.m1.1.1.4.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.8.8.8.2.m1.1c">5^{\circ}/10^{\circ}/20^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.8.8.8.2.m1.1d">5 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT / 10 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT / 20 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math>)</td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.8.9.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.8.8.9.1.1" rowspan="4"><span class="ltx_text" id="S5.T2.8.8.9.1.1.1">Real10K</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.8.8.9.1.2">SVD</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.8.8.9.1.3">37.99</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.8.8.9.1.4">296.95</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.8.8.9.1.5">7.9 / 13.5 / 28.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.8.8.9.1.6">0.2 / 0.7 / 2.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.8.8.9.1.7">6.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.8.8.9.1.8">4.17</td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.8.10.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.10.2.1">MotionCtrl</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.10.2.2">29.23</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.10.2.3">277.05</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.10.2.4">8.1 / 16.5 / 29.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.10.2.5">1.5 / 5.3 / 16.1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.10.2.6">11.45</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.10.2.7">5.90</td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.8.11.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.11.3.1">CameraCtrl</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.11.3.2">12.57</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.11.3.3">131.32</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.11.3.4">22.4 / <span class="ltx_text ltx_font_bold" id="S5.T2.8.8.11.3.4.1">38.5</span> / <span class="ltx_text ltx_font_bold" id="S5.T2.8.8.11.3.4.2">56.2</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.11.3.5">0.6 / 2.5 / 8.2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.11.3.6">19.49</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.11.3.7">11.25</td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.8.12.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.12.4.1">Ours</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.12.4.2"><span class="ltx_text ltx_font_bold" id="S5.T2.8.8.12.4.2.1">8.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.12.4.3"><span class="ltx_text ltx_font_bold" id="S5.T2.8.8.12.4.3.1">94.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.12.4.4">
<span class="ltx_text ltx_font_bold" id="S5.T2.8.8.12.4.4.1">23.9</span> / 37.4 / 52.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.12.4.5">
<span class="ltx_text ltx_font_bold" id="S5.T2.8.8.12.4.5.1">3.3</span> / <span class="ltx_text ltx_font_bold" id="S5.T2.8.8.12.4.5.2">10.2</span> / <span class="ltx_text ltx_font_bold" id="S5.T2.8.8.12.4.5.3">23.5</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.12.4.6"><span class="ltx_text ltx_font_bold" id="S5.T2.8.8.12.4.6.1">29.39</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.12.4.7"><span class="ltx_text ltx_font_bold" id="S5.T2.8.8.12.4.7.1">15.22</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.8.13.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T2.8.8.13.5.1" rowspan="3"><span class="ltx_text" id="S5.T2.8.8.13.5.1.1">General</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.8.8.13.5.2">MotionCtrl</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.8.8.13.5.3">47.31</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.8.8.13.5.4">313.92</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.8.8.13.5.5">4.9 / 11.3 / 21.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.8.8.13.5.6">0.7 / 2.4 / 8.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.8.8.13.5.7">8.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.8.8.13.5.8">3.93</td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.8.14.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.14.6.1">CameraCtrl</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.14.6.2">26.71</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.14.6.3">221.23</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.14.6.4">14.1 / 26.9 / 43.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.14.6.5">0.5 / 1.7 / 5.7</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.14.6.6">15.13</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.14.6.7">7.35</td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.8.15.7">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T2.8.8.15.7.1">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.8.8.15.7.2"><span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.2.1">26.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T2.8.8.15.7.3"><span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.3.1">173.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T2.8.8.15.7.4">
<span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.4.1">19.7</span> / <span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.4.2">32.7 </span>/ <span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.4.3">48.4</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T2.8.8.15.7.5">
<span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.5.1">0.8 </span>/<span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.5.2"> 2.8</span> /<span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.5.3"> 8.7</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.8.8.15.7.6"><span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.6.1">33.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.8.8.15.7.7"><span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.7.1">19.96</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Multi-view Consistency</h4>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.3">Alongside evaluating the individual monocular frame pose accuracy using COLMAP-based metrics, we further assess the cross-video consistency of the corresponding frames from generated multi-view videos.
We randomly sample 1,000 videos, each with 27 frames, from RealEstate10k <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib84" title="">2018</a>)</cite> test set and convert each video into a two-view sequence with 14 frames per view.
The new camera pose sequences are generated by setting the 14th frame as the world origin and positioning the remaining frames relative to it. The scales of the scenes are normalized so that the maximum distance from the origin is 1.
Following CVD <cite class="ltx_cite ltx_citemacro_citep">(Kuang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib27" title="">2024</a>)</cite>, we adopt SuperGlue <cite class="ltx_cite ltx_citemacro_citep">(Sarlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib49" title="">2020</a>)</cite> to find correspondences and estimate the camera poses between each time-aligned set of frames.
SuperGlue not only measures angular errors in the rotation and translation but also computes the epipolar error of the matched correspondences.
We similarly collect the AUC for frame pairs with rotation and translation errors below specific thresholds (<math alttext="5^{\circ}" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S5.SS1.SSS0.Px2.p1.1.m1.1a"><msup id="S5.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.cmml"><mn id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.2" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml">5</mn><mo id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.3" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.1.m1.1b"><apply id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1">superscript</csymbol><cn id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml" type="integer" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.2">5</cn><compose id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.1.m1.1c">5^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px2.p1.1.m1.1d">5 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="10^{\circ}" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.2.m2.1"><semantics id="S5.SS1.SSS0.Px2.p1.2.m2.1a"><msup id="S5.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.cmml"><mn id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml">10</mn><mo id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.3" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.2.m2.1b"><apply id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1">superscript</csymbol><cn id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml" type="integer" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2">10</cn><compose id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.2.m2.1c">10^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px2.p1.2.m2.1d">10 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="20^{\circ}" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.3.m3.1"><semantics id="S5.SS1.SSS0.Px2.p1.3.m3.1a"><msup id="S5.SS1.SSS0.Px2.p1.3.m3.1.1" xref="S5.SS1.SSS0.Px2.p1.3.m3.1.1.cmml"><mn id="S5.SS1.SSS0.Px2.p1.3.m3.1.1.2" xref="S5.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml">20</mn><mo id="S5.SS1.SSS0.Px2.p1.3.m3.1.1.3" xref="S5.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.3.m3.1b"><apply id="S5.SS1.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.3.m3.1.1">superscript</csymbol><cn id="S5.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml" type="integer" xref="S5.SS1.SSS0.Px2.p1.3.m3.1.1.2">20</cn><compose id="S5.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S5.SS1.SSS0.Px2.p1.3.m3.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.3.m3.1c">20^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px2.p1.3.m3.1d">20 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math>). The epipolar errors for the estimated correspondences are summarized to the precision (P) and matching score (MS). As shown in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.T2" title="Table 2 ‣ 3D Consistency of Frames ‣ 5.1 Quantitative Comparisons ‣ 5 Experiments ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">2</span></a>, our method outperforms baselines greatly. The “Real10K” category means that the input images are taken from the corresponding RealEstate10K test sequence, while the “General” means that the input images are taken from 1,000 randomly sampled images in the test split of our monocular video dataset.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Visual Quality</h4>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px3.p1.1">To assess the frame perceptual quality, we evaluate visual quality using FID <cite class="ltx_cite ltx_citemacro_citep">(Heusel et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib15" title="">2017</a>)</cite> and FVD <cite class="ltx_cite ltx_citemacro_citep">(Unterthiner et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib62" title="">2018</a>)</cite>. FID and FVD measure the feature-space similarity of two sets of images and videos, respectively. In our case, they quantify the distribution distance between the generated frame sequences and the ground-truth frames. We provide monocular evaluations in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.T1" title="Table 1 ‣ 3D Consistency of Frames ‣ 5.1 Quantitative Comparisons ‣ 5 Experiments ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">1</span></a> and multi-view evaluations in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.T2" title="Table 2 ‣ 3D Consistency of Frames ‣ 5.1 Quantitative Comparisons ‣ 5 Experiments ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">2</span></a>.
As shown in these tables, our proposed framework enjoys the best visual quality. For both the “Real10K” and “General” categories, the ground-truth videos used to calculate these metrics are the video sequences corresponding to the input frames. These video sequences are from the test set split of the datasets and are not seen during training.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Qualtitative Comparison</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We provide qualitative comparisons on RealEstate10k <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib84" title="">2018</a>)</cite> scenes in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S4.F4" title="Figure 4 ‣ 4.2 Data Curation ‣ 4 Joint Training Strategy on Curated Data Mixtures ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">4</span></a> and text-to-image generated images in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.F5" title="Figure 5 ‣ 3D Consistency of Frames ‣ 5.1 Quantitative Comparisons ‣ 5 Experiments ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">5</span></a>. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S4.F4" title="Figure 4 ‣ 4.2 Data Curation ‣ 4 Joint Training Strategy on Curated Data Mixtures ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">4</span></a>, our method produces videos with precise camera control, whereas MotionCtrl tends to generate overly smooth trajectories that simplify the viewpoint instructions, and CameraCtrl suffers from severe distortions at novel viewpoints. For example, in the first case, the camera instruction involves multiple panning operations, first panning left and then panning right. Still, MotionCtrl only pans left, ignoring the rest of the instructions. CameraCtrl’s outputs, particularly in the first two cases, exhibit noticeable distortion, with the walls bending in the later frames.
Additionally, in the third and fourth cases, where the camera trajectories cover a long distance, both MotionCtrl and CameraCtrl produce unrealistic hallucinations, introducing artifacts such as merging indoor and outdoor pixels or distorting input pixels to compensate for a lack of generation ability.
In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.F5" title="Figure 5 ‣ 3D Consistency of Frames ‣ 5.1 Quantitative Comparisons ‣ 5 Experiments ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">5</span></a>, we observe that MotionCtrl and CameraCtrl tend to generate static scenes without any object motion. Although their methods produce realistic novel views, the synthesized objects remain static. In contrast, our method generates vivid object motion while maintaining accurate camera control. We highlight the object motion in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.F5" title="Figure 5 ‣ 3D Consistency of Frames ‣ 5.1 Quantitative Comparisons ‣ 5 Experiments ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">5</span></a> using auxiliary red lines. We encourage readers to view the supplementary videos for optimal visual comparisons.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Ablation Studies and Applications</h3>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Due to the space limit, we refer readers to the Appendix for ablation studies and applications of our framework. We provide detailed ablation studies in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A4" title="Appendix D Ablation Studies ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">D</span></a> on our proposed framework. Additionally, we explore the 3D reconstruction of our generated frames and four-view generation capabilities in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A5" title="Appendix E Applications ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">E</span></a>. Videos are included in the supplementary material for optimal qualitative comparison.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we propose Cavia, a novel framework for consistent multi-view camera-controllable video generation. Our framework incorporates cross-frame and cross-view attentions for effective camera controllability and view consistency. Our model benefits from joint training using static 3D scenes and objects, animatable objects, and in-the-wild monocular videos.
Extensive experiments demonstrate the superiority of our method over previous works in terms of geometric consistency and perceptual quality.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahmani et al. (2023)</span>
<span class="ltx_bibblock">
Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David B Lindell.

</span>
<span class="ltx_bibblock">4d-fy: Text-to-4d generation using hybrid score distillation sampling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2311.17984</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahmani et al. (2024)</span>
<span class="ltx_bibblock">
Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al.

</span>
<span class="ltx_bibblock">Vd3d: Taming large video diffusion transformers for 3d camera control.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2407.12781</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bain et al. (2021)</span>
<span class="ltx_bibblock">
Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Frozen in time: A joint video and image encoder for end-to-end retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  1728–1738, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blattmann et al. (2023)</span>
<span class="ltx_bibblock">
Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.

</span>
<span class="ltx_bibblock">Align your latents: High-resolution video synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  22563–22575, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Charatan et al. (2023)</span>
<span class="ltx_bibblock">
David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann.

</span>
<span class="ltx_bibblock">pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2312.12337</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024a)</span>
<span class="ltx_bibblock">
Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al.

</span>
<span class="ltx_bibblock">Panda-70m: Captioning 70m videos with multiple cross-modality teachers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  13320–13331, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Yabo Chen, Jiemin Fang, Yuyang Huang, Taoran Yi, Xiaopeng Zhang, Lingxi Xie, Xinggang Wang, Wenrui Dai, Hongkai Xiong, and Qi Tian.

</span>
<span class="ltx_bibblock">Cascade-zero123: One image to highly consistent 3d with self-prompted nearby views.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2312.04424</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024b)</span>
<span class="ltx_bibblock">
Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, and Huaping Liu.

</span>
<span class="ltx_bibblock">V3d: Video diffusion models are effective 3d generators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2403.06738</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deitke et al. (2023a)</span>
<span class="ltx_bibblock">
Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al.

</span>
<span class="ltx_bibblock">Objaverse-xl: A universe of 10m+ 3d objects.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2307.05663</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deitke et al. (2023b)</span>
<span class="ltx_bibblock">
Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi.

</span>
<span class="ltx_bibblock">Objaverse: A universe of annotated 3d objects.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  13142–13153, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2022)</span>
<span class="ltx_bibblock">
Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan.

</span>
<span class="ltx_bibblock">Depth-supervised nerf: Fewer views and faster training for free.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  12882–12891, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2024)</span>
<span class="ltx_bibblock">
Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan T Barron, and Ben Poole.

</span>
<span class="ltx_bibblock">Cat3d: Create anything in 3d with multi-view diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2405.10314</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2023)</span>
<span class="ltx_bibblock">
Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai.

</span>
<span class="ltx_bibblock">Animatediff: Animate your personalized text-to-image diffusion models without specific tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2307.04725</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2024)</span>
<span class="ltx_bibblock">
Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang.

</span>
<span class="ltx_bibblock">Cameractrl: Enabling camera control for text-to-video generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2404.02101</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heusel et al. (2017)</span>
<span class="ltx_bibblock">
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.

</span>
<span class="ltx_bibblock">Gans trained by a two time-scale update rule converge to a local nash equilibrium.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. (2022a)</span>
<span class="ltx_bibblock">
Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al.

</span>
<span class="ltx_bibblock">Imagen video: High definition video generation with diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2210.02303</em>, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. (2022b)</span>
<span class="ltx_bibblock">
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet.

</span>
<span class="ltx_bibblock">Video diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Advances in Neural Information Processing Systems</em>, 35:8633–8646, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al. (2024)</span>
<span class="ltx_bibblock">
Chen Hou, Guoqiang Wei, Yan Zeng, and Zhibo Chen.

</span>
<span class="ltx_bibblock">Training-free camera control for video generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2406.10126</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2021)</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2106.09685</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2024)</span>
<span class="ltx_bibblock">
Zehuan Huang, Hao Wen, Junting Dong, Yaohui Wang, Yangguang Li, Xinyuan Chen, Yan-Pei Cao, Ding Liang, Yu Qiao, Bo Dai, et al.

</span>
<span class="ltx_bibblock">Epidiff: Enhancing multi-view synthesis via localized epipolar-constrained diffusion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  9784–9794, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia (2020)</span>
<span class="ltx_bibblock">
Yan-Bin Jia.

</span>
<span class="ltx_bibblock">Plücker coordinates for lines in the space.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Problem Solver Techniques for Applied Computer Science, Com-S-477/577 Course Handout</em>, 3, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, and Yao Yao.

</span>
<span class="ltx_bibblock">Consistent4d: Consistent 360 <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib22.1.m1.1"><semantics id="bib.bib22.1.m1.1a"><mo id="bib.bib22.1.m1.1.1" stretchy="false" xref="bib.bib22.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib22.1.m1.1b"><ci id="bib.bib22.1.m1.1.1.cmml" xref="bib.bib22.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib22.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib22.1.m1.1d">{</annotation></semantics></math><math alttext="\backslash" class="ltx_Math" display="inline" id="bib.bib22.2.m2.1"><semantics id="bib.bib22.2.m2.1a"><mo id="bib.bib22.2.m2.1.1" xref="bib.bib22.2.m2.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="bib.bib22.2.m2.1b"><ci id="bib.bib22.2.m2.1.1.cmml" xref="bib.bib22.2.m2.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib22.2.m2.1c">\backslash</annotation><annotation encoding="application/x-llamapun" id="bib.bib22.2.m2.1d">\</annotation></semantics></math>deg<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib22.3.m3.1"><semantics id="bib.bib22.3.m3.1a"><mo id="bib.bib22.3.m3.1.1" stretchy="false" xref="bib.bib22.3.m3.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib22.3.m3.1b"><ci id="bib.bib22.3.m3.1.1.cmml" xref="bib.bib22.3.m3.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib22.3.m3.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib22.3.m3.1d">}</annotation></semantics></math> dynamic object generation from monocular video.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.4.1">arXiv preprint arXiv:2311.02848</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2024)</span>
<span class="ltx_bibblock">
Yanqin Jiang, Chaohui Yu, Chenjie Cao, Fan Wang, Weiming Hu, and Jin Gao.

</span>
<span class="ltx_bibblock">Animate3d: Animating any 3d model with multi-view video diffusion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2407.11398</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kant et al. (2024)</span>
<span class="ltx_bibblock">
Yash Kant, Aliaksandr Siarohin, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey Tulyakov, and Igor Gilitschenski.

</span>
<span class="ltx_bibblock">Spad: Spatially aware multi-view diffusers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  10026–10038, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karras et al. (2022)</span>
<span class="ltx_bibblock">
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.

</span>
<span class="ltx_bibblock">Elucidating the design space of diffusion-based generative models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Advances in Neural Information Processing Systems</em>, 35:26565–26577, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kerbl et al. (2023)</span>
<span class="ltx_bibblock">
Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis.

</span>
<span class="ltx_bibblock">3d gaussian splatting for real-time radiance field rendering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">ACM Transactions on Graphics (ToG)</em>, 42(4):1–14, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuang et al. (2024)</span>
<span class="ltx_bibblock">
Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon Wetzstein.

</span>
<span class="ltx_bibblock">Collaborative video diffusion: Consistent multi-video generation with camera control.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2405.17414</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwak et al. (2024)</span>
<span class="ltx_bibblock">
Jeong-gi Kwak, Erqun Dong, Yuhe Jin, Hanseok Ko, Shweta Mahajan, and Kwang Moo Yi.

</span>
<span class="ltx_bibblock">Vivid-1-to-3: Novel view synthesis with video diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  6775–6785, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024a)</span>
<span class="ltx_bibblock">
Bing Li, Cheng Zheng, Wenxuan Zhu, Jinjie Mai, Biao Zhang, Peter Wonka, and Bernard Ghanem.

</span>
<span class="ltx_bibblock">Vivid-zoo: Multi-view video generation with diffusion model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2406.08659</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi.

</span>
<span class="ltx_bibblock">Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">https://arxiv.org/abs/2311.06214</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024b)</span>
<span class="ltx_bibblock">
Peng Li, Yuan Liu, Xiaoxiao Long, Feihu Zhang, Cheng Lin, Mengfei Li, Xingqun Qi, Shanghang Zhang, Wenhan Luo, Ping Tan, et al.

</span>
<span class="ltx_bibblock">Era3d: High-resolution multiview diffusion using efficient row-wise attention.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2405.11616</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024c)</span>
<span class="ltx_bibblock">
Ruining Li, Chuanxia Zheng, Christian Rupprecht, and Andrea Vedaldi.

</span>
<span class="ltx_bibblock">Puppet-master: Scaling interactive video generation as a motion prior for part-level dynamics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2408.04631</em>, 2024c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2024)</span>
<span class="ltx_bibblock">
Hanwen Liang, Yuyang Yin, Dejia Xu, Hanxue Liang, Zhangyang Wang, Konstantinos N Plataniotis, Yao Zhao, and Yunchao Wei.

</span>
<span class="ltx_bibblock">Diffusion4d: Fast spatial-temporal consistent 4d generation via video diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2405.16645</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ling et al. (2023a)</span>
<span class="ltx_bibblock">
Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis.

</span>
<span class="ltx_bibblock">Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2312.13763</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ling et al. (2023b)</span>
<span class="ltx_bibblock">
Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al.

</span>
<span class="ltx_bibblock">Dl3dv-10k: A large-scale scene dataset for deep learning-based 3d vision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2312.16256</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024)</span>
<span class="ltx_bibblock">
Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su.

</span>
<span class="ltx_bibblock">One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.

</span>
<span class="ltx_bibblock">Zero-1-to-3: Zero-shot one image to 3d object.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2303.11328</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang.

</span>
<span class="ltx_bibblock">Syncdreamer: Generating multiview-consistent images from a single-view image.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2309.03453</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long et al. (2024)</span>
<span class="ltx_bibblock">
Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al.

</span>
<span class="ltx_bibblock">Wonder3d: Single image to 3d using cross-domain diffusion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  9970–9980, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2024)</span>
<span class="ltx_bibblock">
Yuanxun Lu, Jingyang Zhang, Shiwei Li, Tian Fang, David McKinnon, Yanghai Tsin, Long Quan, Xun Cao, and Yao Yao.

</span>
<span class="ltx_bibblock">Direct2. 5: Diverse text-to-3d generation via multi-view 2.5 d diffusion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  8744–8753, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Melas-Kyriazi et al. (2024)</span>
<span class="ltx_bibblock">
Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, and Filippos Kokkinos.

</span>
<span class="ltx_bibblock">Im-3d: Iterative multiview diffusion and reconstruction for high-quality 3d generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">International Conference on Machine Learning, 2024</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nan et al. (2024)</span>
<span class="ltx_bibblock">
Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai.

</span>
<span class="ltx_bibblock">Openvid-1m: A large-scale high-quality dataset for text-to-video generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2407.02371</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(43)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Sora — openai.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/sora/" title="">https://openai.com/index/sora/</a>.

</span>
<span class="ltx_bibblock">(Accessed on 09/22/2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. (2024)</span>
<span class="ltx_bibblock">
Zijie Pan, Zeyu Yang, Xiatian Zhu, and Li Zhang.

</span>
<span class="ltx_bibblock">Fast dynamic 3d object generation from a single-view video.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2401.08742</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reizenstein et al. (2021)</span>
<span class="ltx_bibblock">
Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny.

</span>
<span class="ltx_bibblock">Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, pp.  10901–10911, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2023)</span>
<span class="ltx_bibblock">
Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu.

</span>
<span class="ltx_bibblock">Dreamgaussian4d: Generative 4d gaussian splatting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2312.17142</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2024)</span>
<span class="ltx_bibblock">
Jiawei Ren, Kevin Xie, Ashkan Mirzaei, Hanxue Liang, Xiaohui Zeng, Karsten Kreis, Ziwei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, et al.

</span>
<span class="ltx_bibblock">L4gm: Large 4d gaussian reconstruction model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:2406.10324</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al. (2022)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  10684–10695, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarlin et al. (2020)</span>
<span class="ltx_bibblock">
Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich.

</span>
<span class="ltx_bibblock">Superglue: Learning feature matching with graph neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  4938–4947, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schönberger &amp; Frahm (2016)</span>
<span class="ltx_bibblock">
Johannes Lutz Schönberger and Jan-Michael Frahm.

</span>
<span class="ltx_bibblock">Structure-from-motion revisited.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schönberger et al. (2016)</span>
<span class="ltx_bibblock">
Johannes Lutz Schönberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm.

</span>
<span class="ltx_bibblock">Pixelwise view selection for unstructured multi-view stereo.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">European Conference on Computer Vision (ECCV)</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuhmann et al. (2022)</span>
<span class="ltx_bibblock">
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.

</span>
<span class="ltx_bibblock">Laion-5b: An open large-scale dataset for training next generation image-text models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:2210.08402</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2023a)</span>
<span class="ltx_bibblock">
Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su.

</span>
<span class="ltx_bibblock">Zero123++: a single image to consistent multi-view diffusion base model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2310.15110</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2023b)</span>
<span class="ltx_bibblock">
Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang.

</span>
<span class="ltx_bibblock">Mvdream: Multi-view diffusion for 3d generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2308.16512</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singer et al. (2022)</span>
<span class="ltx_bibblock">
Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al.

</span>
<span class="ltx_bibblock">Make-a-video: Text-to-video generation without text-video data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">arXiv preprint arXiv:2209.14792</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singer et al. (2023)</span>
<span class="ltx_bibblock">
Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, et al.

</span>
<span class="ltx_bibblock">Text-to-4d dynamic scene generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2301.11280</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sitzmann et al. (2021)</span>
<span class="ltx_bibblock">
Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand.

</span>
<span class="ltx_bibblock">Light field networks: Neural scene representations with single-evaluation rendering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Advances in Neural Information Processing Systems</em>, 34:19313–19325, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stability (2023)</span>
<span class="ltx_bibblock">
Stability.

</span>
<span class="ltx_bibblock">Stable video diffusion: Scaling latent video diffusion models to large datasets.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets" title="">https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2023)</span>
<span class="ltx_bibblock">
Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and Yasutaka Furukawa.

</span>
<span class="ltx_bibblock">Mvdiffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">arXiv</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2024)</span>
<span class="ltx_bibblock">
Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas Chandra, Yasutaka Furukawa, and Rakesh Ranjan.

</span>
<span class="ltx_bibblock">Mvdiffusion++: A dense high-resolution multi-view diffusion model for single or sparse-view 3d object reconstruction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">arXiv preprint arXiv:2402.12712</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tseng et al. (2023)</span>
<span class="ltx_bibblock">
Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, Jia-Bin Huang, and Johannes Kopf.

</span>
<span class="ltx_bibblock">Consistent view synthesis with pose-guided diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">CVPR</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Unterthiner et al. (2018)</span>
<span class="ltx_bibblock">
Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly.

</span>
<span class="ltx_bibblock">Towards accurate generative models of video: A new metric &amp; challenges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">arXiv preprint arXiv:1812.01717</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voleti et al. (2024)</span>
<span class="ltx_bibblock">
Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani.

</span>
<span class="ltx_bibblock">Sv3d: Novel multi-view synthesis and 3d generation from a single image using latent video diffusion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2403.12008</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023a)</span>
<span class="ltx_bibblock">
Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu.

</span>
<span class="ltx_bibblock">Videofactory: Swap attention in spatiotemporal diffusions for text-to-video generation.

</span>
<span class="ltx_bibblock">2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023b)</span>
<span class="ltx_bibblock">
Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, et al.

</span>
<span class="ltx_bibblock">Internvid: A large-scale video-text dataset for multimodal understanding and generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2307.06942</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023c)</span>
<span class="ltx_bibblock">
Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan.

</span>
<span class="ltx_bibblock">Motionctrl: A unified and flexible motion controller for video generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv preprint arXiv:2312.03641</em>, 2023c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Watson et al. (2022)</span>
<span class="ltx_bibblock">
Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi.

</span>
<span class="ltx_bibblock">Novel view synthesis with diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">arXiv preprint arXiv:2210.04628</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Watson et al. (2024)</span>
<span class="ltx_bibblock">
Daniel Watson, Saurabh Saxena, Lala Li, Andrea Tagliasacchi, and David J Fleet.

</span>
<span class="ltx_bibblock">Controlling space and time with diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">arXiv preprint arXiv:2407.07860</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al. (2024)</span>
<span class="ltx_bibblock">
Hongchi Xia, Yang Fu, Sifei Liu, and Xiaolong Wang.

</span>
<span class="ltx_bibblock">Rgbd objects in the wild: Scaling real-world 3d object learning from rgb-d videos.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2401.12592</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2024)</span>
<span class="ltx_bibblock">
Yiming Xie, Chun-Han Yao, Vikram Voleti, Huaizu Jiang, and Varun Jampani.

</span>
<span class="ltx_bibblock">Sv4d: Dynamic 3d content generation with multi-frame and multi-view consistency.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">arXiv preprint arXiv:2407.17470</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024)</span>
<span class="ltx_bibblock">
Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat.

</span>
<span class="ltx_bibblock">Camco: Camera-controllable 3d-consistent image-to-video generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">arXiv preprint arXiv:2406.02509</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. (2021)</span>
<span class="ltx_bibblock">
Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas.

</span>
<span class="ltx_bibblock">Videogpt: Video generation using vq-vae and transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">arXiv preprint arXiv:2104.10157</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024a)</span>
<span class="ltx_bibblock">
Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hongdong Li.

</span>
<span class="ltx_bibblock">Consistnet: Enforcing 3d consistency for multi-view images diffusion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  7079–7088, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024b)</span>
<span class="ltx_bibblock">
Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al.

</span>
<span class="ltx_bibblock">Cogvideox: Text-to-video diffusion models with an expert transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">arXiv preprint arXiv:2408.06072</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. (2023)</span>
<span class="ltx_bibblock">
Yuyang Yin, Dejia Xu, Zhangyang Wang, Yao Zhao, and Yunchao Wei.

</span>
<span class="ltx_bibblock">4dgen: Grounded 4d content generation with spatial-temporal consistency.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">arXiv preprint arXiv:2312.17225</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, et al.

</span>
<span class="ltx_bibblock">Mvimgnet: A large-scale dataset of multi-view images.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  9150–9161, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. (2024)</span>
<span class="ltx_bibblock">
Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, and Yao Yao.

</span>
<span class="ltx_bibblock">Stag4d: Spatial-temporal anchored generative 4d gaussians.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">arXiv preprint arXiv:2403.14939</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, and Yu Qiao.

</span>
<span class="ltx_bibblock">4diffusion: Multi-view video diffusion model for 4d generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">arXiv preprint arXiv:2405.20674</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang &amp; Agrawala (2023)</span>
<span class="ltx_bibblock">
Lvmin Zhang and Maneesh Agrawala.

</span>
<span class="ltx_bibblock">Adding conditional control to text-to-image diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">arXiv preprint arXiv:2302.05543</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2018)</span>
<span class="ltx_bibblock">
Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.

</span>
<span class="ltx_bibblock">The unreasonable effectiveness of deep features as a perceptual metric.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">CVPR</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2022)</span>
<span class="ltx_bibblock">
Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu.

</span>
<span class="ltx_bibblock">Particlesfm: Exploiting dense point trajectories for localizing moving cameras in the wild.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">European Conference on Computer Vision</em>, pp.  523–542. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023)</span>
<span class="ltx_bibblock">
Yuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhenguo Li, and Gim Hee Lee.

</span>
<span class="ltx_bibblock">Animate124: Animating one image to 4d dynamic scene.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">arXiv preprint arXiv:2311.14603</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2023)</span>
<span class="ltx_bibblock">
Yufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Otmar Hilliges, and Shalini De Mello.

</span>
<span class="ltx_bibblock">A unified approach for text-and image-guided 4d scene generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">arXiv preprint arXiv:2311.16854</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2018)</span>
<span class="ltx_bibblock">
Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely.

</span>
<span class="ltx_bibblock">Stereo magnification: Learning view synthesis using multiplane images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">arXiv preprint arXiv:1805.09817</em>, 2018.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional Implementation Details</h2>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p" id="A1.p1.2">Our training is divided into static stage and dynamic stage.
Our static stage is trained for around 500k iterations and our dynamic stage is trained for roughly 300k iterations.
The effective batch size is 128 and the learning rate is 1e-4. Our video length is 14 frames for each view with the first frame shared across views. Our model is fine-tuned at <math alttext="256\times 256" class="ltx_Math" display="inline" id="A1.p1.1.m1.1"><semantics id="A1.p1.1.m1.1a"><mrow id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml"><mn id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2.cmml">256</mn><mo id="A1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.p1.1.m1.1.1.1.cmml">×</mo><mn id="A1.p1.1.m1.1.1.3" xref="A1.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1"><times id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1.1"></times><cn id="A1.p1.1.m1.1.1.2.cmml" type="integer" xref="A1.p1.1.m1.1.1.2">256</cn><cn id="A1.p1.1.m1.1.1.3.cmml" type="integer" xref="A1.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="A1.p1.1.m1.1d">256 × 256</annotation></semantics></math> spatial resolution from the SVD 1.0 checkpoint. The training data are prepared by first center-cropping the original videos and then resizing each frame to the shape of <math alttext="256\times 256" class="ltx_Math" display="inline" id="A1.p1.2.m2.1"><semantics id="A1.p1.2.m2.1a"><mrow id="A1.p1.2.m2.1.1" xref="A1.p1.2.m2.1.1.cmml"><mn id="A1.p1.2.m2.1.1.2" xref="A1.p1.2.m2.1.1.2.cmml">256</mn><mo id="A1.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.p1.2.m2.1.1.1.cmml">×</mo><mn id="A1.p1.2.m2.1.1.3" xref="A1.p1.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.2.m2.1b"><apply id="A1.p1.2.m2.1.1.cmml" xref="A1.p1.2.m2.1.1"><times id="A1.p1.2.m2.1.1.1.cmml" xref="A1.p1.2.m2.1.1.1"></times><cn id="A1.p1.2.m2.1.1.2.cmml" type="integer" xref="A1.p1.2.m2.1.1.2">256</cn><cn id="A1.p1.2.m2.1.1.3.cmml" type="integer" xref="A1.p1.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.2.m2.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="A1.p1.2.m2.1d">256 × 256</annotation></semantics></math>. In the dynamic stage, 30% of iterations are used to train on monocular videos. During static training, the strides of frames are randomly sampled in the range of <span class="ltx_text ltx_font_typewriter" id="A1.p1.2.1">[1, 8]</span>. For monocular videos, the strides are sampled in the range of <span class="ltx_text ltx_font_typewriter" id="A1.p1.2.2">[1, 2]</span>. For dynamic multi-view object renderings, the strides are fixed to 1 to use all rendered frames since we already introduced randomness in the frame rate during rendering.
At inference time, the decoding chunk is set to 14 so all frames are decoded altogether. We sample 25 steps to obtain all our results.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Data Curation Details</h2>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">In this section, we provide additional details on our data processing and curation pipelines.</p>
</div>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Static 3D Objects</h4>
<div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p1.1">Our static objects data comprises multi-view images rendered from the Objaverse <cite class="ltx_cite ltx_citemacro_citep">(Deitke et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib10" title="">2023b</a>)</cite> and Objaverse-XL<cite class="ltx_cite ltx_citemacro_citep">(Deitke et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib9" title="">2023a</a>)</cite> dataset. Similar to InstantMesh, we use a filtered high-quality subset of the original dataset to train our model. The filtering goal is to remove objects that satisfy any of the following criteria: (<span class="ltx_ERROR undefined" id="A2.SS0.SSS0.Px1.p1.1.1">\romannum</span>1) objects without texture maps, (<span class="ltx_ERROR undefined" id="A2.SS0.SSS0.Px1.p1.1.2">\romannum</span>2) objects with rendered images occupying less than 10% of the view from any angle, (<span class="ltx_ERROR undefined" id="A2.SS0.SSS0.Px1.p1.1.3">\romannum</span>3) including multiple separate objects, (<span class="ltx_ERROR undefined" id="A2.SS0.SSS0.Px1.p1.1.4">\romannum</span>4) objects with no caption information provided by the Cap3D dataset, and (<span class="ltx_ERROR undefined" id="A2.SS0.SSS0.Px1.p1.1.5">\romannum</span>5) low-quality objects. The classification of “low-quality” objects is determined based on the presence of tags such as “lowpoly” and its variants (e.g., “low poly”) in the metadata. By applying our filtering criteria, we curated approximately 400k high-quality instances from the initial pool of 800k objects in the Objaverse dataset.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p2.1">For each 3D object, we use Blender’s EEVEE renderer to render an 84-frame RGBA orbit at <math alttext="512\times 512" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p2.1.m1.1"><semantics id="A2.SS0.SSS0.Px1.p2.1.m1.1a"><mrow id="A2.SS0.SSS0.Px1.p2.1.m1.1.1" xref="A2.SS0.SSS0.Px1.p2.1.m1.1.1.cmml"><mn id="A2.SS0.SSS0.Px1.p2.1.m1.1.1.2" xref="A2.SS0.SSS0.Px1.p2.1.m1.1.1.2.cmml">512</mn><mo id="A2.SS0.SSS0.Px1.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A2.SS0.SSS0.Px1.p2.1.m1.1.1.1.cmml">×</mo><mn id="A2.SS0.SSS0.Px1.p2.1.m1.1.1.3" xref="A2.SS0.SSS0.Px1.p2.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p2.1.m1.1b"><apply id="A2.SS0.SSS0.Px1.p2.1.m1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p2.1.m1.1.1"><times id="A2.SS0.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p2.1.m1.1.1.1"></times><cn id="A2.SS0.SSS0.Px1.p2.1.m1.1.1.2.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p2.1.m1.1.1.2">512</cn><cn id="A2.SS0.SSS0.Px1.p2.1.m1.1.1.3.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p2.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p2.1.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px1.p2.1.m1.1d">512 × 512</annotation></semantics></math> resolution. we adaptively position the camera to a distance sufficient to ensure that the rendered object content makes good and consistent use of the image extents without being clipped in any view. For each frame, the azimuths can be irregularly spaced, and the elevation can vary per view. Specifically, the sequence of camera elevations for each orbit is obtained from a random weighted combination of sinusoids with different frequencies. The azimuth angles are sampled regularly, and then a small amount of noise is added to make them irregular. The elevation values are smoothed using a simple convolution kernel and then clamped to a maximum elevation of 89 degrees.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Static 3D Scenes</h4>
<div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p1.1">Our static scenes data are sourced from RealEstate10k <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib84" title="">2018</a>)</cite>, WildRGBD <cite class="ltx_cite ltx_citemacro_citep">(Xia et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib69" title="">2024</a>)</cite>, MVImgNet <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib76" title="">2023</a>)</cite>, CO3Dv2 <cite class="ltx_cite ltx_citemacro_citep">(Reizenstein et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib45" title="">2021</a>)</cite>, and DL3DV-10K <cite class="ltx_cite ltx_citemacro_citep">(Ling et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib35" title="">2023b</a>)</cite>.
For RealEstate10k, we use the train/test split released by PixelSplat <cite class="ltx_cite ltx_citemacro_citep">(Charatan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib5" title="">2023</a>)</cite>. During training, we sample every 8 original frames to construct the training sequences.
For DL3DV-10K, we construct training sequences from the publicly available 7k subset. Since each video is very long for the DL3DV-10k dataset, we offline randomly sample multiple sequences from a single ground truth video to obtain multiple training data items.
For CO3Dv2, we remove the video sequences that contain whole-black images to avoid temporally inconsistent frames.
For WildRGBD and MVImgNet we use all classes available and removed sequences whose lengths are not enough for two-view training (shorter than 27 frames).</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Dynamic 3D Objects</h4>
<div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p1.1">Our dynamic 3D objects are similarly rendered as the static 3D objects. The filtering pipelines remain mostly the same as the static objects, except that we introduce additional workflows to consider object motion. Inspired by previous works <cite class="ltx_cite ltx_citemacro_citep">(Liang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib33" title="">2024</a>; Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib23" title="">2024</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib29" title="">2024a</a>)</cite> that employ animatable objects from Objaverse. We render multiple fixed-view videos to examine the motion quality of the objects. We utilize lpips <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib80" title="">2018</a>)</cite> to measure the similarity of nearby frames and consider an object to be static if lpips similarity is above a certain threshold. Additionally, we render the alpha masks of the object and use this as an indicator of whether the object has moved out of the visible regions. Consequently, we remove objects with too large or sudden movements as well as objects with little-to-no motion. These filterings result in 19,000 objects. Our rendering strategy is also very similar to that of static 3D objects, introducing random elevation and azimuth changes to complicate the trajectories, except that we additionally introduce a random frame stride at rendering to augment the object motion. The stride is sampled individually for each object from the range <span class="ltx_text ltx_font_typewriter" id="A2.SS0.SSS0.Px3.p1.1.1">[1, 3]</span>. A larger the stride leads to renderings with faster object motion.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Monocular Videos</h4>
<div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p1.1">Our monocular video filtering pipeline involves filtering according to Particle-SfM output, OCR, aesthetic score, and camera motion. As mentioned in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S4.SS2" title="4.2 Data Curation ‣ 4 Joint Training Strategy on Curated Data Mixtures ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">4.2</span></a>, we first attempt to annotate the camera poses for the video frames using Particle-SfM <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib81" title="">2022</a>)</cite>. Take InternVid <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib65" title="">2023b</a>)</cite> as an example, roughly 10 million video clips are processed and around 3 million samples are successfully processed by Particle-SfM. For each video, we start from the first frame and randomly select a frame stride of 1 or 2. The total number of images sent to Particle-SfM is 32 images. Our point count filtering is empirically implemented as a cut-off at 1,000 points and 40,000 points. Point clouds with too few points are removed due to the concern that the frames are poorly registered. Point clouds with too many points are avoided because their limited object motion. This aggressive filtering results in around 2 million samples for further processing. We then evaluate all the video clips using OCR detection algorithms and remove the samples whose detected text regions are larger then <math alttext="{10^{-4}}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px4.p1.1.m1.1"><semantics id="A2.SS0.SSS0.Px4.p1.1.m1.1a"><msup id="A2.SS0.SSS0.Px4.p1.1.m1.1.1" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.cmml"><mn id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.2" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml">10</mn><mrow id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml"><mo id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3a" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml">−</mo><mn id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3.2" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px4.p1.1.m1.1b"><apply id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.cmml" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1">superscript</csymbol><cn id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml" type="integer" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.2">10</cn><apply id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3"><minus id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3.1.cmml" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3"></minus><cn id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3.2.cmml" type="integer" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px4.p1.1.m1.1c">{10^{-4}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px4.p1.1.m1.1d">10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> of the image resolution (<span class="ltx_text ltx_font_italic" id="A2.SS0.SSS0.Px4.p1.1.1">i.e.</span> 6 pixels). This process results in 604,000 samples. The next step is filtering with aesthetic scores and videos with aesthetic score annotations smaller than 4 are removed. 467,000 videos are left after these filtering process. Finally, we employ a camera motion classifier extended from the Open-Sora pipeline<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/hpcaitech/Open-Sora/tree/main/tools/caption/camera_motion" title="">https://github.com/hpcaitech/Open-Sora/tree/main/tools/caption/camera_motion</a></span></span></span>. The main motivation is that optical-flow on consecutive frames can be summarized to a global motion vector, assuming the most parts of the scene is moving in a uniform direction. Optical flow is first obtained using <span class="ltx_text ltx_font_typewriter" id="A2.SS0.SSS0.Px4.p1.1.2">cv2.calcOpticalFlowFarneback</span> for each consecutive frame pairs. Then, the magnitudes and directions are calculated via <span class="ltx_text ltx_font_typewriter" id="A2.SS0.SSS0.Px4.p1.1.3">cv2.cartToPolar</span>. These magnitudes and directions are classified into 8 categories: static, zoom out, zoom in, pan left, tilt up, pan right, tilt down, and unknown. The results of the frame pairs are summarized to obtain the final result of each video clip. When a certain type appears more than 50%, the type for the whole video clip is determined directly. We aggressively classify a video clip as static if any of its frame pairs is categorized into static or unknown. Finally, we obtain 355,000 clips that satisfy our needs. The process is similarly applied to OpenVid <cite class="ltx_cite ltx_citemacro_citep">(Nan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib42" title="">2024</a>)</cite>’s Panda-70M subset <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib6" title="">2024a</a>)</cite> and we obtained 38,000 clips. In summary, our monocular video dataset consists of 393,000 clips.</p>
</div>
<figure class="ltx_table" id="A2.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Ablation Studies on each of our introduced modules. “w/o Plucker” refers to replacing the Plucker coordinate conditioning with one-dimensional conditioning as in MotionCtrl. “w/o Cross-frame” refers to replacing the Cross-frame attention with vanilla 1D temporal attention. “w/o Cross-view” refers to replacing the Cross-view attention with vanilla spatial attention. “Ours (Static)” means the model is only trained on static video datasets. “Ours (w/o Mono)” means that the model is fine-tuned on synthetic multi-view datasets, but is not trained with monocular video datasets. “Ours (Full)” means that the model is trained on all available data sources.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T3.8" style="width:433.6pt;height:183.7pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-18.1pt,7.6pt) scale(0.923139723404239,0.923139723404239) ;">
<table class="ltx_tabular ltx_align_middle" id="A2.T3.8.8">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T3.6.6.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.6.6.6.7" rowspan="2"><span class="ltx_text" id="A2.T3.6.6.6.7.1">Scenes</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.6.6.6.8" rowspan="2"><span class="ltx_text" id="A2.T3.6.6.6.8.1">Methods</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.1.1.1.1" rowspan="2"><span class="ltx_text" id="A2.T3.1.1.1.1.1">FID<math alttext="\downarrow" class="ltx_Math" display="inline" id="A2.T3.1.1.1.1.1.m1.1"><semantics id="A2.T3.1.1.1.1.1.m1.1a"><mo id="A2.T3.1.1.1.1.1.m1.1.1" stretchy="false" xref="A2.T3.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A2.T3.1.1.1.1.1.m1.1b"><ci id="A2.T3.1.1.1.1.1.m1.1.1.cmml" xref="A2.T3.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T3.1.1.1.1.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.2.2.2.2" rowspan="2"><span class="ltx_text" id="A2.T3.2.2.2.2.1">FVD<math alttext="\downarrow" class="ltx_Math" display="inline" id="A2.T3.2.2.2.2.1.m1.1"><semantics id="A2.T3.2.2.2.2.1.m1.1a"><mo id="A2.T3.2.2.2.2.1.m1.1.1" stretchy="false" xref="A2.T3.2.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A2.T3.2.2.2.2.1.m1.1b"><ci id="A2.T3.2.2.2.2.1.m1.1.1.cmml" xref="A2.T3.2.2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.2.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T3.2.2.2.2.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.3.3.3.3">Rot. AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="A2.T3.3.3.3.3.m1.1"><semantics id="A2.T3.3.3.3.3.m1.1a"><mo id="A2.T3.3.3.3.3.m1.1.1" stretchy="false" xref="A2.T3.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A2.T3.3.3.3.3.m1.1b"><ci id="A2.T3.3.3.3.3.m1.1.1.cmml" xref="A2.T3.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T3.3.3.3.3.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.4.4.4.4">Trans. AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="A2.T3.4.4.4.4.m1.1"><semantics id="A2.T3.4.4.4.4.m1.1a"><mo id="A2.T3.4.4.4.4.m1.1.1" stretchy="false" xref="A2.T3.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A2.T3.4.4.4.4.m1.1b"><ci id="A2.T3.4.4.4.4.m1.1.1.cmml" xref="A2.T3.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T3.4.4.4.4.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.5.5.5.5" rowspan="2"><span class="ltx_text" id="A2.T3.5.5.5.5.1">Prec. <math alttext="\uparrow" class="ltx_Math" display="inline" id="A2.T3.5.5.5.5.1.m1.1"><semantics id="A2.T3.5.5.5.5.1.m1.1a"><mo id="A2.T3.5.5.5.5.1.m1.1.1" stretchy="false" xref="A2.T3.5.5.5.5.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A2.T3.5.5.5.5.1.m1.1b"><ci id="A2.T3.5.5.5.5.1.m1.1.1.cmml" xref="A2.T3.5.5.5.5.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.5.5.5.5.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T3.5.5.5.5.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.6.6.6.6" rowspan="2"><span class="ltx_text" id="A2.T3.6.6.6.6.1">MS. <math alttext="\uparrow" class="ltx_Math" display="inline" id="A2.T3.6.6.6.6.1.m1.1"><semantics id="A2.T3.6.6.6.6.1.m1.1a"><mo id="A2.T3.6.6.6.6.1.m1.1.1" stretchy="false" xref="A2.T3.6.6.6.6.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A2.T3.6.6.6.6.1.m1.1b"><ci id="A2.T3.6.6.6.6.1.m1.1.1.cmml" xref="A2.T3.6.6.6.6.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.6.6.6.6.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T3.6.6.6.6.1.m1.1d">↑</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.7.7.7.1">(@<math alttext="5^{\circ}/10^{\circ}/20^{\circ}" class="ltx_Math" display="inline" id="A2.T3.7.7.7.1.m1.1"><semantics id="A2.T3.7.7.7.1.m1.1a"><mrow id="A2.T3.7.7.7.1.m1.1.1" xref="A2.T3.7.7.7.1.m1.1.1.cmml"><msup id="A2.T3.7.7.7.1.m1.1.1.2" xref="A2.T3.7.7.7.1.m1.1.1.2.cmml"><mn id="A2.T3.7.7.7.1.m1.1.1.2.2" xref="A2.T3.7.7.7.1.m1.1.1.2.2.cmml">5</mn><mo id="A2.T3.7.7.7.1.m1.1.1.2.3" xref="A2.T3.7.7.7.1.m1.1.1.2.3.cmml">∘</mo></msup><mo id="A2.T3.7.7.7.1.m1.1.1.1" xref="A2.T3.7.7.7.1.m1.1.1.1.cmml">/</mo><msup id="A2.T3.7.7.7.1.m1.1.1.3" xref="A2.T3.7.7.7.1.m1.1.1.3.cmml"><mn id="A2.T3.7.7.7.1.m1.1.1.3.2" xref="A2.T3.7.7.7.1.m1.1.1.3.2.cmml">10</mn><mo id="A2.T3.7.7.7.1.m1.1.1.3.3" xref="A2.T3.7.7.7.1.m1.1.1.3.3.cmml">∘</mo></msup><mo id="A2.T3.7.7.7.1.m1.1.1.1a" xref="A2.T3.7.7.7.1.m1.1.1.1.cmml">/</mo><msup id="A2.T3.7.7.7.1.m1.1.1.4" xref="A2.T3.7.7.7.1.m1.1.1.4.cmml"><mn id="A2.T3.7.7.7.1.m1.1.1.4.2" xref="A2.T3.7.7.7.1.m1.1.1.4.2.cmml">20</mn><mo id="A2.T3.7.7.7.1.m1.1.1.4.3" xref="A2.T3.7.7.7.1.m1.1.1.4.3.cmml">∘</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.7.7.7.1.m1.1b"><apply id="A2.T3.7.7.7.1.m1.1.1.cmml" xref="A2.T3.7.7.7.1.m1.1.1"><divide id="A2.T3.7.7.7.1.m1.1.1.1.cmml" xref="A2.T3.7.7.7.1.m1.1.1.1"></divide><apply id="A2.T3.7.7.7.1.m1.1.1.2.cmml" xref="A2.T3.7.7.7.1.m1.1.1.2"><csymbol cd="ambiguous" id="A2.T3.7.7.7.1.m1.1.1.2.1.cmml" xref="A2.T3.7.7.7.1.m1.1.1.2">superscript</csymbol><cn id="A2.T3.7.7.7.1.m1.1.1.2.2.cmml" type="integer" xref="A2.T3.7.7.7.1.m1.1.1.2.2">5</cn><compose id="A2.T3.7.7.7.1.m1.1.1.2.3.cmml" xref="A2.T3.7.7.7.1.m1.1.1.2.3"></compose></apply><apply id="A2.T3.7.7.7.1.m1.1.1.3.cmml" xref="A2.T3.7.7.7.1.m1.1.1.3"><csymbol cd="ambiguous" id="A2.T3.7.7.7.1.m1.1.1.3.1.cmml" xref="A2.T3.7.7.7.1.m1.1.1.3">superscript</csymbol><cn id="A2.T3.7.7.7.1.m1.1.1.3.2.cmml" type="integer" xref="A2.T3.7.7.7.1.m1.1.1.3.2">10</cn><compose id="A2.T3.7.7.7.1.m1.1.1.3.3.cmml" xref="A2.T3.7.7.7.1.m1.1.1.3.3"></compose></apply><apply id="A2.T3.7.7.7.1.m1.1.1.4.cmml" xref="A2.T3.7.7.7.1.m1.1.1.4"><csymbol cd="ambiguous" id="A2.T3.7.7.7.1.m1.1.1.4.1.cmml" xref="A2.T3.7.7.7.1.m1.1.1.4">superscript</csymbol><cn id="A2.T3.7.7.7.1.m1.1.1.4.2.cmml" type="integer" xref="A2.T3.7.7.7.1.m1.1.1.4.2">20</cn><compose id="A2.T3.7.7.7.1.m1.1.1.4.3.cmml" xref="A2.T3.7.7.7.1.m1.1.1.4.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.7.7.7.1.m1.1c">5^{\circ}/10^{\circ}/20^{\circ}</annotation><annotation encoding="application/x-llamapun" id="A2.T3.7.7.7.1.m1.1d">5 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT / 10 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT / 20 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.8.2">(@<math alttext="5^{\circ}/10^{\circ}/20^{\circ}" class="ltx_Math" display="inline" id="A2.T3.8.8.8.2.m1.1"><semantics id="A2.T3.8.8.8.2.m1.1a"><mrow id="A2.T3.8.8.8.2.m1.1.1" xref="A2.T3.8.8.8.2.m1.1.1.cmml"><msup id="A2.T3.8.8.8.2.m1.1.1.2" xref="A2.T3.8.8.8.2.m1.1.1.2.cmml"><mn id="A2.T3.8.8.8.2.m1.1.1.2.2" xref="A2.T3.8.8.8.2.m1.1.1.2.2.cmml">5</mn><mo id="A2.T3.8.8.8.2.m1.1.1.2.3" xref="A2.T3.8.8.8.2.m1.1.1.2.3.cmml">∘</mo></msup><mo id="A2.T3.8.8.8.2.m1.1.1.1" xref="A2.T3.8.8.8.2.m1.1.1.1.cmml">/</mo><msup id="A2.T3.8.8.8.2.m1.1.1.3" xref="A2.T3.8.8.8.2.m1.1.1.3.cmml"><mn id="A2.T3.8.8.8.2.m1.1.1.3.2" xref="A2.T3.8.8.8.2.m1.1.1.3.2.cmml">10</mn><mo id="A2.T3.8.8.8.2.m1.1.1.3.3" xref="A2.T3.8.8.8.2.m1.1.1.3.3.cmml">∘</mo></msup><mo id="A2.T3.8.8.8.2.m1.1.1.1a" xref="A2.T3.8.8.8.2.m1.1.1.1.cmml">/</mo><msup id="A2.T3.8.8.8.2.m1.1.1.4" xref="A2.T3.8.8.8.2.m1.1.1.4.cmml"><mn id="A2.T3.8.8.8.2.m1.1.1.4.2" xref="A2.T3.8.8.8.2.m1.1.1.4.2.cmml">20</mn><mo id="A2.T3.8.8.8.2.m1.1.1.4.3" xref="A2.T3.8.8.8.2.m1.1.1.4.3.cmml">∘</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.8.8.8.2.m1.1b"><apply id="A2.T3.8.8.8.2.m1.1.1.cmml" xref="A2.T3.8.8.8.2.m1.1.1"><divide id="A2.T3.8.8.8.2.m1.1.1.1.cmml" xref="A2.T3.8.8.8.2.m1.1.1.1"></divide><apply id="A2.T3.8.8.8.2.m1.1.1.2.cmml" xref="A2.T3.8.8.8.2.m1.1.1.2"><csymbol cd="ambiguous" id="A2.T3.8.8.8.2.m1.1.1.2.1.cmml" xref="A2.T3.8.8.8.2.m1.1.1.2">superscript</csymbol><cn id="A2.T3.8.8.8.2.m1.1.1.2.2.cmml" type="integer" xref="A2.T3.8.8.8.2.m1.1.1.2.2">5</cn><compose id="A2.T3.8.8.8.2.m1.1.1.2.3.cmml" xref="A2.T3.8.8.8.2.m1.1.1.2.3"></compose></apply><apply id="A2.T3.8.8.8.2.m1.1.1.3.cmml" xref="A2.T3.8.8.8.2.m1.1.1.3"><csymbol cd="ambiguous" id="A2.T3.8.8.8.2.m1.1.1.3.1.cmml" xref="A2.T3.8.8.8.2.m1.1.1.3">superscript</csymbol><cn id="A2.T3.8.8.8.2.m1.1.1.3.2.cmml" type="integer" xref="A2.T3.8.8.8.2.m1.1.1.3.2">10</cn><compose id="A2.T3.8.8.8.2.m1.1.1.3.3.cmml" xref="A2.T3.8.8.8.2.m1.1.1.3.3"></compose></apply><apply id="A2.T3.8.8.8.2.m1.1.1.4.cmml" xref="A2.T3.8.8.8.2.m1.1.1.4"><csymbol cd="ambiguous" id="A2.T3.8.8.8.2.m1.1.1.4.1.cmml" xref="A2.T3.8.8.8.2.m1.1.1.4">superscript</csymbol><cn id="A2.T3.8.8.8.2.m1.1.1.4.2.cmml" type="integer" xref="A2.T3.8.8.8.2.m1.1.1.4.2">20</cn><compose id="A2.T3.8.8.8.2.m1.1.1.4.3.cmml" xref="A2.T3.8.8.8.2.m1.1.1.4.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.8.8.8.2.m1.1c">5^{\circ}/10^{\circ}/20^{\circ}</annotation><annotation encoding="application/x-llamapun" id="A2.T3.8.8.8.2.m1.1d">5 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT / 10 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT / 20 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT</annotation></semantics></math>)</td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.9.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.8.8.9.1.1" rowspan="4"><span class="ltx_text" id="A2.T3.8.8.9.1.1.1">Real10K</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.8.8.9.1.2">w/o Plücker</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.8.8.9.1.3">12.75</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.8.8.9.1.4">195.84</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.8.8.9.1.5">12.1 / 21.9 / 35.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.8.8.9.1.6">1.6 / 5.8 / 16.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.8.8.9.1.7">14.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.8.8.9.1.8">10.02</td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.10.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.10.2.1">w/o Cross-frame</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.10.2.2">17.04</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.10.2.3">154.54</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.10.2.4">21.4 / 34.8 / 50.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.10.2.5">3.8 / 11.1 / 24.2</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.10.2.6">25.67</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.10.2.7">12.70</td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.11.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.11.3.1">w/o Cross-view</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.11.3.2">9.45</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.11.3.3">106.82</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.11.3.4">22.8 / 36.7 / 52.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.11.3.5">2.7 / 8.7 / 22.1</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.11.3.6">27.57</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.11.3.7">14.65</td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.12.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.12.4.1">Ours</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.12.4.2"><span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.2.1">8.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.12.4.3"><span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.3.1">94.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.12.4.4">
<span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.4.1">23.9 </span>/ <span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.4.2">37.4</span> / <span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.4.3">52.9</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.12.4.5">
<span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.5.1">3.3</span> / <span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.5.2">10.2</span> / <span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.5.3">23.5</span>
</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.12.4.6"><span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.6.1">29.39</span></td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.12.4.7"><span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.7.1">15.22</span></td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.13.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A2.T3.8.8.13.5.1" rowspan="5"><span class="ltx_text" id="A2.T3.8.8.13.5.1.1">General</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.8.8.13.5.2">w/o Cross-frame</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.8.8.13.5.3">71.39</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.8.8.13.5.4">249.02</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.8.8.13.5.5">9.8 / 19.1 / 32.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.8.8.13.5.6">0.5 / 1.9 / 6.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.8.8.13.5.7">13.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.8.8.13.5.8">8.97</td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.14.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.14.6.1">w/o Cross-view</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.14.6.2">30.89</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.14.6.3">246.68</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.14.6.4">14.9 / 27.4 / 42.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.14.6.5">1.2 / 4.3 / 12.2</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.14.6.6">17.58</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.14.6.7">9.59</td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.15.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.15.7.1">Ours (Static)</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.15.7.2">27.20</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.15.7.3">185.58</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.15.7.4">15.9 / 28.7 / 44.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.15.7.5">1.4 / 4.6 / 12.9</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.15.7.6">21.75</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.15.7.7">12.04</td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.16.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.16.8.1">Ours (w/o Mono)</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.16.8.2">35.79</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.16.8.3">243.05</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.16.8.4">15.0 / 27.1 / 42.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.16.8.5">0.3 / 1.3 / 4.2</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.16.8.6">18.55</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.16.8.7">10.78</td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.17.9">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A2.T3.8.8.17.9.1">Ours (Full)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T3.8.8.17.9.2"><span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.2.1">26.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A2.T3.8.8.17.9.3"><span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.3.1">173.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A2.T3.8.8.17.9.4">
<span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.4.1">19.7</span> / <span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.4.2">32.7 </span>/ <span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.4.3">48.4</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A2.T3.8.8.17.9.5">
<span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.5.1">0.8 </span>/ <span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.5.2">2.8 </span>/ <span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.5.3">8.7</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T3.8.8.17.9.6"><span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.6.1">33.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T3.8.8.17.9.7"><span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.7.1">19.96</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="A2.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="942" id="A2.F6.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Ablation studies on Plücker coordinates and Cross-frame Attention. Video results are provided in supplementary for clearer qualitative comparisons.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="463" id="A2.F7.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Ablation studies on Cross-view Attention. Video results are provided in supplementary for clearer qualitative comparisons.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="462" id="A2.F8.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Ablation studies on the joint training strategy on monocular videos. Video results are provided in supplementary for clearer qualitative comparisons.</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Evaluation Details</h2>
<div class="ltx_para ltx_noindent" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">For MotionCtrl and CameraCtrl, we use the open-source checkpoints trained from SVD released by the authors. These checkpoints are designed for image-to-video tasks so we can have fair comparisons. We use “clean-fid”<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/GaParmar/clean-fid" title="">https://github.com/GaParmar/clean-fid</a></span></span></span> and “common-metrics-on-video-quality”<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/JunyaoHu/common_metrics_on_video_quality" title="">https://github.com/JunyaoHu/common_metrics_on_video_quality</a></span></span></span> for obtaining FID and FVD, respectively. Our FVD results are reported in VideoGPT <cite class="ltx_cite ltx_citemacro_citep">(Yan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib72" title="">2021</a>)</cite> format. Our COLMAP is configured following DSNeRF <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib11" title="">2022</a>)</cite> and CamCo <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite> to improve the few-view reconstruction performance. Concretely speaking, we enable <span class="ltx_text ltx_font_typewriter" id="A3.p1.1.1">--SiftMatching.max_num_matches 65536</span> to support robust feature matching. To ensure that the SfM results best align with our videos, we set <span class="ltx_text ltx_font_typewriter" id="A3.p1.1.2">--ImageReader.single_camera 1</span> since most videos in our datasets consist of frames captured from a single camera.</p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Ablation Studies</h2>
<div class="ltx_para ltx_noindent" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">In this section, we conduct extensive evaluations for ablation studies. We provide video comparisons in the supplementary.
We provide thorough quantitative comparisons in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A2.T3" title="Table 3 ‣ Monocular Videos ‣ Appendix B Additional Data Curation Details ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">3</span></a> to illustrate the importance of our proposed components. The models are evaluated using RealEstate10K camera trajectories. For the “Real10K” and “General” categories, the testing images are from our test set split of RealEstate10K and InternVid, respectively. Our full model enjoys the best perceptual quality and geometric consistency.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.p2">
<p class="ltx_p" id="A4.p2.1">We first examine the importance of Plucker coordinates conditioning and the cross-frame attention modules. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A2.F6" title="Figure 6 ‣ Monocular Videos ‣ Appendix B Additional Data Curation Details ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">6</span></a>, model variants without cross-frame attention contains severe distortion artifacts, such as the bent walls. The model variant without Plucker coordinates results in simplified camera motion that ignores the complex camera viewpoint instructions.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.p3">
<p class="ltx_p" id="A4.p3.1">We then evaluate the model variant without cross-view attention. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A2.F7" title="Figure 7 ‣ Monocular Videos ‣ Appendix B Additional Data Curation Details ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">7</span></a>, we observe that removing the cross-view attention module results in multiple individual video samples that contain different object motions. For example, the penguin moves differently in the first case, and the wood sticks in the fire appear differently in the second case. This behavior is not desirable because our goal is to obtain multiple videos from different camera paths of the same scene.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.p4">
<p class="ltx_p" id="A4.p4.1">Finally, we examine the importance of our monocular video joint training strategy. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A2.F8" title="Figure 8 ‣ Monocular Videos ‣ Appendix B Additional Data Curation Details ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">8</span></a>, we observe that when overfitting on dynamic objects from Objaverse, the generated results tend to contain frames with simplified backgrounds. This is mainly because, during the training, all data samples from Objaverse are implemented with single random color backgrounds. Our model benefits from joint training on monocular videos and preserves the ability to generate complex backgrounds when object motion is present.</p>
</div>
<figure class="ltx_figure" id="A4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="710" id="A4.F9.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Four-view video comparison. The result of CVD is taken from their website. CVD tends to generate black border pixels, potentially due to its homography warping augmentations during training. In comparison, our method produces frames with better geometric consistency and perceptual quality.</figcaption>
</figure>
<figure class="ltx_figure" id="A4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="864" id="A4.F10.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>3D Reconstruction comparison. We render the reconstructed 3D Gaussians from an elliptical trajectory consisting of 16 novel views. The result of CVD is taken from their website. CVD’s reconstruction results suffer from floaters and blurry artifacts due to the inconsistency in their generated frames. In comparison, our method produces sharper results with clearer visual quality.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Applications</h2>
<div class="ltx_para ltx_noindent" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">In this section, we provide additional results on four-view inference and 3D reconstruction of our generated frames.</p>
</div>
<section class="ltx_subsection" id="A5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Advancing to Four Views at Inference</h3>
<div class="ltx_para ltx_noindent" id="A5.SS1.p1">
<p class="ltx_p" id="A5.SS1.p1.1">Our cross-view attention design enables us to extrapolate to more views straightforwardly at inference time. This design is more efficient compared with the concurrent work CVD <cite class="ltx_cite ltx_citemacro_citep">(Kuang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib27" title="">2024</a>)</cite> which requires enumeration of viewpoint pairs at inference time. We conduct a side-by-side comparison for 4-view generation in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A4.F9" title="Figure 9 ‣ Appendix D Ablation Studies ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">9</span></a>. Our method enjoys better consistency and shows more realistic results than CVD <cite class="ltx_cite ltx_citemacro_citep">(Kuang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib27" title="">2024</a>)</cite>. In comparison, CVD tends to produce artifacts at border regions. For example, the structure of the wall (first case) and the window (second case) change when the viewpoint changes. The results from CVD are taken from their author’s website. We provide video comparisons in the supplementary. We also provide more 4-view generation results from Cavia in our supplementary.</p>
</div>
</section>
<section class="ltx_subsection" id="A5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2 </span>3D Reconstruction of Generated Frames</h3>
<div class="ltx_para ltx_noindent" id="A5.SS2.p1">
<p class="ltx_p" id="A5.SS2.p1.1">We further perform 3D reconstruction on our generated frames. We render our reconstructed 3D Gaussians from an elliptical trajectory consisting of 16 novel views. We provide a side-by-side comparison with the concurrent work CVD <cite class="ltx_cite ltx_citemacro_citep">(Kuang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib27" title="">2024</a>)</cite> in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A4.F10" title="Figure 10 ‣ Appendix D Ablation Studies ‣ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">10</span></a>.
Compared with the results of CVD, our frames are more geometrically consistent and result in clearer 3D reconstruction and fewer floaters. For example, the results from CVD produce floaters on the cupboard regions and generate blurry artifacts for the wall and the TV due to inconsistencies. We provide video comparisons in the supplementary for clearer comparisons. We also provide additional 3D reconstruction results of Cavia’s generated frames in the supplementary.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Limitations</h2>
<div class="ltx_para ltx_noindent" id="A6.p1">
<p class="ltx_p" id="A6.p1.1">Our framework has limited ability to generate large object motion, mainly due to the limitation of the base video generator SVD <cite class="ltx_cite ltx_citemacro_citep">(Stability, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib58" title="">2023</a>)</cite>. We will explore better base models in future works.
Moreover, our data curation pipelines assume a simple camera model using shared camera intrinsic across frames. While enabling easier data preparation, this limits our model from generalizing to complex camera intrinsic changes at inference time, which is widely adopted in cinematography.
Additionally, for simplicity, our framework is trained with normalized scales of scenes, which can be further improved if potentially calibrated with metric scale. We will explore calibration techniques for better quality if a well-generalizable metric depth estimator becomes publicly available.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Oct 14 17:44:10 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
