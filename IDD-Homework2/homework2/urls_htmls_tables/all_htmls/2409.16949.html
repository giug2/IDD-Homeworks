<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling</title>
<!--Generated on Wed Sep 25 13:28:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="" lang="en" name="keywords"/>
<base href="/html/2409.16949v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S1" title="In DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S2" title="In DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S2.SS1" title="In 2 Related Work ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data Augmentation with Generative Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S2.SS2" title="In 2 Related Work ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Automated Data Augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S2.SS3" title="In 2 Related Work ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Class-Specific Prompts Augmentation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S3" title="In DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S3.SS1" title="In 3 Methods ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Measuring CLIPScore</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S3.SS2" title="In 3 Methods ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Adaptive Guidance Scaling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S3.SS3" title="In 3 Methods ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Synthetic Data Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4" title="In DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4.SS1" title="In 4 Experiments ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4.SS2" title="In 4 Experiments ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Experimental Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S5" title="In DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S6" title="In DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Limitations and Future Works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S7" title="In DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#Pt0.A1" title="In DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.A </span>Statistical Test</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#Pt0.A2" title="In DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.B </span>Hyperparameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#Pt0.A3" title="In DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.C </span>LLM Prompt details</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Major in Industrial Data Science &amp; Engineering, 
<br class="ltx_break"/>Department of Industrial and Data Engineering, Pukyong National University 
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id1.1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{kkyuhun94,sod7050,jsw6872}@pukyong.ac.kr,sc82.choi@pknu.ac.kr</span></span></span> </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Teamreboott Inc. 
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id2.1"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>jaeyoungkim@reboott.ai</span></span></span> </span></span></span><span class="ltx_note ltx_role_institutetext" id="id3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Tomocube Inc.
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id3.1"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">email: </span>hsmin@tomocube.com</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kyuheon Jung<span class="ltx_ERROR undefined" id="id1.1.id1">\orcidlink</span>0000-0002-5657-7533
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yongdeuk Seo<span class="ltx_ERROR undefined" id="id2.1.id1">\orcidlink</span>0009-0003-3399-8364
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Seongwoo Cho<span class="ltx_ERROR undefined" id="id3.1.id1">\orcidlink</span>0009-0005-5942-5358
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jaeyoung Kim <span class="ltx_ERROR undefined" id="id4.1.id1">\orcidlink</span>0000-0003-0880-0398
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"/>Hyun-seok Min<span class="ltx_ERROR undefined" id="id5.1.id1">\orcidlink</span>0000-0002-7435-7884
</span><span class="ltx_author_notes">33</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sungchul Choi<span class="ltx_ERROR undefined" id="id6.1.id1">\orcidlink</span>0000-0002-5836-3838
</span><span class="ltx_author_notes">11</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1">In this paper, we present an effective data augmentation framework leveraging the Large Language Model (LLM) and Diffusion Model (DM) to tackle the challenges inherent in data-scarce scenarios.
Recently, DMs have opened up the possibility of generating synthetic images to complement a few training images. However, increasing the diversity of synthetic images also raises the risk of generating samples outside the target distribution. Our approach addresses this issue by embedding novel semantic information into text prompts via LLM and utilizing real images as visual prompts, thus generating semantically rich images. To ensure that the generated images remain within the target distribution, we dynamically adjust the guidance weight based on each image’s CLIPScore to control the diversity. Experimental results show that our method produces synthetic images with enhanced diversity while maintaining adherence to the target distribution. Consequently, our approach proves to be more efficient in the few-shot setting on several benchmarks. Our code is available at <a class="ltx_ref ltx_href" href="https://github.com/kkyuhun94/dalda" style="color:#FF00FF;" title="">https://github.com/kkyuhun94/dalda</a><span class="ltx_text" id="id7.id1.1" style="color:#FF00FF;">.</span></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>
<span class="ltx_text" id="id8.id1" style="color:#FF00FF;">Synthetic data Data augmentation Large language models Diffusion models Diversity</span>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The emergence of Diffusion Models (DMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib18" title="">18</a>]</cite> has significantly increased research interest in potential applications of synthetic images, particularly in the field of data augmentation. Recent studies have shown that synthetic images generated by DMs can effectively complement real data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib2" title="">2</a>]</cite>. However, enhancing the diversity of synthetic images while remaining within the target distribution is still a challenge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib28" title="">28</a>]</cite>. For classification tasks, synthetic images produced by DMs are effective in extending semantic representation for in-distribution data, but they can also generate images that fall outside the target distribution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib28" title="">28</a>]</cite>. Particularly, as diversity increases, it can lead to the generation of samples that do not retain invariant characteristics for target classes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib15" title="">15</a>]</cite>. These synthetic images can be noise samples for training a classifier, leading to sub-par performance.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Real Guidance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib10" title="">10</a>]</cite> addresses this issue by using real images to guide the data synthesis process, thereby reducing domain shift. Specifically, it introduces real images into the diffusion process to align synthetic data more closely with the target domain, effectively narrowing the gap between synthetic and real data distributions. This approach has demonstrated that guiding the synthesis process with real image information is beneficial for data augmentation. Furthermore, DA-Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib30" title="">30</a>]</cite> proposes a robust data augmentation technique for few-shot classification using Textual Inversion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib8" title="">8</a>]</cite> to fine-tune DM on training images for each class. By modifying the appearance of objects while preserving their semantic invariances, DA-Fusion enhances the diversity of synthetic images and adapts the DM to new domains, leading to superior performance in few-shot scenarios.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">While the previous methods help the generated data to stay within the target distribution, they may excessively limit diversity. Additionally, these methods may struggle to supplement new semantic information beyond the real image, especially when using CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib22" title="">22</a>]</cite> templates like <span class="ltx_text ltx_font_typewriter" id="S1.p3.1.1">‘‘a photo of a {class}’’</span>. This can be effective in maintaining the consistency of synthetic images, but it can be challenging to supplement various patterns that can be expressed with its class (see <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4.F5" title="In 4.2 Experimental Results ‣ 4 Experiments ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Meanwhile, several studies have explored enhancing text prompts to increase the diversity of suitable synthetic images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib15" title="">15</a>]</cite>. Fan <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">et al</em>.<span class="ltx_text" id="S1.p4.1.2"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib6" title="">6</a>]</cite> compares the diversity and recognizability of synthetic images generated using various prompt methods and demonstrates that improving class-specific prompts aids in generating synthetic images. However, simply expanding text prompts can still lead to DMs generating class-inconsistent synthetic images.
Marwood <em class="ltx_emph ltx_font_italic" id="S1.p4.1.3">et al</em>.<span class="ltx_text" id="S1.p4.1.4"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib15" title="">15</a>]</cite> emphasizes that carefully controlling the influence of prompts is crucial for creating synthetic images that are beneficial for data augmentations.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this paper, we introduce the <span class="ltx_text ltx_font_italic" id="S1.p5.1.1">Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling</span> (DALDA), a novel method leveraging synthetic images to enhance data augmentation. Our approach begins by utilizing the capabilities of the LLM (GPT-4) to generate text prompts that include various descriptions, such as actions, viewpoints, and environmental features. By integrating dataset and class-specific information into the LLM, we produce unique prompts that retain the intrinsic characteristics of each class. This methodology enriches the semantic content, particularly where each class has limited image samples.
To further refine this process, we present Adaptive Guidance Scaling (AGS), a mechanism designed to balance the influence of example images and text prompts during image generation. Prior to generating synthetic images, we employ the CLIP model to calculate the CLIPScore <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib11" title="">11</a>]</cite>, which quantifies the alignment between an example image and its corresponding class name. Based on this score, we dynamically adjust the weights of prompt guidance within the Multi-Modal conditional Diffusion Model (MMDM).
This ensures that synthetic images do not deviate from the target distribution and maintains the consistency of each class while generating diverse synthetic images.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Our framework effectively balances the trade-off between increasing the diversity of synthetic images and ensuring they remain consistent within the target distribution. We conduct experimental analyses to evaluate the impact of each component on the diversity of synthetic images and the performance of downstream classification models. Our results demonstrate that our approach achieves higher diversity and improved performance of downstream classification models with fewer shots compared to existing methods.
The main contributions of our work can be summarized as:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose a data augmentation framework that leverages LLM-generated text prompts, enriched with novel semantic information, to increase the diversity. Additionally, we analyze the impact of LLM-generated prompts on synthetic images created by DM from a data augmentation perspective.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We introduce AGS, which utilizes CLIPScore to adaptively adjust the balance between text and image conditions during image generation. This ensures that the synthetic images remain within the target distribution while reflecting the enriched text prompts.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">DALDA generates diverse synthetic images using pre-trained DM without additional fine-tuning. It shows better results than existing methods in both diversity and classification accuracy in few-shot scenarios.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data Augmentation with Generative Models</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The advancements in generative models have led to a surge of interest in leveraging these models for data augmentation. In particular, text-to-image (T2I) generation models such as Stable Diffusion (SD) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib24" title="">24</a>]</cite> have shown exceptional capability in producing highly diverse synthetic images, making them promising tools for augmenting training data.
For instance, GenImage <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib35" title="">35</a>]</cite> has demonstrated the potential of using T2I DMs to generate synthetic clones that could effectively replace real images in certain contexts. This study emphasized that synthetic images generated solely from text prompts could serve as viable substitutes for real data.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Various fine-tuning techniques such as ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib32" title="">32</a>]</cite>, DreamBooth <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib25" title="">25</a>]</cite>, GLIDE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib18" title="">18</a>]</cite> and Textual Inversion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib8" title="">8</a>]</cite> facilitate more controlled image generation using multi-modal prompts (<em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.1">e.g</em>.<span class="ltx_text" id="S2.SS1.p2.1.2"></span>, image, sketch and segmentation mask), expanding the versatility and applicability of DMs in various tasks. For example, ControlNet allows precise control over the output by conditioning the model on additional input data such as sketches or segmentation maps, thereby enhancing the utility of synthetic images in downstream tasks.
ScribbleGen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib27" title="">27</a>]</cite> develops a data augmentation framework using ControlNet with a scribble prompt, demonstrating that this multi-modal prompt is helpful in data augmentation.
However, despite these advancements, previous data augmentation approaches often require fine-tuning DMs on sufficient training data to generate appropriate synthetic images.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Real Guidance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib10" title="">10</a>]</cite> utilizes GLIDE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib18" title="">18</a>]</cite>, guided by a small number of real images, to generate synthetic images that mitigate domain variations.
DA-Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib30" title="">30</a>]</cite> introduces a framework for few-shot classification using Textual Inversion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib8" title="">8</a>]</cite> that employs image-to-image transformations parameterized by pre-trained T2I DM. However, this method also necessitates fine-tuning DM.
While these methods reduce domain shift and ensure class consistency, they generate synthetic images that are highly dependent on real images, which can be less effective in data-scarce scenarios.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">Recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib31" title="">31</a>]</cite> have explored fine-tuning pre-trained DMs through learnable adapters, enabling the creation of more controllable and diverse synthetic images. In our approach, we adopt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib31" title="">31</a>]</cite>, which focuses on preserving instance characteristics while allowing for diverse variations.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Automated Data Augmentation</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Traditional data augmentation methods increase the diversity of data by manipulating or transforming images (<em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.1">e.g</em>.<span class="ltx_text" id="S2.SS2.p1.1.2"></span>, flip, crop, rotate, and blur). However, these methods require manually finding suitable techniques based on the distribution and patterns of the dataset. To overcome these limitations, automated data augmentation techniques such as AutoAugment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib3" title="">3</a>]</cite> and RandAugment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib4" title="">4</a>]</cite> have been developed. AutoAugment uses reinforcement learning to find the optimal augmentation policies by constructing a search space of various augmentation methods, probabilities, and intensities. RandAugment reduces the search space by applying the same transformation intensity to all augmentation methods and optimizing only the number of augmentation techniques.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">However, these approaches cannot add new semantic information through simple transformation. To extend automated data augmentation with DM-based techniques, we propose a strategy that extracts features from the original image and automatically determines the weights for the image prompts in MMDM.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Class-Specific Prompts Augmentation</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">To utilize DMs more effectively, recent studies have focused on the composition of text prompts.
He <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.1">et al</em>.<span class="ltx_text" id="S2.SS3.p1.1.2"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib10" title="">10</a>]</cite> proposes Language Enhancement that extends class labels into sentence structures using T5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib23" title="">23</a>]</cite> for text prompt enhancement. It enhanced the diversity of images generated by DM and improved the effectiveness of synthetic data.
Additionally, Fan <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.3">et al</em>.<span class="ltx_text" id="S2.SS3.p1.1.4"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib6" title="">6</a>]</cite> shows the impact of synthetic images on classification tasks using various text prompts.
This study observes the effectiveness of several types of text prompts, including simple class names like <span class="ltx_text ltx_font_typewriter" id="S2.SS3.p1.1.5">‘‘dog’’</span>, class names with detailed descriptions from WordNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib16" title="">16</a>]</cite>, and class names with hypernyms. It also examined CLIP templates, Word2Sentence using T5 to change the class names into a sentence (<em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.6">e.g</em>.<span class="ltx_text" id="S2.SS3.p1.1.7"></span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS3.p1.1.8">‘‘a dog on the sofa’’</span>), and image captioning (BLIP2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib14" title="">14</a>]</cite>). Among these, Image captioning achieves the highest Top-1 accuracy on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib5" title="">5</a>]</cite>. However, this method simply generates prompts by captioning the original images, making it difficult to add novel semantic information and resulting in lower diversity compared to CLIP templates and Word2Sentence.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Marwood <em class="ltx_emph ltx_font_italic" id="S2.SS3.p2.1.1">et al</em>.<span class="ltx_text" id="S2.SS3.p2.1.2"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib15" title="">15</a>]</cite> analyzes the potential and limitations of synthetic images in T2I systems, as well as the relationship between synthetic images and text prompts. This study demonstrates that synthetic images exhibit a trade-off between diversity and consistency depending on the influence of text prompts. This underscores the importance of carefully controlling the influence of prompts.
In this paper, we first provide the LLM with target data information and class details, instructing it to generate class-specific prompts enriched with additional semantic information.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="493" id="S2.F1.g1" src="x1.png" width="545"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.6.3.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.4.2" style="font-size:90%;">Overview of the proposed framework. We calculate the CLIPScore for each training image (<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S3.SS1" title="3.1 Measuring CLIPScore ‣ 3 Methods ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>). We then adaptively adjust the weight <math alttext="\lambda" class="ltx_Math" display="inline" id="S2.F1.3.1.m1.1"><semantics id="S2.F1.3.1.m1.1b"><mi id="S2.F1.3.1.m1.1.1" xref="S2.F1.3.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S2.F1.3.1.m1.1c"><ci id="S2.F1.3.1.m1.1.1.cmml" xref="S2.F1.3.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.3.1.m1.1d">\lambda</annotation><annotation encoding="application/x-llamapun" id="S2.F1.3.1.m1.1e">italic_λ</annotation></semantics></math> of the prompt. <math alttext="\lambda" class="ltx_Math" display="inline" id="S2.F1.4.2.m2.1"><semantics id="S2.F1.4.2.m2.1b"><mi id="S2.F1.4.2.m2.1.1" xref="S2.F1.4.2.m2.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S2.F1.4.2.m2.1c"><ci id="S2.F1.4.2.m2.1.1.cmml" xref="S2.F1.4.2.m2.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.4.2.m2.1d">\lambda</annotation><annotation encoding="application/x-llamapun" id="S2.F1.4.2.m2.1e">italic_λ</annotation></semantics></math> on samples with low CLIPScore serves to focus on the image guides and vice versa, weighting the text guides more heavily to obtain synthetic data with increased diversity (<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S3.SS2" title="3.2 Adaptive Guidance Scaling ‣ 3 Methods ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>). Lastly, text prompts generated by LLM and image prompts are fed into MMDM to generate synthetic images (<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S3.SS3" title="3.3 Synthetic Data Generation ‣ 3 Methods ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>).</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our framework is illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S2.F1" title="In 2.3 Class-Specific Prompts Augmentation ‣ 2 Related Work ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>. DALDA consists of multiple stages, including (1) measuring CLIPScore, (2) adaptive guidance scaling, and (3) synthetic data generation. Each of these stages will be explained in detail in the subsequent sections.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Measuring CLIPScore</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Before generating synthetic images with the MMDM, we calculate the CLIPScore <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib11" title="">11</a>]</cite> which is the cosine similarity between the input image and its caption. In this study, we calculate the CLIPScore between the input image prompt and the text prompt <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.1">‘‘a photo of a {class}’’</span>. The CLIP model can be used to evaluate how well synthetic images reproduce the concept of a specific class <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib15" title="">15</a>]</cite>.
In contrast, we measure the relationship between real images and class names and use this as a factor to adjust the influence of image prompts and text prompts.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">For example, a low CLIPScore indicates that the real image and its class are not appropriately matched within the CLIP embedding. This suggests that, at the stage of generating synthetic images, even if the weight of the image prompt is decreased and the weight of the text prompt is increased, the MMDM is likely to fail in appropriately reflecting the text prompt (see red box in <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4.F5.sf2" title="In Figure 5 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5(b)</span></a>). Conversely, a high CLIPScore indicates that the real image and class are well-matched, enabling the generation of synthetic images that accurately reflect the text prompt in the image prompt (see <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4.F5.sf1" title="In Figure 5 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5(a)</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">We hypothesize that this phenomenon occurs because most open MMDMs, using SD as their backbone, utilize a CLIP encoder. When the multi-modal prompts do not align correctly between the image and text, it can lead to mismatches in the MMDMs’ generation process, resulting in synthetic images that deviate from the target distribution.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Adaptive Guidance Scaling</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.2">We adopt IP-Adapter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib31" title="">31</a>]</cite> that specializes in generating synthetic images that maintain the characteristics of the image prompt while reflecting the text prompt. It employs a decoupled cross-attention mechanism with separated cross-attention layers for text features and image features. In this mechanism, the image features <math alttext="c_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">c</mi><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝑐</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">c_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and text features <math alttext="c_{t}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">c</mi><mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝑐</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">c_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> from the CLIP encoders are passed through their respective cross-attention layers and then their outputs are added together.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">The output of the decoupled cross-attention <math alttext="Z^{new}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><msup id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">Z</mi><mrow id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.3.2.cmml">n</mi><mo id="S3.SS2.p2.1.m1.1.1.3.1" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.1.m1.1.1.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.SS2.p2.1.m1.1.1.3.1a" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.1.m1.1.1.3.4" xref="S3.SS2.p2.1.m1.1.1.3.4.cmml">w</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝑍</ci><apply id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><times id="S3.SS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.1"></times><ci id="S3.SS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2">𝑛</ci><ci id="S3.SS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3">𝑒</ci><ci id="S3.SS2.p2.1.m1.1.1.3.4.cmml" xref="S3.SS2.p2.1.m1.1.1.3.4">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">Z^{new}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_Z start_POSTSUPERSCRIPT italic_n italic_e italic_w end_POSTSUPERSCRIPT</annotation></semantics></math> is defined as follows:</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{split}Z^{new}=\text{Attention}(Q,K_{t},V_{t})+\lambda\cdot\text{%
Attention}(Q,K_{i},V_{i}),\end{split}" class="ltx_Math" display="block" id="S3.E1.m1.29"><semantics id="S3.E1.m1.29a"><mtable displaystyle="true" id="S3.E1.m1.29.29.2"><mtr id="S3.E1.m1.29.29.2a"><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.29.29.2b"><mrow id="S3.E1.m1.29.29.2.28.28.28.28"><mrow id="S3.E1.m1.29.29.2.28.28.28.28.1"><msup id="S3.E1.m1.29.29.2.28.28.28.28.1.5"><mi id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml">Z</mi><mrow id="S3.E1.m1.2.2.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.2.2.1.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.1.2" xref="S3.E1.m1.2.2.2.2.2.2.1.2.cmml">n</mi><mo id="S3.E1.m1.2.2.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E1.m1.2.2.2.2.2.2.1.3" xref="S3.E1.m1.2.2.2.2.2.2.1.3.cmml">e</mi><mo id="S3.E1.m1.2.2.2.2.2.2.1.1a" xref="S3.E1.m1.2.2.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E1.m1.2.2.2.2.2.2.1.4" xref="S3.E1.m1.2.2.2.2.2.2.1.4.cmml">w</mi></mrow></msup><mo id="S3.E1.m1.3.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.3.cmml">=</mo><mrow id="S3.E1.m1.29.29.2.28.28.28.28.1.4"><mrow id="S3.E1.m1.29.29.2.28.28.28.28.1.2.2"><mtext id="S3.E1.m1.4.4.4.4.4.4" xref="S3.E1.m1.4.4.4.4.4.4a.cmml">Attention</mtext><mo id="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3" xref="S3.E1.m1.28.28.1.1.1.cmml">⁢</mo><mrow id="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.2.2"><mo id="S3.E1.m1.5.5.5.5.5.5" stretchy="false" xref="S3.E1.m1.28.28.1.1.1.cmml">(</mo><mi id="S3.E1.m1.6.6.6.6.6.6" xref="S3.E1.m1.6.6.6.6.6.6.cmml">Q</mi><mo id="S3.E1.m1.7.7.7.7.7.7" xref="S3.E1.m1.28.28.1.1.1.cmml">,</mo><msub id="S3.E1.m1.29.29.2.28.28.28.28.1.1.1.1.1.1"><mi id="S3.E1.m1.8.8.8.8.8.8" xref="S3.E1.m1.8.8.8.8.8.8.cmml">K</mi><mi id="S3.E1.m1.9.9.9.9.9.9.1" xref="S3.E1.m1.9.9.9.9.9.9.1.cmml">t</mi></msub><mo id="S3.E1.m1.10.10.10.10.10.10" xref="S3.E1.m1.28.28.1.1.1.cmml">,</mo><msub id="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.2.2.2"><mi id="S3.E1.m1.11.11.11.11.11.11" xref="S3.E1.m1.11.11.11.11.11.11.cmml">V</mi><mi id="S3.E1.m1.12.12.12.12.12.12.1" xref="S3.E1.m1.12.12.12.12.12.12.1.cmml">t</mi></msub><mo id="S3.E1.m1.13.13.13.13.13.13" stretchy="false" xref="S3.E1.m1.28.28.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.14.14.14.14.14.14" xref="S3.E1.m1.14.14.14.14.14.14.cmml">+</mo><mrow id="S3.E1.m1.29.29.2.28.28.28.28.1.4.4"><mrow id="S3.E1.m1.29.29.2.28.28.28.28.1.4.4.4"><mi id="S3.E1.m1.15.15.15.15.15.15" xref="S3.E1.m1.15.15.15.15.15.15.cmml">λ</mi><mo id="S3.E1.m1.16.16.16.16.16.16" lspace="0.222em" rspace="0.222em" xref="S3.E1.m1.16.16.16.16.16.16.cmml">⋅</mo><mtext id="S3.E1.m1.17.17.17.17.17.17" xref="S3.E1.m1.17.17.17.17.17.17a.cmml">Attention</mtext></mrow><mo id="S3.E1.m1.29.29.2.28.28.28.28.1.4.4.3" xref="S3.E1.m1.28.28.1.1.1.cmml">⁢</mo><mrow id="S3.E1.m1.29.29.2.28.28.28.28.1.4.4.2.2"><mo id="S3.E1.m1.18.18.18.18.18.18" stretchy="false" xref="S3.E1.m1.28.28.1.1.1.cmml">(</mo><mi id="S3.E1.m1.19.19.19.19.19.19" xref="S3.E1.m1.19.19.19.19.19.19.cmml">Q</mi><mo id="S3.E1.m1.20.20.20.20.20.20" xref="S3.E1.m1.28.28.1.1.1.cmml">,</mo><msub id="S3.E1.m1.29.29.2.28.28.28.28.1.3.3.1.1.1"><mi id="S3.E1.m1.21.21.21.21.21.21" xref="S3.E1.m1.21.21.21.21.21.21.cmml">K</mi><mi id="S3.E1.m1.22.22.22.22.22.22.1" xref="S3.E1.m1.22.22.22.22.22.22.1.cmml">i</mi></msub><mo id="S3.E1.m1.23.23.23.23.23.23" xref="S3.E1.m1.28.28.1.1.1.cmml">,</mo><msub id="S3.E1.m1.29.29.2.28.28.28.28.1.4.4.2.2.2"><mi id="S3.E1.m1.24.24.24.24.24.24" xref="S3.E1.m1.24.24.24.24.24.24.cmml">V</mi><mi id="S3.E1.m1.25.25.25.25.25.25.1" xref="S3.E1.m1.25.25.25.25.25.25.1.cmml">i</mi></msub><mo id="S3.E1.m1.26.26.26.26.26.26" stretchy="false" xref="S3.E1.m1.28.28.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.27.27.27.27.27.27" xref="S3.E1.m1.28.28.1.1.1.cmml">,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E1.m1.29b"><apply id="S3.E1.m1.28.28.1.1.1.cmml" xref="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3"><eq id="S3.E1.m1.3.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3"></eq><apply id="S3.E1.m1.28.28.1.1.1.6.cmml" xref="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.28.28.1.1.1.6.1.cmml" xref="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">𝑍</ci><apply id="S3.E1.m1.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1"><times id="S3.E1.m1.2.2.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.1"></times><ci id="S3.E1.m1.2.2.2.2.2.2.1.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.2">𝑛</ci><ci id="S3.E1.m1.2.2.2.2.2.2.1.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.3">𝑒</ci><ci id="S3.E1.m1.2.2.2.2.2.2.1.4.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.4">𝑤</ci></apply></apply><apply id="S3.E1.m1.28.28.1.1.1.4.cmml" xref="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3"><plus id="S3.E1.m1.14.14.14.14.14.14.cmml" xref="S3.E1.m1.14.14.14.14.14.14"></plus><apply id="S3.E1.m1.28.28.1.1.1.2.2.cmml" xref="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3"><times id="S3.E1.m1.28.28.1.1.1.2.2.3.cmml" xref="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3"></times><ci id="S3.E1.m1.4.4.4.4.4.4a.cmml" xref="S3.E1.m1.4.4.4.4.4.4"><mtext id="S3.E1.m1.4.4.4.4.4.4.cmml" xref="S3.E1.m1.4.4.4.4.4.4">Attention</mtext></ci><vector id="S3.E1.m1.28.28.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3"><ci id="S3.E1.m1.6.6.6.6.6.6.cmml" xref="S3.E1.m1.6.6.6.6.6.6">𝑄</ci><apply id="S3.E1.m1.28.28.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.28.28.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3">subscript</csymbol><ci id="S3.E1.m1.8.8.8.8.8.8.cmml" xref="S3.E1.m1.8.8.8.8.8.8">𝐾</ci><ci id="S3.E1.m1.9.9.9.9.9.9.1.cmml" xref="S3.E1.m1.9.9.9.9.9.9.1">𝑡</ci></apply><apply id="S3.E1.m1.28.28.1.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.28.28.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3">subscript</csymbol><ci id="S3.E1.m1.11.11.11.11.11.11.cmml" xref="S3.E1.m1.11.11.11.11.11.11">𝑉</ci><ci id="S3.E1.m1.12.12.12.12.12.12.1.cmml" xref="S3.E1.m1.12.12.12.12.12.12.1">𝑡</ci></apply></vector></apply><apply id="S3.E1.m1.28.28.1.1.1.4.4.cmml" xref="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3"><times id="S3.E1.m1.28.28.1.1.1.4.4.3.cmml" xref="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3"></times><apply id="S3.E1.m1.28.28.1.1.1.4.4.4.cmml" xref="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3"><ci id="S3.E1.m1.16.16.16.16.16.16.cmml" xref="S3.E1.m1.16.16.16.16.16.16">⋅</ci><ci id="S3.E1.m1.15.15.15.15.15.15.cmml" xref="S3.E1.m1.15.15.15.15.15.15">𝜆</ci><ci id="S3.E1.m1.17.17.17.17.17.17a.cmml" xref="S3.E1.m1.17.17.17.17.17.17"><mtext id="S3.E1.m1.17.17.17.17.17.17.cmml" xref="S3.E1.m1.17.17.17.17.17.17">Attention</mtext></ci></apply><vector id="S3.E1.m1.28.28.1.1.1.4.4.2.3.cmml" xref="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3"><ci id="S3.E1.m1.19.19.19.19.19.19.cmml" xref="S3.E1.m1.19.19.19.19.19.19">𝑄</ci><apply id="S3.E1.m1.28.28.1.1.1.3.3.1.1.1.cmml" xref="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.28.28.1.1.1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3">subscript</csymbol><ci id="S3.E1.m1.21.21.21.21.21.21.cmml" xref="S3.E1.m1.21.21.21.21.21.21">𝐾</ci><ci id="S3.E1.m1.22.22.22.22.22.22.1.cmml" xref="S3.E1.m1.22.22.22.22.22.22.1">𝑖</ci></apply><apply id="S3.E1.m1.28.28.1.1.1.4.4.2.2.2.cmml" xref="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.28.28.1.1.1.4.4.2.2.2.1.cmml" xref="S3.E1.m1.29.29.2.28.28.28.28.1.2.2.3">subscript</csymbol><ci id="S3.E1.m1.24.24.24.24.24.24.cmml" xref="S3.E1.m1.24.24.24.24.24.24">𝑉</ci><ci id="S3.E1.m1.25.25.25.25.25.25.1.cmml" xref="S3.E1.m1.25.25.25.25.25.25.1">𝑖</ci></apply></vector></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.29c">\begin{split}Z^{new}=\text{Attention}(Q,K_{t},V_{t})+\lambda\cdot\text{%
Attention}(Q,K_{i},V_{i}),\end{split}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.29d">start_ROW start_CELL italic_Z start_POSTSUPERSCRIPT italic_n italic_e italic_w end_POSTSUPERSCRIPT = Attention ( italic_Q , italic_K start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + italic_λ ⋅ Attention ( italic_Q , italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p3.1">where <math alttext="\&gt;Q=ZW_{q},\&gt;K_{t}=c_{t}W_{kt},\&gt;V_{t}=c_{t}W_{vt},\&gt;K_{i}=c_{i}W_{ki},\&gt;V_{i}%
=c_{i}W_{vi}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.2"><semantics id="S3.SS2.p3.1.m1.2a"><mrow id="S3.SS2.p3.1.m1.2.2.2" xref="S3.SS2.p3.1.m1.2.2.3.cmml"><mrow id="S3.SS2.p3.1.m1.1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.1.1.2.cmml">Q</mi><mo id="S3.SS2.p3.1.m1.1.1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.SS2.p3.1.m1.1.1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.1.1.3.cmml"><mi id="S3.SS2.p3.1.m1.1.1.1.1.3.2" xref="S3.SS2.p3.1.m1.1.1.1.1.3.2.cmml">Z</mi><mo id="S3.SS2.p3.1.m1.1.1.1.1.3.1" xref="S3.SS2.p3.1.m1.1.1.1.1.3.1.cmml">⁢</mo><msub id="S3.SS2.p3.1.m1.1.1.1.1.3.3" xref="S3.SS2.p3.1.m1.1.1.1.1.3.3.cmml"><mi id="S3.SS2.p3.1.m1.1.1.1.1.3.3.2" xref="S3.SS2.p3.1.m1.1.1.1.1.3.3.2.cmml">W</mi><mi id="S3.SS2.p3.1.m1.1.1.1.1.3.3.3" xref="S3.SS2.p3.1.m1.1.1.1.1.3.3.3.cmml">q</mi></msub></mrow></mrow><mo id="S3.SS2.p3.1.m1.2.2.2.3" rspace="0.387em" xref="S3.SS2.p3.1.m1.2.2.3a.cmml">,</mo><mrow id="S3.SS2.p3.1.m1.2.2.2.2.2" xref="S3.SS2.p3.1.m1.2.2.2.2.3.cmml"><mrow id="S3.SS2.p3.1.m1.2.2.2.2.1.1" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.cmml"><msub id="S3.SS2.p3.1.m1.2.2.2.2.1.1.2" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.2.cmml"><mi id="S3.SS2.p3.1.m1.2.2.2.2.1.1.2.2" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.2.2.cmml">K</mi><mi id="S3.SS2.p3.1.m1.2.2.2.2.1.1.2.3" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.2.3.cmml">t</mi></msub><mo id="S3.SS2.p3.1.m1.2.2.2.2.1.1.1" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.1.cmml">=</mo><mrow id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.cmml"><msub id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.2" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.2.cmml"><mi id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.2.2" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.2.2.cmml">c</mi><mi id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.2.3" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.2.3.cmml">t</mi></msub><mo id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.1" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.1.cmml">⁢</mo><msub id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.cmml"><mi id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.2" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.2.cmml">W</mi><mrow id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.3" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.3.cmml"><mi id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.3.2" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.3.2.cmml">k</mi><mo id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.3.1" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.3.3" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.3.3.cmml">t</mi></mrow></msub></mrow></mrow><mo id="S3.SS2.p3.1.m1.2.2.2.2.2.3" rspace="0.387em" xref="S3.SS2.p3.1.m1.2.2.2.2.3a.cmml">,</mo><mrow id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.3.cmml"><mrow id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.cmml"><msub id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.2.cmml"><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.2.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.2.2.cmml">V</mi><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.2.3" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.2.3.cmml">t</mi></msub><mo id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.1" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.1.cmml">=</mo><mrow id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.cmml"><msub id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.2.cmml"><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.2.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.2.2.cmml">c</mi><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.2.3" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.2.3.cmml">t</mi></msub><mo id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.1" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.1.cmml">⁢</mo><msub id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.cmml"><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.2.cmml">W</mi><mrow id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.3" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.3.cmml"><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.3.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.3.2.cmml">v</mi><mo id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.3.1" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.3.3" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.3.3.cmml">t</mi></mrow></msub></mrow></mrow><mo id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.3" rspace="0.387em" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.3a.cmml">,</mo><mrow id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.3.cmml"><mrow id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.cmml"><msub id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.2.cmml"><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.2.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.2.2.cmml">K</mi><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.2.3" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.1" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.1.cmml">=</mo><mrow id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.cmml"><msub id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.2.cmml"><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.2.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.2.2.cmml">c</mi><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.2.3" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.2.3.cmml">i</mi></msub><mo id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.1" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.1.cmml">⁢</mo><msub id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.cmml"><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.2.cmml">W</mi><mrow id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.3" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.3.cmml"><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.3.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.3.2.cmml">k</mi><mo id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.3.1" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.3.3" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.3.3.cmml">i</mi></mrow></msub></mrow></mrow><mo id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.3" rspace="0.387em" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.3a.cmml">,</mo><mrow id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.cmml"><msub id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.2.cmml"><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.2.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.2.2.cmml">V</mi><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.2.3" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.1" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.1.cmml">=</mo><mrow id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.cmml"><msub id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.2.cmml"><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.2.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.2.2.cmml">c</mi><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.2.3" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.2.3.cmml">i</mi></msub><mo id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.1" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.1.cmml">⁢</mo><msub id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.cmml"><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.2.cmml">W</mi><mrow id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.3" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.3.cmml"><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.3.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.3.2.cmml">v</mi><mo id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.3.1" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.3.3" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.3.3.cmml">i</mi></mrow></msub></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.2b"><apply id="S3.SS2.p3.1.m1.2.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.3a.cmml" xref="S3.SS2.p3.1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS2.p3.1.m1.1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1"><eq id="S3.SS2.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.1"></eq><ci id="S3.SS2.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.2">𝑄</ci><apply id="S3.SS2.p3.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.3"><times id="S3.SS2.p3.1.m1.1.1.1.1.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.3.1"></times><ci id="S3.SS2.p3.1.m1.1.1.1.1.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.3.2">𝑍</ci><apply id="S3.SS2.p3.1.m1.1.1.1.1.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.1.3.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.1.1.3.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.3.3.2">𝑊</ci><ci id="S3.SS2.p3.1.m1.1.1.1.1.3.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.3.3.3">𝑞</ci></apply></apply></apply><apply id="S3.SS2.p3.1.m1.2.2.2.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.2.2.3a.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS2.p3.1.m1.2.2.2.2.1.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1"><eq id="S3.SS2.p3.1.m1.2.2.2.2.1.1.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.1"></eq><apply id="S3.SS2.p3.1.m1.2.2.2.2.1.1.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.2.2.1.1.2.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.2">subscript</csymbol><ci id="S3.SS2.p3.1.m1.2.2.2.2.1.1.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.2.2">𝐾</ci><ci id="S3.SS2.p3.1.m1.2.2.2.2.1.1.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.2.3">𝑡</ci></apply><apply id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3"><times id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.1"></times><apply id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.2.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.2">subscript</csymbol><ci id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.2.2">𝑐</ci><ci id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.2.3">𝑡</ci></apply><apply id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.2">𝑊</ci><apply id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.3"><times id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.3.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.3.1"></times><ci id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.3.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.3.2">𝑘</ci><ci id="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.3.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.3.3.3.3">𝑡</ci></apply></apply></apply></apply><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.2.2.2.2.3a.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1"><eq id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.1"></eq><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.2.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.2">subscript</csymbol><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.2.2">𝑉</ci><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.2.3">𝑡</ci></apply><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3"><times id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.1"></times><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.2.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.2">subscript</csymbol><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.2.2">𝑐</ci><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.2.3">𝑡</ci></apply><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.2">𝑊</ci><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.3"><times id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.3.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.3.1"></times><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.3.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.3.2">𝑣</ci><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.3.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.1.1.3.3.3.3">𝑡</ci></apply></apply></apply></apply><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.3a.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1"><eq id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.1"></eq><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.2.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.2">subscript</csymbol><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.2.2">𝐾</ci><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.2.3">𝑖</ci></apply><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3"><times id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.1"></times><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.2.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.2">subscript</csymbol><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.2.2">𝑐</ci><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.2.3">𝑖</ci></apply><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.2">𝑊</ci><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.3"><times id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.3.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.3.1"></times><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.3.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.3.2">𝑘</ci><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.3.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.1.1.3.3.3.3">𝑖</ci></apply></apply></apply></apply><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2"><eq id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.1"></eq><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.2.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.2.2">𝑉</ci><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.2.3">𝑖</ci></apply><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3"><times id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.1"></times><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.2.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.2">subscript</csymbol><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.2.2">𝑐</ci><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.2.3">𝑖</ci></apply><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3">subscript</csymbol><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.2">𝑊</ci><apply id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.3"><times id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.3.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.3.1"></times><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.3.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.3.2">𝑣</ci><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.3.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2.2.2.2.2.2.3.3.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.2c">\&gt;Q=ZW_{q},\&gt;K_{t}=c_{t}W_{kt},\&gt;V_{t}=c_{t}W_{vt},\&gt;K_{i}=c_{i}W_{ki},\&gt;V_{i}%
=c_{i}W_{vi}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.2d">italic_Q = italic_Z italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_K start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_k italic_t end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_v italic_t end_POSTSUBSCRIPT , italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_k italic_i end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_v italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="176" id="S3.F2.g1" src="x2.png" width="654"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.6.3.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.4.2" style="font-size:90%;">Example of synthetic images varying the <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.F2.3.1.m1.1"><semantics id="S3.F2.3.1.m1.1b"><mi id="S3.F2.3.1.m1.1.1" xref="S3.F2.3.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.F2.3.1.m1.1c"><ci id="S3.F2.3.1.m1.1.1.cmml" xref="S3.F2.3.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.3.1.m1.1d">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.F2.3.1.m1.1e">italic_λ</annotation></semantics></math>. As the weight <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.F2.4.2.m2.1"><semantics id="S3.F2.4.2.m2.1b"><mi id="S3.F2.4.2.m2.1.1" xref="S3.F2.4.2.m2.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.F2.4.2.m2.1c"><ci id="S3.F2.4.2.m2.1.1.cmml" xref="S3.F2.4.2.m2.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.4.2.m2.1d">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.F2.4.2.m2.1e">italic_λ</annotation></semantics></math> gets closer to zero, it becomes more similar to T2I generation.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.16">In <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S3.E1" title="In 3.2 Adaptive Guidance Scaling ‣ 3 Methods ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">italic_Q</annotation></semantics></math> is a matrix derived from query features <math alttext="Z" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2.1"><semantics id="S3.SS2.p4.2.m2.1a"><mi id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><ci id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">Z</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.2.m2.1d">italic_Z</annotation></semantics></math> and weights <math alttext="W_{q}" class="ltx_Math" display="inline" id="S3.SS2.p4.3.m3.1"><semantics id="S3.SS2.p4.3.m3.1a"><msub id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml"><mi id="S3.SS2.p4.3.m3.1.1.2" xref="S3.SS2.p4.3.m3.1.1.2.cmml">W</mi><mi id="S3.SS2.p4.3.m3.1.1.3" xref="S3.SS2.p4.3.m3.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2">𝑊</ci><ci id="S3.SS2.p4.3.m3.1.1.3.cmml" xref="S3.SS2.p4.3.m3.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">W_{q}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.3.m3.1d">italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT</annotation></semantics></math>. <math alttext="K_{t}" class="ltx_Math" display="inline" id="S3.SS2.p4.4.m4.1"><semantics id="S3.SS2.p4.4.m4.1a"><msub id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml"><mi id="S3.SS2.p4.4.m4.1.1.2" xref="S3.SS2.p4.4.m4.1.1.2.cmml">K</mi><mi id="S3.SS2.p4.4.m4.1.1.3" xref="S3.SS2.p4.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><apply id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m4.1.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p4.4.m4.1.1.2.cmml" xref="S3.SS2.p4.4.m4.1.1.2">𝐾</ci><ci id="S3.SS2.p4.4.m4.1.1.3.cmml" xref="S3.SS2.p4.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">K_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.4.m4.1d">italic_K start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="V_{t}" class="ltx_Math" display="inline" id="S3.SS2.p4.5.m5.1"><semantics id="S3.SS2.p4.5.m5.1a"><msub id="S3.SS2.p4.5.m5.1.1" xref="S3.SS2.p4.5.m5.1.1.cmml"><mi id="S3.SS2.p4.5.m5.1.1.2" xref="S3.SS2.p4.5.m5.1.1.2.cmml">V</mi><mi id="S3.SS2.p4.5.m5.1.1.3" xref="S3.SS2.p4.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m5.1b"><apply id="S3.SS2.p4.5.m5.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.5.m5.1.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p4.5.m5.1.1.2.cmml" xref="S3.SS2.p4.5.m5.1.1.2">𝑉</ci><ci id="S3.SS2.p4.5.m5.1.1.3.cmml" xref="S3.SS2.p4.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m5.1c">V_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.5.m5.1d">italic_V start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> are matrices obtained from text features <math alttext="c_{t}" class="ltx_Math" display="inline" id="S3.SS2.p4.6.m6.1"><semantics id="S3.SS2.p4.6.m6.1a"><msub id="S3.SS2.p4.6.m6.1.1" xref="S3.SS2.p4.6.m6.1.1.cmml"><mi id="S3.SS2.p4.6.m6.1.1.2" xref="S3.SS2.p4.6.m6.1.1.2.cmml">c</mi><mi id="S3.SS2.p4.6.m6.1.1.3" xref="S3.SS2.p4.6.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m6.1b"><apply id="S3.SS2.p4.6.m6.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.6.m6.1.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p4.6.m6.1.1.2.cmml" xref="S3.SS2.p4.6.m6.1.1.2">𝑐</ci><ci id="S3.SS2.p4.6.m6.1.1.3.cmml" xref="S3.SS2.p4.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m6.1c">c_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.6.m6.1d">italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and their respective weights <math alttext="W_{kt}" class="ltx_Math" display="inline" id="S3.SS2.p4.7.m7.1"><semantics id="S3.SS2.p4.7.m7.1a"><msub id="S3.SS2.p4.7.m7.1.1" xref="S3.SS2.p4.7.m7.1.1.cmml"><mi id="S3.SS2.p4.7.m7.1.1.2" xref="S3.SS2.p4.7.m7.1.1.2.cmml">W</mi><mrow id="S3.SS2.p4.7.m7.1.1.3" xref="S3.SS2.p4.7.m7.1.1.3.cmml"><mi id="S3.SS2.p4.7.m7.1.1.3.2" xref="S3.SS2.p4.7.m7.1.1.3.2.cmml">k</mi><mo id="S3.SS2.p4.7.m7.1.1.3.1" xref="S3.SS2.p4.7.m7.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p4.7.m7.1.1.3.3" xref="S3.SS2.p4.7.m7.1.1.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.7.m7.1b"><apply id="S3.SS2.p4.7.m7.1.1.cmml" xref="S3.SS2.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.7.m7.1.1.1.cmml" xref="S3.SS2.p4.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p4.7.m7.1.1.2.cmml" xref="S3.SS2.p4.7.m7.1.1.2">𝑊</ci><apply id="S3.SS2.p4.7.m7.1.1.3.cmml" xref="S3.SS2.p4.7.m7.1.1.3"><times id="S3.SS2.p4.7.m7.1.1.3.1.cmml" xref="S3.SS2.p4.7.m7.1.1.3.1"></times><ci id="S3.SS2.p4.7.m7.1.1.3.2.cmml" xref="S3.SS2.p4.7.m7.1.1.3.2">𝑘</ci><ci id="S3.SS2.p4.7.m7.1.1.3.3.cmml" xref="S3.SS2.p4.7.m7.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.7.m7.1c">W_{kt}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.7.m7.1d">italic_W start_POSTSUBSCRIPT italic_k italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="W_{vt}" class="ltx_Math" display="inline" id="S3.SS2.p4.8.m8.1"><semantics id="S3.SS2.p4.8.m8.1a"><msub id="S3.SS2.p4.8.m8.1.1" xref="S3.SS2.p4.8.m8.1.1.cmml"><mi id="S3.SS2.p4.8.m8.1.1.2" xref="S3.SS2.p4.8.m8.1.1.2.cmml">W</mi><mrow id="S3.SS2.p4.8.m8.1.1.3" xref="S3.SS2.p4.8.m8.1.1.3.cmml"><mi id="S3.SS2.p4.8.m8.1.1.3.2" xref="S3.SS2.p4.8.m8.1.1.3.2.cmml">v</mi><mo id="S3.SS2.p4.8.m8.1.1.3.1" xref="S3.SS2.p4.8.m8.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p4.8.m8.1.1.3.3" xref="S3.SS2.p4.8.m8.1.1.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.8.m8.1b"><apply id="S3.SS2.p4.8.m8.1.1.cmml" xref="S3.SS2.p4.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.8.m8.1.1.1.cmml" xref="S3.SS2.p4.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.p4.8.m8.1.1.2.cmml" xref="S3.SS2.p4.8.m8.1.1.2">𝑊</ci><apply id="S3.SS2.p4.8.m8.1.1.3.cmml" xref="S3.SS2.p4.8.m8.1.1.3"><times id="S3.SS2.p4.8.m8.1.1.3.1.cmml" xref="S3.SS2.p4.8.m8.1.1.3.1"></times><ci id="S3.SS2.p4.8.m8.1.1.3.2.cmml" xref="S3.SS2.p4.8.m8.1.1.3.2">𝑣</ci><ci id="S3.SS2.p4.8.m8.1.1.3.3.cmml" xref="S3.SS2.p4.8.m8.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.8.m8.1c">W_{vt}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.8.m8.1d">italic_W start_POSTSUBSCRIPT italic_v italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, while <math alttext="K_{i}" class="ltx_Math" display="inline" id="S3.SS2.p4.9.m9.1"><semantics id="S3.SS2.p4.9.m9.1a"><msub id="S3.SS2.p4.9.m9.1.1" xref="S3.SS2.p4.9.m9.1.1.cmml"><mi id="S3.SS2.p4.9.m9.1.1.2" xref="S3.SS2.p4.9.m9.1.1.2.cmml">K</mi><mi id="S3.SS2.p4.9.m9.1.1.3" xref="S3.SS2.p4.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.9.m9.1b"><apply id="S3.SS2.p4.9.m9.1.1.cmml" xref="S3.SS2.p4.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.9.m9.1.1.1.cmml" xref="S3.SS2.p4.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.p4.9.m9.1.1.2.cmml" xref="S3.SS2.p4.9.m9.1.1.2">𝐾</ci><ci id="S3.SS2.p4.9.m9.1.1.3.cmml" xref="S3.SS2.p4.9.m9.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.9.m9.1c">K_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.9.m9.1d">italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="V_{i}" class="ltx_Math" display="inline" id="S3.SS2.p4.10.m10.1"><semantics id="S3.SS2.p4.10.m10.1a"><msub id="S3.SS2.p4.10.m10.1.1" xref="S3.SS2.p4.10.m10.1.1.cmml"><mi id="S3.SS2.p4.10.m10.1.1.2" xref="S3.SS2.p4.10.m10.1.1.2.cmml">V</mi><mi id="S3.SS2.p4.10.m10.1.1.3" xref="S3.SS2.p4.10.m10.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.10.m10.1b"><apply id="S3.SS2.p4.10.m10.1.1.cmml" xref="S3.SS2.p4.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.10.m10.1.1.1.cmml" xref="S3.SS2.p4.10.m10.1.1">subscript</csymbol><ci id="S3.SS2.p4.10.m10.1.1.2.cmml" xref="S3.SS2.p4.10.m10.1.1.2">𝑉</ci><ci id="S3.SS2.p4.10.m10.1.1.3.cmml" xref="S3.SS2.p4.10.m10.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.10.m10.1c">V_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.10.m10.1d">italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are matrices derived from image features <math alttext="c_{i}" class="ltx_Math" display="inline" id="S3.SS2.p4.11.m11.1"><semantics id="S3.SS2.p4.11.m11.1a"><msub id="S3.SS2.p4.11.m11.1.1" xref="S3.SS2.p4.11.m11.1.1.cmml"><mi id="S3.SS2.p4.11.m11.1.1.2" xref="S3.SS2.p4.11.m11.1.1.2.cmml">c</mi><mi id="S3.SS2.p4.11.m11.1.1.3" xref="S3.SS2.p4.11.m11.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.11.m11.1b"><apply id="S3.SS2.p4.11.m11.1.1.cmml" xref="S3.SS2.p4.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.11.m11.1.1.1.cmml" xref="S3.SS2.p4.11.m11.1.1">subscript</csymbol><ci id="S3.SS2.p4.11.m11.1.1.2.cmml" xref="S3.SS2.p4.11.m11.1.1.2">𝑐</ci><ci id="S3.SS2.p4.11.m11.1.1.3.cmml" xref="S3.SS2.p4.11.m11.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.11.m11.1c">c_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.11.m11.1d">italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and their respective weights <math alttext="W_{ki}" class="ltx_Math" display="inline" id="S3.SS2.p4.12.m12.1"><semantics id="S3.SS2.p4.12.m12.1a"><msub id="S3.SS2.p4.12.m12.1.1" xref="S3.SS2.p4.12.m12.1.1.cmml"><mi id="S3.SS2.p4.12.m12.1.1.2" xref="S3.SS2.p4.12.m12.1.1.2.cmml">W</mi><mrow id="S3.SS2.p4.12.m12.1.1.3" xref="S3.SS2.p4.12.m12.1.1.3.cmml"><mi id="S3.SS2.p4.12.m12.1.1.3.2" xref="S3.SS2.p4.12.m12.1.1.3.2.cmml">k</mi><mo id="S3.SS2.p4.12.m12.1.1.3.1" xref="S3.SS2.p4.12.m12.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p4.12.m12.1.1.3.3" xref="S3.SS2.p4.12.m12.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.12.m12.1b"><apply id="S3.SS2.p4.12.m12.1.1.cmml" xref="S3.SS2.p4.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.12.m12.1.1.1.cmml" xref="S3.SS2.p4.12.m12.1.1">subscript</csymbol><ci id="S3.SS2.p4.12.m12.1.1.2.cmml" xref="S3.SS2.p4.12.m12.1.1.2">𝑊</ci><apply id="S3.SS2.p4.12.m12.1.1.3.cmml" xref="S3.SS2.p4.12.m12.1.1.3"><times id="S3.SS2.p4.12.m12.1.1.3.1.cmml" xref="S3.SS2.p4.12.m12.1.1.3.1"></times><ci id="S3.SS2.p4.12.m12.1.1.3.2.cmml" xref="S3.SS2.p4.12.m12.1.1.3.2">𝑘</ci><ci id="S3.SS2.p4.12.m12.1.1.3.3.cmml" xref="S3.SS2.p4.12.m12.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.12.m12.1c">W_{ki}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.12.m12.1d">italic_W start_POSTSUBSCRIPT italic_k italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="W_{vi}" class="ltx_Math" display="inline" id="S3.SS2.p4.13.m13.1"><semantics id="S3.SS2.p4.13.m13.1a"><msub id="S3.SS2.p4.13.m13.1.1" xref="S3.SS2.p4.13.m13.1.1.cmml"><mi id="S3.SS2.p4.13.m13.1.1.2" xref="S3.SS2.p4.13.m13.1.1.2.cmml">W</mi><mrow id="S3.SS2.p4.13.m13.1.1.3" xref="S3.SS2.p4.13.m13.1.1.3.cmml"><mi id="S3.SS2.p4.13.m13.1.1.3.2" xref="S3.SS2.p4.13.m13.1.1.3.2.cmml">v</mi><mo id="S3.SS2.p4.13.m13.1.1.3.1" xref="S3.SS2.p4.13.m13.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p4.13.m13.1.1.3.3" xref="S3.SS2.p4.13.m13.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.13.m13.1b"><apply id="S3.SS2.p4.13.m13.1.1.cmml" xref="S3.SS2.p4.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.13.m13.1.1.1.cmml" xref="S3.SS2.p4.13.m13.1.1">subscript</csymbol><ci id="S3.SS2.p4.13.m13.1.1.2.cmml" xref="S3.SS2.p4.13.m13.1.1.2">𝑊</ci><apply id="S3.SS2.p4.13.m13.1.1.3.cmml" xref="S3.SS2.p4.13.m13.1.1.3"><times id="S3.SS2.p4.13.m13.1.1.3.1.cmml" xref="S3.SS2.p4.13.m13.1.1.3.1"></times><ci id="S3.SS2.p4.13.m13.1.1.3.2.cmml" xref="S3.SS2.p4.13.m13.1.1.3.2">𝑣</ci><ci id="S3.SS2.p4.13.m13.1.1.3.3.cmml" xref="S3.SS2.p4.13.m13.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.13.m13.1c">W_{vi}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.13.m13.1d">italic_W start_POSTSUBSCRIPT italic_v italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. The weight factor denoted by <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.p4.14.m14.1"><semantics id="S3.SS2.p4.14.m14.1a"><mi id="S3.SS2.p4.14.m14.1.1" xref="S3.SS2.p4.14.m14.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.14.m14.1b"><ci id="S3.SS2.p4.14.m14.1.1.cmml" xref="S3.SS2.p4.14.m14.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.14.m14.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.14.m14.1d">italic_λ</annotation></semantics></math>, assigns weights to image features. A higher value of <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.p4.15.m15.1"><semantics id="S3.SS2.p4.15.m15.1a"><mi id="S3.SS2.p4.15.m15.1.1" xref="S3.SS2.p4.15.m15.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.15.m15.1b"><ci id="S3.SS2.p4.15.m15.1.1.cmml" xref="S3.SS2.p4.15.m15.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.15.m15.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.15.m15.1d">italic_λ</annotation></semantics></math> increases the weight of image guidance in the inference stage, while a lower value of <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.p4.16.m16.1"><semantics id="S3.SS2.p4.16.m16.1a"><mi id="S3.SS2.p4.16.m16.1.1" xref="S3.SS2.p4.16.m16.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.16.m16.1b"><ci id="S3.SS2.p4.16.m16.1.1.cmml" xref="S3.SS2.p4.16.m16.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.16.m16.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.16.m16.1d">italic_λ</annotation></semantics></math> relatively increases the weight of text guidance (<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S3.F2" title="In 3.2 Adaptive Guidance Scaling ‣ 3 Methods ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.2">Our framework dynamically adjusts the guidance weight <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.1"><semantics id="S3.SS2.p5.1.m1.1a"><mi id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><ci id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.1d">italic_λ</annotation></semantics></math> based on the CLIPScore of the example image during the synthetic image generation process. It can also be applied to other MMDMs by setting and using a parameter <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.p5.2.m2.1"><semantics id="S3.SS2.p5.2.m2.1a"><mi id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><ci id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.2.m2.1d">italic_λ</annotation></semantics></math> to adjust the weights of image prompts or text prompts.</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.2">We measure the CLIPScore between each image prompt and class, then apply AGS to determine the direction of MMDM’s image generation. AGS samples the <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.p6.1.m1.1"><semantics id="S3.SS2.p6.1.m1.1a"><mi id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><ci id="S3.SS2.p6.1.m1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.1.m1.1d">italic_λ</annotation></semantics></math> from a truncated normal distribution based on the CLIPScore for each real image. A truncated normal distribution is a variation of the normal distribution where values are bound within a specific range. This ensures that sampled values, in this case, the prompt guidance weight <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.p6.2.m2.1"><semantics id="S3.SS2.p6.2.m2.1a"><mi id="S3.SS2.p6.2.m2.1.1" xref="S3.SS2.p6.2.m2.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.2.m2.1b"><ci id="S3.SS2.p6.2.m2.1.1.cmml" xref="S3.SS2.p6.2.m2.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.2.m2.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.2.m2.1d">italic_λ</annotation></semantics></math> do not exceed predefined limits.</p>
</div>
<div class="ltx_para" id="S3.SS2.p7">
<p class="ltx_p" id="S3.SS2.p7.5">We categorize CLIPScore into two levels: High and Low, using a threshold <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS2.p7.1.m1.1"><semantics id="S3.SS2.p7.1.m1.1a"><mi id="S3.SS2.p7.1.m1.1.1" xref="S3.SS2.p7.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.1.m1.1b"><ci id="S3.SS2.p7.1.m1.1.1.cmml" xref="S3.SS2.p7.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.1.m1.1d">italic_α</annotation></semantics></math> (default = 0.3). For image prompts with low CLIPScore (Low), we restrict the prompt weight <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.p7.2.m2.1"><semantics id="S3.SS2.p7.2.m2.1a"><mi id="S3.SS2.p7.2.m2.1.1" xref="S3.SS2.p7.2.m2.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.2.m2.1b"><ci id="S3.SS2.p7.2.m2.1.1.cmml" xref="S3.SS2.p7.2.m2.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.2.m2.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.2.m2.1d">italic_λ</annotation></semantics></math> to the range <math alttext="[\text{min}_{\text{Low}},\text{max}_{\text{Low}}]" class="ltx_Math" display="inline" id="S3.SS2.p7.3.m3.2"><semantics id="S3.SS2.p7.3.m3.2a"><mrow id="S3.SS2.p7.3.m3.2.2.2" xref="S3.SS2.p7.3.m3.2.2.3.cmml"><mo id="S3.SS2.p7.3.m3.2.2.2.3" stretchy="false" xref="S3.SS2.p7.3.m3.2.2.3.cmml">[</mo><msub id="S3.SS2.p7.3.m3.1.1.1.1" xref="S3.SS2.p7.3.m3.1.1.1.1.cmml"><mtext id="S3.SS2.p7.3.m3.1.1.1.1.2" xref="S3.SS2.p7.3.m3.1.1.1.1.2a.cmml">min</mtext><mtext id="S3.SS2.p7.3.m3.1.1.1.1.3" xref="S3.SS2.p7.3.m3.1.1.1.1.3a.cmml">Low</mtext></msub><mo id="S3.SS2.p7.3.m3.2.2.2.4" xref="S3.SS2.p7.3.m3.2.2.3.cmml">,</mo><msub id="S3.SS2.p7.3.m3.2.2.2.2" xref="S3.SS2.p7.3.m3.2.2.2.2.cmml"><mtext id="S3.SS2.p7.3.m3.2.2.2.2.2" xref="S3.SS2.p7.3.m3.2.2.2.2.2a.cmml">max</mtext><mtext id="S3.SS2.p7.3.m3.2.2.2.2.3" xref="S3.SS2.p7.3.m3.2.2.2.2.3a.cmml">Low</mtext></msub><mo id="S3.SS2.p7.3.m3.2.2.2.5" stretchy="false" xref="S3.SS2.p7.3.m3.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.3.m3.2b"><interval closure="closed" id="S3.SS2.p7.3.m3.2.2.3.cmml" xref="S3.SS2.p7.3.m3.2.2.2"><apply id="S3.SS2.p7.3.m3.1.1.1.1.cmml" xref="S3.SS2.p7.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.3.m3.1.1.1.1.1.cmml" xref="S3.SS2.p7.3.m3.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p7.3.m3.1.1.1.1.2a.cmml" xref="S3.SS2.p7.3.m3.1.1.1.1.2"><mtext id="S3.SS2.p7.3.m3.1.1.1.1.2.cmml" xref="S3.SS2.p7.3.m3.1.1.1.1.2">min</mtext></ci><ci id="S3.SS2.p7.3.m3.1.1.1.1.3a.cmml" xref="S3.SS2.p7.3.m3.1.1.1.1.3"><mtext id="S3.SS2.p7.3.m3.1.1.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p7.3.m3.1.1.1.1.3">Low</mtext></ci></apply><apply id="S3.SS2.p7.3.m3.2.2.2.2.cmml" xref="S3.SS2.p7.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p7.3.m3.2.2.2.2.1.cmml" xref="S3.SS2.p7.3.m3.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p7.3.m3.2.2.2.2.2a.cmml" xref="S3.SS2.p7.3.m3.2.2.2.2.2"><mtext id="S3.SS2.p7.3.m3.2.2.2.2.2.cmml" xref="S3.SS2.p7.3.m3.2.2.2.2.2">max</mtext></ci><ci id="S3.SS2.p7.3.m3.2.2.2.2.3a.cmml" xref="S3.SS2.p7.3.m3.2.2.2.2.3"><mtext id="S3.SS2.p7.3.m3.2.2.2.2.3.cmml" mathsize="70%" xref="S3.SS2.p7.3.m3.2.2.2.2.3">Low</mtext></ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.3.m3.2c">[\text{min}_{\text{Low}},\text{max}_{\text{Low}}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.3.m3.2d">[ min start_POSTSUBSCRIPT Low end_POSTSUBSCRIPT , max start_POSTSUBSCRIPT Low end_POSTSUBSCRIPT ]</annotation></semantics></math> to reduce the influence of the text prompt and encourage variations closer to the image prompt. Conversely, for image prompts with high CLIPScore (High), data augmentation can be more effective by increasing the influence of the text guidance. In this case, we restrict the prompt weight <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.p7.4.m4.1"><semantics id="S3.SS2.p7.4.m4.1a"><mi id="S3.SS2.p7.4.m4.1.1" xref="S3.SS2.p7.4.m4.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.4.m4.1b"><ci id="S3.SS2.p7.4.m4.1.1.cmml" xref="S3.SS2.p7.4.m4.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.4.m4.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.4.m4.1d">italic_λ</annotation></semantics></math> to the range <math alttext="[\text{min}_{\text{High}},\text{max}_{\text{High}}]" class="ltx_Math" display="inline" id="S3.SS2.p7.5.m5.2"><semantics id="S3.SS2.p7.5.m5.2a"><mrow id="S3.SS2.p7.5.m5.2.2.2" xref="S3.SS2.p7.5.m5.2.2.3.cmml"><mo id="S3.SS2.p7.5.m5.2.2.2.3" stretchy="false" xref="S3.SS2.p7.5.m5.2.2.3.cmml">[</mo><msub id="S3.SS2.p7.5.m5.1.1.1.1" xref="S3.SS2.p7.5.m5.1.1.1.1.cmml"><mtext id="S3.SS2.p7.5.m5.1.1.1.1.2" xref="S3.SS2.p7.5.m5.1.1.1.1.2a.cmml">min</mtext><mtext id="S3.SS2.p7.5.m5.1.1.1.1.3" xref="S3.SS2.p7.5.m5.1.1.1.1.3a.cmml">High</mtext></msub><mo id="S3.SS2.p7.5.m5.2.2.2.4" xref="S3.SS2.p7.5.m5.2.2.3.cmml">,</mo><msub id="S3.SS2.p7.5.m5.2.2.2.2" xref="S3.SS2.p7.5.m5.2.2.2.2.cmml"><mtext id="S3.SS2.p7.5.m5.2.2.2.2.2" xref="S3.SS2.p7.5.m5.2.2.2.2.2a.cmml">max</mtext><mtext id="S3.SS2.p7.5.m5.2.2.2.2.3" xref="S3.SS2.p7.5.m5.2.2.2.2.3a.cmml">High</mtext></msub><mo id="S3.SS2.p7.5.m5.2.2.2.5" stretchy="false" xref="S3.SS2.p7.5.m5.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.5.m5.2b"><interval closure="closed" id="S3.SS2.p7.5.m5.2.2.3.cmml" xref="S3.SS2.p7.5.m5.2.2.2"><apply id="S3.SS2.p7.5.m5.1.1.1.1.cmml" xref="S3.SS2.p7.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.5.m5.1.1.1.1.1.cmml" xref="S3.SS2.p7.5.m5.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p7.5.m5.1.1.1.1.2a.cmml" xref="S3.SS2.p7.5.m5.1.1.1.1.2"><mtext id="S3.SS2.p7.5.m5.1.1.1.1.2.cmml" xref="S3.SS2.p7.5.m5.1.1.1.1.2">min</mtext></ci><ci id="S3.SS2.p7.5.m5.1.1.1.1.3a.cmml" xref="S3.SS2.p7.5.m5.1.1.1.1.3"><mtext id="S3.SS2.p7.5.m5.1.1.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p7.5.m5.1.1.1.1.3">High</mtext></ci></apply><apply id="S3.SS2.p7.5.m5.2.2.2.2.cmml" xref="S3.SS2.p7.5.m5.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p7.5.m5.2.2.2.2.1.cmml" xref="S3.SS2.p7.5.m5.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p7.5.m5.2.2.2.2.2a.cmml" xref="S3.SS2.p7.5.m5.2.2.2.2.2"><mtext id="S3.SS2.p7.5.m5.2.2.2.2.2.cmml" xref="S3.SS2.p7.5.m5.2.2.2.2.2">max</mtext></ci><ci id="S3.SS2.p7.5.m5.2.2.2.2.3a.cmml" xref="S3.SS2.p7.5.m5.2.2.2.2.3"><mtext id="S3.SS2.p7.5.m5.2.2.2.2.3.cmml" mathsize="70%" xref="S3.SS2.p7.5.m5.2.2.2.2.3">High</mtext></ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.5.m5.2c">[\text{min}_{\text{High}},\text{max}_{\text{High}}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.5.m5.2d">[ min start_POSTSUBSCRIPT High end_POSTSUBSCRIPT , max start_POSTSUBSCRIPT High end_POSTSUBSCRIPT ]</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p8">
<p class="ltx_p" id="S3.SS2.p8.5">In both scenarios, the sampling process is defined by a truncated normal distribution as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\lambda\sim\text{Trunc}\mathcal{N}(\mu,\sigma^{2},\text{min},\text{max})," class="ltx_Math" display="block" id="S3.E2.m1.4"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4.1" xref="S3.E2.m1.4.4.1.1.cmml"><mrow id="S3.E2.m1.4.4.1.1" xref="S3.E2.m1.4.4.1.1.cmml"><mi id="S3.E2.m1.4.4.1.1.3" xref="S3.E2.m1.4.4.1.1.3.cmml">λ</mi><mo id="S3.E2.m1.4.4.1.1.2" xref="S3.E2.m1.4.4.1.1.2.cmml">∼</mo><mrow id="S3.E2.m1.4.4.1.1.1" xref="S3.E2.m1.4.4.1.1.1.cmml"><mtext id="S3.E2.m1.4.4.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.3a.cmml">Trunc</mtext><mo id="S3.E2.m1.4.4.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.2.cmml">⁢</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.4.4.1.1.1.4" xref="S3.E2.m1.4.4.1.1.1.4.cmml">𝒩</mi><mo id="S3.E2.m1.4.4.1.1.1.2a" xref="S3.E2.m1.4.4.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.2.cmml"><mo id="S3.E2.m1.4.4.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.4.4.1.1.1.1.2.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">μ</mi><mo id="S3.E2.m1.4.4.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.2.cmml">,</mo><msup id="S3.E2.m1.4.4.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.cmml">σ</mi><mn id="S3.E2.m1.4.4.1.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.cmml">2</mn></msup><mo id="S3.E2.m1.4.4.1.1.1.1.1.4" xref="S3.E2.m1.4.4.1.1.1.1.2.cmml">,</mo><mtext id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2a.cmml">min</mtext><mo id="S3.E2.m1.4.4.1.1.1.1.1.5" xref="S3.E2.m1.4.4.1.1.1.1.2.cmml">,</mo><mtext id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3a.cmml">max</mtext><mo id="S3.E2.m1.4.4.1.1.1.1.1.6" stretchy="false" xref="S3.E2.m1.4.4.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.4.4.1.2" xref="S3.E2.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.1.1.cmml" xref="S3.E2.m1.4.4.1"><csymbol cd="latexml" id="S3.E2.m1.4.4.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.2">similar-to</csymbol><ci id="S3.E2.m1.4.4.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.3">𝜆</ci><apply id="S3.E2.m1.4.4.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1"><times id="S3.E2.m1.4.4.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.2"></times><ci id="S3.E2.m1.4.4.1.1.1.3a.cmml" xref="S3.E2.m1.4.4.1.1.1.3"><mtext id="S3.E2.m1.4.4.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.3">Trunc</mtext></ci><ci id="S3.E2.m1.4.4.1.1.1.4.cmml" xref="S3.E2.m1.4.4.1.1.1.4">𝒩</ci><vector id="S3.E2.m1.4.4.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝜇</ci><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2">𝜎</ci><cn id="S3.E2.m1.4.4.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3">2</cn></apply><ci id="S3.E2.m1.2.2a.cmml" xref="S3.E2.m1.2.2"><mtext id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">min</mtext></ci><ci id="S3.E2.m1.3.3a.cmml" xref="S3.E2.m1.3.3"><mtext id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">max</mtext></ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">\lambda\sim\text{Trunc}\mathcal{N}(\mu,\sigma^{2},\text{min},\text{max}),</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.4d">italic_λ ∼ Trunc caligraphic_N ( italic_μ , italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , min , max ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p8.1">where the mean <math alttext="\mu" class="ltx_Math" display="inline" id="S3.SS2.p8.1.m1.1"><semantics id="S3.SS2.p8.1.m1.1a"><mi id="S3.SS2.p8.1.m1.1.1" xref="S3.SS2.p8.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.1.m1.1b"><ci id="S3.SS2.p8.1.m1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.1.m1.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.1.m1.1d">italic_μ</annotation></semantics></math> depends on the CLIPScore and is calculated as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mu=\text{min}+(\text{max}-\text{min})\times(1-\text{CLIPScore})." class="ltx_Math" display="block" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.4" xref="S3.E3.m1.1.1.1.1.4.cmml">μ</mi><mo id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml"><mtext id="S3.E3.m1.1.1.1.1.2.4" xref="S3.E3.m1.1.1.1.1.2.4a.cmml">min</mtext><mo id="S3.E3.m1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.2.3.cmml">+</mo><mrow id="S3.E3.m1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.2.2.cmml"><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml"><mtext id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2a.cmml">max</mtext><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mtext id="S3.E3.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.3a.cmml">min</mtext></mrow><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E3.m1.1.1.1.1.2.2.3" rspace="0.222em" xref="S3.E3.m1.1.1.1.1.2.2.3.cmml">×</mo><mrow id="S3.E3.m1.1.1.1.1.2.2.2.1" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.2.2.2.1.2" stretchy="false" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.2.2.2.1.1" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.cmml"><mn id="S3.E3.m1.1.1.1.1.2.2.2.1.1.2" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.2.cmml">1</mn><mo id="S3.E3.m1.1.1.1.1.2.2.2.1.1.1" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.1.cmml">−</mo><mtext id="S3.E3.m1.1.1.1.1.2.2.2.1.1.3" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.3a.cmml">CLIPScore</mtext></mrow><mo id="S3.E3.m1.1.1.1.1.2.2.2.1.3" stretchy="false" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E3.m1.1.1.1.2" lspace="0em" xref="S3.E3.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3"></eq><ci id="S3.E3.m1.1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.1.4">𝜇</ci><apply id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"><plus id="S3.E3.m1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.3"></plus><ci id="S3.E3.m1.1.1.1.1.2.4a.cmml" xref="S3.E3.m1.1.1.1.1.2.4"><mtext id="S3.E3.m1.1.1.1.1.2.4.cmml" xref="S3.E3.m1.1.1.1.1.2.4">min</mtext></ci><apply id="S3.E3.m1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2"><times id="S3.E3.m1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.3"></times><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1"><minus id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1"></minus><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2a.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2"><mtext id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2">max</mtext></ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.3"><mtext id="S3.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.3">min</mtext></ci></apply><apply id="S3.E3.m1.1.1.1.1.2.2.2.1.1.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.1"><minus id="S3.E3.m1.1.1.1.1.2.2.2.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.1"></minus><cn id="S3.E3.m1.1.1.1.1.2.2.2.1.1.2.cmml" type="integer" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.2">1</cn><ci id="S3.E3.m1.1.1.1.1.2.2.2.1.1.3a.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.3"><mtext id="S3.E3.m1.1.1.1.1.2.2.2.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.3">CLIPScore</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\mu=\text{min}+(\text{max}-\text{min})\times(1-\text{CLIPScore}).</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">italic_μ = min + ( max - min ) × ( 1 - CLIPScore ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p8.4">This way, higher CLIPScore results in lower <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.p8.2.m1.1"><semantics id="S3.SS2.p8.2.m1.1a"><mi id="S3.SS2.p8.2.m1.1.1" xref="S3.SS2.p8.2.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.2.m1.1b"><ci id="S3.SS2.p8.2.m1.1.1.cmml" xref="S3.SS2.p8.2.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.2.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.2.m1.1d">italic_λ</annotation></semantics></math> values, while lower CLIPScore leads to relatively higher <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.p8.3.m2.1"><semantics id="S3.SS2.p8.3.m2.1a"><mi id="S3.SS2.p8.3.m2.1.1" xref="S3.SS2.p8.3.m2.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.3.m2.1b"><ci id="S3.SS2.p8.3.m2.1.1.cmml" xref="S3.SS2.p8.3.m2.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.3.m2.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.3.m2.1d">italic_λ</annotation></semantics></math> values being sampled. The reason for increasing the <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.p8.4.m3.1"><semantics id="S3.SS2.p8.4.m3.1a"><mi id="S3.SS2.p8.4.m3.1.1" xref="S3.SS2.p8.4.m3.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.4.m3.1b"><ci id="S3.SS2.p8.4.m3.1.1.cmml" xref="S3.SS2.p8.4.m3.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.4.m3.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.4.m3.1d">italic_λ</annotation></semantics></math> for images with low CLIPScore is to mitigate the risk of MMDM generating images that deviate from the target distribution. While this adjustment may not yield significant performance improvements, it is deemed preferable to avoid generating incorrect samples that could adversely impact the results. Whereas, for images with high CLIPScore, the probability of deviating from the target distribution is low. Therefore, the weight of the text prompt is increased to enhance the diversity of the generated synthetic images.</p>
</div>
<div class="ltx_para" id="S3.SS2.p9">
<p class="ltx_p" id="S3.SS2.p9.2">As the number of examples per class increases, the effectiveness of synthetic images may decrease <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib10" title="">10</a>]</cite>. This is because the increased number of real images can compensate for a part of the required diversity. Additionally, as more real images are used, the risk of domain shift decreases, leading us to mitigate the limitations on diversity. To gradually relax the constraints on diversity as the number of real image examples increases, we set <math alttext="\sigma=0.05\times n" class="ltx_Math" display="inline" id="S3.SS2.p9.1.m1.1"><semantics id="S3.SS2.p9.1.m1.1a"><mrow id="S3.SS2.p9.1.m1.1.1" xref="S3.SS2.p9.1.m1.1.1.cmml"><mi id="S3.SS2.p9.1.m1.1.1.2" xref="S3.SS2.p9.1.m1.1.1.2.cmml">σ</mi><mo id="S3.SS2.p9.1.m1.1.1.1" xref="S3.SS2.p9.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS2.p9.1.m1.1.1.3" xref="S3.SS2.p9.1.m1.1.1.3.cmml"><mn id="S3.SS2.p9.1.m1.1.1.3.2" xref="S3.SS2.p9.1.m1.1.1.3.2.cmml">0.05</mn><mo id="S3.SS2.p9.1.m1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p9.1.m1.1.1.3.1.cmml">×</mo><mi id="S3.SS2.p9.1.m1.1.1.3.3" xref="S3.SS2.p9.1.m1.1.1.3.3.cmml">n</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.1.m1.1b"><apply id="S3.SS2.p9.1.m1.1.1.cmml" xref="S3.SS2.p9.1.m1.1.1"><eq id="S3.SS2.p9.1.m1.1.1.1.cmml" xref="S3.SS2.p9.1.m1.1.1.1"></eq><ci id="S3.SS2.p9.1.m1.1.1.2.cmml" xref="S3.SS2.p9.1.m1.1.1.2">𝜎</ci><apply id="S3.SS2.p9.1.m1.1.1.3.cmml" xref="S3.SS2.p9.1.m1.1.1.3"><times id="S3.SS2.p9.1.m1.1.1.3.1.cmml" xref="S3.SS2.p9.1.m1.1.1.3.1"></times><cn id="S3.SS2.p9.1.m1.1.1.3.2.cmml" type="float" xref="S3.SS2.p9.1.m1.1.1.3.2">0.05</cn><ci id="S3.SS2.p9.1.m1.1.1.3.3.cmml" xref="S3.SS2.p9.1.m1.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.1.m1.1c">\sigma=0.05\times n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p9.1.m1.1d">italic_σ = 0.05 × italic_n</annotation></semantics></math>, where <math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.p9.2.m2.1"><semantics id="S3.SS2.p9.2.m2.1a"><mi id="S3.SS2.p9.2.m2.1.1" xref="S3.SS2.p9.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.2.m2.1b"><ci id="S3.SS2.p9.2.m2.1.1.cmml" xref="S3.SS2.p9.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p9.2.m2.1d">italic_n</annotation></semantics></math> represents the number of examples per class. This strategy allows us to leverage the information of each image to maximize diversity while maintaining the semantic features of the class.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Synthetic Data Generation</h3>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="114" id="S3.F3.g1" src="x3.png" width="545"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Example of text prompts generated by the LLM.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p1.1.1">Text Prompt Generation with LLM.</span>
To generate text prompts for the MMDM, we enhance the input by providing information about the target dataset and class details to the LLM. Additional instructions are added to increase the diversity of semantic content. For this purpose, we use the GPT-4 (gpt4-0613) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib1" title="">1</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.2">In this stage, we generate multiple prompts for each sample, iterating <math alttext="M" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">italic_M</annotation></semantics></math> times for <math alttext="N" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">italic_N</annotation></semantics></math> training samples. Specifically, we incorporate descriptions of the downstream dataset and class information into the LLM. This aids the LLM in producing more natural and diverse prompts. Additionally, when generating prompts for a specific class, we supply the in-distribution classes to the LLM and include instructions to ensure the LLM avoided representing multiple in-distribution classes within a single prompt.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">In <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S3.F3" title="In 3.3 Synthetic Data Generation ‣ 3 Methods ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, the generated text prompts show that the LLM can create class-centric sentences that are realistic, stay within the provided information, and maintain the semantic characteristics of each class while generating scenarios in various environments.
Details of the hyperparameters and actual prompts are reported in the supplementary material.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.4"><span class="ltx_text ltx_font_bold" id="S3.SS3.p4.4.1">Synthetic Image Generation.</span>
The resultant text and image prompts are randomly sampled and then fed into the MMDM. For each sample, we generate <math alttext="M" class="ltx_Math" display="inline" id="S3.SS3.p4.1.m1.1"><semantics id="S3.SS3.p4.1.m1.1a"><mi id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><ci id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.1.m1.1d">italic_M</annotation></semantics></math> synthetic images using <math alttext="M" class="ltx_Math" display="inline" id="S3.SS3.p4.2.m2.1"><semantics id="S3.SS3.p4.2.m2.1a"><mi id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><ci id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.2.m2.1d">italic_M</annotation></semantics></math> text prompts. In the 1-shot setting, the total number of synthetic data is <math alttext="M\times N" class="ltx_Math" display="inline" id="S3.SS3.p4.3.m3.1"><semantics id="S3.SS3.p4.3.m3.1a"><mrow id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml"><mi id="S3.SS3.p4.3.m3.1.1.2" xref="S3.SS3.p4.3.m3.1.1.2.cmml">M</mi><mo id="S3.SS3.p4.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p4.3.m3.1.1.1.cmml">×</mo><mi id="S3.SS3.p4.3.m3.1.1.3" xref="S3.SS3.p4.3.m3.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><apply id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1"><times id="S3.SS3.p4.3.m3.1.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1.1"></times><ci id="S3.SS3.p4.3.m3.1.1.2.cmml" xref="S3.SS3.p4.3.m3.1.1.2">𝑀</ci><ci id="S3.SS3.p4.3.m3.1.1.3.cmml" xref="S3.SS3.p4.3.m3.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">M\times N</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.3.m3.1d">italic_M × italic_N</annotation></semantics></math>. As the number of real examples per class increases, the total number of synthetic images becomes <math alttext="M\times N" class="ltx_Math" display="inline" id="S3.SS3.p4.4.m4.1"><semantics id="S3.SS3.p4.4.m4.1a"><mrow id="S3.SS3.p4.4.m4.1.1" xref="S3.SS3.p4.4.m4.1.1.cmml"><mi id="S3.SS3.p4.4.m4.1.1.2" xref="S3.SS3.p4.4.m4.1.1.2.cmml">M</mi><mo id="S3.SS3.p4.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p4.4.m4.1.1.1.cmml">×</mo><mi id="S3.SS3.p4.4.m4.1.1.3" xref="S3.SS3.p4.4.m4.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.4.m4.1b"><apply id="S3.SS3.p4.4.m4.1.1.cmml" xref="S3.SS3.p4.4.m4.1.1"><times id="S3.SS3.p4.4.m4.1.1.1.cmml" xref="S3.SS3.p4.4.m4.1.1.1"></times><ci id="S3.SS3.p4.4.m4.1.1.2.cmml" xref="S3.SS3.p4.4.m4.1.1.2">𝑀</ci><ci id="S3.SS3.p4.4.m4.1.1.3.cmml" xref="S3.SS3.p4.4.m4.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m4.1c">M\times N</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.4.m4.1d">italic_M × italic_N</annotation></semantics></math> multiplied by the number of real examples per class.
For downstream classification tasks, a classifier can be trained with both real and synthetic datasets.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Setup</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Datasets.</span></p>
</div>
<figure class="ltx_figure" id="S4.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="468" id="S4.F4.sf1.g1" src="extracted/5878964/imgs/clipscore_hist_pets.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F4.sf1.3.2" style="font-size:90%;">Oxford Pets (HC)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="460" id="S4.F4.sf2.g1" src="extracted/5878964/imgs/clipscore_hist_caltech.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F4.sf2.3.2" style="font-size:90%;">Caltech-101 (HC)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F4.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="460" id="S4.F4.sf3.g1" src="extracted/5878964/imgs/clipscore_hist_flowers.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S4.F4.sf3.3.2" style="font-size:90%;">Flowers102 (LC)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.3.2" style="font-size:90%;">CLIPScore distribution of datasets. Oxford Pets and Caltech-101 belong to the High CLIPScore (HC) group, with images from each class showing high CLIPScores. In contrast, Flowers102 has a higher proportion of classes with low CLIPScores, placing it in the Low CLIPScore (LC) group.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">To demonstrate that our method can generate diverse synthetic images in data-scarce scenarios, we measure the accuracy in few-shot classification tasks.
We recognize the importance of accurate alignment between real images and class names in the generation of synthetic data. For a comprehensive and fair analysis, we select a subset of few-shot datasets from those used in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib10" title="">10</a>]</cite>, based on their CLIPScores. These CLIPScores are calculated using the same methodology described in <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S3.SS1" title="3.1 Measuring CLIPScore ‣ 3 Methods ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>.
Each dataset exhibits various distributions in <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4.F4" title="In 4.1 Setup ‣ 4 Experiments ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>;
Caltech-101 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib7" title="">7</a>]</cite>, which belongs to the common dataset, recorded a high average CLIPScore of 0.8406, categorizing it as a High CLIPScore (HC) group. Oxford Pets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib21" title="">21</a>]</cite>, which belongs to the fine-grained dataset, also showed a relatively high CLIPScore of 0.7782, placing it in the HC group as well. In contrast, Flowers102 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib19" title="">19</a>]</cite>, another fine-grained dataset, has an average CLIPScore of 0.5548, placing it in the Low CLIPScore (LC) group.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Implementation Details.</span>
For all experiments on classification tasks, we use <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p3.1.2">ResNet50</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib9" title="">9</a>]</cite> pre-trained on ImageNet-1k and <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p3.1.3">CLIP-ViT-B/16</span>. For each training sample, the number of text prompts <math alttext="M=10" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">M</mi><mo id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><eq id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1"></eq><ci id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">𝑀</ci><cn id="S4.SS1.p3.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.p3.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">M=10</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">italic_M = 10</annotation></semantics></math>. For training the classifier, we sample the real training images and the generated images with a uniform distribution.
More details on hyperparameters can be found <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#Pt0.A2.T4" title="In Appendix 0.B Hyperparameters ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a> in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#Pt0.A2" title="Appendix 0.B Hyperparameters ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">0.B</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">We compare our method with existing state-of-the-art methods, Real Guidance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib10" title="">10</a>]</cite> and DA-Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib30" title="">30</a>]</cite>. For these methods, we utilize the implementations found in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib30" title="">30</a>]</cite> to reproduce their methods. The hyperparameters are set according to the specifications provided in each respective paper. Base Prompt refers to using the CLIP template, <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p4.1.1">‘‘a photo of a {class}’’</span>, and LLM Prompt refers to using prompts generated by LLM. Random Scaling (RS) refers to generating synthetic images by randomly scaling the <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.SS1.p4.1.m1.1"><semantics id="S4.SS1.p4.1.m1.1a"><mi id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><ci id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.1.m1.1d">italic_λ</annotation></semantics></math> values in MMDM. For all quantitative experiments, we measure the average of three trials.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experimental Results</h3>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.2.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.3.2" style="font-size:90%;">Diversity analysis results on the 1-shot setting. DALDA achieves enhanced diversity compared to other methods. The best results are highlighted in bold.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.4" style="width:433.6pt;height:131.5pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-22.3pt,6.7pt) scale(0.906705388198835,0.906705388198835) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.4.1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T1.4.1.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T1.4.1.1.1.2">Oxford Pets (HC)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T1.4.1.1.1.3">Caltech-101 (HC)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T1.4.1.1.1.4">Flowers102 (LC)</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.1.2.2">
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.2.2.1">Methods</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.2.2.2">CLIP-I(↓)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.2.2.3">LPIPS(↑)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.2.2.4">CLIP-I(↓)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.2.2.5">LPIPS(↑)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.2.2.6">CLIP-I(↓)</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.1.2.2.7">LPIPS(↑)</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.3.3.1">Real Guidance</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.3.3.2">0.9450</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.3.3.3">0.3886</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.3.3.4">0.9272</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.3.3.5">0.3426</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.3.3.6">0.9523</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.4.1.3.3.7">0.4194</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.1.4.4">
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.4.4.1">DA-Fusion</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.4.4.2">0.9105</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.4.4.3">0.4978</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.4.4.4">0.8793</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.4.4.5">0.4633</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.4.4.6">0.9271</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.1.4.4.7">0.5284</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.5.5.1">Base Prompt + RS</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.5.5.2">0.9196</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.5.5.3">0.6744</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.5.5.4">0.8850</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.5.5.5">0.6866</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.1.5.5.6">0.9168</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.4.1.5.5.7">0.7492</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.1.6.6">
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.6.6.1">Base Prompt + AGS</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.6.6.2">0.9229</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.6.6.3">0.6967</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.6.6.4">0.8978</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.6.6.5">0.6945</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.6.6.6">0.9297</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.1.6.6.7"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.6.6.7.1">0.7505</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.1.7.7">
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.7.7.1">LLM Prompt + RS</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.7.7.2">0.8947</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.7.7.3">0.6912</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.7.7.4"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.7.7.4.1">0.8708</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.7.7.5">0.6951</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.1.7.7.6"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.7.7.6.1">0.9161</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.1.7.7.7">0.7474</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.1.8.8">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.8.8.1"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.8.8.1.1">LLM Prompt + AGS (DALDA)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.8.8.2"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.8.8.2.1">0.8816</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.8.8.3"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.8.8.3.1">0.7090</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.8.8.4">0.8794</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.8.8.5"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.8.8.5.1">0.7050</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.1.8.8.6">0.9208</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T1.4.1.8.8.7">0.7459</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.2.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.3.2" style="font-size:90%;">1-shot classification accuracy of classifiers trained on synthetic images.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.4" style="width:433.6pt;height:129.2pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-26.5pt,7.8pt) scale(0.891176991162653,0.891176991162653) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.4.1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T2.4.1.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T2.4.1.1.1.2">Oxford Pets (HC)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T2.4.1.1.1.3">Caltech-101 (HC)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T2.4.1.1.1.4">Flowers102 (LC)</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.1.2.2">
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.2.2.1">Methods</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.2.2.2">Acc-CLIP</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.2.2.3">Acc-RN50</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.2.2.4">Acc-CLIP</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.2.2.5">Acc-RN50</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.2.2.6">Acc-CLIP</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.1.2.2.7">Acc-RN50</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.3.3.1">Real Guidance</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.3.3.2">0.8623</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.3.3.3">0.6654</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.3.3.4">0.9182</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.3.3.5">0.7010</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.3.3.6">0.8176</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.4.1.3.3.7">0.5038</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.1.4.4">
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.4.4.1">DA-Fusion</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.4.4.2">0.8662</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.4.4.3">0.6324</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.4.4.4">0.9159</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.4.4.5">0.6851</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.4.4.6"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.4.4.6.1">0.8333</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.1.4.4.7">0.5259</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.5.5.1">Base Prompt + RS</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.5.5.2">0.8715</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.5.5.3">0.7773</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.5.5.4">0.9148</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.5.5.5">0.7829</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.1.5.5.6">0.8109</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.4.1.5.5.7">0.5280</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.1.6.6">
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.6.6.1">Base Prompt + AGS</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.6.6.2">0.8728</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.6.6.3">0.7891</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.6.6.4">0.9176</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.6.6.5"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.6.6.5.1">0.7916</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.6.6.6">0.8140</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.1.6.6.7">0.5212</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.1.7.7">
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.7.7.1">LLM Prompt + RS</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.7.7.2">0.8742</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.7.7.3">0.8017</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.7.7.4">0.9184</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.7.7.5">0.7808</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.1.7.7.6">0.8285</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.1.7.7.7"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.7.7.7.1">0.5348</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.1.8.8">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.1.8.8.1"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.8.8.1.1">LLM Prompt + AGS (DALDA)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.1.8.8.2"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.8.8.2.1">0.8745</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.1.8.8.3"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.8.8.3.1">0.8069</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.1.8.8.4"><span class="ltx_text ltx_font_bold" id="S4.T2.4.1.8.8.4.1">0.9202</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.1.8.8.5">0.7879</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.1.8.8.6">0.8189</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.4.1.8.8.7">0.5208</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In this section, we present a comprehensive analysis of the diversity and efficacy of synthetic images generated by DALDA, across different datasets. Our evaluation utilizes well-established metrics, CLIP-I <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib25" title="">25</a>]</cite> and LPIPS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib33" title="">33</a>]</cite> to quantify diversity. For CLIP-I, a lower score indicates greater diversity, while for LPIPS, a higher score indicates greater diversity.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4.T1" title="In 4.2 Experimental Results ‣ 4 Experiments ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a> shows that the diversity of synthetic images generated by our method is generally higher compared to Real Guidance and DA-Fusion. This indicates that our method can supplement diversity with only one image per class compared to existing methods.
In the HC dataset, DALDA shows outstanding performance, demonstrating its ability to generate synthetic images that are diverse and rich in information. This underscores its utility as an invaluable resource for training classifiers, especially when data availability is constrained. However, as detailed in <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S3.SS2" title="3.2 Adaptive Guidance Scaling ‣ 3 Methods ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>, when dealing with image prompts with low CLIPScore, the strategy shifts towards minimizing diversity to mitigate the risk of generating inaccurate samples. Consequently, in the LC dataset, DALDA utilizing LLM Prompt and AGS tends to limit diversity more than other methods applying RS. This cautious approach ensures that the generated sample closely fits the target distribution, aiming to maintain alignment with the target distribution, even if it does not always lead to higher accuracy.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Additionally, we measure the accuracy of classifiers trained on synthetic images, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4.T2" title="In 4.2 Experimental Results ‣ 4 Experiments ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>. While DALDA does not outperform in the LC dataset, it consistently shows robust performance and achieves remarkable results in the HC dataset without additional fine-tuning. Additional statistical tests are provided in <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#Pt0.A1.T3" title="In Appendix 0.A Statistical Test ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a> of <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#Pt0.A1" title="Appendix 0.A Statistical Test ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">0.A</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">In <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4.F5" title="In 4.2 Experimental Results ‣ 4 Experiments ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>, we conduct a qualitative comparison of our method, including the baselines. The visualization results analyze how synthetic images are generated for example images with low and high CLIPScore, demonstrating the effectiveness of our method in each case. Both Real Guidance and DA-Fusion fall short of the other methods in terms of diversity across the examples. Our final method, LLM Prompt + AGS, exhibits the highest diversity in high CLIPScore examples compared to the existing methods, while it shows the lowest diversity in low CLIPScore examples, excluding the two prior methods. This visually demonstrates that our intended AGS is designed to produce images with greater diversity in high CLIPScore examples while reducing diversity in low CLIPScore examples. When using high CLIPScore examples as image prompts (<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4.F5.sf1" title="In Figure 5 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5(a)</span></a>), AGS increases the diversity of synthetic images. Conversely, when using low CLIPScore examples as image prompts (<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4.F5.sf2" title="In Figure 5 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5(b)</span></a>), AGS decreases diversity to keep the synthetic images within the target distribution.</p>
</div>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="743" id="S4.F5.sf1.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F5.sf1.3.2" style="font-size:90%;">High CLIPScore Examples</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="743" id="S4.F5.sf2.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F5.sf2.3.2" style="font-size:90%;">Low CLIPScore Examples</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.3.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.4.2" style="font-size:90%;">Synthetic image comparisons. Base Prompt refers to using the CLIP template, <span class="ltx_text ltx_font_typewriter" id="S4.F5.4.2.1">‘‘a photo of a {class}’’</span>, and LLM Prompt refers to using prompts generated by the LLM. In the case of a low CLIPScore example, a Base Prompt might fail to maintain class consistency (red box), resulting in the generation of synthetic images that do not accurately represent the intended class.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="445" id="S4.F6.sf1.g1" src="extracted/5878964/imgs/16shot_pets_clip_i.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.sf1.4.2.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F6.sf1.2.1" style="font-size:90%;">CLIP-I (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.F6.sf1.2.1.m1.1"><semantics id="S4.F6.sf1.2.1.m1.1b"><mo id="S4.F6.sf1.2.1.m1.1.1" stretchy="false" xref="S4.F6.sf1.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.F6.sf1.2.1.m1.1c"><ci id="S4.F6.sf1.2.1.m1.1.1.cmml" xref="S4.F6.sf1.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.sf1.2.1.m1.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F6.sf1.2.1.m1.1e">↓</annotation></semantics></math>)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="446" id="S4.F6.sf2.g1" src="extracted/5878964/imgs/16shot_pets_lpips.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.sf2.4.2.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F6.sf2.2.1" style="font-size:90%;">LPIPS (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.F6.sf2.2.1.m1.1"><semantics id="S4.F6.sf2.2.1.m1.1b"><mo id="S4.F6.sf2.2.1.m1.1.1" stretchy="false" xref="S4.F6.sf2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.F6.sf2.2.1.m1.1c"><ci id="S4.F6.sf2.2.1.m1.1.1.cmml" xref="S4.F6.sf2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.sf2.2.1.m1.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.F6.sf2.2.1.m1.1e">↑</annotation></semantics></math>)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S4.F6.3.2" style="font-size:90%;">Diversity comparison on Oxford Pets. Our method maintains a similar level of diversity even as the number of examples increases, compared to other methods. This demonstrates that our approach quickly converges to the target diversity even when there is only one example per class.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="446" id="S4.F7.sf1.g1" src="extracted/5878964/imgs/3methods_pets_rn50.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F7.sf1.3.2" style="font-size:90%;">ResNet</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="445" id="S4.F7.sf2.g1" src="extracted/5878964/imgs/3methods_pets_clip.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F7.sf2.3.2" style="font-size:90%;">CLIP</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S4.F7.3.2" style="font-size:90%;">Comparison results with state-of-the-art approaches on the Oxford Pets shows that the proposed method achieves the highest N-shot accuracies in most settings. Our 1-shot accuracy with ResNet is comparable to the 4-shot accuracies of other methods.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F8.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="442" id="S4.F8.sf1.g1" src="extracted/5878964/imgs/scale0to1_pets_rn50_avg.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F8.sf1.3.2" style="font-size:90%;">Oxford Pets (ResNet)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F8.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="442" id="S4.F8.sf2.g1" src="extracted/5878964/imgs/scale0to1_caltech_rn50_avg.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F8.sf2.3.2" style="font-size:90%;">Caltech-101 (ResNet)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F8.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="442" id="S4.F8.sf3.g1" src="extracted/5878964/imgs/scale0to1_flowers_rn50_avg.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S4.F8.sf3.3.2" style="font-size:90%;">Flowers102 (ResNet)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F8.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="442" id="S4.F8.sf4.g1" src="extracted/5878964/imgs/scale0to1_pets_clip_avg.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.sf4.2.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S4.F8.sf4.3.2" style="font-size:90%;">Oxford Pets (CLIP)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F8.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="442" id="S4.F8.sf5.g1" src="extracted/5878964/imgs/scale0to1_caltech_clip_avg.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.sf5.2.1.1" style="font-size:90%;">(e)</span> </span><span class="ltx_text" id="S4.F8.sf5.3.2" style="font-size:90%;">Caltech-101 (CLIP)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F8.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="448" id="S4.F8.sf6.g1" src="extracted/5878964/imgs/scale0to1_flowers_clip_avg.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.sf6.2.1.1" style="font-size:90%;">(f)</span> </span><span class="ltx_text" id="S4.F8.sf6.3.2" style="font-size:90%;">Flowers102 (CLIP)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.4.2.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S4.F8.2.1" style="font-size:90%;">LLM Prompt vs. Base Prompt (1-shot accuracy). At a fixed prompt weight <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.F8.2.1.m1.1"><semantics id="S4.F8.2.1.m1.1b"><mi id="S4.F8.2.1.m1.1.1" xref="S4.F8.2.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S4.F8.2.1.m1.1c"><ci id="S4.F8.2.1.m1.1.1.cmml" xref="S4.F8.2.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F8.2.1.m1.1d">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.F8.2.1.m1.1e">italic_λ</annotation></semantics></math>, LLM Prompt is generally more effective for model training compared to Base Prompt. Each horizontal line represents the average 1-shot accuracy for both the LLM and Base Prompt methods, as well as the accuracy achieved by DALDA.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.p1.1.1">Effectiveness of DALDA.</span>
We conducted comparative experiments (<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4.F6" title="In 4.2 Experimental Results ‣ 4 Experiments ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Figs.</span> <span class="ltx_text ltx_ref_tag">6</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4.F7" title="Figure 7 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">7</span></a>) on Oxford Pets to evaluate the diversity and downstream models’ accuracy when increasing the number of examples per class. Across all methods, the increase in the number of examples generally leads to a reduction in changes to diversity. In contrast, our method maintains a consistent level of diversity regardless of the number of examples. This indicates that our approach can quickly achieve the desired diversity with a small number of examples. For the CLIP-I score, our value falls between Real Guidance and DA-Fusion in <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4.F6.sf1" title="In Figure 6 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6(a)</span></a>. However, in terms of accuracy, our method surpasses both. This indicates that our approach not only focuses on enhancing diversity but also on limiting excessive diversity that could negatively affect the outcomes. Our method outperforms existing methods in terms of accuracy in <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4.F7" title="In 4.2 Experimental Results ‣ 4 Experiments ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a>. In particular, with ResNet, our approach achieves similar or better results with just one example image compared to using four example images with other methods in <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4.F7.sf1" title="In Figure 7 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7(a)</span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F9.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="442" id="S5.F9.sf1.g1" src="extracted/5878964/imgs/prompt5to20_acc_rn50.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F9.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S5.F9.sf1.3.2" style="font-size:90%;">ResNet Accuracy</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F9.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="442" id="S5.F9.sf2.g1" src="extracted/5878964/imgs/prompt5to20_acc_clip.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F9.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S5.F9.sf2.3.2" style="font-size:90%;">CLIP Accuracy</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F9.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="442" id="S5.F9.sf3.g1" src="extracted/5878964/imgs/prompt5to20_clip_i.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F9.sf3.4.2.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S5.F9.sf3.2.1" style="font-size:90%;">CLIP-I (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.F9.sf3.2.1.m1.1"><semantics id="S5.F9.sf3.2.1.m1.1b"><mo id="S5.F9.sf3.2.1.m1.1.1" stretchy="false" xref="S5.F9.sf3.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.F9.sf3.2.1.m1.1c"><ci id="S5.F9.sf3.2.1.m1.1.1.cmml" xref="S5.F9.sf3.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F9.sf3.2.1.m1.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.F9.sf3.2.1.m1.1e">↓</annotation></semantics></math>)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F9.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="442" id="S5.F9.sf4.g1" src="extracted/5878964/imgs/prompt5to20_lpips.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F9.sf4.4.2.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S5.F9.sf4.2.1" style="font-size:90%;">LPIPS (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.F9.sf4.2.1.m1.1"><semantics id="S5.F9.sf4.2.1.m1.1b"><mo id="S5.F9.sf4.2.1.m1.1.1" stretchy="false" xref="S5.F9.sf4.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.F9.sf4.2.1.m1.1c"><ci id="S5.F9.sf4.2.1.m1.1.1.cmml" xref="S5.F9.sf4.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F9.sf4.2.1.m1.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.F9.sf4.2.1.m1.1e">↑</annotation></semantics></math>)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S5.F9.3.2" style="font-size:90%;">Comparison experiments with scaling methods on Oxford Pets (1-shot accuracy). Regardless of the number of prompts, in all cases where the LLM is used, AGS generates more diverse and effective synthetic images than RS.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">LLM Can Provide Beneficial Semantic Information.</span>
We conducted a comparative analysis of models trained on synthetic images generated by fixing <math alttext="\lambda" class="ltx_Math" display="inline" id="S5.p2.1.m1.1"><semantics id="S5.p2.1.m1.1a"><mi id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><ci id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S5.p2.1.m1.1d">italic_λ</annotation></semantics></math> at increments of 0.1 from 0 to 1 according to two different text prompts formats (<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4.F8" title="In 4.2 Experimental Results ‣ 4 Experiments ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a>). Across all datasets, we found that using LLM Prompt generally yielded higher performance than using the Base Prompt. This indicates that leveraging the expanded semantic information from the LLM is more effective for data augmentation than generating synthetic images with a Base Prompt using MMDM. However, even with the LLM, the performance can be lower than the Base Prompt depending on how the prompt weight is utilized. This underscores the necessity of controlling for diversity to ensure it aligns with the purpose of data augmentation. In <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S5.F9" title="In 5 Analysis ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">9</span></a>, we analyzed the impact of the number of prompts on Oxford Pets. Even as the number of LLM prompts increases, diversity does not change significantly. However, the accuracy of downstream models increases linearly with the number of prompts. This suggests that semantic diversity, in addition to visual diversity, influences model performance. It also demonstrates that AGS can produce more effective synergies when using LLM prompts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p" id="S5.p3.6"><span class="ltx_text ltx_font_bold" id="S5.p3.6.1">Fixing Guidance weight is Vulnerable to Distribution Changes.</span>
In <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4.F8" title="In 4.2 Experimental Results ‣ 4 Experiments ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a>, we found that fixing the <math alttext="\lambda" class="ltx_Math" display="inline" id="S5.p3.1.m1.1"><semantics id="S5.p3.1.m1.1a"><mi id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><ci id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S5.p3.1.m1.1d">italic_λ</annotation></semantics></math> value according to the dataset can significantly impact the performance of synthetic images. Particularly, fixing the <math alttext="\lambda" class="ltx_Math" display="inline" id="S5.p3.2.m2.1"><semantics id="S5.p3.2.m2.1a"><mi id="S5.p3.2.m2.1.1" xref="S5.p3.2.m2.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S5.p3.2.m2.1b"><ci id="S5.p3.2.m2.1.1.cmml" xref="S5.p3.2.m2.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.2.m2.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S5.p3.2.m2.1d">italic_λ</annotation></semantics></math> value for all classes can make it dependent on the overall CLIPScore distribution of the dataset. For example, in the cases of Oxford Pets and Caltech-101 (HC), the best performance is achieved at lower <math alttext="\lambda" class="ltx_Math" display="inline" id="S5.p3.3.m3.1"><semantics id="S5.p3.3.m3.1a"><mi id="S5.p3.3.m3.1.1" xref="S5.p3.3.m3.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S5.p3.3.m3.1b"><ci id="S5.p3.3.m3.1.1.cmml" xref="S5.p3.3.m3.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.3.m3.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S5.p3.3.m3.1d">italic_λ</annotation></semantics></math> values, which reflect more on the text prompts. On the other hand, for Flowers102 (LC), it is difficult to find a linear relationship with changes in the <math alttext="\lambda" class="ltx_Math" display="inline" id="S5.p3.4.m4.1"><semantics id="S5.p3.4.m4.1a"><mi id="S5.p3.4.m4.1.1" xref="S5.p3.4.m4.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S5.p3.4.m4.1b"><ci id="S5.p3.4.m4.1.1.cmml" xref="S5.p3.4.m4.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.4.m4.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S5.p3.4.m4.1d">italic_λ</annotation></semantics></math>, and even at lower <math alttext="\lambda" class="ltx_Math" display="inline" id="S5.p3.5.m5.1"><semantics id="S5.p3.5.m5.1a"><mi id="S5.p3.5.m5.1.1" xref="S5.p3.5.m5.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S5.p3.5.m5.1b"><ci id="S5.p3.5.m5.1.1.cmml" xref="S5.p3.5.m5.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.5.m5.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S5.p3.5.m5.1d">italic_λ</annotation></semantics></math> values, the accuracy for both Base and LLM Prompts is significantly lower than the average values. Therefore, it is important to recognize that fixing the <math alttext="\lambda" class="ltx_Math" display="inline" id="S5.p3.6.m6.1"><semantics id="S5.p3.6.m6.1a"><mi id="S5.p3.6.m6.1.1" xref="S5.p3.6.m6.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S5.p3.6.m6.1b"><ci id="S5.p3.6.m6.1.1.cmml" xref="S5.p3.6.m6.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.6.m6.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S5.p3.6.m6.1d">italic_λ</annotation></semantics></math> value or guidance weight according to the dataset can significantly degrade performance under certain conditions, emphasizing the need to adjust these parameters to suit the characteristics of the data. This is demonstrated by the values represented by each horizontal line in <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#S4.F8" title="In 4.2 Experimental Results ‣ 4 Experiments ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a>. In the LC dataset, the accuracy of DALDA may not be significantly higher than the average accuracy of the Base Prompt and LLM Prompt. However, in the HC dataset, DALDA achieves higher accuracy compared to the average accuracy of the Base Prompt and LLM Prompt.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitations and Future Works</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Our approach starts by measuring the CLIPScore of example images. This implies that our method, which adjusts the generation strategy based on CLIPScore, may inherently reflect the limitations of CLIP itself. This is especially true for LC datasets like Flowers102, where effectiveness is limited. Recent studies have investigated metrics that leverage feature values extracted from the embeddings of models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib20" title="">20</a>]</cite> pre-trained using self-supervised methods. Additionally, Hu <em class="ltx_emph ltx_font_italic" id="S6.p1.1.1">et al</em>.<span class="ltx_text" id="S6.p1.1.2"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#bib.bib13" title="">13</a>]</cite> proposed to evaluate how faithfully generated images adhere to text inputs using visual question answering. Exploring better metrics beyond CLIPScore could be a valuable research direction.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">We evaluate synthetic images using image diversity metrics and the few-shot accuracy. While our method shows more pronounced performance improvements with ResNet, the performance improvements are relatively smaller for the CLIP classifier. This is likely because CLIP already includes a certain level of semantic information. This implies that the distribution of effective synthetic images may vary depending on the downstream model. Further exploring synthetic image generation strategies with downstream model knowledge could be beneficial.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusions</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">We propose a framework, DALDA, that effectively utilizes LLM and DM for data augmentation in data-scarce scenarios. Our method aims to generate synthetic images that maintain class consistency while enriching diversity, even with just one example per class. By leveraging the knowledge of LLM, we create class-specific text prompts that introduce novel semantic information. We then apply AGS, dynamically adjusting the image generation strategy based on each image’s CLIPScore.
Our method enhances the diversity of synthetic images and improves downstream model performance in experiments. In 1-shot classification, DALDA achieved a mean accuracy improvement of 8.18% over RG and 9.08% over DA-Fusion. Through ablation studies, we provide insights by analyzing the components of our framework from a data augmentation perspective. This demonstrates that our approach is a powerful data augmentation method capable of achieving appropriate diversity in data-scarce situations.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was supported by the IITP (Institute of Information &amp; Communications Technology Planning &amp; Evaluation)-ICAN (ICT Challenge and Advanced Network of HRD) (IITP-2024-RS-2023-00259806, 20%) grant funded by the Korea government (Ministry of Science and ICT) and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2024-00354675, 50% and No. RS-2024-00352184, 30%).</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Azizi, S., Kornblith, S., Saharia, C., Norouzi, M., Fleet, D.J.: Synthetic data from diffusion models improves imagenet classification. Transactions on Machine Learning Research (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning augmentation strategies from data. In: Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition. pp. 113–123 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated data augmentation with a reduced search space. In: Advances in Neural Information Processing Systems. vol. 33, pp. 18613–18624 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition. pp. 248–255. IEEE (2009)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Fan, L., Chen, K., Krishnan, D., Katabi, D., Isola, P., Tian, Y.: Scaling laws of synthetic images for model training… for now. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7382–7392 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Fei-Fei, L., Fergus, R., Perona, P.: One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence <span class="ltx_text ltx_font_bold" id="bib.bib7.1.1">28</span>(4), 594–611 (2006)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G., Cohen-Or, D.: An image is worth one word: Personalizing text-to-image generation using textual inversion. In: The Eleventh International Conference on Learning Representations (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 770–778 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
He, R., Sun, S., Yu, X., Xue, C., Zhang, W., Torr, P., Bai, S., QI, X.: Is synthetic data from generative models ready for image recognition? In: The Eleventh International Conference on Learning Representations (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Hessel, J., Holtzman, A., Forbes, M., Le Bras, R., Choi, Y.: Clipscore: A reference-free evaluation metric for image captioning. In: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. pp. 7514–7528 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib12.1.1">33</span>, 6840–6851 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Hu, Y., Liu, B., Kasai, J., Wang, Y., Ostendorf, M., Krishna, R., Smith, N.A.: Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 20406–20417 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In: International conference on machine learning. pp. 19730–19742. PMLR (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Marwood, D., Baluja, S., Alon, Y.: Diversity and diffusion: Observations on synthetic image distributions with stable diffusion. arXiv preprint arXiv:2311.00056 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Miller, G.A.: Wordnet: a lexical database for english. Communications of the ACM <span class="ltx_text ltx_font_bold" id="bib.bib16.1.1">38</span>(11), 39–41 (1995)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Mou, C., Wang, X., Xie, L., Wu, Y., Zhang, J., Qi, Z., Shan, Y.: T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In: Proceedings of the AAAI Conference on Artificial Intelligence. pp. 4296–4304 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Nichol, A.Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., Mcgrew, B., Sutskever, I., Chen, M.: Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In: International Conference on Machine Learning. pp. 16784–16804. PMLR (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Nilsback, M.E., Zisserman, A.: Automated flower classification over a large number of classes. In: 2008 Sixth Indian conference on computer vision, graphics &amp; image processing. pp. 722–729. IEEE (2008)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Oquab, M., Darcet, T., Moutakanni, T., Vo, H.V., Szafraniec, M., Khalidov, V., Fernandez, P., HAZIZA, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W., Howes, R., Huang, P.Y., Li, S.W., Misra, I., Rabbat, M., Sharma, V., Synnaeve, G., Xu, H., Jegou, H., Mairal, J., Labatut, P., Joulin, A., Bojanowski, P.: DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Parkhi, O.M., Vedaldi, A., Zisserman, A., Jawahar, C.: Cats and dogs. In: 2012 IEEE conference on Computer Vision and Pattern Recognition. pp. 3498–3505. IEEE (2012)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International Conference on Machine Learning. pp. 8748–8763. PMLR (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research <span class="ltx_text ltx_font_bold" id="bib.bib23.1.1">21</span>(140), 1–67 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition. pp. 10684–10695 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 22500–22510 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib26.1.1">35</span>, 36479–36494 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Schnell, J., Wang, J., Qi, L., Hu, V.T., Tang, M.: Generative data augmentation improves scribble-supervised semantic segmentation. In: CVPR 2024 Workshop SyntaGen: Harnessing Generative Models for Synthetic Visual Datasets (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Singh, K., Navaratnam, T., Holmer, J., Schaub-Meyer, S., Roth, S.: Is synthetic data all we need? benchmarking the robustness of models trained with synthetic images. In: CVPR 2024 Workshop SyntaGen: Harnessing Generative Models for Synthetic Visual Datasets (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. In: International Conference on Learning Representations (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Trabucco, B., Doherty, K., Gurinas, M., Salakhutdinov, R.: Effective data augmentation with diffusion models. In: The Twelfth International Conference on Learning Representations (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Ye, H., Zhang, J., Liu, S., Han, X., Yang, W.: Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3836–3847 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. pp. 586–595 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Zhao, S., Chen, D., Chen, Y.C., Bao, J., Hao, S., Yuan, L., Wong, K.Y.K.: Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib34.1.1">36</span> (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Zhu, M., Chen, H., Yan, Q., Huang, X., Lin, G., Li, W., Tu, Z., Hu, H., Hu, J., Wang, Y.: Genimage: A million-scale benchmark for detecting ai-generated image. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib35.1.1">36</span> (2024)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="Pt0.Ax1">
<h2 class="ltx_title ltx_title_appendix">Appendix</h2>
</section>
<section class="ltx_appendix" id="Pt0.A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.A </span>Statistical Test</h2>
<div class="ltx_para" id="Pt0.A1.p1">
<p class="ltx_p" id="Pt0.A1.p1.1">To provide a clearer analysis of the main results, we conducted additional statistical tests comparing our method with the baselines in <a class="ltx_ref" href="https://arxiv.org/html/2409.16949v1#Pt0.A1.T3" title="In Appendix 0.A Statistical Test ‣ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with Adaptive Guidance Scaling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a>. In the high CLIPScore (HC) datasets, our method shows an average improvement of +11.42% in Acc-RN50 and +0.72% in Acc-CLIP compared to Real Guidance (RG), and +13.87% and +0.64% compared to DA-Fusion (DF). These improvements are also statistically significant, with p-values of p&lt;0.01 when compared to baselines. In contrast, in the low CLIPScore (LC) datasets, our method still shows an average improvement of +1.70% in Acc-RN50 and +0.14% in Acc-CLIP compared to RG, but a slight decrease of -0.51% in Acc-RN50 and -1.43% in Acc-CLIP when compared to DF. However, the improvement over RG in LC datasets remains statistically significant (p&lt;0.05), and the differences compared to DF are not statistically significant (p&gt;0.1). These results demonstrate that our method provides significant performance improvements over RG across all datasets. Additionally, while DF requires a fine-tuning stage that takes longer than generating all of our synthetic images, our approach still outperforms it in HC and shows only minimal performance differences in LC, further highlighting the practicality of our method.</p>
</div>
<figure class="ltx_table" id="Pt0.A1.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Pt0.A1.T3.14.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="Pt0.A1.T3.15.2" style="font-size:90%;">Comparison of average accuracy with baselines in 1-shot setting. All values in the table represent the average of 3 trials, with the standard deviation included.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Pt0.A1.T3.12" style="width:433.6pt;height:111.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(42.3pt,-10.9pt) scale(1.24266209722132,1.24266209722132) ;">
<table class="ltx_tabular ltx_align_middle" id="Pt0.A1.T3.12.12">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Pt0.A1.T3.12.12.13.1">
<td class="ltx_td ltx_border_tt" id="Pt0.A1.T3.12.12.13.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="Pt0.A1.T3.12.12.13.1.2">HC</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="Pt0.A1.T3.12.12.13.1.3">LC</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.12.12.14.2">
<td class="ltx_td ltx_align_center" id="Pt0.A1.T3.12.12.14.2.1">Methods</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T3.12.12.14.2.2">Acc-RN50</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T3.12.12.14.2.3">Acc-CLIP</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T3.12.12.14.2.4">Acc-RN50</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T3.12.12.14.2.5">Acc-CLIP</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.4.4.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A1.T3.4.4.4.5">RG</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A1.T3.1.1.1.1"><math alttext="0.683\pm 0.035" class="ltx_Math" display="inline" id="Pt0.A1.T3.1.1.1.1.m1.1"><semantics id="Pt0.A1.T3.1.1.1.1.m1.1a"><mrow id="Pt0.A1.T3.1.1.1.1.m1.1.1" xref="Pt0.A1.T3.1.1.1.1.m1.1.1.cmml"><mn id="Pt0.A1.T3.1.1.1.1.m1.1.1.2" xref="Pt0.A1.T3.1.1.1.1.m1.1.1.2.cmml">0.683</mn><mo id="Pt0.A1.T3.1.1.1.1.m1.1.1.1" xref="Pt0.A1.T3.1.1.1.1.m1.1.1.1.cmml">±</mo><mn id="Pt0.A1.T3.1.1.1.1.m1.1.1.3" xref="Pt0.A1.T3.1.1.1.1.m1.1.1.3.cmml">0.035</mn></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A1.T3.1.1.1.1.m1.1b"><apply id="Pt0.A1.T3.1.1.1.1.m1.1.1.cmml" xref="Pt0.A1.T3.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="Pt0.A1.T3.1.1.1.1.m1.1.1.1.cmml" xref="Pt0.A1.T3.1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="Pt0.A1.T3.1.1.1.1.m1.1.1.2.cmml" type="float" xref="Pt0.A1.T3.1.1.1.1.m1.1.1.2">0.683</cn><cn id="Pt0.A1.T3.1.1.1.1.m1.1.1.3.cmml" type="float" xref="Pt0.A1.T3.1.1.1.1.m1.1.1.3">0.035</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.T3.1.1.1.1.m1.1c">0.683\pm 0.035</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.T3.1.1.1.1.m1.1d">0.683 ± 0.035</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A1.T3.2.2.2.2"><math alttext="0.890\pm 0.041" class="ltx_Math" display="inline" id="Pt0.A1.T3.2.2.2.2.m1.1"><semantics id="Pt0.A1.T3.2.2.2.2.m1.1a"><mrow id="Pt0.A1.T3.2.2.2.2.m1.1.1" xref="Pt0.A1.T3.2.2.2.2.m1.1.1.cmml"><mn id="Pt0.A1.T3.2.2.2.2.m1.1.1.2" xref="Pt0.A1.T3.2.2.2.2.m1.1.1.2.cmml">0.890</mn><mo id="Pt0.A1.T3.2.2.2.2.m1.1.1.1" xref="Pt0.A1.T3.2.2.2.2.m1.1.1.1.cmml">±</mo><mn id="Pt0.A1.T3.2.2.2.2.m1.1.1.3" xref="Pt0.A1.T3.2.2.2.2.m1.1.1.3.cmml">0.041</mn></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A1.T3.2.2.2.2.m1.1b"><apply id="Pt0.A1.T3.2.2.2.2.m1.1.1.cmml" xref="Pt0.A1.T3.2.2.2.2.m1.1.1"><csymbol cd="latexml" id="Pt0.A1.T3.2.2.2.2.m1.1.1.1.cmml" xref="Pt0.A1.T3.2.2.2.2.m1.1.1.1">plus-or-minus</csymbol><cn id="Pt0.A1.T3.2.2.2.2.m1.1.1.2.cmml" type="float" xref="Pt0.A1.T3.2.2.2.2.m1.1.1.2">0.890</cn><cn id="Pt0.A1.T3.2.2.2.2.m1.1.1.3.cmml" type="float" xref="Pt0.A1.T3.2.2.2.2.m1.1.1.3">0.041</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.T3.2.2.2.2.m1.1c">0.890\pm 0.041</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.T3.2.2.2.2.m1.1d">0.890 ± 0.041</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A1.T3.3.3.3.3"><math alttext="0.504\pm 0.019" class="ltx_Math" display="inline" id="Pt0.A1.T3.3.3.3.3.m1.1"><semantics id="Pt0.A1.T3.3.3.3.3.m1.1a"><mrow id="Pt0.A1.T3.3.3.3.3.m1.1.1" xref="Pt0.A1.T3.3.3.3.3.m1.1.1.cmml"><mn id="Pt0.A1.T3.3.3.3.3.m1.1.1.2" xref="Pt0.A1.T3.3.3.3.3.m1.1.1.2.cmml">0.504</mn><mo id="Pt0.A1.T3.3.3.3.3.m1.1.1.1" xref="Pt0.A1.T3.3.3.3.3.m1.1.1.1.cmml">±</mo><mn id="Pt0.A1.T3.3.3.3.3.m1.1.1.3" xref="Pt0.A1.T3.3.3.3.3.m1.1.1.3.cmml">0.019</mn></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A1.T3.3.3.3.3.m1.1b"><apply id="Pt0.A1.T3.3.3.3.3.m1.1.1.cmml" xref="Pt0.A1.T3.3.3.3.3.m1.1.1"><csymbol cd="latexml" id="Pt0.A1.T3.3.3.3.3.m1.1.1.1.cmml" xref="Pt0.A1.T3.3.3.3.3.m1.1.1.1">plus-or-minus</csymbol><cn id="Pt0.A1.T3.3.3.3.3.m1.1.1.2.cmml" type="float" xref="Pt0.A1.T3.3.3.3.3.m1.1.1.2">0.504</cn><cn id="Pt0.A1.T3.3.3.3.3.m1.1.1.3.cmml" type="float" xref="Pt0.A1.T3.3.3.3.3.m1.1.1.3">0.019</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.T3.3.3.3.3.m1.1c">0.504\pm 0.019</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.T3.3.3.3.3.m1.1d">0.504 ± 0.019</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A1.T3.4.4.4.4"><math alttext="0.817\pm 0.003" class="ltx_Math" display="inline" id="Pt0.A1.T3.4.4.4.4.m1.1"><semantics id="Pt0.A1.T3.4.4.4.4.m1.1a"><mrow id="Pt0.A1.T3.4.4.4.4.m1.1.1" xref="Pt0.A1.T3.4.4.4.4.m1.1.1.cmml"><mn id="Pt0.A1.T3.4.4.4.4.m1.1.1.2" xref="Pt0.A1.T3.4.4.4.4.m1.1.1.2.cmml">0.817</mn><mo id="Pt0.A1.T3.4.4.4.4.m1.1.1.1" xref="Pt0.A1.T3.4.4.4.4.m1.1.1.1.cmml">±</mo><mn id="Pt0.A1.T3.4.4.4.4.m1.1.1.3" xref="Pt0.A1.T3.4.4.4.4.m1.1.1.3.cmml">0.003</mn></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A1.T3.4.4.4.4.m1.1b"><apply id="Pt0.A1.T3.4.4.4.4.m1.1.1.cmml" xref="Pt0.A1.T3.4.4.4.4.m1.1.1"><csymbol cd="latexml" id="Pt0.A1.T3.4.4.4.4.m1.1.1.1.cmml" xref="Pt0.A1.T3.4.4.4.4.m1.1.1.1">plus-or-minus</csymbol><cn id="Pt0.A1.T3.4.4.4.4.m1.1.1.2.cmml" type="float" xref="Pt0.A1.T3.4.4.4.4.m1.1.1.2">0.817</cn><cn id="Pt0.A1.T3.4.4.4.4.m1.1.1.3.cmml" type="float" xref="Pt0.A1.T3.4.4.4.4.m1.1.1.3">0.003</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.T3.4.4.4.4.m1.1c">0.817\pm 0.003</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.T3.4.4.4.4.m1.1d">0.817 ± 0.003</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.8.8.8">
<td class="ltx_td ltx_align_center" id="Pt0.A1.T3.8.8.8.5">DF</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T3.5.5.5.1"><math alttext="0.658\pm 0.050" class="ltx_Math" display="inline" id="Pt0.A1.T3.5.5.5.1.m1.1"><semantics id="Pt0.A1.T3.5.5.5.1.m1.1a"><mrow id="Pt0.A1.T3.5.5.5.1.m1.1.1" xref="Pt0.A1.T3.5.5.5.1.m1.1.1.cmml"><mn id="Pt0.A1.T3.5.5.5.1.m1.1.1.2" xref="Pt0.A1.T3.5.5.5.1.m1.1.1.2.cmml">0.658</mn><mo id="Pt0.A1.T3.5.5.5.1.m1.1.1.1" xref="Pt0.A1.T3.5.5.5.1.m1.1.1.1.cmml">±</mo><mn id="Pt0.A1.T3.5.5.5.1.m1.1.1.3" xref="Pt0.A1.T3.5.5.5.1.m1.1.1.3.cmml">0.050</mn></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A1.T3.5.5.5.1.m1.1b"><apply id="Pt0.A1.T3.5.5.5.1.m1.1.1.cmml" xref="Pt0.A1.T3.5.5.5.1.m1.1.1"><csymbol cd="latexml" id="Pt0.A1.T3.5.5.5.1.m1.1.1.1.cmml" xref="Pt0.A1.T3.5.5.5.1.m1.1.1.1">plus-or-minus</csymbol><cn id="Pt0.A1.T3.5.5.5.1.m1.1.1.2.cmml" type="float" xref="Pt0.A1.T3.5.5.5.1.m1.1.1.2">0.658</cn><cn id="Pt0.A1.T3.5.5.5.1.m1.1.1.3.cmml" type="float" xref="Pt0.A1.T3.5.5.5.1.m1.1.1.3">0.050</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.T3.5.5.5.1.m1.1c">0.658\pm 0.050</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.T3.5.5.5.1.m1.1d">0.658 ± 0.050</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T3.6.6.6.2"><math alttext="0.891\pm 0.036" class="ltx_Math" display="inline" id="Pt0.A1.T3.6.6.6.2.m1.1"><semantics id="Pt0.A1.T3.6.6.6.2.m1.1a"><mrow id="Pt0.A1.T3.6.6.6.2.m1.1.1" xref="Pt0.A1.T3.6.6.6.2.m1.1.1.cmml"><mn id="Pt0.A1.T3.6.6.6.2.m1.1.1.2" xref="Pt0.A1.T3.6.6.6.2.m1.1.1.2.cmml">0.891</mn><mo id="Pt0.A1.T3.6.6.6.2.m1.1.1.1" xref="Pt0.A1.T3.6.6.6.2.m1.1.1.1.cmml">±</mo><mn id="Pt0.A1.T3.6.6.6.2.m1.1.1.3" xref="Pt0.A1.T3.6.6.6.2.m1.1.1.3.cmml">0.036</mn></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A1.T3.6.6.6.2.m1.1b"><apply id="Pt0.A1.T3.6.6.6.2.m1.1.1.cmml" xref="Pt0.A1.T3.6.6.6.2.m1.1.1"><csymbol cd="latexml" id="Pt0.A1.T3.6.6.6.2.m1.1.1.1.cmml" xref="Pt0.A1.T3.6.6.6.2.m1.1.1.1">plus-or-minus</csymbol><cn id="Pt0.A1.T3.6.6.6.2.m1.1.1.2.cmml" type="float" xref="Pt0.A1.T3.6.6.6.2.m1.1.1.2">0.891</cn><cn id="Pt0.A1.T3.6.6.6.2.m1.1.1.3.cmml" type="float" xref="Pt0.A1.T3.6.6.6.2.m1.1.1.3">0.036</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.T3.6.6.6.2.m1.1c">0.891\pm 0.036</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.T3.6.6.6.2.m1.1d">0.891 ± 0.036</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T3.7.7.7.3"><span class="ltx_text ltx_font_bold" id="Pt0.A1.T3.7.7.7.3.1">0.526 <math alttext="\pm" class="ltx_Math" display="inline" id="Pt0.A1.T3.7.7.7.3.1.m1.1"><semantics id="Pt0.A1.T3.7.7.7.3.1.m1.1a"><mo id="Pt0.A1.T3.7.7.7.3.1.m1.1.1" xref="Pt0.A1.T3.7.7.7.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="Pt0.A1.T3.7.7.7.3.1.m1.1b"><csymbol cd="latexml" id="Pt0.A1.T3.7.7.7.3.1.m1.1.1.cmml" xref="Pt0.A1.T3.7.7.7.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.T3.7.7.7.3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.T3.7.7.7.3.1.m1.1d">±</annotation></semantics></math> 0.023</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T3.8.8.8.4"><span class="ltx_text ltx_font_bold" id="Pt0.A1.T3.8.8.8.4.1">0.833 <math alttext="\pm" class="ltx_Math" display="inline" id="Pt0.A1.T3.8.8.8.4.1.m1.1"><semantics id="Pt0.A1.T3.8.8.8.4.1.m1.1a"><mo id="Pt0.A1.T3.8.8.8.4.1.m1.1.1" xref="Pt0.A1.T3.8.8.8.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="Pt0.A1.T3.8.8.8.4.1.m1.1b"><csymbol cd="latexml" id="Pt0.A1.T3.8.8.8.4.1.m1.1.1.cmml" xref="Pt0.A1.T3.8.8.8.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.T3.8.8.8.4.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.T3.8.8.8.4.1.m1.1d">±</annotation></semantics></math> 0.021</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.12.12.12">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Pt0.A1.T3.12.12.12.5"><span class="ltx_text ltx_font_bold" id="Pt0.A1.T3.12.12.12.5.1">Ours</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Pt0.A1.T3.9.9.9.1"><span class="ltx_text ltx_font_bold" id="Pt0.A1.T3.9.9.9.1.1">0.797 <math alttext="\pm" class="ltx_Math" display="inline" id="Pt0.A1.T3.9.9.9.1.1.m1.1"><semantics id="Pt0.A1.T3.9.9.9.1.1.m1.1a"><mo id="Pt0.A1.T3.9.9.9.1.1.m1.1.1" xref="Pt0.A1.T3.9.9.9.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="Pt0.A1.T3.9.9.9.1.1.m1.1b"><csymbol cd="latexml" id="Pt0.A1.T3.9.9.9.1.1.m1.1.1.cmml" xref="Pt0.A1.T3.9.9.9.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.T3.9.9.9.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.T3.9.9.9.1.1.m1.1d">±</annotation></semantics></math> 0.023</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Pt0.A1.T3.10.10.10.2"><span class="ltx_text ltx_font_bold" id="Pt0.A1.T3.10.10.10.2.1">0.897 <math alttext="\pm" class="ltx_Math" display="inline" id="Pt0.A1.T3.10.10.10.2.1.m1.1"><semantics id="Pt0.A1.T3.10.10.10.2.1.m1.1a"><mo id="Pt0.A1.T3.10.10.10.2.1.m1.1.1" xref="Pt0.A1.T3.10.10.10.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="Pt0.A1.T3.10.10.10.2.1.m1.1b"><csymbol cd="latexml" id="Pt0.A1.T3.10.10.10.2.1.m1.1.1.cmml" xref="Pt0.A1.T3.10.10.10.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.T3.10.10.10.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.T3.10.10.10.2.1.m1.1d">±</annotation></semantics></math> 0.034</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Pt0.A1.T3.11.11.11.3"><math alttext="0.521\pm 0.014" class="ltx_Math" display="inline" id="Pt0.A1.T3.11.11.11.3.m1.1"><semantics id="Pt0.A1.T3.11.11.11.3.m1.1a"><mrow id="Pt0.A1.T3.11.11.11.3.m1.1.1" xref="Pt0.A1.T3.11.11.11.3.m1.1.1.cmml"><mn id="Pt0.A1.T3.11.11.11.3.m1.1.1.2" xref="Pt0.A1.T3.11.11.11.3.m1.1.1.2.cmml">0.521</mn><mo id="Pt0.A1.T3.11.11.11.3.m1.1.1.1" xref="Pt0.A1.T3.11.11.11.3.m1.1.1.1.cmml">±</mo><mn id="Pt0.A1.T3.11.11.11.3.m1.1.1.3" xref="Pt0.A1.T3.11.11.11.3.m1.1.1.3.cmml">0.014</mn></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A1.T3.11.11.11.3.m1.1b"><apply id="Pt0.A1.T3.11.11.11.3.m1.1.1.cmml" xref="Pt0.A1.T3.11.11.11.3.m1.1.1"><csymbol cd="latexml" id="Pt0.A1.T3.11.11.11.3.m1.1.1.1.cmml" xref="Pt0.A1.T3.11.11.11.3.m1.1.1.1">plus-or-minus</csymbol><cn id="Pt0.A1.T3.11.11.11.3.m1.1.1.2.cmml" type="float" xref="Pt0.A1.T3.11.11.11.3.m1.1.1.2">0.521</cn><cn id="Pt0.A1.T3.11.11.11.3.m1.1.1.3.cmml" type="float" xref="Pt0.A1.T3.11.11.11.3.m1.1.1.3">0.014</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.T3.11.11.11.3.m1.1c">0.521\pm 0.014</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.T3.11.11.11.3.m1.1d">0.521 ± 0.014</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Pt0.A1.T3.12.12.12.4"><math alttext="0.819\pm 0.011" class="ltx_Math" display="inline" id="Pt0.A1.T3.12.12.12.4.m1.1"><semantics id="Pt0.A1.T3.12.12.12.4.m1.1a"><mrow id="Pt0.A1.T3.12.12.12.4.m1.1.1" xref="Pt0.A1.T3.12.12.12.4.m1.1.1.cmml"><mn id="Pt0.A1.T3.12.12.12.4.m1.1.1.2" xref="Pt0.A1.T3.12.12.12.4.m1.1.1.2.cmml">0.819</mn><mo id="Pt0.A1.T3.12.12.12.4.m1.1.1.1" xref="Pt0.A1.T3.12.12.12.4.m1.1.1.1.cmml">±</mo><mn id="Pt0.A1.T3.12.12.12.4.m1.1.1.3" xref="Pt0.A1.T3.12.12.12.4.m1.1.1.3.cmml">0.011</mn></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A1.T3.12.12.12.4.m1.1b"><apply id="Pt0.A1.T3.12.12.12.4.m1.1.1.cmml" xref="Pt0.A1.T3.12.12.12.4.m1.1.1"><csymbol cd="latexml" id="Pt0.A1.T3.12.12.12.4.m1.1.1.1.cmml" xref="Pt0.A1.T3.12.12.12.4.m1.1.1.1">plus-or-minus</csymbol><cn id="Pt0.A1.T3.12.12.12.4.m1.1.1.2.cmml" type="float" xref="Pt0.A1.T3.12.12.12.4.m1.1.1.2">0.819</cn><cn id="Pt0.A1.T3.12.12.12.4.m1.1.1.3.cmml" type="float" xref="Pt0.A1.T3.12.12.12.4.m1.1.1.3">0.011</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.T3.12.12.12.4.m1.1c">0.819\pm 0.011</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.T3.12.12.12.4.m1.1d">0.819 ± 0.011</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="Pt0.A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.B </span>Hyperparameters</h2>
<figure class="ltx_table" id="Pt0.A2.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Pt0.A2.T4.9.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="Pt0.A2.T4.10.2" style="font-size:90%;">Hyperparameters Settings.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Pt0.A2.T4.7">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Pt0.A2.T4.7.8.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="Pt0.A2.T4.7.8.1.1">Hyperparameters</th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="Pt0.A2.T4.7.8.1.2">Values</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Pt0.A2.T4.1.1.1">Synthetic Probability <math alttext="p" class="ltx_Math" display="inline" id="Pt0.A2.T4.1.1.1.m1.1"><semantics id="Pt0.A2.T4.1.1.1.m1.1a"><mi id="Pt0.A2.T4.1.1.1.m1.1.1" xref="Pt0.A2.T4.1.1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="Pt0.A2.T4.1.1.1.m1.1b"><ci id="Pt0.A2.T4.1.1.1.m1.1.1.cmml" xref="Pt0.A2.T4.1.1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T4.1.1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T4.1.1.1.m1.1d">italic_p</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Pt0.A2.T4.1.1.2">0.5</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T4.2.2.1">Number of Prompts <math alttext="M" class="ltx_Math" display="inline" id="Pt0.A2.T4.2.2.1.m1.1"><semantics id="Pt0.A2.T4.2.2.1.m1.1a"><mi id="Pt0.A2.T4.2.2.1.m1.1.1" xref="Pt0.A2.T4.2.2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="Pt0.A2.T4.2.2.1.m1.1b"><ci id="Pt0.A2.T4.2.2.1.m1.1.1.cmml" xref="Pt0.A2.T4.2.2.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T4.2.2.1.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T4.2.2.1.m1.1d">italic_M</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="Pt0.A2.T4.2.2.2">10</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.7.9.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T4.7.9.2.1">GPT4 Prompt Temperature</th>
<td class="ltx_td ltx_align_left" id="Pt0.A2.T4.7.9.2.2">1.0</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.7.10.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T4.7.10.3.1">GPT4 Prompt Top Probability</th>
<td class="ltx_td ltx_align_left" id="Pt0.A2.T4.7.10.3.2">1.0</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.7.11.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T4.7.11.4.1">GPT4 Prompt Frequency Penalty</th>
<td class="ltx_td ltx_align_left" id="Pt0.A2.T4.7.11.4.2">0.0</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.7.12.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T4.7.12.5.1">GPT4 Prompt Presence Penalty</th>
<td class="ltx_td ltx_align_left" id="Pt0.A2.T4.7.12.5.2">0.0</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.7.13.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T4.7.13.6.1">Stable Diffusion Checkpoint</th>
<td class="ltx_td ltx_align_left" id="Pt0.A2.T4.7.13.6.2">CompVis/stable-diffusion-v1-4</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.7.14.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T4.7.14.7.1">Stable Diffusion Guidance Scale</th>
<td class="ltx_td ltx_align_left" id="Pt0.A2.T4.7.14.7.2">7.5</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.7.15.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T4.7.15.8.1">Stable Diffusion Image Size</th>
<td class="ltx_td ltx_align_left" id="Pt0.A2.T4.7.15.8.2">512</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.7.16.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T4.7.16.9.1">IP-Adapter Checkpoint</th>
<td class="ltx_td ltx_align_left" id="Pt0.A2.T4.7.16.9.2">ip-adapter_sd15</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T4.3.3.1">Adaptive Guidance Scaling <math alttext="\alpha" class="ltx_Math" display="inline" id="Pt0.A2.T4.3.3.1.m1.1"><semantics id="Pt0.A2.T4.3.3.1.m1.1a"><mi id="Pt0.A2.T4.3.3.1.m1.1.1" xref="Pt0.A2.T4.3.3.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="Pt0.A2.T4.3.3.1.m1.1b"><ci id="Pt0.A2.T4.3.3.1.m1.1.1.cmml" xref="Pt0.A2.T4.3.3.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T4.3.3.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T4.3.3.1.m1.1d">italic_α</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="Pt0.A2.T4.3.3.2">0.3</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T4.4.4.1">Adaptive Guidance Scaling <math alttext="\text{min}_{\text{low}}" class="ltx_Math" display="inline" id="Pt0.A2.T4.4.4.1.m1.1"><semantics id="Pt0.A2.T4.4.4.1.m1.1a"><msub id="Pt0.A2.T4.4.4.1.m1.1.1" xref="Pt0.A2.T4.4.4.1.m1.1.1.cmml"><mtext id="Pt0.A2.T4.4.4.1.m1.1.1.2" xref="Pt0.A2.T4.4.4.1.m1.1.1.2a.cmml">min</mtext><mtext id="Pt0.A2.T4.4.4.1.m1.1.1.3" xref="Pt0.A2.T4.4.4.1.m1.1.1.3a.cmml">low</mtext></msub><annotation-xml encoding="MathML-Content" id="Pt0.A2.T4.4.4.1.m1.1b"><apply id="Pt0.A2.T4.4.4.1.m1.1.1.cmml" xref="Pt0.A2.T4.4.4.1.m1.1.1"><csymbol cd="ambiguous" id="Pt0.A2.T4.4.4.1.m1.1.1.1.cmml" xref="Pt0.A2.T4.4.4.1.m1.1.1">subscript</csymbol><ci id="Pt0.A2.T4.4.4.1.m1.1.1.2a.cmml" xref="Pt0.A2.T4.4.4.1.m1.1.1.2"><mtext id="Pt0.A2.T4.4.4.1.m1.1.1.2.cmml" xref="Pt0.A2.T4.4.4.1.m1.1.1.2">min</mtext></ci><ci id="Pt0.A2.T4.4.4.1.m1.1.1.3a.cmml" xref="Pt0.A2.T4.4.4.1.m1.1.1.3"><mtext id="Pt0.A2.T4.4.4.1.m1.1.1.3.cmml" mathsize="70%" xref="Pt0.A2.T4.4.4.1.m1.1.1.3">low</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T4.4.4.1.m1.1c">\text{min}_{\text{low}}</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T4.4.4.1.m1.1d">min start_POSTSUBSCRIPT low end_POSTSUBSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="Pt0.A2.T4.4.4.2">0.7</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T4.5.5.1">Adaptive Guidance Scaling <math alttext="\text{max}_{\text{low}}" class="ltx_Math" display="inline" id="Pt0.A2.T4.5.5.1.m1.1"><semantics id="Pt0.A2.T4.5.5.1.m1.1a"><msub id="Pt0.A2.T4.5.5.1.m1.1.1" xref="Pt0.A2.T4.5.5.1.m1.1.1.cmml"><mtext id="Pt0.A2.T4.5.5.1.m1.1.1.2" xref="Pt0.A2.T4.5.5.1.m1.1.1.2a.cmml">max</mtext><mtext id="Pt0.A2.T4.5.5.1.m1.1.1.3" xref="Pt0.A2.T4.5.5.1.m1.1.1.3a.cmml">low</mtext></msub><annotation-xml encoding="MathML-Content" id="Pt0.A2.T4.5.5.1.m1.1b"><apply id="Pt0.A2.T4.5.5.1.m1.1.1.cmml" xref="Pt0.A2.T4.5.5.1.m1.1.1"><csymbol cd="ambiguous" id="Pt0.A2.T4.5.5.1.m1.1.1.1.cmml" xref="Pt0.A2.T4.5.5.1.m1.1.1">subscript</csymbol><ci id="Pt0.A2.T4.5.5.1.m1.1.1.2a.cmml" xref="Pt0.A2.T4.5.5.1.m1.1.1.2"><mtext id="Pt0.A2.T4.5.5.1.m1.1.1.2.cmml" xref="Pt0.A2.T4.5.5.1.m1.1.1.2">max</mtext></ci><ci id="Pt0.A2.T4.5.5.1.m1.1.1.3a.cmml" xref="Pt0.A2.T4.5.5.1.m1.1.1.3"><mtext id="Pt0.A2.T4.5.5.1.m1.1.1.3.cmml" mathsize="70%" xref="Pt0.A2.T4.5.5.1.m1.1.1.3">low</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T4.5.5.1.m1.1c">\text{max}_{\text{low}}</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T4.5.5.1.m1.1d">max start_POSTSUBSCRIPT low end_POSTSUBSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="Pt0.A2.T4.5.5.2">0.9</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T4.6.6.1">Adaptive Guidance Scaling <math alttext="\text{min}_{\text{high}}" class="ltx_Math" display="inline" id="Pt0.A2.T4.6.6.1.m1.1"><semantics id="Pt0.A2.T4.6.6.1.m1.1a"><msub id="Pt0.A2.T4.6.6.1.m1.1.1" xref="Pt0.A2.T4.6.6.1.m1.1.1.cmml"><mtext id="Pt0.A2.T4.6.6.1.m1.1.1.2" xref="Pt0.A2.T4.6.6.1.m1.1.1.2a.cmml">min</mtext><mtext id="Pt0.A2.T4.6.6.1.m1.1.1.3" xref="Pt0.A2.T4.6.6.1.m1.1.1.3a.cmml">high</mtext></msub><annotation-xml encoding="MathML-Content" id="Pt0.A2.T4.6.6.1.m1.1b"><apply id="Pt0.A2.T4.6.6.1.m1.1.1.cmml" xref="Pt0.A2.T4.6.6.1.m1.1.1"><csymbol cd="ambiguous" id="Pt0.A2.T4.6.6.1.m1.1.1.1.cmml" xref="Pt0.A2.T4.6.6.1.m1.1.1">subscript</csymbol><ci id="Pt0.A2.T4.6.6.1.m1.1.1.2a.cmml" xref="Pt0.A2.T4.6.6.1.m1.1.1.2"><mtext id="Pt0.A2.T4.6.6.1.m1.1.1.2.cmml" xref="Pt0.A2.T4.6.6.1.m1.1.1.2">min</mtext></ci><ci id="Pt0.A2.T4.6.6.1.m1.1.1.3a.cmml" xref="Pt0.A2.T4.6.6.1.m1.1.1.3"><mtext id="Pt0.A2.T4.6.6.1.m1.1.1.3.cmml" mathsize="70%" xref="Pt0.A2.T4.6.6.1.m1.1.1.3">high</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T4.6.6.1.m1.1c">\text{min}_{\text{high}}</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T4.6.6.1.m1.1d">min start_POSTSUBSCRIPT high end_POSTSUBSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="Pt0.A2.T4.6.6.2">0.1</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T4.7.7.1">Adaptive Guidance Scaling <math alttext="\text{max}_{\text{high}}" class="ltx_Math" display="inline" id="Pt0.A2.T4.7.7.1.m1.1"><semantics id="Pt0.A2.T4.7.7.1.m1.1a"><msub id="Pt0.A2.T4.7.7.1.m1.1.1" xref="Pt0.A2.T4.7.7.1.m1.1.1.cmml"><mtext id="Pt0.A2.T4.7.7.1.m1.1.1.2" xref="Pt0.A2.T4.7.7.1.m1.1.1.2a.cmml">max</mtext><mtext id="Pt0.A2.T4.7.7.1.m1.1.1.3" xref="Pt0.A2.T4.7.7.1.m1.1.1.3a.cmml">high</mtext></msub><annotation-xml encoding="MathML-Content" id="Pt0.A2.T4.7.7.1.m1.1b"><apply id="Pt0.A2.T4.7.7.1.m1.1.1.cmml" xref="Pt0.A2.T4.7.7.1.m1.1.1"><csymbol cd="ambiguous" id="Pt0.A2.T4.7.7.1.m1.1.1.1.cmml" xref="Pt0.A2.T4.7.7.1.m1.1.1">subscript</csymbol><ci id="Pt0.A2.T4.7.7.1.m1.1.1.2a.cmml" xref="Pt0.A2.T4.7.7.1.m1.1.1.2"><mtext id="Pt0.A2.T4.7.7.1.m1.1.1.2.cmml" xref="Pt0.A2.T4.7.7.1.m1.1.1.2">max</mtext></ci><ci id="Pt0.A2.T4.7.7.1.m1.1.1.3a.cmml" xref="Pt0.A2.T4.7.7.1.m1.1.1.3"><mtext id="Pt0.A2.T4.7.7.1.m1.1.1.3.cmml" mathsize="70%" xref="Pt0.A2.T4.7.7.1.m1.1.1.3">high</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T4.7.7.1.m1.1c">\text{max}_{\text{high}}</annotation><annotation encoding="application/x-llamapun" id="Pt0.A2.T4.7.7.1.m1.1d">max start_POSTSUBSCRIPT high end_POSTSUBSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="Pt0.A2.T4.7.7.2">0.4</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.7.17.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T4.7.17.10.1">Classifier Model</th>
<td class="ltx_td ltx_align_left" id="Pt0.A2.T4.7.17.10.2">{CLIP-ViT-B/16, ResNet50}</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.7.18.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T4.7.18.11.1">Classifier Optimizer</th>
<td class="ltx_td ltx_align_left" id="Pt0.A2.T4.7.18.11.2">{AdamW, Adam}</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.7.19.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T4.7.19.12.1">Classifier Training Epochs</th>
<td class="ltx_td ltx_align_left" id="Pt0.A2.T4.7.19.12.2">50</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.7.20.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T4.7.20.13.1">Classifier Learning Late</th>
<td class="ltx_td ltx_align_left" id="Pt0.A2.T4.7.20.13.2">{0.0002, 0.0001}</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.7.21.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.T4.7.21.14.1">Classifier Image Size</th>
<td class="ltx_td ltx_align_left" id="Pt0.A2.T4.7.21.14.2">224</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.T4.7.22.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="Pt0.A2.T4.7.22.15.1">Classifier Batch Size</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Pt0.A2.T4.7.22.15.2">32</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="Pt0.A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.C </span>LLM Prompt details</h2>
<figure class="ltx_figure" id="Pt0.A3.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="394" id="Pt0.A3.F10.g1" src="x6.png" width="354"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A3.F10.4.2.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="Pt0.A3.F10.2.1" style="font-size:90%;">We use the LLM to generate one sentence for each class, iterating this process <math alttext="M" class="ltx_Math" display="inline" id="Pt0.A3.F10.2.1.m1.1"><semantics id="Pt0.A3.F10.2.1.m1.1b"><mi id="Pt0.A3.F10.2.1.m1.1.1" xref="Pt0.A3.F10.2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="Pt0.A3.F10.2.1.m1.1c"><ci id="Pt0.A3.F10.2.1.m1.1.1.cmml" xref="Pt0.A3.F10.2.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A3.F10.2.1.m1.1d">M</annotation><annotation encoding="application/x-llamapun" id="Pt0.A3.F10.2.1.m1.1e">italic_M</annotation></semantics></math> times.</span></figcaption>
</figure>
<figure class="ltx_table" id="Pt0.A3.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Pt0.A3.T5.2.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="Pt0.A3.T5.3.2" style="font-size:90%;">Examples of dataset description. Each description was partially taken from the official websites of the respective datasets.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Pt0.A3.T5.4" style="width:433.6pt;height:108.8pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.6pt,6.9pt) scale(0.887026019478182,0.887026019478182) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Pt0.A3.T5.4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Pt0.A3.T5.4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="Pt0.A3.T5.4.1.1.1.1">Dataset</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="Pt0.A3.T5.4.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A3.T5.4.1.1.1.2.1">
<span class="ltx_p" id="Pt0.A3.T5.4.1.1.1.2.1.1" style="width:398.3pt;">Dataset Description</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Pt0.A3.T5.4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Pt0.A3.T5.4.1.2.1.1">Oxford Pets</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A3.T5.4.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A3.T5.4.1.2.1.2.1">
<span class="ltx_p" id="Pt0.A3.T5.4.1.2.1.2.1.1" style="width:398.3pt;"><span class="ltx_text ltx_font_typewriter" id="Pt0.A3.T5.4.1.2.1.2.1.1.1">‘‘We have created a 37 category pet dataset with roughly 200 images for each class. The images have a large variations in scale, pose and lighting.’’</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A3.T5.4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Pt0.A3.T5.4.1.3.2.1">Caltech-101</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A3.T5.4.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A3.T5.4.1.3.2.2.1">
<span class="ltx_p" id="Pt0.A3.T5.4.1.3.2.2.1.1" style="width:398.3pt;"><span class="ltx_text ltx_font_typewriter" id="Pt0.A3.T5.4.1.3.2.2.1.1.1">‘‘Caltech-101 dataset is pictures of objects belonging to 101 categories. About 40 to 800 images per category. Most categories have about 50 images. The size of each image is roughly 300 x 200 pixels. We have carefully clicked outlines of each object in these pictures.’’</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A3.T5.4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="Pt0.A3.T5.4.1.4.3.1">Flowers102</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="Pt0.A3.T5.4.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A3.T5.4.1.4.3.2.1">
<span class="ltx_p" id="Pt0.A3.T5.4.1.4.3.2.1.1" style="width:398.3pt;"><span class="ltx_text ltx_font_typewriter" id="Pt0.A3.T5.4.1.4.3.2.1.1.1">‘‘We have created a 102 category dataset, consisting of 102 flower categories. The flowers chosen to be flower commonly occuring in the United Kingdom. Each class consists of between 40 and 258 images.’’</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Sep 25 13:28:32 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
