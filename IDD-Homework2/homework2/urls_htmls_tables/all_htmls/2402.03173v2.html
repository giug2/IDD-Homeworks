<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.03173] Multi: Multimodal Understanding Leaderboard with Text and Images</title><meta property="og:description" content="Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community, while existing benchmarks primarily focus on understanding sim…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multi: Multimodal Understanding Leaderboard with Text and Images">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multi: Multimodal Understanding Leaderboard with Text and Images">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.03173">

<!--Generated on Tue Mar  5 15:10:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">
<span id="id1.id1" class="ltx_text ltx_font_smallcaps">Multi</span>: Multimodal Understanding Leaderboard with Text and Images</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zichen Zhu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yang Xu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lu Chen<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">2</span></span></span></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jingkai Yang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yichuan Ma 
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_bold">Yiming Sun  and Hailin Wen  and Jiaqi Liu  and Jinyu Cai</span> 
<br class="ltx_break"><span id="id3.2.id2" class="ltx_text ltx_font_bold">Yingzi Ma  and Situo Zhang  and Zihan Zhao  and Liangtai Sun  and Kai Yu<span id="footnotex2" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span id="footnotex2.1.1.1" class="ltx_text ltx_font_medium">2</span></span></span></span></span></span> 
<br class="ltx_break">X-LANCE Lab, Department of Computer Science and Engineering 
<br class="ltx_break">MoE Key Lab of Artificial Intelligence, SJTU AI Institute 
<br class="ltx_break">Shanghai Jiao Tong University, Shanghai, China 
<br class="ltx_break"><span id="id4.3.id3" class="ltx_text ltx_font_typewriter">{JamesZhutheThird, xuyang0112, chenlusz, kai.yu}@sjtu.edu.cn</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community, while existing benchmarks primarily focus on understanding simple natural images and short context. In this paper, we present <span id="id5.id1.1" class="ltx_text ltx_font_smallcaps">Multi</span> as a cutting-edge benchmark for evaluating MLLMs on understanding complex tables and images and reasoning with long context. <span id="id5.id1.2" class="ltx_text ltx_font_smallcaps">Multi</span> provides multimodal inputs and requires responses that are either precise or open-ended, reflecting real-life examination styles. <span id="id5.id1.3" class="ltx_text ltx_font_smallcaps">Multi</span> includes over 18,000 questions, and challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis and cross-modality reasoning.
We also introduce <span id="id5.id1.4" class="ltx_text ltx_font_smallcaps">Multi-Elite</span>, a 500-question selected hard subset, and <span id="id5.id1.5" class="ltx_text ltx_font_smallcaps">Multi-Extend</span> with more than 4,500 external knowledge context pieces. Our evaluation indicates significant potential for MLLM advancement, with GPT-4V achieving a 63.7% accuracy rate on <span id="id5.id1.6" class="ltx_text ltx_font_smallcaps">Multi</span>, in contrast to other MLLMs scoring between 28.5% and 55.3%. <span id="id5.id1.7" class="ltx_text ltx_font_smallcaps">Multi</span> serves not only as a robust evaluation platform but also paves the way for the development of expert-level AI.
Details and access are available at: <a target="_blank" href="https://OpenDFM.github.io/MULTI-Benchmark/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://OpenDFM.github.io/MULTI-Benchmark/</a>.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.1" class="ltx_block ltx_align_bottom">
<p id="p1.1.1" class="ltx_p"><span id="p1.1.1.1" class="ltx_text ltx_font_smallcaps">Multi</span><span id="p1.1.1.2" class="ltx_text ltx_font_bold">: Multimodal Understanding Leaderboard with Text and Images</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.2" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.2.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">

<span id="p1.1.2.1.1" class="ltx_tabular ltx_align_top">
<span id="p1.1.2.1.1.1" class="ltx_tr">
<span id="p1.1.2.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Zichen Zhu  </span>and<span id="p1.1.2.1.1.1.1.2" class="ltx_text ltx_font_bold"> Yang Xu  </span>and<span id="p1.1.2.1.1.1.1.3" class="ltx_text ltx_font_bold"> Lu Chen<span id="footnotex3" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span id="footnotex3.1.1.1" class="ltx_text ltx_font_medium">2</span></span></span></span></span>  </span>and<span id="p1.1.2.1.1.1.1.4" class="ltx_text ltx_font_bold"> Jingkai Yang  </span>and<span id="p1.1.2.1.1.1.1.5" class="ltx_text ltx_font_bold"> Yichuan Ma</span></span></span>
<span id="p1.1.2.1.1.2" class="ltx_tr">
<span id="p1.1.2.1.1.2.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.2.1.1" class="ltx_text ltx_font_bold">Yiming Sun  </span>and<span id="p1.1.2.1.1.2.1.2" class="ltx_text ltx_font_bold"> Hailin Wen  </span>and<span id="p1.1.2.1.1.2.1.3" class="ltx_text ltx_font_bold"> Jiaqi Liu  </span>and<span id="p1.1.2.1.1.2.1.4" class="ltx_text ltx_font_bold"> Jinyu Cai</span></span></span>
<span id="p1.1.2.1.1.3" class="ltx_tr">
<span id="p1.1.2.1.1.3.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.3.1.1" class="ltx_text ltx_font_bold">Yingzi Ma  </span>and<span id="p1.1.2.1.1.3.1.2" class="ltx_text ltx_font_bold"> Situo Zhang  </span>and<span id="p1.1.2.1.1.3.1.3" class="ltx_text ltx_font_bold"> Zihan Zhao  </span>and<span id="p1.1.2.1.1.3.1.4" class="ltx_text ltx_font_bold"> Liangtai Sun  </span>and<span id="p1.1.2.1.1.3.1.5" class="ltx_text ltx_font_bold"> Kai Yu<span id="footnotex4" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span id="footnotex4.1.1.1" class="ltx_text ltx_font_medium">2</span></span></span></span></span></span></span></span>
<span id="p1.1.2.1.1.4" class="ltx_tr">
<span id="p1.1.2.1.1.4.1" class="ltx_td ltx_align_center">X-LANCE Lab, Department of Computer Science and Engineering</span></span>
<span id="p1.1.2.1.1.5" class="ltx_tr">
<span id="p1.1.2.1.1.5.1" class="ltx_td ltx_align_center">MoE Key Lab of Artificial Intelligence, SJTU AI Institute</span></span>
<span id="p1.1.2.1.1.6" class="ltx_tr">
<span id="p1.1.2.1.1.6.1" class="ltx_td ltx_align_center">Shanghai Jiao Tong University, Shanghai, China</span></span>
<span id="p1.1.2.1.1.7" class="ltx_tr">
<span id="p1.1.2.1.1.7.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.7.1.1" class="ltx_text ltx_font_typewriter">{JamesZhutheThird, xuyang0112, chenlusz, kai.yu}@sjtu.edu.cn</span></span></span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2402.03173/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="166" height="269" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An example of <span id="S0.F1.2.1" class="ltx_text ltx_font_smallcaps">Multi</span>. English translations of Chinese text are shown for better readability. The markdown format remains as it is.</figcaption>
</figure>
<figure id="S0.F2" class="ltx_figure"><img src="/html/2402.03173/assets/x2.png" id="S0.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="92" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The overview of <span id="S0.F2.2.1" class="ltx_text ltx_font_smallcaps">Multi</span>.</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The rapid advancement in large-scale language models (LLMs) has led to significant achievements in natural language processing and related disciplines. Yet, human communication and understanding extend beyond language, encompassing images, tables, mathematical and chemical formulas, graphs, diagrams, cartoons, posters, and other visual mediums. They play a crucial role in conveying information, particularly in scientific areas. Therefore, there’s a growing interest in developing Multimodal LLMs (MLLMs) capable of processing and generating across various modalities, including visual ones, and performing tasks that require cross-modal reasoning.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Evaluating MLLMs presents unique challenges. Current benchmarks <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022</a>); Li et al. (<a href="#bib.bib15" title="" class="ltx_ref">2023b</a>); Yue et al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite> either focus narrowly on natural scene images or are simplistic, failing to thoroughly assess the models’ abilities. Many scientific benchmarks <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib32" title="" class="ltx_ref">2023a</a>); Huang et al. (<a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite> rely on multiple-choice questions with a single answer, which may not accurately gauge a model’s comprehension and can lead to superficial learning, i.e., the model will not look into other choices if the correct choice is straightforward. A more robust, detailed, and multi-scale dataset is necessary to effectively evaluate MLLMs under diverse conditions and scenarios. Current benchmarks mentioned above are evaluated with English context, while the rapid progression of Chinese MLLMs highlights the need for a Chinese multimodal benchmark with Chinese contents both in text and image and brings new challenges to the community.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we introduce <span id="S1.p3.1.1" class="ltx_text ltx_font_smallcaps">Multi</span>, a novel benchmark named <span id="S1.p3.1.2" class="ltx_text ltx_font_bold">M</span>ultimodal <span id="S1.p3.1.3" class="ltx_text ltx_font_bold">U</span>nderstanding <span id="S1.p3.1.4" class="ltx_text ltx_font_bold">L</span>eaderboard with <span id="S1.p3.1.5" class="ltx_text ltx_font_bold">T</span>ext and <span id="S1.p3.1.6" class="ltx_text ltx_font_bold">I</span>mages, specifically designed to evaluate multimodal LLMs on cross-modal questions. <span id="S1.p3.1.7" class="ltx_text ltx_font_smallcaps">Multi</span> comprises 18,430 questions sourced from various educational and online materials, with most questions undergoing multiple rounds of human annotation for quality assurance. These questions cover a variety of scientific disciplines, including mathematics, physics, computer science, etc., and also pose significant challenges to intricate image reasoning. <span id="S1.p3.1.8" class="ltx_text ltx_font_smallcaps">Multi</span> serves as the first benchmark incorporating driving tests and administrative aptitude tests in China. The questions are crafted to test understanding and generation in various formats and complexity levels and are categorized into multiple-choice (with single or multiple correct answers), fill-in-the-blank, and open-ended questions.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To further challenge multimodal LLMs, we develop two subsets within <span id="S1.p4.1.1" class="ltx_text ltx_font_smallcaps">Multi</span>: <span id="S1.p4.1.2" class="ltx_text ltx_font_smallcaps">Multi-Elite</span>  consists of 500 carefully selected tough questions aiming to probe the limits of these models, and <span id="S1.p4.1.3" class="ltx_text ltx_font_smallcaps">Multi-Extend</span>  featuring 4,596 knowledge pieces tests the models’ capabilities of learning and knowledge transfer. These subsets offer deeper insights into the strengths and weaknesses of multimodal LLMs, fostering new research avenues. An example of <span id="S1.p4.1.4" class="ltx_text ltx_font_smallcaps">Multi</span> is shown in <a href="#S0.F1" title="In Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>, and more are presented in <a href="#A6" title="Appendix F More Examples ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">F</span></a>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We conduct comprehensive experiments on <span id="S1.p5.1.1" class="ltx_text ltx_font_smallcaps">Multi</span> using leading-edge multimodal and single-modality LLMs.
Our findings reveal that multimodal LLMs still lag behind human performance in many aspects of <span id="S1.p5.1.2" class="ltx_text ltx_font_smallcaps">Multi</span>, highlighting challenges like cross-modal alignment, logical reasoning, mathematical computations, and image comprehension. Results show that the benchmark is challenging for current models, not to mention the <span id="S1.p5.1.3" class="ltx_text ltx_font_smallcaps">Multi-Elite</span> set where GPT-4V only gets a 14.0% score, and most of the other models get a score near random, indicating a large space for improvement.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In conclusion, We make the following contributions in this work:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_smallcaps">Multi</span>, a substantial and challenging multimodal benchmark focusing on Chinese scientific questions, designed to evaluate multimodal LLMs.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We introduce <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_smallcaps">Multi-Elite</span> and <span id="S1.I1.i2.p1.1.2" class="ltx_text ltx_font_smallcaps">Multi-Extend</span> sets to test models’ bottleneck and in-context learning abilities, aiming for a more nuanced evaluation of multimodal LLMs.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We present detailed experiments with various state-of-the-art multimodal and single-modality LLMs on <span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_smallcaps">Multi</span>, providing both qualitative and quantitative insights into their performance.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We make the <span id="S1.I1.i4.p1.1.1" class="ltx_text ltx_font_smallcaps">Multi</span> leaderboard, dataset, evaluation code, and the two subsets available to the research community, encouraging further participation and advancement in the field of multimodal LLMs.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Multimodal Large Language Models (MLLMs).</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">With advancements in aligning features across multiple modalities, like CLIP <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite> and ALBEF <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>, recent studies have explored projecting vision features into the latent space of LLMs, aiming to enhance their capabilities of comprehending visual information. For example, BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib16" title="" class="ltx_ref">2023c</a>)</cite> pioneers this approach by employing Q-Former to translate image features into text representations. Following this, LLaVA <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite>, MiniGPT-4 <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite>, and InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">Dai et al. (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite> have introduced visual instruction tuning to bolster the capability of MLLMs of following instructions. Our primary focus is on the proficiency of MLLMs in comprehending instructions in Chinese, which are divided into two main branches: open-source models, which typically build upon existing Chinese LLMs or are fine-tuned on Chinese instruction datasets, examples of which include Chinese-LLaVA <cite class="ltx_cite ltx_citemacro_cite">LinkSoul-AI (<a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite>, VisualGLM <cite class="ltx_cite ltx_citemacro_cite">Du et al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite>, VisCPM <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>, Qwen-VL <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a href="#bib.bib3" title="" class="ltx_ref">2023a</a>)</cite>, InternVL <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib38" title="" class="ltx_ref">2023a</a>)</cite>, Yi-VL <cite class="ltx_cite ltx_citemacro_cite">01.ai (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>; and closed-source models, which are often highly powerful, multi-lingual systems such as GPT-4V(ision) <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib30" title="" class="ltx_ref">2023b</a>)</cite> and Gemini <cite class="ltx_cite ltx_citemacro_cite">Team (<a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>. In this paper, we intend to evaluate these models across a range of scientific fields on the <span id="S2.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">Multi</span> benchmark, offering an extensive assessment and guidance for the onward trajectory of Chinese MLLMs.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Benchmarks for MLLMs.</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">In assessing MLLMs, traditional methods primarily rely on established vision-language (VL) benchmark datasets. Renowned benchmarks such as VQA <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib10" title="" class="ltx_ref">2017</a>)</cite>, OK-VQA <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite>, GQA <cite class="ltx_cite ltx_citemacro_cite">Hudson and Manning (<a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite>, and MSCOCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib21" title="" class="ltx_ref">2014</a>)</cite> are tailored to specific VL tasks like image captioning, open-domain visual question answering, and visual reasoning. While the evaluation based on standard benchmark datasets yields significant insights into MLLMs’ capabilities, these approaches may not entirely capture their comprehensive intelligence in real-world scenarios. Therefore, a diverse array of benchmarks has been developed to examine MLLMs on dealing with various tasks in real world. Benchmarks like LLaVA-Bench <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite>, MMBench <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023c</a>)</cite>, MM-VET <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite>, TouchStone <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a href="#bib.bib4" title="" class="ltx_ref">2023b</a>)</cite>, MLLM-bench <cite class="ltx_cite ltx_citemacro_cite">Ge et al. (<a href="#bib.bib9" title="" class="ltx_ref">2023</a>)</cite>, and SEED-Bench<cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib15" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib14" title="" class="ltx_ref">a</a>)</cite>, for instance, leverage GPT to evaluate the relevance and helpfulness of human-like long responses in the reality. POPE <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023d</a>)</cite> and HallusionBench <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib23" title="" class="ltx_ref">2023a</a>)</cite> introduce various analytical criteria for the holistic evaluation of MLLMs’ hallucinations. Furthermore, M3Exam <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib39" title="" class="ltx_ref">2023b</a>)</cite>, SciGraphQA <cite class="ltx_cite ltx_citemacro_cite">Li and Tajbakhsh (<a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>, MathVista <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>, AGIEval <cite class="ltx_cite ltx_citemacro_cite">Zhong et al. (<a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite>, and MMMU <cite class="ltx_cite ltx_citemacro_cite">Yue et al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite> consider MLLMs as experts to extend the evaluation scope by incorporating advanced perception and reasoning within domain-specific knowledge, for example, scientific questions and driving tests. The works most related to us are M3Exam, ScienceQA, SciEval <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib32" title="" class="ltx_ref">2023a</a>)</cite> and C-Eval <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite>. Our approach distinguishes itself by offering a broader spectrum of question types compared to the first two and supports a multimodal evaluation in contrast to the last two.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The <span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Multi</span> Benchmark</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We propose <span id="S3.p1.1.1" class="ltx_text ltx_font_smallcaps">Multi</span>, a <span id="S3.p1.1.2" class="ltx_text ltx_font_typewriter">M</span>ultimodal <span id="S3.p1.1.3" class="ltx_text ltx_font_typewriter">U</span>nderstanding <span id="S3.p1.1.4" class="ltx_text ltx_font_typewriter">L</span>eaderboard with <span id="S3.p1.1.5" class="ltx_text ltx_font_typewriter">T</span>ext and <span id="S3.p1.1.6" class="ltx_text ltx_font_typewriter">I</span>mages, which can serve as a challenging and diverse benchmark for the MLLM community. The detailed statistics are provided in <a href="#A1" title="Appendix A Statistics ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Construction Process</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2402.03173/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="113" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The construction pipeline of <span id="S3.F3.2.1" class="ltx_text ltx_font_smallcaps">Multi</span>.</figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The data construction pipeline is shown in <a href="#S3.F3" title="In 3.1 Data Construction Process ‣ 3 The Multi Benchmark ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>. To develop <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">Multi</span>, we follow several key steps to ensure high-quality and precise annotation. Firstly, we crawl open-source raw question data from the Internet and transcript close-source exams from paper documents. Secondly, we format each question and knowledge piece into markdown and <span id="S3.SS1.p1.1.2" class="ltx_text ltx_LaTeX_logo" style="letter-spacing:-0.2em; margin-right:0.1em;">L<span id="S3.SS1.p1.1.2.1" class="ltx_text" style="position:relative; bottom:0.4ex;font-variant:small-caps;;">a</span>T<span id="S3.SS1.p1.1.2.2" class="ltx_text" style="position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;">e</span>X</span> formula format to maintain precision and quality. Thirdly, we revise and refine each question multiple times to prevent data leakage and increase difficulty. Lastly, We rate every question based on its difficulty and content richness.</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data Source</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">We collect more than 2.7M raw data from the Internet, ranging from exams and quizzes from Chinese junior and senior schools and several society exams. We design an algorithm to pick out a proportion of the questions as the fundamental data of our benchmark. The selection is based on the questions’ text length, number of images, corresponding subjects, and knowledge pieces, to reach a higher diversity of questions and coverage of knowledge. The details are presented in <a href="#A4" title="Appendix D Data Selection Algorithm ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">D</span></a>. We also collect questions from internal exams and practices of several top universities. After the selection, we obtain over 18K questions as the raw data.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data Process and Annotation</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">The data process and annotation for our dataset involve a comprehensive series of steps to ensure high-quality, diverse content.</p>
</div>
<div id="S3.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p2.1" class="ltx_p">In the <span id="S3.SS1.SSS0.Px2.p2.1.1" class="ltx_text ltx_font_bold">Data Pre-process</span> stage, raw data with formats like HTML, photocopy, hand script, or plain text are refined by removing irrelevant HTML tags, converting text styles into markdown format, and transcribing math functions and chemical structures into <span id="S3.SS1.SSS0.Px2.p2.1.2" class="ltx_text ltx_LaTeX_logo" style="letter-spacing:-0.2em; margin-right:0.1em;">L<span id="S3.SS1.SSS0.Px2.p2.1.2.1" class="ltx_text" style="position:relative; bottom:0.4ex;font-variant:small-caps;;">a</span>T<span id="S3.SS1.SSS0.Px2.p2.1.2.2" class="ltx_text" style="position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;">e</span>X</span> format, with complex tables saved as screenshot images after HTML rendering. OCR tools are utilized for text conversion from photocopies and hand scripts.</p>
</div>
<div id="S3.SS1.SSS0.Px2.p3" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p3.1" class="ltx_p">During the <span id="S3.SS1.SSS0.Px2.p3.1.1" class="ltx_text ltx_font_bold">Data Annotation</span> stage, an online platform facilitates annotators, primarily skilled undergraduates (involved in the work as authors), in tasks across format, content, label, and semantic levels. This includes converting content into markdown and <span id="S3.SS1.SSS0.Px2.p3.1.2" class="ltx_text ltx_LaTeX_logo" style="letter-spacing:-0.2em; margin-right:0.1em;">L<span id="S3.SS1.SSS0.Px2.p3.1.2.1" class="ltx_text" style="position:relative; bottom:0.4ex;font-variant:small-caps;;">a</span>T<span id="S3.SS1.SSS0.Px2.p3.1.2.2" class="ltx_text" style="position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;">e</span>X</span>, splitting sub-questions into individual ones, evaluating the difficulty and quality, and correcting errors for factual accuracy.</p>
</div>
<div id="S3.SS1.SSS0.Px2.p4" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p4.1" class="ltx_p">The <span id="S3.SS1.SSS0.Px2.p4.1.1" class="ltx_text ltx_font_bold">Data Post-process</span> stage employs strategies like formation, disambiguration, distillation, and transformation to enhance question difficulty and diversity, including modifying question formats and reducing assistance information.</p>
</div>
<div id="S3.SS1.SSS0.Px2.p5" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p5.1" class="ltx_p">Throughout these stages, we process 2.7 million questions in total and pick out 18,430, incorporating 23,320 scoring points, 7,658 images, and 4,595 knowledge pieces. <span id="S3.SS1.SSS0.Px2.p5.1.1" class="ltx_text ltx_font_smallcaps">Multi</span> highlights a broad diversity in question types, including multiple-choice questions with both single and multiple answers, along with fill-in-the-blank and open-ended writing questions enriching the testing scenarios.
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>For the sake of simplifying writing, in the following paragraphs we may use abbreviations. We denote multiple-choice questions with a single answer as <span id="footnote1.1" class="ltx_text ltx_font_bold">SA</span> or <span id="footnote1.2" class="ltx_text ltx_font_italic">Single Answer Choosing</span> and those with multiple answers as <span id="footnote1.3" class="ltx_text ltx_font_bold">MA</span> or <span id="footnote1.4" class="ltx_text ltx_font_italic">Multi Answer Choosing</span>. We use <span id="footnote1.5" class="ltx_text ltx_font_bold">FB</span> for fill-in-the-blank questions and <span id="footnote1.6" class="ltx_text ltx_font_bold">OP</span> for open-ended writing questions.</span></span></span>
The stages during data processing and annotation significantly increase the diversity and difficulty of the dataset. For details of data processing and annotation, please refer to <a href="#A5" title="Appendix E Data Process and Annotation ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>The <span id="S3.SS2.1.1" class="ltx_text ltx_font_smallcaps">Multi-Elite</span> Set</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We select an additional set of 500 questions to create the advanced dataset. This set is comprised of objective questions, i.e. multiple-choice and fill-in-the-blank questions. The questions are averagely distributed in all of the subjects and education levels, evaluated as with high difficulty and quality by annotators, and with rich text and image content. The evaluation results presented in <a href="#S4" title="4 Experiments ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">4</span></a> are also referred to in the selection, where the results of GPT-4V<cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib30" title="" class="ltx_ref">2023b</a>)</cite> are given the most consideration.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>The <span id="S3.SS3.1.1" class="ltx_text ltx_font_smallcaps">Multi-Extend</span> Background Knowledge Dataset</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">External knowledge is crucial to provide critical information that assists in solving questions using the In-Context Learning (ICL) abilities. Some of the raw questions retrieved from the Internet have corresponding knowledge pieces attached. We also collect more knowledge pieces for uncovered questions with the assistance of LLMs and outer knowledge source (e.g. New Bing<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://bing.com/new" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://bing.com/new</a></span></span></span> and Wikipedia<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://wikipedia.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://wikipedia.org</a></span></span></span>). We conduct annotations on these knowledge pieces to confirm the correctness of the content and present them in the <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_smallcaps">Multi-Extend</span> dataset. This dataset consists of about 4.6K knowledge pieces, designed to test the in-context learning abilities and knowledge transfer skills of models. This dataset provides comprehensive insights into the capabilities and limitations of multimodal LLMs, opening new pathways for research exploration.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Comparison with Existing Benchmarks</h3>

<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" style="padding-left:1.5pt;padding-right:1.5pt;" rowspan="2"><span id="S3.T1.1.1.1.1" class="ltx_text ltx_font_bold">Benchmark</span></td>
<td id="S3.T1.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:1.5pt;padding-right:1.5pt;" rowspan="2"><span id="S3.T1.1.1.2.1" class="ltx_text ltx_font_bold">Lang</span></td>
<td id="S3.T1.1.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" style="padding-left:1.5pt;padding-right:1.5pt;" colspan="5"><span id="S3.T1.1.1.3.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S3.T1.1.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" style="padding-left:1.5pt;padding-right:1.5pt;" colspan="3"><span id="S3.T1.1.1.4.1" class="ltx_text ltx_font_bold">Image</span></td>
<td id="S3.T1.1.1.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" style="padding-left:1.5pt;padding-right:1.5pt;" colspan="4"><span id="S3.T1.1.1.5.1" class="ltx_text ltx_font_bold">Answer Type</span></td>
<td id="S3.T1.1.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:1.5pt;padding-right:1.5pt;" rowspan="2"><span id="S3.T1.1.1.6.1" class="ltx_text ltx_font_bold">Source</span></td>
</tr>
<tr id="S3.T1.1.2" class="ltx_tr">
<td id="S3.T1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;"><span id="S3.T1.1.2.1.1" class="ltx_text ltx_font_bold">Sub</span></td>
<td id="S3.T1.1.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;"><span id="S3.T1.1.2.2.1" class="ltx_text ltx_font_bold">Q</span></td>
<td id="S3.T1.1.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;"><span id="S3.T1.1.2.3.1" class="ltx_text ltx_font_bold">Ana</span></td>
<td id="S3.T1.1.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;"><span id="S3.T1.1.2.4.1" class="ltx_text ltx_font_bold">Img</span></td>
<td id="S3.T1.1.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;"><span id="S3.T1.1.2.5.1" class="ltx_text ltx_font_bold">Kn</span></td>
<td id="S3.T1.1.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;"><span id="S3.T1.1.2.6.1" class="ltx_text ltx_font_bold">NI</span></td>
<td id="S3.T1.1.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;"><span id="S3.T1.1.2.7.1" class="ltx_text ltx_font_bold">SI</span></td>
<td id="S3.T1.1.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;"><span id="S3.T1.1.2.8.1" class="ltx_text ltx_font_bold">MI</span></td>
<td id="S3.T1.1.2.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;"><span id="S3.T1.1.2.9.1" class="ltx_text ltx_font_bold">SA</span></td>
<td id="S3.T1.1.2.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;"><span id="S3.T1.1.2.10.1" class="ltx_text ltx_font_bold">MA</span></td>
<td id="S3.T1.1.2.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;"><span id="S3.T1.1.2.11.1" class="ltx_text ltx_font_bold">FB</span></td>
<td id="S3.T1.1.2.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;"><span id="S3.T1.1.2.12.1" class="ltx_text ltx_font_bold">OP</span></td>
</tr>
<tr id="S3.T1.1.3" class="ltx_tr">
<td id="S3.T1.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">VQA <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite>
</td>
<td id="S3.T1.1.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">en</td>
<td id="S3.T1.1.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">36</td>
<td id="S3.T1.1.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">764K</td>
<td id="S3.T1.1.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">265K</td>
<td id="S3.T1.1.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.3.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.3.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.3.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.3.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.3.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.3.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.3.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.3.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">Repurposed</td>
</tr>
<tr id="S3.T1.1.4" class="ltx_tr">
<td id="S3.T1.1.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.5pt;padding-right:1.5pt;">ScienceQA <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S3.T1.1.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">en</td>
<td id="S3.T1.1.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">21</td>
<td id="S3.T1.1.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">21K</td>
<td id="S3.T1.1.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">19K</td>
<td id="S3.T1.1.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">10K</td>
<td id="S3.T1.1.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">0.3K</td>
<td id="S3.T1.1.4.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.4.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.4.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.4.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.4.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.4.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.4.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.4.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">Textbooks</td>
</tr>
<tr id="S3.T1.1.5" class="ltx_tr">
<td id="S3.T1.1.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.5pt;padding-right:1.5pt;">SciBench <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S3.T1.1.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">en</td>
<td id="S3.T1.1.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">6</td>
<td id="S3.T1.1.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">0.8K</td>
<td id="S3.T1.1.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">0.1K</td>
<td id="S3.T1.1.5.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.5.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.5.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.5.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.5.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.5.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.5.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.5.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.5.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">Textbooks</td>
</tr>
<tr id="S3.T1.1.6" class="ltx_tr">
<td id="S3.T1.1.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.5pt;padding-right:1.5pt;">M3Exam <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib39" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="S3.T1.1.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">9 langs</td>
<td id="S3.T1.1.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">4</td>
<td id="S3.T1.1.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">12K</td>
<td id="S3.T1.1.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">3.1K</td>
<td id="S3.T1.1.6.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.6.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.6.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.6.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.6.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.6.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.6.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.6.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.6.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">Exams</td>
</tr>
<tr id="S3.T1.1.7" class="ltx_tr">
<td id="S3.T1.1.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.5pt;padding-right:1.5pt;">AGIEval <cite class="ltx_cite ltx_citemacro_cite">Zhong et al. (<a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S3.T1.1.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">zh, en</td>
<td id="S3.T1.1.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">20</td>
<td id="S3.T1.1.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">8K</td>
<td id="S3.T1.1.7.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">a few</td>
<td id="S3.T1.1.7.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.7.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.7.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.7.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.7.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.7.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.7.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.7.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.7.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.7.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">Exams</td>
</tr>
<tr id="S3.T1.1.8" class="ltx_tr">
<td id="S3.T1.1.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.5pt;padding-right:1.5pt;">MMBench <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023c</a>)</cite>
</td>
<td id="S3.T1.1.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">en</td>
<td id="S3.T1.1.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">20</td>
<td id="S3.T1.1.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">3K</td>
<td id="S3.T1.1.8.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.8.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">3K</td>
<td id="S3.T1.1.8.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.8.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.8.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.8.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.8.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.8.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.8.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.8.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.8.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">Web, Repurposed</td>
</tr>
<tr id="S3.T1.1.9" class="ltx_tr">
<td id="S3.T1.1.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.5pt;padding-right:1.5pt;">SEED-Bench <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib15" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="S3.T1.1.9.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">en</td>
<td id="S3.T1.1.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">12</td>
<td id="S3.T1.1.9.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">19K</td>
<td id="S3.T1.1.9.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.9.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">19K+</td>
<td id="S3.T1.1.9.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.9.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.9.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.9.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.9.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.9.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.9.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.9.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.9.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">Anno.</td>
</tr>
<tr id="S3.T1.1.10" class="ltx_tr">
<td id="S3.T1.1.10.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.5pt;padding-right:1.5pt;">SEED-Bench-2 <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib14" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S3.T1.1.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">en</td>
<td id="S3.T1.1.10.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">27</td>
<td id="S3.T1.1.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">24K</td>
<td id="S3.T1.1.10.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.10.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">22K+</td>
<td id="S3.T1.1.10.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.10.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.10.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.10.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.10.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.10.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.10.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.10.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.10.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">Anno.</td>
</tr>
<tr id="S3.T1.1.11" class="ltx_tr">
<td id="S3.T1.1.11.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.5pt;padding-right:1.5pt;">MLLM-Bench <cite class="ltx_cite ltx_citemacro_cite">Ge et al. (<a href="#bib.bib9" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S3.T1.1.11.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">en</td>
<td id="S3.T1.1.11.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">42</td>
<td id="S3.T1.1.11.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">0.4K</td>
<td id="S3.T1.1.11.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.11.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">0.4K</td>
<td id="S3.T1.1.11.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.11.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.11.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.11.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.11.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.11.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.11.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.11.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.11.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">Anno.</td>
</tr>
<tr id="S3.T1.1.12" class="ltx_tr">
<td id="S3.T1.1.12.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.5pt;padding-right:1.5pt;">Touchstone <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a href="#bib.bib4" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="S3.T1.1.12.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">en</td>
<td id="S3.T1.1.12.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">27</td>
<td id="S3.T1.1.12.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">0.9K</td>
<td id="S3.T1.1.12.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.12.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">0.9K</td>
<td id="S3.T1.1.12.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.12.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.12.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.12.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.12.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.12.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.12.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.12.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.12.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">Anno.</td>
</tr>
<tr id="S3.T1.1.13" class="ltx_tr">
<td id="S3.T1.1.13.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.5pt;padding-right:1.5pt;">C-Eval <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S3.T1.1.13.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">zh</td>
<td id="S3.T1.1.13.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">52</td>
<td id="S3.T1.1.13.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">14K</td>
<td id="S3.T1.1.13.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">a few</td>
<td id="S3.T1.1.13.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.13.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.13.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.13.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.13.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.13.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.13.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.13.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.13.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.13.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">Exams, Web</td>
</tr>
<tr id="S3.T1.1.14" class="ltx_tr">
<td id="S3.T1.1.14.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.5pt;padding-right:1.5pt;">SciEval <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib32" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S3.T1.1.14.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">en</td>
<td id="S3.T1.1.14.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">3</td>
<td id="S3.T1.1.14.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">18K</td>
<td id="S3.T1.1.14.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.14.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.14.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.14.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.14.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.14.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.14.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.14.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.14.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.14.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.14.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">Web, Repurposed</td>
</tr>
<tr id="S3.T1.1.15" class="ltx_tr">
<td id="S3.T1.1.15.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.5pt;padding-right:1.5pt;">MMMU <cite class="ltx_cite ltx_citemacro_cite">Yue et al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S3.T1.1.15.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">en</td>
<td id="S3.T1.1.15.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">30</td>
<td id="S3.T1.1.15.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">12K</td>
<td id="S3.T1.1.15.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">2K</td>
<td id="S3.T1.1.15.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">11K+</td>
<td id="S3.T1.1.15.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">-</td>
<td id="S3.T1.1.15.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.15.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.15.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.15.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.15.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.15.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.15.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">✗</td>
<td id="S3.T1.1.15.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.5pt;padding-right:1.5pt;">Anno., Web, Textbooks</td>
</tr>
<tr id="S3.T1.1.16" class="ltx_tr">
<td id="S3.T1.1.16.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">
<span id="S3.T1.1.16.1.1" class="ltx_text ltx_font_smallcaps">Multi</span> (ours)</td>
<td id="S3.T1.1.16.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">zh</td>
<td id="S3.T1.1.16.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">23</td>
<td id="S3.T1.1.16.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">18K</td>
<td id="S3.T1.1.16.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">10K+</td>
<td id="S3.T1.1.16.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">7.7K</td>
<td id="S3.T1.1.16.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">4.6K</td>
<td id="S3.T1.1.16.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.16.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.16.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.16.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.16.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.16.13" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.16.14" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">✓</td>
<td id="S3.T1.1.16.15" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:1.5pt;padding-right:1.5pt;">Anno., Exams, Web</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The comparison between <span id="S3.T1.3.1" class="ltx_text ltx_font_smallcaps">Multi</span> and other existing benchmarks. Sub: Subject or Field, Q: Question, Ana: Analysis or Explanations, Img: Images, Kn: Knowledge or Lecture. NI: the question with pure text, SI: the question with a single image, MI: the question with multiple images. SA: multiple-choice question with single correct answer, MA: multiple-choice question with multiple correct answers, FB: fill-in-the-blank question (no more than 10 words), OP: open-ended writing question (more than 10 words). Anno.: Annotation</figcaption>
</figure>
<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p"><span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_smallcaps">Multi</span> demonstrates a comprehensive blend of features that surpasses existing benchmarks in several dimensions. Notably, <span id="S3.SS4.p1.1.2" class="ltx_text ltx_font_smallcaps">Multi</span> covers a wide array of subjects and a substantial number of questions (18K), as well as over 10K analysis and 4.6K extensive knowledge content, which is considerably larger than most benchmarks, ensuring a broad and diverse testing environment. <span id="S3.SS4.p1.1.3" class="ltx_text ltx_font_smallcaps">Multi</span> possesses 7.7K images, which is essential for benchmarking MLLMs that require visual understanding alongside textual information. The inclusion of both single and multiple image questions, as well as a variety of answer types, makes <span id="S3.SS4.p1.1.4" class="ltx_text ltx_font_smallcaps">Multi</span> a versatile and challenging benchmark. Furthermore, the questions without images also test the MLLMs’ ability on dealing with plain text information. Meanwhile, the various sources, complex annotation, and processing stages provide sufficient augmentation to alleviate data leakage. <span id="S3.SS4.p1.1.5" class="ltx_text ltx_font_smallcaps">Multi</span> not only encompasses variations of classic questions but also includes recently updated questions, which significantly enhances its diversity.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">We list the features of existing benchmarks and make a comparison with <span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_smallcaps">Multi</span> in <a href="#S3.T1" title="In 3.4 Comparison with Existing Benchmarks ‣ 3 The Multi Benchmark ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>. We believe that <span id="S3.SS4.p2.1.2" class="ltx_text ltx_font_smallcaps">Multi</span> assembles the most advantages of the existing benchmarks and is sure to provide a good option for the community to test the capabilities of their Vision LLMs.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Models</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We evaluate a wide range of MLLMs that support Chinese, including Chinese-LLaVA <cite class="ltx_cite ltx_citemacro_cite">LinkSoul-AI (<a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite>, Qwen-VL <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a href="#bib.bib3" title="" class="ltx_ref">2023a</a>)</cite>, VisCPM <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>, VisualGLM <cite class="ltx_cite ltx_citemacro_cite">Du et al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite>, InternVL <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>, Yi-VL <cite class="ltx_cite ltx_citemacro_cite">01.ai (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>, Gemini Vision <cite class="ltx_cite ltx_citemacro_cite">Team (<a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>, and GPT-4V <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib30" title="" class="ltx_ref">2023b</a>)</cite>. We evaluate these models with both multimodal input and text-only input to verify the information gain of input images. We also select several most capable LLMs for comparison with text-only input, including DFM-2 <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>, MOSS <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib33" title="" class="ltx_ref">2023b</a>)</cite>, ChatGPT <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>, GPT-4 <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib29" title="" class="ltx_ref">2023a</a>)</cite>, and Gemini <cite class="ltx_cite ltx_citemacro_cite">Team (<a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>, and the performance of these models on questions with images will reflect their abilities on finding loss of information. Model specifications are listed in <a href="#A2.T10" title="In Appendix B Models ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">10</span></a>. Due to the API request rate limit of Gemini and GPTs, ablation studies are mostly performed on weight-accessible models. We choose the checkpoints with the largest model size and the latest version, and use FP16 or INT4 quantization to accelerate inference if officially provided. We follow the official guidelines to prompt each model so that the outputs go in the desired way.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Settings</h3>

<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Prompt</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">We use specialized prompts for each question, an example shown in <a href="#S4.F4" title="In Prompt ‣ 4.2 Settings ‣ 4 Experiments ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>. The prompts are designed carefully according to the features of each type of question and the answer patterns expected. We also modify the input format to fit into official inference guidelines. The complete collection of prompts is presented in <a href="#A3" title="Appendix C Prompts ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>An example of the prompts used when evaluating a multiple-choice question with image context, knowledge piece and single correct answer.</figcaption>
</figure>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Image</h4>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p1.1" class="ltx_p"><span id="S4.SS2.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_smallcaps">Multi</span> includes questions with either none, single, or multiple images. Most MLLMs accept text accompanied by one image as input or a pure-text input. For questions with a single image, the image and text are fed in one turn. We simply drop image information when evaluating LLMs.</p>
</div>
<div id="S4.SS2.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p2.1" class="ltx_p">For pure-text questions, we use the text as input. For some models like VisCPM, InternVL and Yi-VL which compulsorily demand an image in each turn, we feed the model a blank image with color set to RGB(0,0,0) along with plain text in evaluation. For efficiency, results of GPT-4 and Gemini on pure-text questions are directly used as the results of GPT-4V and Gemini Vision respectively.</p>
</div>
<div id="S4.SS2.SSS0.Px2.p3" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p3.1" class="ltx_p">For questions with multiple images, as the positions of images matter a lot, e.g., a multiple-choice question where each choice is an image, special patterns with <span id="S4.SS2.SSS0.Px2.p3.1.1" class="ltx_text ltx_font_typewriter">[IMAGE_{index}]</span> are used to indicate the position and order of images. Qwen-VL, GPT-4V, and Gemini Vision naturally support multiple images as input in one turn, while VisCPM and VisualGLM support only one image as input in one turn. We adopt the strategy of splitting the content into multiple segments divided by each image and feeding them into the MLLM sequentially as rounds of conversation, where the MLLM receives each segment along with the corresponding image. We tune our prompts so that the MLLM may receive all the information but should only give a finalized answer after we show a signal that the question ends. The prompt we use in multi-turn input is shown in <a href="#A3.F5" title="In Appendix C Prompts ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a>. As the released versions of Chinese-LLaVA, InternVL and Yi-VL do not support multiple images as input, currently only the first image is used for evaluating each question.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Metrics</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We focus on objective questions with a certain answer, including multiple-choice and blank-filling questions. We also give a score to each subjective open-ended question based on the similarity to the reference answer. The metrics we use for each type of question:</p>
</div>
<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Multiple-choice with Single Answer (SA)</h4>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p">Each question worth one point. We calculate the accuracy of the given answer.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Multiple-choice with Multiple Answers (MA)</h4>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p1.1" class="ltx_p">We define the total points of an MA question as the number of correct choices, and each correct choice selected is rewarded one point. If the given answer contains any wrong choice, the score will be counted to zero. We report the score ratio (# points / # total points) as the metric. We also report accuracy as a more rigorous metric, where only correctly giving all the choices without wrong ones will be granted points. <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>For example, a question with correct answer <span id="footnote4.1" class="ltx_text ltx_font_typewriter">ACE</span> worth 3 points, and answer <span id="footnote4.2" class="ltx_text ltx_font_typewriter">AC</span> will be granted 2 points and answer <span id="footnote4.3" class="ltx_text ltx_font_typewriter">BC</span> or <span id="footnote4.4" class="ltx_text ltx_font_typewriter">ABCE</span> will be granted 0 points. However, on calculating accuracy none will be counted, and only <span id="footnote4.5" class="ltx_text ltx_font_typewriter">ACE</span> will be calculated as correct.</span></span></span></p>
</div>
</section>
<section id="S4.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Fill in the Blank (FB)</h4>

<div id="S4.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px3.p1.1" class="ltx_p">We define the total points of a blank-filling question as the number of the blanks marked as <span id="S4.SS3.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_typewriter">[MASK]</span>. It is required in prompts that each line of the given answer corresponds to a blank in order. We follow the most strict standard of <span id="S4.SS3.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_italic">exact match</span>. Therefore, only answers exactly matching the standard answers will be granted points. We report the score ratio as the final metric.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Open-ended Question (OP)</h4>

<div id="S4.SS3.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px4.p1.1" class="ltx_p">The points and counting method is similar to FB, but we use a loose standard and report normalized ROUGE-L <cite class="ltx_cite ltx_citemacro_cite">Lin (<a href="#bib.bib20" title="" class="ltx_ref">2004</a>)</cite> score for each point. Please note that the reference answer may be concise or in detail, and there could be other possible answers.</p>
</div>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Main Experiment Results</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We report the overall and field-specific performance of tested models on the whole benchmark in Table <a href="#S4.T2" title="Table 2 ‣ 4.4 Main Experiment Results ‣ 4 Experiments ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, <a href="#S4.T3" title="Table 3 ‣ Comparison by number of images. ‣ 4.4 Main Experiment Results ‣ 4 Experiments ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, and <a href="#S4.T4" title="Table 4 ‣ Comparison by question type. ‣ 4.4 Main Experiment Results ‣ 4 Experiments ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T2.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S4.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.1.2.1" class="ltx_text ltx_font_bold">Overall</span></td>
<td id="S4.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.1.3.1" class="ltx_text ltx_font_bold">NI</span></td>
<td id="S4.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.1.4.1" class="ltx_text ltx_font_bold">SI</span></td>
<td id="S4.T2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.1.5.1" class="ltx_text ltx_font_bold">MI</span></td>
</tr>
<tr id="S4.T2.1.2" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.T2.1.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="5"><span id="S4.T2.1.2.1.1" class="ltx_text ltx_font_italic" style="background-color:#E6E6E6;">Puretext (LLM)</span></td>
</tr>
<tr id="S4.T2.1.3" class="ltx_tr">
<td id="S4.T2.1.3.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">MOSS</td>
<td id="S4.T2.1.3.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">32.6</td>
<td id="S4.T2.1.3.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">36.1</td>
<td id="S4.T2.1.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">27.3</td>
<td id="S4.T2.1.3.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">17.1</td>
</tr>
<tr id="S4.T2.1.4" class="ltx_tr">
<td id="S4.T2.1.4.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">DFM-2.0</td>
<td id="S4.T2.1.4.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">49.7</td>
<td id="S4.T2.1.4.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">63.0</td>
<td id="S4.T2.1.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">28.7</td>
<td id="S4.T2.1.4.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">11.3</td>
</tr>
<tr id="S4.T2.1.5" class="ltx_tr">
<td id="S4.T2.1.5.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Gemini</td>
<td id="S4.T2.1.5.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.5.2.1" class="ltx_text ltx_font_bold">52.2</span></td>
<td id="S4.T2.1.5.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">62.5</td>
<td id="S4.T2.1.5.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.5.4.1" class="ltx_text ltx_font_bold">36.2</span></td>
<td id="S4.T2.1.5.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.5.5.1" class="ltx_text ltx_font_bold">18.3</span></td>
</tr>
<tr id="S4.T2.1.6" class="ltx_tr">
<td id="S4.T2.1.6.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ChatGPT</td>
<td id="S4.T2.1.6.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">35.9</td>
<td id="S4.T2.1.6.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">54.0</td>
<td id="S4.T2.1.6.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.8</td>
<td id="S4.T2.1.6.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.1</td>
</tr>
<tr id="S4.T2.1.7" class="ltx_tr">
<td id="S4.T2.1.7.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">GPT-4</td>
<td id="S4.T2.1.7.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">50.2</td>
<td id="S4.T2.1.7.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.7.3.1" class="ltx_text ltx_font_bold">74.5</span></td>
<td id="S4.T2.1.7.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">11.3</td>
<td id="S4.T2.1.7.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8.8</td>
</tr>
<tr id="S4.T2.1.8" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.T2.1.8.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="5"><span id="S4.T2.1.8.1.1" class="ltx_text ltx_font_italic" style="background-color:#E6E6E6;">Text+Image (MLLM)</span></td>
</tr>
<tr id="S4.T2.1.9" class="ltx_tr">
<td id="S4.T2.1.9.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Chinese-LLaVA</td>
<td id="S4.T2.1.9.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">28.5</td>
<td id="S4.T2.1.9.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">32.3</td>
<td id="S4.T2.1.9.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">22.6</td>
<td id="S4.T2.1.9.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">17.8</td>
</tr>
<tr id="S4.T2.1.10" class="ltx_tr">
<td id="S4.T2.1.10.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">VisualGLM</td>
<td id="S4.T2.1.10.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">31.1</td>
<td id="S4.T2.1.10.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">35.1</td>
<td id="S4.T2.1.10.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">25.2</td>
<td id="S4.T2.1.10.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">9.7</td>
</tr>
<tr id="S4.T2.1.11" class="ltx_tr">
<td id="S4.T2.1.11.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">VisCPM</td>
<td id="S4.T2.1.11.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">33.4</td>
<td id="S4.T2.1.11.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">36.8</td>
<td id="S4.T2.1.11.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">28.4</td>
<td id="S4.T2.1.11.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">16.6</td>
</tr>
<tr id="S4.T2.1.12" class="ltx_tr">
<td id="S4.T2.1.12.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Qwen-VL</td>
<td id="S4.T2.1.12.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">39.0</td>
<td id="S4.T2.1.12.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">43.2</td>
<td id="S4.T2.1.12.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">32.7</td>
<td id="S4.T2.1.12.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">20.7</td>
</tr>
<tr id="S4.T2.1.13" class="ltx_tr">
<td id="S4.T2.1.13.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">InternVL</td>
<td id="S4.T2.1.13.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">44.9</td>
<td id="S4.T2.1.13.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">50.9</td>
<td id="S4.T2.1.13.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">35.5</td>
<td id="S4.T2.1.13.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">25.1</td>
</tr>
<tr id="S4.T2.1.14" class="ltx_tr">
<td id="S4.T2.1.14.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Yi-VL</td>
<td id="S4.T2.1.14.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">55.3</td>
<td id="S4.T2.1.14.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">63.8</td>
<td id="S4.T2.1.14.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">42.0</td>
<td id="S4.T2.1.14.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">24.5</td>
</tr>
<tr id="S4.T2.1.15" class="ltx_tr">
<td id="S4.T2.1.15.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Gemini Vision</td>
<td id="S4.T2.1.15.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">53.7</td>
<td id="S4.T2.1.15.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">62.5</td>
<td id="S4.T2.1.15.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">40.0</td>
<td id="S4.T2.1.15.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">24.5</td>
</tr>
<tr id="S4.T2.1.16" class="ltx_tr">
<td id="S4.T2.1.16.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">GPT-4V</td>
<td id="S4.T2.1.16.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.16.2.1" class="ltx_text ltx_font_bold">63.7</span></td>
<td id="S4.T2.1.16.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.16.3.1" class="ltx_text ltx_font_bold">74.5</span></td>
<td id="S4.T2.1.16.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.16.4.1" class="ltx_text ltx_font_bold">46.9</span></td>
<td id="S4.T2.1.16.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.1.16.5.1" class="ltx_text ltx_font_bold">28.1</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The main performance of models evaluated on <span id="S4.T2.3.1" class="ltx_text ltx_font_smallcaps">Multi</span>. NI: the question with no image, SI: the question with a single image, MI: the question with multiple images.</figcaption>
</figure>
<section id="S4.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Overall comparison.</h4>

<div id="S4.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px1.p1.1" class="ltx_p">We report the overall performance in <a href="#S4.T2" title="In 4.4 Main Experiment Results ‣ 4 Experiments ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>. <span id="S4.SS4.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_bold">The most powerful competitor, GPT-4V, achieves a mere 63.7% score</span>, underscoring the benchmark’s complexity and challenge. Yi-VL outperforms other open-source models, but there still remains a notable gap with GPT-4V, and those smaller models do not get as much as half of the scores.</p>
</div>
</section>
<section id="S4.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Comparison by number of images.</h4>

<div id="S4.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px2.p1.1" class="ltx_p">In <a href="#S4.T2" title="In 4.4 Main Experiment Results ‣ 4 Experiments ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>, we also present the performance categorized by image number. For MLLMs, a higher score on the Non-Image (NI) set suggests improved performance on multimodal questions, including the Single Image (SI) set and Multiple Image (MI) set. <span id="S4.SS4.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_bold">It is evident that questions requiring more images are more challenging.</span> A significant drop in performance is observed when answering questions with more than one image. Only GPT-4V (28.1%) manages to exceed the average baseline set by random guessing.</p>
</div>
<div id="S4.SS4.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS4.SSS0.Px2.p2.1" class="ltx_p">Conversely, for LLMs, there exists a reverse correlation between scores on the NI set and those on the SI and MI sets. This is because we prompt the model to determine whether visual information is necessary for answering a question and if so the model needs to refuse to answer. Less capable models may simply make a guess, but more sophisticated models tend to withhold an answer, resulting in lower but more reliable overall scores. The results on SI and MI sets for LLMs indicate a long way before mitigating hallucination.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T3.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T3.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S4.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T3.1.1.2.1" class="ltx_text ltx_font_bold">SA</span></td>
<td id="S4.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T3.1.1.3.1" class="ltx_text ltx_font_bold">MA</span></td>
<td id="S4.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T3.1.1.4.1" class="ltx_text"></span> <span id="S4.T3.1.1.4.2" class="ltx_text">
<span id="S4.T3.1.1.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.1.1.4.2.1.1" class="ltx_tr">
<span id="S4.T3.1.1.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T3.1.1.4.2.1.1.1.1" class="ltx_text ltx_font_bold">MA</span></span></span>
<span id="S4.T3.1.1.4.2.1.2" class="ltx_tr">
<span id="S4.T3.1.1.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T3.1.1.4.2.1.2.1.1" class="ltx_text ltx_font_bold">Acc.</span></span></span>
</span></span><span id="S4.T3.1.1.4.3" class="ltx_text"></span></td>
<td id="S4.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T3.1.1.5.1" class="ltx_text ltx_font_bold">FB</span></td>
<td id="S4.T3.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T3.1.1.6.1" class="ltx_text ltx_font_bold">OP</span></td>
</tr>
<tr id="S4.T3.1.2" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.T3.1.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="6"><span id="S4.T3.1.2.1.1" class="ltx_text ltx_font_italic" style="background-color:#E6E6E6;">Puretext (LLM)</span></td>
</tr>
<tr id="S4.T3.1.3" class="ltx_tr">
<td id="S4.T3.1.3.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">MOSS</td>
<td id="S4.T3.1.3.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">38.5</td>
<td id="S4.T3.1.3.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">33.1</td>
<td id="S4.T3.1.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.8</td>
<td id="S4.T3.1.3.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2.7</td>
<td id="S4.T3.1.3.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8.7</td>
</tr>
<tr id="S4.T3.1.4" class="ltx_tr">
<td id="S4.T3.1.4.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">DFM-2.0</td>
<td id="S4.T3.1.4.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">55.8</td>
<td id="S4.T3.1.4.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">53.9</td>
<td id="S4.T3.1.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">29.7</td>
<td id="S4.T3.1.4.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">13.3</td>
<td id="S4.T3.1.4.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">10.3</td>
</tr>
<tr id="S4.T3.1.5" class="ltx_tr">
<td id="S4.T3.1.5.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Gemini</td>
<td id="S4.T3.1.5.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T3.1.5.2.1" class="ltx_text ltx_font_bold">58.2</span></td>
<td id="S4.T3.1.5.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">52.7</td>
<td id="S4.T3.1.5.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">22.8</td>
<td id="S4.T3.1.5.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">29.1</td>
<td id="S4.T3.1.5.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.9</td>
</tr>
<tr id="S4.T3.1.6" class="ltx_tr">
<td id="S4.T3.1.6.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ChatGPT</td>
<td id="S4.T3.1.6.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">40.0</td>
<td id="S4.T3.1.6.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">39.4</td>
<td id="S4.T3.1.6.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">17.9</td>
<td id="S4.T3.1.6.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">10.5</td>
<td id="S4.T3.1.6.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.7</td>
</tr>
<tr id="S4.T3.1.7" class="ltx_tr">
<td id="S4.T3.1.7.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">GPT-4</td>
<td id="S4.T3.1.7.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">51.3</td>
<td id="S4.T3.1.7.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T3.1.7.3.1" class="ltx_text ltx_font_bold">60.0</span></td>
<td id="S4.T3.1.7.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T3.1.7.4.1" class="ltx_text ltx_font_bold">53.1</span></td>
<td id="S4.T3.1.7.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T3.1.7.5.1" class="ltx_text ltx_font_bold">32.9</span></td>
<td id="S4.T3.1.7.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.8</td>
</tr>
<tr id="S4.T3.1.8" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.T3.1.8.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="6"><span id="S4.T3.1.8.1.1" class="ltx_text ltx_font_italic" style="background-color:#E6E6E6;">Text+Image (MLLM)</span></td>
</tr>
<tr id="S4.T3.1.9" class="ltx_tr">
<td id="S4.T3.1.9.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Chinese-LLaVA</td>
<td id="S4.T3.1.9.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">34.5</td>
<td id="S4.T3.1.9.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">26.9</td>
<td id="S4.T3.1.9.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.9</td>
<td id="S4.T3.1.9.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2.4</td>
<td id="S4.T3.1.9.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8.4</td>
</tr>
<tr id="S4.T3.1.10" class="ltx_tr">
<td id="S4.T3.1.10.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">VisualGLM</td>
<td id="S4.T3.1.10.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">37.9</td>
<td id="S4.T3.1.10.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">30.2</td>
<td id="S4.T3.1.10.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">1.9</td>
<td id="S4.T3.1.10.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.7</td>
<td id="S4.T3.1.10.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.6</td>
</tr>
<tr id="S4.T3.1.11" class="ltx_tr">
<td id="S4.T3.1.11.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">VisCPM</td>
<td id="S4.T3.1.11.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">41.7</td>
<td id="S4.T3.1.11.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">27.7</td>
<td id="S4.T3.1.11.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.0</td>
<td id="S4.T3.1.11.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.8</td>
<td id="S4.T3.1.11.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T3.1.11.6.1" class="ltx_text ltx_font_bold">14.1</span></td>
</tr>
<tr id="S4.T3.1.12" class="ltx_tr">
<td id="S4.T3.1.12.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Qwen-VL</td>
<td id="S4.T3.1.12.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">49.8</td>
<td id="S4.T3.1.12.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">29.4</td>
<td id="S4.T3.1.12.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2.8</td>
<td id="S4.T3.1.12.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.8</td>
<td id="S4.T3.1.12.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">13.7</td>
</tr>
<tr id="S4.T3.1.13" class="ltx_tr">
<td id="S4.T3.1.13.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">InternVL</td>
<td id="S4.T3.1.13.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">56.4</td>
<td id="S4.T3.1.13.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">33.4</td>
<td id="S4.T3.1.13.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2.1</td>
<td id="S4.T3.1.13.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">14.2</td>
<td id="S4.T3.1.13.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">13.1</td>
</tr>
<tr id="S4.T3.1.14" class="ltx_tr">
<td id="S4.T3.1.14.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Yi-VL</td>
<td id="S4.T3.1.14.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">61.3</td>
<td id="S4.T3.1.14.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">42.0</td>
<td id="S4.T3.1.14.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">36.4</td>
<td id="S4.T3.1.14.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">14.6</td>
<td id="S4.T3.1.14.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8.9</td>
</tr>
<tr id="S4.T3.1.15" class="ltx_tr">
<td id="S4.T3.1.15.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Gemini Vision</td>
<td id="S4.T3.1.15.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">59.4</td>
<td id="S4.T3.1.15.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">54.4</td>
<td id="S4.T3.1.15.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">24.3</td>
<td id="S4.T3.1.15.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">30.5</td>
<td id="S4.T3.1.15.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12.5</td>
</tr>
<tr id="S4.T3.1.16" class="ltx_tr">
<td id="S4.T3.1.16.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">GPT-4V</td>
<td id="S4.T3.1.16.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T3.1.16.2.1" class="ltx_text ltx_font_bold">67.1</span></td>
<td id="S4.T3.1.16.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T3.1.16.3.1" class="ltx_text ltx_font_bold">70.6</span></td>
<td id="S4.T3.1.16.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T3.1.16.4.1" class="ltx_text ltx_font_bold">58.2</span></td>
<td id="S4.T3.1.16.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T3.1.16.5.1" class="ltx_text ltx_font_bold">42.4</span></td>
<td id="S4.T3.1.16.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">11.7</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance of models on each type of questions of <span id="S4.T3.3.1" class="ltx_text ltx_font_smallcaps">Multi</span>. MA Acc.: Accuracy of MA questions.</figcaption>
</figure>
</section>
<section id="S4.SS4.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Comparison by question type.</h4>

<div id="S4.SS4.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px3.p1.1" class="ltx_p">In <a href="#S4.T3" title="In Comparison by number of images. ‣ 4.4 Main Experiment Results ‣ 4 Experiments ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>, we present the performance categorized by question type. <span id="S4.SS4.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_bold">A majority of the models achieve their highest scores on the Single Answer Choosing (SA) set, with lower performance on the Multiple Answers Choosing (MA) set</span>. A notable discrepancy is observed between the scores for the MA set and its accuracy, highlighting the smaller models’ inability to identify all correct options accurately.</p>
</div>
<div id="S4.SS4.SSS0.Px3.p2" class="ltx_para">
<p id="S4.SS4.SSS0.Px3.p2.1" class="ltx_p"><span id="S4.SS4.SSS0.Px3.p2.1.1" class="ltx_text ltx_font_bold">For the Fill-in-the-Blank (FB) set</span>, which requires short but exact matches, <span id="S4.SS4.SSS0.Px3.p2.1.2" class="ltx_text ltx_font_bold">the scores further decline</span>. This is partially due to failure to follow the specified instructions, often leading to correct responses being presented in an unacceptable format.</p>
</div>
<div id="S4.SS4.SSS0.Px3.p3" class="ltx_para">
<p id="S4.SS4.SSS0.Px3.p3.1" class="ltx_p">Furthermore, <span id="S4.SS4.SSS0.Px3.p3.1.1" class="ltx_text ltx_font_bold">we note significantly lower scores on the Open-ended Writing (OP) set in comparison to the FB set</span>. VisCPM stands out but only with the best score of 14.1% on the OP set, suggesting that our dataset minimizes the risk of data leakage and poses considerable challenges for models in generation across modalities.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T4.1.1" class="ltx_tr">
<td id="S4.T4.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T4.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S4.T4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T4.1.1.2.1" class="ltx_text ltx_font_bold">JuH</span></td>
<td id="S4.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T4.1.1.3.1" class="ltx_text ltx_font_bold">SeH</span></td>
<td id="S4.T4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T4.1.1.4.1" class="ltx_text ltx_font_bold">Uni</span></td>
<td id="S4.T4.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T4.1.1.5.1" class="ltx_text ltx_font_bold">Driv</span></td>
<td id="S4.T4.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T4.1.1.6.1" class="ltx_text ltx_font_bold">AAT</span></td>
</tr>
<tr id="S4.T4.1.2" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.T4.1.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="6"><span id="S4.T4.1.2.1.1" class="ltx_text ltx_font_italic" style="background-color:#E6E6E6;">Puretext (LLM)</span></td>
</tr>
<tr id="S4.T4.1.3" class="ltx_tr">
<td id="S4.T4.1.3.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">MOSS</td>
<td id="S4.T4.1.3.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">21.2</td>
<td id="S4.T4.1.3.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">26.7</td>
<td id="S4.T4.1.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">23.8</td>
<td id="S4.T4.1.3.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">44.1</td>
<td id="S4.T4.1.3.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T4.1.3.6.1" class="ltx_text ltx_font_bold">25.5</span></td>
</tr>
<tr id="S4.T4.1.4" class="ltx_tr">
<td id="S4.T4.1.4.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">DFM-2.0</td>
<td id="S4.T4.1.4.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">42.3</td>
<td id="S4.T4.1.4.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T4.1.4.3.1" class="ltx_text ltx_font_bold">42.5</span></td>
<td id="S4.T4.1.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">35.7</td>
<td id="S4.T4.1.4.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">66.3</td>
<td id="S4.T4.1.4.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.9</td>
</tr>
<tr id="S4.T4.1.5" class="ltx_tr">
<td id="S4.T4.1.5.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Gemini</td>
<td id="S4.T4.1.5.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">47.7</td>
<td id="S4.T4.1.5.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">42.3</td>
<td id="S4.T4.1.5.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">41.4</td>
<td id="S4.T4.1.5.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">66.9</td>
<td id="S4.T4.1.5.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">22.5</td>
</tr>
<tr id="S4.T4.1.6" class="ltx_tr">
<td id="S4.T4.1.6.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ChatGPT</td>
<td id="S4.T4.1.6.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">31.6</td>
<td id="S4.T4.1.6.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">23.7</td>
<td id="S4.T4.1.6.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">34.9</td>
<td id="S4.T4.1.6.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">52.1</td>
<td id="S4.T4.1.6.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">1.3</td>
</tr>
<tr id="S4.T4.1.7" class="ltx_tr">
<td id="S4.T4.1.7.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">GPT-4</td>
<td id="S4.T4.1.7.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T4.1.7.2.1" class="ltx_text ltx_font_bold">49.2</span></td>
<td id="S4.T4.1.7.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">33.7</td>
<td id="S4.T4.1.7.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T4.1.7.4.1" class="ltx_text ltx_font_bold">55.1</span></td>
<td id="S4.T4.1.7.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T4.1.7.5.1" class="ltx_text ltx_font_bold">69.9</span></td>
<td id="S4.T4.1.7.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.9</td>
</tr>
<tr id="S4.T4.1.8" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.T4.1.8.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="6"><span id="S4.T4.1.8.1.1" class="ltx_text ltx_font_italic" style="background-color:#E6E6E6;">Text+Image (MLLM)</span></td>
</tr>
<tr id="S4.T4.1.9" class="ltx_tr">
<td id="S4.T4.1.9.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Chinese-LLaVA</td>
<td id="S4.T4.1.9.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">21.1</td>
<td id="S4.T4.1.9.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">25.4</td>
<td id="S4.T4.1.9.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">20.7</td>
<td id="S4.T4.1.9.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">35.8</td>
<td id="S4.T4.1.9.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">21.8</td>
</tr>
<tr id="S4.T4.1.10" class="ltx_tr">
<td id="S4.T4.1.10.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">VisualGLM</td>
<td id="S4.T4.1.10.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">22.2</td>
<td id="S4.T4.1.10.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">25.6</td>
<td id="S4.T4.1.10.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">23.6</td>
<td id="S4.T4.1.10.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">40.9</td>
<td id="S4.T4.1.10.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">24.9</td>
</tr>
<tr id="S4.T4.1.11" class="ltx_tr">
<td id="S4.T4.1.11.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">VisCPM</td>
<td id="S4.T4.1.11.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">25.2</td>
<td id="S4.T4.1.11.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">28.1</td>
<td id="S4.T4.1.11.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">23.0</td>
<td id="S4.T4.1.11.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">43.4</td>
<td id="S4.T4.1.11.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">23.7</td>
</tr>
<tr id="S4.T4.1.12" class="ltx_tr">
<td id="S4.T4.1.12.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Qwen-VL</td>
<td id="S4.T4.1.12.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">32.6</td>
<td id="S4.T4.1.12.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">32.9</td>
<td id="S4.T4.1.12.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">27.2</td>
<td id="S4.T4.1.12.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">49.3</td>
<td id="S4.T4.1.12.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">26.4</td>
</tr>
<tr id="S4.T4.1.13" class="ltx_tr">
<td id="S4.T4.1.13.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">InternVL</td>
<td id="S4.T4.1.13.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">39.3</td>
<td id="S4.T4.1.13.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">36.5</td>
<td id="S4.T4.1.13.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">30.6</td>
<td id="S4.T4.1.13.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">57.7</td>
<td id="S4.T4.1.13.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">24.8</td>
</tr>
<tr id="S4.T4.1.14" class="ltx_tr">
<td id="S4.T4.1.14.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Yi-VL</td>
<td id="S4.T4.1.14.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">46.6</td>
<td id="S4.T4.1.14.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">46.0</td>
<td id="S4.T4.1.14.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">45.4</td>
<td id="S4.T4.1.14.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">71.1</td>
<td id="S4.T4.1.14.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">26.5</td>
</tr>
<tr id="S4.T4.1.15" class="ltx_tr">
<td id="S4.T4.1.15.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Gemini Vision</td>
<td id="S4.T4.1.15.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">48.2</td>
<td id="S4.T4.1.15.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">45.2</td>
<td id="S4.T4.1.15.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">41.7</td>
<td id="S4.T4.1.15.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">67.4</td>
<td id="S4.T4.1.15.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T4.1.15.6.1" class="ltx_text ltx_font_bold">27.0</span></td>
</tr>
<tr id="S4.T4.1.16" class="ltx_tr">
<td id="S4.T4.1.16.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">GPT-4V</td>
<td id="S4.T4.1.16.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T4.1.16.2.1" class="ltx_text ltx_font_bold">58.5</span></td>
<td id="S4.T4.1.16.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T4.1.16.3.1" class="ltx_text ltx_font_bold">52.9</span></td>
<td id="S4.T4.1.16.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T4.1.16.4.1" class="ltx_text ltx_font_bold">59.0</span></td>
<td id="S4.T4.1.16.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T4.1.16.5.1" class="ltx_text ltx_font_bold">80.1</span></td>
<td id="S4.T4.1.16.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">26.2</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance of models on each subject of <span id="S4.T4.3.1" class="ltx_text ltx_font_smallcaps">Multi</span>. JuH: level of Junior High school, SeH: level of Senior High school, Uni: level of University, Driv: Chinese driving test, AAT: Administrative Aptitude Test.</figcaption>
</figure>
</section>
<section id="S4.SS4.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Comparison by education level and subjects.</h4>

<div id="S4.SS4.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px4.p1.1" class="ltx_p">In <a href="#S4.T4" title="In Comparison by question type. ‣ 4.4 Main Experiment Results ‣ 4 Experiments ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>, we present the performance categorized by educational levels and subjects. <span id="S4.SS4.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_bold">The performance trends for high school and university level questions remain consistent with the overall results observed</span>. For questions at the society level, we anticipate higher scores on the Driving Test. This may be caused by a larger percentage of judgmental questions (in the format of SA with two options), as well as its nature with knowledge of regulations.</p>
</div>
<div id="S4.SS4.SSS0.Px4.p2" class="ltx_para">
<p id="S4.SS4.SSS0.Px4.p2.1" class="ltx_p">Furthermore, questions from the Administrative Aptitude Test (AAT), which typically include at least one image and often examine skills on image pattern recognition (illustrated in the first two examples in the left column of <a href="#A6.F9" title="In Appendix F More Examples ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9</span></a>), tend to have scores around or below randomly choosing baseline. <span id="S4.SS4.SSS0.Px4.p2.1.1" class="ltx_text ltx_font_bold">Even the strongest competitor, GPT-4V, shows limited success, with a performance of only 27.0% on these questions</span> as detailed in the study cited in the paper <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib30" title="" class="ltx_ref">2023b</a>)</cite>. This underscores the significant challenge posed by multimodal questions. Notably, the stronger LLMs, specifically DFM-2.0 and the text-only versions of GPT perform poorly on AAT questions as expected, as they often reject answering the majority of them.</p>
</div>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Ablation Study on Image Information Gain</h3>

<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T5.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;" rowspan="2"><span id="S4.T5.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S4.T5.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;" rowspan="2"><span id="S4.T5.1.1.2.1" class="ltx_text ltx_font_bold">NI</span></td>
<td id="S4.T5.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="4"><span id="S4.T5.1.1.3.1" class="ltx_text ltx_font_bold">SI</span></td>
<td id="S4.T5.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="4"><span id="S4.T5.1.1.4.1" class="ltx_text ltx_font_bold">MI</span></td>
</tr>
<tr id="S4.T5.1.2" class="ltx_tr">
<td id="S4.T5.1.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T5.1.2.1.1" class="ltx_text ltx_font_bold">w/o. image</span></td>
<td id="S4.T5.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T5.1.2.2.1" class="ltx_text ltx_font_bold">w. caption</span></td>
<td id="S4.T5.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T5.1.2.3.1" class="ltx_text ltx_font_bold">w. ocr</span></td>
<td id="S4.T5.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T5.1.2.4.1" class="ltx_text ltx_font_bold">w. image</span></td>
<td id="S4.T5.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T5.1.2.5.1" class="ltx_text ltx_font_bold">w/o. image</span></td>
<td id="S4.T5.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T5.1.2.6.1" class="ltx_text ltx_font_bold">w. caption</span></td>
<td id="S4.T5.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T5.1.2.7.1" class="ltx_text ltx_font_bold">w. ocr</span></td>
<td id="S4.T5.1.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T5.1.2.8.1" class="ltx_text ltx_font_bold">w. image</span></td>
</tr>
<tr id="S4.T5.1.3" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.T5.1.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="10"><span id="S4.T5.1.3.1.1" class="ltx_text ltx_font_italic" style="background-color:#E6E6E6;">Puretext (LLM)</span></td>
</tr>
<tr id="S4.T5.1.4" class="ltx_tr">
<td id="S4.T5.1.4.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">MOSS</td>
<td id="S4.T5.1.4.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">36.1</td>
<td id="S4.T5.1.4.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">27.3</td>
<td id="S4.T5.1.4.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">27.3 (+0.0)</td>
<td id="S4.T5.1.4.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">27.6 (+0.3)</td>
<td id="S4.T5.1.4.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S4.T5.1.4.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T5.1.4.7.1" class="ltx_text ltx_font_bold">17.1</span></td>
<td id="S4.T5.1.4.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T5.1.4.8.1" class="ltx_text ltx_font_bold">20.7</span> (+3.6)</td>
<td id="S4.T5.1.4.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T5.1.4.9.1" class="ltx_text ltx_font_bold">19.0</span> (+1.9)</td>
<td id="S4.T5.1.4.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S4.T5.1.5" class="ltx_tr">
<td id="S4.T5.1.5.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">ChatGPT</td>
<td id="S4.T5.1.5.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">54.0</td>
<td id="S4.T5.1.5.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">6.8</td>
<td id="S4.T5.1.5.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">9.9 (+3.1)</td>
<td id="S4.T5.1.5.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">6.6 (-0.2)</td>
<td id="S4.T5.1.5.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S4.T5.1.5.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">5.1</td>
<td id="S4.T5.1.5.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">10.7 (+5.6)</td>
<td id="S4.T5.1.5.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">5.5 (+0.4)</td>
<td id="S4.T5.1.5.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S4.T5.1.6" class="ltx_tr">
<td id="S4.T5.1.6.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">DFM-2.0</td>
<td id="S4.T5.1.6.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T5.1.6.2.1" class="ltx_text ltx_font_bold">63.0</span></td>
<td id="S4.T5.1.6.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T5.1.6.3.1" class="ltx_text ltx_font_bold">28.7</span></td>
<td id="S4.T5.1.6.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T5.1.6.4.1" class="ltx_text ltx_font_bold">30.2</span> (+1.5)</td>
<td id="S4.T5.1.6.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T5.1.6.5.1" class="ltx_text ltx_font_bold">33.4</span> (+4.7)</td>
<td id="S4.T5.1.6.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S4.T5.1.6.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">11.3</td>
<td id="S4.T5.1.6.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">15.6 (+4.3)</td>
<td id="S4.T5.1.6.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">14.9 (+3.6)</td>
<td id="S4.T5.1.6.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S4.T5.1.7" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.T5.1.7.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="10"><span id="S4.T5.1.7.1.1" class="ltx_text ltx_font_italic" style="background-color:#E6E6E6;">Text+Image (MLLM)</span></td>
</tr>
<tr id="S4.T5.1.8" class="ltx_tr">
<td id="S4.T5.1.8.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Chinese-LLaVA</td>
<td id="S4.T5.1.8.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">32.3</td>
<td id="S4.T5.1.8.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">26.1</td>
<td id="S4.T5.1.8.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">26.3 (+0.2)</td>
<td id="S4.T5.1.8.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">25.5 (-0.6)</td>
<td id="S4.T5.1.8.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">22.6 (-3.5)</td>
<td id="S4.T5.1.8.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">17.6</td>
<td id="S4.T5.1.8.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">19.9 (+2.3)</td>
<td id="S4.T5.1.8.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">19.6 (+2.0)</td>
<td id="S4.T5.1.8.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">17.8 (+0.2)</td>
</tr>
<tr id="S4.T5.1.9" class="ltx_tr">
<td id="S4.T5.1.9.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">VisualGLM</td>
<td id="S4.T5.1.9.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">35.1</td>
<td id="S4.T5.1.9.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">20.8</td>
<td id="S4.T5.1.9.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">21.4 (+0.6)</td>
<td id="S4.T5.1.9.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">20.4 (-0.4)</td>
<td id="S4.T5.1.9.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">25.2 (+4.4)</td>
<td id="S4.T5.1.9.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">15.3</td>
<td id="S4.T5.1.9.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">15.1 (-0.2)</td>
<td id="S4.T5.1.9.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">14.5 (-0.8)</td>
<td id="S4.T5.1.9.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">9.7 (-5.6)</td>
</tr>
<tr id="S4.T5.1.10" class="ltx_tr">
<td id="S4.T5.1.10.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">VisCPM</td>
<td id="S4.T5.1.10.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">36.8</td>
<td id="S4.T5.1.10.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">27.1</td>
<td id="S4.T5.1.10.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">27.6 (+0.5)</td>
<td id="S4.T5.1.10.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">27.2 (+0.1)</td>
<td id="S4.T5.1.10.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">28.4 (+1.3)</td>
<td id="S4.T5.1.10.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">24.8</td>
<td id="S4.T5.1.10.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">21.6 (-3.2)</td>
<td id="S4.T5.1.10.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">20.9 (-3.9)</td>
<td id="S4.T5.1.10.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">16.6 (-8.2)</td>
</tr>
<tr id="S4.T5.1.11" class="ltx_tr">
<td id="S4.T5.1.11.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Qwen-VL</td>
<td id="S4.T5.1.11.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">43.2</td>
<td id="S4.T5.1.11.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">30.7</td>
<td id="S4.T5.1.11.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">30.3 (-0.4)</td>
<td id="S4.T5.1.11.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">31.0 (+0.3)</td>
<td id="S4.T5.1.11.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">32.7 (+2.0)</td>
<td id="S4.T5.1.11.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T5.1.11.7.1" class="ltx_text ltx_font_bold">25.5</span></td>
<td id="S4.T5.1.11.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">25.0 (-0.5)</td>
<td id="S4.T5.1.11.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T5.1.11.9.1" class="ltx_text ltx_font_bold">26.2</span> (+0.7)</td>
<td id="S4.T5.1.11.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">20.7 (-4.8)</td>
</tr>
<tr id="S4.T5.1.12" class="ltx_tr">
<td id="S4.T5.1.12.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">InternVL</td>
<td id="S4.T5.1.12.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">50.9</td>
<td id="S4.T5.1.12.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">33.4</td>
<td id="S4.T5.1.12.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">33.3 (-0.1)</td>
<td id="S4.T5.1.12.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">33.1 (-0.3)</td>
<td id="S4.T5.1.12.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">35.5 (+2.1)</td>
<td id="S4.T5.1.12.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">24.8</td>
<td id="S4.T5.1.12.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">21.9 (-2.9)</td>
<td id="S4.T5.1.12.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">22.9 (-1.9)</td>
<td id="S4.T5.1.12.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">25.1 (+0.3)</td>
</tr>
<tr id="S4.T5.1.13" class="ltx_tr">
<td id="S4.T5.1.13.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Gemini/Vision</td>
<td id="S4.T5.1.13.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">62.5</td>
<td id="S4.T5.1.13.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">36.2</td>
<td id="S4.T5.1.13.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">36.9 (+0.7)</td>
<td id="S4.T5.1.13.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">38.4 (+2.2)</td>
<td id="S4.T5.1.13.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">40.0 (+3.8)</td>
<td id="S4.T5.1.13.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">18.3</td>
<td id="S4.T5.1.13.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">23.2 (+4.9)</td>
<td id="S4.T5.1.13.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">18.6 (+0.3)</td>
<td id="S4.T5.1.13.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">24.5 (+6.2)</td>
</tr>
<tr id="S4.T5.1.14" class="ltx_tr">
<td id="S4.T5.1.14.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Yi-VL</td>
<td id="S4.T5.1.14.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">63.8</td>
<td id="S4.T5.1.14.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T5.1.14.3.1" class="ltx_text ltx_font_bold">39.9</span></td>
<td id="S4.T5.1.14.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T5.1.14.4.1" class="ltx_text ltx_font_bold">38.7</span> (-1.2)</td>
<td id="S4.T5.1.14.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T5.1.14.5.1" class="ltx_text ltx_font_bold">39.4</span> (-0.5)</td>
<td id="S4.T5.1.14.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">42.0 (+2.1)</td>
<td id="S4.T5.1.14.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">24.1</td>
<td id="S4.T5.1.14.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T5.1.14.8.1" class="ltx_text ltx_font_bold">26.5</span> (+2.4)</td>
<td id="S4.T5.1.14.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">24.2(+0.1)</td>
<td id="S4.T5.1.14.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">24.5 (+0.4)</td>
</tr>
<tr id="S4.T5.1.15" class="ltx_tr">
<td id="S4.T5.1.15.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">GPT-4/V</td>
<td id="S4.T5.1.15.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T5.1.15.2.1" class="ltx_text ltx_font_bold">74.5</span></td>
<td id="S4.T5.1.15.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">11.3</td>
<td id="S4.T5.1.15.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">9.7 (-1.6)</td>
<td id="S4.T5.1.15.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">1.9 (-9.4)</td>
<td id="S4.T5.1.15.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T5.1.15.6.1" class="ltx_text ltx_font_bold">46.9</span> (+35.6)</td>
<td id="S4.T5.1.15.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">8.8</td>
<td id="S4.T5.1.15.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">9.4 (+0.6)</td>
<td id="S4.T5.1.15.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">3.1 (-5.7)</td>
<td id="S4.T5.1.15.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S4.T5.1.15.10.1" class="ltx_text ltx_font_bold">28.1</span> (+19.3)</td>
</tr>
<tr id="S4.T5.1.16" class="ltx_tr">
<td id="S4.T5.1.16.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">average</td>
<td id="S4.T5.1.16.2" class="ltx_td ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S4.T5.1.16.3" class="ltx_td ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S4.T5.1.16.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">+0.30</td>
<td id="S4.T5.1.16.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-0.35</td>
<td id="S4.T5.1.16.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">+5.98</td>
<td id="S4.T5.1.16.7" class="ltx_td ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S4.T5.1.16.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">+1.54</td>
<td id="S4.T5.1.16.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-0.30</td>
<td id="S4.T5.1.16.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">+0.98</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Performance of models evaluated on the image set of <span id="S4.T5.3.1" class="ltx_text ltx_font_smallcaps">Multi</span>.</figcaption>
</figure>
<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">To assess the necessity of images in <span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_smallcaps">Multi</span> for solving problems, we conduct an ablation study where we either remove images from the SI and MI sets or substitute them with textual descriptions, such as captions and OCR-derived text. We utilize BLIP2 <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib16" title="" class="ltx_ref">2023c</a>)</cite> for generating image captions and EasyOCR<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://pypi.org/project/easyocr/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pypi.org/project/easyocr/</a></span></span></span> to extract text from images. The results are shown in <a href="#S4.T5" title="In 4.5 Ablation Study on Image Information Gain ‣ 4 Experiments ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">For questions that incorporate a single image (as indicated in the SI column), the presence of images significantly aids in answering the questions, with an average performance boost of 5.98%. Notably, GPT-4V experiences a substantial increase of 35.6% in performance, primarily due to its tendency to abstain from answering in the absence of images.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">In settings where images are omitted and replaced by their textual descriptions (captions or OCR text), there’s a marginal improvement of 0.30% observed with captions, but a minor reduction of -0.35% with OCR text. Captions, which generally summarize the images, introduce bilingual elements to the models and usually miss details. OCR text, while detailed, lacks spatial information and is not universally applicable, as some images contain no text at all. Both forms of textual information lower the models’ refusal rate, and LLMs benefit more from these than MLLMs. However, they potentially complicate reasoning processes. Nevertheless, a generic caption is found to be more beneficial than scattered OCR fragments.</p>
</div>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.1" class="ltx_p">For questions that involve multiple images (as discussed in the MI column), we categorize models into three groups: 1) Close-source models, specifically GPT-4V and Gemini Pro, which leverage all images and achieve significant improvement. 2) Open-source models capable of handling multiple images within a dialogue or at a time, namely VisualGLM, VisCPM, and Qwen-VL, all of which exhibit a notable performance decline. 3) Open-source models without multi-image support, like Chinese-LLaVA, InternVL, and Yi-VL, show slight improvements. The second group’s decline could be attributed to their inability to utilize conversation history effectively and remember previously seen images. The third group’s limitation likely stems from providing only the first image, insufficient for comprehending all necessary information to answer the question, but to some degree avoiding distraction.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Evaluation on <span id="S4.SS6.1.1" class="ltx_text ltx_font_smallcaps">Multi-Elite</span>
</h3>

<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T6.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="S4.T6.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S4.T6.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="S4.T6.1.1.2.1" class="ltx_text ltx_font_bold">Overall</span></td>
<td id="S4.T6.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="S4.T6.1.1.3.1" class="ltx_text ltx_font_bold">SA</span></td>
<td id="S4.T6.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="S4.T6.1.1.4.1" class="ltx_text ltx_font_bold">MA</span></td>
<td id="S4.T6.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;">
<span id="S4.T6.1.1.5.1" class="ltx_text"></span> <span id="S4.T6.1.1.5.2" class="ltx_text">
<span id="S4.T6.1.1.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T6.1.1.5.2.1.1" class="ltx_tr">
<span id="S4.T6.1.1.5.2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="S4.T6.1.1.5.2.1.1.1.1" class="ltx_text ltx_font_bold">MA</span></span></span>
<span id="S4.T6.1.1.5.2.1.2" class="ltx_tr">
<span id="S4.T6.1.1.5.2.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="S4.T6.1.1.5.2.1.2.1.1" class="ltx_text ltx_font_bold">Acc.</span></span></span>
</span></span><span id="S4.T6.1.1.5.3" class="ltx_text"></span></td>
<td id="S4.T6.1.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="S4.T6.1.1.6.1" class="ltx_text ltx_font_bold">FB</span></td>
<td id="S4.T6.1.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="S4.T6.1.1.7.1" class="ltx_text ltx_font_bold">NI</span></td>
<td id="S4.T6.1.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="S4.T6.1.1.8.1" class="ltx_text ltx_font_bold">SI</span></td>
<td id="S4.T6.1.1.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="S4.T6.1.1.9.1" class="ltx_text ltx_font_bold">MI</span></td>
</tr>
<tr id="S4.T6.1.2" class="ltx_tr">
<td id="S4.T6.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">Chinese-LLaVA</td>
<td id="S4.T6.1.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">12.3</td>
<td id="S4.T6.1.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">15.7</td>
<td id="S4.T6.1.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">13.1</td>
<td id="S4.T6.1.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">1.0</td>
<td id="S4.T6.1.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">1.6</td>
<td id="S4.T6.1.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">13.7</td>
<td id="S4.T6.1.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">11.0</td>
<td id="S4.T6.1.2.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.4pt;padding-right:1.4pt;">15.3</td>
</tr>
<tr id="S4.T6.1.3" class="ltx_tr">
<td id="S4.T6.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.4pt;padding-right:1.4pt;">VisualGLM</td>
<td id="S4.T6.1.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">12.8</td>
<td id="S4.T6.1.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">14.5</td>
<td id="S4.T6.1.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">16.6</td>
<td id="S4.T6.1.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">0.0</td>
<td id="S4.T6.1.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">0.8</td>
<td id="S4.T6.1.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">16.2</td>
<td id="S4.T6.1.3.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">11.7</td>
<td id="S4.T6.1.3.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">6.8</td>
</tr>
<tr id="S4.T6.1.4" class="ltx_tr">
<td id="S4.T6.1.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.4pt;padding-right:1.4pt;">VisCPM</td>
<td id="S4.T6.1.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">13.0</td>
<td id="S4.T6.1.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">10.4</td>
<td id="S4.T6.1.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">22.0</td>
<td id="S4.T6.1.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">0.0</td>
<td id="S4.T6.1.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">0.8</td>
<td id="S4.T6.1.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">10.3</td>
<td id="S4.T6.1.4.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">14.2</td>
<td id="S4.T6.1.4.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">15.3</td>
</tr>
<tr id="S4.T6.1.5" class="ltx_tr">
<td id="S4.T6.1.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.4pt;padding-right:1.4pt;">Qwen-VL</td>
<td id="S4.T6.1.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">10.5</td>
<td id="S4.T6.1.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">7.2</td>
<td id="S4.T6.1.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">19.3</td>
<td id="S4.T6.1.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">1.9</td>
<td id="S4.T6.1.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">0.8</td>
<td id="S4.T6.1.5.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">8.5</td>
<td id="S4.T6.1.5.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">10.8</td>
<td id="S4.T6.1.5.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">16.9</td>
</tr>
<tr id="S4.T6.1.6" class="ltx_tr">
<td id="S4.T6.1.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.4pt;padding-right:1.4pt;">InternVL</td>
<td id="S4.T6.1.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">20.7</td>
<td id="S4.T6.1.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">24.8</td>
<td id="S4.T6.1.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">23.2</td>
<td id="S4.T6.1.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">0.0</td>
<td id="S4.T6.1.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">4.8</td>
<td id="S4.T6.1.6.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">17.9</td>
<td id="S4.T6.1.6.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">21.0</td>
<td id="S4.T6.1.6.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">28.8</td>
</tr>
<tr id="S4.T6.1.7" class="ltx_tr">
<td id="S4.T6.1.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.4pt;padding-right:1.4pt;">Yi-VL</td>
<td id="S4.T6.1.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="S4.T6.1.7.2.1" class="ltx_text ltx_font_bold">26.2</span></td>
<td id="S4.T6.1.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="S4.T6.1.7.3.1" class="ltx_text ltx_font_bold">33.0</span></td>
<td id="S4.T6.1.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="S4.T6.1.7.4.1" class="ltx_text ltx_font_bold">29.0</span></td>
<td id="S4.T6.1.7.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">8.7</td>
<td id="S4.T6.1.7.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">3.2</td>
<td id="S4.T6.1.7.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="S4.T6.1.7.7.1" class="ltx_text ltx_font_bold">32.5</span></td>
<td id="S4.T6.1.7.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="S4.T6.1.7.8.1" class="ltx_text ltx_font_bold">22.7</span></td>
<td id="S4.T6.1.7.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">25.4</td>
</tr>
<tr id="S4.T6.1.8" class="ltx_tr">
<td id="S4.T6.1.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.4pt;padding-right:1.4pt;">Gemini Vision</td>
<td id="S4.T6.1.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">12.4</td>
<td id="S4.T6.1.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">5.3</td>
<td id="S4.T6.1.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">21.2</td>
<td id="S4.T6.1.8.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">5.8</td>
<td id="S4.T6.1.8.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="S4.T6.1.8.6.1" class="ltx_text ltx_font_bold">12.0</span></td>
<td id="S4.T6.1.8.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">6.8</td>
<td id="S4.T6.1.8.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;">12.0</td>
<td id="S4.T6.1.8.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="S4.T6.1.8.9.1" class="ltx_text ltx_font_bold">37.3</span></td>
</tr>
<tr id="S4.T6.1.9" class="ltx_tr">
<td id="S4.T6.1.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">GPT-4V</td>
<td id="S4.T6.1.9.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">14.0</td>
<td id="S4.T6.1.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">5.3</td>
<td id="S4.T6.1.9.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">25.5</td>
<td id="S4.T6.1.9.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="S4.T6.1.9.5.1" class="ltx_text ltx_font_bold">15.4</span></td>
<td id="S4.T6.1.9.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;"><span id="S4.T6.1.9.6.1" class="ltx_text ltx_font_bold">12.0</span></td>
<td id="S4.T6.1.9.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">7.3</td>
<td id="S4.T6.1.9.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">14.9</td>
<td id="S4.T6.1.9.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.4pt;padding-right:1.4pt;">33.9</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Performance of models on <span id="S4.T6.3.1" class="ltx_text ltx_font_smallcaps">Multi-Elite</span>.</figcaption>
</figure>
<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">We conduct evaluations on <span id="S4.SS6.p1.1.1" class="ltx_text ltx_font_smallcaps">Multi-Elite</span>, as outlined in <a href="#S4.T6" title="In 4.6 Evaluation on Multi-Elite ‣ 4 Experiments ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">6</span></a>, which includes 500 specifically chosen questions. These questions are selected based on pre-annotated quality and difficulty scores, in addition to the evaluation results on <span id="S4.SS6.p1.1.2" class="ltx_text ltx_font_smallcaps">Multi</span> discussed in <a href="#S4.SS4" title="4.4 Main Experiment Results ‣ 4 Experiments ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">4.4</span></a>. The selection aims to ensure a distribution that mirrors <span id="S4.SS6.p1.1.3" class="ltx_text ltx_font_smallcaps">Multi</span>’s but also brings challenge to strong MLLMs. Yi-VL achieves the highest score on <span id="S4.SS6.p1.1.4" class="ltx_text ltx_font_smallcaps">Multi-Elite</span> with 26.2%, while scores for other models vary between 10.5% and 20.7%. This highlights the substantial challenge presented by <span id="S4.SS6.p1.1.5" class="ltx_text ltx_font_smallcaps">Multi-Elite</span>, indicating significant potential for improvement in tackling extremely difficult questions that require in-depth image understanding and intricate reasoning across modalities. It is important to highlight the accuracy of multiple answers choosing (MA Acc.) as the most demanding task for MLLMs, necessitating a thorough grasp of the relationships between the choices and the questions, and reflecting model reliability of selecting all answers correctly.</p>
</div>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Evaluation on <span id="S4.SS7.1.1" class="ltx_text ltx_font_smallcaps">Multi-Elite</span> with <span id="S4.SS7.2.2" class="ltx_text ltx_font_smallcaps">Multi-Extend</span>
</h3>

<figure id="S4.T7" class="ltx_table">
<table id="S4.T7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T7.1.1" class="ltx_tr">
<td id="S4.T7.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T7.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S4.T7.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T7.1.1.2.1" class="ltx_text ltx_font_bold">window size</span></td>
<td id="S4.T7.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T7.1.1.3.1" class="ltx_text ltx_font_bold">w/o. kn</span></td>
<td id="S4.T7.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T7.1.1.4.1" class="ltx_text ltx_font_bold">w. kn</span></td>
</tr>
<tr id="S4.T7.1.2" class="ltx_tr">
<td id="S4.T7.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">InternVL</td>
<td id="S4.T7.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">768 tokens</td>
<td id="S4.T7.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">20.7</td>
<td id="S4.T7.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">19.9 (-0.8)</td>
</tr>
<tr id="S4.T7.1.3" class="ltx_tr">
<td id="S4.T7.1.3.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Yi-VL</td>
<td id="S4.T7.1.3.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">4,096 tokens</td>
<td id="S4.T7.1.3.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">26.2</td>
<td id="S4.T7.1.3.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">21.4 (-4.8)</td>
</tr>
<tr id="S4.T7.1.4" class="ltx_tr">
<td id="S4.T7.1.4.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Qwen-VL</td>
<td id="S4.T7.1.4.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">8,192 tokens</td>
<td id="S4.T7.1.4.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">10.5</td>
<td id="S4.T7.1.4.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">13.0 (+2.5)</td>
</tr>
<tr id="S4.T7.1.5" class="ltx_tr">
<td id="S4.T7.1.5.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Gemini Vision</td>
<td id="S4.T7.1.5.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">30,720 tokens</td>
<td id="S4.T7.1.5.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">12.4</td>
<td id="S4.T7.1.5.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">17.0 (+4.6)</td>
</tr>
<tr id="S4.T7.1.6" class="ltx_tr">
<td id="S4.T7.1.6.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-4V</td>
<td id="S4.T7.1.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">128,000 tokens</td>
<td id="S4.T7.1.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">14.0</td>
<td id="S4.T7.1.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">21.3 (+7.3)</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Performance of models with <span id="S4.T7.4.1" class="ltx_text ltx_font_smallcaps">Multi-Extend</span> on <span id="S4.T7.5.2" class="ltx_text ltx_font_smallcaps">Multi-Elite</span>.</figcaption>
</figure>
<div id="S4.SS7.p1" class="ltx_para">
<p id="S4.SS7.p1.1" class="ltx_p">The significant challenges posed by <span id="S4.SS7.p1.1.1" class="ltx_text ltx_font_smallcaps">Multi-Elite</span> prompt further investigation into the In-Context Learning (ICL) capabilities of MLLMs through the utilization of the <span id="S4.SS7.p1.1.2" class="ltx_text ltx_font_smallcaps">Multi-Extend</span> knowledge set. This set is designed to include relevant concepts and frequently utilized solutions related to the problems. The study is conducted on several MLLMs, with the prompts for incorporating these knowledge pieces shown in <a href="#A3.F5" title="In Appendix C Prompts ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a>, and the results are listed in <a href="#S4.T7" title="In 4.7 Evaluation on Multi-Elite with Multi-Extend ‣ 4 Experiments ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7</span></a>. Notably, the average number of tokens per question escalates from 65 to 250, and further to 850, following the integration of prompts and the adoption of <span id="S4.SS7.p1.1.3" class="ltx_text ltx_font_smallcaps">Multi-Extend</span>, with the most extensive examples surpassing 10,000 tokens. <span id="S4.SS7.p1.1.4" class="ltx_text ltx_font_smallcaps">Multi-Extend</span> poses a significant challenge in terms of the necessary window size and the capacity to handle lengthy contexts. It is observed that models equipped with larger window sizes, i.e. Gemini Vision and GPT-4V, benefit more from <span id="S4.SS7.p1.1.5" class="ltx_text ltx_font_smallcaps">Multi-Extend</span>, whereas there is a notable decline in performance for MLLMs with smaller window sizes. The increase in tokens may also present a hurdle for models, as the concise question may become overshadowed by the extensive context.</p>
</div>
</section>
<section id="S4.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.8 </span>Takeaways</h3>

<div id="S4.SS8.p1" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">GPT-4V demonstrates the highest performance with a 63.7% score, indicating a significant challenge of <span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_smallcaps">Multi</span>, while Yi-VL leads among open-source models.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">MLLMs show a performance drop in questions requiring more images, with only GPT-4V exceeding a basic guessing baseline in multi-image scenarios.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">LLMs show a reverse correlation in performance between non-image and single/multiple image sets, highlighting the challenge of avoiding hallucination in visual questions.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Models generally perform better on questions requiring shorter answers, i.e. SA &gt; MA &gt; FB &gt; OP. The results of MA Acc. emphasize the importance of balancing recall and precision.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p">Performance trends are consistent across educational levels, with lower scores on AAT questions due to their multimodal complexity.</p>
</div>
</li>
<li id="S4.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i6.p1" class="ltx_para">
<p id="S4.I1.i6.p1.1" class="ltx_p">The inclusion of images significantly boosts question-answering performance, with captions offering a slight improvement and OCR text potentially complicating reasoning processes.</p>
</div>
</li>
<li id="S4.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i7.p1" class="ltx_para">
<p id="S4.I1.i7.p1.1" class="ltx_p">In the <span id="S4.I1.i7.p1.1.1" class="ltx_text ltx_font_smallcaps">Multi-Elite</span> evaluation, Yi-VL achieves the highest 26.2% score, illustrating the difficulty of <span id="S4.I1.i7.p1.1.2" class="ltx_text ltx_font_smallcaps">Multi-Elite</span> and the need for advanced image understanding and reasoning across modalities.</p>
</div>
</li>
<li id="S4.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i8.p1" class="ltx_para">
<p id="S4.I1.i8.p1.1" class="ltx_p">The aid of <span id="S4.I1.i8.p1.1.1" class="ltx_text ltx_font_smallcaps">Multi-Extend</span> help improve performance on models with long window sizes, yet it may yield adverse effects on less capable models.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we introduce <span id="S5.p1.1.1" class="ltx_text ltx_font_smallcaps">Multi</span>, a comprehensive and challenging benchmark designed to rigorously evaluate the performance of MLLMs in detailed cross-modality understanding and scientific reasoning. Our experiments with state-of-the-art models like Qwen-VL, InternVL, Yi-VL, Gemini, and GPT-4 demonstrate that while these models exhibit promising capabilities, there remains a significant gap compared to human performance, particularly in tasks involving cross-modal alignment, logical reasoning, and complex comprehension. This underscores the need for continuous research and development in this domain.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The creation of the <span id="S5.p2.1.1" class="ltx_text ltx_font_smallcaps">Multi-Elite</span> and <span id="S5.p2.1.2" class="ltx_text ltx_font_smallcaps">Multi-Extend</span> subsets further contributes to the field by providing insights into the strengths and limitations of current MLLMs. These subsets challenge the models’ learning and reasoning abilities and encourage the development of more sophisticated and robust multimodal understanding systems.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_smallcaps">Multi</span> benchmark opens new avenues for research, particularly in enhancing the MLLMs’ ability to integrate and reason over diverse data types, including images, text, and structured data. Future work may focus on expanding the benchmark to include more diverse modalities and question types, further pushing the boundaries of what MLLMs can achieve. By making <span id="S5.p3.1.2" class="ltx_text ltx_font_smallcaps">Multi</span> publicly available, we hope to foster a collaborative environment where researchers can continuously test and improve the capabilities of MLLMs, driving the field toward the development of truly intelligent and versatile AI systems.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations and Future Work</h2>

<section id="Sx1.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Multilingual Capabilities</h4>

<div id="Sx1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="Sx1.SS0.SSS0.Px1.p1.1" class="ltx_p"><span id="Sx1.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">Multi</span> predominantly features simplified Chinese and mainly focuses on subjects taught in Chinese schools, with limited English multimodal content that’s relatively straightforward for LLMs. We plan to include translations in future versions. Nonetheless, the presence of Chinese characters in figures poses a significant challenge for MLLMs trained on different linguistic datasets.</p>
</div>
</section>
<section id="Sx1.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Use of Explanations</h4>

<div id="Sx1.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="Sx1.SS0.SSS0.Px2.p1.1" class="ltx_p">While we have annotated explanations in detail, the utilization in subsequent studies remains limited. These explanations could potentially serve as valuable training data for model fine-tuning and few-shot learning using methods like CoT (Chain-of-Thoughts) or RAG (Retrieval Augmented Generation) and may aid in evaluating reasoning skills.</p>
</div>
</section>
<section id="Sx1.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Metrics for evaluating blank-filling, open-ended writing and others</h4>

<div id="Sx1.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="Sx1.SS0.SSS0.Px3.p1.1" class="ltx_p">Our evaluation primarily uses exact match, which might be overly stringent for assessing MLLMs’ true capabilities. Assessing open-ended writing tasks that require complex knowledge and reasoning is still a challenge. We also have 100 questions that do not belong to traditional categories, such as questions requiring geographic drawing, and the evaluation of them will be even more challenging. Now that only a few studies <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite> involve human evaluation, developing automatic and reliable methods remains an open research area.</p>
</div>
</section>
<section id="Sx1.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Adaptation to various MLLMs</h4>

<div id="Sx1.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="Sx1.SS0.SSS0.Px4.p1.1" class="ltx_p">Although we have tested several MLLMs, numerous others exist and new ones are continuously emerging. We encourage the community to evaluate their MLLMs using our benchmark to gauge their cognitive reasoning abilities. We will test more models as soon as the multilingual version is released.</p>
</div>
</section>
<section id="Sx1.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Expansion to more modalities and subjects</h4>

<div id="Sx1.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="Sx1.SS0.SSS0.Px5.p1.1" class="ltx_p">Our benchmark currently focuses on static images, but incorporating other modalities like audio and video, and subjects like art, music theory, medicine, and sports could present new topics. Thus, expanding our question set to cover these areas is a promising direction for future research.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">01.ai (2023)</span>
<span class="ltx_bibblock">
01.ai. 2023.

</span>
<span class="ltx_bibblock">Yi-vl.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/01-ai/Yi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/01-ai/Yi</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>, pages 2425–2433.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2023a)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023a.

</span>
<span class="ltx_bibblock">Qwen-vl: A frontier large vision-language model with versatile abilities.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.12966</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2023b)</span>
<span class="ltx_bibblock">
Shuai Bai, Shusheng Yang, Jinze Bai, Peng Wang, Xingxuan Zhang, Junyang Lin, Xinggang Wang, Chang Zhou, and Jingren Zhou. 2023b.

</span>
<span class="ltx_bibblock">Touchstone: Evaluating vision-language models by language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.16890</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. 2023.

</span>
<span class="ltx_bibblock">Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.14238</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022)</span>
<span class="ltx_bibblock">
Zhi Chen, Jijia Bao, Lu Chen, Yuncong Liu, Da Ma, Bei Chen, Mengyue Wu, Su Zhu, Jian-Guang Lou, and Kai Yu. 2022.

</span>
<span class="ltx_bibblock">Dfm: Dialogue foundation model for universal large-scale dialogue-oriented task learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.12662</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2023)</span>
<span class="ltx_bibblock">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, and Steven C. H. Hoi. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:258615266" title="" class="ltx_ref ltx_href">Instructblip: Towards general-purpose vision-language models with instruction tuning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2305.06500.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. (2022)</span>
<span class="ltx_bibblock">
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022.

</span>
<span class="ltx_bibblock">Glm: General language model pretraining with autoregressive blank infilling.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 320–335.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et al. (2023)</span>
<span class="ltx_bibblock">
Wentao Ge, Shunian Chen, Guiming Chen, Junying Chen, Zhihong Chen, Shuo Yan, Chenghao Zhu, Ziyue Lin, Wenya Xie, Xidong Wang, et al. 2023.

</span>
<span class="ltx_bibblock">Mllm-bench, evaluating multi-modal llms using gpt-4v.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.13951</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding in visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 6904–6913.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2023)</span>
<span class="ltx_bibblock">
Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.12038" title="" class="ltx_ref ltx_href">Large multilingual models pivot zero-shot multimodal learning across languages</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2023)</span>
<span class="ltx_bibblock">
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. 2023.

</span>
<span class="ltx_bibblock">C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.08322</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning (2019)</span>
<span class="ltx_bibblock">
Drew A Hudson and Christopher D Manning. 2019.

</span>
<span class="ltx_bibblock">Gqa: A new dataset for real-world visual reasoning and compositional question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 6700–6709.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023a)</span>
<span class="ltx_bibblock">
Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. 2023a.

</span>
<span class="ltx_bibblock">Seed-bench-2: Benchmarking multimodal large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.17092</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023b)</span>
<span class="ltx_bibblock">
Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023b.

</span>
<span class="ltx_bibblock">Seed-bench: Benchmarking multimodal llms with generative comprehension.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.16125</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023c)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023c.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.12597</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq R. Joty, Caiming Xiong, and Steven C. H. Hoi. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:236034189" title="" class="ltx_ref ltx_href">Align before fuse: Vision and language representation learning with momentum distillation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Neural Information Processing Systems</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Tajbakhsh (2023)</span>
<span class="ltx_bibblock">
Shengzhi Li and Nima Tajbakhsh. 2023.

</span>
<span class="ltx_bibblock">Scigraphqa: A large-scale synthetic multi-turn question-answering dataset for scientific graphs.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.03349</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023d)</span>
<span class="ltx_bibblock">
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023d.

</span>
<span class="ltx_bibblock">Evaluating object hallucination in large vision-language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.10355</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/W04-1013" title="" class="ltx_ref ltx_href">ROUGE: A package for automatic evaluation of summaries</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Text Summarization Branches Out</em>, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em>, pages 740–755. Springer.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LinkSoul-AI (2023)</span>
<span class="ltx_bibblock">
LinkSoul-AI. 2023.

</span>
<span class="ltx_bibblock">Chinese llava.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/LinkSoul-AI/Chinese-LLaVA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/LinkSoul-AI/Chinese-LLaVA</a>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. 2023a.

</span>
<span class="ltx_bibblock">Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.14566</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.08485</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023c)</span>
<span class="ltx_bibblock">
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. 2023c.

</span>
<span class="ltx_bibblock">Mmbench: Is your multi-modal model an all-around player?

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.06281</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2023)</span>
<span class="ltx_bibblock">
Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 2023.

</span>
<span class="ltx_bibblock">Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.02255</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2022)</span>
<span class="ltx_bibblock">
Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022.

</span>
<span class="ltx_bibblock">Learn to explain: Multimodal reasoning via thought chains for science question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">The 36th Conference on Neural Information Processing Systems (NeurIPS)</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI. 2022.

</span>
<span class="ltx_bibblock">Chatgpt: Optimizing language models for dialogue.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openai.com/blog/chatgpt" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/blog/chatgpt</a>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023a)</span>
<span class="ltx_bibblock">
OpenAI. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.08774" title="" class="ltx_ref ltx_href">Gpt-4 technical report</a>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023b)</span>
<span class="ltx_bibblock">
OpenAI. 2023b.

</span>
<span class="ltx_bibblock">Gpt-4v(ision) system card.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openai.com/research/gpt-4v-system-card" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/research/gpt-4v-system-card</a>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:231591445" title="" class="ltx_ref ltx_href">Learning transferable visual models from natural language supervision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2023a)</span>
<span class="ltx_bibblock">
Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. 2023a.

</span>
<span class="ltx_bibblock">Scieval: A multi-level large language model evaluation benchmark for scientific research.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.13149</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2023b)</span>
<span class="ltx_bibblock">
Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan, Xiangyang Liu, Yunfan Shao, Qiong Tang, Xingjian Zhao, Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang, and Xipeng Qiu. 2023b.

</span>
<span class="ltx_bibblock">Moss: Training conversational language models from synthetic data.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team (2023)</span>
<span class="ltx_bibblock">
Gemini Team. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2312.11805" title="" class="ltx_ref ltx_href">Gemini: A family of highly capable multimodal models</a>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. 2023.

</span>
<span class="ltx_bibblock">Scibench: Evaluating college-level scientific problem-solving abilities of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.10635</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023.

</span>
<span class="ltx_bibblock">Mm-vet: Evaluating large multimodal models for integrated capabilities.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.02490</em>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al. (2023)</span>
<span class="ltx_bibblock">
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. 2023.

</span>
<span class="ltx_bibblock">Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.16502</em>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023a)</span>
<span class="ltx_bibblock">
Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al. 2023a.

</span>
<span class="ltx_bibblock">Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.15112</em>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023b)</span>
<span class="ltx_bibblock">
Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing. 2023b.

</span>
<span class="ltx_bibblock">M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.05179</em>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al. (2023)</span>
<span class="ltx_bibblock">
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023.

</span>
<span class="ltx_bibblock">Agieval: A human-centric benchmark for evaluating foundation models.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.06364</em>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2023)</span>
<span class="ltx_bibblock">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.

</span>
<span class="ltx_bibblock">Minigpt-4: Enhancing vision-language understanding with advanced large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.10592</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Statistics</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">We providedetailed statistics in <a href="#A1.T8" title="In Appendix A Statistics ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">8</span></a>. One question may contain more than one scoring points as mentioned in <a href="#S4.SS3" title="4.3 Metrics ‣ 4 Experiments ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
<figure id="A1.T8" class="ltx_table">
<table id="A1.T8.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A1.T8.1.1" class="ltx_tr">
<td id="A1.T8.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="A1.T8.1.1.1.1" class="ltx_text ltx_font_bold">Statistics</span></td>
<td id="A1.T8.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="A1.T8.1.1.2.1" class="ltx_text ltx_font_bold">Number</span></td>
<td id="A1.T8.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="A1.T8.1.1.3.1" class="ltx_text ltx_font_bold">Points</span></td>
</tr>
<tr id="A1.T8.1.2" class="ltx_tr">
<td id="A1.T8.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Total Problems</td>
<td id="A1.T8.1.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">17251</td>
<td id="A1.T8.1.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
</tr>
<tr id="A1.T8.1.3" class="ltx_tr">
<td id="A1.T8.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;">Total Questions</td>
<td id="A1.T8.1.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">18430</td>
<td id="A1.T8.1.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
</tr>
<tr id="A1.T8.1.4" class="ltx_tr">
<td id="A1.T8.1.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;">Total Points</td>
<td id="A1.T8.1.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">23320</td>
<td id="A1.T8.1.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
</tr>
<tr id="A1.T8.1.5" class="ltx_tr">
<td id="A1.T8.1.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;">Total Images</td>
<td id="A1.T8.1.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">7658</td>
<td id="A1.T8.1.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
</tr>
<tr id="A1.T8.1.6" class="ltx_tr">
<td id="A1.T8.1.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;">Total Knowledge</td>
<td id="A1.T8.1.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">4595</td>
<td id="A1.T8.1.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
</tr>
<tr id="A1.T8.1.7" class="ltx_tr">
<td id="A1.T8.1.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Multiple Choices</td>
<td id="A1.T8.1.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">16100(87.36%)</td>
<td id="A1.T8.1.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">19904(85.35%)</td>
</tr>
<tr id="A1.T8.1.8" class="ltx_tr">
<td id="A1.T8.1.8.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;">- Single Answer</td>
<td id="A1.T8.1.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">13963(75.76%)</td>
<td id="A1.T8.1.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">13963(59.88%)</td>
</tr>
<tr id="A1.T8.1.9" class="ltx_tr">
<td id="A1.T8.1.9.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;">- Multiple Answers</td>
<td id="A1.T8.1.9.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">2137(11.60%)</td>
<td id="A1.T8.1.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">5941(25.48%)</td>
</tr>
<tr id="A1.T8.1.10" class="ltx_tr">
<td id="A1.T8.1.10.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;">Fill in the Blank</td>
<td id="A1.T8.1.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">1432(7.77%)</td>
<td id="A1.T8.1.10.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">2211(9.48%)</td>
</tr>
<tr id="A1.T8.1.11" class="ltx_tr">
<td id="A1.T8.1.11.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;">Open Ended Writing</td>
<td id="A1.T8.1.11.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">798(4.33%)</td>
<td id="A1.T8.1.11.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">1205(5.17%)</td>
</tr>
<tr id="A1.T8.1.12" class="ltx_tr">
<td id="A1.T8.1.12.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;">Others</td>
<td id="A1.T8.1.12.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">100(0.54%)</td>
<td id="A1.T8.1.12.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">-</td>
</tr>
<tr id="A1.T8.1.13" class="ltx_tr">
<td id="A1.T8.1.13.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">Question with Images</td>
<td id="A1.T8.1.13.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">7489(40.63%)</td>
<td id="A1.T8.1.13.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">9042(38.77%)</td>
</tr>
<tr id="A1.T8.1.14" class="ltx_tr">
<td id="A1.T8.1.14.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;">- Single Image</td>
<td id="A1.T8.1.14.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">7265(39.42%)</td>
<td id="A1.T8.1.14.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">8767(37.59%)</td>
</tr>
<tr id="A1.T8.1.15" class="ltx_tr">
<td id="A1.T8.1.15.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;">- Choices within Image</td>
<td id="A1.T8.1.15.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">1179(6.40%)</td>
<td id="A1.T8.1.15.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">1181(5.06%)</td>
</tr>
<tr id="A1.T8.1.16" class="ltx_tr">
<td id="A1.T8.1.16.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;">- Multiple Images</td>
<td id="A1.T8.1.16.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">224(1.22%)</td>
<td id="A1.T8.1.16.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">275(1.18%)</td>
</tr>
<tr id="A1.T8.1.17" class="ltx_tr">
<td id="A1.T8.1.17.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding-left:1.0pt;padding-right:1.0pt;">Question with Explanations</td>
<td id="A1.T8.1.17.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">10565(57.33%)</td>
<td id="A1.T8.1.17.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;">13186(56.54%)</td>
</tr>
<tr id="A1.T8.1.18" class="ltx_tr">
<td id="A1.T8.1.18.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" style="padding-left:1.0pt;padding-right:1.0pt;">Question with Knowledge</td>
<td id="A1.T8.1.18.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.0pt;padding-right:1.0pt;">9048(49.09%)</td>
<td id="A1.T8.1.18.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.0pt;padding-right:1.0pt;">12919(55.40%)</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>The statistic overview of <span id="A1.T8.3.1" class="ltx_text ltx_font_smallcaps">Multi</span>.</figcaption>
</figure>
<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Data Distribution on Question Types</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">Our benchmark showcases a remarkable diversity in the choice setting of multiple-choice questions, encompassing options that range from 2 to as many as 13. Furthermore, it includes questions that vary in the number of correct answers, from questions with a unique correct option to those with multiple correct choices. We provide the distribution of choices in multiple-choice questions as shown in <a href="#A1.T9" title="In A.1 Data Distribution on Question Types ‣ Appendix A Statistics ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">9</span></a>. Each row corresponds to a different total number of options available in the questions. The columns represent the frequency of each specific choice option. The table showcases a well-balanced distribution of choices. Notably, the distribution reveals a higher frequency of questions with four choices and a single correct answer, indicating a common format in multiple-choice questions.</p>
</div>
<figure id="A1.T9" class="ltx_table">
<table id="A1.T9.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A1.T9.1.1" class="ltx_tr">
<td id="A1.T9.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A1.T9.1.1.1.1" class="ltx_text ltx_font_bold">Type</span></td>
<td id="A1.T9.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A1.T9.1.1.2.1" class="ltx_text ltx_font_bold"># choices</span></td>
<td id="A1.T9.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A1.T9.1.1.3.1" class="ltx_text ltx_font_bold"># A</span></td>
<td id="A1.T9.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A1.T9.1.1.4.1" class="ltx_text ltx_font_bold"># B</span></td>
<td id="A1.T9.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A1.T9.1.1.5.1" class="ltx_text ltx_font_bold"># C</span></td>
<td id="A1.T9.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A1.T9.1.1.6.1" class="ltx_text ltx_font_bold"># D</span></td>
<td id="A1.T9.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A1.T9.1.1.7.1" class="ltx_text ltx_font_bold"># E,F,G…</span></td>
</tr>
<tr id="A1.T9.1.2" class="ltx_tr">
<td id="A1.T9.1.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="4"><span id="A1.T9.1.2.1.1" class="ltx_text">SA</span></td>
<td id="A1.T9.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">2</td>
<td id="A1.T9.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">1819</td>
<td id="A1.T9.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">1376</td>
<td id="A1.T9.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
<td id="A1.T9.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
<td id="A1.T9.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
</tr>
<tr id="A1.T9.1.3" class="ltx_tr">
<td id="A1.T9.1.3.1" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">3</td>
<td id="A1.T9.1.3.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">272</td>
<td id="A1.T9.1.3.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">287</td>
<td id="A1.T9.1.3.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">262</td>
<td id="A1.T9.1.3.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
<td id="A1.T9.1.3.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
</tr>
<tr id="A1.T9.1.4" class="ltx_tr">
<td id="A1.T9.1.4.1" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">4</td>
<td id="A1.T9.1.4.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2193</td>
<td id="A1.T9.1.4.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2638</td>
<td id="A1.T9.1.4.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2708</td>
<td id="A1.T9.1.4.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2379</td>
<td id="A1.T9.1.4.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
</tr>
<tr id="A1.T9.1.5" class="ltx_tr">
<td id="A1.T9.1.5.1" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">5</td>
<td id="A1.T9.1.5.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
<td id="A1.T9.1.5.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2</td>
<td id="A1.T9.1.5.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">7</td>
<td id="A1.T9.1.5.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">9</td>
<td id="A1.T9.1.5.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
</tr>
<tr id="A1.T9.1.6" class="ltx_tr">
<td id="A1.T9.1.6.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">MA</td>
<td id="A1.T9.1.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">3-13</td>
<td id="A1.T9.1.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">1467</td>
<td id="A1.T9.1.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">1568</td>
<td id="A1.T9.1.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">1510</td>
<td id="A1.T9.1.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">1303</td>
<td id="A1.T9.1.6.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">91</td>
</tr>
<tr id="A1.T9.1.7" class="ltx_tr">
<td id="A1.T9.1.7.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Total</td>
<td id="A1.T9.1.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">2-13</td>
<td id="A1.T9.1.7.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">5751</td>
<td id="A1.T9.1.7.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">5871</td>
<td id="A1.T9.1.7.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">4487</td>
<td id="A1.T9.1.7.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">3691</td>
<td id="A1.T9.1.7.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">91</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>The choice distribution for multiple-choice questions.</figcaption>
</figure>
<div id="A1.SS1.p2" class="ltx_para">
<p id="A1.SS1.p2.1" class="ltx_p">In addition to multiple-choice questions, our benchmark also includes a substantial number of fill-in-the-blank and open-ended questions, creating a diverse and comprehensive range of testing scenarios. Moreover, we have incorporated unique open-response questions that require creative answers, such as drawings. It is important to note that these open-response questions are not included in our formal evaluation and scoring procedures; they are primarily proposed for qualitative research and development in the field of MLLMs. Our benchmark is carefully designed to thoroughly assess and enhance the ability of MLLMs to process and respond to various question types, resembling real-world scenarios.</p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Models</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">The model specifications are listed in <a href="#A2.T10" title="In Appendix B Models ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<figure id="A2.T10" class="ltx_table">
<table id="A2.T10.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A2.T10.1.1" class="ltx_tr">
<td id="A2.T10.1.1.1" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.1.1.1" class="ltx_text ltx_font_bold">Creator</span></td>
<td id="A2.T10.1.1.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.1.2.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="A2.T10.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.1.3.1" class="ltx_text ltx_font_bold"># Paras</span></td>
<td id="A2.T10.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.1.4.1" class="ltx_text ltx_font_bold">Form</span></td>
<td id="A2.T10.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.1.5.1" class="ltx_text ltx_font_bold">Modality</span></td>
<td id="A2.T10.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.1.6.1" class="ltx_text ltx_font_bold">Lang</span></td>
<td id="A2.T10.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.1.7.1" class="ltx_text ltx_font_bold">Version</span></td>
</tr>
<tr id="A2.T10.1.2" class="ltx_tr">
<td id="A2.T10.1.2.1" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">FDU</td>
<td id="A2.T10.1.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">MOSS <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib33" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="A2.T10.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">16B</td>
<td id="A2.T10.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Weight</td>
<td id="A2.T10.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">T</td>
<td id="A2.T10.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">zh, en</td>
<td id="A2.T10.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.2.7.1" class="ltx_text ltx_font_typewriter">moss-moon-003-sft</span></td>
</tr>
<tr id="A2.T10.1.3" class="ltx_tr">
<td id="A2.T10.1.3.1" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">SJTU&amp;AISpeech</td>
<td id="A2.T10.1.3.2" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">DFM-2.0 <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="A2.T10.1.3.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">70B</td>
<td id="A2.T10.1.3.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Weight</td>
<td id="A2.T10.1.3.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">T</td>
<td id="A2.T10.1.3.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">zh, en</td>
<td id="A2.T10.1.3.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.3.7.1" class="ltx_text ltx_font_typewriter">dfm-2.0-70b-preview</span></td>
</tr>
<tr id="A2.T10.1.4" class="ltx_tr">
<td id="A2.T10.1.4.1" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">LinkSoul-AI</td>
<td id="A2.T10.1.4.2" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Chinese-LLaVA <cite class="ltx_cite ltx_citemacro_cite">LinkSoul-AI (<a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="A2.T10.1.4.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td id="A2.T10.1.4.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Weight</td>
<td id="A2.T10.1.4.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">One</td>
<td id="A2.T10.1.4.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">zh, en</td>
<td id="A2.T10.1.4.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.4.7.1" class="ltx_text ltx_font_typewriter">Chinese-LLaVA-Cllama2</span></td>
</tr>
<tr id="A2.T10.1.5" class="ltx_tr">
<td id="A2.T10.1.5.1" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">THU</td>
<td id="A2.T10.1.5.2" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">VisualGLM <cite class="ltx_cite ltx_citemacro_cite">Du et al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="A2.T10.1.5.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">6B</td>
<td id="A2.T10.1.5.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Weight</td>
<td id="A2.T10.1.5.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">SI</td>
<td id="A2.T10.1.5.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">zh, en</td>
<td id="A2.T10.1.5.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.5.7.1" class="ltx_text ltx_font_typewriter">visualglm-6b</span></td>
</tr>
<tr id="A2.T10.1.6" class="ltx_tr">
<td id="A2.T10.1.6.1" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">ModelBest</td>
<td id="A2.T10.1.6.2" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">VisCPM <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="A2.T10.1.6.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">10B</td>
<td id="A2.T10.1.6.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Weight</td>
<td id="A2.T10.1.6.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.6.5.1" class="ltx_text ltx_framed ltx_framed_underline">SI</span></td>
<td id="A2.T10.1.6.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">zh, en</td>
<td id="A2.T10.1.6.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.6.7.1" class="ltx_text ltx_font_typewriter">VisCPM-Chat</span></td>
</tr>
<tr id="A2.T10.1.7" class="ltx_tr">
<td id="A2.T10.1.7.1" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">Alibaba</td>
<td id="A2.T10.1.7.2" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Qwen-VL <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a href="#bib.bib3" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="A2.T10.1.7.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td id="A2.T10.1.7.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Weight</td>
<td id="A2.T10.1.7.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">MI</td>
<td id="A2.T10.1.7.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">zh, en</td>
<td id="A2.T10.1.7.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.7.7.1" class="ltx_text ltx_font_typewriter">Qwen-VL-Chat</span></td>
</tr>
<tr id="A2.T10.1.8" class="ltx_tr">
<td id="A2.T10.1.8.1" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">OpenGVLab</td>
<td id="A2.T10.1.8.2" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">InternVL <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="A2.T10.1.8.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">19B</td>
<td id="A2.T10.1.8.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Weight</td>
<td id="A2.T10.1.8.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.8.5.1" class="ltx_text ltx_framed ltx_framed_underline">One</span></td>
<td id="A2.T10.1.8.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">zh, en</td>
<td id="A2.T10.1.8.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.8.7.1" class="ltx_text ltx_font_typewriter">InternVL-Chat-Chinese-V1.1</span></td>
</tr>
<tr id="A2.T10.1.9" class="ltx_tr">
<td id="A2.T10.1.9.1" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">01-ai</td>
<td id="A2.T10.1.9.2" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Yi-VL <cite class="ltx_cite ltx_citemacro_cite">01.ai (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="A2.T10.1.9.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">34B</td>
<td id="A2.T10.1.9.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Weight</td>
<td id="A2.T10.1.9.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.9.5.1" class="ltx_text ltx_framed ltx_framed_underline">One</span></td>
<td id="A2.T10.1.9.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">zh, en</td>
<td id="A2.T10.1.9.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.9.7.1" class="ltx_text ltx_font_typewriter">Yi-34B-Chat</span></td>
</tr>
<tr id="A2.T10.1.10" class="ltx_tr">
<td id="A2.T10.1.10.1" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">Google</td>
<td id="A2.T10.1.10.2" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Gemini <cite class="ltx_cite ltx_citemacro_cite">Team (<a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="A2.T10.1.10.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="A2.T10.1.10.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">API</td>
<td id="A2.T10.1.10.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">T</td>
<td id="A2.T10.1.10.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">ML</td>
<td id="A2.T10.1.10.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.10.7.1" class="ltx_text ltx_font_typewriter">gemini-pro</span></td>
</tr>
<tr id="A2.T10.1.11" class="ltx_tr">
<td id="A2.T10.1.11.1" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">Google</td>
<td id="A2.T10.1.11.2" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Gemini Vision <cite class="ltx_cite ltx_citemacro_cite">Team (<a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="A2.T10.1.11.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="A2.T10.1.11.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">API</td>
<td id="A2.T10.1.11.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">MI</td>
<td id="A2.T10.1.11.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">ML</td>
<td id="A2.T10.1.11.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.11.7.1" class="ltx_text ltx_font_typewriter">gemini-pro-vision</span></td>
</tr>
<tr id="A2.T10.1.12" class="ltx_tr">
<td id="A2.T10.1.12.1" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">OpenAI</td>
<td id="A2.T10.1.12.2" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">ChatGPT <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="A2.T10.1.12.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="A2.T10.1.12.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">API</td>
<td id="A2.T10.1.12.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">T</td>
<td id="A2.T10.1.12.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">ML</td>
<td id="A2.T10.1.12.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.12.7.1" class="ltx_text ltx_font_typewriter">gpt-3.5-turbo-1106</span></td>
</tr>
<tr id="A2.T10.1.13" class="ltx_tr">
<td id="A2.T10.1.13.1" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">OpenAI</td>
<td id="A2.T10.1.13.2" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">GPT-4 <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib29" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="A2.T10.1.13.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="A2.T10.1.13.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">API</td>
<td id="A2.T10.1.13.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">T</td>
<td id="A2.T10.1.13.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">ML</td>
<td id="A2.T10.1.13.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.13.7.1" class="ltx_text ltx_font_typewriter">gpt-4-1106-preview</span></td>
</tr>
<tr id="A2.T10.1.14" class="ltx_tr">
<td id="A2.T10.1.14.1" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">OpenAI</td>
<td id="A2.T10.1.14.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">GPT-4V <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib30" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="A2.T10.1.14.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="A2.T10.1.14.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">API</td>
<td id="A2.T10.1.14.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">MI</td>
<td id="A2.T10.1.14.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">ML</td>
<td id="A2.T10.1.14.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="A2.T10.1.14.7.1" class="ltx_text ltx_font_typewriter">gpt-4-vision-preview</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>The list of models evaluated on <span id="A2.T10.4.1" class="ltx_text ltx_font_smallcaps">Multi</span>. We report Modality as how many images can the model take in one turn. Note that those MLLMs commonly support multiple-image input with chatting in several turns. W: accessible through weight. T: pure text LLM, One: only one image in the beginning, SI: single image in each turn, MI: multiple images in one turn. The <span id="A2.T10.5.2" class="ltx_text ltx_framed ltx_framed_underline">underline</span> means the model must have an image as input. ML: Multi-lingual. </figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Prompts</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">The complete collection of prompts designed for evaluation on <span id="A3.p1.1.1" class="ltx_text ltx_font_smallcaps">Multi</span> is shown in <a href="#A3.F5" title="In Appendix C Prompts ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a>. One of the prompt pieces in each row is selected according to the evaluation setting and data format. Please note that some prompts will not take effect under certain cases, for instance, the prompt related to knowledge will be omitted if the knowledge is not given.</p>
</div>
<figure id="A3.F5" class="ltx_figure"><img src="/html/2402.03173/assets/x5.png" id="A3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="298" height="221" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The prompts for evaluation on <span id="A3.F5.2.1" class="ltx_text ltx_font_smallcaps">Multi</span>.</figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Data Selection Algorithm</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">We mostly pick questions based on its content length <math id="A4.p1.1.m1.1" class="ltx_Math" alttext="L_{q}" display="inline"><semantics id="A4.p1.1.m1.1a"><msub id="A4.p1.1.m1.1.1" xref="A4.p1.1.m1.1.1.cmml"><mi id="A4.p1.1.m1.1.1.2" xref="A4.p1.1.m1.1.1.2.cmml">L</mi><mi id="A4.p1.1.m1.1.1.3" xref="A4.p1.1.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="A4.p1.1.m1.1b"><apply id="A4.p1.1.m1.1.1.cmml" xref="A4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A4.p1.1.m1.1.1.1.cmml" xref="A4.p1.1.m1.1.1">subscript</csymbol><ci id="A4.p1.1.m1.1.1.2.cmml" xref="A4.p1.1.m1.1.1.2">𝐿</ci><ci id="A4.p1.1.m1.1.1.3.cmml" xref="A4.p1.1.m1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.1.m1.1c">L_{q}</annotation></semantics></math>, calculated with function</p>
</div>
<div id="A4.p2" class="ltx_para">
<table id="A4.Ex1" class="ltx_equationgroup ltx_eqn_table">

<tbody id="A4.Ex1X"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A4.Ex1X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle L_{q}=" display="inline"><semantics id="A4.Ex1X.2.1.1.m1.1a"><mrow id="A4.Ex1X.2.1.1.m1.1.1" xref="A4.Ex1X.2.1.1.m1.1.1.cmml"><msub id="A4.Ex1X.2.1.1.m1.1.1.2" xref="A4.Ex1X.2.1.1.m1.1.1.2.cmml"><mi id="A4.Ex1X.2.1.1.m1.1.1.2.2" xref="A4.Ex1X.2.1.1.m1.1.1.2.2.cmml">L</mi><mi id="A4.Ex1X.2.1.1.m1.1.1.2.3" xref="A4.Ex1X.2.1.1.m1.1.1.2.3.cmml">q</mi></msub><mo id="A4.Ex1X.2.1.1.m1.1.1.1" xref="A4.Ex1X.2.1.1.m1.1.1.1.cmml">=</mo><mi id="A4.Ex1X.2.1.1.m1.1.1.3" xref="A4.Ex1X.2.1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="A4.Ex1X.2.1.1.m1.1b"><apply id="A4.Ex1X.2.1.1.m1.1.1.cmml" xref="A4.Ex1X.2.1.1.m1.1.1"><eq id="A4.Ex1X.2.1.1.m1.1.1.1.cmml" xref="A4.Ex1X.2.1.1.m1.1.1.1"></eq><apply id="A4.Ex1X.2.1.1.m1.1.1.2.cmml" xref="A4.Ex1X.2.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="A4.Ex1X.2.1.1.m1.1.1.2.1.cmml" xref="A4.Ex1X.2.1.1.m1.1.1.2">subscript</csymbol><ci id="A4.Ex1X.2.1.1.m1.1.1.2.2.cmml" xref="A4.Ex1X.2.1.1.m1.1.1.2.2">𝐿</ci><ci id="A4.Ex1X.2.1.1.m1.1.1.2.3.cmml" xref="A4.Ex1X.2.1.1.m1.1.1.2.3">𝑞</ci></apply><csymbol cd="latexml" id="A4.Ex1X.2.1.1.m1.1.1.3.cmml" xref="A4.Ex1X.2.1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.Ex1X.2.1.1.m1.1c">\displaystyle L_{q}=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A4.Ex1X.3.2.2.m1.1" class="ltx_math_unparsed" alttext="\displaystyle\left(a\times\begin{bmatrix}\mathcal{H}(L_{q,\#\text{characters in question}})\\
\mathcal{H}(L_{q,\#\text{characters in answer}})\\
\mathcal{H}(L_{q,\#\text{characters in analysis}})\end{bmatrix}\right.+" display="inline"><semantics id="A4.Ex1X.3.2.2.m1.1a"><mrow id="A4.Ex1X.3.2.2.m1.1b"><mo id="A4.Ex1X.3.2.2.m1.1.1">(</mo><mi id="A4.Ex1X.3.2.2.m1.1.2">a</mi><mo lspace="0.222em" rspace="0.222em" id="A4.Ex1X.3.2.2.m1.1.3">×</mo><mrow id="A4.Ex1.m1.1.1.1.1.1.1a.3"><mo id="A4.Ex1.m1.1.1.1.1.1.1a.3.1">[</mo><mtable rowspacing="0pt" id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf"><mtr id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mfa"><mtd id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mfb"><mrow id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.3.3.3.3"><mi class="ltx_font_mathcaligraphic" id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.3.3.3.3.5">ℋ</mi><mo lspace="0em" rspace="0em" id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.3.3.3.3.4">​</mo><mrow id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.3.3.3.3.3.1"><mo stretchy="false" id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.3.3.3.3.3.1.2">(</mo><msub id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.3.3.3.3.3.1.1"><mi id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.3.3.3.3.3.1.1.2">L</mi><mrow id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.2.2.2.2.2.2.2"><mi id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.1.1.1.1.1.1.1">q</mi><mo id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.2.2.2.2.2.2.2.2">,</mo><mrow id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.2.2.2.2.2.2.2.1"><mi mathvariant="normal" id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.2.2.2.2.2.2.2.1.2">#</mi><mo lspace="0em" rspace="0em" id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.2.2.2.2.2.2.2.1.1">​</mo><mtext id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.2.2.2.2.2.2.2.1.3">characters in question</mtext></mrow></mrow></msub><mo stretchy="false" id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.3.3.3.3.3.1.3">)</mo></mrow></mrow></mtd></mtr><mtr id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mfc"><mtd id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mfd"><mrow id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.6.6.3.3"><mi class="ltx_font_mathcaligraphic" id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.6.6.3.3.5">ℋ</mi><mo lspace="0em" rspace="0em" id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.6.6.3.3.4">​</mo><mrow id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.6.6.3.3.3.1"><mo stretchy="false" id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.6.6.3.3.3.1.2">(</mo><msub id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.6.6.3.3.3.1.1"><mi id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.6.6.3.3.3.1.1.2">L</mi><mrow id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.5.5.2.2.2.2.2"><mi id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.4.4.1.1.1.1.1">q</mi><mo id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.5.5.2.2.2.2.2.2">,</mo><mrow id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.5.5.2.2.2.2.2.1"><mi mathvariant="normal" id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.5.5.2.2.2.2.2.1.2">#</mi><mo lspace="0em" rspace="0em" id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.5.5.2.2.2.2.2.1.1">​</mo><mtext id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.5.5.2.2.2.2.2.1.3">characters in answer</mtext></mrow></mrow></msub><mo stretchy="false" id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.6.6.3.3.3.1.3">)</mo></mrow></mrow></mtd></mtr><mtr id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mfe"><mtd id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mff"><mrow id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.9.9.3.3"><mi class="ltx_font_mathcaligraphic" id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.9.9.3.3.5">ℋ</mi><mo lspace="0em" rspace="0em" id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.9.9.3.3.4">​</mo><mrow id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.9.9.3.3.3.1"><mo stretchy="false" id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.9.9.3.3.3.1.2">(</mo><msub id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.9.9.3.3.3.1.1"><mi id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.9.9.3.3.3.1.1.2">L</mi><mrow id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.8.8.2.2.2.2.2"><mi id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.7.7.1.1.1.1.1">q</mi><mo id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.8.8.2.2.2.2.2.2">,</mo><mrow id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.8.8.2.2.2.2.2.1"><mi mathvariant="normal" id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.8.8.2.2.2.2.2.1.2">#</mi><mo lspace="0em" rspace="0em" id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.8.8.2.2.2.2.2.1.1">​</mo><mtext id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.8.8.2.2.2.2.2.1.3">characters in analysis</mtext></mrow></mrow></msub><mo stretchy="false" id="A4.Ex1.m1.1.1.1.1.1.1.1.1.mf.9.9.3.3.3.1.3">)</mo></mrow></mrow></mtd></mtr></mtable><mo id="A4.Ex1.m1.1.1.1.1.1.1a.3.2">]</mo></mrow><mo id="A4.Ex1X.3.2.2.m1.1.4">+</mo></mrow><annotation encoding="application/x-tex" id="A4.Ex1X.3.2.2.m1.1c">\displaystyle\left(a\times\begin{bmatrix}\mathcal{H}(L_{q,\#\text{characters in question}})\\
\mathcal{H}(L_{q,\#\text{characters in answer}})\\
\mathcal{H}(L_{q,\#\text{characters in analysis}})\end{bmatrix}\right.+</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex1Xa"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="A4.Ex1Xa.2.1.1.m1.1" class="ltx_math_unparsed" alttext="\displaystyle\left.b\times\begin{bmatrix}\mathcal{H}(L_{q,\#\text{images in question}})\\
\mathcal{H}(L_{q,\#\text{images in answer}})\\
\mathcal{H}(L_{q,\#\text{images in analysis}})\end{bmatrix}\right)^{\top}\begin{bmatrix}1.0\\
0.1\\
0.5\end{bmatrix}" display="inline"><semantics id="A4.Ex1Xa.2.1.1.m1.1a"><mrow id="A4.Ex1Xa.2.1.1.m1.1b"><mi id="A4.Ex1Xa.2.1.1.m1.1.1">b</mi><mo lspace="0.222em" rspace="0.222em" id="A4.Ex1Xa.2.1.1.m1.1.2">×</mo><mrow id="A4.Ex1.m1.2.2.2.1.1.1a.3"><mo id="A4.Ex1.m1.2.2.2.1.1.1a.3.1">[</mo><mtable rowspacing="0pt" id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf"><mtr id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mfa"><mtd id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mfb"><mrow id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.3.3.3.3"><mi class="ltx_font_mathcaligraphic" id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.3.3.3.3.5">ℋ</mi><mo lspace="0em" rspace="0em" id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.3.3.3.3.4">​</mo><mrow id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.3.3.3.3.3.1"><mo stretchy="false" id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.3.3.3.3.3.1.2">(</mo><msub id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.3.3.3.3.3.1.1"><mi id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.3.3.3.3.3.1.1.2">L</mi><mrow id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.2.2.2.2.2.2.2"><mi id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.1.1.1.1.1.1.1">q</mi><mo id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.2.2.2.2.2.2.2.2">,</mo><mrow id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.2.2.2.2.2.2.2.1"><mi mathvariant="normal" id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.2.2.2.2.2.2.2.1.2">#</mi><mo lspace="0em" rspace="0em" id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.2.2.2.2.2.2.2.1.1">​</mo><mtext id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.2.2.2.2.2.2.2.1.3">images in question</mtext></mrow></mrow></msub><mo stretchy="false" id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.3.3.3.3.3.1.3">)</mo></mrow></mrow></mtd></mtr><mtr id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mfc"><mtd id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mfd"><mrow id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.6.6.3.3"><mi class="ltx_font_mathcaligraphic" id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.6.6.3.3.5">ℋ</mi><mo lspace="0em" rspace="0em" id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.6.6.3.3.4">​</mo><mrow id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.6.6.3.3.3.1"><mo stretchy="false" id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.6.6.3.3.3.1.2">(</mo><msub id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.6.6.3.3.3.1.1"><mi id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.6.6.3.3.3.1.1.2">L</mi><mrow id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.5.5.2.2.2.2.2"><mi id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.4.4.1.1.1.1.1">q</mi><mo id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.5.5.2.2.2.2.2.2">,</mo><mrow id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.5.5.2.2.2.2.2.1"><mi mathvariant="normal" id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.5.5.2.2.2.2.2.1.2">#</mi><mo lspace="0em" rspace="0em" id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.5.5.2.2.2.2.2.1.1">​</mo><mtext id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.5.5.2.2.2.2.2.1.3">images in answer</mtext></mrow></mrow></msub><mo stretchy="false" id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.6.6.3.3.3.1.3">)</mo></mrow></mrow></mtd></mtr><mtr id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mfe"><mtd id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mff"><mrow id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.9.9.3.3"><mi class="ltx_font_mathcaligraphic" id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.9.9.3.3.5">ℋ</mi><mo lspace="0em" rspace="0em" id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.9.9.3.3.4">​</mo><mrow id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.9.9.3.3.3.1"><mo stretchy="false" id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.9.9.3.3.3.1.2">(</mo><msub id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.9.9.3.3.3.1.1"><mi id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.9.9.3.3.3.1.1.2">L</mi><mrow id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.8.8.2.2.2.2.2"><mi id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.7.7.1.1.1.1.1">q</mi><mo id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.8.8.2.2.2.2.2.2">,</mo><mrow id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.8.8.2.2.2.2.2.1"><mi mathvariant="normal" id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.8.8.2.2.2.2.2.1.2">#</mi><mo lspace="0em" rspace="0em" id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.8.8.2.2.2.2.2.1.1">​</mo><mtext id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.8.8.2.2.2.2.2.1.3">images in analysis</mtext></mrow></mrow></msub><mo stretchy="false" id="A4.Ex1.m1.2.2.2.1.1.1.1.1.mf.9.9.3.3.3.1.3">)</mo></mrow></mrow></mtd></mtr></mtable><mo id="A4.Ex1.m1.2.2.2.1.1.1a.3.2">]</mo></mrow><mo id="A4.Ex1Xa.2.1.1.m1.1.3">)</mo><msup id="A4.Ex1Xa.2.1.1.m1.1.4"><mi id="A4.Ex1Xa.2.1.1.m1.1.4a"></mi><mo id="A4.Ex1Xa.2.1.1.m1.1.4.1">⊤</mo></msup><mrow id="A4.Ex1.m1.3.3.3.2.2.2a.3"><mo id="A4.Ex1.m1.3.3.3.2.2.2a.3.1">[</mo><mtable rowspacing="0pt" id="A4.Ex1.m1.3.3.3.2.2.2.1.1.mf"><mtr id="A4.Ex1.m1.3.3.3.2.2.2.1.1.mfa"><mtd id="A4.Ex1.m1.3.3.3.2.2.2.1.1.mfb"><mn id="A4.Ex1.m1.3.3.3.2.2.2.1.1.mf.1.1.1">1.0</mn></mtd></mtr><mtr id="A4.Ex1.m1.3.3.3.2.2.2.1.1.mfc"><mtd id="A4.Ex1.m1.3.3.3.2.2.2.1.1.mfd"><mn id="A4.Ex1.m1.3.3.3.2.2.2.1.1.mf.2.1.1">0.1</mn></mtd></mtr><mtr id="A4.Ex1.m1.3.3.3.2.2.2.1.1.mfe"><mtd id="A4.Ex1.m1.3.3.3.2.2.2.1.1.mff"><mn id="A4.Ex1.m1.3.3.3.2.2.2.1.1.mf.3.1.1">0.5</mn></mtd></mtr></mtable><mo id="A4.Ex1.m1.3.3.3.2.2.2a.3.2">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="A4.Ex1Xa.2.1.1.m1.1c">\displaystyle\left.b\times\begin{bmatrix}\mathcal{H}(L_{q,\#\text{images in question}})\\
\mathcal{H}(L_{q,\#\text{images in answer}})\\
\mathcal{H}(L_{q,\#\text{images in analysis}})\end{bmatrix}\right)^{\top}\begin{bmatrix}1.0\\
0.1\\
0.5\end{bmatrix}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="A4.p2.1" class="ltx_p">where <math id="A4.p2.1.m1.2" class="ltx_Math" alttext="q=1,b=1" display="inline"><semantics id="A4.p2.1.m1.2a"><mrow id="A4.p2.1.m1.2.2.2" xref="A4.p2.1.m1.2.2.3.cmml"><mrow id="A4.p2.1.m1.1.1.1.1" xref="A4.p2.1.m1.1.1.1.1.cmml"><mi id="A4.p2.1.m1.1.1.1.1.2" xref="A4.p2.1.m1.1.1.1.1.2.cmml">q</mi><mo id="A4.p2.1.m1.1.1.1.1.1" xref="A4.p2.1.m1.1.1.1.1.1.cmml">=</mo><mn id="A4.p2.1.m1.1.1.1.1.3" xref="A4.p2.1.m1.1.1.1.1.3.cmml">1</mn></mrow><mo id="A4.p2.1.m1.2.2.2.3" xref="A4.p2.1.m1.2.2.3a.cmml">,</mo><mrow id="A4.p2.1.m1.2.2.2.2" xref="A4.p2.1.m1.2.2.2.2.cmml"><mi id="A4.p2.1.m1.2.2.2.2.2" xref="A4.p2.1.m1.2.2.2.2.2.cmml">b</mi><mo id="A4.p2.1.m1.2.2.2.2.1" xref="A4.p2.1.m1.2.2.2.2.1.cmml">=</mo><mn id="A4.p2.1.m1.2.2.2.2.3" xref="A4.p2.1.m1.2.2.2.2.3.cmml">1</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.p2.1.m1.2b"><apply id="A4.p2.1.m1.2.2.3.cmml" xref="A4.p2.1.m1.2.2.2"><csymbol cd="ambiguous" id="A4.p2.1.m1.2.2.3a.cmml" xref="A4.p2.1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="A4.p2.1.m1.1.1.1.1.cmml" xref="A4.p2.1.m1.1.1.1.1"><eq id="A4.p2.1.m1.1.1.1.1.1.cmml" xref="A4.p2.1.m1.1.1.1.1.1"></eq><ci id="A4.p2.1.m1.1.1.1.1.2.cmml" xref="A4.p2.1.m1.1.1.1.1.2">𝑞</ci><cn type="integer" id="A4.p2.1.m1.1.1.1.1.3.cmml" xref="A4.p2.1.m1.1.1.1.1.3">1</cn></apply><apply id="A4.p2.1.m1.2.2.2.2.cmml" xref="A4.p2.1.m1.2.2.2.2"><eq id="A4.p2.1.m1.2.2.2.2.1.cmml" xref="A4.p2.1.m1.2.2.2.2.1"></eq><ci id="A4.p2.1.m1.2.2.2.2.2.cmml" xref="A4.p2.1.m1.2.2.2.2.2">𝑏</ci><cn type="integer" id="A4.p2.1.m1.2.2.2.2.3.cmml" xref="A4.p2.1.m1.2.2.2.2.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.1.m1.2c">q=1,b=1</annotation></semantics></math> are customized weights.</p>
</div>
<div id="A4.p3" class="ltx_para">
<p id="A4.p3.4" class="ltx_p">In the formula above, we use a harmonic mean function <math id="A4.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{H}" display="inline"><semantics id="A4.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A4.p3.1.m1.1.1" xref="A4.p3.1.m1.1.1.cmml">ℋ</mi><annotation-xml encoding="MathML-Content" id="A4.p3.1.m1.1b"><ci id="A4.p3.1.m1.1.1.cmml" xref="A4.p3.1.m1.1.1">ℋ</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.1.m1.1c">\mathcal{H}</annotation></semantics></math> to normalize content length <math id="A4.p3.2.m2.2" class="ltx_Math" alttext="L_{q,i}" display="inline"><semantics id="A4.p3.2.m2.2a"><msub id="A4.p3.2.m2.2.3" xref="A4.p3.2.m2.2.3.cmml"><mi id="A4.p3.2.m2.2.3.2" xref="A4.p3.2.m2.2.3.2.cmml">L</mi><mrow id="A4.p3.2.m2.2.2.2.4" xref="A4.p3.2.m2.2.2.2.3.cmml"><mi id="A4.p3.2.m2.1.1.1.1" xref="A4.p3.2.m2.1.1.1.1.cmml">q</mi><mo id="A4.p3.2.m2.2.2.2.4.1" xref="A4.p3.2.m2.2.2.2.3.cmml">,</mo><mi id="A4.p3.2.m2.2.2.2.2" xref="A4.p3.2.m2.2.2.2.2.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A4.p3.2.m2.2b"><apply id="A4.p3.2.m2.2.3.cmml" xref="A4.p3.2.m2.2.3"><csymbol cd="ambiguous" id="A4.p3.2.m2.2.3.1.cmml" xref="A4.p3.2.m2.2.3">subscript</csymbol><ci id="A4.p3.2.m2.2.3.2.cmml" xref="A4.p3.2.m2.2.3.2">𝐿</ci><list id="A4.p3.2.m2.2.2.2.3.cmml" xref="A4.p3.2.m2.2.2.2.4"><ci id="A4.p3.2.m2.1.1.1.1.cmml" xref="A4.p3.2.m2.1.1.1.1">𝑞</ci><ci id="A4.p3.2.m2.2.2.2.2.cmml" xref="A4.p3.2.m2.2.2.2.2">𝑖</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.2.m2.2c">L_{q,i}</annotation></semantics></math> of each target value <math id="A4.p3.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="A4.p3.3.m3.1a"><mi id="A4.p3.3.m3.1.1" xref="A4.p3.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="A4.p3.3.m3.1b"><ci id="A4.p3.3.m3.1.1.cmml" xref="A4.p3.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.3.m3.1c">i</annotation></semantics></math> within each knowledge piece <math id="A4.p3.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A4.p3.4.m4.1a"><mi id="A4.p3.4.m4.1.1" xref="A4.p3.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A4.p3.4.m4.1b"><ci id="A4.p3.4.m4.1.1.cmml" xref="A4.p3.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.4.m4.1c">k</annotation></semantics></math>.<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Note that for those questions without knowledge information, we simply use a ”null” string as a keyword.</span></span></span></p>
</div>
<div id="A4.p4" class="ltx_para">
<table id="A4.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A4.Ex2.m1.15" class="ltx_Math" alttext="\mathcal{H}(L_{q,i})=\frac{1}{\frac{1}{L_{q,i}}+\frac{1}{\overline{L_{q,i}}}}=\frac{2L_{q,i}\overline{L_{q,i}}}{L_{q,i}^{2}+\overline{L_{q,i}}^{2}}" display="block"><semantics id="A4.Ex2.m1.15a"><mrow id="A4.Ex2.m1.15.15" xref="A4.Ex2.m1.15.15.cmml"><mrow id="A4.Ex2.m1.15.15.1" xref="A4.Ex2.m1.15.15.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A4.Ex2.m1.15.15.1.3" xref="A4.Ex2.m1.15.15.1.3.cmml">ℋ</mi><mo lspace="0em" rspace="0em" id="A4.Ex2.m1.15.15.1.2" xref="A4.Ex2.m1.15.15.1.2.cmml">​</mo><mrow id="A4.Ex2.m1.15.15.1.1.1" xref="A4.Ex2.m1.15.15.1.1.1.1.cmml"><mo stretchy="false" id="A4.Ex2.m1.15.15.1.1.1.2" xref="A4.Ex2.m1.15.15.1.1.1.1.cmml">(</mo><msub id="A4.Ex2.m1.15.15.1.1.1.1" xref="A4.Ex2.m1.15.15.1.1.1.1.cmml"><mi id="A4.Ex2.m1.15.15.1.1.1.1.2" xref="A4.Ex2.m1.15.15.1.1.1.1.2.cmml">L</mi><mrow id="A4.Ex2.m1.2.2.2.4" xref="A4.Ex2.m1.2.2.2.3.cmml"><mi id="A4.Ex2.m1.1.1.1.1" xref="A4.Ex2.m1.1.1.1.1.cmml">q</mi><mo id="A4.Ex2.m1.2.2.2.4.1" xref="A4.Ex2.m1.2.2.2.3.cmml">,</mo><mi id="A4.Ex2.m1.2.2.2.2" xref="A4.Ex2.m1.2.2.2.2.cmml">i</mi></mrow></msub><mo stretchy="false" id="A4.Ex2.m1.15.15.1.1.1.3" xref="A4.Ex2.m1.15.15.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="A4.Ex2.m1.15.15.3" xref="A4.Ex2.m1.15.15.3.cmml">=</mo><mfrac id="A4.Ex2.m1.6.6" xref="A4.Ex2.m1.6.6.cmml"><mn id="A4.Ex2.m1.6.6.6" xref="A4.Ex2.m1.6.6.6.cmml">1</mn><mrow id="A4.Ex2.m1.6.6.4" xref="A4.Ex2.m1.6.6.4.cmml"><mfrac id="A4.Ex2.m1.4.4.2.2" xref="A4.Ex2.m1.4.4.2.2.cmml"><mn id="A4.Ex2.m1.4.4.2.2.4" xref="A4.Ex2.m1.4.4.2.2.4.cmml">1</mn><msub id="A4.Ex2.m1.4.4.2.2.2" xref="A4.Ex2.m1.4.4.2.2.2.cmml"><mi id="A4.Ex2.m1.4.4.2.2.2.4" xref="A4.Ex2.m1.4.4.2.2.2.4.cmml">L</mi><mrow id="A4.Ex2.m1.4.4.2.2.2.2.2.4" xref="A4.Ex2.m1.4.4.2.2.2.2.2.3.cmml"><mi id="A4.Ex2.m1.3.3.1.1.1.1.1.1" xref="A4.Ex2.m1.3.3.1.1.1.1.1.1.cmml">q</mi><mo id="A4.Ex2.m1.4.4.2.2.2.2.2.4.1" xref="A4.Ex2.m1.4.4.2.2.2.2.2.3.cmml">,</mo><mi id="A4.Ex2.m1.4.4.2.2.2.2.2.2" xref="A4.Ex2.m1.4.4.2.2.2.2.2.2.cmml">i</mi></mrow></msub></mfrac><mo id="A4.Ex2.m1.6.6.4.5" xref="A4.Ex2.m1.6.6.4.5.cmml">+</mo><mfrac id="A4.Ex2.m1.6.6.4.4" xref="A4.Ex2.m1.6.6.4.4.cmml"><mn id="A4.Ex2.m1.6.6.4.4.4" xref="A4.Ex2.m1.6.6.4.4.4.cmml">1</mn><mover accent="true" id="A4.Ex2.m1.6.6.4.4.2" xref="A4.Ex2.m1.6.6.4.4.2.cmml"><msub id="A4.Ex2.m1.6.6.4.4.2.2.2" xref="A4.Ex2.m1.6.6.4.4.2.2.2.cmml"><mi id="A4.Ex2.m1.6.6.4.4.2.2.2.4" xref="A4.Ex2.m1.6.6.4.4.2.2.2.4.cmml">L</mi><mrow id="A4.Ex2.m1.6.6.4.4.2.2.2.2.2.4" xref="A4.Ex2.m1.6.6.4.4.2.2.2.2.2.3.cmml"><mi id="A4.Ex2.m1.5.5.3.3.1.1.1.1.1.1" xref="A4.Ex2.m1.5.5.3.3.1.1.1.1.1.1.cmml">q</mi><mo id="A4.Ex2.m1.6.6.4.4.2.2.2.2.2.4.1" xref="A4.Ex2.m1.6.6.4.4.2.2.2.2.2.3.cmml">,</mo><mi id="A4.Ex2.m1.6.6.4.4.2.2.2.2.2.2" xref="A4.Ex2.m1.6.6.4.4.2.2.2.2.2.2.cmml">i</mi></mrow></msub><mo id="A4.Ex2.m1.6.6.4.4.2.3" xref="A4.Ex2.m1.6.6.4.4.2.3.cmml">¯</mo></mover></mfrac></mrow></mfrac><mo id="A4.Ex2.m1.15.15.4" xref="A4.Ex2.m1.15.15.4.cmml">=</mo><mfrac id="A4.Ex2.m1.14.14" xref="A4.Ex2.m1.14.14.cmml"><mrow id="A4.Ex2.m1.10.10.4" xref="A4.Ex2.m1.10.10.4.cmml"><mn id="A4.Ex2.m1.10.10.4.6" xref="A4.Ex2.m1.10.10.4.6.cmml">2</mn><mo lspace="0em" rspace="0em" id="A4.Ex2.m1.10.10.4.5" xref="A4.Ex2.m1.10.10.4.5.cmml">​</mo><msub id="A4.Ex2.m1.10.10.4.7" xref="A4.Ex2.m1.10.10.4.7.cmml"><mi id="A4.Ex2.m1.10.10.4.7.2" xref="A4.Ex2.m1.10.10.4.7.2.cmml">L</mi><mrow id="A4.Ex2.m1.8.8.2.2.2.4" xref="A4.Ex2.m1.8.8.2.2.2.3.cmml"><mi id="A4.Ex2.m1.7.7.1.1.1.1" xref="A4.Ex2.m1.7.7.1.1.1.1.cmml">q</mi><mo id="A4.Ex2.m1.8.8.2.2.2.4.1" xref="A4.Ex2.m1.8.8.2.2.2.3.cmml">,</mo><mi id="A4.Ex2.m1.8.8.2.2.2.2" xref="A4.Ex2.m1.8.8.2.2.2.2.cmml">i</mi></mrow></msub><mo lspace="0em" rspace="0em" id="A4.Ex2.m1.10.10.4.5a" xref="A4.Ex2.m1.10.10.4.5.cmml">​</mo><mover accent="true" id="A4.Ex2.m1.10.10.4.4" xref="A4.Ex2.m1.10.10.4.4.cmml"><msub id="A4.Ex2.m1.10.10.4.4.2" xref="A4.Ex2.m1.10.10.4.4.2.cmml"><mi id="A4.Ex2.m1.10.10.4.4.2.4" xref="A4.Ex2.m1.10.10.4.4.2.4.cmml">L</mi><mrow id="A4.Ex2.m1.10.10.4.4.2.2.2.4" xref="A4.Ex2.m1.10.10.4.4.2.2.2.3.cmml"><mi id="A4.Ex2.m1.9.9.3.3.1.1.1.1" xref="A4.Ex2.m1.9.9.3.3.1.1.1.1.cmml">q</mi><mo id="A4.Ex2.m1.10.10.4.4.2.2.2.4.1" xref="A4.Ex2.m1.10.10.4.4.2.2.2.3.cmml">,</mo><mi id="A4.Ex2.m1.10.10.4.4.2.2.2.2" xref="A4.Ex2.m1.10.10.4.4.2.2.2.2.cmml">i</mi></mrow></msub><mo id="A4.Ex2.m1.10.10.4.4.3" xref="A4.Ex2.m1.10.10.4.4.3.cmml">¯</mo></mover></mrow><mrow id="A4.Ex2.m1.14.14.8" xref="A4.Ex2.m1.14.14.8.cmml"><msubsup id="A4.Ex2.m1.14.14.8.6" xref="A4.Ex2.m1.14.14.8.6.cmml"><mi id="A4.Ex2.m1.14.14.8.6.2.2" xref="A4.Ex2.m1.14.14.8.6.2.2.cmml">L</mi><mrow id="A4.Ex2.m1.12.12.6.2.2.4" xref="A4.Ex2.m1.12.12.6.2.2.3.cmml"><mi id="A4.Ex2.m1.11.11.5.1.1.1" xref="A4.Ex2.m1.11.11.5.1.1.1.cmml">q</mi><mo id="A4.Ex2.m1.12.12.6.2.2.4.1" xref="A4.Ex2.m1.12.12.6.2.2.3.cmml">,</mo><mi id="A4.Ex2.m1.12.12.6.2.2.2" xref="A4.Ex2.m1.12.12.6.2.2.2.cmml">i</mi></mrow><mn id="A4.Ex2.m1.14.14.8.6.3" xref="A4.Ex2.m1.14.14.8.6.3.cmml">2</mn></msubsup><mo id="A4.Ex2.m1.14.14.8.5" xref="A4.Ex2.m1.14.14.8.5.cmml">+</mo><msup id="A4.Ex2.m1.14.14.8.7" xref="A4.Ex2.m1.14.14.8.7.cmml"><mover accent="true" id="A4.Ex2.m1.14.14.8.4" xref="A4.Ex2.m1.14.14.8.4.cmml"><msub id="A4.Ex2.m1.14.14.8.4.2" xref="A4.Ex2.m1.14.14.8.4.2.cmml"><mi id="A4.Ex2.m1.14.14.8.4.2.4" xref="A4.Ex2.m1.14.14.8.4.2.4.cmml">L</mi><mrow id="A4.Ex2.m1.14.14.8.4.2.2.2.4" xref="A4.Ex2.m1.14.14.8.4.2.2.2.3.cmml"><mi id="A4.Ex2.m1.13.13.7.3.1.1.1.1" xref="A4.Ex2.m1.13.13.7.3.1.1.1.1.cmml">q</mi><mo id="A4.Ex2.m1.14.14.8.4.2.2.2.4.1" xref="A4.Ex2.m1.14.14.8.4.2.2.2.3.cmml">,</mo><mi id="A4.Ex2.m1.14.14.8.4.2.2.2.2" xref="A4.Ex2.m1.14.14.8.4.2.2.2.2.cmml">i</mi></mrow></msub><mo id="A4.Ex2.m1.14.14.8.4.3" xref="A4.Ex2.m1.14.14.8.4.3.cmml">¯</mo></mover><mn id="A4.Ex2.m1.14.14.8.7.2" xref="A4.Ex2.m1.14.14.8.7.2.cmml">2</mn></msup></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="A4.Ex2.m1.15b"><apply id="A4.Ex2.m1.15.15.cmml" xref="A4.Ex2.m1.15.15"><and id="A4.Ex2.m1.15.15a.cmml" xref="A4.Ex2.m1.15.15"></and><apply id="A4.Ex2.m1.15.15b.cmml" xref="A4.Ex2.m1.15.15"><eq id="A4.Ex2.m1.15.15.3.cmml" xref="A4.Ex2.m1.15.15.3"></eq><apply id="A4.Ex2.m1.15.15.1.cmml" xref="A4.Ex2.m1.15.15.1"><times id="A4.Ex2.m1.15.15.1.2.cmml" xref="A4.Ex2.m1.15.15.1.2"></times><ci id="A4.Ex2.m1.15.15.1.3.cmml" xref="A4.Ex2.m1.15.15.1.3">ℋ</ci><apply id="A4.Ex2.m1.15.15.1.1.1.1.cmml" xref="A4.Ex2.m1.15.15.1.1.1"><csymbol cd="ambiguous" id="A4.Ex2.m1.15.15.1.1.1.1.1.cmml" xref="A4.Ex2.m1.15.15.1.1.1">subscript</csymbol><ci id="A4.Ex2.m1.15.15.1.1.1.1.2.cmml" xref="A4.Ex2.m1.15.15.1.1.1.1.2">𝐿</ci><list id="A4.Ex2.m1.2.2.2.3.cmml" xref="A4.Ex2.m1.2.2.2.4"><ci id="A4.Ex2.m1.1.1.1.1.cmml" xref="A4.Ex2.m1.1.1.1.1">𝑞</ci><ci id="A4.Ex2.m1.2.2.2.2.cmml" xref="A4.Ex2.m1.2.2.2.2">𝑖</ci></list></apply></apply><apply id="A4.Ex2.m1.6.6.cmml" xref="A4.Ex2.m1.6.6"><divide id="A4.Ex2.m1.6.6.5.cmml" xref="A4.Ex2.m1.6.6"></divide><cn type="integer" id="A4.Ex2.m1.6.6.6.cmml" xref="A4.Ex2.m1.6.6.6">1</cn><apply id="A4.Ex2.m1.6.6.4.cmml" xref="A4.Ex2.m1.6.6.4"><plus id="A4.Ex2.m1.6.6.4.5.cmml" xref="A4.Ex2.m1.6.6.4.5"></plus><apply id="A4.Ex2.m1.4.4.2.2.cmml" xref="A4.Ex2.m1.4.4.2.2"><divide id="A4.Ex2.m1.4.4.2.2.3.cmml" xref="A4.Ex2.m1.4.4.2.2"></divide><cn type="integer" id="A4.Ex2.m1.4.4.2.2.4.cmml" xref="A4.Ex2.m1.4.4.2.2.4">1</cn><apply id="A4.Ex2.m1.4.4.2.2.2.cmml" xref="A4.Ex2.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="A4.Ex2.m1.4.4.2.2.2.3.cmml" xref="A4.Ex2.m1.4.4.2.2.2">subscript</csymbol><ci id="A4.Ex2.m1.4.4.2.2.2.4.cmml" xref="A4.Ex2.m1.4.4.2.2.2.4">𝐿</ci><list id="A4.Ex2.m1.4.4.2.2.2.2.2.3.cmml" xref="A4.Ex2.m1.4.4.2.2.2.2.2.4"><ci id="A4.Ex2.m1.3.3.1.1.1.1.1.1.cmml" xref="A4.Ex2.m1.3.3.1.1.1.1.1.1">𝑞</ci><ci id="A4.Ex2.m1.4.4.2.2.2.2.2.2.cmml" xref="A4.Ex2.m1.4.4.2.2.2.2.2.2">𝑖</ci></list></apply></apply><apply id="A4.Ex2.m1.6.6.4.4.cmml" xref="A4.Ex2.m1.6.6.4.4"><divide id="A4.Ex2.m1.6.6.4.4.3.cmml" xref="A4.Ex2.m1.6.6.4.4"></divide><cn type="integer" id="A4.Ex2.m1.6.6.4.4.4.cmml" xref="A4.Ex2.m1.6.6.4.4.4">1</cn><apply id="A4.Ex2.m1.6.6.4.4.2.cmml" xref="A4.Ex2.m1.6.6.4.4.2"><ci id="A4.Ex2.m1.6.6.4.4.2.3.cmml" xref="A4.Ex2.m1.6.6.4.4.2.3">¯</ci><apply id="A4.Ex2.m1.6.6.4.4.2.2.2.cmml" xref="A4.Ex2.m1.6.6.4.4.2.2.2"><csymbol cd="ambiguous" id="A4.Ex2.m1.6.6.4.4.2.2.2.3.cmml" xref="A4.Ex2.m1.6.6.4.4.2.2.2">subscript</csymbol><ci id="A4.Ex2.m1.6.6.4.4.2.2.2.4.cmml" xref="A4.Ex2.m1.6.6.4.4.2.2.2.4">𝐿</ci><list id="A4.Ex2.m1.6.6.4.4.2.2.2.2.2.3.cmml" xref="A4.Ex2.m1.6.6.4.4.2.2.2.2.2.4"><ci id="A4.Ex2.m1.5.5.3.3.1.1.1.1.1.1.cmml" xref="A4.Ex2.m1.5.5.3.3.1.1.1.1.1.1">𝑞</ci><ci id="A4.Ex2.m1.6.6.4.4.2.2.2.2.2.2.cmml" xref="A4.Ex2.m1.6.6.4.4.2.2.2.2.2.2">𝑖</ci></list></apply></apply></apply></apply></apply></apply><apply id="A4.Ex2.m1.15.15c.cmml" xref="A4.Ex2.m1.15.15"><eq id="A4.Ex2.m1.15.15.4.cmml" xref="A4.Ex2.m1.15.15.4"></eq><share href="#A4.Ex2.m1.6.6.cmml" id="A4.Ex2.m1.15.15d.cmml" xref="A4.Ex2.m1.15.15"></share><apply id="A4.Ex2.m1.14.14.cmml" xref="A4.Ex2.m1.14.14"><divide id="A4.Ex2.m1.14.14.9.cmml" xref="A4.Ex2.m1.14.14"></divide><apply id="A4.Ex2.m1.10.10.4.cmml" xref="A4.Ex2.m1.10.10.4"><times id="A4.Ex2.m1.10.10.4.5.cmml" xref="A4.Ex2.m1.10.10.4.5"></times><cn type="integer" id="A4.Ex2.m1.10.10.4.6.cmml" xref="A4.Ex2.m1.10.10.4.6">2</cn><apply id="A4.Ex2.m1.10.10.4.7.cmml" xref="A4.Ex2.m1.10.10.4.7"><csymbol cd="ambiguous" id="A4.Ex2.m1.10.10.4.7.1.cmml" xref="A4.Ex2.m1.10.10.4.7">subscript</csymbol><ci id="A4.Ex2.m1.10.10.4.7.2.cmml" xref="A4.Ex2.m1.10.10.4.7.2">𝐿</ci><list id="A4.Ex2.m1.8.8.2.2.2.3.cmml" xref="A4.Ex2.m1.8.8.2.2.2.4"><ci id="A4.Ex2.m1.7.7.1.1.1.1.cmml" xref="A4.Ex2.m1.7.7.1.1.1.1">𝑞</ci><ci id="A4.Ex2.m1.8.8.2.2.2.2.cmml" xref="A4.Ex2.m1.8.8.2.2.2.2">𝑖</ci></list></apply><apply id="A4.Ex2.m1.10.10.4.4.cmml" xref="A4.Ex2.m1.10.10.4.4"><ci id="A4.Ex2.m1.10.10.4.4.3.cmml" xref="A4.Ex2.m1.10.10.4.4.3">¯</ci><apply id="A4.Ex2.m1.10.10.4.4.2.cmml" xref="A4.Ex2.m1.10.10.4.4.2"><csymbol cd="ambiguous" id="A4.Ex2.m1.10.10.4.4.2.3.cmml" xref="A4.Ex2.m1.10.10.4.4.2">subscript</csymbol><ci id="A4.Ex2.m1.10.10.4.4.2.4.cmml" xref="A4.Ex2.m1.10.10.4.4.2.4">𝐿</ci><list id="A4.Ex2.m1.10.10.4.4.2.2.2.3.cmml" xref="A4.Ex2.m1.10.10.4.4.2.2.2.4"><ci id="A4.Ex2.m1.9.9.3.3.1.1.1.1.cmml" xref="A4.Ex2.m1.9.9.3.3.1.1.1.1">𝑞</ci><ci id="A4.Ex2.m1.10.10.4.4.2.2.2.2.cmml" xref="A4.Ex2.m1.10.10.4.4.2.2.2.2">𝑖</ci></list></apply></apply></apply><apply id="A4.Ex2.m1.14.14.8.cmml" xref="A4.Ex2.m1.14.14.8"><plus id="A4.Ex2.m1.14.14.8.5.cmml" xref="A4.Ex2.m1.14.14.8.5"></plus><apply id="A4.Ex2.m1.14.14.8.6.cmml" xref="A4.Ex2.m1.14.14.8.6"><csymbol cd="ambiguous" id="A4.Ex2.m1.14.14.8.6.1.cmml" xref="A4.Ex2.m1.14.14.8.6">superscript</csymbol><apply id="A4.Ex2.m1.14.14.8.6.2.cmml" xref="A4.Ex2.m1.14.14.8.6"><csymbol cd="ambiguous" id="A4.Ex2.m1.14.14.8.6.2.1.cmml" xref="A4.Ex2.m1.14.14.8.6">subscript</csymbol><ci id="A4.Ex2.m1.14.14.8.6.2.2.cmml" xref="A4.Ex2.m1.14.14.8.6.2.2">𝐿</ci><list id="A4.Ex2.m1.12.12.6.2.2.3.cmml" xref="A4.Ex2.m1.12.12.6.2.2.4"><ci id="A4.Ex2.m1.11.11.5.1.1.1.cmml" xref="A4.Ex2.m1.11.11.5.1.1.1">𝑞</ci><ci id="A4.Ex2.m1.12.12.6.2.2.2.cmml" xref="A4.Ex2.m1.12.12.6.2.2.2">𝑖</ci></list></apply><cn type="integer" id="A4.Ex2.m1.14.14.8.6.3.cmml" xref="A4.Ex2.m1.14.14.8.6.3">2</cn></apply><apply id="A4.Ex2.m1.14.14.8.7.cmml" xref="A4.Ex2.m1.14.14.8.7"><csymbol cd="ambiguous" id="A4.Ex2.m1.14.14.8.7.1.cmml" xref="A4.Ex2.m1.14.14.8.7">superscript</csymbol><apply id="A4.Ex2.m1.14.14.8.4.cmml" xref="A4.Ex2.m1.14.14.8.4"><ci id="A4.Ex2.m1.14.14.8.4.3.cmml" xref="A4.Ex2.m1.14.14.8.4.3">¯</ci><apply id="A4.Ex2.m1.14.14.8.4.2.cmml" xref="A4.Ex2.m1.14.14.8.4.2"><csymbol cd="ambiguous" id="A4.Ex2.m1.14.14.8.4.2.3.cmml" xref="A4.Ex2.m1.14.14.8.4.2">subscript</csymbol><ci id="A4.Ex2.m1.14.14.8.4.2.4.cmml" xref="A4.Ex2.m1.14.14.8.4.2.4">𝐿</ci><list id="A4.Ex2.m1.14.14.8.4.2.2.2.3.cmml" xref="A4.Ex2.m1.14.14.8.4.2.2.2.4"><ci id="A4.Ex2.m1.13.13.7.3.1.1.1.1.cmml" xref="A4.Ex2.m1.13.13.7.3.1.1.1.1">𝑞</ci><ci id="A4.Ex2.m1.14.14.8.4.2.2.2.2.cmml" xref="A4.Ex2.m1.14.14.8.4.2.2.2.2">𝑖</ci></list></apply></apply><cn type="integer" id="A4.Ex2.m1.14.14.8.7.2.cmml" xref="A4.Ex2.m1.14.14.8.7.2">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.Ex2.m1.15c">\mathcal{H}(L_{q,i})=\frac{1}{\frac{1}{L_{q,i}}+\frac{1}{\overline{L_{q,i}}}}=\frac{2L_{q,i}\overline{L_{q,i}}}{L_{q,i}^{2}+\overline{L_{q,i}}^{2}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="A4.p4.3" class="ltx_p">where <math id="A4.p4.1.m1.2" class="ltx_Math" alttext="\overline{L_{q,i}}" display="inline"><semantics id="A4.p4.1.m1.2a"><mover accent="true" id="A4.p4.1.m1.2.2" xref="A4.p4.1.m1.2.2.cmml"><msub id="A4.p4.1.m1.2.2.2" xref="A4.p4.1.m1.2.2.2.cmml"><mi id="A4.p4.1.m1.2.2.2.4" xref="A4.p4.1.m1.2.2.2.4.cmml">L</mi><mrow id="A4.p4.1.m1.2.2.2.2.2.4" xref="A4.p4.1.m1.2.2.2.2.2.3.cmml"><mi id="A4.p4.1.m1.1.1.1.1.1.1" xref="A4.p4.1.m1.1.1.1.1.1.1.cmml">q</mi><mo id="A4.p4.1.m1.2.2.2.2.2.4.1" xref="A4.p4.1.m1.2.2.2.2.2.3.cmml">,</mo><mi id="A4.p4.1.m1.2.2.2.2.2.2" xref="A4.p4.1.m1.2.2.2.2.2.2.cmml">i</mi></mrow></msub><mo id="A4.p4.1.m1.2.2.3" xref="A4.p4.1.m1.2.2.3.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="A4.p4.1.m1.2b"><apply id="A4.p4.1.m1.2.2.cmml" xref="A4.p4.1.m1.2.2"><ci id="A4.p4.1.m1.2.2.3.cmml" xref="A4.p4.1.m1.2.2.3">¯</ci><apply id="A4.p4.1.m1.2.2.2.cmml" xref="A4.p4.1.m1.2.2.2"><csymbol cd="ambiguous" id="A4.p4.1.m1.2.2.2.3.cmml" xref="A4.p4.1.m1.2.2.2">subscript</csymbol><ci id="A4.p4.1.m1.2.2.2.4.cmml" xref="A4.p4.1.m1.2.2.2.4">𝐿</ci><list id="A4.p4.1.m1.2.2.2.2.2.3.cmml" xref="A4.p4.1.m1.2.2.2.2.2.4"><ci id="A4.p4.1.m1.1.1.1.1.1.1.cmml" xref="A4.p4.1.m1.1.1.1.1.1.1">𝑞</ci><ci id="A4.p4.1.m1.2.2.2.2.2.2.cmml" xref="A4.p4.1.m1.2.2.2.2.2.2">𝑖</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.1.m1.2c">\overline{L_{q,i}}</annotation></semantics></math> is the arithmetic average of <math id="A4.p4.2.m2.2" class="ltx_Math" alttext="L_{q,i}" display="inline"><semantics id="A4.p4.2.m2.2a"><msub id="A4.p4.2.m2.2.3" xref="A4.p4.2.m2.2.3.cmml"><mi id="A4.p4.2.m2.2.3.2" xref="A4.p4.2.m2.2.3.2.cmml">L</mi><mrow id="A4.p4.2.m2.2.2.2.4" xref="A4.p4.2.m2.2.2.2.3.cmml"><mi id="A4.p4.2.m2.1.1.1.1" xref="A4.p4.2.m2.1.1.1.1.cmml">q</mi><mo id="A4.p4.2.m2.2.2.2.4.1" xref="A4.p4.2.m2.2.2.2.3.cmml">,</mo><mi id="A4.p4.2.m2.2.2.2.2" xref="A4.p4.2.m2.2.2.2.2.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A4.p4.2.m2.2b"><apply id="A4.p4.2.m2.2.3.cmml" xref="A4.p4.2.m2.2.3"><csymbol cd="ambiguous" id="A4.p4.2.m2.2.3.1.cmml" xref="A4.p4.2.m2.2.3">subscript</csymbol><ci id="A4.p4.2.m2.2.3.2.cmml" xref="A4.p4.2.m2.2.3.2">𝐿</ci><list id="A4.p4.2.m2.2.2.2.3.cmml" xref="A4.p4.2.m2.2.2.2.4"><ci id="A4.p4.2.m2.1.1.1.1.cmml" xref="A4.p4.2.m2.1.1.1.1">𝑞</ci><ci id="A4.p4.2.m2.2.2.2.2.cmml" xref="A4.p4.2.m2.2.2.2.2">𝑖</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.2.m2.2c">L_{q,i}</annotation></semantics></math> for all questions with <math id="A4.p4.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A4.p4.3.m3.1a"><mi id="A4.p4.3.m3.1.1" xref="A4.p4.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A4.p4.3.m3.1b"><ci id="A4.p4.3.m3.1.1.cmml" xref="A4.p4.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.3.m3.1c">k</annotation></semantics></math>.</p>
</div>
<div id="A4.p5" class="ltx_para">
<p id="A4.p5.2" class="ltx_p">Then we pick <math id="A4.p5.1.m1.1" class="ltx_Math" alttext="N_{k}" display="inline"><semantics id="A4.p5.1.m1.1a"><msub id="A4.p5.1.m1.1.1" xref="A4.p5.1.m1.1.1.cmml"><mi id="A4.p5.1.m1.1.1.2" xref="A4.p5.1.m1.1.1.2.cmml">N</mi><mi id="A4.p5.1.m1.1.1.3" xref="A4.p5.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="A4.p5.1.m1.1b"><apply id="A4.p5.1.m1.1.1.cmml" xref="A4.p5.1.m1.1.1"><csymbol cd="ambiguous" id="A4.p5.1.m1.1.1.1.cmml" xref="A4.p5.1.m1.1.1">subscript</csymbol><ci id="A4.p5.1.m1.1.1.2.cmml" xref="A4.p5.1.m1.1.1.2">𝑁</ci><ci id="A4.p5.1.m1.1.1.3.cmml" xref="A4.p5.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p5.1.m1.1c">N_{k}</annotation></semantics></math> questions within each knowledge piece <math id="A4.p5.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A4.p5.2.m2.1a"><mi id="A4.p5.2.m2.1.1" xref="A4.p5.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A4.p5.2.m2.1b"><ci id="A4.p5.2.m2.1.1.cmml" xref="A4.p5.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p5.2.m2.1c">k</annotation></semantics></math>.</p>
</div>
<div id="A4.p6" class="ltx_para">
<table id="A4.Ex3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A4.Ex3.m1.2" class="ltx_Math" alttext="N_{k}=\lceil\alpha\times\lg(\#\text{questions of }k)\rceil" display="block"><semantics id="A4.Ex3.m1.2a"><mrow id="A4.Ex3.m1.2.2" xref="A4.Ex3.m1.2.2.cmml"><msub id="A4.Ex3.m1.2.2.3" xref="A4.Ex3.m1.2.2.3.cmml"><mi id="A4.Ex3.m1.2.2.3.2" xref="A4.Ex3.m1.2.2.3.2.cmml">N</mi><mi id="A4.Ex3.m1.2.2.3.3" xref="A4.Ex3.m1.2.2.3.3.cmml">k</mi></msub><mo id="A4.Ex3.m1.2.2.2" xref="A4.Ex3.m1.2.2.2.cmml">=</mo><mrow id="A4.Ex3.m1.2.2.1.1" xref="A4.Ex3.m1.2.2.1.2.cmml"><mo stretchy="false" id="A4.Ex3.m1.2.2.1.1.2" xref="A4.Ex3.m1.2.2.1.2.1.cmml">⌈</mo><mrow id="A4.Ex3.m1.2.2.1.1.1" xref="A4.Ex3.m1.2.2.1.1.1.cmml"><mi id="A4.Ex3.m1.2.2.1.1.1.3" xref="A4.Ex3.m1.2.2.1.1.1.3.cmml">α</mi><mo lspace="0.222em" rspace="0.222em" id="A4.Ex3.m1.2.2.1.1.1.2" xref="A4.Ex3.m1.2.2.1.1.1.2.cmml">×</mo><mrow id="A4.Ex3.m1.2.2.1.1.1.1.1" xref="A4.Ex3.m1.2.2.1.1.1.1.2.cmml"><mi id="A4.Ex3.m1.1.1" xref="A4.Ex3.m1.1.1.cmml">lg</mi><mo id="A4.Ex3.m1.2.2.1.1.1.1.1a" xref="A4.Ex3.m1.2.2.1.1.1.1.2.cmml">⁡</mo><mrow id="A4.Ex3.m1.2.2.1.1.1.1.1.1" xref="A4.Ex3.m1.2.2.1.1.1.1.2.cmml"><mo stretchy="false" id="A4.Ex3.m1.2.2.1.1.1.1.1.1.2" xref="A4.Ex3.m1.2.2.1.1.1.1.2.cmml">(</mo><mrow id="A4.Ex3.m1.2.2.1.1.1.1.1.1.1" xref="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.cmml"><mi mathvariant="normal" id="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.2" xref="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.2.cmml">#</mi><mo lspace="0em" rspace="0em" id="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.1" xref="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.1.cmml">​</mo><mtext id="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.3" xref="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.3a.cmml">questions of </mtext><mo lspace="0em" rspace="0em" id="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.1a" xref="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.1.cmml">​</mo><mi id="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.4" xref="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.4.cmml">k</mi></mrow><mo stretchy="false" id="A4.Ex3.m1.2.2.1.1.1.1.1.1.3" xref="A4.Ex3.m1.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="A4.Ex3.m1.2.2.1.1.3" xref="A4.Ex3.m1.2.2.1.2.1.cmml">⌉</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.Ex3.m1.2b"><apply id="A4.Ex3.m1.2.2.cmml" xref="A4.Ex3.m1.2.2"><eq id="A4.Ex3.m1.2.2.2.cmml" xref="A4.Ex3.m1.2.2.2"></eq><apply id="A4.Ex3.m1.2.2.3.cmml" xref="A4.Ex3.m1.2.2.3"><csymbol cd="ambiguous" id="A4.Ex3.m1.2.2.3.1.cmml" xref="A4.Ex3.m1.2.2.3">subscript</csymbol><ci id="A4.Ex3.m1.2.2.3.2.cmml" xref="A4.Ex3.m1.2.2.3.2">𝑁</ci><ci id="A4.Ex3.m1.2.2.3.3.cmml" xref="A4.Ex3.m1.2.2.3.3">𝑘</ci></apply><apply id="A4.Ex3.m1.2.2.1.2.cmml" xref="A4.Ex3.m1.2.2.1.1"><ceiling id="A4.Ex3.m1.2.2.1.2.1.cmml" xref="A4.Ex3.m1.2.2.1.1.2"></ceiling><apply id="A4.Ex3.m1.2.2.1.1.1.cmml" xref="A4.Ex3.m1.2.2.1.1.1"><times id="A4.Ex3.m1.2.2.1.1.1.2.cmml" xref="A4.Ex3.m1.2.2.1.1.1.2"></times><ci id="A4.Ex3.m1.2.2.1.1.1.3.cmml" xref="A4.Ex3.m1.2.2.1.1.1.3">𝛼</ci><apply id="A4.Ex3.m1.2.2.1.1.1.1.2.cmml" xref="A4.Ex3.m1.2.2.1.1.1.1.1"><ci id="A4.Ex3.m1.1.1.cmml" xref="A4.Ex3.m1.1.1">lg</ci><apply id="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.cmml" xref="A4.Ex3.m1.2.2.1.1.1.1.1.1.1"><times id="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.1"></times><ci id="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.2">#</ci><ci id="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.3a.cmml" xref="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.3"><mtext id="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.3">questions of </mtext></ci><ci id="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.4.cmml" xref="A4.Ex3.m1.2.2.1.1.1.1.1.1.1.4">𝑘</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.Ex3.m1.2c">N_{k}=\lceil\alpha\times\lg(\#\text{questions of }k)\rceil</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="A4.p6.1" class="ltx_p">where <math id="A4.p6.1.m1.1" class="ltx_Math" alttext="\alpha=3" display="inline"><semantics id="A4.p6.1.m1.1a"><mrow id="A4.p6.1.m1.1.1" xref="A4.p6.1.m1.1.1.cmml"><mi id="A4.p6.1.m1.1.1.2" xref="A4.p6.1.m1.1.1.2.cmml">α</mi><mo id="A4.p6.1.m1.1.1.1" xref="A4.p6.1.m1.1.1.1.cmml">=</mo><mn id="A4.p6.1.m1.1.1.3" xref="A4.p6.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.p6.1.m1.1b"><apply id="A4.p6.1.m1.1.1.cmml" xref="A4.p6.1.m1.1.1"><eq id="A4.p6.1.m1.1.1.1.cmml" xref="A4.p6.1.m1.1.1.1"></eq><ci id="A4.p6.1.m1.1.1.2.cmml" xref="A4.p6.1.m1.1.1.2">𝛼</ci><cn type="integer" id="A4.p6.1.m1.1.1.3.cmml" xref="A4.p6.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p6.1.m1.1c">\alpha=3</annotation></semantics></math> is a customized parameter.</p>
</div>
<div id="A4.p7" class="ltx_para">
<p id="A4.p7.1" class="ltx_p">Now we sort <math id="A4.p7.1.m1.2" class="ltx_Math" alttext="L_{q,k}={L_{q}:q\in k}" display="inline"><semantics id="A4.p7.1.m1.2a"><mrow id="A4.p7.1.m1.2.3" xref="A4.p7.1.m1.2.3.cmml"><mrow id="A4.p7.1.m1.2.3.2" xref="A4.p7.1.m1.2.3.2.cmml"><msub id="A4.p7.1.m1.2.3.2.2" xref="A4.p7.1.m1.2.3.2.2.cmml"><mi id="A4.p7.1.m1.2.3.2.2.2" xref="A4.p7.1.m1.2.3.2.2.2.cmml">L</mi><mrow id="A4.p7.1.m1.2.2.2.4" xref="A4.p7.1.m1.2.2.2.3.cmml"><mi id="A4.p7.1.m1.1.1.1.1" xref="A4.p7.1.m1.1.1.1.1.cmml">q</mi><mo id="A4.p7.1.m1.2.2.2.4.1" xref="A4.p7.1.m1.2.2.2.3.cmml">,</mo><mi id="A4.p7.1.m1.2.2.2.2" xref="A4.p7.1.m1.2.2.2.2.cmml">k</mi></mrow></msub><mo id="A4.p7.1.m1.2.3.2.1" xref="A4.p7.1.m1.2.3.2.1.cmml">=</mo><msub id="A4.p7.1.m1.2.3.2.3" xref="A4.p7.1.m1.2.3.2.3.cmml"><mi id="A4.p7.1.m1.2.3.2.3.2" xref="A4.p7.1.m1.2.3.2.3.2.cmml">L</mi><mi id="A4.p7.1.m1.2.3.2.3.3" xref="A4.p7.1.m1.2.3.2.3.3.cmml">q</mi></msub></mrow><mo lspace="0.278em" rspace="0.278em" id="A4.p7.1.m1.2.3.1" xref="A4.p7.1.m1.2.3.1.cmml">:</mo><mrow id="A4.p7.1.m1.2.3.3" xref="A4.p7.1.m1.2.3.3.cmml"><mi id="A4.p7.1.m1.2.3.3.2" xref="A4.p7.1.m1.2.3.3.2.cmml">q</mi><mo id="A4.p7.1.m1.2.3.3.1" xref="A4.p7.1.m1.2.3.3.1.cmml">∈</mo><mi id="A4.p7.1.m1.2.3.3.3" xref="A4.p7.1.m1.2.3.3.3.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.p7.1.m1.2b"><apply id="A4.p7.1.m1.2.3.cmml" xref="A4.p7.1.m1.2.3"><ci id="A4.p7.1.m1.2.3.1.cmml" xref="A4.p7.1.m1.2.3.1">:</ci><apply id="A4.p7.1.m1.2.3.2.cmml" xref="A4.p7.1.m1.2.3.2"><eq id="A4.p7.1.m1.2.3.2.1.cmml" xref="A4.p7.1.m1.2.3.2.1"></eq><apply id="A4.p7.1.m1.2.3.2.2.cmml" xref="A4.p7.1.m1.2.3.2.2"><csymbol cd="ambiguous" id="A4.p7.1.m1.2.3.2.2.1.cmml" xref="A4.p7.1.m1.2.3.2.2">subscript</csymbol><ci id="A4.p7.1.m1.2.3.2.2.2.cmml" xref="A4.p7.1.m1.2.3.2.2.2">𝐿</ci><list id="A4.p7.1.m1.2.2.2.3.cmml" xref="A4.p7.1.m1.2.2.2.4"><ci id="A4.p7.1.m1.1.1.1.1.cmml" xref="A4.p7.1.m1.1.1.1.1">𝑞</ci><ci id="A4.p7.1.m1.2.2.2.2.cmml" xref="A4.p7.1.m1.2.2.2.2">𝑘</ci></list></apply><apply id="A4.p7.1.m1.2.3.2.3.cmml" xref="A4.p7.1.m1.2.3.2.3"><csymbol cd="ambiguous" id="A4.p7.1.m1.2.3.2.3.1.cmml" xref="A4.p7.1.m1.2.3.2.3">subscript</csymbol><ci id="A4.p7.1.m1.2.3.2.3.2.cmml" xref="A4.p7.1.m1.2.3.2.3.2">𝐿</ci><ci id="A4.p7.1.m1.2.3.2.3.3.cmml" xref="A4.p7.1.m1.2.3.2.3.3">𝑞</ci></apply></apply><apply id="A4.p7.1.m1.2.3.3.cmml" xref="A4.p7.1.m1.2.3.3"><in id="A4.p7.1.m1.2.3.3.1.cmml" xref="A4.p7.1.m1.2.3.3.1"></in><ci id="A4.p7.1.m1.2.3.3.2.cmml" xref="A4.p7.1.m1.2.3.3.2">𝑞</ci><ci id="A4.p7.1.m1.2.3.3.3.cmml" xref="A4.p7.1.m1.2.3.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p7.1.m1.2c">L_{q,k}={L_{q}:q\in k}</annotation></semantics></math> in descendent order.</p>
</div>
<div id="A4.p8" class="ltx_para">
<p id="A4.p8.1" class="ltx_p">Then we assign a pick-up probability to select these questions</p>
<table id="A4.Ex4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A4.Ex4.m1.5" class="ltx_math_unparsed" alttext="Pr[\text{pick up }q]=\begin{cases}1\text{ , for }q\text{ s.t. }L_{q,k}[0]\\
p\text{ , if }q=1\text{ , for }q\text{ of }L_{q,k}[1:m]\\
\quad\quad\quad\quad\quad\quad\quad\text{ or }L_{q,k}[-m:]\\
p\frac{N_{k}-2m}{\#\text{questions of }k}\text{ , otherwise}\end{cases}" display="block"><semantics id="A4.Ex4.m1.5a"><mrow id="A4.Ex4.m1.5.5"><mrow id="A4.Ex4.m1.5.5.1"><mi id="A4.Ex4.m1.5.5.1.3">P</mi><mo lspace="0em" rspace="0em" id="A4.Ex4.m1.5.5.1.2">​</mo><mi id="A4.Ex4.m1.5.5.1.4">r</mi><mo lspace="0em" rspace="0em" id="A4.Ex4.m1.5.5.1.2a">​</mo><mrow id="A4.Ex4.m1.5.5.1.1.1"><mo stretchy="false" id="A4.Ex4.m1.5.5.1.1.1.2">[</mo><mrow id="A4.Ex4.m1.5.5.1.1.1.1"><mtext id="A4.Ex4.m1.5.5.1.1.1.1.2">pick up </mtext><mo lspace="0em" rspace="0em" id="A4.Ex4.m1.5.5.1.1.1.1.1">​</mo><mi id="A4.Ex4.m1.5.5.1.1.1.1.3">q</mi></mrow><mo stretchy="false" id="A4.Ex4.m1.5.5.1.1.1.3">]</mo></mrow></mrow><mo id="A4.Ex4.m1.5.5.2">=</mo><mrow id="A4.Ex4.m1.4.4"><mo id="A4.Ex4.m1.4.4.5">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="A4.Ex4.m1.4.4.4"><mtr id="A4.Ex4.m1.4.4.4a"><mtd class="ltx_align_left" columnalign="left" id="A4.Ex4.m1.4.4.4b"><mrow id="A4.Ex4.m1.1.1.1.1.1.1"><mn id="A4.Ex4.m1.1.1.1.1.1.1.5">1</mn><mo lspace="0em" rspace="0em" id="A4.Ex4.m1.1.1.1.1.1.1.4">​</mo><mtext id="A4.Ex4.m1.1.1.1.1.1.1.6"> , for </mtext><mo lspace="0em" rspace="0em" id="A4.Ex4.m1.1.1.1.1.1.1.4a">​</mo><mi id="A4.Ex4.m1.1.1.1.1.1.1.7">q</mi><mo lspace="0em" rspace="0em" id="A4.Ex4.m1.1.1.1.1.1.1.4b">​</mo><mtext id="A4.Ex4.m1.1.1.1.1.1.1.8"> s.t. </mtext><mo lspace="0em" rspace="0em" id="A4.Ex4.m1.1.1.1.1.1.1.4c">​</mo><msub id="A4.Ex4.m1.1.1.1.1.1.1.9"><mi id="A4.Ex4.m1.1.1.1.1.1.1.9.2">L</mi><mrow id="A4.Ex4.m1.1.1.1.1.1.1.2.2.4"><mi id="A4.Ex4.m1.1.1.1.1.1.1.1.1.1">q</mi><mo id="A4.Ex4.m1.1.1.1.1.1.1.2.2.4.1">,</mo><mi id="A4.Ex4.m1.1.1.1.1.1.1.2.2.2">k</mi></mrow></msub><mo lspace="0em" rspace="0em" id="A4.Ex4.m1.1.1.1.1.1.1.4d">​</mo><mrow id="A4.Ex4.m1.1.1.1.1.1.1.10.2"><mo stretchy="false" id="A4.Ex4.m1.1.1.1.1.1.1.10.2.1">[</mo><mn id="A4.Ex4.m1.1.1.1.1.1.1.3">0</mn><mo stretchy="false" id="A4.Ex4.m1.1.1.1.1.1.1.10.2.2">]</mo></mrow></mrow></mtd><mtd id="A4.Ex4.m1.4.4.4c"></mtd></mtr><mtr id="A4.Ex4.m1.4.4.4d"><mtd class="ltx_align_left" columnalign="left" id="A4.Ex4.m1.4.4.4e"><mrow id="A4.Ex4.m1.2.2.2.2.1.1"><mi id="A4.Ex4.m1.2.2.2.2.1.1.3">p</mi><mtext id="A4.Ex4.m1.2.2.2.2.1.1.4"> , if </mtext><mi id="A4.Ex4.m1.2.2.2.2.1.1.5">q</mi><mo id="A4.Ex4.m1.2.2.2.2.1.1.6">=</mo><mn id="A4.Ex4.m1.2.2.2.2.1.1.7">1</mn><mtext id="A4.Ex4.m1.2.2.2.2.1.1.8"> , for </mtext><mi id="A4.Ex4.m1.2.2.2.2.1.1.9">q</mi><mtext id="A4.Ex4.m1.2.2.2.2.1.1.10"> of </mtext><msub id="A4.Ex4.m1.2.2.2.2.1.1.11"><mi id="A4.Ex4.m1.2.2.2.2.1.1.11.2">L</mi><mrow id="A4.Ex4.m1.2.2.2.2.1.1.2.2.4"><mi id="A4.Ex4.m1.2.2.2.2.1.1.1.1.1">q</mi><mo id="A4.Ex4.m1.2.2.2.2.1.1.2.2.4.1">,</mo><mi id="A4.Ex4.m1.2.2.2.2.1.1.2.2.2">k</mi></mrow></msub><mrow id="A4.Ex4.m1.2.2.2.2.1.1.12"><mo stretchy="false" id="A4.Ex4.m1.2.2.2.2.1.1.12.1">[</mo><mn id="A4.Ex4.m1.2.2.2.2.1.1.12.2">1</mn><mo lspace="0.278em" rspace="0.278em" id="A4.Ex4.m1.2.2.2.2.1.1.12.3">:</mo><mi id="A4.Ex4.m1.2.2.2.2.1.1.12.4">m</mi><mo stretchy="false" id="A4.Ex4.m1.2.2.2.2.1.1.12.5">]</mo></mrow></mrow></mtd><mtd id="A4.Ex4.m1.4.4.4f"></mtd></mtr><mtr id="A4.Ex4.m1.4.4.4g"><mtd class="ltx_align_left" columnalign="left" id="A4.Ex4.m1.4.4.4h"><mrow id="A4.Ex4.m1.3.3.3.3.1.1"><mtext id="A4.Ex4.m1.3.3.3.3.1.1.3"> or </mtext><msub id="A4.Ex4.m1.3.3.3.3.1.1.4"><mi id="A4.Ex4.m1.3.3.3.3.1.1.4.2">L</mi><mrow id="A4.Ex4.m1.3.3.3.3.1.1.2.2.4"><mi id="A4.Ex4.m1.3.3.3.3.1.1.1.1.1">q</mi><mo id="A4.Ex4.m1.3.3.3.3.1.1.2.2.4.1">,</mo><mi id="A4.Ex4.m1.3.3.3.3.1.1.2.2.2">k</mi></mrow></msub><mrow id="A4.Ex4.m1.3.3.3.3.1.1.5"><mo stretchy="false" id="A4.Ex4.m1.3.3.3.3.1.1.5.1">[</mo><mo lspace="0em" id="A4.Ex4.m1.3.3.3.3.1.1.5.2">−</mo><mi id="A4.Ex4.m1.3.3.3.3.1.1.5.3">m</mi><mo lspace="0.278em" rspace="0em" id="A4.Ex4.m1.3.3.3.3.1.1.5.4">:</mo><mo stretchy="false" id="A4.Ex4.m1.3.3.3.3.1.1.5.5">]</mo></mrow></mrow></mtd><mtd id="A4.Ex4.m1.4.4.4i"></mtd></mtr><mtr id="A4.Ex4.m1.4.4.4j"><mtd class="ltx_align_left" columnalign="left" id="A4.Ex4.m1.4.4.4k"><mrow id="A4.Ex4.m1.4.4.4.4.1.1"><mi id="A4.Ex4.m1.4.4.4.4.1.1.2">p</mi><mo lspace="0em" rspace="0em" id="A4.Ex4.m1.4.4.4.4.1.1.1">​</mo><mstyle displaystyle="false" id="A4.Ex4.m1.4.4.4.4.1.1.3"><mfrac id="A4.Ex4.m1.4.4.4.4.1.1.3a"><mrow id="A4.Ex4.m1.4.4.4.4.1.1.3.2"><msub id="A4.Ex4.m1.4.4.4.4.1.1.3.2.2"><mi id="A4.Ex4.m1.4.4.4.4.1.1.3.2.2.2">N</mi><mi id="A4.Ex4.m1.4.4.4.4.1.1.3.2.2.3">k</mi></msub><mo id="A4.Ex4.m1.4.4.4.4.1.1.3.2.1">−</mo><mrow id="A4.Ex4.m1.4.4.4.4.1.1.3.2.3"><mn id="A4.Ex4.m1.4.4.4.4.1.1.3.2.3.2">2</mn><mo lspace="0em" rspace="0em" id="A4.Ex4.m1.4.4.4.4.1.1.3.2.3.1">​</mo><mi id="A4.Ex4.m1.4.4.4.4.1.1.3.2.3.3">m</mi></mrow></mrow><mrow id="A4.Ex4.m1.4.4.4.4.1.1.3.3"><mi mathvariant="normal" id="A4.Ex4.m1.4.4.4.4.1.1.3.3.2">#</mi><mo lspace="0em" rspace="0em" id="A4.Ex4.m1.4.4.4.4.1.1.3.3.1">​</mo><mtext id="A4.Ex4.m1.4.4.4.4.1.1.3.3.3">questions of </mtext><mo lspace="0em" rspace="0em" id="A4.Ex4.m1.4.4.4.4.1.1.3.3.1a">​</mo><mi id="A4.Ex4.m1.4.4.4.4.1.1.3.3.4">k</mi></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em" id="A4.Ex4.m1.4.4.4.4.1.1.1a">​</mo><mtext id="A4.Ex4.m1.4.4.4.4.1.1.4"> , otherwise</mtext></mrow></mtd><mtd id="A4.Ex4.m1.4.4.4l"></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex" id="A4.Ex4.m1.5b">Pr[\text{pick up }q]=\begin{cases}1\text{ , for }q\text{ s.t. }L_{q,k}[0]\\
p\text{ , if }q=1\text{ , for }q\text{ of }L_{q,k}[1:m]\\
\quad\quad\quad\quad\quad\quad\quad\text{ or }L_{q,k}[-m:]\\
p\frac{N_{k}-2m}{\#\text{questions of }k}\text{ , otherwise}\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Data Process and Annotation</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">Initially, we extract a total of 2.7 million questions from the internet. Through an algorithmic selection in the preprocessing stage, we narrow this down to 18,000 questions with wide coverage. During the construction, we conduct two rounds of data annotation and three rounds of automatic checking to ensure the granularity and credibility of every question in our set. In the first round of annotation, we filter out and modify questions based on predefined criteria. The second round of data annotation focuses more on semantic analysis and data enhancement. This post-processing stage significantly increases the number of MA questions by 3.22 times, and the total point proportion of non-SA questions rose from 26.0% to 40.1%. We also remove over 800 similar questions.</p>
</div>
<section id="A5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Data Pre-process</h3>

<div id="A5.SS1.p1" class="ltx_para">
<p id="A5.SS1.p1.1" class="ltx_p">The raw data range from HTML, photocopy, hand script, and plain text, and we conduct pre-processing to reduce the load of further annotation. We remove most HTML tags indicating irrelevant content of the question such as alignment, color, etc. We reserve tags for underlines (<span id="A5.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">&lt;u&gt; &lt;/u&gt;</span>), and we transfer several tagged styles including bold, italic, and tabular data into markdown format. For some complex tables that cannot be well converted, we save them as a screenshot picture after rendering with HTML.</p>
</div>
<div id="A5.SS1.p2" class="ltx_para">
<p id="A5.SS1.p2.1" class="ltx_p">For photocopy and hand script, we adopt OCR tools to convert text content, crop images, and figures, and integrate them into markdown. We further transcript most of the math functions and chemistry structures into <span id="A5.SS1.p2.1.1" class="ltx_text ltx_LaTeX_logo" style="letter-spacing:-0.2em; margin-right:0.1em;">L<span id="A5.SS1.p2.1.1.1" class="ltx_text" style="position:relative; bottom:0.4ex;font-variant:small-caps;;">a</span>T<span id="A5.SS1.p2.1.1.2" class="ltx_text" style="position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;">e</span>X</span> format, with a small portion remaining as images.</p>
</div>
</section>
<section id="A5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2 </span>Data Annotation</h3>

<figure id="A5.F6" class="ltx_figure"><img src="/html/2402.03173/assets/x6.png" id="A5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="298" height="208" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>A screenshot for the main page of the data annotation platform.</figcaption>
</figure>
<div id="A5.SS2.p1" class="ltx_para">
<p id="A5.SS2.p1.1" class="ltx_p">We develop an online platform for data annotation stage. The platform consists of text boxes for editing contents and regions for rendering the text to see the final appearance of the data as shown in <a href="#A5.F6" title="In E.2 Data Annotation ‣ Appendix E Data Process and Annotation ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a>. We employ skilled human annotators and involve them as authors, primarily undergraduate students from top universities in China familiar with exam quizzes and markdown rules, to undertake this comprehensive task covering various aspects from formatting to semantic analysis:</p>
<ul id="A5.I1" class="ltx_itemize">
<li id="A5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A5.I1.i1.p1" class="ltx_para">
<p id="A5.I1.i1.p1.1" class="ltx_p"><span id="A5.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Format Level</span>. Tasks at this level involve the removal of residual HTML tags and the conversion of content into markdown format (refer to examples (1) and (3) in <a href="#A5.F7" title="In E.2 Data Annotation ‣ Appendix E Data Process and Annotation ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a>). This includes transforming complex mathematical and chemical equations, usually in HTML, into <span id="A5.I1.i1.p1.1.2" class="ltx_text ltx_LaTeX_logo" style="letter-spacing:-0.2em; margin-right:0.1em;">L<span id="A5.I1.i1.p1.1.2.1" class="ltx_text" style="position:relative; bottom:0.4ex;font-variant:small-caps;;">a</span>T<span id="A5.I1.i1.p1.1.2.2" class="ltx_text" style="position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;">e</span>X</span>. For this purpose, Mathpix <span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://mathpix.com/snipping-tool" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://mathpix.com/snipping-tool</a></span></span></span> is utilized for efficiency. The annotators also correct any character-level errors in text and formulas, often resulting from OCR inaccuracies.</p>
</div>
</li>
<li id="A5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A5.I1.i2.p1" class="ltx_para">
<p id="A5.I1.i2.p1.1" class="ltx_p"><span id="A5.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Content Level</span>. Annotators split the raw content into distinct sub-questions, segregating parts like the question, answer, and analysis (if presented in raw data). We divide the question content into general and specific parts. The general part includes the problem introduction, background information, or instructions applicable across all sub-questions, while the specific part contains details unique to each sub-question. Annotators also standardize typesetting and image placement, ensuring a consistent format across questions of the same type (e.g., for multiple-choice questions with a single image, the format follows <span id="A5.I1.i2.p1.1.2" class="ltx_text ltx_font_typewriter">problem content(general) + question content(specific) + [MASK] + [IMAGE_1] + choices</span>).</p>
</div>
</li>
<li id="A5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A5.I1.i3.p1" class="ltx_para">
<p id="A5.I1.i3.p1.1" class="ltx_p"><span id="A5.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Label Level</span>. Annotators evaluate each question’s difficulty and quality. A question is considered of higher quality if it includes comprehensive content, multiple images, or detailed explanations. Difficulty assessment is subjective. These evaluations aid in curating our <span id="A5.I1.i3.p1.1.2" class="ltx_text ltx_font_smallcaps">Multi-Elite</span> dataset. Annotators also verify information like question type, educational level, and related knowledge pieces.</p>
</div>
</li>
<li id="A5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A5.I1.i4.p1" class="ltx_para">
<p id="A5.I1.i4.p1.1" class="ltx_p"><span id="A5.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Semantic Level</span>. At this stage, annotators are advised to identify and correct both superficial errors (e.g., empty/duplicate choices, incomplete mathematical functions such as between <span id="A5.I1.i4.p1.1.2" class="ltx_text ltx_font_typewriter">$32$</span>, <span id="A5.I1.i4.p1.1.3" class="ltx_text ltx_font_typewriter">$3^2$</span>, <span id="A5.I1.i4.p1.1.4" class="ltx_text ltx_font_typewriter">$\sqrt[3]{2}$</span>, <span id="A5.I1.i4.p1.1.5" class="ltx_text ltx_font_typewriter">$3\sqrt{2}$</span>, <span id="A5.I1.i4.p1.1.6" class="ltx_text ltx_font_typewriter">$\frac{3}{2}$</span>) and more profound errors relating to factual accuracy and logical reasoning, such as content that is lacking or leads to inconclusive results. Those questions with profound errors will be dropped.</p>
</div>
</li>
</ul>
</div>
<div id="A5.SS2.p2" class="ltx_para">
<p id="A5.SS2.p2.1" class="ltx_p">In <a href="#A5.F7" title="In E.2 Data Annotation ‣ Appendix E Data Process and Annotation ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a>, we show several examples of complex formation and modification during the data annotation stage. The markdown, <span id="A5.SS2.p2.1.1" class="ltx_text ltx_LaTeX_logo" style="letter-spacing:-0.2em; margin-right:0.1em;">L<span id="A5.SS2.p2.1.1.1" class="ltx_text" style="position:relative; bottom:0.4ex;font-variant:small-caps;;">a</span>T<span id="A5.SS2.p2.1.1.2" class="ltx_text" style="position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;">e</span>X</span>, and HTML format code are remained for better format clarity.</p>
</div>
<figure id="A5.F7" class="ltx_figure"><img src="/html/2402.03173/assets/x7.png" id="A5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="298" height="211" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Several data annotation examples when constructing <span id="A5.F7.2.1" class="ltx_text ltx_font_smallcaps">Multi</span>. </figcaption>
</figure>
</section>
<section id="A5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.3 </span>Data Post-process</h3>

<div id="A5.SS3.p1" class="ltx_para">
<p id="A5.SS3.p1.1" class="ltx_p">To collect more challenging data for our benchmark, we adopt several data post-process strategies:</p>
<ul id="A5.I2" class="ltx_itemize">
<li id="A5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A5.I2.i1.p1" class="ltx_para">
<p id="A5.I2.i1.p1.1" class="ltx_p"><span id="A5.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Formation</span>. During the data preprocessing stage and annotation stage, we format the questions in a render-friendly manner, and meanwhile reduce the similarity to contents that the MLLMs are trained on. During this stage, we assess if there are any omissions or missing elements.</p>
</div>
</li>
<li id="A5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A5.I2.i2.p1" class="ltx_para">
<p id="A5.I2.i2.p1.1" class="ltx_p"><span id="A5.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Disambiguration</span>. For blank-filling questions containing multiple <span id="A5.I2.i2.p1.1.2" class="ltx_text ltx_font_typewriter">[MASK]</span>s, we manually modify those with parallel relations into two sub-questions (refer to example (5) in <a href="#A5.F8" title="In E.3 Data Post-process ‣ Appendix E Data Process and Annotation ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a>) to determine a unique fixed answer for each question.</p>
</div>
</li>
<li id="A5.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A5.I2.i3.p1" class="ltx_para">
<p id="A5.I2.i3.p1.1" class="ltx_p"><span id="A5.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Distillation</span>. This is completed during our annotation process. We reduce assistance information so that the answer must depend on more detailed analysis (refer to example (4) in <a href="#A5.F8" title="In E.3 Data Post-process ‣ Appendix E Data Process and Annotation ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a>). In this way, we greatly enhance question difficulty.</p>
</div>
</li>
<li id="A5.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A5.I2.i4.p1" class="ltx_para">
<p id="A5.I2.i4.p1.1" class="ltx_p"><span id="A5.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">Transformation</span>. We randomly modify the questions such as from single-choice to blank-filling (refer to example (2) in <a href="#A5.F8" title="In E.3 Data Post-process ‣ Appendix E Data Process and Annotation ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a>), or convert certain kinds of single-choice questions into multiple-choice ones (refer to example (1) and (5) in <a href="#A5.F8" title="In E.3 Data Post-process ‣ Appendix E Data Process and Annotation ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a>). Lots of single-choice questions have a list of options and the choices are presented as the combination of those options where only one is correct. We transform those questions into multiple-choice questions where the options become new choices and the correct answer corresponds to the combinations. In this way we successfully increase the scale of multiple-choice questions, improving the diversity of the questions.</p>
</div>
</li>
</ul>
</div>
<figure id="A5.F8" class="ltx_figure"><img src="/html/2402.03173/assets/x8.png" id="A5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="298" height="176" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Several data augmentation examples when constructing <span id="A5.F8.2.1" class="ltx_text ltx_font_smallcaps">Multi</span>.</figcaption>
</figure>
<div id="A5.SS3.p2" class="ltx_para">
<p id="A5.SS3.p2.1" class="ltx_p">In <a href="#A5.F8" title="In E.3 Data Post-process ‣ Appendix E Data Process and Annotation ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a>, we show several examples of complex formation and modification during the data postprocess stage. English translations of Chinese text are shown for better readability.</p>
</div>
</section>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>More Examples</h2>

<div id="A6.p1" class="ltx_para">
<p id="A6.p1.1" class="ltx_p">In <a href="#A6.F9" title="In Appendix F More Examples ‣ Multi: Multimodal Understanding Leaderboard with Text and Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9</span></a>, we show more examples for annotated questions. English translations of Chinese text are shown for better readability.</p>
</div>
<figure id="A6.F9" class="ltx_figure"><img src="/html/2402.03173/assets/x9.png" id="A6.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="298" height="224" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>More example of <span id="A6.F9.2.1" class="ltx_text ltx_font_smallcaps">Multi</span>.</figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2402.03172" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2402.03173" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2402.03173">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.03173" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2402.03174" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 15:10:43 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
