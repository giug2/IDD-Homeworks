<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.14120] Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning (Corresponding author: Aryan Kaushik) This work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) under Grant RGPIN-2021-04050. The authors would also like to thank Denvr Dataworks, Calgary, Canada for their high-performance compute used to conduct this research.</title><meta property="og:description" content="The industrial Internet of Things (IIoT) under Industry 4.0 heralds an era of interconnected smart devices where data-driven insights and machine learning (ML) fuse to revolutionize manufacturing. A noteworthy developm…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning (Corresponding author: Aryan Kaushik) This work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) under Grant RGPIN-2021-04050. The authors would also like to thank Denvr Dataworks, Calgary, Canada for their high-performance compute used to conduct this research.">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning (Corresponding author: Aryan Kaushik) This work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) under Grant RGPIN-2021-04050. The authors would also like to thank Denvr Dataworks, Calgary, Canada for their high-performance compute used to conduct this research.">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.14120">

<!--Generated on Fri Apr  5 16:52:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning
<span id="id1.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>(<span id="id1.id1.1" class="ltx_text ltx_font_italic">Corresponding author: Aryan Kaushik</span>)</span></span></span>
<span id="id2.id2" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>This work was supported by the Natural Sciences and Engineering
Research Council of Canada (NSERC) under Grant RGPIN-2021-04050. The authors would also like to thank Denvr Dataworks, Calgary, Canada for their high-performance compute used to conduct this research.
</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fazal Muhammad Ali Khan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Hatem Abou-Zeid
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Aryan Kaushik
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Syed Ali Hassan
</span><span class="ltx_author_notes">Fazal Muhammad Ali Khan is with the School of Electrical Engineering &amp; Computer Science (SEECS), National University of Sciences &amp; Technology (NUST), Islamabad, Pakistan, and also a visiting research scholar at the Electrical and Software Engineering Department, University of Calgary, Calgary, Alberta, Canada (email: fkhan.phd20seecs@seecs.edu.pk).Hatem Abou-Zeid is with the Electrical and Software Engineering Department, University of Calgary, Calgary, Alberta, Canada (email: hatem.abouzeid@ucalgary.ca).Aryan Kaushik is with the School of Engineering and Informatics, University of Sussex, Brighton, UK, (email: aryan.kaushik@sussex.ac.uk).Syed Ali Hassan is with the School of Electrical Engineering &amp; Computer Science (SEECS), National University of Sciences &amp; Technology (NUST), Islamabad, Pakistan (email: ali.hassan@seecs.edu.pk).
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p"><span id="id3.id1.1" class="ltx_text">The industrial Internet of Things (IIoT) under Industry 4.0 heralds an era of interconnected smart devices where data-driven insights and machine learning (ML) fuse to revolutionize manufacturing. A noteworthy development in IIoT is the integration of federated learning (FL), which addresses data privacy and security among devices. FL enables edge sensors, also known as peripheral intelligence units (PIUs) to learn and adapt using their data locally, without explicit sharing of confidential data, to facilitate a collaborative yet confidential learning process. However, the lower memory footprint and computational power of PIUs inherently require deep neural network (DNN) models that have a very compact size. Model compression techniques such as pruning can be used to reduce the size of DNN models by removing unnecessary connections that have little impact on the model’s performance, thus making the models more suitable for the limited resources of PIUs. Targeting the notion of compact yet robust DNN models, we propose the integration of iterative magnitude pruning (IMP) of the DNN model being trained in an over-the-air FL (OTA-FL) environment for IIoT. We provide a tutorial overview and also present a case study of the effectiveness of IMP in OTA-FL for an IIoT environment. Finally, we present future directions for enhancing and optimizing these deep compression techniques further, aiming to push the boundaries of IIoT capabilities in acquiring compact yet robust and high-performing DNN models.</span></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
<span id="id4.id1" class="ltx_text">
Federated learning (FL), over-the-air (OTA) computation, DNNs, industrial IoT (IIOT), compression.
</span>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The fourth phase of the industrial revolution, known as Industry 4.0, focuses on the digital overhaul across multiple fields, including smart manufacturing, advanced automation, and the aerospace industry. The aim is to integrate cutting-edge technologies to enhance efficiency and innovation, ushering a new level of connectivity and intelligence within the industries. This would foster the development of smart factories that can self-monitor, self-diagnose, and even predict future issues, leading to minimal downtime and increased productivity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Central to this transformation are intelligent edge devices, such as sensors and actuators, also known as peripheral intelligence units (PIUs), which are deployed extensively to gather, process, and disseminate information. Therefore, the core of these advancements lies in the PIUs, which facilitate predictive maintenance, energy management, and automated quality control in the industry.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The cornerstone of these devices becoming intelligent is the advent of machine learning (ML) models and deep neural networks (DNNs) that enable these devices to learn from past data, predict future conditions, and make informed decisions autonomously. As a result, PIUs have transitioned from being mere data collectors to smart, decision-making entities. This shift enhances the efficiency, reliability, and innovation of industrial operations, marking a significant milestone towards fully autonomous and intelligent industrial systems. Despite these benefits, the growth of PIUs and adoption of intelligence in the IIoT landscape face significant challenges due to data security, high-compute requirements of deep neural networks, and high energy consumption.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">DNN model compression techniques can however be adopted to reduce the complexity and storage requirements of large DNNs used in IIoT systems. Model pruning is one of the primary methods to achieve this by identifying and eliminating unnecessary or less important connections within the DNN. The pruning process can be conducted iteratively and can lead to significant reductions in model size and computational complexity without impacting model accuracy. This enables the widespread deployment of advanced AI systems on PIUs that have limited resources.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><svg id="S1.F1.pic1" class="ltx_picture ltx_centering" height="246.24" overflow="visible" version="1.1" width="611.47"><g transform="translate(0,246.24) matrix(1 0 0 -1 0 0) translate(305.73,0) translate(0,123.12)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M -305.46 -122.84 h 610.91 v 245.69 h -610.91 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -301.31 -117.31)" fill="#000000" stroke="#000000"><foreignObject width="602.61" height="236" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2403.14120/assets/fl_drawing_iiot5.png" id="S1.F1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="236" alt="Refer to caption"><span id="S1.F1.pic1.2.2.2.2.1" class="ltx_text">
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">A depiction of OTA FL in IIoT. The model updates of PIU obtained via training with available datasets are transmitted and aggregated over a wireless medium, leveraging the superposition property of wireless channels. This simultaneous transmission and aggregation significantly reduces communication overhead and latency. The process ensures privacy and efficiency, making it ideal for real-world applications where data privacy is crucial and bandwidth is limited.</span></figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Federated learning (FL) is a privacy-preserving distributed learning approach where model training occurs on decentralized devices. Each PIU trains a local model using its data, and only the <em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">model parameters</em> are transmitted to a central server for aggregation toward a global model. This enables collaborative learning in an IIoT system without requiring sensitive data transfers from the PIUs. FL therefore ensures privacy and security, a key concern in IIoT environments, making it an essential technology for advancing IIoT operations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Given the benefits of FL and DNN model compression, this article proposes adopting these strategies to address the aforementioned challenges. Compressed DNNs also further enable the adoption of FL in distributed low-cost PIUs for IIoT applications. We present a case study that demonstrates the efficacy of these methods in an IIoT environment where optimizing PIUs within the FL framework enhances system performance and security. With reduced DNN model size via compression, and privacy achieved via FL, a more robust and effective IIoT system can be achieved.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The rest of the paper is organized as follows: The next section will introduce a high-level discussion of FL. This is followed by a discussion of model compression, specifically addressing pruning and its common variants. We then present a case study that proposes the usage of FL and DNN model compression to acquire compact yet reliable DNN models for PIUs in IIoT. Next we present key future research directions to achieve even more robust and efficient FL industrial IoT systems. Finally, the conclusion of this article is provided.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Federated Learning at the Edge: OTA Aggregation Insights</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The following section introduces the system by discussing federated edge learning and OTA-aggregation.</p>
</div>
<section id="S2.SS0.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS0.SSS1.4.1.1" class="ltx_text">II-</span>1 </span>Federated Edge Learning</h4>

<div id="S2.SS0.SSS1.p1" class="ltx_para">
<p id="S2.SS0.SSS1.p1.1" class="ltx_p">FL a technique that intertwines wireless communication and ML is central to advancing the industrial IoT era. It involves training a DNN model, known as the global model, which is broadcast by a parameter server (PS) to a multitude of decentralized edge devices such as IoT sensors or PIUs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. For each PIU, the received global model now becomes a local model, independently trained with their local dataset. After training, only the gradient vectors (GVs) are sent back to the PS. The broadcasting, local training, and aggregation process continues until the global model reaches a desired level of accuracy, making FL an ideal solution for collaborative yet privacy-preserving computational intelligence techniques in a distributed learning environment.</p>
</div>
<div id="S2.SS0.SSS1.p2" class="ltx_para">
<p id="S2.SS0.SSS1.p2.1" class="ltx_p">In an FL network in IIoT, as shown in Figure <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning (Corresponding author: Aryan Kaushik) This work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) under Grant RGPIN-2021-04050. The authors would also like to thank Denvr Dataworks, Calgary, Canada for their high-performance compute used to conduct this research." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, multiple PIUs collaborate to train a DNN model, referred to as the global model. Initially, the PS broadcasts the global model to all PIUs, where each receives an identical version. Subsequently, every PIU trains this model with their respective local datasets. This personalizes the global model into a distinct local model at each PIU.</p>
</div>
<div id="S2.SS0.SSS1.p3" class="ltx_para">
<p id="S2.SS0.SSS1.p3.1" class="ltx_p">As each PIU has different data, it trains the model differently, resulting in considerable heterogeneity in training the model. With the completion of local training, the GVs of each local model are now transmitted OTA back to the PS. The PS thus receives an aggregated version of all GVs due to OTA aggregation.</p>
</div>
<div id="S2.SS0.SSS1.p4" class="ltx_para">
<p id="S2.SS0.SSS1.p4.1" class="ltx_p">In ML, a crucial concept is the ‘cost function’, a mathematical formulation used to measure the error or difference between the predicted outcomes of a model and the actual results. The FL process aims to find the best set of GVs that minimize this error, hence acquiring a global model that performs optimally in terms of accuracy. As this global model is trained with GVs reflecting the unique data and experiences from each PIU, it becomes more robust, enabling it to adapt effectively to the dynamic environment of industrial setups.</p>
</div>
<div id="S2.SS0.SSS1.p5" class="ltx_para">
<p id="S2.SS0.SSS1.p5.1" class="ltx_p">The process of transmitting and receiving local and global GVs continues until the cost function reaches the lowest possible value. Ideally, this set of GVs shall be achieved in as few FL epochs as possible. Reducing the cost function involves making use of optimization algorithms, one of which is stochastic gradient descent (SGD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Overall, this process highlights the collaborative yet decentralized nature of training a DNN model, where multiple PIUs contribute by maintaining their local data privacy.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><svg id="S2.F2.pic1" class="ltx_picture ltx_centering" height="515.24" overflow="visible" version="1.1" width="611.47"><g transform="translate(0,515.24) matrix(1 0 0 -1 0 0) translate(305.73,0) translate(0,257.62)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M -305.46 -257.34 h 610.91 v 514.69 h -610.91 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -301.31 -251.81)" fill="#000000" stroke="#000000"><foreignObject width="602.61" height="505" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2403.14120/assets/Drawing1_updated.png" id="S2.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="505" alt="Refer to caption"><span id="S2.F2.pic1.2.2.2.2.1" class="ltx_text">
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">Contrast between OSP and IMP of a DNN. The top sequence shows a network before and after OSP where a significant one-time reduction in connections is obtained after completion of model training. The bottom sequence illustrates the process of IMP, where the model is gradually pruned to the desired ratio, resulting in a selectively pruned network across multiple iterations.</span></figcaption>
</figure>
</section>
<section id="S2.SS0.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS0.SSS2.4.1.1" class="ltx_text">II-</span>2 </span>Over-the-Air (OTA) Aggregation and Wireless Medium</h4>

<div id="S2.SS0.SSS2.p1" class="ltx_para">
<p id="S2.SS0.SSS2.p1.1" class="ltx_p">OTA aggregation is an efficient technique for federated learning proposed in wireless networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
By leveraging the superposition property of wireless channels, OTA aggregation allows the PS to receive a composite signal, combining GVs from multiple PIUs. This process effectively consolidates data transmission, significantly reducing the need for bandwidth and processing and thereby alleviating the computational load at the PS. Particularly useful in settings like manufacturing plants, where PIUs collect vital data such as temperature, this method underscores the practical advantages in industrial environments by streamlining data aggregation and improving the efficiency of distributed learning systems.</p>
</div>
<div id="S2.SS0.SSS2.p2" class="ltx_para">
<p id="S2.SS0.SSS2.p2.1" class="ltx_p">In FL, OTA aggregation facilitates the reception of a unified GV set from all participating PIUs without requiring individual transmission resources for each PIU. In the context of the IIoT, where most transmissions involve OTA communication, these data exchanges become particularly susceptible to the complexities of wireless channels. Moreover, the additive white Gaussian noise (AWGN) introduced by PIUs can further degrade DNN model’s performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<figure id="S2.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F4.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;"><img src="/html/2403.14120/assets/x1.png" id="S2.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="415" height="311" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.1.1.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F4.1.2.2" class="ltx_text" style="font-size:90%;">OSP with full participation</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F4.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;"><img src="/html/2403.14120/assets/x2.png" id="S2.F4.2.g1" class="ltx_graphics ltx_img_landscape" width="415" height="311" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.2.1.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S2.F4.2.2.2" class="ltx_text" style="font-size:90%;">OSP with partial participation</span></figcaption>
</figure>
</div>
</div>
</figure>
<figure id="S2.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F6.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;"><img src="/html/2403.14120/assets/x3.png" id="S2.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="415" height="311" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F6.1.1.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S2.F6.1.2.2" class="ltx_text" style="font-size:90%;">IMP with full participation</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F6.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;"><img src="/html/2403.14120/assets/x4.png" id="S2.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="415" height="311" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F6.2.1.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S2.F6.2.2.2" class="ltx_text" style="font-size:90%;">IMP with partial participation</span></figcaption>
</figure>
</div>
</div>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Deep Neural Network Model Compression: Pruning for Efficiency</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">DNN model compression plays a crucial role in enhancing computational efficiency and reducing storage demands, particularly vital for deploying sophisticated ML algorithms on resource-limited PIUs within the IIoT landscape. The primary goal is to streamline DNNs by eliminating redundant parameters, thereby simplifying models without significantly compromising accuracy. Pruning and quantization stand out among compression techniques for their effectiveness in downsizing models while maintaining performance, aligning with the needs of PIUs that operate with constrained memory, bandwidth, and energy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Compression through Pruning</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Pruning cuts down the number of parameters and operations required for making predictions, leading to faster and more energy-efficient inference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Having significantly smaller models also enables edge-training in the PIUs which is a fundamental requirement for federated learning. Additionally, pruning helps in identifying and eliminating redundancies within the DNN, potentially enhancing model robustness and reducing the likelihood of overfitting. This makes pruning particularly valuable for IIoT applications where efficiency, reliability, and performance are paramount.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The process of pruning can be broadly categorized into two types: structured and unstructured. Unstructured pruning involves removing individual weights across the DNN based on certain criteria, such as the smallest magnitudes, which leads to a DNN with sparser weights. On the other hand, structured pruning removes entire neurons or channels, leading to reduced dimensions of layers and a more hardware-friendly model structure.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">A typical pruning process involves three main steps: training, pruning, and fine-tuning. Initially, the DNN is trained to learn the patterns necessary for performing a specific task. After training, the DNN is pruned by removing weights that contribute the least to the DNN’s output based on a predefined metric. Finally, the pruned DNN undergoes fine-tuning, where it is retrained to adjust the remaining weights and recover any lost performance due to pruning. Training and pruning can also happen iteratively as discussed below.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Hence implementing DNN model compression through pruning significantly reduces the size of the DNNs making it an invaluable technique in IIoT where PIUs have limited resources but the cost incurred may be the reduced performance of DNN. Two of the prominent pruning techniques are briefly described below.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.4.1.1" class="ltx_text">III-A</span>1 </span>One-shot Pruning (OSP)</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">OSP involves aggressive pruning of network connections after an initial training period <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. The fundamental premise of OSP is based on identifying and removing redundant or less significant connections within a DNN, which is achieved after a comprehensive training process as shown in Figure <a href="#S2.F2" title="Figure 2 ‣ II-1 Federated Edge Learning ‣ II Federated Learning at the Edge: OTA Aggregation Insights ‣ Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning (Corresponding author: Aryan Kaushik) This work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) under Grant RGPIN-2021-04050. The authors would also like to thank Denvr Dataworks, Calgary, Canada for their high-performance compute used to conduct this research." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The core idea is that a substantial number of the weights of the DNN model are not critical for maintaining its predictive performance and hence are redundant to the model. By pruning these superfluous connections, OSP significantly reduces the model’s size and complexity, leading to lower memory requirements and faster inference time.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">Due to its simple one-shot approach, OSP is prone to introducing loss in the model performance at a high pruning percentage. This is because of the pruning of connections that can potentially be important for the DNN. Furthermore, the static nature of OSP implies that it does not adapt or change in response to evolving data or environments, potentially limiting its efficacy in dynamic or complex scenarios.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.4.1.1" class="ltx_text">III-A</span>2 </span>Iterative Magnitude Pruning (IMP)</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">Unlike OSP, which prunes a network in a single step via an aggressive approach, IMP adopts a cyclic process of pruning and retraining to reduce the complexity of a DNN by repeatedly pruning a small percentage of the network’s least significant weights <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. This technique involves training a DNN to its full capacity, followed by the incremental removal of the least important weights based on their magnitudes. After each pruning step, the network is retrained to recover performance lost due to the pruning. As shown in Figure <a href="#S2.F2" title="Figure 2 ‣ II-1 Federated Edge Learning ‣ II Federated Learning at the Edge: OTA Aggregation Insights ‣ Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning (Corresponding author: Aryan Kaushik) This work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) under Grant RGPIN-2021-04050. The authors would also like to thank Denvr Dataworks, Calgary, Canada for their high-performance compute used to conduct this research." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, this cycle of pruning and retraining is iterated several times, allowing the network to adapt progressively and retain its essential connections <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">The criteria for pruning in each iteration is primarily based on the magnitude of the weights, similar to OSP. The effectiveness of IMP lies in its ability to incrementally discover an optimal subset of connections crucial for maintaining the network’s performance. The iterative nature of IMP enables a more nuanced and controlled pruning process, ultimately leading to a more efficient and compact DNN model that can maintain or even surpass the original network performance.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Case Study: Iterative Magnitude Pruning in OTA-FL for IIoT</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.9" class="ltx_p">To test the efficacy of IMP in OTA-FL for an IIoT network, this case study utilizes the ResNet18 DNN model which is trained on the CIFAR-10 dataset at the PIUs. This dataset has a total of <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="50000" display="inline"><semantics id="S4.p1.1.m1.1a"><mn id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">50000</mn><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><cn type="integer" id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">50000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">50000</annotation></semantics></math> RGB training and <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="10000" display="inline"><semantics id="S4.p1.2.m2.1a"><mn id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">10000</mn><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><cn type="integer" id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">10000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">10000</annotation></semantics></math> test images divided into 10 classes. The dataset is distributed to a total of <span id="S4.p1.9.1" class="ltx_text ltx_font_italic">E</span> = <math id="S4.p1.3.m3.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.p1.3.m3.1a"><mn id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><cn type="integer" id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">100</annotation></semantics></math> PIUs in an independent and identically distributed (i.i.d.) manner. The performance of the model is tested for both maximum, i.e., full participation (FP) of <math id="S4.p1.4.m4.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.p1.4.m4.1a"><mn id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><cn type="integer" id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">100</annotation></semantics></math> PIUs and partial participation (PP) of <math id="S4.p1.5.m5.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S4.p1.5.m5.1a"><mn id="S4.p1.5.m5.1.1" xref="S4.p1.5.m5.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S4.p1.5.m5.1b"><cn type="integer" id="S4.p1.5.m5.1.1.cmml" xref="S4.p1.5.m5.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.5.m5.1c">50</annotation></semantics></math> PIUs. Both IMP and OSP are performed at each PIU. Further, communication between PIUs and PS stops when the time <span id="S4.p1.9.2" class="ltx_text ltx_font_italic">t</span> reaches <math id="S4.p1.6.m6.1" class="ltx_Math" alttext="1000" display="inline"><semantics id="S4.p1.6.m6.1a"><mn id="S4.p1.6.m6.1.1" xref="S4.p1.6.m6.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="S4.p1.6.m6.1b"><cn type="integer" id="S4.p1.6.m6.1.1.cmml" xref="S4.p1.6.m6.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.6.m6.1c">1000</annotation></semantics></math> epochs, the upper limit chosen for this setup. The batch size utilized for DNNs in this work is <math id="S4.p1.7.m7.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S4.p1.7.m7.1a"><mn id="S4.p1.7.m7.1.1" xref="S4.p1.7.m7.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S4.p1.7.m7.1b"><cn type="integer" id="S4.p1.7.m7.1.1.cmml" xref="S4.p1.7.m7.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.7.m7.1c">64</annotation></semantics></math>, while the learning rate is <math id="S4.p1.8.m8.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S4.p1.8.m8.1a"><mn id="S4.p1.8.m8.1.1" xref="S4.p1.8.m8.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S4.p1.8.m8.1b"><cn type="float" id="S4.p1.8.m8.1.1.cmml" xref="S4.p1.8.m8.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.8.m8.1c">0.001</annotation></semantics></math>. The learning curves are provided for a signal-to-noise ratio (SNR) of <math id="S4.p1.9.m9.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.p1.9.m9.1a"><mn id="S4.p1.9.m9.1.1" xref="S4.p1.9.m9.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.p1.9.m9.1b"><cn type="integer" id="S4.p1.9.m9.1.1.cmml" xref="S4.p1.9.m9.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.9.m9.1c">10</annotation></semantics></math> dB.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Accuracy Results with OSP</span>
</h3>

<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS1.4.1.1" class="ltx_text">IV-A</span>1 </span>Full participation</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">Figure <a href="#S2.F4" title="Figure 4 ‣ II-2 Over-the-Air (OTA) Aggregation and Wireless Medium ‣ II Federated Learning at the Edge: OTA Aggregation Insights ‣ Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning (Corresponding author: Aryan Kaushik) This work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) under Grant RGPIN-2021-04050. The authors would also like to thank Denvr Dataworks, Calgary, Canada for their high-performance compute used to conduct this research." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> displays the result for full participation of PIUs, where the unpruned model reaches an accuracy of 90% while 30P (30% pruned) and 50P (50% pruned) manage to attain 86% and 81% accuracy, respectively. A significant drop in the accuracy is noted for 70P (70% pruned) as the model reaches an accuracy of roughly 74%. Again, among the pruned models, 90P (90% pruned) roughly reaches 59% accuracy at the final epoch, which is a significant decrease in the model performance in terms of accuracy. In terms of size reduction, we note a significant reduction as the pruning ratio increases. From unpruned model size of 44.65 Megabytes (MBs), the 30P acquires a size of 32.73 MBs, 50P has a size of 22.39 MBs, while the 70P and 90P manage to acquire sizes of 13.28 and 4.78 MBs, respectively.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS2.4.1.1" class="ltx_text">IV-A</span>2 </span>Partial Participation</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">When the participation from PIUs is reduced to half, i.e., <span id="S4.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_italic">E</span> = <math id="S4.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S4.SS1.SSS2.p1.1.m1.1a"><mn id="S4.SS1.SSS2.p1.1.m1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.1.m1.1b"><cn type="integer" id="S4.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.1.m1.1c">50</annotation></semantics></math>, the result for accuracy performance for ResNet18 further reduces as compared to the full participation as evident from Figure <a href="#S2.F4" title="Figure 4 ‣ II-2 Over-the-Air (OTA) Aggregation and Wireless Medium ‣ II Federated Learning at the Edge: OTA Aggregation Insights ‣ Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning (Corresponding author: Aryan Kaushik) This work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) under Grant RGPIN-2021-04050. The authors would also like to thank Denvr Dataworks, Calgary, Canada for their high-performance compute used to conduct this research." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The unpruned model rests with 80 % accuracy, while 30P and 50P maintain little difference in accuracies, reaching roughly 73 and 70% accuracy at the final epoch, respectively. While fairly stable convergence can be noted for 70P, the 50P, with notably slow convergence, reaches an accuracy of 52%.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Accuracy Results with Iterative Magnitude Pruning</span>
</h3>

<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS1.4.1.1" class="ltx_text">IV-B</span>1 </span>Full Participation</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.3" class="ltx_p">Figure <a href="#S2.F6" title="Figure 6 ‣ II-2 Over-the-Air (OTA) Aggregation and Wireless Medium ‣ II Federated Learning at the Edge: OTA Aggregation Insights ‣ Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning (Corresponding author: Aryan Kaushik) This work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) under Grant RGPIN-2021-04050. The authors would also like to thank Denvr Dataworks, Calgary, Canada for their high-performance compute used to conduct this research." class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> displays remarkable improvement in accuracy for all the pruned versions of ResNet18 DNN when IMP is employed. It can be observed that 30P manages to even surpass the accuracy of the unpruned model. 50P also manages to perform almost at par with the unpruned model, while 70P moves from <math id="S4.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="74" display="inline"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><mn id="S4.SS2.SSS1.p1.1.m1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.cmml">74</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.1b"><cn type="integer" id="S4.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1">74</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.1c">74</annotation></semantics></math>% with OSP to almost <math id="S4.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="85" display="inline"><semantics id="S4.SS2.SSS1.p1.2.m2.1a"><mn id="S4.SS2.SSS1.p1.2.m2.1.1" xref="S4.SS2.SSS1.p1.2.m2.1.1.cmml">85</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.2.m2.1b"><cn type="integer" id="S4.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1">85</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.2.m2.1c">85</annotation></semantics></math>%. Similarly, 90P also shows a gain of roughly <math id="S4.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="23" display="inline"><semantics id="S4.SS2.SSS1.p1.3.m3.1a"><mn id="S4.SS2.SSS1.p1.3.m3.1.1" xref="S4.SS2.SSS1.p1.3.m3.1.1.cmml">23</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.3.m3.1b"><cn type="integer" id="S4.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.1">23</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.3.m3.1c">23</annotation></semantics></math>% versus basic OSP.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS2.4.1.1" class="ltx_text">IV-B</span>2 </span>Partial Participation</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.4" class="ltx_p">A similar improvement in accuracy for all the pruned models can be observed for ResNet18 in Figure <a href="#S2.F6" title="Figure 6 ‣ II-2 Over-the-Air (OTA) Aggregation and Wireless Medium ‣ II Federated Learning at the Edge: OTA Aggregation Insights ‣ Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning (Corresponding author: Aryan Kaushik) This work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) under Grant RGPIN-2021-04050. The authors would also like to thank Denvr Dataworks, Calgary, Canada for their high-performance compute used to conduct this research." class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, where 30P surpasses the accuracy of the unpruned model. This notes an accuracy improvement of roughly <math id="S4.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S4.SS2.SSS2.p1.1.m1.1a"><mn id="S4.SS2.SSS2.p1.1.m1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.1b"><cn type="integer" id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.1c">8</annotation></semantics></math>% from the previous strategy. 50P, though with slower convergence, manages to reach <math id="S4.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="78" display="inline"><semantics id="S4.SS2.SSS2.p1.2.m2.1a"><mn id="S4.SS2.SSS2.p1.2.m2.1.1" xref="S4.SS2.SSS2.p1.2.m2.1.1.cmml">78</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.2.m2.1b"><cn type="integer" id="S4.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1">78</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.2.m2.1c">78</annotation></semantics></math>% accuracy as well. More stable convergence curves are acquired for 70P and 90P which manage to gain an improvement of roughly <math id="S4.SS2.SSS2.p1.3.m3.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S4.SS2.SSS2.p1.3.m3.1a"><mn id="S4.SS2.SSS2.p1.3.m3.1.1" xref="S4.SS2.SSS2.p1.3.m3.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.3.m3.1b"><cn type="integer" id="S4.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.3.m3.1c">16</annotation></semantics></math>% and <math id="S4.SS2.SSS2.p1.4.m4.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.SS2.SSS2.p1.4.m4.1a"><mn id="S4.SS2.SSS2.p1.4.m4.1.1" xref="S4.SS2.SSS2.p1.4.m4.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.4.m4.1b"><cn type="integer" id="S4.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS2.p1.4.m4.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.4.m4.1c">20</annotation></semantics></math>%, respectively.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Future Research Directions</span>
</h2>

<section id="S5.SS0.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS0.SSS1.4.1.1" class="ltx_text">V-</span>1 </span>Explainable AI for Effective Pruning</h4>

<div id="S5.SS0.SSS1.p1" class="ltx_para">
<p id="S5.SS0.SSS1.p1.1" class="ltx_p">Incorporating explainable AI (XAI) into the compression process in OTA-FL for PIUs can make the pruning results more transparent and understandable <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. This would be particularly interesting for PIUs that gather distinct information and hence have non-i.i.d. data to train the global model. XAI can help identify the most relevant features of the dataset as well as the models for specific tasks, thereby making the compression process more robust. This leads to the creation of efficient and simple models for OTA transmission without sacrificing performance. XAI’s role is to clarify the rationale behind compression decisions, enhancing the trust and reliability of the resulting compressed models.</p>
</div>
</section>
<section id="S5.SS0.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS0.SSS2.4.1.1" class="ltx_text">V-</span>2 </span>Adaptive Compression</h4>

<div id="S5.SS0.SSS2.p1" class="ltx_para">
<p id="S5.SS0.SSS2.p1.1" class="ltx_p">OTA-FL and compression techniques can be tested further by employing an adaptive data handling strategy that considers the unique characteristics and heterogeneity of each PIUs dataset in the network and adjusts the compression rate accordingly. This could involve deeper pruning or other compression techniques, e.g., quantization, to ensure that areas of the model sensitive to specific data distributions undergo minimal compression to maintain the accuracy of the global model. This can be done by developing a feedback loop that assesses the impact of compression on the model’s effectiveness in real time, allowing on-the-fly adjustments to compression techniques. Further, dynamically updating the probability of a PIU being selected for model training based on a metric called cumulative model strength, which measures the statistical heterogeneity and previous training performance of a PIU, will help improve the overall network performance in IIoT.</p>
</div>
</section>
<section id="S5.SS0.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS0.SSS3.4.1.1" class="ltx_text">V-</span>3 </span>Retraining at PS</h4>

<div id="S5.SS0.SSS3.p1" class="ltx_para">
<p id="S5.SS0.SSS3.p1.1" class="ltx_p">Another prospective future work would be the incorporation of model retraining at the PS. In this strategy, the PS may have access to some samples of data that is used for local training. The PIUs after training the model transmit the GVs to PS, where the PS updates the model with received GVs and retrains it with the data it has access to. This retraining of the global model can make the model more robust, hence enhancing its performance, particularly for PIUs that operate in non-i.i.d., setups. The requirement, however, is that the PS can access some generic representative data. In this approach, PIUs can also locally compress and fine-tune their models to reduce size and complexity before transmitting them for aggregation.</p>
</div>
</section>
<section id="S5.SS0.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS0.SSS4.4.1.1" class="ltx_text">V-</span>4 </span>Multi Agent Reinforcement Learning for PIUs Selection</h4>

<div id="S5.SS0.SSS4.p1" class="ltx_para">
<p id="S5.SS0.SSS4.p1.1" class="ltx_p">The work can further be extended with multi-agent reinforcement learning (MARL) for efficient PIUs selection. By independently learning optimal compression strategies, MARL-based frameworks can optimize PIUs selection, enhancing model accuracy while reducing processing latency and communication costs in IIoT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. This can produce robust communication protocols that involve efficient model updates based on the noisy channel condition and the accuracy affected by deeper compression. Each edge device, functioning as an agent, learns to adapt its compression strategy based on individual data and industrial conditions. This would optimize local model updates and result in better performance of the OTA-FL network in IIoT. One particular challenge will include ensuring efficient learning across agents handling the variability of wireless channels in OTA scenarios, designing a practical reward function, and ensuring efficient communication among agents.</p>
</div>
</section>
<section id="S5.SS0.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS0.SSS5.4.1.1" class="ltx_text">V-</span>5 </span>Investigating Performance with More Complex Tasks</h4>

<div id="S5.SS0.SSS5.p1" class="ltx_para">
<p id="S5.SS0.SSS5.p1.1" class="ltx_p">As industrial setups are becoming more diverse with the presence of various formats of data, sophisticated algorithms, and increasingly complex DNN models are being proposed to handle them effectively. Among these techniques, video monitoring has emerged as a crucial tool for quality control and surveillance within industrial frameworks. However, employing complex DNN models like 3D convolutional networks (3D-CNNs) for video processing introduces significant challenges due to the complexity of video data, resulting in 3D-CNNs with substantially large number of parameters. Future research could focus on compression techniques that effectively reduce the volume of these complex DNN models, before transmitting their parameters OTA. This would significantly lower the demand for network resources while retaining the essential quality and utility of the data for critical functions such as activity recognition. In-depth exploration and enhancement of these data compression strategies could lead to substantial improvements in bandwidth efficiency and processing speed, particularly in OTA-FL scenarios and broader industrial IoT contexts.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Industrial IoT enables a plethora of applications across sectors with improved safety and security of information exchange among PIUs, and other operational efficiencies. In this article, we provided a detailed overview to understanding the role of model pruning in the context of OTA federated learning without sacrificing model performance. Following this, we delved into a case study that showcases the practical benefits and potential opportunities of applying IMP within an OTA-FL network. To reduce the DNN model in a bid to save bandwidth and energy of PIUs, simple one-shot pruning can be utilized. However, one-shot pruning results in a considerable performance loss, particularly in the OTA-FL context where channel noise is problematic. To overcome this loss in performance we demonstrate how iterative magnitude pruning can be used. The results show considerable gains from IMP to regain the accuracy lost with OSP, particularly with full participation of the PIUs for model training. Finally, we outlined future research opportunities and directions that can further improve the usefulness of model pruning and federated learning for IIoT networks.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Z. Wan, Z. Gao, M. Di Renzo, and L. Hanzo, “The road to industry 4.0 and beyond: A communications-, information-, and operation technology collaboration perspective,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE Network</em>, vol. 36, no. 6, pp. 157–164, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
G. Zheng, Q. Ni, K. Navaie, H. Pervaiz, G. Min, A. Kaushik, and C. Zarakovitis, “Mobility-aware split-federated with transfer learning for vehicular semantic communication networks,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, pp. 1–1, 2024.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
X. Cao, T. Başar, S. Diggavi, Y. C. Eldar, K. B. Letaief, H. V. Poor, and J. Zhang, “Communication-efficient distributed learning: An overview,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected Areas in Communications</em>, vol. 41, no. 4, pp. 851–873, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M. Li, T. Zhang, Y. Chen, and A. J. Smola, “Efficient mini-batch training for stochastic optimization,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</em>, pp. 661–670, 2014.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Z. Chen, E. G. Larsson, C. Fischione, M. Johansson, and Y. Malitsky, “Over-the-air computation for distributed systems: Something old and something new,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Network</em>, pp. 1–7, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
F. M. A. Khan, H. Abou-Zeid, and S. A. Hassan, “Deep compression for efficient and accelerated over-the-air federated learning,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, pp. 1–1, 2024.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Z. Li, H. Li, and L. Meng, “Model compression for deep neural networks: A survey,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Computers</em>, vol. 12, no. 3, p. 60, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Q. Zhou, Z. Qu, S. Guo, B. Luo, J. Guo, Z. Xu, and R. Akerkar, “On-device learning systems for edge intelligence: A software and hardware synergy perspective,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, vol. 8, no. 15, pp. 11 916–11 934, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights and connections for efficient neural network,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 28, 2015.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. Frankle and M. Carbin, “The lottery ticket hypothesis: Finding sparse, trainable neural networks.” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2019. [Online]. Available: http://dblp.uni-trier.de/db/conf/iclr/iclr2019.html#FrankleC19

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
C. M. J. Tan and M. Motani, “DropNet: Reducing neural network complexity via iterative pruning,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 37th International Conference on Machine Learning</em>, vol. 119, pp. 9356–9366, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
I. Ahmed, G. Jeon, and F. Piccialli, “From artificial intelligence to explainable artificial intelligence in industry 4.0: A survey on what, how, and where,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Industrial Informatics</em>, vol. 18, no. 8, pp. 5031–5042, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
L. Geng, H. Zhao, J. Wang, A. Kaushik, S. Yuan, and W. Feng, “Deep-reinforcement-learning-based distributed computation offloading in vehicular edge computing networks,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, vol. 10, no. 14, pp. 12 416–12 433, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.14119" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.14120" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.14120">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.14120" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.14121" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 16:52:33 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
