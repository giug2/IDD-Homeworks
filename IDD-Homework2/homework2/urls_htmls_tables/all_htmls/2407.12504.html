<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.12504] Case2Code: Learning Inductive Reasoning with Synthetic Data</title><meta property="og:description" content="Complex reasoning is an impressive ability shown by large language models (LLMs).
Most LLMs are skilled in deductive reasoning, such as chain-of-thought prompting or iterative tool-using to solve challenging tasks stepâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Case2Code: Learning Inductive Reasoning with Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Case2Code: Learning Inductive Reasoning with Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.12504">

<!--Generated on Mon Aug  5 14:19:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Case2Code: Learning Inductive Reasoning with Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yunfan Shao<sup id="id18.18.id1" class="ltx_sup"><span id="id18.18.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>,
Linyang Li<sup id="id19.19.id2" class="ltx_sup"><span id="id19.19.id2.1" class="ltx_text ltx_font_italic">2â€ </span></sup>,
Yichuan Ma<sup id="id20.20.id3" class="ltx_sup"><span id="id20.20.id3.1" class="ltx_text ltx_font_italic">1,2</span></sup>,
Peiji Li<sup id="id21.21.id4" class="ltx_sup"><span id="id21.21.id4.1" class="ltx_text ltx_font_italic">1,2</span></sup>,
Demin Song<sup id="id22.22.id5" class="ltx_sup"><span id="id22.22.id5.1" class="ltx_text ltx_font_italic">2</span></sup>, 
<br class="ltx_break"><span id="id17.17.12" class="ltx_text ltx_font_bold">Qinyuan Cheng<sup id="id17.17.12.1" class="ltx_sup"><span id="id17.17.12.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1,2</span></sup>,
Shimin Li<sup id="id17.17.12.2" class="ltx_sup"><span id="id17.17.12.2.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>,
Xiaonan Li<sup id="id17.17.12.3" class="ltx_sup"><span id="id17.17.12.3.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>,
Pengyu Wang<sup id="id17.17.12.4" class="ltx_sup"><span id="id17.17.12.4.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>,
Qipeng Guo<sup id="id17.17.12.5" class="ltx_sup"><span id="id17.17.12.5.1" class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup>, 
<br class="ltx_break">Hang Yan<sup id="id17.17.12.6" class="ltx_sup"><span id="id17.17.12.6.1" class="ltx_text ltx_font_medium ltx_font_italic">2,3</span></sup>,
Xipeng Qiu<sup id="id17.17.12.7" class="ltx_sup"><span id="id17.17.12.7.1" class="ltx_text ltx_font_medium ltx_font_italic">1â€ </span></sup>,
Xuanjing Huang<sup id="id17.17.12.8" class="ltx_sup"><span id="id17.17.12.8.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>,
Dahua Lin<sup id="id17.17.12.9" class="ltx_sup"><span id="id17.17.12.9.1" class="ltx_text ltx_font_medium ltx_font_italic">2,3</span></sup> 
<br class="ltx_break"><sup id="id17.17.12.10" class="ltx_sup"><span id="id17.17.12.10.1" class="ltx_text ltx_font_medium">1</span></sup>School of Computer Science, Fudan University
<br class="ltx_break"><sup id="id17.17.12.11" class="ltx_sup"><span id="id17.17.12.11.1" class="ltx_text ltx_font_medium">2</span></sup>Shanghai AI Laboratory
<br class="ltx_break"><sup id="id17.17.12.12" class="ltx_sup"><span id="id17.17.12.12.1" class="ltx_text ltx_font_medium">3</span></sup>The Chinese University of Hong Kong 
<br class="ltx_break"></span><span id="id23.23.id6" class="ltx_text ltx_font_typewriter">{yfshao19, xpqiu}@fudan.edu.cn</span><span id="id24.24.id7" class="ltx_text ltx_font_bold"> 
<br class="ltx_break"></span><span id="id25.25.id8" class="ltx_text ltx_font_typewriter">{lilinyang, yanhang}@pjlab.org.cn</span><span id="id26.26.id9" class="ltx_text ltx_font_bold">
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id27.id1" class="ltx_p">Complex reasoning is an impressive ability shown by large language models (LLMs).
Most LLMs are skilled in deductive reasoning, such as chain-of-thought prompting or iterative tool-using to solve challenging tasks step-by-step.
In this paper, we hope to focus on evaluating and teaching LLMs to conduct inductive reasoning, that is, LLMs are supposed to infer underlying rules by observing examples or sequential transformations.
However, collecting large-scale and diverse human-generated inductive data is challenging. We focus on data synthesis in the code domain and propose a <span id="id27.id1.1" class="ltx_text ltx_font_bold">Case2Code</span> task by exploiting the expressiveness and correctness of programs.
Specifically, we collect a diverse set of executable programs, synthesize input-output transformations for each program, and force LLMs to infer the underlying code implementations based on the synthetic I/O cases.
We first evaluate representative LLMs on the synthesized Case2Code task and demonstrate that the Case-to-code induction is challenging for LLMs. Then, we synthesize large-scale Case2Code training samples to train LLMs to perform inductive reasoning.
Experimental results show that such induction training benefits not only in distribution Case2Code performance but also enhances various coding abilities of trained LLMs, demonstrating the great potential of learning inductive reasoning via synthetic data.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Code and datasets will be available at <a target="_blank" href="https://github.com/choosewhatulike/case2code" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/choosewhatulike/case2code</a>.</span></span></span></p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.17" class="ltx_block ltx_align_bottom">
<p id="p1.17.18" class="ltx_p"><span id="p1.17.18.1" class="ltx_text ltx_font_bold">Case2Code: Learning Inductive Reasoning with Synthetic Data</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.17.17" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.17.17.17" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.17.17.17.17" class="ltx_tabular ltx_align_top">
<span id="p1.5.5.5.5.5" class="ltx_tr">
<span id="p1.5.5.5.5.5.5" class="ltx_td ltx_align_center"><span id="p1.5.5.5.5.5.5.5" class="ltx_text ltx_font_bold">
Yunfan Shao<sup id="p1.5.5.5.5.5.5.5.1" class="ltx_sup"><span id="p1.5.5.5.5.5.5.5.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1,2</span></sup>,
Linyang Li<sup id="p1.5.5.5.5.5.5.5.2" class="ltx_sup"><span id="p1.5.5.5.5.5.5.5.2.1" class="ltx_text ltx_font_medium ltx_font_italic">2â€ </span></sup>,
Yichuan Ma<sup id="p1.5.5.5.5.5.5.5.3" class="ltx_sup"><span id="p1.5.5.5.5.5.5.5.3.1" class="ltx_text ltx_font_medium ltx_font_italic">1,2</span></sup>,
Peiji Li<sup id="p1.5.5.5.5.5.5.5.4" class="ltx_sup"><span id="p1.5.5.5.5.5.5.5.4.1" class="ltx_text ltx_font_medium ltx_font_italic">1,2</span></sup>,
Demin Song<sup id="p1.5.5.5.5.5.5.5.5" class="ltx_sup"><span id="p1.5.5.5.5.5.5.5.5.1" class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup>,</span></span></span>
<span id="p1.10.10.10.10.10" class="ltx_tr">
<span id="p1.10.10.10.10.10.5" class="ltx_td ltx_align_center"><span id="p1.10.10.10.10.10.5.5" class="ltx_text ltx_font_bold">Qinyuan Cheng<sup id="p1.10.10.10.10.10.5.5.1" class="ltx_sup"><span id="p1.10.10.10.10.10.5.5.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1,2</span></sup>,
Shimin Li<sup id="p1.10.10.10.10.10.5.5.2" class="ltx_sup"><span id="p1.10.10.10.10.10.5.5.2.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>,
Xiaonan Li<sup id="p1.10.10.10.10.10.5.5.3" class="ltx_sup"><span id="p1.10.10.10.10.10.5.5.3.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>,
Pengyu Wang<sup id="p1.10.10.10.10.10.5.5.4" class="ltx_sup"><span id="p1.10.10.10.10.10.5.5.4.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>,
Qipeng Guo<sup id="p1.10.10.10.10.10.5.5.5" class="ltx_sup"><span id="p1.10.10.10.10.10.5.5.5.1" class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup>,</span></span></span>
<span id="p1.14.14.14.14.14" class="ltx_tr">
<span id="p1.14.14.14.14.14.4" class="ltx_td ltx_align_center"><span id="p1.14.14.14.14.14.4.4" class="ltx_text ltx_font_bold">Hang Yan<sup id="p1.14.14.14.14.14.4.4.1" class="ltx_sup"><span id="p1.14.14.14.14.14.4.4.1.1" class="ltx_text ltx_font_medium ltx_font_italic">2,3</span></sup>,
Xipeng Qiu<sup id="p1.14.14.14.14.14.4.4.2" class="ltx_sup"><span id="p1.14.14.14.14.14.4.4.2.1" class="ltx_text ltx_font_medium ltx_font_italic">1â€ </span></sup>,
Xuanjing Huang<sup id="p1.14.14.14.14.14.4.4.3" class="ltx_sup"><span id="p1.14.14.14.14.14.4.4.3.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>,
Dahua Lin<sup id="p1.14.14.14.14.14.4.4.4" class="ltx_sup"><span id="p1.14.14.14.14.14.4.4.4.1" class="ltx_text ltx_font_medium ltx_font_italic">2,3</span></sup></span></span></span>
<span id="p1.15.15.15.15.15" class="ltx_tr">
<span id="p1.15.15.15.15.15.1" class="ltx_td ltx_align_center"><sup id="p1.15.15.15.15.15.1.1" class="ltx_sup">1</sup>School of Computer Science, Fudan University</span></span>
<span id="p1.16.16.16.16.16" class="ltx_tr">
<span id="p1.16.16.16.16.16.1" class="ltx_td ltx_align_center"><sup id="p1.16.16.16.16.16.1.1" class="ltx_sup">2</sup>Shanghai AI Laboratory</span></span>
<span id="p1.17.17.17.17.17" class="ltx_tr">
<span id="p1.17.17.17.17.17.1" class="ltx_td ltx_align_center"><sup id="p1.17.17.17.17.17.1.1" class="ltx_sup">3</sup>The Chinese University of Hong Kong</span></span>
<span id="p1.17.17.17.17.18" class="ltx_tr">
<span id="p1.17.17.17.17.18.1" class="ltx_td ltx_align_center"><span id="p1.17.17.17.17.18.1.1" class="ltx_text ltx_font_typewriter">{yfshao19, xpqiu}@fudan.edu.cn</span></span></span>
<span id="p1.17.17.17.17.19" class="ltx_tr">
<span id="p1.17.17.17.17.19.1" class="ltx_td ltx_align_center"><span id="p1.17.17.17.17.19.1.1" class="ltx_text ltx_font_typewriter">{lilinyang, yanhang}@pjlab.org.cn</span></span></span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">${\dagger}$</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">${\dagger}$</sup><span class="ltx_note_type">footnotetext: </span>Corresponding Authors.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The success of large language models (LLMs), exemplified by GPT-4Â <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite> has revolutionized the AI community.
One of the most impressive abilities of LLMs is the deductive reasoning ability via chain-of-thoughtsÂ <cite class="ltx_cite ltx_citemacro_cite">Wei etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2022b</a>)</cite>, exemplified by solving mathematical reasoning tasks such as GSM8KÂ <cite class="ltx_cite ltx_citemacro_cite">Cobbe etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite> and MATHÂ <cite class="ltx_cite ltx_citemacro_cite">Hendrycks etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>.
The major contribution to the high-level performances of reasoning problem solving is to generate and train on deductive reasoning paths via chain-of-thoughts (CoTs) using LLMsÂ <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a href="#bib.bib31" title="" class="ltx_ref">2023</a>); Mitra etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2024</a>)</cite>.
Through various searching strategies and prompting algorithms <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2024</a>); Wang etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2023a</a>); Yao etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>); Wang etÂ al. (<a href="#bib.bib24" title="" class="ltx_ref">2022b</a>)</cite>, LLMs can synthesize high-quality CoTs and perform deduction reasoning in many domains.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2407.12504/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="185" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples of deductive and inductive reasoning in the code domain. Compared with instructions that need deductive reasoning, inductive reasoning instructions are rare in the training data, which makes it challenging for LLMs to learn.
</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Despite the success of deducing chain-of-thought reasoning, LLMs are rarely trained to perform inductive reasoning.
Inductive reasoning is a fundamental cognitive process for humans, playing a crucial role in learning, problem-solving, and scientific discovery.
For instance, famous scientists make inductive judgments from actual physical phenomena, such as Newtonâ€™s Law of Motion, and Keplerâ€™s Law based on data from Tycho.
It is essential to teach artificial intelligence systems utilizing inductive reasoning to find underlying rules from facts, transformations, and logs.
In this paper, to equip LLMs with such inductive reasoning ability, we introduce <span id="S1.p2.1.1" class="ltx_text ltx_font_bold">Case2Code</span>, a diverse and challenging inductive reasoning task for LLMs.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Specific and limited inductive reasoning has been studied in the machine learning field.
Works such as DEER <cite class="ltx_cite ltx_citemacro_cite">Yang etÂ al. (<a href="#bib.bib29" title="" class="ltx_ref">2022</a>)</cite> usually design a common knowledge induction process and challenge neural models to reason inductively to find the hidden fact.
Works such as DreamCoder <cite class="ltx_cite ltx_citemacro_cite">Ellis etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite> and DeepCoder <cite class="ltx_cite ltx_citemacro_cite">Balog etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2016</a>)</cite>, usually synthesize a small-scale toy task to train a domain-specific model, such as learning to summarize the list operations from given list change logs.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Different from these inductive reasoning tasks, we focus on large-scale data synthesis with programs in the real world. In Case2Code, inductive reasoning samples are synthesized from real-world productive functions, which are closer to the actual distribution of general LLM applications and production.
Specifically, the Case2Code challenge requires the LLM to infer the underlying program based on several input-to-output cases generated by the real-world program.
In Case2Code learning, LLMs are supposed to write solutions formulated by codes based on the example outputs,
which is one common scenario in the real-world working process, using examples to convey knowledge.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To obtain large-scale and diverse Case2Code data, we first gather a diverse collection of executable code texts that cover a wide range of real-world applications.
Then, we generate the input-output transformation cases with the assistance of LLMs and code interpreters,
which do not require powerful LLMs with advanced reasoning capabilities, resulting in a strong-to-weak distilling process.
By incorporating LLMs to write input examples for each program and execute the program with these inputs to gather the corresponding outputs, we can synthesize large-scale Case2CodeÂ samples with diverse data transformations and complicated control logic.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Based on the synthetic data, we can form a unique and challenging task to evaluate and further train the LLMs and study the induction reasoning ability of LLMs.
In the Case2Code challenge, we first test how current LLMs perform in making inductive reasoning.
We then train LLMs with Case2Code data to further study whether such data can improve the induction reasoning ability and generalize to other commonly used reasoning tasks.
Experimental results show that Case2Code is a challenging task for LLMs, even for powerful LLMs like LLaMA3-70B, GPT-3.5, and GPT-4.
With constructed Case2Code data, we can boost LLMs to learn to make such inductive reasoning, while such ability can be transferred to help improve general reasoning tasks such as HumanEval and MBPP in code generation.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">To summarize, in this paper, we:</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">(1) We introduce an induction reasoning task for LLMs, <span id="S1.p8.1.1" class="ltx_text ltx_font_bold">Case2Code</span>, pointing out the necessity to synthesize inductive reasoning data for LLMs.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">(2) We can teach LLMs to make induction reasoning, improving open-source LLMs by a great margin in the Case2Code challenge.</p>
</div>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">(3) Equipped with our proposed Case2Code ability, open-source LLMs can be further improved in general reasoning tasks.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Our work discusses the reasoning ability of LLMs, touching on the following grounds:</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Inductive Reasoning</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">While Reasoning is one major topic for neural networks, especially in the era of LLMs <cite class="ltx_cite ltx_citemacro_cite">Huang and Chang (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>,
inductive reasoning is rarely discussed in LLM reasoning, most research focuses on specific scenarios with limited inductive reasoning.
One pioneer work is prerequisite toy tasks <cite class="ltx_cite ltx_citemacro_cite">Weston etÂ al. (<a href="#bib.bib28" title="" class="ltx_ref">2015</a>)</cite> where the task goal is to solve simple induction.
Later, <cite class="ltx_cite ltx_citemacro_citet">Yang etÂ al. (<a href="#bib.bib29" title="" class="ltx_ref">2022</a>)</cite> introduces various world-wide knowledge such as botany, history and geography into the facts given and asks neural models to predict whether a given rule is correct.
On the other hand, several works focus on training inductive program synthesis models for constrained scenarios with limited search spaces, such as operations on list, string, and manually-defined objectsÂ <cite class="ltx_cite ltx_citemacro_cite">Balog etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2016</a>); Devlin etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2017</a>); Ellis etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2021</a>); Shi etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite>.
Different from previous reasoning ability studies, our proposed Case2Code task leverages diverse code in the real world as a powerful platform for LLMs to learn inductive reasoning under various challenging scenarios.
</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2407.12504/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="127" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Our synthetic framework incorporates an LLM and a code interpreter to construct Case2CodeÂ training samples at scale automatically.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Synthetic Reasoning Data</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The most widely studied LLMs reasoning task is the deductive reasoning ability, represented by chain-of-thoughts (CoTs) reasoningÂ <cite class="ltx_cite ltx_citemacro_cite">Wei etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2022b</a>)</cite>, which instructs LLMs to solve problems in detailed deductive steps.
Recent works focus on building high-quality CoTs through strong LLMs such as GPT-4 to enhance smaller LLMsÂ <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a href="#bib.bib31" title="" class="ltx_ref">2023</a>); Mitra etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2024</a>); Luo etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite>.
While a particular line of work focuses on studying different search strategies of reasoning paths, including self-consistencyÂ <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a href="#bib.bib23" title="" class="ltx_ref">2022a</a>)</cite>, rejection samplingÂ <cite class="ltx_cite ltx_citemacro_cite">Huang etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2023</a>); Yuan etÂ al. (<a href="#bib.bib32" title="" class="ltx_ref">2023</a>); Wang etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2023a</a>)</cite>, tree-structure CoT (ToT) searching <cite class="ltx_cite ltx_citemacro_cite">Yao etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>, Monte Carlo Tree Searching <cite class="ltx_cite ltx_citemacro_cite">Silver etÂ al. (<a href="#bib.bib20" title="" class="ltx_ref">2016</a>); Chen etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2024</a>)</cite>, etc.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we illustrate the framework for synthesizing Case2CodeÂ data in detail, which focuses on producing large-scale and high-quality inductive reasoning data in the code domain.
Unlike other synthetic data frameworks that distill high-quality training data from a strong teacher LLM to provide supervision signals to improve student LLMs, our Case2CodeÂ synthetic framework introduces a writer LLM to assist the synthesis of data samples. Thus the overall data quality does not directly rely on the performance of the LLM generator. And we can efficiently obtain reliable Case2CodeÂ training data at scale.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Problem Formulation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The inductive reasoning task aims to find a general hypothesis based on a small set of observations to explain a phenomenon. In this paper, we define Case2Code, an inductive reasoning task in the code domain.
Case2CodeÂ is a program synthesis task that targets the reconstruction of unknown programs based on observations of the program behaviors.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.10" class="ltx_p">Formally, for a functional program <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{P}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">ğ’«</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ğ’«</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\mathcal{P}</annotation></semantics></math>, we have a set of <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">n</annotation></semantics></math> input-output examples <math id="S3.SS1.p2.3.m3.4" class="ltx_Math" alttext="\mathcal{S_{P}}=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\}" display="inline"><semantics id="S3.SS1.p2.3.m3.4a"><mrow id="S3.SS1.p2.3.m3.4.4" xref="S3.SS1.p2.3.m3.4.4.cmml"><msub id="S3.SS1.p2.3.m3.4.4.5" xref="S3.SS1.p2.3.m3.4.4.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.3.m3.4.4.5.2" xref="S3.SS1.p2.3.m3.4.4.5.2.cmml">ğ’®</mi><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.3.m3.4.4.5.3" xref="S3.SS1.p2.3.m3.4.4.5.3.cmml">ğ’«</mi></msub><mo id="S3.SS1.p2.3.m3.4.4.4" xref="S3.SS1.p2.3.m3.4.4.4.cmml">=</mo><mrow id="S3.SS1.p2.3.m3.4.4.3.3" xref="S3.SS1.p2.3.m3.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS1.p2.3.m3.4.4.3.3.4" xref="S3.SS1.p2.3.m3.4.4.3.4.cmml">{</mo><mrow id="S3.SS1.p2.3.m3.2.2.1.1.1.2" xref="S3.SS1.p2.3.m3.2.2.1.1.1.3.cmml"><mo stretchy="false" id="S3.SS1.p2.3.m3.2.2.1.1.1.2.3" xref="S3.SS1.p2.3.m3.2.2.1.1.1.3.cmml">(</mo><msub id="S3.SS1.p2.3.m3.2.2.1.1.1.1.1" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.2" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.2.cmml">x</mi><mn id="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.3" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p2.3.m3.2.2.1.1.1.2.4" xref="S3.SS1.p2.3.m3.2.2.1.1.1.3.cmml">,</mo><msub id="S3.SS1.p2.3.m3.2.2.1.1.1.2.2" xref="S3.SS1.p2.3.m3.2.2.1.1.1.2.2.cmml"><mi id="S3.SS1.p2.3.m3.2.2.1.1.1.2.2.2" xref="S3.SS1.p2.3.m3.2.2.1.1.1.2.2.2.cmml">y</mi><mn id="S3.SS1.p2.3.m3.2.2.1.1.1.2.2.3" xref="S3.SS1.p2.3.m3.2.2.1.1.1.2.2.3.cmml">1</mn></msub><mo stretchy="false" id="S3.SS1.p2.3.m3.2.2.1.1.1.2.5" xref="S3.SS1.p2.3.m3.2.2.1.1.1.3.cmml">)</mo></mrow><mo id="S3.SS1.p2.3.m3.4.4.3.3.5" xref="S3.SS1.p2.3.m3.4.4.3.4.cmml">,</mo><mrow id="S3.SS1.p2.3.m3.3.3.2.2.2.2" xref="S3.SS1.p2.3.m3.3.3.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p2.3.m3.3.3.2.2.2.2.3" xref="S3.SS1.p2.3.m3.3.3.2.2.2.3.cmml">(</mo><msub id="S3.SS1.p2.3.m3.3.3.2.2.2.1.1" xref="S3.SS1.p2.3.m3.3.3.2.2.2.1.1.cmml"><mi id="S3.SS1.p2.3.m3.3.3.2.2.2.1.1.2" xref="S3.SS1.p2.3.m3.3.3.2.2.2.1.1.2.cmml">x</mi><mn id="S3.SS1.p2.3.m3.3.3.2.2.2.1.1.3" xref="S3.SS1.p2.3.m3.3.3.2.2.2.1.1.3.cmml">2</mn></msub><mo id="S3.SS1.p2.3.m3.3.3.2.2.2.2.4" xref="S3.SS1.p2.3.m3.3.3.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p2.3.m3.3.3.2.2.2.2.2" xref="S3.SS1.p2.3.m3.3.3.2.2.2.2.2.cmml"><mi id="S3.SS1.p2.3.m3.3.3.2.2.2.2.2.2" xref="S3.SS1.p2.3.m3.3.3.2.2.2.2.2.2.cmml">y</mi><mn id="S3.SS1.p2.3.m3.3.3.2.2.2.2.2.3" xref="S3.SS1.p2.3.m3.3.3.2.2.2.2.2.3.cmml">2</mn></msub><mo stretchy="false" id="S3.SS1.p2.3.m3.3.3.2.2.2.2.5" xref="S3.SS1.p2.3.m3.3.3.2.2.2.3.cmml">)</mo></mrow><mo id="S3.SS1.p2.3.m3.4.4.3.3.6" xref="S3.SS1.p2.3.m3.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">â€¦</mi><mo id="S3.SS1.p2.3.m3.4.4.3.3.7" xref="S3.SS1.p2.3.m3.4.4.3.4.cmml">,</mo><mrow id="S3.SS1.p2.3.m3.4.4.3.3.3.2" xref="S3.SS1.p2.3.m3.4.4.3.3.3.3.cmml"><mo stretchy="false" id="S3.SS1.p2.3.m3.4.4.3.3.3.2.3" xref="S3.SS1.p2.3.m3.4.4.3.3.3.3.cmml">(</mo><msub id="S3.SS1.p2.3.m3.4.4.3.3.3.1.1" xref="S3.SS1.p2.3.m3.4.4.3.3.3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.4.4.3.3.3.1.1.2" xref="S3.SS1.p2.3.m3.4.4.3.3.3.1.1.2.cmml">x</mi><mi id="S3.SS1.p2.3.m3.4.4.3.3.3.1.1.3" xref="S3.SS1.p2.3.m3.4.4.3.3.3.1.1.3.cmml">n</mi></msub><mo id="S3.SS1.p2.3.m3.4.4.3.3.3.2.4" xref="S3.SS1.p2.3.m3.4.4.3.3.3.3.cmml">,</mo><msub id="S3.SS1.p2.3.m3.4.4.3.3.3.2.2" xref="S3.SS1.p2.3.m3.4.4.3.3.3.2.2.cmml"><mi id="S3.SS1.p2.3.m3.4.4.3.3.3.2.2.2" xref="S3.SS1.p2.3.m3.4.4.3.3.3.2.2.2.cmml">y</mi><mi id="S3.SS1.p2.3.m3.4.4.3.3.3.2.2.3" xref="S3.SS1.p2.3.m3.4.4.3.3.3.2.2.3.cmml">n</mi></msub><mo stretchy="false" id="S3.SS1.p2.3.m3.4.4.3.3.3.2.5" xref="S3.SS1.p2.3.m3.4.4.3.3.3.3.cmml">)</mo></mrow><mo stretchy="false" id="S3.SS1.p2.3.m3.4.4.3.3.8" xref="S3.SS1.p2.3.m3.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.4b"><apply id="S3.SS1.p2.3.m3.4.4.cmml" xref="S3.SS1.p2.3.m3.4.4"><eq id="S3.SS1.p2.3.m3.4.4.4.cmml" xref="S3.SS1.p2.3.m3.4.4.4"></eq><apply id="S3.SS1.p2.3.m3.4.4.5.cmml" xref="S3.SS1.p2.3.m3.4.4.5"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.4.4.5.1.cmml" xref="S3.SS1.p2.3.m3.4.4.5">subscript</csymbol><ci id="S3.SS1.p2.3.m3.4.4.5.2.cmml" xref="S3.SS1.p2.3.m3.4.4.5.2">ğ’®</ci><ci id="S3.SS1.p2.3.m3.4.4.5.3.cmml" xref="S3.SS1.p2.3.m3.4.4.5.3">ğ’«</ci></apply><set id="S3.SS1.p2.3.m3.4.4.3.4.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3"><interval closure="open" id="S3.SS1.p2.3.m3.2.2.1.1.1.3.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.2"><apply id="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.2">ğ‘¥</ci><cn type="integer" id="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.3">1</cn></apply><apply id="S3.SS1.p2.3.m3.2.2.1.1.1.2.2.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.2.2.1.1.1.2.2.1.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.2.2">subscript</csymbol><ci id="S3.SS1.p2.3.m3.2.2.1.1.1.2.2.2.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.2.2.2">ğ‘¦</ci><cn type="integer" id="S3.SS1.p2.3.m3.2.2.1.1.1.2.2.3.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.2.2.3">1</cn></apply></interval><interval closure="open" id="S3.SS1.p2.3.m3.3.3.2.2.2.3.cmml" xref="S3.SS1.p2.3.m3.3.3.2.2.2.2"><apply id="S3.SS1.p2.3.m3.3.3.2.2.2.1.1.cmml" xref="S3.SS1.p2.3.m3.3.3.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.3.3.2.2.2.1.1.1.cmml" xref="S3.SS1.p2.3.m3.3.3.2.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.3.3.2.2.2.1.1.2.cmml" xref="S3.SS1.p2.3.m3.3.3.2.2.2.1.1.2">ğ‘¥</ci><cn type="integer" id="S3.SS1.p2.3.m3.3.3.2.2.2.1.1.3.cmml" xref="S3.SS1.p2.3.m3.3.3.2.2.2.1.1.3">2</cn></apply><apply id="S3.SS1.p2.3.m3.3.3.2.2.2.2.2.cmml" xref="S3.SS1.p2.3.m3.3.3.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.3.3.2.2.2.2.2.1.cmml" xref="S3.SS1.p2.3.m3.3.3.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.3.m3.3.3.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.3.m3.3.3.2.2.2.2.2.2">ğ‘¦</ci><cn type="integer" id="S3.SS1.p2.3.m3.3.3.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.3.m3.3.3.2.2.2.2.2.3">2</cn></apply></interval><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">â€¦</ci><interval closure="open" id="S3.SS1.p2.3.m3.4.4.3.3.3.3.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3.3.2"><apply id="S3.SS1.p2.3.m3.4.4.3.3.3.1.1.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.4.4.3.3.3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3.3.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.4.4.3.3.3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3.3.1.1.2">ğ‘¥</ci><ci id="S3.SS1.p2.3.m3.4.4.3.3.3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3.3.1.1.3">ğ‘›</ci></apply><apply id="S3.SS1.p2.3.m3.4.4.3.3.3.2.2.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.4.4.3.3.3.2.2.1.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p2.3.m3.4.4.3.3.3.2.2.2.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3.3.2.2.2">ğ‘¦</ci><ci id="S3.SS1.p2.3.m3.4.4.3.3.3.2.2.3.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3.3.2.2.3">ğ‘›</ci></apply></interval></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.4c">\mathcal{S_{P}}=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\}</annotation></semantics></math>, where <math id="S3.SS1.p2.4.m4.6" class="ltx_Math" alttext="y_{i}=\mathcal{P}(x_{i}),i=1,2,...,n" display="inline"><semantics id="S3.SS1.p2.4.m4.6a"><mrow id="S3.SS1.p2.4.m4.6.6.2" xref="S3.SS1.p2.4.m4.6.6.3.cmml"><mrow id="S3.SS1.p2.4.m4.5.5.1.1" xref="S3.SS1.p2.4.m4.5.5.1.1.cmml"><msub id="S3.SS1.p2.4.m4.5.5.1.1.3" xref="S3.SS1.p2.4.m4.5.5.1.1.3.cmml"><mi id="S3.SS1.p2.4.m4.5.5.1.1.3.2" xref="S3.SS1.p2.4.m4.5.5.1.1.3.2.cmml">y</mi><mi id="S3.SS1.p2.4.m4.5.5.1.1.3.3" xref="S3.SS1.p2.4.m4.5.5.1.1.3.3.cmml">i</mi></msub><mo id="S3.SS1.p2.4.m4.5.5.1.1.2" xref="S3.SS1.p2.4.m4.5.5.1.1.2.cmml">=</mo><mrow id="S3.SS1.p2.4.m4.5.5.1.1.1" xref="S3.SS1.p2.4.m4.5.5.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.4.m4.5.5.1.1.1.3" xref="S3.SS1.p2.4.m4.5.5.1.1.1.3.cmml">ğ’«</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.4.m4.5.5.1.1.1.2" xref="S3.SS1.p2.4.m4.5.5.1.1.1.2.cmml">â€‹</mo><mrow id="S3.SS1.p2.4.m4.5.5.1.1.1.1.1" xref="S3.SS1.p2.4.m4.5.5.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p2.4.m4.5.5.1.1.1.1.1.2" xref="S3.SS1.p2.4.m4.5.5.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p2.4.m4.5.5.1.1.1.1.1.1" xref="S3.SS1.p2.4.m4.5.5.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.4.m4.5.5.1.1.1.1.1.1.2" xref="S3.SS1.p2.4.m4.5.5.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS1.p2.4.m4.5.5.1.1.1.1.1.1.3" xref="S3.SS1.p2.4.m4.5.5.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.p2.4.m4.5.5.1.1.1.1.1.3" xref="S3.SS1.p2.4.m4.5.5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.SS1.p2.4.m4.6.6.2.3" xref="S3.SS1.p2.4.m4.6.6.3a.cmml">,</mo><mrow id="S3.SS1.p2.4.m4.6.6.2.2" xref="S3.SS1.p2.4.m4.6.6.2.2.cmml"><mi id="S3.SS1.p2.4.m4.6.6.2.2.2" xref="S3.SS1.p2.4.m4.6.6.2.2.2.cmml">i</mi><mo id="S3.SS1.p2.4.m4.6.6.2.2.1" xref="S3.SS1.p2.4.m4.6.6.2.2.1.cmml">=</mo><mrow id="S3.SS1.p2.4.m4.6.6.2.2.3.2" xref="S3.SS1.p2.4.m4.6.6.2.2.3.1.cmml"><mn id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">1</mn><mo id="S3.SS1.p2.4.m4.6.6.2.2.3.2.1" xref="S3.SS1.p2.4.m4.6.6.2.2.3.1.cmml">,</mo><mn id="S3.SS1.p2.4.m4.2.2" xref="S3.SS1.p2.4.m4.2.2.cmml">2</mn><mo id="S3.SS1.p2.4.m4.6.6.2.2.3.2.2" xref="S3.SS1.p2.4.m4.6.6.2.2.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p2.4.m4.3.3" xref="S3.SS1.p2.4.m4.3.3.cmml">â€¦</mi><mo id="S3.SS1.p2.4.m4.6.6.2.2.3.2.3" xref="S3.SS1.p2.4.m4.6.6.2.2.3.1.cmml">,</mo><mi id="S3.SS1.p2.4.m4.4.4" xref="S3.SS1.p2.4.m4.4.4.cmml">n</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.6b"><apply id="S3.SS1.p2.4.m4.6.6.3.cmml" xref="S3.SS1.p2.4.m4.6.6.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.6.6.3a.cmml" xref="S3.SS1.p2.4.m4.6.6.2.3">formulae-sequence</csymbol><apply id="S3.SS1.p2.4.m4.5.5.1.1.cmml" xref="S3.SS1.p2.4.m4.5.5.1.1"><eq id="S3.SS1.p2.4.m4.5.5.1.1.2.cmml" xref="S3.SS1.p2.4.m4.5.5.1.1.2"></eq><apply id="S3.SS1.p2.4.m4.5.5.1.1.3.cmml" xref="S3.SS1.p2.4.m4.5.5.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.5.5.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.5.5.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.4.m4.5.5.1.1.3.2.cmml" xref="S3.SS1.p2.4.m4.5.5.1.1.3.2">ğ‘¦</ci><ci id="S3.SS1.p2.4.m4.5.5.1.1.3.3.cmml" xref="S3.SS1.p2.4.m4.5.5.1.1.3.3">ğ‘–</ci></apply><apply id="S3.SS1.p2.4.m4.5.5.1.1.1.cmml" xref="S3.SS1.p2.4.m4.5.5.1.1.1"><times id="S3.SS1.p2.4.m4.5.5.1.1.1.2.cmml" xref="S3.SS1.p2.4.m4.5.5.1.1.1.2"></times><ci id="S3.SS1.p2.4.m4.5.5.1.1.1.3.cmml" xref="S3.SS1.p2.4.m4.5.5.1.1.1.3">ğ’«</ci><apply id="S3.SS1.p2.4.m4.5.5.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.5.5.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.5.5.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.5.5.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.5.5.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.4.m4.5.5.1.1.1.1.1.1.2">ğ‘¥</ci><ci id="S3.SS1.p2.4.m4.5.5.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.4.m4.5.5.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply><apply id="S3.SS1.p2.4.m4.6.6.2.2.cmml" xref="S3.SS1.p2.4.m4.6.6.2.2"><eq id="S3.SS1.p2.4.m4.6.6.2.2.1.cmml" xref="S3.SS1.p2.4.m4.6.6.2.2.1"></eq><ci id="S3.SS1.p2.4.m4.6.6.2.2.2.cmml" xref="S3.SS1.p2.4.m4.6.6.2.2.2">ğ‘–</ci><list id="S3.SS1.p2.4.m4.6.6.2.2.3.1.cmml" xref="S3.SS1.p2.4.m4.6.6.2.2.3.2"><cn type="integer" id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">1</cn><cn type="integer" id="S3.SS1.p2.4.m4.2.2.cmml" xref="S3.SS1.p2.4.m4.2.2">2</cn><ci id="S3.SS1.p2.4.m4.3.3.cmml" xref="S3.SS1.p2.4.m4.3.3">â€¦</ci><ci id="S3.SS1.p2.4.m4.4.4.cmml" xref="S3.SS1.p2.4.m4.4.4">ğ‘›</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.6c">y_{i}=\mathcal{P}(x_{i}),i=1,2,...,n</annotation></semantics></math>. The goal of Case2CodeÂ is to implement a program <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="\mathcal{P^{\prime}}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><msup id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">ğ’«</mi><mo id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">superscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">ğ’«</ci><ci id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">\mathcal{P^{\prime}}</annotation></semantics></math> that captures the functionality of the program <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="\mathcal{P}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml">ğ’«</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><ci id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">ğ’«</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">\mathcal{P}</annotation></semantics></math> based on the observed set of input-output example cases <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="\mathcal{S_{P}}" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><msub id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.7.m7.1.1.2" xref="S3.SS1.p2.7.m7.1.1.2.cmml">ğ’®</mi><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml">ğ’«</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2">ğ’®</ci><ci id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3">ğ’«</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">\mathcal{S_{P}}</annotation></semantics></math>. And for any new input case <math id="S3.SS1.p2.8.m8.1" class="ltx_Math" alttext="x_{\text{new}}\notin\mathcal{S_{P}}" display="inline"><semantics id="S3.SS1.p2.8.m8.1a"><mrow id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml"><msub id="S3.SS1.p2.8.m8.1.1.2" xref="S3.SS1.p2.8.m8.1.1.2.cmml"><mi id="S3.SS1.p2.8.m8.1.1.2.2" xref="S3.SS1.p2.8.m8.1.1.2.2.cmml">x</mi><mtext id="S3.SS1.p2.8.m8.1.1.2.3" xref="S3.SS1.p2.8.m8.1.1.2.3a.cmml">new</mtext></msub><mo id="S3.SS1.p2.8.m8.1.1.1" xref="S3.SS1.p2.8.m8.1.1.1.cmml">âˆ‰</mo><msub id="S3.SS1.p2.8.m8.1.1.3" xref="S3.SS1.p2.8.m8.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.8.m8.1.1.3.2" xref="S3.SS1.p2.8.m8.1.1.3.2.cmml">ğ’®</mi><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.8.m8.1.1.3.3" xref="S3.SS1.p2.8.m8.1.1.3.3.cmml">ğ’«</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><apply id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1"><notin id="S3.SS1.p2.8.m8.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1.1"></notin><apply id="S3.SS1.p2.8.m8.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.1.1.2.1.cmml" xref="S3.SS1.p2.8.m8.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.8.m8.1.1.2.2.cmml" xref="S3.SS1.p2.8.m8.1.1.2.2">ğ‘¥</ci><ci id="S3.SS1.p2.8.m8.1.1.2.3a.cmml" xref="S3.SS1.p2.8.m8.1.1.2.3"><mtext mathsize="70%" id="S3.SS1.p2.8.m8.1.1.2.3.cmml" xref="S3.SS1.p2.8.m8.1.1.2.3">new</mtext></ci></apply><apply id="S3.SS1.p2.8.m8.1.1.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.1.1.3.1.cmml" xref="S3.SS1.p2.8.m8.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.8.m8.1.1.3.2.cmml" xref="S3.SS1.p2.8.m8.1.1.3.2">ğ’®</ci><ci id="S3.SS1.p2.8.m8.1.1.3.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3.3">ğ’«</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">x_{\text{new}}\notin\mathcal{S_{P}}</annotation></semantics></math>, the implemented program <math id="S3.SS1.p2.9.m9.1" class="ltx_Math" alttext="\mathcal{P^{\prime}}" display="inline"><semantics id="S3.SS1.p2.9.m9.1a"><msup id="S3.SS1.p2.9.m9.1.1" xref="S3.SS1.p2.9.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.9.m9.1.1.2" xref="S3.SS1.p2.9.m9.1.1.2.cmml">ğ’«</mi><mo id="S3.SS1.p2.9.m9.1.1.3" xref="S3.SS1.p2.9.m9.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.1b"><apply id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.1.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1">superscript</csymbol><ci id="S3.SS1.p2.9.m9.1.1.2.cmml" xref="S3.SS1.p2.9.m9.1.1.2">ğ’«</ci><ci id="S3.SS1.p2.9.m9.1.1.3.cmml" xref="S3.SS1.p2.9.m9.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.1c">\mathcal{P^{\prime}}</annotation></semantics></math> should satisfy that <math id="S3.SS1.p2.10.m10.2" class="ltx_Math" alttext="\mathcal{P}(x_{\text{new}})=\mathcal{P^{\prime}}(x_{\text{new}})" display="inline"><semantics id="S3.SS1.p2.10.m10.2a"><mrow id="S3.SS1.p2.10.m10.2.2" xref="S3.SS1.p2.10.m10.2.2.cmml"><mrow id="S3.SS1.p2.10.m10.1.1.1" xref="S3.SS1.p2.10.m10.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.10.m10.1.1.1.3" xref="S3.SS1.p2.10.m10.1.1.1.3.cmml">ğ’«</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.10.m10.1.1.1.2" xref="S3.SS1.p2.10.m10.1.1.1.2.cmml">â€‹</mo><mrow id="S3.SS1.p2.10.m10.1.1.1.1.1" xref="S3.SS1.p2.10.m10.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p2.10.m10.1.1.1.1.1.2" xref="S3.SS1.p2.10.m10.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p2.10.m10.1.1.1.1.1.1" xref="S3.SS1.p2.10.m10.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.10.m10.1.1.1.1.1.1.2" xref="S3.SS1.p2.10.m10.1.1.1.1.1.1.2.cmml">x</mi><mtext id="S3.SS1.p2.10.m10.1.1.1.1.1.1.3" xref="S3.SS1.p2.10.m10.1.1.1.1.1.1.3a.cmml">new</mtext></msub><mo stretchy="false" id="S3.SS1.p2.10.m10.1.1.1.1.1.3" xref="S3.SS1.p2.10.m10.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p2.10.m10.2.2.3" xref="S3.SS1.p2.10.m10.2.2.3.cmml">=</mo><mrow id="S3.SS1.p2.10.m10.2.2.2" xref="S3.SS1.p2.10.m10.2.2.2.cmml"><msup id="S3.SS1.p2.10.m10.2.2.2.3" xref="S3.SS1.p2.10.m10.2.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.10.m10.2.2.2.3.2" xref="S3.SS1.p2.10.m10.2.2.2.3.2.cmml">ğ’«</mi><mo id="S3.SS1.p2.10.m10.2.2.2.3.3" xref="S3.SS1.p2.10.m10.2.2.2.3.3.cmml">â€²</mo></msup><mo lspace="0em" rspace="0em" id="S3.SS1.p2.10.m10.2.2.2.2" xref="S3.SS1.p2.10.m10.2.2.2.2.cmml">â€‹</mo><mrow id="S3.SS1.p2.10.m10.2.2.2.1.1" xref="S3.SS1.p2.10.m10.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p2.10.m10.2.2.2.1.1.2" xref="S3.SS1.p2.10.m10.2.2.2.1.1.1.cmml">(</mo><msub id="S3.SS1.p2.10.m10.2.2.2.1.1.1" xref="S3.SS1.p2.10.m10.2.2.2.1.1.1.cmml"><mi id="S3.SS1.p2.10.m10.2.2.2.1.1.1.2" xref="S3.SS1.p2.10.m10.2.2.2.1.1.1.2.cmml">x</mi><mtext id="S3.SS1.p2.10.m10.2.2.2.1.1.1.3" xref="S3.SS1.p2.10.m10.2.2.2.1.1.1.3a.cmml">new</mtext></msub><mo stretchy="false" id="S3.SS1.p2.10.m10.2.2.2.1.1.3" xref="S3.SS1.p2.10.m10.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m10.2b"><apply id="S3.SS1.p2.10.m10.2.2.cmml" xref="S3.SS1.p2.10.m10.2.2"><eq id="S3.SS1.p2.10.m10.2.2.3.cmml" xref="S3.SS1.p2.10.m10.2.2.3"></eq><apply id="S3.SS1.p2.10.m10.1.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1.1"><times id="S3.SS1.p2.10.m10.1.1.1.2.cmml" xref="S3.SS1.p2.10.m10.1.1.1.2"></times><ci id="S3.SS1.p2.10.m10.1.1.1.3.cmml" xref="S3.SS1.p2.10.m10.1.1.1.3">ğ’«</ci><apply id="S3.SS1.p2.10.m10.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.10.m10.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.10.m10.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.10.m10.1.1.1.1.1.1.2">ğ‘¥</ci><ci id="S3.SS1.p2.10.m10.1.1.1.1.1.1.3a.cmml" xref="S3.SS1.p2.10.m10.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S3.SS1.p2.10.m10.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.10.m10.1.1.1.1.1.1.3">new</mtext></ci></apply></apply><apply id="S3.SS1.p2.10.m10.2.2.2.cmml" xref="S3.SS1.p2.10.m10.2.2.2"><times id="S3.SS1.p2.10.m10.2.2.2.2.cmml" xref="S3.SS1.p2.10.m10.2.2.2.2"></times><apply id="S3.SS1.p2.10.m10.2.2.2.3.cmml" xref="S3.SS1.p2.10.m10.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.10.m10.2.2.2.3.1.cmml" xref="S3.SS1.p2.10.m10.2.2.2.3">superscript</csymbol><ci id="S3.SS1.p2.10.m10.2.2.2.3.2.cmml" xref="S3.SS1.p2.10.m10.2.2.2.3.2">ğ’«</ci><ci id="S3.SS1.p2.10.m10.2.2.2.3.3.cmml" xref="S3.SS1.p2.10.m10.2.2.2.3.3">â€²</ci></apply><apply id="S3.SS1.p2.10.m10.2.2.2.1.1.1.cmml" xref="S3.SS1.p2.10.m10.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.10.m10.2.2.2.1.1.1.1.cmml" xref="S3.SS1.p2.10.m10.2.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p2.10.m10.2.2.2.1.1.1.2.cmml" xref="S3.SS1.p2.10.m10.2.2.2.1.1.1.2">ğ‘¥</ci><ci id="S3.SS1.p2.10.m10.2.2.2.1.1.1.3a.cmml" xref="S3.SS1.p2.10.m10.2.2.2.1.1.1.3"><mtext mathsize="70%" id="S3.SS1.p2.10.m10.2.2.2.1.1.1.3.cmml" xref="S3.SS1.p2.10.m10.2.2.2.1.1.1.3">new</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m10.2c">\mathcal{P}(x_{\text{new}})=\mathcal{P^{\prime}}(x_{\text{new}})</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Framework Overview</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In our synthetic data generation framework, we focus on generating large-scale and diverse Case2CodeÂ data automatically. As shown in FigureÂ <a href="#S2.F2" title="Figure 2 â€£ 2.1 Inductive Reasoning â€£ 2 Related Work â€£ Case2Code: Learning Inductive Reasoning with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we first collect diverse programs from large-scale datasets with rule-based filters. Then we incorporate LLMs to write diverse example inputs and utilize the code interpreter to calculate their corresponding outputs for each program. Finally, we filter out low-quality programs based on their outputs and convert the obtained triple (program, inputs, outputs) into Case2CodeÂ data for inductive reasoning in the code domain.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Note that the correctness of our synthetic data does not depend on the capabilities of the used LLMs. Therefore, we can synthetic high-quality Case2CodeÂ data at scale using small LLMs with low costs.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Collecting Programs</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To obtain massive data samples for inductive reasoning learning, we first need to acquire massive and diverse programs that take input arguments, do some complicated processes, and return output values.
Instead of prompting LLMs to generate functions that meet these requirements, we collect human-written high-quality programs in the wild to enhance diversity.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Specifically, we sample valid Python functions from The StackÂ <cite class="ltx_cite ltx_citemacro_cite">Kocetkov etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite> to construct our reasoning dataset. We incorporate the out-of-box Abstract Syntax Tree (AST) parsing toolÂ <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://docs.python.org/3/library/ast.html</span></span></span> to parse each file in The Stack to obtain Python functions. We only keep self-contained high-quality functions that satisfy all of these filtering rules: (1) pass the syntax check; (2) have one or more input arguments and return values; and (3) do not rely on third-party packages or external I/O operations. After collecting these functions, we can easily execute and verify these functions to obtain diverse Case2CodeÂ data with a simple and fast code interpreter at scale, which avoids extra file or network operations that require a sophisticated sandbox.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Generating Inputs</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Once we collect large-scale functions, the next step is to obtain the corresponding input-output pairs for each function to construct the Case2CodeÂ data. It is infeasible to write test cases for each function manually. So, we utilize LLMs to generate suitable input examples for these functions. We prompt LLMs to write some example input arguments for each function based on the corresponding function implementation. Detailed prompt is listed in TableÂ <a href="#A1.T6" title="Table 6 â€£ Appendix A Prompts Used in Case2Code â€£ Case2Code: Learning Inductive Reasoning with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> in the appendix.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">To generate suitable input arguments, the LLM needs first to analyze the implementation of the functions, then infer the possible types and value ranges of the input arguments, and finally come up with correct input arguments. However, we argue that a powerful LLM is not the key factor for our synthetic data. As we find that while strong LLMs can write high-quality inputs to generate Case2CodeÂ training data that boosts the reasoning performance of weak LLMs, the weak LLM can also write inputs for creating Case2CodeÂ data to self-improve their reasoning ability (see SecÂ <a href="#S4.SS4.SSS0.Px2" title="LLM for Generating Inputs â€£ 4.4 Ablation Study â€£ 4 Experiment â€£ Case2Code: Learning Inductive Reasoning with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>). Therefore, the generation process can be scaled efficiently at a low cost by using small LLMs.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Obtain Outputs</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">After collecting self-contained functions and the corresponding inputs, it is intuitive to incorporate a code interpreter to run these functions on their inputs for output curation. Since the LLM-generated input examples can contain errors, we introduced a filtering procedure to reject invalid inputs or functions based on their returned outputs. Specifically, if the outputs of a function do not change as the inputs change (e.g. always return the same output or exceptions), the function is considered invalid and will be filtered out.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">Moreover, we also filter out functions that generate very long output values to ensure the length of the generated Case2CodeÂ data is within the context window size of current LLMs. Note that we do not filter out inputs that lead to exceptions or runtime errors, as we believe that failure call attempts can also provide valuable information for inductive reasoning to reconstruct the function.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Post-processing</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.8" class="ltx_p">The final step is to convert the obtained functions and their corresponding input-output pairs into Case2CodeÂ style data. Formally, for a given function <math id="S3.SS6.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{P}" display="inline"><semantics id="S3.SS6.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS6.p1.1.m1.1.1" xref="S3.SS6.p1.1.m1.1.1.cmml">ğ’«</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.1.m1.1b"><ci id="S3.SS6.p1.1.m1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1">ğ’«</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.1.m1.1c">\mathcal{P}</annotation></semantics></math> and its <math id="S3.SS6.p1.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS6.p1.2.m2.1a"><mi id="S3.SS6.p1.2.m2.1.1" xref="S3.SS6.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.2.m2.1b"><ci id="S3.SS6.p1.2.m2.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.2.m2.1c">n</annotation></semantics></math> test cases <math id="S3.SS6.p1.3.m3.4" class="ltx_Math" alttext="\mathcal{S_{P}}=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\}" display="inline"><semantics id="S3.SS6.p1.3.m3.4a"><mrow id="S3.SS6.p1.3.m3.4.4" xref="S3.SS6.p1.3.m3.4.4.cmml"><msub id="S3.SS6.p1.3.m3.4.4.5" xref="S3.SS6.p1.3.m3.4.4.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS6.p1.3.m3.4.4.5.2" xref="S3.SS6.p1.3.m3.4.4.5.2.cmml">ğ’®</mi><mi class="ltx_font_mathcaligraphic" id="S3.SS6.p1.3.m3.4.4.5.3" xref="S3.SS6.p1.3.m3.4.4.5.3.cmml">ğ’«</mi></msub><mo id="S3.SS6.p1.3.m3.4.4.4" xref="S3.SS6.p1.3.m3.4.4.4.cmml">=</mo><mrow id="S3.SS6.p1.3.m3.4.4.3.3" xref="S3.SS6.p1.3.m3.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS6.p1.3.m3.4.4.3.3.4" xref="S3.SS6.p1.3.m3.4.4.3.4.cmml">{</mo><mrow id="S3.SS6.p1.3.m3.2.2.1.1.1.2" xref="S3.SS6.p1.3.m3.2.2.1.1.1.3.cmml"><mo stretchy="false" id="S3.SS6.p1.3.m3.2.2.1.1.1.2.3" xref="S3.SS6.p1.3.m3.2.2.1.1.1.3.cmml">(</mo><msub id="S3.SS6.p1.3.m3.2.2.1.1.1.1.1" xref="S3.SS6.p1.3.m3.2.2.1.1.1.1.1.cmml"><mi id="S3.SS6.p1.3.m3.2.2.1.1.1.1.1.2" xref="S3.SS6.p1.3.m3.2.2.1.1.1.1.1.2.cmml">x</mi><mn id="S3.SS6.p1.3.m3.2.2.1.1.1.1.1.3" xref="S3.SS6.p1.3.m3.2.2.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS6.p1.3.m3.2.2.1.1.1.2.4" xref="S3.SS6.p1.3.m3.2.2.1.1.1.3.cmml">,</mo><msub id="S3.SS6.p1.3.m3.2.2.1.1.1.2.2" xref="S3.SS6.p1.3.m3.2.2.1.1.1.2.2.cmml"><mi id="S3.SS6.p1.3.m3.2.2.1.1.1.2.2.2" xref="S3.SS6.p1.3.m3.2.2.1.1.1.2.2.2.cmml">y</mi><mn id="S3.SS6.p1.3.m3.2.2.1.1.1.2.2.3" xref="S3.SS6.p1.3.m3.2.2.1.1.1.2.2.3.cmml">1</mn></msub><mo stretchy="false" id="S3.SS6.p1.3.m3.2.2.1.1.1.2.5" xref="S3.SS6.p1.3.m3.2.2.1.1.1.3.cmml">)</mo></mrow><mo id="S3.SS6.p1.3.m3.4.4.3.3.5" xref="S3.SS6.p1.3.m3.4.4.3.4.cmml">,</mo><mrow id="S3.SS6.p1.3.m3.3.3.2.2.2.2" xref="S3.SS6.p1.3.m3.3.3.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS6.p1.3.m3.3.3.2.2.2.2.3" xref="S3.SS6.p1.3.m3.3.3.2.2.2.3.cmml">(</mo><msub id="S3.SS6.p1.3.m3.3.3.2.2.2.1.1" xref="S3.SS6.p1.3.m3.3.3.2.2.2.1.1.cmml"><mi id="S3.SS6.p1.3.m3.3.3.2.2.2.1.1.2" xref="S3.SS6.p1.3.m3.3.3.2.2.2.1.1.2.cmml">x</mi><mn id="S3.SS6.p1.3.m3.3.3.2.2.2.1.1.3" xref="S3.SS6.p1.3.m3.3.3.2.2.2.1.1.3.cmml">2</mn></msub><mo id="S3.SS6.p1.3.m3.3.3.2.2.2.2.4" xref="S3.SS6.p1.3.m3.3.3.2.2.2.3.cmml">,</mo><msub id="S3.SS6.p1.3.m3.3.3.2.2.2.2.2" xref="S3.SS6.p1.3.m3.3.3.2.2.2.2.2.cmml"><mi id="S3.SS6.p1.3.m3.3.3.2.2.2.2.2.2" xref="S3.SS6.p1.3.m3.3.3.2.2.2.2.2.2.cmml">y</mi><mn id="S3.SS6.p1.3.m3.3.3.2.2.2.2.2.3" xref="S3.SS6.p1.3.m3.3.3.2.2.2.2.2.3.cmml">2</mn></msub><mo stretchy="false" id="S3.SS6.p1.3.m3.3.3.2.2.2.2.5" xref="S3.SS6.p1.3.m3.3.3.2.2.2.3.cmml">)</mo></mrow><mo id="S3.SS6.p1.3.m3.4.4.3.3.6" xref="S3.SS6.p1.3.m3.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS6.p1.3.m3.1.1" xref="S3.SS6.p1.3.m3.1.1.cmml">â€¦</mi><mo id="S3.SS6.p1.3.m3.4.4.3.3.7" xref="S3.SS6.p1.3.m3.4.4.3.4.cmml">,</mo><mrow id="S3.SS6.p1.3.m3.4.4.3.3.3.2" xref="S3.SS6.p1.3.m3.4.4.3.3.3.3.cmml"><mo stretchy="false" id="S3.SS6.p1.3.m3.4.4.3.3.3.2.3" xref="S3.SS6.p1.3.m3.4.4.3.3.3.3.cmml">(</mo><msub id="S3.SS6.p1.3.m3.4.4.3.3.3.1.1" xref="S3.SS6.p1.3.m3.4.4.3.3.3.1.1.cmml"><mi id="S3.SS6.p1.3.m3.4.4.3.3.3.1.1.2" xref="S3.SS6.p1.3.m3.4.4.3.3.3.1.1.2.cmml">x</mi><mi id="S3.SS6.p1.3.m3.4.4.3.3.3.1.1.3" xref="S3.SS6.p1.3.m3.4.4.3.3.3.1.1.3.cmml">n</mi></msub><mo id="S3.SS6.p1.3.m3.4.4.3.3.3.2.4" xref="S3.SS6.p1.3.m3.4.4.3.3.3.3.cmml">,</mo><msub id="S3.SS6.p1.3.m3.4.4.3.3.3.2.2" xref="S3.SS6.p1.3.m3.4.4.3.3.3.2.2.cmml"><mi id="S3.SS6.p1.3.m3.4.4.3.3.3.2.2.2" xref="S3.SS6.p1.3.m3.4.4.3.3.3.2.2.2.cmml">y</mi><mi id="S3.SS6.p1.3.m3.4.4.3.3.3.2.2.3" xref="S3.SS6.p1.3.m3.4.4.3.3.3.2.2.3.cmml">n</mi></msub><mo stretchy="false" id="S3.SS6.p1.3.m3.4.4.3.3.3.2.5" xref="S3.SS6.p1.3.m3.4.4.3.3.3.3.cmml">)</mo></mrow><mo stretchy="false" id="S3.SS6.p1.3.m3.4.4.3.3.8" xref="S3.SS6.p1.3.m3.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.3.m3.4b"><apply id="S3.SS6.p1.3.m3.4.4.cmml" xref="S3.SS6.p1.3.m3.4.4"><eq id="S3.SS6.p1.3.m3.4.4.4.cmml" xref="S3.SS6.p1.3.m3.4.4.4"></eq><apply id="S3.SS6.p1.3.m3.4.4.5.cmml" xref="S3.SS6.p1.3.m3.4.4.5"><csymbol cd="ambiguous" id="S3.SS6.p1.3.m3.4.4.5.1.cmml" xref="S3.SS6.p1.3.m3.4.4.5">subscript</csymbol><ci id="S3.SS6.p1.3.m3.4.4.5.2.cmml" xref="S3.SS6.p1.3.m3.4.4.5.2">ğ’®</ci><ci id="S3.SS6.p1.3.m3.4.4.5.3.cmml" xref="S3.SS6.p1.3.m3.4.4.5.3">ğ’«</ci></apply><set id="S3.SS6.p1.3.m3.4.4.3.4.cmml" xref="S3.SS6.p1.3.m3.4.4.3.3"><interval closure="open" id="S3.SS6.p1.3.m3.2.2.1.1.1.3.cmml" xref="S3.SS6.p1.3.m3.2.2.1.1.1.2"><apply id="S3.SS6.p1.3.m3.2.2.1.1.1.1.1.cmml" xref="S3.SS6.p1.3.m3.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.3.m3.2.2.1.1.1.1.1.1.cmml" xref="S3.SS6.p1.3.m3.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS6.p1.3.m3.2.2.1.1.1.1.1.2.cmml" xref="S3.SS6.p1.3.m3.2.2.1.1.1.1.1.2">ğ‘¥</ci><cn type="integer" id="S3.SS6.p1.3.m3.2.2.1.1.1.1.1.3.cmml" xref="S3.SS6.p1.3.m3.2.2.1.1.1.1.1.3">1</cn></apply><apply id="S3.SS6.p1.3.m3.2.2.1.1.1.2.2.cmml" xref="S3.SS6.p1.3.m3.2.2.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS6.p1.3.m3.2.2.1.1.1.2.2.1.cmml" xref="S3.SS6.p1.3.m3.2.2.1.1.1.2.2">subscript</csymbol><ci id="S3.SS6.p1.3.m3.2.2.1.1.1.2.2.2.cmml" xref="S3.SS6.p1.3.m3.2.2.1.1.1.2.2.2">ğ‘¦</ci><cn type="integer" id="S3.SS6.p1.3.m3.2.2.1.1.1.2.2.3.cmml" xref="S3.SS6.p1.3.m3.2.2.1.1.1.2.2.3">1</cn></apply></interval><interval closure="open" id="S3.SS6.p1.3.m3.3.3.2.2.2.3.cmml" xref="S3.SS6.p1.3.m3.3.3.2.2.2.2"><apply id="S3.SS6.p1.3.m3.3.3.2.2.2.1.1.cmml" xref="S3.SS6.p1.3.m3.3.3.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.3.m3.3.3.2.2.2.1.1.1.cmml" xref="S3.SS6.p1.3.m3.3.3.2.2.2.1.1">subscript</csymbol><ci id="S3.SS6.p1.3.m3.3.3.2.2.2.1.1.2.cmml" xref="S3.SS6.p1.3.m3.3.3.2.2.2.1.1.2">ğ‘¥</ci><cn type="integer" id="S3.SS6.p1.3.m3.3.3.2.2.2.1.1.3.cmml" xref="S3.SS6.p1.3.m3.3.3.2.2.2.1.1.3">2</cn></apply><apply id="S3.SS6.p1.3.m3.3.3.2.2.2.2.2.cmml" xref="S3.SS6.p1.3.m3.3.3.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS6.p1.3.m3.3.3.2.2.2.2.2.1.cmml" xref="S3.SS6.p1.3.m3.3.3.2.2.2.2.2">subscript</csymbol><ci id="S3.SS6.p1.3.m3.3.3.2.2.2.2.2.2.cmml" xref="S3.SS6.p1.3.m3.3.3.2.2.2.2.2.2">ğ‘¦</ci><cn type="integer" id="S3.SS6.p1.3.m3.3.3.2.2.2.2.2.3.cmml" xref="S3.SS6.p1.3.m3.3.3.2.2.2.2.2.3">2</cn></apply></interval><ci id="S3.SS6.p1.3.m3.1.1.cmml" xref="S3.SS6.p1.3.m3.1.1">â€¦</ci><interval closure="open" id="S3.SS6.p1.3.m3.4.4.3.3.3.3.cmml" xref="S3.SS6.p1.3.m3.4.4.3.3.3.2"><apply id="S3.SS6.p1.3.m3.4.4.3.3.3.1.1.cmml" xref="S3.SS6.p1.3.m3.4.4.3.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.3.m3.4.4.3.3.3.1.1.1.cmml" xref="S3.SS6.p1.3.m3.4.4.3.3.3.1.1">subscript</csymbol><ci id="S3.SS6.p1.3.m3.4.4.3.3.3.1.1.2.cmml" xref="S3.SS6.p1.3.m3.4.4.3.3.3.1.1.2">ğ‘¥</ci><ci id="S3.SS6.p1.3.m3.4.4.3.3.3.1.1.3.cmml" xref="S3.SS6.p1.3.m3.4.4.3.3.3.1.1.3">ğ‘›</ci></apply><apply id="S3.SS6.p1.3.m3.4.4.3.3.3.2.2.cmml" xref="S3.SS6.p1.3.m3.4.4.3.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS6.p1.3.m3.4.4.3.3.3.2.2.1.cmml" xref="S3.SS6.p1.3.m3.4.4.3.3.3.2.2">subscript</csymbol><ci id="S3.SS6.p1.3.m3.4.4.3.3.3.2.2.2.cmml" xref="S3.SS6.p1.3.m3.4.4.3.3.3.2.2.2">ğ‘¦</ci><ci id="S3.SS6.p1.3.m3.4.4.3.3.3.2.2.3.cmml" xref="S3.SS6.p1.3.m3.4.4.3.3.3.2.2.3">ğ‘›</ci></apply></interval></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.3.m3.4c">\mathcal{S_{P}}=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\}</annotation></semantics></math>, we randomly sample <math id="S3.SS6.p1.4.m4.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS6.p1.4.m4.1a"><mi id="S3.SS6.p1.4.m4.1.1" xref="S3.SS6.p1.4.m4.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.4.m4.1b"><ci id="S3.SS6.p1.4.m4.1.1.cmml" xref="S3.SS6.p1.4.m4.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.4.m4.1c">m</annotation></semantics></math> examples (<math id="S3.SS6.p1.5.m5.1" class="ltx_Math" alttext="m&lt;=n" display="inline"><semantics id="S3.SS6.p1.5.m5.1a"><mrow id="S3.SS6.p1.5.m5.1.1" xref="S3.SS6.p1.5.m5.1.1.cmml"><mi id="S3.SS6.p1.5.m5.1.1.2" xref="S3.SS6.p1.5.m5.1.1.2.cmml">m</mi><mo id="S3.SS6.p1.5.m5.1.1.1" xref="S3.SS6.p1.5.m5.1.1.1.cmml">&lt;=</mo><mi id="S3.SS6.p1.5.m5.1.1.3" xref="S3.SS6.p1.5.m5.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.5.m5.1b"><apply id="S3.SS6.p1.5.m5.1.1.cmml" xref="S3.SS6.p1.5.m5.1.1"><leq id="S3.SS6.p1.5.m5.1.1.1.cmml" xref="S3.SS6.p1.5.m5.1.1.1"></leq><ci id="S3.SS6.p1.5.m5.1.1.2.cmml" xref="S3.SS6.p1.5.m5.1.1.2">ğ‘š</ci><ci id="S3.SS6.p1.5.m5.1.1.3.cmml" xref="S3.SS6.p1.5.m5.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.5.m5.1c">m&lt;=n</annotation></semantics></math>) as the observed set <math id="S3.SS6.p1.6.m6.1" class="ltx_Math" alttext="\mathcal{S^{\prime}_{P}}" display="inline"><semantics id="S3.SS6.p1.6.m6.1a"><msubsup id="S3.SS6.p1.6.m6.1.1" xref="S3.SS6.p1.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS6.p1.6.m6.1.1.2.2" xref="S3.SS6.p1.6.m6.1.1.2.2.cmml">ğ’®</mi><mi class="ltx_font_mathcaligraphic" id="S3.SS6.p1.6.m6.1.1.3" xref="S3.SS6.p1.6.m6.1.1.3.cmml">ğ’«</mi><mo id="S3.SS6.p1.6.m6.1.1.2.3" xref="S3.SS6.p1.6.m6.1.1.2.3.cmml">â€²</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.6.m6.1b"><apply id="S3.SS6.p1.6.m6.1.1.cmml" xref="S3.SS6.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.6.m6.1.1.1.cmml" xref="S3.SS6.p1.6.m6.1.1">subscript</csymbol><apply id="S3.SS6.p1.6.m6.1.1.2.cmml" xref="S3.SS6.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.6.m6.1.1.2.1.cmml" xref="S3.SS6.p1.6.m6.1.1">superscript</csymbol><ci id="S3.SS6.p1.6.m6.1.1.2.2.cmml" xref="S3.SS6.p1.6.m6.1.1.2.2">ğ’®</ci><ci id="S3.SS6.p1.6.m6.1.1.2.3.cmml" xref="S3.SS6.p1.6.m6.1.1.2.3">â€²</ci></apply><ci id="S3.SS6.p1.6.m6.1.1.3.cmml" xref="S3.SS6.p1.6.m6.1.1.3">ğ’«</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.6.m6.1c">\mathcal{S^{\prime}_{P}}</annotation></semantics></math>. We generate the prompted data that facilitate the LLM to conduct inductive reasoning on the observed examples <math id="S3.SS6.p1.7.m7.1" class="ltx_Math" alttext="\mathcal{S^{\prime}_{P}}" display="inline"><semantics id="S3.SS6.p1.7.m7.1a"><msubsup id="S3.SS6.p1.7.m7.1.1" xref="S3.SS6.p1.7.m7.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS6.p1.7.m7.1.1.2.2" xref="S3.SS6.p1.7.m7.1.1.2.2.cmml">ğ’®</mi><mi class="ltx_font_mathcaligraphic" id="S3.SS6.p1.7.m7.1.1.3" xref="S3.SS6.p1.7.m7.1.1.3.cmml">ğ’«</mi><mo id="S3.SS6.p1.7.m7.1.1.2.3" xref="S3.SS6.p1.7.m7.1.1.2.3.cmml">â€²</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.7.m7.1b"><apply id="S3.SS6.p1.7.m7.1.1.cmml" xref="S3.SS6.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.7.m7.1.1.1.cmml" xref="S3.SS6.p1.7.m7.1.1">subscript</csymbol><apply id="S3.SS6.p1.7.m7.1.1.2.cmml" xref="S3.SS6.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.7.m7.1.1.2.1.cmml" xref="S3.SS6.p1.7.m7.1.1">superscript</csymbol><ci id="S3.SS6.p1.7.m7.1.1.2.2.cmml" xref="S3.SS6.p1.7.m7.1.1.2.2">ğ’®</ci><ci id="S3.SS6.p1.7.m7.1.1.2.3.cmml" xref="S3.SS6.p1.7.m7.1.1.2.3">â€²</ci></apply><ci id="S3.SS6.p1.7.m7.1.1.3.cmml" xref="S3.SS6.p1.7.m7.1.1.3">ğ’«</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.7.m7.1c">\mathcal{S^{\prime}_{P}}</annotation></semantics></math> to reconstruct the given function <math id="S3.SS6.p1.8.m8.1" class="ltx_Math" alttext="\mathcal{P}" display="inline"><semantics id="S3.SS6.p1.8.m8.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS6.p1.8.m8.1.1" xref="S3.SS6.p1.8.m8.1.1.cmml">ğ’«</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.8.m8.1b"><ci id="S3.SS6.p1.8.m8.1.1.cmml" xref="S3.SS6.p1.8.m8.1.1">ğ’«</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.8.m8.1c">\mathcal{P}</annotation></semantics></math>. Converted training examples are shown in TableÂ <a href="#A1.T7" title="Table 7 â€£ Appendix A Prompts Used in Case2Code â€£ Case2Code: Learning Inductive Reasoning with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> in the appendix.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">We find that the diversity of the prompts can substantially affect the generalization of the model reasoning performance (as shown in SecÂ <a href="#S4.SS4.SSS0.Px1" title="Prompt Diversity â€£ 4.4 Ablation Study â€£ 4 Experiment â€£ Case2Code: Learning Inductive Reasoning with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>). Therefore, we manually construct about 10 prompts with different styles to enhance the data diversity.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we illustrate the experimental setups and discuss the experimental results to demonstrate the challenge of solving Case2CodeÂ problems and show the effectiveness of large-scale Case2CodeÂ synthetic data.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.1.1.2.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S4.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.1.1.3.1" class="ltx_text ltx_font_bold">HumanEval</span></td>
<td id="S4.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S4.T1.1.1.4.1" class="ltx_text ltx_font_bold">HumanEval</span>+</td>
<td id="S4.T1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.1.1.5.1" class="ltx_text ltx_font_bold">MBPP</span></td>
<td id="S4.T1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<span id="S4.T1.1.1.6.1" class="ltx_text ltx_font_bold">MBPP</span>+</td>
<td id="S4.T1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.1.1.7.1" class="ltx_text ltx_font_bold">Case2Code</span></td>
</tr>
<tr id="S4.T1.1.2" class="ltx_tr">
<td id="S4.T1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">GPT-4</td>
<td id="S4.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">90.2</td>
<td id="S4.T1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.6</td>
<td id="S4.T1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">85.7</td>
<td id="S4.T1.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">73.3</td>
<td id="S4.T1.1.2.7" class="ltx_td ltx_align_center ltx_border_t">43.6</td>
</tr>
<tr id="S4.T1.1.3" class="ltx_tr">
<td id="S4.T1.1.3.1" class="ltx_td ltx_align_left">GPT-3.5</td>
<td id="S4.T1.1.3.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T1.1.3.3" class="ltx_td ltx_align_center">76.8</td>
<td id="S4.T1.1.3.4" class="ltx_td ltx_align_center ltx_border_r">70.7</td>
<td id="S4.T1.1.3.5" class="ltx_td ltx_align_center">82.5</td>
<td id="S4.T1.1.3.6" class="ltx_td ltx_align_center ltx_border_r">69.7</td>
<td id="S4.T1.1.3.7" class="ltx_td ltx_align_center">34.2</td>
</tr>
<tr id="S4.T1.1.4" class="ltx_tr">
<td id="S4.T1.1.4.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="4"><span id="S4.T1.1.4.1.1" class="ltx_text">LLaMA2-Chat</span></td>
<td id="S4.T1.1.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7B</td>
<td id="S4.T1.1.4.3" class="ltx_td ltx_align_center ltx_border_t">14.0</td>
<td id="S4.T1.1.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.6</td>
<td id="S4.T1.1.4.5" class="ltx_td ltx_align_center ltx_border_t">26.8</td>
<td id="S4.T1.1.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20.3</td>
<td id="S4.T1.1.4.7" class="ltx_td ltx_align_center ltx_border_t">0.2</td>
</tr>
<tr id="S4.T1.1.5" class="ltx_tr">
<td id="S4.T1.1.5.1" class="ltx_td ltx_align_center ltx_border_r">13B</td>
<td id="S4.T1.1.5.2" class="ltx_td ltx_align_center">23.1</td>
<td id="S4.T1.1.5.3" class="ltx_td ltx_align_center ltx_border_r">19.5</td>
<td id="S4.T1.1.5.4" class="ltx_td ltx_align_center">37.0</td>
<td id="S4.T1.1.5.5" class="ltx_td ltx_align_center ltx_border_r">27.6</td>
<td id="S4.T1.1.5.6" class="ltx_td ltx_align_center">8.2</td>
</tr>
<tr id="S4.T1.1.6" class="ltx_tr">
<td id="S4.T1.1.6.1" class="ltx_td ltx_align_center ltx_border_r">34B</td>
<td id="S4.T1.1.6.2" class="ltx_td ltx_align_center">22.6</td>
<td id="S4.T1.1.6.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T1.1.6.4" class="ltx_td ltx_align_center">33.0</td>
<td id="S4.T1.1.6.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T1.1.6.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T1.1.7" class="ltx_tr">
<td id="S4.T1.1.7.1" class="ltx_td ltx_align_center ltx_border_r">70B</td>
<td id="S4.T1.1.7.2" class="ltx_td ltx_align_center">36.6</td>
<td id="S4.T1.1.7.3" class="ltx_td ltx_align_center ltx_border_r">28.7</td>
<td id="S4.T1.1.7.4" class="ltx_td ltx_align_center">46.3</td>
<td id="S4.T1.1.7.5" class="ltx_td ltx_align_center ltx_border_r">35.1</td>
<td id="S4.T1.1.7.6" class="ltx_td ltx_align_center">7.8</td>
</tr>
<tr id="S4.T1.1.8" class="ltx_tr">
<td id="S4.T1.1.8.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="3"><span id="S4.T1.1.8.1.1" class="ltx_text">CodeLLaMA-Instruct</span></td>
<td id="S4.T1.1.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7B</td>
<td id="S4.T1.1.8.3" class="ltx_td ltx_align_center ltx_border_t">37.8</td>
<td id="S4.T1.1.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.4</td>
<td id="S4.T1.1.8.5" class="ltx_td ltx_align_center ltx_border_t">59.5</td>
<td id="S4.T1.1.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">46.8</td>
<td id="S4.T1.1.8.7" class="ltx_td ltx_align_center ltx_border_t">14.2</td>
</tr>
<tr id="S4.T1.1.9" class="ltx_tr">
<td id="S4.T1.1.9.1" class="ltx_td ltx_align_center ltx_border_r">13B</td>
<td id="S4.T1.1.9.2" class="ltx_td ltx_align_center">42.7</td>
<td id="S4.T1.1.9.3" class="ltx_td ltx_align_center ltx_border_r">38.4</td>
<td id="S4.T1.1.9.4" class="ltx_td ltx_align_center">63.5</td>
<td id="S4.T1.1.9.5" class="ltx_td ltx_align_center ltx_border_r">52.6</td>
<td id="S4.T1.1.9.6" class="ltx_td ltx_align_center">19.0</td>
</tr>
<tr id="S4.T1.1.10" class="ltx_tr">
<td id="S4.T1.1.10.1" class="ltx_td ltx_align_center ltx_border_r">34B</td>
<td id="S4.T1.1.10.2" class="ltx_td ltx_align_center">51.8</td>
<td id="S4.T1.1.10.3" class="ltx_td ltx_align_center ltx_border_r">43.9</td>
<td id="S4.T1.1.10.4" class="ltx_td ltx_align_center">69.3</td>
<td id="S4.T1.1.10.5" class="ltx_td ltx_align_center ltx_border_r">56.3</td>
<td id="S4.T1.1.10.6" class="ltx_td ltx_align_center">22.6</td>
</tr>
<tr id="S4.T1.1.11" class="ltx_tr">
<td id="S4.T1.1.11.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="2"><span id="S4.T1.1.11.1.1" class="ltx_text">LLaMA3-Instruct</span></td>
<td id="S4.T1.1.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8B</td>
<td id="S4.T1.1.11.3" class="ltx_td ltx_align_center ltx_border_t">61.6</td>
<td id="S4.T1.1.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">56.7</td>
<td id="S4.T1.1.11.5" class="ltx_td ltx_align_center ltx_border_t">70.1</td>
<td id="S4.T1.1.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">59.3</td>
<td id="S4.T1.1.11.7" class="ltx_td ltx_align_center ltx_border_t">10.4</td>
</tr>
<tr id="S4.T1.1.12" class="ltx_tr">
<td id="S4.T1.1.12.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">70B</td>
<td id="S4.T1.1.12.2" class="ltx_td ltx_align_center ltx_border_bb">77.4</td>
<td id="S4.T1.1.12.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">72.0</td>
<td id="S4.T1.1.12.4" class="ltx_td ltx_align_center ltx_border_bb">82.3</td>
<td id="S4.T1.1.12.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">69</td>
<td id="S4.T1.1.12.6" class="ltx_td ltx_align_center ltx_border_bb">22.6</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results of Code Benchmarks and zero-shot Case2CodeÂ performance of various representative LLMs.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Data Construction</h5>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">We randomly sampled about 2.3 million functions from The Stack pre-training dataset, in which we already performed data deduplication with the evaluation benchmarks (e.g. HumanEval, MBPP, etc). We conduct the data synthetic pipeline incorporating InternLM2-7bÂ <cite class="ltx_cite ltx_citemacro_cite">Cai etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2024</a>)</cite> to generate input examples for each function. The temperature is set to 0.2 and the top_p is set to 0.95. The generation takes about 500 GPU hours using A800 GPUs. Then we use 64 CPUs to execute and filter functions, which takes about 1 hour. The execution is under a constrained Python environment to ensure safety. We eventually obtained 1.3M high-quality functions with input-output pairs for Case2CodeÂ reasoning. We hold out 500 samples for evaluation and the rest for training.
For the hold-out evaluation samples, we further prompted GPT-4 (<span id="S4.SS1.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_typewriter">gpt-4-turbo-2024-04-09</span>) to generate additional input examples and collect the corresponding outputs for a more strict inductive reasoning evaluation.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Training Setup</h5>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">To demonstrate the generalization and effectiveness of our synthetic training data, we conduct three variants of Case2CodeÂ training: direct fine-tuning, mixed pre-training, and mixed fine-tuning.
All Case2CodeÂ variants are trained for 5k steps with a batch size of 64, a maximum context window size of 4096, and apply linear warmup and cosine decay of the learning rate from the peak value of 2e-5 to 5e-6. All model training is completed on two servers of eight A800 GPUs. We conduct training on open-sourced models, i.e. InternLM2-7BÂ <cite class="ltx_cite ltx_citemacro_cite">Cai etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2024</a>)</cite> and LLaMA3-8BÂ <cite class="ltx_cite ltx_citemacro_cite">AI@Meta (<a href="#bib.bib1" title="" class="ltx_ref">2024</a>)</cite> to verify the effectiveness of synthetic training data on different model series.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Evaluation Setup</h5>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p">We evaluate the coding ability of trained LLMs with HumanEval, MBPP. To conduct strict evaluation, we use EvalPlus, an extension to the original HumanEval and MBPP with massive additional test cases. For models that are not instructed tuned, we apply zero-shot prompting and four-shot prompting for HumanEval and MBPP evaluation, respectively. And for instructed-aligned LLMs, we use zero-shot prompting on all these benchmarks. To evaluate inductive reasoning on code, we test various LLMs on solving Case2CodeÂ tasks, with zero-shot prompting. When evaluating the instructed models that are not tuned on Case2CodeÂ task, we find the performance is unstable and sensitive to the prompts. We manually optimized the prompts for Case2CodeÂ evaluation to elicit the actual inductive reasoning ability of these models. We use greedy decoding during the inference for all experiments.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Models</h5>

<div id="S4.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px4.p1.1" class="ltx_p">We compare the trained models with several families of representative LLMs: GPT series<cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>, CodeLLaMAÂ <cite class="ltx_cite ltx_citemacro_cite">RoziÃ¨re etÂ al. (<a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>, LLaMA2Â <cite class="ltx_cite ltx_citemacro_cite">Touvron etÂ al. (<a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite> and LLaMA3Â <cite class="ltx_cite ltx_citemacro_cite">AI@Meta (<a href="#bib.bib1" title="" class="ltx_ref">2024</a>)</cite>. For GPT series, we evaluate GPT-3.5 (<span id="S4.SS1.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_typewriter">gpt-3.5-turbo-0125</span>) and GPT-4 (<span id="S4.SS1.SSS0.Px4.p1.1.2" class="ltx_text ltx_font_typewriter">gpt-4-turbo-2024-04-09</span>). For other model series, we evaluate their available open-sourced versions.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:216.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-34.9pt,18.3pt) scale(0.854963762428119,0.854963762428119) ;">
<table id="S4.T2.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.3.3.4" class="ltx_tr">
<td id="S4.T2.3.3.4.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T2.3.3.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.3.3.4.2.1" class="ltx_text">
<span id="S4.T2.3.3.4.2.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:42.7pt;">
<span id="S4.T2.3.3.4.2.1.1.1" class="ltx_p"><span id="S4.T2.3.3.4.2.1.1.1.1" class="ltx_text ltx_font_bold">Train w/ 
<br class="ltx_break">Ours</span></span>
</span></span></td>
<td id="S4.T2.3.3.4.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.3.3.4.3.1" class="ltx_text ltx_font_bold">HumanEval</span></td>
<td id="S4.T2.3.3.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.3.3.4.4.1" class="ltx_text"><span id="S4.T2.3.3.4.4.1.1" class="ltx_text ltx_font_bold">HumanEval</span>+</span></td>
<td id="S4.T2.3.3.4.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.3.3.4.5.1" class="ltx_text ltx_font_bold">MBPP</span></td>
<td id="S4.T2.3.3.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.3.3.4.6.1" class="ltx_text"><span id="S4.T2.3.3.4.6.1.1" class="ltx_text ltx_font_bold">MBPP</span>+</span></td>
<td id="S4.T2.3.3.4.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.3.3.4.7.1" class="ltx_text ltx_font_bold">Case2Code</span></td>
</tr>
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_border_t">InternLM2-7B-Base</td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ—</td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">31.1</td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21.3</td>
<td id="S4.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t">51.4</td>
<td id="S4.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.3</td>
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">27.2<sup id="S4.T2.1.1.1.1.1" class="ltx_sup"><span id="S4.T2.1.1.1.1.1.1" class="ltx_text ltx_font_italic">â€ </span></sup>
</td>
</tr>
<tr id="S4.T2.3.3.5" class="ltx_tr">
<td id="S4.T2.3.3.5.1" class="ltx_td ltx_align_left">w/ Direct Fine-tuning</td>
<td id="S4.T2.3.3.5.2" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S4.T2.3.3.5.3" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.5.3.1" class="ltx_text ltx_font_bold">44.5</span></td>
<td id="S4.T2.3.3.5.4" class="ltx_td ltx_align_center ltx_border_r">34.8</td>
<td id="S4.T2.3.3.5.5" class="ltx_td ltx_align_center">56.0</td>
<td id="S4.T2.3.3.5.6" class="ltx_td ltx_align_center ltx_border_r">40.4</td>
<td id="S4.T2.3.3.5.7" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.5.7.1" class="ltx_text ltx_font_bold">44.4</span></td>
</tr>
<tr id="S4.T2.3.3.6" class="ltx_tr">
<td id="S4.T2.3.3.6.1" class="ltx_td ltx_align_left">w/ Mixed Pre-training</td>
<td id="S4.T2.3.3.6.2" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S4.T2.3.3.6.3" class="ltx_td ltx_align_center">43.9</td>
<td id="S4.T2.3.3.6.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.3.3.6.4.1" class="ltx_text ltx_font_bold">40.9</span></td>
<td id="S4.T2.3.3.6.5" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.6.5.1" class="ltx_text ltx_font_bold">58.4</span></td>
<td id="S4.T2.3.3.6.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.3.3.6.6.1" class="ltx_text ltx_font_bold">42.6</span></td>
<td id="S4.T2.3.3.6.7" class="ltx_td ltx_align_center">41.4</td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_left ltx_border_t">InternLM2-7B</td>
<td id="S4.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ—</td>
<td id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t">39.0</td>
<td id="S4.T2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">33.4</td>
<td id="S4.T2.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t">56.8</td>
<td id="S4.T2.2.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.2.7.1" class="ltx_text ltx_font_bold">54.1</span></td>
<td id="S4.T2.2.2.2.1" class="ltx_td ltx_align_center ltx_border_t">25.6<sup id="S4.T2.2.2.2.1.1" class="ltx_sup"><span id="S4.T2.2.2.2.1.1.1" class="ltx_text ltx_font_italic">â€ </span></sup>
</td>
</tr>
<tr id="S4.T2.3.3.7" class="ltx_tr">
<td id="S4.T2.3.3.7.1" class="ltx_td ltx_align_left">w/ Direct Fine-tuning</td>
<td id="S4.T2.3.3.7.2" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S4.T2.3.3.7.3" class="ltx_td ltx_align_center">43.3</td>
<td id="S4.T2.3.3.7.4" class="ltx_td ltx_align_center ltx_border_r">40.9</td>
<td id="S4.T2.3.3.7.5" class="ltx_td ltx_align_center">54.5</td>
<td id="S4.T2.3.3.7.6" class="ltx_td ltx_align_center ltx_border_r">40.6</td>
<td id="S4.T2.3.3.7.7" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.7.7.1" class="ltx_text ltx_font_bold">44.5</span></td>
</tr>
<tr id="S4.T2.3.3.8" class="ltx_tr">
<td id="S4.T2.3.3.8.1" class="ltx_td ltx_align_left">w/ Mixed Pre-training</td>
<td id="S4.T2.3.3.8.2" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S4.T2.3.3.8.3" class="ltx_td ltx_align_center">47.6</td>
<td id="S4.T2.3.3.8.4" class="ltx_td ltx_align_center ltx_border_r">37.2</td>
<td id="S4.T2.3.3.8.5" class="ltx_td ltx_align_center">58.4</td>
<td id="S4.T2.3.3.8.6" class="ltx_td ltx_align_center ltx_border_r">45.6</td>
<td id="S4.T2.3.3.8.7" class="ltx_td ltx_align_center">42.4</td>
</tr>
<tr id="S4.T2.3.3.9" class="ltx_tr">
<td id="S4.T2.3.3.9.1" class="ltx_td ltx_align_left">w/ Insturction-tuning</td>
<td id="S4.T2.3.3.9.2" class="ltx_td ltx_align_center ltx_border_r">âœ—</td>
<td id="S4.T2.3.3.9.3" class="ltx_td ltx_align_center">49.4</td>
<td id="S4.T2.3.3.9.4" class="ltx_td ltx_align_center ltx_border_r">43.9</td>
<td id="S4.T2.3.3.9.5" class="ltx_td ltx_align_center">58.0</td>
<td id="S4.T2.3.3.9.6" class="ltx_td ltx_align_center ltx_border_r">50.4</td>
<td id="S4.T2.3.3.9.7" class="ltx_td ltx_align_center">6.2</td>
</tr>
<tr id="S4.T2.3.3.10" class="ltx_tr">
<td id="S4.T2.3.3.10.1" class="ltx_td ltx_align_left">w/ Mixed Instruction-tuning</td>
<td id="S4.T2.3.3.10.2" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S4.T2.3.3.10.3" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.10.3.1" class="ltx_text ltx_font_bold">64.6</span></td>
<td id="S4.T2.3.3.10.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.3.3.10.4.1" class="ltx_text ltx_font_bold">56.7</span></td>
<td id="S4.T2.3.3.10.5" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.10.5.1" class="ltx_text ltx_font_bold">63.4</span></td>
<td id="S4.T2.3.3.10.6" class="ltx_td ltx_align_center ltx_border_r">52.4</td>
<td id="S4.T2.3.3.10.7" class="ltx_td ltx_align_center">44.0</td>
</tr>
<tr id="S4.T2.3.3.3" class="ltx_tr">
<td id="S4.T2.3.3.3.2" class="ltx_td ltx_align_left ltx_border_t">LLaMA3-8B</td>
<td id="S4.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ—</td>
<td id="S4.T2.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t">35.4</td>
<td id="S4.T2.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20.1</td>
<td id="S4.T2.3.3.3.6" class="ltx_td ltx_align_center ltx_border_t">59.1</td>
<td id="S4.T2.3.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">45.1</td>
<td id="S4.T2.3.3.3.1" class="ltx_td ltx_align_center ltx_border_t">29.2<sup id="S4.T2.3.3.3.1.1" class="ltx_sup"><span id="S4.T2.3.3.3.1.1.1" class="ltx_text ltx_font_italic">â€ </span></sup>
</td>
</tr>
<tr id="S4.T2.3.3.11" class="ltx_tr">
<td id="S4.T2.3.3.11.1" class="ltx_td ltx_align_left">w/ Direct Fine-tuning</td>
<td id="S4.T2.3.3.11.2" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S4.T2.3.3.11.3" class="ltx_td ltx_align_center">43.2</td>
<td id="S4.T2.3.3.11.4" class="ltx_td ltx_align_center ltx_border_r">39.0</td>
<td id="S4.T2.3.3.11.5" class="ltx_td ltx_align_center">50.6</td>
<td id="S4.T2.3.3.11.6" class="ltx_td ltx_align_center ltx_border_r">35.1</td>
<td id="S4.T2.3.3.11.7" class="ltx_td ltx_align_center">44.8</td>
</tr>
<tr id="S4.T2.3.3.12" class="ltx_tr">
<td id="S4.T2.3.3.12.1" class="ltx_td ltx_align_left">w/ Mixed Pre-training</td>
<td id="S4.T2.3.3.12.2" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S4.T2.3.3.12.3" class="ltx_td ltx_align_center">47.6</td>
<td id="S4.T2.3.3.12.4" class="ltx_td ltx_align_center ltx_border_r">40.9</td>
<td id="S4.T2.3.3.12.5" class="ltx_td ltx_align_center">55.6</td>
<td id="S4.T2.3.3.12.6" class="ltx_td ltx_align_center ltx_border_r">41.1</td>
<td id="S4.T2.3.3.12.7" class="ltx_td ltx_align_center">42.6</td>
</tr>
<tr id="S4.T2.3.3.13" class="ltx_tr">
<td id="S4.T2.3.3.13.1" class="ltx_td ltx_align_left">w/ Insturction-tuning</td>
<td id="S4.T2.3.3.13.2" class="ltx_td ltx_align_center ltx_border_r">âœ—</td>
<td id="S4.T2.3.3.13.3" class="ltx_td ltx_align_center">49.8</td>
<td id="S4.T2.3.3.13.4" class="ltx_td ltx_align_center ltx_border_r">45.7</td>
<td id="S4.T2.3.3.13.5" class="ltx_td ltx_align_center">57.6</td>
<td id="S4.T2.3.3.13.6" class="ltx_td ltx_align_center ltx_border_r">47.9</td>
<td id="S4.T2.3.3.13.7" class="ltx_td ltx_align_center">8.6</td>
</tr>
<tr id="S4.T2.3.3.14" class="ltx_tr">
<td id="S4.T2.3.3.14.1" class="ltx_td ltx_align_left ltx_border_bb">w/ Mixed Instruction-tuning</td>
<td id="S4.T2.3.3.14.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">âœ“</td>
<td id="S4.T2.3.3.14.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.3.14.3.1" class="ltx_text ltx_font_bold">64.8</span></td>
<td id="S4.T2.3.3.14.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T2.3.3.14.4.1" class="ltx_text ltx_font_bold">57.9</span></td>
<td id="S4.T2.3.3.14.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.3.14.5.1" class="ltx_text ltx_font_bold">71.2</span></td>
<td id="S4.T2.3.3.14.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T2.3.3.14.6.1" class="ltx_text ltx_font_bold">53.1</span></td>
<td id="S4.T2.3.3.14.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.3.14.7.1" class="ltx_text ltx_font_bold">45.0</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results of models trained with Case2CodeÂ synthetic dataset and the corresponding generalization performance. Case2CodeÂ performance are evaluated with zero-shot prompting, except results with <sup id="S4.T2.7.1" class="ltx_sup"><span id="S4.T2.7.1.1" class="ltx_text ltx_font_italic">â€ </span></sup>, which are evaluated with four-shot prompting.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Zero-shot Case2CodeÂ Performance</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">As shown in TableÂ <a href="#S4.T1" title="Table 1 â€£ 4 Experiment â€£ Case2Code: Learning Inductive Reasoning with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we report the zero-shot Case2CodeÂ performance of different representative LLMs and their programming performance. We can find that the zero-shot Case2CodeÂ performance of representative models is strongly related to their corresponding program synthesis performance. Models with higher program synthesis scores tend to achieve higher Case2CodeÂ performance. And larger models often outperform small models. This indicates that Case2CodeÂ can become a good benchmark to reflect the code reasoning performance of different LLMs. However, the zero-shot Case2CodeÂ scores of LLMs have a large gap compared with their coding accuracy, which demonstrates that existing LLMs are better at some types of reasoning (e.g. writing programs based on instructions) than others (e.g. inductive programs by their behaviors). This can be explained as the LLMs are trained with massive program generation data but fewer samples similar to Case2CodeÂ that need inductive reasoning. Similar to the Reverse CurseÂ <cite class="ltx_cite ltx_citemacro_cite">Berglund etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite>, models trained with deductive reasoning data struggle to transfer to inductive reasoning tasks.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Generalization of Case2Code</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">One essential issue of synthetic data is its generalization ability. Therefore, we train different LLMs with our synthetic Case2CodeÂ dataset under various settings to explore how it affects the learning of code reasoning of LLMs.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Direct Fine-tuning</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">First, we find that LLMs that are directly trained on the Case2CodeÂ reasoning samples can effectively learn coding based on cases. As shown in TableÂ <a href="#S4.T2" title="Table 2 â€£ Models â€£ 4.1 Experimental Setup â€£ 4 Experiment â€£ Case2Code: Learning Inductive Reasoning with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, by direct fine-tuning, Internlm2-7B and LLaMA3-8B can significantly outperform the few-shot prompting baselines by up to 18.9%, achieve up to 44.5% and 42.0% accuracy on Case2CodeÂ evaluation set, respectively, which even outperforms the more powerful LLMs like LLaMA3-70B, GPT-3.5, and comparable with GPT-4 (results in TableÂ <a href="#S4.T1" title="Table 1 â€£ 4 Experiment â€£ Case2Code: Learning Inductive Reasoning with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Moreover, models trained with Case2CodeÂ reasoning also improve their program synthesis performance on benchmarks like HumanEval and MBPP. This indicates that the Case2CodeÂ reasoning is general and challenging. Training on Case2CodeÂ samples not only boosts the inductive reasoning performance in distribution but enhances the code understanding and code generation abilities of LLMs. As the Case2CodeÂ samples can be synthetic at scale, we believe that synthesizing large-scale and high-quality inductive reasoning data is a promising path to consistently improve LLMs without exhausting data.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Mixed Training</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">Then, we explore how to better incorporate our synthetic Case2CodeÂ data into different stages of LLM training to enhance the reasoning ability of LLMs in general. Specifically, we train LLMs with two variants of data mixing, either during pre-training or in the supervised fine-tuning (SFT) stage. The first mixing strategy introduces natural language pre-training texts from the PileÂ <cite class="ltx_cite ltx_citemacro_cite">Gao etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite> and the code pre-training samples from The StackÂ <cite class="ltx_cite ltx_citemacro_cite">Kocetkov etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite>. The mixing ratio is 1:1:2 for samples from the Pile, The Stack, and the Case2CodeÂ dataset, respectively. On the other hand, we incorporate a supervised fine-tuning (SFT) dataset from WizardCoderÂ <cite class="ltx_cite ltx_citemacro_cite">Luo etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite> to demonstrate that the performance gain of Case2CodeÂ training does not come from the understanding of instructions but the learning of inductive reasoning of code execution. We combine the SFT dataset with Case2CodeÂ samples in a 1:3 ratio, as the size of our synthetic dataset is much larger.</p>
</div>
<section id="S4.SS3.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Mixed Pre-training</h5>

<div id="S4.SS3.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS2.Px1.p1.1" class="ltx_p">As shown in TableÂ <a href="#S4.T2" title="Table 2 â€£ Models â€£ 4.1 Experimental Setup â€£ 4 Experiment â€£ Case2Code: Learning Inductive Reasoning with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, when incorporated into the pre-training stage, the Case2CodeÂ training data helps the model to connect the execution states with the function implementation, which further facilitates the program synthesis performance of these LLMs. Compared with directly fine-tuned on Case2CodeÂ dataset, training these samples with pre-training texts enables the generalization of inductive reasoning of code states learned by the Case2CodeÂ task.</p>
</div>
</section>
<section id="S4.SS3.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Mixed Instruction-tuning</h5>

<div id="S4.SS3.SSS2.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.Px2.p1.1" class="ltx_p">When trained with instruction-following datasets, the Case2CodeÂ data also improves the performance of the programming with instruction tasks, as reported in TableÂ <a href="#S4.T2" title="Table 2 â€£ Models â€£ 4.1 Experimental Setup â€£ 4 Experiment â€£ Case2Code: Learning Inductive Reasoning with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We evaluate the SFT models with the zero-shot instructed version of programming synthesis tasks, HumanEval, and MBPP. We find that incorporating Case2CodeÂ data boosts the performance of various LLMs on code generation tasks. Compared to the corresponding SFT baselines, InternLM2-7B improves on HumanEval from 49.4% to 64.6%, with more than 10% improvements. LLaMA3-8B achieves 64.6%, 57.9%, and 71.2% on HumanEval, HumanEval+, and MBPP, respectively, with significant improvements compared to the SFT version. These results demonstrate the effectiveness of learning on Case2CodeÂ and the necessity of incorporating inductive reasoning data into LLM training.</p>
</div>
</section>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Study</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">In this section, we conduct ablation studies to demonstrate the effectiveness of the Case2CodeÂ synthetic pipeline across different families and scales of LLMs.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2407.12504/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="221" height="172" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Downstream results when directly fine-tuning InternLM2-7B with different Case2CodeÂ prompt templates. Diverse prompts not only help the model to learn Case2CodeÂ reasoning but also significantly advance the generalization of the code inductive reasoning.</figcaption>
</figure>
<section id="S4.SS4.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Prompt Diversity</h5>

<div id="S4.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px1.p1.1" class="ltx_p">Since the synthetic Case2CodeÂ training data is converted by triples of (<span id="S4.SS4.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">programs</span>, <span id="S4.SS4.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_bold ltx_font_italic">inputs</span>, <span id="S4.SS4.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_bold ltx_font_italic">outputs</span>), during the construction, the prompt templates are utilized to embed the input-output pairs to form natural language texts for LLM to learn. As the LLM can only rely on these converted prompts to learn the Case2Code, it is important to understand the effectiveness of how different prompt templates affect the training of LLMs. Intuitively, the diversity of prompt templates plays an important role in the learning of LLMs. Therefore, we compare synthetic data prompted using a single template style with data utilizing diverse styles of templates. The result is reported in FigureÂ <a href="#S4.F3" title="Figure 3 â€£ 4.4 Ablation Study â€£ 4 Experiment â€£ Case2Code: Learning Inductive Reasoning with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, in which diverse prompts may have little effect on the in-domain Case2CodeÂ performance, however, the diversity significantly affects the accuracy of LLMs on out-of-domain program synthesis tasks. It is indicated that diversity can be critical during LLM learning, which also has been discussed in other domains like in general natural language processing tasksÂ <cite class="ltx_cite ltx_citemacro_cite">Wei etÂ al. (<a href="#bib.bib26" title="" class="ltx_ref">2022a</a>)</cite> and alignmentÂ <cite class="ltx_cite ltx_citemacro_cite">Ouyang etÂ al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>); Wang etÂ al. (<a href="#bib.bib25" title="" class="ltx_ref">2023b</a>)</cite>.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:208.1pt;height:41.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-31.6pt,6.3pt) scale(0.76724099187968,0.76724099187968) ;">
<table id="S4.T3.2.2" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.2.2.3" class="ltx_tr">
<td id="S4.T3.2.2.3.1" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S4.T3.2.2.3.2" class="ltx_td ltx_align_center ltx_border_tt">TP</td>
<td id="S4.T3.2.2.3.3" class="ltx_td ltx_align_center ltx_border_tt">TGS</td>
<td id="S4.T3.2.2.3.4" class="ltx_td ltx_align_center ltx_border_tt">Costs</td>
<td id="S4.T3.2.2.3.5" class="ltx_td ltx_align_center ltx_border_tt"># Samples</td>
</tr>
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">InternLM2-7B</td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">1600 tokens/s</td>
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">1<math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><times id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\times</annotation></semantics></math>
</td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">1.3M</td>
</tr>
<tr id="S4.T3.2.2.2" class="ltx_tr">
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">LLaMA3-70B</td>
<td id="S4.T3.2.2.2.3" class="ltx_td ltx_align_center ltx_border_bb">4</td>
<td id="S4.T3.2.2.2.4" class="ltx_td ltx_align_center ltx_border_bb">720 tokens/s</td>
<td id="S4.T3.2.2.2.1" class="ltx_td ltx_align_center ltx_border_bb">4.5<math id="S4.T3.2.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.2.2.2.1.m1.1a"><mo id="S4.T3.2.2.2.1.m1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b"><times id="S4.T3.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">\times</annotation></semantics></math>
</td>
<td id="S4.T3.2.2.2.5" class="ltx_td ltx_align_center ltx_border_bb">700K</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Efficiency of using different LLM Writers for Input Generation. â€œTPâ€ refers to the size of the tensor parallel for inference. â€œTGSâ€ refers to the inference throughput (tokens/s) of each LLM instance. â€œCostsâ€ refers to the relative compute costs of different LLM generators. Due to the large TP and low throughput, the large LMs can be more costly than the small LMs when inferencing on the same number of GPUs. In our data synthetic process, using LLaMA3-70B costs about 9<math id="S4.T3.5.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.5.m1.1b"><mo id="S4.T3.5.m1.1.1" xref="S4.T3.5.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.m1.1c"><times id="S4.T3.5.m1.1.1.cmml" xref="S4.T3.5.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.m1.1d">\times</annotation></semantics></math> compute resources compared to small models like InternLM2-7B. Due to the high costs of LLaMA3-70B, we only sub-sample the raw data to run the data synthesis. The total costs are still 4.5<math id="S4.T3.6.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.6.m2.1b"><mo id="S4.T3.6.m2.1.1" xref="S4.T3.6.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.m2.1c"><times id="S4.T3.6.m2.1.1.cmml" xref="S4.T3.6.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.m2.1d">\times</annotation></semantics></math> compared to InternLM2-7B.</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2407.12504/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="152" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Downstream results when fine-tuning InternLM2-7B with synthetic data using different input example generators. Generator â€œNoneâ€ refers to the baseline InternLM2-7B not trained on any Case2CodeÂ data. The computational overhead of using LLaMA3-70B is <math id="S4.F4.2.m1.1" class="ltx_math_unparsed" alttext="4.5\times" display="inline"><semantics id="S4.F4.2.m1.1b"><mrow id="S4.F4.2.m1.1c"><mn id="S4.F4.2.m1.1.1">4.5</mn><mo lspace="0.222em" id="S4.F4.2.m1.1.2">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S4.F4.2.m1.1d">4.5\times</annotation></semantics></math> that of InternLM2-7B.</figcaption>
</figure>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T4.1.1" class="ltx_tr">
<td id="S4.T4.1.1.1" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S4.T4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">HumanEval</td>
<td id="S4.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">HumanEval+</td>
<td id="S4.T4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">MBPP</td>
<td id="S4.T4.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">MBPP+</td>
<td id="S4.T4.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">Case2Code</td>
</tr>
<tr id="S4.T4.1.2" class="ltx_tr">
<td id="S4.T4.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">InternLM2-1.8B</td>
<td id="S4.T4.1.2.2" class="ltx_td ltx_align_center ltx_border_t">32.3</td>
<td id="S4.T4.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.9</td>
<td id="S4.T4.1.2.4" class="ltx_td ltx_align_center ltx_border_t">43.6</td>
<td id="S4.T4.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24.3</td>
<td id="S4.T4.1.2.6" class="ltx_td ltx_align_center ltx_border_t">27.8</td>
</tr>
<tr id="S4.T4.1.3" class="ltx_tr">
<td id="S4.T4.1.3.1" class="ltx_td ltx_align_left ltx_border_r">InternLM2-7B</td>
<td id="S4.T4.1.3.2" class="ltx_td ltx_align_center">64.6</td>
<td id="S4.T4.1.3.3" class="ltx_td ltx_align_center ltx_border_r">56.7</td>
<td id="S4.T4.1.3.4" class="ltx_td ltx_align_center">63.4</td>
<td id="S4.T4.1.3.5" class="ltx_td ltx_align_center ltx_border_r">52.4</td>
<td id="S4.T4.1.3.6" class="ltx_td ltx_align_center">42.2</td>
</tr>
<tr id="S4.T4.1.4" class="ltx_tr">
<td id="S4.T4.1.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">InternLM2-20B</td>
<td id="S4.T4.1.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.4.2.1" class="ltx_text ltx_font_bold">73.1</span></td>
<td id="S4.T4.1.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T4.1.4.3.1" class="ltx_text ltx_font_bold">65.2</span></td>
<td id="S4.T4.1.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.4.4.1" class="ltx_text ltx_font_bold">77.4</span></td>
<td id="S4.T4.1.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T4.1.4.5.1" class="ltx_text ltx_font_bold">55.4</span></td>
<td id="S4.T4.1.4.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.4.6.1" class="ltx_text ltx_font_bold">46.0</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Code results with different scales of models, after supervised fine-tuning on the instruction-following dataset mixed with Case2CodeÂ synthetic data.</figcaption>
</figure>
</section>
<section id="S4.SS4.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">LLM for Generating Inputs</h5>

<div id="S4.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px2.p1.1" class="ltx_p">During the synthesis of Case2CodeÂ data, a critical step is prompting the LLM to write several input examples for each program. These inputs are then executed with the corresponding programs one by one to obtain the program outputs, thus we can utilize these important contexts to construct Case2CodeÂ training data. To explore whether the reasoning ability of the LLM writer affects the synthetic data quality, we replace the LLM generator from Interlm2-7B to LLaMA3-70B, and rerun the data synthesis pipeline to obtain a new version of Case2CodeÂ training data. Due to the high costs of LLaMA3-70B, we only generate half the size of our original synthetic data. Detailed generation costs are reported in TableÂ <a href="#S4.T3" title="Table 3 â€£ Prompt Diversity â€£ 4.4 Ablation Study â€£ 4 Experiment â€£ Case2Code: Learning Inductive Reasoning with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We train Interlm2-7B with this version of Case2CodeÂ dataset under the instruction-tuning setup to evaluate the data quality. As shown in FigureÂ <a href="#S4.F4" title="Figure 4 â€£ Prompt Diversity â€£ 4.4 Ablation Study â€£ 4 Experiment â€£ Case2Code: Learning Inductive Reasoning with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, compared with the InternLM2-7B generator, large LMs like LLaMA3-70B can write high-quality input samples that help trained LLMs to achieve comparable code reasoning capability with fewer training data. It indicates that the input generation step can affect the overall synthetic data quality, suggesting data collectors choose a strong LLM to be the input writer if compute resources are sufficient. However, we note that LLaMA3-70B contains too many parameters that are <math id="S4.SS4.SSS0.Px2.p1.1.m1.1" class="ltx_math_unparsed" alttext="4.5\times" display="inline"><semantics id="S4.SS4.SSS0.Px2.p1.1.m1.1a"><mrow id="S4.SS4.SSS0.Px2.p1.1.m1.1b"><mn id="S4.SS4.SSS0.Px2.p1.1.m1.1.1">4.5</mn><mo lspace="0.222em" id="S4.SS4.SSS0.Px2.p1.1.m1.1.2">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px2.p1.1.m1.1c">4.5\times</annotation></semantics></math> more costly than InternLM2-7B. By generating inputs with InternLM2-7B, our Case2CodeÂ data synthesis framework maintains generation efficiency and data quality. It also demonstrates the possibility of self-improving for LLMs on their code reasoning capabilities.</p>
</div>
</section>
<section id="S4.SS4.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Model Scale</h5>

<div id="S4.SS4.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px3.p1.1" class="ltx_p">We want to explore whether the Case2CodeÂ data synthesized using a small model can still improve a large model, and how the model scale affects the learning process. Therefore, we use Case2CodeÂ data generated with InternLM2-7B to train models in the InternLM2 series to investigate these questions. The training is taken under the setting of data mixing with SFT datasetÂ <cite class="ltx_cite ltx_citemacro_cite">Luo etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite> and the results are shown in TableÂ <a href="#S4.T4" title="Table 4 â€£ Prompt Diversity â€£ 4.4 Ablation Study â€£ 4 Experiment â€£ Case2Code: Learning Inductive Reasoning with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Our synthetic data consistently enhances the code reasoning performance of various sizes of LLMs, even though one of the student models is almost three times larger than the model used for data synthesis. These results demonstrate the possibilities of weak-to-strong supervision in code-related tasks at scale.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We first construct a new benchmark Case2CodeÂ to evaluate the inductive reasoning capability of LLMs in the code domain. Then, we propose a data synthetic framework to construct Case2CodeÂ training samples at scale. By just using small LLMs and a code interpreter, we can collect high-quality Case2CodeÂ data from pre-training code texts automatically and efficiently. By training on various LLMs in multiple settings, we demonstrate the Case2CodeÂ can improve not only the inductive reasoning ability of LLM but also the general coding capabilities. We believe synthetic Case2CodeÂ is a promising way to continue improving the LLMs when human-generated data is exhausted.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">In this work, we study Case2Code, a synthetic task for learning inductive reasoning capabilities. Our work is still limited in several aspects:</p>
<ul id="Sx1.I1" class="ltx_itemize">
<li id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx1.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i1.p1.1" class="ltx_p">Potential harmful programs: we gather and filter programs from the pre-training code corpus, which excludes code that may contain dangerous operations like system calls, file manipulation, and network traffic that require careful safety checks and vulnerability mitigation. In the future one can incorporate a safe and reliable execution environment that supports these operations for Case2Code synthesis.</p>
</div>
</li>
<li id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx1.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i2.p1.1" class="ltx_p">Programming languages: we focus on synthesizing Case2Code data using Python programs, as it is a commonly used programming language and can be easily and reliably manipulated and executed. Future work can extend the data synthesis framework to more programming languages and applications.</p>
</div>
</li>
<li id="Sx1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx1.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i3.p1.1" class="ltx_p">Long context: some inputs or outputs of the given programs can be extremely long, which can be challenging to fit into the context window of current LLMs. Future work can explore efficient methods of representing and learning long-context case-to-code induction.</p>
</div>
</li>
<li id="Sx1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx1.I1.i4.p1" class="ltx_para">
<p id="Sx1.I1.i4.p1.1" class="ltx_p">Data modality: we represent cases in our Case2Code data as texts for LLM training, however, real-world programs often interact with multi-modal inputs and outputs like audio, image, and video. How to effectively collect and learn multi-modal inductive reasoning remains a big challenge.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI@Meta (2024)</span>
<span class="ltx_bibblock">
AI@Meta. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md" title="" class="ltx_ref ltx_href">Llama 3 model card</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balog etÂ al. (2016)</span>
<span class="ltx_bibblock">
Matej Balog, AlexanderÂ L Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow. 2016.

</span>
<span class="ltx_bibblock">Deepcoder: Learning to write programs.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1611.01989</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berglund etÂ al. (2023)</span>
<span class="ltx_bibblock">
Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, AsaÂ Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2309.12288" title="" class="ltx_ref ltx_href">The reversal curse: Llms trained on "a is b" fail to learn "b is a"</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2309.12288.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai etÂ al. (2024)</span>
<span class="ltx_bibblock">
Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, QiÂ Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, LiÂ Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, YuÂ Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, and etÂ al. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2403.17297" title="" class="ltx_ref ltx_href">Internlm2 technical report</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2403.17297.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2024)</span>
<span class="ltx_bibblock">
Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:269605484" title="" class="ltx_ref ltx_href">Alphamath almost zero: process supervision without process</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2405.03553.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe etÂ al. (2021)</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2110.14168" title="" class="ltx_ref ltx_href">Training verifiers to solve math word problems</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2110.14168.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin etÂ al. (2017)</span>
<span class="ltx_bibblock">
Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://proceedings.mlr.press/v70/devlin17a.html" title="" class="ltx_ref ltx_href">Robustfill: Neural program learning under noisy I/O</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017</em>, volumeÂ 70 of <em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 990â€“998. PMLR.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ellis etÂ al. (2021)</span>
<span class="ltx_bibblock">
Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias SablÃ©-Meyer, Lucas Morales, Luke Hewitt, Luc Cary, Armando Solar-Lezama, and JoshuaÂ B Tenenbaum. 2021.

</span>
<span class="ltx_bibblock">Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 42nd acm sigplan international conference on programming language design and implementation</em>, pages 835â€“850.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2021)</span>
<span class="ltx_bibblock">
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2101.00027" title="" class="ltx_ref ltx_href">The pile: An 800gb dataset of diverse text for language modeling</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2101.00027.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks etÂ al. (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html" title="" class="ltx_ref ltx_href">Measuring mathematical problem solving with the MATH dataset</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jiaxin Huang, Shixiang Gu, LeÂ Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/2023.EMNLP-MAIN.67" title="" class="ltx_ref ltx_href">Large language models can self-improve</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>, pages 1051â€“1068. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and Chang (2022)</span>
<span class="ltx_bibblock">
Jie Huang and Kevin Chen-Chuan Chang. 2022.

</span>
<span class="ltx_bibblock">Towards reasoning in large language models: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.10403</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocetkov etÂ al. (2022)</span>
<span class="ltx_bibblock">
Denis Kocetkov, Raymond Li, LoubnaÂ Ben Allal, Jia Li, Chenghao Mou, CarlosÂ MuÃ±oz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm deÂ Vries. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2211.15533" title="" class="ltx_ref ltx_href">The stack: 3 TB of permissively licensed source code</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2211.15533.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ziyang Luo, Can Xu, PuÂ Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2306.08568" title="" class="ltx_ref ltx_href">Wizardcoder: Empowering code large language models with evol-instruct</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2306.08568.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitra etÂ al. (2024)</span>
<span class="ltx_bibblock">
Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2402.14830" title="" class="ltx_ref ltx_href">Orca-math: Unlocking the potential of slms in grade school math</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2402.14830.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2303.08774" title="" class="ltx_ref ltx_href">GPT-4 technical report</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.08774.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, XuÂ Jiang, Diogo Almeida, CarrollÂ L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, PaulÂ F. Christiano, Jan Leike, and Ryan Lowe. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html" title="" class="ltx_ref ltx_href">Training language models to follow instructions with human feedback</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">RoziÃ¨re etÂ al. (2023)</span>
<span class="ltx_bibblock">
Baptiste RoziÃ¨re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, XiaoqingÂ Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, JÃ©rÃ©my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre DÃ©fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2308.12950" title="" class="ltx_ref ltx_href">Code llama: Open foundation models for code</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2308.12950.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi etÂ al. (2023)</span>
<span class="ltx_bibblock">
Kensen Shi, Joey Hong, Manzil Zaheer, Pengcheng Yin, and Charles Sutton. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2307.13883" title="" class="ltx_ref ltx_href">Exedec: Execution decomposition for compositional generalization in neural program synthesis</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.13883.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silver etÂ al. (2016)</span>
<span class="ltx_bibblock">
David Silver, Aja Huang, ChrisÂ J. Maddison, Arthur Guez, Laurent Sifre, George vanÂ den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, TimothyÂ P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1038/NATURE16961" title="" class="ltx_ref ltx_href">Mastering the game of go with deep neural networks and tree search</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Nat.</em>, 529(7587):484â€“489.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, PunitÂ Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, EricÂ Michael Smith, Ranjan Subramanian, XiaoqingÂ Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, JianÂ Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, AurÃ©lien Rodriguez, Robert Stojnic, Sergey Edunov,
and Thomas Scialom. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2307.09288" title="" class="ltx_ref ltx_href">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.09288.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Peiyi Wang, Lei Li, Zhihong Shao, R.Â X. Xu, Damai Dai, Yifei Li, Deli Chen, Y.Â Wu, and Zhifang Sui. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2312.08935" title="" class="ltx_ref ltx_href">Math-shepherd: Verify and reinforce llms step-by-step without human annotations</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2312.08935.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, EdÂ Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022a.

</span>
<span class="ltx_bibblock">Self-consistency improves chain of thought reasoning in language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.11171</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, EdÂ Huai hsin Chi, and Denny Zhou. 2022b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:247595263" title="" class="ltx_ref ltx_href">Self-consistency improves chain of thought reasoning in language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2203.11171.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, NoahÂ A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/2023.ACL-LONG.754" title="" class="ltx_ref ltx_href">Self-instruct: Aligning language models with self-generated instructions</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 13484â€“13508. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, VincentÂ Y. Zhao, Kelvin Guu, AdamsÂ Wei Yu, Brian Lester, Nan Du, AndrewÂ M. Dai, and QuocÂ V. Le. 2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=gEZrGCozdqR" title="" class="ltx_ref ltx_href">Finetuned language models are zero-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, EdÂ H. Chi, QuocÂ V. Le, and Denny Zhou. 2022b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html" title="" class="ltx_ref ltx_href">Chain-of-thought prompting elicits reasoning in large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weston etÂ al. (2015)</span>
<span class="ltx_bibblock">
Jason Weston, Antoine Bordes, Sumit Chopra, AlexanderÂ M Rush, Bart VanÂ MerriÃ«nboer, Armand Joulin, and Tomas Mikolov. 2015.

</span>
<span class="ltx_bibblock">Towards ai-complete question answering: A set of prerequisite toy tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1502.05698</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Zonglin Yang, LiÂ Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, and Furu Wei. 2022.

</span>
<span class="ltx_bibblock">Language models as inductive reasoners.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.10923</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, ThomasÂ L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:258762525" title="" class="ltx_ref ltx_href">Tree of thoughts: Deliberate problem solving with large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2305.10601.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, YuÂ Zhang, JamesÂ T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023.

</span>
<span class="ltx_bibblock">Metamath: Bootstrap your own mathematical questions for large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.12284</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2308.01825" title="" class="ltx_ref ltx_href">Scaling relationship on learning mathematical reasoning with large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2308.01825.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Prompts Used in Case2Code</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">We demonstrate the prompts used during the Case2CodeÂ synthesis, training, and evaluation as follows:</p>
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p">The prompt template for evaluating zero-shot Case2CodeÂ performance of various LLMs is listed in TableÂ <a href="#A1.T5" title="Table 5 â€£ Appendix A Prompts Used in Case2Code â€£ Case2Code: Learning Inductive Reasoning with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p">We show the prompt for using LLMs as input generators for synthesizing Case2CodeÂ data in TableÂ <a href="#A1.T6" title="Table 6 â€£ Appendix A Prompts Used in Case2Code â€£ Case2Code: Learning Inductive Reasoning with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p">We randomly sample some Case2CodeÂ  data to demonstrate in TableÂ <a href="#A1.T7" title="Table 7 â€£ Appendix A Prompts Used in Case2Code â€£ Case2Code: Learning Inductive Reasoning with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
</li>
</ul>
</div>
<figure id="A1.T5" class="ltx_table">
<table id="A1.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A1.T5.1.1" class="ltx_tr">
<td id="A1.T5.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A1.T5.1.1.1.1" class="ltx_text ltx_font_bold">Prompt Template for Zero-shot Case2CodeÂ Evaluation.</span></td>
</tr>
<tr id="A1.T5.1.2" class="ltx_tr">
<td id="A1.T5.1.2.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="A1.T5.1.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:346.9pt;"><span id="A1.T5.1.2.1.1.1" class="ltx_ERROR undefined">{spverbatim}</span>
<span id="A1.T5.1.2.1.1.2" class="ltx_p">prompt</span>
<span id="A1.T5.1.2.1.1.3" class="ltx_p">Please write the correct names of arguments. As the function you implement will be called by: func_name(**input_dict). Keep the original type. No need to convert the output to string.</span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Prompt template for zero-shot Case2CodeÂ evaluation. We inject <span id="A1.T5.4.1" class="ltx_text ltx_font_typewriter">{prompt}</span> and <span id="A1.T5.5.2" class="ltx_text ltx_font_typewriter">{func_name}</span> for each test sample for evaluation.</figcaption>
</figure>
<figure id="A1.T6" class="ltx_table">
<table id="A1.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A1.T6.1.1" class="ltx_tr">
<td id="A1.T6.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A1.T6.1.1.1.1" class="ltx_text ltx_font_bold">Prompt for LLM Input Generator</span></td>
</tr>
<tr id="A1.T6.1.2" class="ltx_tr">
<td id="A1.T6.1.2.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="A1.T6.1.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:346.9pt;"><span id="A1.T6.1.2.1.1.1" class="ltx_ERROR undefined">{spverbatim}</span>
<span id="A1.T6.1.2.1.1.2" class="ltx_p">Given the function, first analyze the types of the function arguments, then write 10 different example inputs for the function, each example should be a dict with function argumentsâ€™ names and their values.
Output format:
â€œâ€˜python
examples = [
dict(argname=argvalue),
â€¦.
]
â€œâ€˜</span>
<span id="A1.T6.1.2.1.1.3" class="ltx_p">Function:
â€œâ€˜python
def test_func(a: int, b: str) -&gt; str:
return str(a) + b
â€œâ€˜
Examples:
â€œâ€˜python
examples = [
dict(a=1, b=â€™aâ€™),
dict(a=2, b=â€™bâ€™),
dict(a=3, b=â€™câ€™),
dict(a=4, b=â€™dâ€™),
dict(a=5, b=â€™eâ€™),
dict(a=6, b=â€™fâ€™),
dict(a=7, b=â€™gâ€™),
dict(a=8, b=â€™hâ€™),
dict(a=9, b=â€™iâ€™),
dict(a=10, b=â€™jâ€™),
]
â€œâ€˜</span>
<span id="A1.T6.1.2.1.1.4" class="ltx_p">Function:
â€œâ€˜python
code
â€œâ€˜
Examples:</span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Prompt for LLM input generator, we replace <span id="A1.T6.3.1" class="ltx_text ltx_font_typewriter">{code}</span> with programs collected in for Case2Code.</figcaption>
</figure>
<figure id="A1.T7" class="ltx_table">
<table id="A1.T7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A1.T7.1.1" class="ltx_tr">
<td id="A1.T7.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A1.T7.1.1.1.1" class="ltx_text ltx_font_bold">Case2CodeÂ Examples</span></td>
</tr>
<tr id="A1.T7.1.2" class="ltx_tr">
<td id="A1.T7.1.2.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="A1.T7.1.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:346.9pt;"><span id="A1.T7.1.2.1.1.1" class="ltx_ERROR undefined">{spverbatim}</span>
<span id="A1.T7.1.2.1.1.2" class="ltx_p">Input: dict(s="abcba", center=2), Output: "(5, 0, 4)"
Input: dict(s="abcdefg", center=3), Output: "(1, 3, 3)"
Input: dict(s="aba", center=1), Output: "(3, 0, 2)"
Input: dict(s="racecar", center=3), Output: "(7, 0, 6)"
Input: dict(s="madam", center=2), Output: "(5, 0, 4)"
Input: dict(s="abcabcabc", center=4), Output: "(1, 4, 4)"
Input: dict(s="xyzyx", center=2), Output: "(5, 0, 4)"
Input: dict(s="hello", center=2), Output: "(1, 2, 2)"
Input: dict(s="ab", center=0), Output: "(1, 0, 0)"
Input: dict(s="a", center=0), Output: "(1, 0, 0)"</span>
<span id="A1.T7.1.2.1.1.3" class="ltx_p">Write a function that generates the output from the input.
Function: greatest_palindrome_size_odd</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.3" class="ltx_tr">
<td id="A1.T7.1.3.1" class="ltx_td ltx_align_left">
<span id="A1.T7.1.3.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:346.9pt;"><span id="A1.T7.1.3.1.1.1" class="ltx_ERROR undefined">{spverbatim}</span>
<span id="A1.T7.1.3.1.1.2" class="ltx_p">The function is:</span>
<span id="A1.T7.1.3.1.1.3" class="ltx_p">â€œâ€˜python
def greatest_palindrome_size_odd(s, center):
right = center + 1
left = center - 1
size = 1
optimum_right = optimum_left = center
while left &gt;= 0 and right &lt; len(s):
if s[left] == s[right]:
size += 2
optimum_left = left
optimum_right = right
right += 1
left -= 1
else:
break
return size, optimum_left, optimum_right</span>
<span id="A1.T7.1.3.1.1.4" class="ltx_p">â€œâ€˜</span>
</span>
</td>
</tr>
<tr id="A1.T7.1.4" class="ltx_tr">
<td id="A1.T7.1.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="A1.T7.1.4.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:346.9pt;"><span id="A1.T7.1.4.1.1.1" class="ltx_ERROR undefined">{spverbatim}</span>
<span id="A1.T7.1.4.1.1.2" class="ltx_p">Arguments and results:
Input: dict(seq="ATCG", complementarity=â€™Aâ€™: â€™Uâ€™, â€™Tâ€™: â€™Aâ€™, â€™Câ€™: â€™Gâ€™, â€™Gâ€™: â€™Câ€™), Output: "â€™CGAUâ€™"
Input: "ATCG", â€™Aâ€™: â€™Tâ€™, â€™Tâ€™: â€™Aâ€™, â€™Câ€™: â€™Gâ€™, â€™Gâ€™: â€™Câ€™, Output: "â€™CGATâ€™"
Input: seq:"ACGT", complementarity:â€™Aâ€™: â€™Uâ€™, â€™Tâ€™: â€™Aâ€™, â€™Câ€™: â€™Gâ€™, â€™Gâ€™: â€™Câ€™, Output: "â€™ACGUâ€™"
Input: "ACGT", â€™Aâ€™: â€™Tâ€™, â€™Tâ€™: â€™Aâ€™, â€™Câ€™: â€™Gâ€™, â€™Gâ€™: â€™Câ€™, Output: "â€™ACGTâ€™"</span>
<span id="A1.T7.1.4.1.1.3" class="ltx_p">Please write a function to process the input arguments and produce the specified outputs.</span>
<span id="A1.T7.1.4.1.1.4" class="ltx_p">Start with the function:
reverse_complement</span>
<span id="A1.T7.1.4.1.1.5" class="ltx_p">The function is:</span>
<span id="A1.T7.1.4.1.1.6" class="ltx_p">â€œâ€˜python
def reverse_complement(seq, complementarity):
bases = list(seq)
bases = [complementarity[base] for base in bases]
reversed_complement = â€.join(bases)
return reversed_complement[::-1]</span>
<span id="A1.T7.1.4.1.1.7" class="ltx_p">â€œâ€˜</span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Case2CodeÂ data examples.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.12503" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.12504" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.12504">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.12504" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.12505" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 14:19:21 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
