<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting</title>
<!--Generated on Fri Dec  8 02:46:35 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2312.04807v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S1" title="1 Introduction â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S2" title="2 Related Work â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="3 Approach â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS1" title="3.1 Training â€£ 3 Approach â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS2" title="3.2 Inference â€£ 3 Approach â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Inference</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S3.SS3" title="3.3 Knowledge Acquisition â€£ 3 Approach â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Knowledge Acquisition</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS3.SSS0.Px1" title="Retrieving Similar Sentence â€£ 3.3 Knowledge Acquisition â€£ 3 Approach â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title">Retrieving Similar Sentence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS3.SSS0.Px2" title="Matching Terminology â€£ 3.3 Knowledge Acquisition â€£ 3 Approach â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title">Matching Terminology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS3.SSS0.Px3" title="Translation Template Prediction â€£ 3.3 Knowledge Acquisition â€£ 3 Approach â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title">Translation Template Prediction</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="4 Experiments â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS1" title="4.1 Setup â€£ 4 Experiments â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS1.SSS0.Px1" title="Corpus â€£ 4.1 Setup â€£ 4 Experiments â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title">Corpus</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS1.SSS0.Px2" title="Baseline â€£ 4.1 Setup â€£ 4 Experiments â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title">Baseline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS1.SSS0.Px3" title="Training Data â€£ 4.1 Setup â€£ 4 Experiments â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title">Training Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S4.SS1.SSS0.Px4" title="Test Data â€£ 4.1 Setup â€£ 4 Experiments â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title">Test Data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS2" title="4.2 Main Results â€£ 4 Experiments â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Main Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS3" title="4.3 Ablation Studies â€£ 4 Experiments â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Ablation Studies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS4" title="4.4 Effect of Similarity Threshold ğœ† â€£ 4 Experiments â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Effect of Similarity Threshold <math alttext="\lambda" class="ltx_Math" display="inline"><semantics><mi>Î»</mi><annotation-xml encoding="MathML-Content"><ci>ğœ†</ci></annotation-xml><annotation encoding="application/x-tex">\lambda</annotation><annotation encoding="application/x-llamapun">italic_Î»</annotation></semantics></math></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS5" title="4.5 Inference Speed â€£ 4 Experiments â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Inference Speed</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S5" title="5 Conclusions â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusions</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="package-alerts ltx_document" role="alert">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewbox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by selecting from this list of <a href="https://corpora.mathweb.org/corpus/arxmliv/tex_to_html/info/loaded_file" target="_blank">supported packages</a>.</p>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2312.04807v1 [cs.CL] 08 Dec 2023</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Ke Wang, Jun Xie<math alttext="{}^{\ast}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><ci id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1">âˆ—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">{}^{\ast}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT âˆ— end_FLOATSUPERSCRIPT</annotation></semantics></math>, Yuqi Zhang, Yu Zhao 
<br class="ltx_break"/>Alibaba Group 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id1">{wk258730,qingjing.xj,chenwei.zyq}@alibaba-inc.com,kongyu@taobao.com</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">Improving neural machine translation (NMT) systems with prompting has achieved significant progress in recent years. In this work, we focus on how to integrate multi-knowledge, multiple types of knowledge, into NMT models to enhance the performance with prompting. We propose a unified framework, which can integrate effectively multiple types of knowledge including sentences, terminologies/phrases and translation templates into NMT models. We utilize multiple types of knowledge as prefix-prompts of input for the encoder and decoder of NMT models to guide the translation process. The approach requires no changes to the model architecture and effectively adapts to domain-specific translation without retraining. The experiments on English-Chinese and English-German translation demonstrate that our approach significantly outperform strong baselines, achieving high translation quality and terminology match accuracy.</p>
</div>
<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><math alttext="{}^{\ast}" class="ltx_Math" display="inline" id="footnote1.m1.1"><semantics id="footnote1.m1.1b"><msup id="footnote1.m1.1.1" xref="footnote1.m1.1.1.cmml"><mi id="footnote1.m1.1.1b" xref="footnote1.m1.1.1.cmml"></mi><mo id="footnote1.m1.1.1.1" xref="footnote1.m1.1.1.1.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="footnote1.m1.1c"><apply id="footnote1.m1.1.1.cmml" xref="footnote1.m1.1.1"><ci id="footnote1.m1.1.1.1.cmml" xref="footnote1.m1.1.1.1">âˆ—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m1.1d">{}^{\ast}</annotation><annotation encoding="application/x-llamapun" id="footnote1.m1.1e">start_FLOATSUPERSCRIPT âˆ— end_FLOATSUPERSCRIPT</annotation></semantics></math>Corresponding author.</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In the workflow of translation, human translators generally utilize different types of external knowledge to simplify the process and improve translation quality and speed, such as matching terminologies and similar example sentences. The knowledge used in machine translation mainly includes high-quality bilingual sentences, a bilingual terminology dictionary and translation templates. Intuitively, it is reasonable to believe that it is beneficial for improving translation quality to integrate multiple types of knowledge into NMT models in a flexible and efficient way. However, most existing methods focus on only how to integrate a single type of knowledge into NMT models, either a terminology dictionaryÂ <cite class="ltx_cite ltx_citemacro_cite">Dinu etÂ al. (<a class="ltx_ref" href="#bib.bib10" title="">2019</a>); Dougal and Lonsdale (<a class="ltx_ref" href="#bib.bib11" title="">2020</a>)</cite>, bilingual sentencesÂ <cite class="ltx_cite ltx_citemacro_cite">Cao and Xiong (<a class="ltx_ref" href="#bib.bib7" title="">2018</a>); Liu etÂ al. (<a class="ltx_ref" href="#bib.bib26" title="">2019a</a>)</cite> or translation templatesÂ <cite class="ltx_cite ltx_citemacro_cite">Yang etÂ al. (<a class="ltx_ref" href="#bib.bib48" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">As a primary technique to utilize a terminology dictionary, lexically constrained translation allows for explicit phrase-based constraints to be placed on target output stringsÂ <cite class="ltx_cite ltx_citemacro_cite">Hu etÂ al. (<a class="ltx_ref" href="#bib.bib17" title="">2019</a>)</cite>. Several research worksÂ <cite class="ltx_cite ltx_citemacro_cite">Hokamp and Liu (<a class="ltx_ref" href="#bib.bib16" title="">2017</a>); Post and Vilar (<a class="ltx_ref" href="#bib.bib34" title="">2018</a>)</cite> impose lexical constraints by modifying the beam search decoding algorithm. Another line of approach trains the model to copy the target constraints by data augmentationÂ <cite class="ltx_cite ltx_citemacro_cite">Song etÂ al. (<a class="ltx_ref" href="#bib.bib37" title="">2019</a>); Dinu etÂ al. (<a class="ltx_ref" href="#bib.bib10" title="">2019</a>); Chen etÂ al. (<a class="ltx_ref" href="#bib.bib9" title="">2020</a>)</cite>. Some researchersÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a class="ltx_ref" href="#bib.bib24" title="">2019</a>); Wang etÂ al. (<a class="ltx_ref" href="#bib.bib44" title="">2022b</a>)</cite> introduce attention modules in the architecture of NMT models to integrate constraints. These methods using terminologies or phrases as the knowledge suffer from either high computational overheads or low terminology translation success rates.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In the majority of methods that utilize sentence pairs, the most similar source-target sentence pairs are retrieved from a translation memory (TM) for the input source sentenceÂ <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a class="ltx_ref" href="#bib.bib26" title="">2019a</a>); Huang etÂ al. (<a class="ltx_ref" href="#bib.bib18" title="">2021</a>); He etÂ al. (<a class="ltx_ref" href="#bib.bib14" title="">2021</a>)</cite>. Several approaches focus on integrating a TM into statistical machine translation (SMT)Â <cite class="ltx_cite ltx_citemacro_cite">Ma etÂ al. (<a class="ltx_ref" href="#bib.bib29" title="">2011</a>); Wang etÂ al. (<a class="ltx_ref" href="#bib.bib42" title="">2013</a>); Liu etÂ al. (<a class="ltx_ref" href="#bib.bib27" title="">2019b</a>)</cite>. Some researchers use a TM to augment an NMT model, including using n-grams from a TM to reward translationÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a class="ltx_ref" href="#bib.bib51" title="">2018b</a>)</cite>, employing an auxiliary network to integrate similar sentences into the NMTÂ <cite class="ltx_cite ltx_citemacro_cite">Gu etÂ al. (<a class="ltx_ref" href="#bib.bib13" title="">2018</a>); Xia etÂ al. (<a class="ltx_ref" href="#bib.bib45" title="">2019</a>)</cite> and data augmentation based on TMÂ <cite class="ltx_cite ltx_citemacro_cite">Bulte and Tezcan (<a class="ltx_ref" href="#bib.bib4" title="">2019a</a>); Xu etÂ al. (<a class="ltx_ref" href="#bib.bib46" title="">2020</a>)</cite>. These methods consume considerable computational overheads in training or testing.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Although these approaches have demonstrated the benefits of combining an NMT model with a single type of knowledge, how to integrate multiple types of knowledge into NMT models remains a challenge. In this work, we propose a prompt-based neural machine translation that can integrate multiple types of knowledge including both sentences, terminologies/phrases and translation templates into NMT models in a unified framework. Inspired byÂ <cite class="ltx_cite ltx_citemacro_cite">Brown etÂ al. (<a class="ltx_ref" href="#bib.bib3" title="">2020</a>)</cite>, which has redefined different NLP tasks as fill in the blanks problems by different prompts, we concatenate the source and target side of the knowledge as prefix-prompts of input for the encoder and decoder of NMT models, respectively. During training, this model learns dynamically to incorporate helpful information from the prefixes into generating translations. At inference time, new knowledge from multiple sources can be applied in real time. The model has automatic domain adaption capability and can be extended to new domains without updating parameters. We evaluate the approach in two tasks domain adaptation and soft lexical (terminology) constraint. The metric of â€˜exact matchâ€™ for terminology match accuracy has significantly improved compared to strong baselines both in English to German and English to Chinese translation.
This approach has shown its robustness in domain adaptation and performs better than fine-tuning when there are domain mismatch or noise data.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The contributions of this work include:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose a simple and effective approach to integrate multi-knowledge into NMT models with prompting.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We demonstrate that an NMT model can benefit from multiple types of knowledge simultaneously, including sentence, terminology/phrases and translation template knowledge.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">NMT is increasingly improving translation quality. However, the interpolation of the reasoning process has been less clear due to the deep neural architectures with hundreds of millions of parameters. How to guide an NMT system with user-specified different types of knowledge is an important issue of NMT applications in real world.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The first and most studied knowledge is simple constraints such as lexical constraints or in-domain dictionaries.
<cite class="ltx_cite ltx_citemacro_cite">Hokamp and Liu (<a class="ltx_ref" href="#bib.bib16" title="">2017</a>)</cite> proposes grid beam search (GBS) by modifying the decoding algorithm to add lexical constraints.Â <cite class="ltx_cite ltx_citemacro_cite">Post and Vilar (<a class="ltx_ref" href="#bib.bib34" title="">2018</a>)</cite> introduces dynamic beam allocation to reduce the runtime complexity of GBS by dividing a fixed size of beam for candidates.Â <cite class="ltx_cite ltx_citemacro_cite">Hu etÂ al. (<a class="ltx_ref" href="#bib.bib17" title="">2019</a>)</cite> proposes vectorized dynamic beam allocation (VDBA) to improve the efficiency of the decoding algorithm further. The beam search decoding algorithm by adding lexical constraints is still significantly slower than the beam search algorithm. Some data augmentation works propose to replace the corresponding source phrases with the target constraintsÂ <cite class="ltx_cite ltx_citemacro_cite">Song etÂ al. (<a class="ltx_ref" href="#bib.bib37" title="">2019</a>)</cite>, to integrate constraints as inline annotations in the source sentenceÂ <cite class="ltx_cite ltx_citemacro_cite">Dinu etÂ al. (<a class="ltx_ref" href="#bib.bib10" title="">2019</a>)</cite>, to insert target constraints using an alignment modelÂ <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a class="ltx_ref" href="#bib.bib8" title="">2021</a>)</cite> and to append constraints after the source sentence with a separation symbolÂ <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a class="ltx_ref" href="#bib.bib9" title="">2020</a>); Jon etÂ al. (<a class="ltx_ref" href="#bib.bib19" title="">2021</a>)</cite>. These data augmentation methods can not guarantee the presence of the target constraints in the output.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Some works concentrate on adapting the architecture of NMT models to add lexical constraints.Â <cite class="ltx_cite ltx_citemacro_cite">Susanto etÂ al. (<a class="ltx_ref" href="#bib.bib38" title="">2020</a>)</cite> invokes lexical constraints using a non-autoregressive decoding approach.Â <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a class="ltx_ref" href="#bib.bib50" title="">2021</a>)</cite> introduces explicit phrase alignment into the translation process of NMT models by building a search space similar to phrase-based SMT.Â <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a class="ltx_ref" href="#bib.bib24" title="">2019</a>)</cite> proposes to use external continuous memory to store constraints and integrate the constraint memories into NMT models through the decoder network.Â <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a class="ltx_ref" href="#bib.bib43" title="">2022a</a>)</cite> proposes a template-based method for constrained translation while maintaining the inference speed.Â <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a class="ltx_ref" href="#bib.bib44" title="">2022b</a>)</cite> proposes to integrate vectorized lexical source and target constraints into attention modules of the NMT model to model constraint pairs. These methods may still suffer from low match accuracy of terminology when decoding without the VDBA algorithm.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="319" id="S2.F1.g1" src="extracted/5189224/images/model.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Knowledge integration framework with prompting and an example representation. </figcaption>
</figure>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">The use of TM is very necessary for computer-aided translationÂ <cite class="ltx_cite ltx_citemacro_cite">Yamada (<a class="ltx_ref" href="#bib.bib47" title="">2011</a>)</cite> and computational approaches for machine translationÂ <cite class="ltx_cite ltx_citemacro_cite">Koehn and Senellart (<a class="ltx_ref" href="#bib.bib22" title="">2010</a>)</cite>.
Similar sentence pairs retrieved from a TM are also utilized as a type of knowledge to enhance the translationÂ <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a class="ltx_ref" href="#bib.bib26" title="">2019a</a>); He etÂ al. (<a class="ltx_ref" href="#bib.bib14" title="">2021</a>); Khandelwal etÂ al. (<a class="ltx_ref" href="#bib.bib20" title="">2021</a>)</cite>. Â <cite class="ltx_cite ltx_citemacro_cite">Farajian etÂ al. (<a class="ltx_ref" href="#bib.bib12" title="">2017</a>)</cite> exploits the retrieved sentence pairs from a TM to update the generic NMT models on-the-fly.Â <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a class="ltx_ref" href="#bib.bib51" title="">2018b</a>)</cite> utilizes translation pieces based on n-grams extracted from a TM during beam search by adding rewards for matched translation pieces into the NMT model output layer.Â <cite class="ltx_cite ltx_citemacro_cite">He etÂ al. (<a class="ltx_ref" href="#bib.bib15" title="">2019</a>)</cite> proposes to add the word position information from a TM as additional rewards to guide the decoding of NMT models.
<cite class="ltx_cite ltx_citemacro_cite">Gu etÂ al. (<a class="ltx_ref" href="#bib.bib13" title="">2018</a>)</cite> uses an auxiliary network to fuse information from the source sentence and the retrieved value from a TM and then integrate it into the NMT model architecture.Â <cite class="ltx_cite ltx_citemacro_cite">Xia etÂ al. (<a class="ltx_ref" href="#bib.bib45" title="">2019</a>)</cite> proposes to pack a TM into a compact graph corresponding to multiple words for different sentences in a TM, and then encode the packed graph into a deep representation during the decoding phase.Â <cite class="ltx_cite ltx_citemacro_cite">Xu etÂ al. (<a class="ltx_ref" href="#bib.bib46" title="">2020</a>)</cite> utilizes data augmentation to train an NMT model whose training instances are bilingual sentences augmented with the translation retrieved from the TM. For input sentences that are not very similar to their TMs, the translation performance of these methods suffers significantly.Â <cite class="ltx_cite ltx_citemacro_cite">He etÂ al. (<a class="ltx_ref" href="#bib.bib14" title="">2021</a>)</cite> introduces Example Layer consisting of multi-head attention and cross attention to translate any input sentences whether they are similar to their TM or no.Â <cite class="ltx_cite ltx_citemacro_cite">Cai etÂ al. (<a class="ltx_ref" href="#bib.bib6" title="">2021</a>)</cite> extends the TM from the bilingual setting to the monolingual setting through learnable memory retrieval in a cross-lingual manner. The key idea of TM is to integrate the retrieved sentence pairs from a TM into the NMT architecture for accurate translations. Most of the works integrate the TM knowledge via model modification and the models need to be retrained when loading another TM in new domains.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">The general knowledge integration to NMT is an ongoing work <cite class="ltx_cite ltx_citemacro_cite">Tang etÂ al. (<a class="ltx_ref" href="#bib.bib39" title="">2016</a>); Liu etÂ al. (<a class="ltx_ref" href="#bib.bib25" title="">2016</a>); Zhang etÂ al. (<a class="ltx_ref" href="#bib.bib49" title="">2018a</a>)</cite>.Â <cite class="ltx_cite ltx_citemacro_cite">Yang etÂ al. (<a class="ltx_ref" href="#bib.bib48" title="">2020</a>)</cite> proposes to use extracted templates from tree structures as soft target templates to incorporate the template information into the encoder-decoder framework.Â <cite class="ltx_cite ltx_citemacro_cite">Shang etÂ al. (<a class="ltx_ref" href="#bib.bib36" title="">2021</a>)</cite> introduce to a template-based machine translation (TBMT) model to integrate the syntactic knowledge of the retrieved target template in the NMT decoder.Â <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a class="ltx_ref" href="#bib.bib49" title="">2018a</a>)</cite> represents prior knowledge sources as features in a log-linear model to guide the learning process of NMT models. These approaches have demonstrated the clear benefits by incorporating different single types of knowledge into NMT models. Our approach is designed to integrate multiple types of knowledge into NMT models through an unified framework.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">As shown in FigureÂ <a class="ltx_ref" href="#S2.F1" title="Figure 1 â€£ 2 Related Work â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">1</span></a>, we use source-side knowledge sequence and target-side knowledge sequence to prepend a source sentence and a target sentence as a prefix separately. The model uses multiple types of knowledge as prefix-prompts of the input to guide the process of translating target sentence.
The form is intended to lead the NMT model how to utilize relevant information from the redundant prefixes to guide the translation process for improving translation quality.
We use three types of special tokens to separate different types of knowledge sequence, source sentence and target sentence.</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">[Sentence]/[Term]/[Template]: It indicates similar sentences, matching terminologies and translation templates respectively. The first token of each knowledge sequence is always the special token.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">[Input]: It is used to separate the knowledge sequence and the source sentence.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">[Output]: It is used to separate the knowledge sequence and the target sentence.
</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.8">For each sentence pair <math alttext="\left\langle\mathbf{x},\mathbf{y}\right\rangle" class="ltx_Math" display="inline" id="S3.p2.1.m1.2"><semantics id="S3.p2.1.m1.2a"><mrow id="S3.p2.1.m1.2.3.2" xref="S3.p2.1.m1.2.3.1.cmml"><mo id="S3.p2.1.m1.2.3.2.1" xref="S3.p2.1.m1.2.3.1.cmml">âŸ¨</mo><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">ğ±</mi><mo id="S3.p2.1.m1.2.3.2.2" xref="S3.p2.1.m1.2.3.1.cmml">,</mo><mi id="S3.p2.1.m1.2.2" xref="S3.p2.1.m1.2.2.cmml">ğ²</mi><mo id="S3.p2.1.m1.2.3.2.3" xref="S3.p2.1.m1.2.3.1.cmml">âŸ©</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.2b"><list id="S3.p2.1.m1.2.3.1.cmml" xref="S3.p2.1.m1.2.3.2"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">ğ±</ci><ci id="S3.p2.1.m1.2.2.cmml" xref="S3.p2.1.m1.2.2">ğ²</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.2c">\left\langle\mathbf{x},\mathbf{y}\right\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.2d">âŸ¨ bold_x , bold_y âŸ©</annotation></semantics></math>, corresponding similar sentence pairs, matching terminologies and translation templates are concatenated into source knowledge sequence <math alttext="\mathbf{x_{k}}" class="ltx_Math" display="inline" id="S3.p2.2.m2.1"><semantics id="S3.p2.2.m2.1a"><msub id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><mi id="S3.p2.2.m2.1.1.2" xref="S3.p2.2.m2.1.1.2.cmml">ğ±</mi><mi id="S3.p2.2.m2.1.1.3" xref="S3.p2.2.m2.1.1.3.cmml">ğ¤</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.p2.2.m2.1.1.2.cmml" xref="S3.p2.2.m2.1.1.2">ğ±</ci><ci id="S3.p2.2.m2.1.1.3.cmml" xref="S3.p2.2.m2.1.1.3">ğ¤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">\mathbf{x_{k}}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.2.m2.1d">bold_x start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT</annotation></semantics></math> and target knowledge sequence <math alttext="\mathbf{y_{k}}" class="ltx_Math" display="inline" id="S3.p2.3.m3.1"><semantics id="S3.p2.3.m3.1a"><msub id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml"><mi id="S3.p2.3.m3.1.1.2" xref="S3.p2.3.m3.1.1.2.cmml">ğ²</mi><mi id="S3.p2.3.m3.1.1.3" xref="S3.p2.3.m3.1.1.3.cmml">ğ¤</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><apply id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.1.cmml" xref="S3.p2.3.m3.1.1">subscript</csymbol><ci id="S3.p2.3.m3.1.1.2.cmml" xref="S3.p2.3.m3.1.1.2">ğ²</ci><ci id="S3.p2.3.m3.1.1.3.cmml" xref="S3.p2.3.m3.1.1.3">ğ¤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">\mathbf{y_{k}}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.3.m3.1d">bold_y start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT</annotation></semantics></math> with the corresponding special token [Sentence], [Term] and [Template] on the source and target side, respectively.
Then the sentence pair <math alttext="\left\langle\mathbf{x},\mathbf{y}\right\rangle" class="ltx_Math" display="inline" id="S3.p2.4.m4.2"><semantics id="S3.p2.4.m4.2a"><mrow id="S3.p2.4.m4.2.3.2" xref="S3.p2.4.m4.2.3.1.cmml"><mo id="S3.p2.4.m4.2.3.2.1" xref="S3.p2.4.m4.2.3.1.cmml">âŸ¨</mo><mi id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml">ğ±</mi><mo id="S3.p2.4.m4.2.3.2.2" xref="S3.p2.4.m4.2.3.1.cmml">,</mo><mi id="S3.p2.4.m4.2.2" xref="S3.p2.4.m4.2.2.cmml">ğ²</mi><mo id="S3.p2.4.m4.2.3.2.3" xref="S3.p2.4.m4.2.3.1.cmml">âŸ©</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.2b"><list id="S3.p2.4.m4.2.3.1.cmml" xref="S3.p2.4.m4.2.3.2"><ci id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1">ğ±</ci><ci id="S3.p2.4.m4.2.2.cmml" xref="S3.p2.4.m4.2.2">ğ²</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.2c">\left\langle\mathbf{x},\mathbf{y}\right\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.p2.4.m4.2d">âŸ¨ bold_x , bold_y âŸ©</annotation></semantics></math> are preprocessed as follows: the source sentence <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.p2.5.m5.1"><semantics id="S3.p2.5.m5.1a"><mi id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><ci id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.5.m5.1d">bold_x</annotation></semantics></math> and target sentence <math alttext="\mathbf{y}" class="ltx_Math" display="inline" id="S3.p2.6.m6.1"><semantics id="S3.p2.6.m6.1a"><mi id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml">ğ²</mi><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><ci id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1">ğ²</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">\mathbf{y}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.6.m6.1d">bold_y</annotation></semantics></math> are concatenated with source knowledge sequence <math alttext="\mathbf{x_{k}}" class="ltx_Math" display="inline" id="S3.p2.7.m7.1"><semantics id="S3.p2.7.m7.1a"><msub id="S3.p2.7.m7.1.1" xref="S3.p2.7.m7.1.1.cmml"><mi id="S3.p2.7.m7.1.1.2" xref="S3.p2.7.m7.1.1.2.cmml">ğ±</mi><mi id="S3.p2.7.m7.1.1.3" xref="S3.p2.7.m7.1.1.3.cmml">ğ¤</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.7.m7.1b"><apply id="S3.p2.7.m7.1.1.cmml" xref="S3.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.p2.7.m7.1.1.1.cmml" xref="S3.p2.7.m7.1.1">subscript</csymbol><ci id="S3.p2.7.m7.1.1.2.cmml" xref="S3.p2.7.m7.1.1.2">ğ±</ci><ci id="S3.p2.7.m7.1.1.3.cmml" xref="S3.p2.7.m7.1.1.3">ğ¤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m7.1c">\mathbf{x_{k}}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.7.m7.1d">bold_x start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT</annotation></semantics></math> and target knowledge sequence <math alttext="\mathbf{y_{k}}" class="ltx_Math" display="inline" id="S3.p2.8.m8.1"><semantics id="S3.p2.8.m8.1a"><msub id="S3.p2.8.m8.1.1" xref="S3.p2.8.m8.1.1.cmml"><mi id="S3.p2.8.m8.1.1.2" xref="S3.p2.8.m8.1.1.2.cmml">ğ²</mi><mi id="S3.p2.8.m8.1.1.3" xref="S3.p2.8.m8.1.1.3.cmml">ğ¤</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.8.m8.1b"><apply id="S3.p2.8.m8.1.1.cmml" xref="S3.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.p2.8.m8.1.1.1.cmml" xref="S3.p2.8.m8.1.1">subscript</csymbol><ci id="S3.p2.8.m8.1.1.2.cmml" xref="S3.p2.8.m8.1.1.2">ğ²</ci><ci id="S3.p2.8.m8.1.1.3.cmml" xref="S3.p2.8.m8.1.1.3">ğ¤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.8.m8.1c">\mathbf{y_{k}}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.8.m8.1d">bold_y start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT</annotation></semantics></math>, respectively.
An Example of the format of the input and output sequences is given in FigureÂ <a class="ltx_ref" href="#S2.F1" title="Figure 1 â€£ 2 Related Work â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">1</span></a>. When similar sentences and matching terminologies retrieved are empty, the input sequence and output sequence contain only translation template knowledge sequence. Although in this paper we integrate similar sentences, terminologies and translation templates into the NMT models, our approach can utilize more types of knowledge to improve translation performance by using this labeling strategy.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Training</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.3">Given a source sentence <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">bold_x</annotation></semantics></math>, the conditional probability of the corresponding target sentence <math alttext="\mathbf{y}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">ğ²</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ğ²</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mathbf{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">bold_y</annotation></semantics></math> by incorporating source and target knowledge sequence <math alttext="\left\langle\mathbf{x_{k}},\mathbf{y_{k}}\right\rangle" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.2"><semantics id="S3.SS1.p1.3.m3.2a"><mrow id="S3.SS1.p1.3.m3.2.2.2" xref="S3.SS1.p1.3.m3.2.2.3.cmml"><mo id="S3.SS1.p1.3.m3.2.2.2.3" xref="S3.SS1.p1.3.m3.2.2.3.cmml">âŸ¨</mo><msub id="S3.SS1.p1.3.m3.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.1.1.2" xref="S3.SS1.p1.3.m3.1.1.1.1.2.cmml">ğ±</mi><mi id="S3.SS1.p1.3.m3.1.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.1.3.cmml">ğ¤</mi></msub><mo id="S3.SS1.p1.3.m3.2.2.2.4" xref="S3.SS1.p1.3.m3.2.2.3.cmml">,</mo><msub id="S3.SS1.p1.3.m3.2.2.2.2" xref="S3.SS1.p1.3.m3.2.2.2.2.cmml"><mi id="S3.SS1.p1.3.m3.2.2.2.2.2" xref="S3.SS1.p1.3.m3.2.2.2.2.2.cmml">ğ²</mi><mi id="S3.SS1.p1.3.m3.2.2.2.2.3" xref="S3.SS1.p1.3.m3.2.2.2.2.3.cmml">ğ¤</mi></msub><mo id="S3.SS1.p1.3.m3.2.2.2.5" xref="S3.SS1.p1.3.m3.2.2.3.cmml">âŸ©</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.2b"><list id="S3.SS1.p1.3.m3.2.2.3.cmml" xref="S3.SS1.p1.3.m3.2.2.2"><apply id="S3.SS1.p1.3.m3.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.2">ğ±</ci><ci id="S3.SS1.p1.3.m3.1.1.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.3">ğ¤</ci></apply><apply id="S3.SS1.p1.3.m3.2.2.2.2.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.2.2.2.2.1.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.3.m3.2.2.2.2.2.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2.2">ğ²</ci><ci id="S3.SS1.p1.3.m3.2.2.2.2.3.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2.3">ğ¤</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.2c">\left\langle\mathbf{x_{k}},\mathbf{y_{k}}\right\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.2d">âŸ¨ bold_x start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT , bold_y start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT âŸ©</annotation></semantics></math> is defined as follows:
</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P(\mathbf{y}|\mathbf{x},\mathbf{x_{k}},\mathbf{y_{k}};\theta)=\prod_{i=1}^{n}P%
(\mathbf{y_{j}}|\mathbf{x},\mathbf{y_{&lt;j}},\mathbf{x_{k}},\mathbf{y_{k}};%
\theta)," class="ltx_Math" display="block" id="S3.E1.m1.5"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1.1" xref="S3.E1.m1.5.5.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.3.cmml">P</mi><mo id="S3.E1.m1.5.5.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.5.5.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.4" xref="S3.E1.m1.5.5.1.1.1.1.1.1.4.cmml">ğ²</mi><mo fence="false" id="S3.E1.m1.5.5.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.3.cmml">|</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">ğ±</mi><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml">ğ±</mi><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3.cmml">ğ¤</mi></msub><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.4" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.2.cmml">ğ²</mi><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.3.cmml">ğ¤</mi></msub><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.5" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.3.cmml">;</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">Î¸</mi></mrow></mrow><mo id="S3.E1.m1.5.5.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.5.5.1.1.3" rspace="0.111em" xref="S3.E1.m1.5.5.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.5.5.1.1.2" xref="S3.E1.m1.5.5.1.1.2.cmml"><munderover id="S3.E1.m1.5.5.1.1.2.2" xref="S3.E1.m1.5.5.1.1.2.2.cmml"><mo id="S3.E1.m1.5.5.1.1.2.2.2.2" movablelimits="false" xref="S3.E1.m1.5.5.1.1.2.2.2.2.cmml">âˆ</mo><mrow id="S3.E1.m1.5.5.1.1.2.2.2.3" xref="S3.E1.m1.5.5.1.1.2.2.2.3.cmml"><mi id="S3.E1.m1.5.5.1.1.2.2.2.3.2" xref="S3.E1.m1.5.5.1.1.2.2.2.3.2.cmml">i</mi><mo id="S3.E1.m1.5.5.1.1.2.2.2.3.1" xref="S3.E1.m1.5.5.1.1.2.2.2.3.1.cmml">=</mo><mn id="S3.E1.m1.5.5.1.1.2.2.2.3.3" xref="S3.E1.m1.5.5.1.1.2.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.5.5.1.1.2.2.3" xref="S3.E1.m1.5.5.1.1.2.2.3.cmml">n</mi></munderover><mrow id="S3.E1.m1.5.5.1.1.2.1" xref="S3.E1.m1.5.5.1.1.2.1.cmml"><mi id="S3.E1.m1.5.5.1.1.2.1.3" xref="S3.E1.m1.5.5.1.1.2.1.3.cmml">P</mi><mo id="S3.E1.m1.5.5.1.1.2.1.2" xref="S3.E1.m1.5.5.1.1.2.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.5.5.1.1.2.1.1.1" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.cmml"><mo id="S3.E1.m1.5.5.1.1.2.1.1.1.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.5.5.1.1.2.1.1.1.1" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.cmml"><msub id="S3.E1.m1.5.5.1.1.2.1.1.1.1.5" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.cmml"><mi id="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.2" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.2.cmml">ğ²</mi><mi id="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.3" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.3.cmml">ğ£</mi></msub><mo fence="false" id="S3.E1.m1.5.5.1.1.2.1.1.1.1.4" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.4.cmml">|</mo><mrow id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.4.cmml"><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">ğ±</mi><mo id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.4" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.2.cmml">ğ²</mi><mrow id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.3.cmml">ğ£</mi></mrow></msub><mo id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.5" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.cmml"><mi id="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.2" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.2.cmml">ğ±</mi><mi id="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.3" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.3.cmml">ğ¤</mi></msub><mo id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.6" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.cmml"><mi id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.2" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.2.cmml">ğ²</mi><mi id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.3" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.3.cmml">ğ¤</mi></msub><mo id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.7" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.4.cmml">;</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">Î¸</mi></mrow></mrow><mo id="S3.E1.m1.5.5.1.1.2.1.1.1.3" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.5.5.1.2" xref="S3.E1.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1"><eq id="S3.E1.m1.5.5.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.3"></eq><apply id="S3.E1.m1.5.5.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1"><times id="S3.E1.m1.5.5.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.2"></times><ci id="S3.E1.m1.5.5.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.3">ğ‘ƒ</ci><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.5.5.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.3">conditional</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.4">ğ²</ci><list id="S3.E1.m1.5.5.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ğ±</ci><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2">ğ±</ci><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3">ğ¤</ci></apply><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.2">ğ²</ci><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.2.2.2.3">ğ¤</ci></apply><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">ğœƒ</ci></list></apply></apply><apply id="S3.E1.m1.5.5.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.2"><apply id="S3.E1.m1.5.5.1.1.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.2.2.1.cmml" xref="S3.E1.m1.5.5.1.1.2.2">superscript</csymbol><apply id="S3.E1.m1.5.5.1.1.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.2.2.2.1.cmml" xref="S3.E1.m1.5.5.1.1.2.2">subscript</csymbol><csymbol cd="latexml" id="S3.E1.m1.5.5.1.1.2.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2.2">product</csymbol><apply id="S3.E1.m1.5.5.1.1.2.2.2.3.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2.3"><eq id="S3.E1.m1.5.5.1.1.2.2.2.3.1.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2.3.1"></eq><ci id="S3.E1.m1.5.5.1.1.2.2.2.3.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2.3.2">ğ‘–</ci><cn id="S3.E1.m1.5.5.1.1.2.2.2.3.3.cmml" type="integer" xref="S3.E1.m1.5.5.1.1.2.2.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.5.5.1.1.2.2.3.cmml" xref="S3.E1.m1.5.5.1.1.2.2.3">ğ‘›</ci></apply><apply id="S3.E1.m1.5.5.1.1.2.1.cmml" xref="S3.E1.m1.5.5.1.1.2.1"><times id="S3.E1.m1.5.5.1.1.2.1.2.cmml" xref="S3.E1.m1.5.5.1.1.2.1.2"></times><ci id="S3.E1.m1.5.5.1.1.2.1.3.cmml" xref="S3.E1.m1.5.5.1.1.2.1.3">ğ‘ƒ</ci><apply id="S3.E1.m1.5.5.1.1.2.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.5.5.1.1.2.1.1.1.1.4.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.4">conditional</csymbol><apply id="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.1.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.5">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.2.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.2">ğ²</ci><ci id="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.3.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.5.3">ğ£</ci></apply><list id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.4.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3"><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">ğ±</ci><apply id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.2">ğ²</ci><apply id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3"><lt id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.1.1.1.3.3">ğ£</ci></apply></apply><apply id="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.2">ğ±</ci><ci id="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.2.2.2.3">ğ¤</ci></apply><apply id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.2">ğ²</ci><ci id="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.3.cmml" xref="S3.E1.m1.5.5.1.1.2.1.1.1.1.3.3.3.3">ğ¤</ci></apply><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">ğœƒ</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">P(\mathbf{y}|\mathbf{x},\mathbf{x_{k}},\mathbf{y_{k}};\theta)=\prod_{i=1}^{n}P%
(\mathbf{y_{j}}|\mathbf{x},\mathbf{y_{&lt;j}},\mathbf{x_{k}},\mathbf{y_{k}};%
\theta),</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.5d">italic_P ( bold_y | bold_x , bold_x start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT , bold_y start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT ; italic_Î¸ ) = âˆ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_P ( bold_y start_POSTSUBSCRIPT bold_j end_POSTSUBSCRIPT | bold_x , bold_y start_POSTSUBSCRIPT &lt; bold_j end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT , bold_y start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT ; italic_Î¸ ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.8">where <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m1.1"><semantics id="S3.SS1.p1.4.m1.1a"><mi id="S3.SS1.p1.4.m1.1.1" xref="S3.SS1.p1.4.m1.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m1.1b"><ci id="S3.SS1.p1.4.m1.1.1.cmml" xref="S3.SS1.p1.4.m1.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m1.1d">italic_Î¸</annotation></semantics></math> is a set of model parameters, <math alttext="\mathbf{y_{&lt;j}}=y_{1},...,y_{j-1}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m2.3"><semantics id="S3.SS1.p1.5.m2.3a"><mrow id="S3.SS1.p1.5.m2.3.3" xref="S3.SS1.p1.5.m2.3.3.cmml"><msub id="S3.SS1.p1.5.m2.3.3.4" xref="S3.SS1.p1.5.m2.3.3.4.cmml"><mi id="S3.SS1.p1.5.m2.3.3.4.2" xref="S3.SS1.p1.5.m2.3.3.4.2.cmml">ğ²</mi><mrow id="S3.SS1.p1.5.m2.3.3.4.3" xref="S3.SS1.p1.5.m2.3.3.4.3.cmml"><mi id="S3.SS1.p1.5.m2.3.3.4.3.2" xref="S3.SS1.p1.5.m2.3.3.4.3.2.cmml"></mi><mo id="S3.SS1.p1.5.m2.3.3.4.3.1" xref="S3.SS1.p1.5.m2.3.3.4.3.1.cmml">&lt;</mo><mi id="S3.SS1.p1.5.m2.3.3.4.3.3" xref="S3.SS1.p1.5.m2.3.3.4.3.3.cmml">ğ£</mi></mrow></msub><mo id="S3.SS1.p1.5.m2.3.3.3" xref="S3.SS1.p1.5.m2.3.3.3.cmml">=</mo><mrow id="S3.SS1.p1.5.m2.3.3.2.2" xref="S3.SS1.p1.5.m2.3.3.2.3.cmml"><msub id="S3.SS1.p1.5.m2.2.2.1.1.1" xref="S3.SS1.p1.5.m2.2.2.1.1.1.cmml"><mi id="S3.SS1.p1.5.m2.2.2.1.1.1.2" xref="S3.SS1.p1.5.m2.2.2.1.1.1.2.cmml">y</mi><mn id="S3.SS1.p1.5.m2.2.2.1.1.1.3" xref="S3.SS1.p1.5.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.5.m2.3.3.2.2.3" xref="S3.SS1.p1.5.m2.3.3.2.3.cmml">,</mo><mi id="S3.SS1.p1.5.m2.1.1" mathvariant="normal" xref="S3.SS1.p1.5.m2.1.1.cmml">â€¦</mi><mo id="S3.SS1.p1.5.m2.3.3.2.2.4" xref="S3.SS1.p1.5.m2.3.3.2.3.cmml">,</mo><msub id="S3.SS1.p1.5.m2.3.3.2.2.2" xref="S3.SS1.p1.5.m2.3.3.2.2.2.cmml"><mi id="S3.SS1.p1.5.m2.3.3.2.2.2.2" xref="S3.SS1.p1.5.m2.3.3.2.2.2.2.cmml">y</mi><mrow id="S3.SS1.p1.5.m2.3.3.2.2.2.3" xref="S3.SS1.p1.5.m2.3.3.2.2.2.3.cmml"><mi id="S3.SS1.p1.5.m2.3.3.2.2.2.3.2" xref="S3.SS1.p1.5.m2.3.3.2.2.2.3.2.cmml">j</mi><mo id="S3.SS1.p1.5.m2.3.3.2.2.2.3.1" xref="S3.SS1.p1.5.m2.3.3.2.2.2.3.1.cmml">âˆ’</mo><mn id="S3.SS1.p1.5.m2.3.3.2.2.2.3.3" xref="S3.SS1.p1.5.m2.3.3.2.2.2.3.3.cmml">1</mn></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m2.3b"><apply id="S3.SS1.p1.5.m2.3.3.cmml" xref="S3.SS1.p1.5.m2.3.3"><eq id="S3.SS1.p1.5.m2.3.3.3.cmml" xref="S3.SS1.p1.5.m2.3.3.3"></eq><apply id="S3.SS1.p1.5.m2.3.3.4.cmml" xref="S3.SS1.p1.5.m2.3.3.4"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m2.3.3.4.1.cmml" xref="S3.SS1.p1.5.m2.3.3.4">subscript</csymbol><ci id="S3.SS1.p1.5.m2.3.3.4.2.cmml" xref="S3.SS1.p1.5.m2.3.3.4.2">ğ²</ci><apply id="S3.SS1.p1.5.m2.3.3.4.3.cmml" xref="S3.SS1.p1.5.m2.3.3.4.3"><lt id="S3.SS1.p1.5.m2.3.3.4.3.1.cmml" xref="S3.SS1.p1.5.m2.3.3.4.3.1"></lt><csymbol cd="latexml" id="S3.SS1.p1.5.m2.3.3.4.3.2.cmml" xref="S3.SS1.p1.5.m2.3.3.4.3.2">absent</csymbol><ci id="S3.SS1.p1.5.m2.3.3.4.3.3.cmml" xref="S3.SS1.p1.5.m2.3.3.4.3.3">ğ£</ci></apply></apply><list id="S3.SS1.p1.5.m2.3.3.2.3.cmml" xref="S3.SS1.p1.5.m2.3.3.2.2"><apply id="S3.SS1.p1.5.m2.2.2.1.1.1.cmml" xref="S3.SS1.p1.5.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m2.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.5.m2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m2.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.5.m2.2.2.1.1.1.2">ğ‘¦</ci><cn id="S3.SS1.p1.5.m2.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.5.m2.2.2.1.1.1.3">1</cn></apply><ci id="S3.SS1.p1.5.m2.1.1.cmml" xref="S3.SS1.p1.5.m2.1.1">â€¦</ci><apply id="S3.SS1.p1.5.m2.3.3.2.2.2.cmml" xref="S3.SS1.p1.5.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m2.3.3.2.2.2.1.cmml" xref="S3.SS1.p1.5.m2.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.5.m2.3.3.2.2.2.2.cmml" xref="S3.SS1.p1.5.m2.3.3.2.2.2.2">ğ‘¦</ci><apply id="S3.SS1.p1.5.m2.3.3.2.2.2.3.cmml" xref="S3.SS1.p1.5.m2.3.3.2.2.2.3"><minus id="S3.SS1.p1.5.m2.3.3.2.2.2.3.1.cmml" xref="S3.SS1.p1.5.m2.3.3.2.2.2.3.1"></minus><ci id="S3.SS1.p1.5.m2.3.3.2.2.2.3.2.cmml" xref="S3.SS1.p1.5.m2.3.3.2.2.2.3.2">ğ‘—</ci><cn id="S3.SS1.p1.5.m2.3.3.2.2.2.3.3.cmml" type="integer" xref="S3.SS1.p1.5.m2.3.3.2.2.2.3.3">1</cn></apply></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m2.3c">\mathbf{y_{&lt;j}}=y_{1},...,y_{j-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m2.3d">bold_y start_POSTSUBSCRIPT &lt; bold_j end_POSTSUBSCRIPT = italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_y start_POSTSUBSCRIPT italic_j - 1 end_POSTSUBSCRIPT</annotation></semantics></math> denotes a sequence of translation prefix tokens at time step <math alttext="j" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m3.1"><semantics id="S3.SS1.p1.6.m3.1a"><mi id="S3.SS1.p1.6.m3.1.1" xref="S3.SS1.p1.6.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m3.1b"><ci id="S3.SS1.p1.6.m3.1.1.cmml" xref="S3.SS1.p1.6.m3.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m3.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m3.1d">italic_j</annotation></semantics></math> and <math alttext="n" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m4.1"><semantics id="S3.SS1.p1.7.m4.1a"><mi id="S3.SS1.p1.7.m4.1.1" xref="S3.SS1.p1.7.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m4.1b"><ci id="S3.SS1.p1.7.m4.1.1.cmml" xref="S3.SS1.p1.7.m4.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m4.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m4.1d">italic_n</annotation></semantics></math> is length of the target sentence <math alttext="\mathbf{y}" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m5.1"><semantics id="S3.SS1.p1.8.m5.1a"><mi id="S3.SS1.p1.8.m5.1.1" xref="S3.SS1.p1.8.m5.1.1.cmml">ğ²</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m5.1b"><ci id="S3.SS1.p1.8.m5.1.1.cmml" xref="S3.SS1.p1.8.m5.1.1">ğ²</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m5.1c">\mathbf{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m5.1d">bold_y</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Similar to the vanilla NMT, we use the maximum likelihood estimation (MLE) loss function to find a set of model parameters on training set <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">caligraphic_D</annotation></semantics></math>. In order to focus the model on learning the target sentence, we utilize only tokens from the target sentence to calculate the loss function instead of the whole output sentence that contains knowledge sequence. Formally, we minimize the following loss function:</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}=\frac{1}{|\mathcal{D}|}\sum_{\mathbf{(x,y)}\in\mathcal{D}}-\log P(%
\mathbf{y|x,x_{k},y_{k}}{;}\theta).\\
" class="ltx_Math" display="block" id="S3.E2.m1.6"><semantics id="S3.E2.m1.6a"><mrow id="S3.E2.m1.6.6.1" xref="S3.E2.m1.6.6.1.1.cmml"><mrow id="S3.E2.m1.6.6.1.1" xref="S3.E2.m1.6.6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.6.6.1.1.3" xref="S3.E2.m1.6.6.1.1.3.cmml">â„’</mi><mo id="S3.E2.m1.6.6.1.1.2" xref="S3.E2.m1.6.6.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.6.6.1.1.1" xref="S3.E2.m1.6.6.1.1.1.cmml"><mrow id="S3.E2.m1.6.6.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.3.cmml"><mfrac id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mn id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml">1</mn><mrow id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.2.cmml"><mo id="S3.E2.m1.1.1.1.3.1" stretchy="false" xref="S3.E2.m1.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">ğ’Ÿ</mi><mo id="S3.E2.m1.1.1.1.3.2" stretchy="false" xref="S3.E2.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S3.E2.m1.6.6.1.1.1.3.1" xref="S3.E2.m1.6.6.1.1.1.3.1.cmml">â¢</mo><munder id="S3.E2.m1.6.6.1.1.1.3.2" xref="S3.E2.m1.6.6.1.1.1.3.2.cmml"><mo id="S3.E2.m1.6.6.1.1.1.3.2.2" movablelimits="false" rspace="0em" xref="S3.E2.m1.6.6.1.1.1.3.2.2.cmml">âˆ‘</mo><mrow id="S3.E2.m1.3.3.2" xref="S3.E2.m1.3.3.2.cmml"><mrow id="S3.E2.m1.3.3.2.4.2" xref="S3.E2.m1.3.3.2.4.1.cmml"><mo id="S3.E2.m1.3.3.2.4.2.1" stretchy="false" xref="S3.E2.m1.3.3.2.4.1.cmml">(</mo><mi id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml">ğ±</mi><mo id="S3.E2.m1.3.3.2.4.2.2" xref="S3.E2.m1.3.3.2.4.1.cmml">,</mo><mi id="S3.E2.m1.3.3.2.2" xref="S3.E2.m1.3.3.2.2.cmml">ğ²</mi><mo id="S3.E2.m1.3.3.2.4.2.3" stretchy="false" xref="S3.E2.m1.3.3.2.4.1.cmml">)</mo></mrow><mo id="S3.E2.m1.3.3.2.3" xref="S3.E2.m1.3.3.2.3.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.3.3.2.5" xref="S3.E2.m1.3.3.2.5.cmml">ğ’Ÿ</mi></mrow></munder></mrow><mo id="S3.E2.m1.6.6.1.1.1.2" lspace="0em" xref="S3.E2.m1.6.6.1.1.1.2.cmml">âˆ’</mo><mrow id="S3.E2.m1.6.6.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.cmml"><mrow id="S3.E2.m1.6.6.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.3.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.3.1" xref="S3.E2.m1.6.6.1.1.1.1.3.1.cmml">log</mi><mo id="S3.E2.m1.6.6.1.1.1.1.3a" lspace="0.167em" xref="S3.E2.m1.6.6.1.1.1.1.3.cmml">â¡</mo><mi id="S3.E2.m1.6.6.1.1.1.1.3.2" xref="S3.E2.m1.6.6.1.1.1.1.3.2.cmml">P</mi></mrow><mo id="S3.E2.m1.6.6.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.4" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.4.cmml">ğ²</mi><mo fence="false" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.3.cmml">|</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">ğ±</mi><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.cmml">ğ±</mi><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.cmml">ğ¤</mi></msub><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.4" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.2.cmml">ğ²</mi><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.3.cmml">ğ¤</mi></msub><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.5" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.3.cmml">;</mo><mi id="S3.E2.m1.5.5" xref="S3.E2.m1.5.5.cmml">Î¸</mi></mrow></mrow><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.6.6.1.2" lspace="0em" xref="S3.E2.m1.6.6.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.6b"><apply id="S3.E2.m1.6.6.1.1.cmml" xref="S3.E2.m1.6.6.1"><eq id="S3.E2.m1.6.6.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.2"></eq><ci id="S3.E2.m1.6.6.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.3">â„’</ci><apply id="S3.E2.m1.6.6.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1"><minus id="S3.E2.m1.6.6.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.2"></minus><apply id="S3.E2.m1.6.6.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.3"><times id="S3.E2.m1.6.6.1.1.1.3.1.cmml" xref="S3.E2.m1.6.6.1.1.1.3.1"></times><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><divide id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1"></divide><cn id="S3.E2.m1.1.1.3.cmml" type="integer" xref="S3.E2.m1.1.1.3">1</cn><apply id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.3"><abs id="S3.E2.m1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.3.1"></abs><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">ğ’Ÿ</ci></apply></apply><apply id="S3.E2.m1.6.6.1.1.1.3.2.cmml" xref="S3.E2.m1.6.6.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.3.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.3.2">subscript</csymbol><sum id="S3.E2.m1.6.6.1.1.1.3.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.3.2.2"></sum><apply id="S3.E2.m1.3.3.2.cmml" xref="S3.E2.m1.3.3.2"><in id="S3.E2.m1.3.3.2.3.cmml" xref="S3.E2.m1.3.3.2.3"></in><interval closure="open" id="S3.E2.m1.3.3.2.4.1.cmml" xref="S3.E2.m1.3.3.2.4.2"><ci id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1.1">ğ±</ci><ci id="S3.E2.m1.3.3.2.2.cmml" xref="S3.E2.m1.3.3.2.2">ğ²</ci></interval><ci id="S3.E2.m1.3.3.2.5.cmml" xref="S3.E2.m1.3.3.2.5">ğ’Ÿ</ci></apply></apply></apply><apply id="S3.E2.m1.6.6.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1"><times id="S3.E2.m1.6.6.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.2"></times><apply id="S3.E2.m1.6.6.1.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.3"><log id="S3.E2.m1.6.6.1.1.1.1.3.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.3.1"></log><ci id="S3.E2.m1.6.6.1.1.1.1.3.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.3.2">ğ‘ƒ</ci></apply><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.3">conditional</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.4">ğ²</ci><list id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2"><ci id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">ğ±</ci><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2">ğ±</ci><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3">ğ¤</ci></apply><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.2">ğ²</ci><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.2.2.3">ğ¤</ci></apply><ci id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5">ğœƒ</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.6c">\mathcal{L}=\frac{1}{|\mathcal{D}|}\sum_{\mathbf{(x,y)}\in\mathcal{D}}-\log P(%
\mathbf{y|x,x_{k},y_{k}}{;}\theta).\\
</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.6d">caligraphic_L = divide start_ARG 1 end_ARG start_ARG | caligraphic_D | end_ARG âˆ‘ start_POSTSUBSCRIPT ( bold_x , bold_y ) âˆˆ caligraphic_D end_POSTSUBSCRIPT - roman_log italic_P ( bold_y | bold_x , bold_x start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT , bold_y start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT ; italic_Î¸ ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p3.1">Note that the proposed method is different from the priming methodÂ <cite class="ltx_cite ltx_citemacro_cite">Bulte and Tezcan (<a class="ltx_ref" href="#bib.bib5" title="">2019b</a>); Pham etÂ al. (<a class="ltx_ref" href="#bib.bib32" title="">2020</a>)</cite>. The priming techniques retrieve similar source sentences and corresponding translations, and then the similar sentence pairs can be used as an input prompt for NMT models. First, we train the model based on our proposed loss function EquationÂ <a class="ltx_ref" href="#S3.E2" title="2 â€£ 3.1 Training â€£ 3 Approach â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">2</span></a> and the NMT model is trained with standard loss function in their works. Also, our method can be applied to multi-knowledge and their work can only be is limited to sentences.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">For model optimization, we adopt a two-stage training strategy. In the first stage, we train the standard NMT model based on the standard training objective using the original training data set. Then, in the second stage, we use a training data set constructed from multiple types of knowledge to learn the model parameters based on EquationÂ <a class="ltx_ref" href="#S3.E2" title="2 â€£ 3.1 Training â€£ 3 Approach â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">2</span></a>. The proposed model can also be initialized with pre-trained NMT models and then trained on the training data set that contains multi-knowledge.
</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Inference</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The model receives the whole input sequence and the target prefix composed of target knowledge sequence during decoding. Before beginning translation sequence generation, the encoder encodes the input sequence, and the decoder encodes the target prefix. The initial steps of the beam search use the given prefix <math alttext="\mathbf{y_{k}}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">ğ²</mi><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">ğ¤</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">ğ²</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">ğ¤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\mathbf{y_{k}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">bold_y start_POSTSUBSCRIPT bold_k end_POSTSUBSCRIPT</annotation></semantics></math> to decode the tokens after the special separator token [Output] in forced decoding mode. The decoder can gain indirect access to whole input sequence tokens while also gaining direct access to target prefix tokens by self-attention and cross-attention mechanisms. It enables the NMT model learn to how to extract and make use of valuable information from the redundant prefixes during training, and use the prefixes to guide the translation process during inference.
</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Knowledge Acquisition</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We describe the methods employed in this work to how to obtain knowledge from bilingual sentences (sentence knowledge), terminology dictionaries (terminology knowledge) and translation templates (template knowledge).</p>
</div>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Retrieving Similar Sentence</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.6">For each source sentence <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.1.m1.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.1.m1.1d">bold_x</annotation></semantics></math>, we retrieve the most similar bilingual sentences <math alttext="\left\langle\mathbf{x_{s}},\mathbf{y_{s}}\right\rangle" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.2.m2.2"><semantics id="S3.SS3.SSS0.Px1.p1.2.m2.2a"><mrow id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.3.cmml"><mo id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.3" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.3.cmml">âŸ¨</mo><msub id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.2" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.2.cmml">ğ±</mi><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.3" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.3.cmml">ğ¬</mi></msub><mo id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.4" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.3.cmml">,</mo><msub id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.cmml"><mi id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.2" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.2.cmml">ğ²</mi><mi id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.3" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.3.cmml">ğ¬</mi></msub><mo id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.5" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.3.cmml">âŸ©</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.2.m2.2b"><list id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.3.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2"><apply id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.2">ğ±</ci><ci id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.1.1.3">ğ¬</ci></apply><apply id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.1.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.2.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.2">ğ²</ci><ci id="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.3.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.2.2.2.2.3">ğ¬</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.2.m2.2c">\left\langle\mathbf{x_{s}},\mathbf{y_{s}}\right\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.2.m2.2d">âŸ¨ bold_x start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT , bold_y start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT âŸ©</annotation></semantics></math> from sentence knowledge. We use token-based edit distanceÂ <cite class="ltx_cite ltx_citemacro_cite">Levenshtein (<a class="ltx_ref" href="#bib.bib23" title="">1965</a>)</cite> to calculate the similarity score. Formally, for a given source sentence <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.3.m3.1"><semantics id="S3.SS3.SSS0.Px1.p1.3.m3.1a"><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS3.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.3.m3.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.3.m3.1d">bold_x</annotation></semantics></math>, similarity score <math alttext="\text{sim}(\mathbf{x},\mathbf{x_{s}})" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.4.m4.2"><semantics id="S3.SS3.SSS0.Px1.p1.4.m4.2a"><mrow id="S3.SS3.SSS0.Px1.p1.4.m4.2.2" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.cmml"><mtext id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.3" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.3a.cmml">sim</mtext><mo id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.2" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.2.cmml">â¢</mo><mrow id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.2.cmml"><mo id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.2" stretchy="false" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.2.cmml">(</mo><mi id="S3.SS3.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS3.SSS0.Px1.p1.4.m4.1.1.cmml">ğ±</mi><mo id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.3" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.2.cmml">,</mo><msub id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.2" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.2.cmml">ğ±</mi><mi id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.3" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.3.cmml">ğ¬</mi></msub><mo id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.4" stretchy="false" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.4.m4.2b"><apply id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2"><times id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.2.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.2"></times><ci id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.3a.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.3"><mtext id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.3.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.3">sim</mtext></ci><interval closure="open" id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1"><ci id="S3.SS3.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.1.1">ğ±</ci><apply id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.2">ğ±</ci><ci id="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.2.2.1.1.1.3">ğ¬</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.4.m4.2c">\text{sim}(\mathbf{x},\mathbf{x_{s}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.4.m4.2d">sim ( bold_x , bold_x start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT )</annotation></semantics></math> between two source sentences <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.5.m5.1"><semantics id="S3.SS3.SSS0.Px1.p1.5.m5.1a"><mi id="S3.SS3.SSS0.Px1.p1.5.m5.1.1" xref="S3.SS3.SSS0.Px1.p1.5.m5.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.5.m5.1b"><ci id="S3.SS3.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.5.m5.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.5.m5.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.5.m5.1d">bold_x</annotation></semantics></math> and <math alttext="\mathbf{x_{s}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.6.m6.1"><semantics id="S3.SS3.SSS0.Px1.p1.6.m6.1a"><msub id="S3.SS3.SSS0.Px1.p1.6.m6.1.1" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.2" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.2.cmml">ğ±</mi><mi id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.3" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.3.cmml">ğ¬</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.6.m6.1b"><apply id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.2">ğ±</ci><ci id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.3">ğ¬</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.6.m6.1c">\mathbf{x_{s}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.6.m6.1d">bold_x start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{sim}(\mathbf{x},\mathbf{x_{s}})=1-\frac{{ED}(\mathbf{x},\mathbf{x_{s}})}%
{{max}(|\mathbf{x}|,|\mathbf{x_{s}}|)}," class="ltx_Math" display="block" id="S3.E3.m1.7"><semantics id="S3.E3.m1.7a"><mrow id="S3.E3.m1.7.7.1" xref="S3.E3.m1.7.7.1.1.cmml"><mrow id="S3.E3.m1.7.7.1.1" xref="S3.E3.m1.7.7.1.1.cmml"><mrow id="S3.E3.m1.7.7.1.1.1" xref="S3.E3.m1.7.7.1.1.1.cmml"><mtext id="S3.E3.m1.7.7.1.1.1.3" xref="S3.E3.m1.7.7.1.1.1.3a.cmml">sim</mtext><mo id="S3.E3.m1.7.7.1.1.1.2" xref="S3.E3.m1.7.7.1.1.1.2.cmml">â¢</mo><mrow id="S3.E3.m1.7.7.1.1.1.1.1" xref="S3.E3.m1.7.7.1.1.1.1.2.cmml"><mo id="S3.E3.m1.7.7.1.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.7.7.1.1.1.1.2.cmml">(</mo><mi id="S3.E3.m1.6.6" xref="S3.E3.m1.6.6.cmml">ğ±</mi><mo id="S3.E3.m1.7.7.1.1.1.1.1.3" xref="S3.E3.m1.7.7.1.1.1.1.2.cmml">,</mo><msub id="S3.E3.m1.7.7.1.1.1.1.1.1" xref="S3.E3.m1.7.7.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.7.7.1.1.1.1.1.1.2" xref="S3.E3.m1.7.7.1.1.1.1.1.1.2.cmml">ğ±</mi><mi id="S3.E3.m1.7.7.1.1.1.1.1.1.3" xref="S3.E3.m1.7.7.1.1.1.1.1.1.3.cmml">ğ¬</mi></msub><mo id="S3.E3.m1.7.7.1.1.1.1.1.4" stretchy="false" xref="S3.E3.m1.7.7.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.7.7.1.1.2" xref="S3.E3.m1.7.7.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.7.7.1.1.3" xref="S3.E3.m1.7.7.1.1.3.cmml"><mn id="S3.E3.m1.7.7.1.1.3.2" xref="S3.E3.m1.7.7.1.1.3.2.cmml">1</mn><mo id="S3.E3.m1.7.7.1.1.3.1" xref="S3.E3.m1.7.7.1.1.3.1.cmml">âˆ’</mo><mfrac id="S3.E3.m1.5.5" xref="S3.E3.m1.5.5.cmml"><mrow id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml"><mi id="S3.E3.m1.2.2.2.4" xref="S3.E3.m1.2.2.2.4.cmml">E</mi><mo id="S3.E3.m1.2.2.2.3" xref="S3.E3.m1.2.2.2.3.cmml">â¢</mo><mi id="S3.E3.m1.2.2.2.5" xref="S3.E3.m1.2.2.2.5.cmml">D</mi><mo id="S3.E3.m1.2.2.2.3a" xref="S3.E3.m1.2.2.2.3.cmml">â¢</mo><mrow id="S3.E3.m1.2.2.2.2.1" xref="S3.E3.m1.2.2.2.2.2.cmml"><mo id="S3.E3.m1.2.2.2.2.1.2" stretchy="false" xref="S3.E3.m1.2.2.2.2.2.cmml">(</mo><mi id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">ğ±</mi><mo id="S3.E3.m1.2.2.2.2.1.3" xref="S3.E3.m1.2.2.2.2.2.cmml">,</mo><msub id="S3.E3.m1.2.2.2.2.1.1" xref="S3.E3.m1.2.2.2.2.1.1.cmml"><mi id="S3.E3.m1.2.2.2.2.1.1.2" xref="S3.E3.m1.2.2.2.2.1.1.2.cmml">ğ±</mi><mi id="S3.E3.m1.2.2.2.2.1.1.3" xref="S3.E3.m1.2.2.2.2.1.1.3.cmml">ğ¬</mi></msub><mo id="S3.E3.m1.2.2.2.2.1.4" stretchy="false" xref="S3.E3.m1.2.2.2.2.2.cmml">)</mo></mrow></mrow><mrow id="S3.E3.m1.5.5.5" xref="S3.E3.m1.5.5.5.cmml"><mi id="S3.E3.m1.5.5.5.5" xref="S3.E3.m1.5.5.5.5.cmml">m</mi><mo id="S3.E3.m1.5.5.5.4" xref="S3.E3.m1.5.5.5.4.cmml">â¢</mo><mi id="S3.E3.m1.5.5.5.6" xref="S3.E3.m1.5.5.5.6.cmml">a</mi><mo id="S3.E3.m1.5.5.5.4a" xref="S3.E3.m1.5.5.5.4.cmml">â¢</mo><mi id="S3.E3.m1.5.5.5.7" xref="S3.E3.m1.5.5.5.7.cmml">x</mi><mo id="S3.E3.m1.5.5.5.4b" xref="S3.E3.m1.5.5.5.4.cmml">â¢</mo><mrow id="S3.E3.m1.5.5.5.3.2" xref="S3.E3.m1.5.5.5.3.3.cmml"><mo id="S3.E3.m1.5.5.5.3.2.3" stretchy="false" xref="S3.E3.m1.5.5.5.3.3.cmml">(</mo><mrow id="S3.E3.m1.4.4.4.2.1.1.2" xref="S3.E3.m1.4.4.4.2.1.1.1.cmml"><mo id="S3.E3.m1.4.4.4.2.1.1.2.1" stretchy="false" xref="S3.E3.m1.4.4.4.2.1.1.1.1.cmml">|</mo><mi id="S3.E3.m1.3.3.3.1" xref="S3.E3.m1.3.3.3.1.cmml">ğ±</mi><mo id="S3.E3.m1.4.4.4.2.1.1.2.2" stretchy="false" xref="S3.E3.m1.4.4.4.2.1.1.1.1.cmml">|</mo></mrow><mo id="S3.E3.m1.5.5.5.3.2.4" xref="S3.E3.m1.5.5.5.3.3.cmml">,</mo><mrow id="S3.E3.m1.5.5.5.3.2.2.1" xref="S3.E3.m1.5.5.5.3.2.2.2.cmml"><mo id="S3.E3.m1.5.5.5.3.2.2.1.2" stretchy="false" xref="S3.E3.m1.5.5.5.3.2.2.2.1.cmml">|</mo><msub id="S3.E3.m1.5.5.5.3.2.2.1.1" xref="S3.E3.m1.5.5.5.3.2.2.1.1.cmml"><mi id="S3.E3.m1.5.5.5.3.2.2.1.1.2" xref="S3.E3.m1.5.5.5.3.2.2.1.1.2.cmml">ğ±</mi><mi id="S3.E3.m1.5.5.5.3.2.2.1.1.3" xref="S3.E3.m1.5.5.5.3.2.2.1.1.3.cmml">ğ¬</mi></msub><mo id="S3.E3.m1.5.5.5.3.2.2.1.3" stretchy="false" xref="S3.E3.m1.5.5.5.3.2.2.2.1.cmml">|</mo></mrow><mo id="S3.E3.m1.5.5.5.3.2.5" stretchy="false" xref="S3.E3.m1.5.5.5.3.3.cmml">)</mo></mrow></mrow></mfrac></mrow></mrow><mo id="S3.E3.m1.7.7.1.2" xref="S3.E3.m1.7.7.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.7b"><apply id="S3.E3.m1.7.7.1.1.cmml" xref="S3.E3.m1.7.7.1"><eq id="S3.E3.m1.7.7.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.2"></eq><apply id="S3.E3.m1.7.7.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1"><times id="S3.E3.m1.7.7.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.2"></times><ci id="S3.E3.m1.7.7.1.1.1.3a.cmml" xref="S3.E3.m1.7.7.1.1.1.3"><mtext id="S3.E3.m1.7.7.1.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.1.3">sim</mtext></ci><interval closure="open" id="S3.E3.m1.7.7.1.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1"><ci id="S3.E3.m1.6.6.cmml" xref="S3.E3.m1.6.6">ğ±</ci><apply id="S3.E3.m1.7.7.1.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.7.7.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.2">ğ±</ci><ci id="S3.E3.m1.7.7.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.3">ğ¬</ci></apply></interval></apply><apply id="S3.E3.m1.7.7.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.3"><minus id="S3.E3.m1.7.7.1.1.3.1.cmml" xref="S3.E3.m1.7.7.1.1.3.1"></minus><cn id="S3.E3.m1.7.7.1.1.3.2.cmml" type="integer" xref="S3.E3.m1.7.7.1.1.3.2">1</cn><apply id="S3.E3.m1.5.5.cmml" xref="S3.E3.m1.5.5"><divide id="S3.E3.m1.5.5.6.cmml" xref="S3.E3.m1.5.5"></divide><apply id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2"><times id="S3.E3.m1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.3"></times><ci id="S3.E3.m1.2.2.2.4.cmml" xref="S3.E3.m1.2.2.2.4">ğ¸</ci><ci id="S3.E3.m1.2.2.2.5.cmml" xref="S3.E3.m1.2.2.2.5">ğ·</ci><interval closure="open" id="S3.E3.m1.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.1"><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">ğ±</ci><apply id="S3.E3.m1.2.2.2.2.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S3.E3.m1.2.2.2.2.1.1.2.cmml" xref="S3.E3.m1.2.2.2.2.1.1.2">ğ±</ci><ci id="S3.E3.m1.2.2.2.2.1.1.3.cmml" xref="S3.E3.m1.2.2.2.2.1.1.3">ğ¬</ci></apply></interval></apply><apply id="S3.E3.m1.5.5.5.cmml" xref="S3.E3.m1.5.5.5"><times id="S3.E3.m1.5.5.5.4.cmml" xref="S3.E3.m1.5.5.5.4"></times><ci id="S3.E3.m1.5.5.5.5.cmml" xref="S3.E3.m1.5.5.5.5">ğ‘š</ci><ci id="S3.E3.m1.5.5.5.6.cmml" xref="S3.E3.m1.5.5.5.6">ğ‘</ci><ci id="S3.E3.m1.5.5.5.7.cmml" xref="S3.E3.m1.5.5.5.7">ğ‘¥</ci><interval closure="open" id="S3.E3.m1.5.5.5.3.3.cmml" xref="S3.E3.m1.5.5.5.3.2"><apply id="S3.E3.m1.4.4.4.2.1.1.1.cmml" xref="S3.E3.m1.4.4.4.2.1.1.2"><abs id="S3.E3.m1.4.4.4.2.1.1.1.1.cmml" xref="S3.E3.m1.4.4.4.2.1.1.2.1"></abs><ci id="S3.E3.m1.3.3.3.1.cmml" xref="S3.E3.m1.3.3.3.1">ğ±</ci></apply><apply id="S3.E3.m1.5.5.5.3.2.2.2.cmml" xref="S3.E3.m1.5.5.5.3.2.2.1"><abs id="S3.E3.m1.5.5.5.3.2.2.2.1.cmml" xref="S3.E3.m1.5.5.5.3.2.2.1.2"></abs><apply id="S3.E3.m1.5.5.5.3.2.2.1.1.cmml" xref="S3.E3.m1.5.5.5.3.2.2.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.5.3.2.2.1.1.1.cmml" xref="S3.E3.m1.5.5.5.3.2.2.1.1">subscript</csymbol><ci id="S3.E3.m1.5.5.5.3.2.2.1.1.2.cmml" xref="S3.E3.m1.5.5.5.3.2.2.1.1.2">ğ±</ci><ci id="S3.E3.m1.5.5.5.3.2.2.1.1.3.cmml" xref="S3.E3.m1.5.5.5.3.2.2.1.1.3">ğ¬</ci></apply></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.7c">\text{sim}(\mathbf{x},\mathbf{x_{s}})=1-\frac{{ED}(\mathbf{x},\mathbf{x_{s}})}%
{{max}(|\mathbf{x}|,|\mathbf{x_{s}}|)},</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.7d">sim ( bold_x , bold_x start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT ) = 1 - divide start_ARG italic_E italic_D ( bold_x , bold_x start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT ) end_ARG start_ARG italic_m italic_a italic_x ( | bold_x | , | bold_x start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT | ) end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p2.8">where <math alttext="ED(\mathbf{x},\mathbf{x_{s}})" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.1.m1.2"><semantics id="S3.SS3.SSS0.Px1.p2.1.m1.2a"><mrow id="S3.SS3.SSS0.Px1.p2.1.m1.2.2" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.cmml"><mi id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.3" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.3.cmml">E</mi><mo id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.2" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.2.cmml">â¢</mo><mi id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.4" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.4.cmml">D</mi><mo id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.2a" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.2.cmml">â¢</mo><mrow id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.2.cmml"><mo id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.2" stretchy="false" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.2.cmml">(</mo><mi id="S3.SS3.SSS0.Px1.p2.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.cmml">ğ±</mi><mo id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.3" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.2.cmml">,</mo><msub id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.2" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.2.cmml">ğ±</mi><mi id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.3" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.3.cmml">ğ¬</mi></msub><mo id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.4" stretchy="false" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.1.m1.2b"><apply id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2"><times id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.2.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.2"></times><ci id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.3.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.3">ğ¸</ci><ci id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.4.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.4">ğ·</ci><interval closure="open" id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1"><ci id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1">ğ±</ci><apply id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.2">ğ±</ci><ci id="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.2.2.1.1.1.3">ğ¬</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.1.m1.2c">ED(\mathbf{x},\mathbf{x_{s}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.1.m1.2d">italic_E italic_D ( bold_x , bold_x start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT )</annotation></semantics></math> denotes the Edit Distance between <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.2.m2.1"><semantics id="S3.SS3.SSS0.Px1.p2.2.m2.1a"><mi id="S3.SS3.SSS0.Px1.p2.2.m2.1.1" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.2.m2.1b"><ci id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.2.m2.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.2.m2.1d">bold_x</annotation></semantics></math> and <math alttext="\mathbf{x_{s}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.3.m3.1"><semantics id="S3.SS3.SSS0.Px1.p2.3.m3.1a"><msub id="S3.SS3.SSS0.Px1.p2.3.m3.1.1" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2.cmml">ğ±</mi><mi id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.cmml">ğ¬</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.3.m3.1b"><apply id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2">ğ±</ci><ci id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3">ğ¬</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.3.m3.1c">\mathbf{x_{s}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.3.m3.1d">bold_x start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="|\mathbf{x}|" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.4.m4.1"><semantics id="S3.SS3.SSS0.Px1.p2.4.m4.1a"><mrow id="S3.SS3.SSS0.Px1.p2.4.m4.1.2.2" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.2.1.cmml"><mo id="S3.SS3.SSS0.Px1.p2.4.m4.1.2.2.1" stretchy="false" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.2.1.1.cmml">|</mo><mi id="S3.SS3.SSS0.Px1.p2.4.m4.1.1" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.1.cmml">ğ±</mi><mo id="S3.SS3.SSS0.Px1.p2.4.m4.1.2.2.2" stretchy="false" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.4.m4.1b"><apply id="S3.SS3.SSS0.Px1.p2.4.m4.1.2.1.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.2.2"><abs id="S3.SS3.SSS0.Px1.p2.4.m4.1.2.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.2.2.1"></abs><ci id="S3.SS3.SSS0.Px1.p2.4.m4.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.1">ğ±</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.4.m4.1c">|\mathbf{x}|</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.4.m4.1d">| bold_x |</annotation></semantics></math> is the length of <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.5.m5.1"><semantics id="S3.SS3.SSS0.Px1.p2.5.m5.1a"><mi id="S3.SS3.SSS0.Px1.p2.5.m5.1.1" xref="S3.SS3.SSS0.Px1.p2.5.m5.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.5.m5.1b"><ci id="S3.SS3.SSS0.Px1.p2.5.m5.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.5.m5.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.5.m5.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.5.m5.1d">bold_x</annotation></semantics></math>. <math alttext="\mathbf{x_{s}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.6.m6.1"><semantics id="S3.SS3.SSS0.Px1.p2.6.m6.1a"><msub id="S3.SS3.SSS0.Px1.p2.6.m6.1.1" xref="S3.SS3.SSS0.Px1.p2.6.m6.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.6.m6.1.1.2" xref="S3.SS3.SSS0.Px1.p2.6.m6.1.1.2.cmml">ğ±</mi><mi id="S3.SS3.SSS0.Px1.p2.6.m6.1.1.3" xref="S3.SS3.SSS0.Px1.p2.6.m6.1.1.3.cmml">ğ¬</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.6.m6.1b"><apply id="S3.SS3.SSS0.Px1.p2.6.m6.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.6.m6.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.6.m6.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.6.m6.1.1.2">ğ±</ci><ci id="S3.SS3.SSS0.Px1.p2.6.m6.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.6.m6.1.1.3">ğ¬</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.6.m6.1c">\mathbf{x_{s}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.6.m6.1d">bold_x start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT</annotation></semantics></math> is a source sentence from the sentence knowledge. Each source sentence <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.7.m7.1"><semantics id="S3.SS3.SSS0.Px1.p2.7.m7.1a"><mi id="S3.SS3.SSS0.Px1.p2.7.m7.1.1" xref="S3.SS3.SSS0.Px1.p2.7.m7.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.7.m7.1b"><ci id="S3.SS3.SSS0.Px1.p2.7.m7.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.7.m7.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.7.m7.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.7.m7.1d">bold_x</annotation></semantics></math> is compared to all the sources from the sentence knowledge using the similarity score. We ignore perfect matches and keep the single best match sentence pair if its similarity score is higher than a specified thresholdÂ <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.8.m8.1"><semantics id="S3.SS3.SSS0.Px1.p2.8.m8.1a"><mi id="S3.SS3.SSS0.Px1.p2.8.m8.1.1" xref="S3.SS3.SSS0.Px1.p2.8.m8.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.8.m8.1b"><ci id="S3.SS3.SSS0.Px1.p2.8.m8.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.8.m8.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.8.m8.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.8.m8.1d">italic_Î»</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Matching Terminology</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.6">Terminology dictionaries specify phrase-level corresponding relationships of a sentence pair. When two matching terminologies from a sentence pair have overlapping ranges, a â€˜hardâ€™ match selects only one of them as matching results, such as maximum matching using the longest matching terminologies. The match strategy causes boundary errors, which could negatively impact the quality of the translation. Therefore, we adopt a â€˜softâ€™ match strategy to utilize all matching terminologies from sentence pairs. For each source <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px2.p1.1.m1.1a"><mi id="S3.SS3.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.1.m1.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.1.m1.1d">bold_x</annotation></semantics></math> and the corresponding target <math alttext="\mathbf{y}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.2.m2.1"><semantics id="S3.SS3.SSS0.Px2.p1.2.m2.1a"><mi id="S3.SS3.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml">ğ²</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.2.m2.1b"><ci id="S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1">ğ²</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.2.m2.1c">\mathbf{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.2.m2.1d">bold_y</annotation></semantics></math>, we record any matching bilingual terminologies that are fully contained in both the source <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.3.m3.1"><semantics id="S3.SS3.SSS0.Px2.p1.3.m3.1a"><mi id="S3.SS3.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.3.m3.1b"><ci id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.3.m3.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.3.m3.1d">bold_x</annotation></semantics></math> and target <math alttext="\mathbf{y}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.4.m4.1"><semantics id="S3.SS3.SSS0.Px2.p1.4.m4.1a"><mi id="S3.SS3.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.cmml">ğ²</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.4.m4.1b"><ci id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1">ğ²</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.4.m4.1c">\mathbf{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.4.m4.1d">bold_y</annotation></semantics></math>. For each sentence pair, source tokens of the matching terminologies are concatenated into <math alttext="\mathbf{x_{tm}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.5.m5.1"><semantics id="S3.SS3.SSS0.Px2.p1.5.m5.1a"><msub id="S3.SS3.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.cmml"><mi id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2.cmml">ğ±</mi><mi id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.cmml">ğ­ğ¦</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.5.m5.1b"><apply id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2">ğ±</ci><ci id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3">ğ­ğ¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.5.m5.1c">\mathbf{x_{tm}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.5.m5.1d">bold_x start_POSTSUBSCRIPT bold_tm end_POSTSUBSCRIPT</annotation></semantics></math> with the special token [Term], and corresponding target tokens are similarly concatenated into <math alttext="\mathbf{y_{tm}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.6.m6.1"><semantics id="S3.SS3.SSS0.Px2.p1.6.m6.1a"><msub id="S3.SS3.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.cmml"><mi id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2.cmml">ğ²</mi><mi id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.cmml">ğ­ğ¦</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.6.m6.1b"><apply id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2">ğ²</ci><ci id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3">ğ­ğ¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.6.m6.1c">\mathbf{y_{tm}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.6.m6.1d">bold_y start_POSTSUBSCRIPT bold_tm end_POSTSUBSCRIPT</annotation></semantics></math>. The NMT model learns to automatically choose the proper terminologies based on the terminological context by redundant prefixes that contain the all matching bilingual terminologies.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Translation Template Prediction</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px3.p1.1">To construct translation template sequence <math alttext="\mathbf{x_{tp}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px3.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px3.p1.1.m1.1a"><msub id="S3.SS3.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.2" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.2.cmml">ğ±</mi><mi id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.3" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.3.cmml">ğ­ğ©</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.2">ğ±</ci><ci id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.3">ğ­ğ©</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.1.m1.1c">\mathbf{x_{tp}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px3.p1.1.m1.1d">bold_x start_POSTSUBSCRIPT bold_tp end_POSTSUBSCRIPT</annotation></semantics></math>, we followÂ <cite class="ltx_cite ltx_citemacro_cite">Yang etÂ al. (<a class="ltx_ref" href="#bib.bib48" title="">2020</a>)</cite> to extract templates from a sub-tree by pruning the nodes deeper than a specific depth on the sentence corresponding constituency-based parser tree. We gain a parallel training data using the source sentences, extracted source and target templates. The constructed data is employed to train a sequence generation model to predict target template sequence. The model is to take the source sentence and corresponding source template sequence as inputs and generate template sequence as outputs.
</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we validate the effectiveness of the proposed approach on translation quality and terminology match accuracy by comparing with the previous methods used only a single type of knowledge.
We evaluate translation quality with the case-insensitive detokenized SacreBLEU scoreÂ <cite class="ltx_cite ltx_citemacro_cite">Post (<a class="ltx_ref" href="#bib.bib33" title="">2018</a>)</cite> and terminology match accuracy with exact match accuracyÂ <cite class="ltx_cite ltx_citemacro_cite">Anastasopoulos etÂ al. (<a class="ltx_ref" href="#bib.bib2" title="">2021</a>)</cite> which is defined as the ratio between the number of matched source term translations in the output and the total number of source terms.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Setup</h3>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Corpus</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">We evaluate our approach on English-Chinese (En-Zh) and English-German (En-De) translation tasks. For English-German, we use the WMT16 dataset as the training corpus of our model, consisting of 4.5M sentence pairs. We randomly divided the corpus into 4,000 sentences for the validation set and the rest for training. For English-Chinese, we train our model on CCMT2022 Corpus, containing 8.2M sentence pairs. The WMT newsdev2017 is used as the validation set.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p2.1">We measure the effectiveness of our model on multi-domain test sets. For English-German, we use the multi-domain English-German parallel dataÂ <cite class="ltx_cite ltx_citemacro_cite">Aharoni and Goldberg (<a class="ltx_ref" href="#bib.bib1" title="">2020</a>)</cite> as in-domain test sets, which include IT, Medical, Koran, and Law. For English-Chinese, We use the multi-domain English-Chinese parallel datasetÂ <cite class="ltx_cite ltx_citemacro_cite">Tian etÂ al. (<a class="ltx_ref" href="#bib.bib40" title="">2014</a>)</cite> as in-domain test sets, including Subtitles, News and Education. To distinguish the multi-domain sets for testing and the WMT16 or CCMT2022 sets for training, we call the multi-domain datasets as the in-domain training set or in-domain test set. We use only the training sets to train the models and evaluate results on in-domain test sets in our experiments. We retrieve similar sentence pairs from corresponding in-domain training sets for each in-domain test set. We used randomly selected 2,000 sentences from UM-Corpus as the validation sets of Fine-Tuning models. The sentence statistics of datasets are illustrated in TableÂ <a class="ltx_ref" href="#S4.T1" title="Table 1 â€£ Corpus â€£ 4.1 Setup â€£ 4 Experiments â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.1.1.1.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Task</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.1.1.1.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">Domain</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">Train</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">Vaild</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">Test</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.2.2.1" rowspan="3" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text" id="S4.T1.1.2.2.1.1">En-Zh</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.2.2.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">Subtitles</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.2.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">298K</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.2.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.2.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">579</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.3.3.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">News</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.3.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">448K</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.3.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.3.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">1,500</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.4.4.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Education</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.4.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">448K</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.4.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.4.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">790</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_tt" id="S4.T1.1.5.5.1" rowspan="4" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text" id="S4.T1.1.5.5.1.1">En-De</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.1.5.5.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">IT</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.5.5.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">223K</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.5.5.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.5.5.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.6.6.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Medical</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.6.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">248K</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.6.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.6.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.7.7.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Law</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.7.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">467K</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.7.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.7.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S4.T1.1.8.8.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Koran</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.8.8.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">18K</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.8.8.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.8.8.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">2,000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The number of training, validation, and test data sets of English-German and English-Chinese multi-domains.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p3">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p3.2">For all datasets, we tokenize English and German text with MosesÂ <span class="ltx_note ltx_role_footnote" id="footnote1a"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/moses-smt/mosesdecoder</span></span></span> and the Chinese text with JiebaÂ <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/fxsjy/jieba</span></span></span> tokenizer. We train a joint Byte Pair Encoding (BPE)Â <cite class="ltx_cite ltx_citemacro_cite">Sennrich etÂ al. (<a class="ltx_ref" href="#bib.bib35" title="">2016</a>)</cite> with 32k merge operations and use a joint vocabulary for both source and target text. The models in all experiments follow the state-of-the-art Transformer base architectureÂ <cite class="ltx_cite ltx_citemacro_cite">Vaswani etÂ al. (<a class="ltx_ref" href="#bib.bib41" title="">2017</a>)</cite> implemented in the Fairseq toolkitÂ <cite class="ltx_cite ltx_citemacro_cite">Ott etÂ al. (<a class="ltx_ref" href="#bib.bib31" title="">2019</a>)</cite>. The models are trained on 4 NVIDIA V100 GPUs and optimized with Adam algorithmÂ <cite class="ltx_cite ltx_citemacro_cite">Kingma and Ba (<a class="ltx_ref" href="#bib.bib21" title="">2015</a>)</cite> with <math alttext="{\beta}_{1}=0.9" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p3.1.m1.1"><semantics id="S4.SS1.SSS0.Px1.p3.1.m1.1a"><mrow id="S4.SS1.SSS0.Px1.p3.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.cmml"><msub id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.cmml"><mi id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.2" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.2.cmml">Î²</mi><mn id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.3" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.1" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.3" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p3.1.m1.1b"><apply id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1"><eq id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.1"></eq><apply id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.1.cmml" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.2.cmml" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.2">ğ›½</ci><cn id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.3.cmml" type="integer" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.2.3">1</cn></apply><cn id="S4.SS1.SSS0.Px1.p3.1.m1.1.1.3.cmml" type="float" xref="S4.SS1.SSS0.Px1.p3.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p3.1.m1.1c">{\beta}_{1}=0.9</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p3.1.m1.1d">italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9</annotation></semantics></math> and <math alttext="{\beta}_{2}=0.98" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p3.2.m2.1"><semantics id="S4.SS1.SSS0.Px1.p3.2.m2.1a"><mrow id="S4.SS1.SSS0.Px1.p3.2.m2.1.1" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.cmml"><msub id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.cmml"><mi id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.2" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.2.cmml">Î²</mi><mn id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.3" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.1" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.3" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.3.cmml">0.98</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p3.2.m2.1b"><apply id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1"><eq id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.1"></eq><apply id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.1.cmml" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.2.cmml" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.2">ğ›½</ci><cn id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.3.cmml" type="integer" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.2.3">2</cn></apply><cn id="S4.SS1.SSS0.Px1.p3.2.m2.1.1.3.cmml" type="float" xref="S4.SS1.SSS0.Px1.p3.2.m2.1.1.3">0.98</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p3.2.m2.1c">{\beta}_{2}=0.98</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p3.2.m2.1d">italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.98</annotation></semantics></math>. We set the learning rate to 0.0007. In all experiments, the dropout rate is set to 0.3 for English-German and 0.1 for English-Chinese. We use early stopping with a patience of 30 for all experiments. We averaged the last 5 checkpoints in all testing.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Baseline</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">We compare our approach with the following representative baselines:</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Vanilla NMT</span>Â <cite class="ltx_cite ltx_citemacro_cite">Vaswani etÂ al. (<a class="ltx_ref" href="#bib.bib41" title="">2017</a>)</cite>: We directly train a standard Transformer base model using the training set.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Fine-Tuning</span>: The model is fine-tuned using each in-domain training data set based on vanilla NMT. As a single-domain model with an upper bound on the performance, it loses the ability of multi-domain adaption.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.3"><math alttext="k" class="ltx_Math" display="inline" id="S4.I1.i3.p1.1.m1.1"><semantics id="S4.I1.i3.p1.1.m1.1a"><mi id="S4.I1.i3.p1.1.m1.1.1" xref="S4.I1.i3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.1.m1.1b"><ci id="S4.I1.i3.p1.1.m1.1.1.cmml" xref="S4.I1.i3.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i3.p1.1.m1.1d">italic_k</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.3.1">NN-MT</span>Â <cite class="ltx_cite ltx_citemacro_cite">Khandelwal etÂ al. (<a class="ltx_ref" href="#bib.bib20" title="">2021</a>)</cite>: The non-parametric method combines a NMT model with token-level <math alttext="k" class="ltx_Math" display="inline" id="S4.I1.i3.p1.2.m2.1"><semantics id="S4.I1.i3.p1.2.m2.1a"><mi id="S4.I1.i3.p1.2.m2.1.1" xref="S4.I1.i3.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.2.m2.1b"><ci id="S4.I1.i3.p1.2.m2.1.1.cmml" xref="S4.I1.i3.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i3.p1.2.m2.1d">italic_k</annotation></semantics></math>-nearest-neighbor(<math alttext="k" class="ltx_Math" display="inline" id="S4.I1.i3.p1.3.m3.1"><semantics id="S4.I1.i3.p1.3.m3.1a"><mi id="S4.I1.i3.p1.3.m3.1.1" xref="S4.I1.i3.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.3.m3.1b"><ci id="S4.I1.i3.p1.3.m3.1.1.cmml" xref="S4.I1.i3.p1.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i3.p1.3.m3.1d">italic_k</annotation></semantics></math>NN) by retrieving relevant token examples. It uses an in-domain training data set for domain adaptation tasks without additional training. The datastore is generated by an in-domain training set.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i4.p1.1.1">Priming-NMT</span>Â <cite class="ltx_cite ltx_citemacro_cite">Pham etÂ al. (<a class="ltx_ref" href="#bib.bib32" title="">2020</a>)</cite>: It only uses similar sentences as prefixes of a NMT model to force the model to generate a translation.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i5.p1.1.1">VecConstNMT</span>Â <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a class="ltx_ref" href="#bib.bib44" title="">2022b</a>)</cite>: It vectorizes and integrates lexical constraints (matching terminologies) into NMT models by attention modules. The method outperforms several strong baselines, including the worksÂ <cite class="ltx_cite ltx_citemacro_cite">Song etÂ al. (<a class="ltx_ref" href="#bib.bib37" title="">2019</a>); Chen etÂ al. (<a class="ltx_ref" href="#bib.bib8" title="">2021</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.2">En-De</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.3">En-Zh</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.2.1.1">Sentence</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.2">33.57%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.3">39.28%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.1.3.2.1">Terminology</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.3.2.2">42.66%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.3.2.3">18.23%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Percentage of source sentences with similar sentences and with matching terminologies on the training sets.
</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.2" style="width:433.6pt;height:246.6pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-5.7pt,3.2pt) scale(0.974485597836756,0.974485597836756) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.2.2.3.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T3.2.2.3.1.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5" id="S4.T3.2.2.3.1.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.2.2.3.1.2.1">English-German</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S4.T3.2.2.3.1.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.2.2.3.1.3.1">English-Chinese</span></th>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.4.2.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.1.1">Metric</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T3.2.2.4.2.2" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.2.1">Method (<span class="ltx_text ltx_font_italic" id="S4.T3.2.2.4.2.2.1.1">Knowledge</span>)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.4.2.3" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.3.1">IT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.4.2.4" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.4.1">Medical</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.4.2.5" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.5.1">Law</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T3.2.2.4.2.6" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.6.1">Koran</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt" id="S4.T3.2.2.4.2.7" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.7.1">Avg.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.4.2.8" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.8.1">Subtitles</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.4.2.9" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.9.1">News</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T3.2.2.4.2.10" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.10.1">Education</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.4.2.11" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.11.1">Avg.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.2.2.5.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.2.2.5.1.1" rowspan="6" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text" id="S4.T3.2.2.5.1.1.1">BLEU</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.2.2.5.1.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">Fine-Tuning</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.5.1.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">40.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.5.1.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">53.14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.5.1.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">56.68</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.5.1.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">28.08</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S4.T3.2.2.5.1.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">44.67</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.5.1.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">27.53</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.5.1.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">33.91</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.5.1.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">47.96</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.5.1.11" style="padding-top:0.4pt;padding-bottom:0.4pt;">36.47</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.6.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.2.6.2.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Vanilla NMT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.6.2.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">23.07</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.6.2.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">30.72</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.6.2.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">35.57</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.6.2.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">10.16</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.2.2.6.2.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">24.88</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.6.2.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">24.34</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.6.2.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">31.43</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.6.2.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">38.54</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.6.2.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">31.44</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.7.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.2.7.3.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">VecConstNMT (<span class="ltx_text ltx_font_italic" id="S4.T3.2.2.7.3.1.1">Term.</span>)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.7.3.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">23.74</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.7.3.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">30.41</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.7.3.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">35.19</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.7.3.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">10.21</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.2.2.7.3.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">24.89</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.7.3.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">24.70</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.7.3.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">31.61</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.7.3.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">38.78</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.7.3.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">31.70</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.8.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.2.8.4.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Priming-NMT (<span class="ltx_text ltx_font_italic" id="S4.T3.2.2.8.4.1.1">Sent.</span>)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.8.4.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">25.93</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.8.4.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">37.94</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.8.4.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">41.28</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.8.4.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">11.41</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.2.2.8.4.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">29.14</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.8.4.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">38.79</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.8.4.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">34.83</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.8.4.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">52.83</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.8.4.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">42.15</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.1.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">
<math alttext="k" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.m1.1a"><mi id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.m1.1d">italic_k</annotation></semantics></math>NN-MT (<span class="ltx_text ltx_font_italic" id="S4.T3.1.1.1.1.1">Sent.</span>)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">31.00</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">45.59</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">50.70</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.1.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">17.66</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.1.1.1.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">36.23</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">41.31</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">35.07</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.1.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">51.32</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">42.57</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.9.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.2.9.5.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.2.2.9.5.1.1">Ours</span> (<span class="ltx_text ltx_font_italic" id="S4.T3.2.2.9.5.1.2">Term.+Sent.+Temp.</span>)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.9.5.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">32.43</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.9.5.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">45.14</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.9.5.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">50.00</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.9.5.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">18.89</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.2.2.9.5.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">36.62</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.9.5.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">40.13</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.9.5.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">37.87</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.9.5.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">55.25</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.9.5.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">44.42</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.10.6">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T3.2.2.10.6.1" rowspan="6" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text" id="S4.T3.2.2.10.6.1.1">Exact Match</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.2.2.10.6.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">Fine-Tuning</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.10.6.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">59.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.10.6.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">69.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.10.6.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">66.29</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.10.6.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">35.45</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S4.T3.2.2.10.6.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">57.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.10.6.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">53.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.10.6.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">58.29</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.10.6.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">65.89</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.10.6.11" style="padding-top:0.4pt;padding-bottom:0.4pt;">59.14</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.11.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.2.11.7.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Vanilla NMT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.11.7.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">34.09</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.11.7.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">41.92</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.11.7.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">43.29</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.11.7.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">19.09</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.2.2.11.7.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">34.60</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.11.7.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">59.17</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.11.7.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">55.50</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.11.7.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">60.75</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.11.7.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">58.47</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.12.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.2.12.8.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">VecConstNMT (<span class="ltx_text ltx_font_italic" id="S4.T3.2.2.12.8.1.1">Term.</span>)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.12.8.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">80.91</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.12.8.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">83.99</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.12.8.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">83.87</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.12.8.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">77.27</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.2.2.12.8.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">81.51</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.12.8.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.31</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.12.8.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">88.01</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.12.8.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">94.39</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.12.8.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">91.57</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.13.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.2.13.9.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Priming-NMT (<span class="ltx_text ltx_font_italic" id="S4.T3.2.2.13.9.1.1">Sent.</span>)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.13.9.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">37.48</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.13.9.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">50.00</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.13.9.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">49.38</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.13.9.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">15.45</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.2.2.13.9.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">38.08</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.13.9.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">66.86</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.13.9.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">56.32</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.13.9.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">66.82</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.13.9.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">63.33</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.2.2.2.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">
<math alttext="k" class="ltx_Math" display="inline" id="S4.T3.2.2.2.1.m1.1"><semantics id="S4.T3.2.2.2.1.m1.1a"><mi id="S4.T3.2.2.2.1.m1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b"><ci id="S4.T3.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.1.m1.1d">italic_k</annotation></semantics></math>NN-MT (<span class="ltx_text ltx_font_italic" id="S4.T3.2.2.2.1.1">Sent.</span>)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">43.56</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">61.15</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">61.16</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.2.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">24.55</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.2.2.2.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">47.61</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">50.30</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">47.87</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.2.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">63.55</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">53.91</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.14.10">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T3.2.2.14.10.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.2.2.14.10.1.1">Ours</span> (<span class="ltx_text ltx_font_italic" id="S4.T3.2.2.14.10.1.2">Term.+Sent.+Temp.</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.2.14.10.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">80.20</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.2.14.10.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">86.58</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.2.14.10.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">89.52</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.2.2.14.10.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">81.82</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" id="S4.T3.2.2.14.10.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">84.53</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.2.14.10.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.90</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.2.14.10.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">94.66</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.2.2.14.10.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">90.65</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.2.14.10.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.74</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Evaluation results on the English-German and English-Chinese multi-domain test sets, reported on BLEU and exact match accuracy of terminology. <span class="ltx_text ltx_font_italic" id="S4.T3.6.1">Term.</span>, <span class="ltx_text ltx_font_italic" id="S4.T3.7.2">Sent.</span> and <span class="ltx_text ltx_font_italic" id="S4.T3.8.3">Temp.</span> indicate terminology, sentence and template knowledge, respectively.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Training Data</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.1">The similar sentence pairs are extracted from an training data set using a specified similarity threshold of 0.4 in our experiments. For terminology knowledge, we extract a bilingual terminology dictionary from the training data set using a term extraction tool TM2TBÂ <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/luismond/tm2tb</span></span></span> with default parameters and use the dictionary to match each source and target sentences in the training data set by the â€˜softâ€™ match strategy. We use Stanford parserÂ <cite class="ltx_cite ltx_citemacro_cite">Manning etÂ al. (<a class="ltx_ref" href="#bib.bib30" title="">2014</a>)</cite> to generate source and target templates based on a specific depth 4 from the training data. We build our methodâ€™s training data by combining corresponding the similar sentence pair, matching terminologies and translation templates for each sentence pair from the training data set. TableÂ <a class="ltx_ref" href="#S4.T2" title="Table 2 â€£ Baseline â€£ 4.1 Setup â€£ 4 Experiments â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">2</span></a> provides the percentage of source sentences with a similar sentence pair where the score is higher than the similarity threshold of 0.4 and the percentage of sentences with matching terminologies in the training data set. We use the vanilla NMT to train our proposed models based on the training data.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Test Data</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px4.p1.1">For each test set of multi-domain sets, we retrieve similar sentence pairs from the training data set and corresponding in-domain training data set. The number of terminologies extracted by TM2TB with the default threshold of 0.9 is not sufficient to validate terminology accuracy in the in-domain test sets. Therefore, we use TM2TB with a similarity threshold of 0.7 to extract the bilingual terminologies from the in-domain test set, and then use the terminologies to match each sentence pair. We train a model based on the pre-trained model mBARTÂ <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a class="ltx_ref" href="#bib.bib28" title="">2020</a>)</cite> using source sentences, source and target templates extracted from parse tree on in-domain training data. Then we use the model to predict target templates of in-domain test data.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.1" style="width:433.6pt;height:197pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-22.1pt,10.0pt) scale(0.907680323070687,0.907680323070687) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.1.1.1">
<td class="ltx_td ltx_border_tt" colspan="2" id="S4.T4.1.1.1.1.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="S4.T4.1.1.1.1.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">
<span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.2.1">English-German</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T4.1.1.1.1.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">
<span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.3.1">English-Chinese</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T4.1.1.1.1.4" style="padding-top:0.4pt;padding-bottom:0.4pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.1.1.2.2.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.1.1">Metric</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.1.1.2.2.2" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.2.1">Method</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T4.1.1.2.2.3" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T4.1.1.2.2.3.1">Knowledge</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.2.2.4" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.4.1">IT</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.2.2.5" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.5.1">Medical</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.2.2.6" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.6.1">Law</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.1.1.2.2.7" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.7.1">Koran</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" id="S4.T4.1.1.2.2.8" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.8.1">Avg.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.2.2.9" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.9.1">Subtitles</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.2.2.10" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.10.1">News</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.1.1.2.2.11" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.11.1">Education</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.2.2.12" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.12.1">Avg.</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.1.3.3.1" rowspan="5" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text" id="S4.T4.1.1.3.3.1.1">BLEU</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.1.3.3.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">Priming-NMT</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.3" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.3.3.3.1">Sent.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">25.93</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">37.94</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">41.28</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">11.41</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S4.T4.1.1.3.3.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">29.14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">38.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">34.83</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.11" style="padding-top:0.4pt;padding-bottom:0.4pt;">52.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.3.3.12" style="padding-top:0.4pt;padding-bottom:0.4pt;">42.15</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.1.4.4.1" rowspan="4" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.4.4.1.1">Ours</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.1.1.4.4.2" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.4.4.2.1">Term.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">25.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">31.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">35.24</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.4.4.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">10.33</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S4.T4.1.1.4.4.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">25.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">25.32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">33.67</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.4.4.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">41.47</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.11" style="padding-top:0.4pt;padding-bottom:0.4pt;">33.49</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.1.1.5.5.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.5.5.1.1">Sent.</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">29.49</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">44.11</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">49.52</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.5.5.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">13.86</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T4.1.1.5.5.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">34.25</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">39.03</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">35.03</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.5.5.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">52.39</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">42.15</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.1.1.6.6.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.6.6.1.1">Term.+Sent.</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">31.34</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">43.74</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">49.77</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.6.6.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">13.61</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T4.1.1.6.6.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">34.62</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">39.95</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">37.53</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.6.6.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">53.73</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">43.74</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.1.1.7.7.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.7.7.1.1">Term.+Sent.+Temp.</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.7.7.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">32.43</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.7.7.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">45.14</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.7.7.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">50.00</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.7.7.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">18.89</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T4.1.1.7.7.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">36.62</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.7.7.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">40.13</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.7.7.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">37.87</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.7.7.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">55.25</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.7.7.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">44.42</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T4.1.1.8.8.1" rowspan="5" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text" id="S4.T4.1.1.8.8.1.1">Exact Match</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.1.8.8.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">VecConstNMT</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.1.1.8.8.3" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.8.8.3.1">Term.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.8.8.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">80.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.8.8.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">83.99</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.8.8.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">83.87</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.8.8.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">77.27</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S4.T4.1.1.8.8.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">81.51</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.8.8.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.31</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.8.8.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">88.01</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.8.8.11" style="padding-top:0.4pt;padding-bottom:0.4pt;">94.39</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.8.8.12" style="padding-top:0.4pt;padding-bottom:0.4pt;">91.57</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.9.9">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T4.1.1.9.9.1" rowspan="4" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.9.9.1.1">Ours</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.1.1.9.9.2" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.9.9.2.1">Term.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.9.9.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">88.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.9.9.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.85</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.9.9.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">91.09</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.9.9.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">87.27</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S4.T4.1.1.9.9.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">89.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.9.9.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.9.9.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">91.54</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.9.9.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">97.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.9.9.11" style="padding-top:0.4pt;padding-bottom:0.4pt;">93.88</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.10.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.1.1.10.10.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.10.10.1.1">Sent.</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">39.75</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">56.83</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">57.72</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.10.10.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">23.64</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T4.1.1.10.10.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">44.49</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">52.94</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">57.20</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.10.10.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">61.11</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">57.08</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.11.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.1.1.11.11.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.11.11.1.1">Term.+Sent.</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.11.11.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">80.05</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.11.11.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">86.89</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.11.11.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">89.97</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.11.11.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">78.18</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T4.1.1.11.11.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">83.77</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.11.11.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.31</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.11.11.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">94.91</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.11.11.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.52</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.11.11.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">93.25</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.12.12">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T4.1.1.12.12.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.1.12.12.1.1">Term.+Sent.+Temp.</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.12.12.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">80.20</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.12.12.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">86.58</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.12.12.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">89.52</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.1.1.12.12.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">81.82</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" id="S4.T4.1.1.12.12.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">84.53</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.12.12.7" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.90</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.12.12.8" style="padding-top:0.4pt;padding-bottom:0.4pt;">94.66</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.1.1.12.12.9" style="padding-top:0.4pt;padding-bottom:0.4pt;">90.65</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.12.12.10" style="padding-top:0.4pt;padding-bottom:0.4pt;">92.74</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Ablation result on BLEU and exact match accuracy using only partial type of knowledge on the English-German and English-Chinese.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2">TableÂ <a class="ltx_ref" href="#S4.T3" title="Table 3 â€£ Baseline â€£ 4.1 Setup â€£ 4 Experiments â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">3</span></a> shows the BLEU and exact match accuracy on Fine-Tuning, NMT, VecConstNMT, <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_k</annotation></semantics></math>NN-MT, Priming-NMT, and our proposed method on the English-German and English-Chinese multi-domain test data sets. Our method outperforms all baselines in terms of exact match accuracy on average, demonstrating the benefits of integrating sentence, terminology and template knowledge into NMT models. On English-German multi-domain test sets, our method improves an average of 10.74 BLEU and 49.93% exact match accuracy over vanilla NNT. Compared with Priming-NMT using sentence knowledge, our method enhances performance by up to 7.48 BLEU on average. Our method outperforms than <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mi id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">italic_k</annotation></semantics></math>NN-MT in the IT and Koran domains.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.2">For English-Chinese multi-domain test sets, our method performs better than Fine-Tuning. The Subtitles, News and Education three domains training data contains noises, such as sentence pairs or domain mismatches. The performance improvement of Fine-Tuning relied on the quality of the in-domain the training data is not significant. Similarly, the performance of <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">italic_k</annotation></semantics></math>NN-MT depends on training data quality, and our method achieves better BLEU in News and Education domains compared to <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2.1"><semantics id="S4.SS2.p2.2.m2.1a"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.2.m2.1d">italic_k</annotation></semantics></math>NN-MT. Therefore, our approach has stronger generalization ability and significant performance improvements in these domains compared to the baselines.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.3.3">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T5.3.3.4" style="padding-top:0.4pt;padding-bottom:0.4pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"><math alttext="\lambda\geq 0.4" class="ltx_Math" display="inline" id="S4.T5.1.1.1.m1.1"><semantics id="S4.T5.1.1.1.m1.1a"><mrow id="S4.T5.1.1.1.m1.1.1" xref="S4.T5.1.1.1.m1.1.1.cmml"><mi id="S4.T5.1.1.1.m1.1.1.2" xref="S4.T5.1.1.1.m1.1.1.2.cmml">Î»</mi><mo id="S4.T5.1.1.1.m1.1.1.1" xref="S4.T5.1.1.1.m1.1.1.1.cmml">â‰¥</mo><mn id="S4.T5.1.1.1.m1.1.1.3" xref="S4.T5.1.1.1.m1.1.1.3.cmml">0.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.m1.1b"><apply id="S4.T5.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1"><geq id="S4.T5.1.1.1.m1.1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1.1"></geq><ci id="S4.T5.1.1.1.m1.1.1.2.cmml" xref="S4.T5.1.1.1.m1.1.1.2">ğœ†</ci><cn id="S4.T5.1.1.1.m1.1.1.3.cmml" type="float" xref="S4.T5.1.1.1.m1.1.1.3">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.m1.1c">\lambda\geq 0.4</annotation><annotation encoding="application/x-llamapun" id="S4.T5.1.1.1.m1.1d">italic_Î» â‰¥ 0.4</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.2.2.2" style="padding-top:0.4pt;padding-bottom:0.4pt;"><math alttext="\lambda\geq 0.5" class="ltx_Math" display="inline" id="S4.T5.2.2.2.m1.1"><semantics id="S4.T5.2.2.2.m1.1a"><mrow id="S4.T5.2.2.2.m1.1.1" xref="S4.T5.2.2.2.m1.1.1.cmml"><mi id="S4.T5.2.2.2.m1.1.1.2" xref="S4.T5.2.2.2.m1.1.1.2.cmml">Î»</mi><mo id="S4.T5.2.2.2.m1.1.1.1" xref="S4.T5.2.2.2.m1.1.1.1.cmml">â‰¥</mo><mn id="S4.T5.2.2.2.m1.1.1.3" xref="S4.T5.2.2.2.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.m1.1b"><apply id="S4.T5.2.2.2.m1.1.1.cmml" xref="S4.T5.2.2.2.m1.1.1"><geq id="S4.T5.2.2.2.m1.1.1.1.cmml" xref="S4.T5.2.2.2.m1.1.1.1"></geq><ci id="S4.T5.2.2.2.m1.1.1.2.cmml" xref="S4.T5.2.2.2.m1.1.1.2">ğœ†</ci><cn id="S4.T5.2.2.2.m1.1.1.3.cmml" type="float" xref="S4.T5.2.2.2.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.m1.1c">\lambda\geq 0.5</annotation><annotation encoding="application/x-llamapun" id="S4.T5.2.2.2.m1.1d">italic_Î» â‰¥ 0.5</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.3.3.3" style="padding-top:0.4pt;padding-bottom:0.4pt;"><math alttext="\lambda\geq 0.6" class="ltx_Math" display="inline" id="S4.T5.3.3.3.m1.1"><semantics id="S4.T5.3.3.3.m1.1a"><mrow id="S4.T5.3.3.3.m1.1.1" xref="S4.T5.3.3.3.m1.1.1.cmml"><mi id="S4.T5.3.3.3.m1.1.1.2" xref="S4.T5.3.3.3.m1.1.1.2.cmml">Î»</mi><mo id="S4.T5.3.3.3.m1.1.1.1" xref="S4.T5.3.3.3.m1.1.1.1.cmml">â‰¥</mo><mn id="S4.T5.3.3.3.m1.1.1.3" xref="S4.T5.3.3.3.m1.1.1.3.cmml">0.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.3.m1.1b"><apply id="S4.T5.3.3.3.m1.1.1.cmml" xref="S4.T5.3.3.3.m1.1.1"><geq id="S4.T5.3.3.3.m1.1.1.1.cmml" xref="S4.T5.3.3.3.m1.1.1.1"></geq><ci id="S4.T5.3.3.3.m1.1.1.2.cmml" xref="S4.T5.3.3.3.m1.1.1.2">ğœ†</ci><cn id="S4.T5.3.3.3.m1.1.1.3.cmml" type="float" xref="S4.T5.3.3.3.m1.1.1.3">0.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.3.m1.1c">\lambda\geq 0.6</annotation><annotation encoding="application/x-llamapun" id="S4.T5.3.3.3.m1.1d">italic_Î» â‰¥ 0.6</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T5.3.4.1.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">IT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.4.1.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">32.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.4.1.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">31.61</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.4.1.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">30.91</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T5.3.5.2.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Medical</th>
<td class="ltx_td ltx_align_center" id="S4.T5.3.5.2.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">45.14</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.5.2.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">44.41</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.5.2.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">42.85</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T5.3.6.3.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Law</th>
<td class="ltx_td ltx_align_center" id="S4.T5.3.6.3.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">50.00</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.6.3.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">49.10</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.6.3.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">47.76</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T5.3.7.4.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Koran</th>
<td class="ltx_td ltx_align_center" id="S4.T5.3.7.4.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">18.89</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.7.4.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">17.56</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.7.4.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">16.38</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S4.T5.3.8.5.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Avg</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.3.8.5.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">36.62</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.3.8.5.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">35.67</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.3.8.5.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">34.46</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Effect of the threshold <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.T5.5.m1.1"><semantics id="S4.T5.5.m1.1b"><mi id="S4.T5.5.m1.1.1" xref="S4.T5.5.m1.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S4.T5.5.m1.1c"><ci id="S4.T5.5.m1.1.1.cmml" xref="S4.T5.5.m1.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.5.m1.1d">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.T5.5.m1.1e">italic_Î»</annotation></semantics></math> for similar sentence retrieval on BLUE on the English to German.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation Studies</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In this subsection, we perform ablation experiments on proposed models in order to better understand their relative importance. TableÂ <a class="ltx_ref" href="#S4.T4" title="Table 4 â€£ Test Data â€£ 4.1 Setup â€£ 4 Experiments â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">4</span></a> shows evaluation results of the proposed model using only one or two type of knowledge on in-domain test sets. Our proposed method (<span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">Sent.</span>) using sentence knowledge outperforms the strong baseline Priming-NMT by 5.11 BLEU on English-German on average, which indicates that the loss function EquationÂ <a class="ltx_ref" href="#S3.E2" title="2 â€£ 3.1 Training â€£ 3 Approach â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">2</span></a> could significantly enhance the performance during training. Our method (<span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.2">Term.</span>) using terminology knowledge outperforms the strong baseline VecConstNMT in terms of exact match accuracy on average.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Our method (<span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.1">Term.+Sent.</span>) using sentence and terminology knowledge achieves both BLEU and exact match accuracy improvements compared with our methods used only sentence or terminology knowledge. When sentence, terminology and template knowledge are used simultaneously, our method (<span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.2">Term.+Sent.+Temp.</span>) outperforms the method (<span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.3">Term.+Sent.</span>) using sentence and terminology knowledge by 2 BLEU on English-German and by 0.68 BLEU on average on English-Chinese on average respectively, which shows that the translation templates could effectively improve the translation performance.</p>
</div>
<figure class="ltx_table" id="S4.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T6.3" style="width:433.6pt;height:290.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(136.3pt,-91.4pt) scale(2.69328576898435,2.69328576898435) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T6.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T6.3.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T6.3.1.1.1.1" style="padding-top:0.4pt;padding-bottom:0.4pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.3.1.1.1.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">IT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.3.1.1.1.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">Medical</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.3.1.1.1.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">Law</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T6.3.1.1.1.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">Koran</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.3.1.1.1.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">Avg.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T6.3.1.2.1.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">#Term.</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.3.1.2.1.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">1.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.3.1.2.1.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">4.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.3.1.2.1.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">5.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.1.2.1.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">0.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.3.1.2.1.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">2.9</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T6.3.1.3.2.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">#Sent.</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.3.2.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">6.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.3.2.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">12.8</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.3.2.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">17.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.3.1.3.2.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">11.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.3.2.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">12.1</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T6.3.1.4.3.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">#Temp.</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.4.3.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">5.6</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.4.3.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">4.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.4.3.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">2.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.3.1.4.3.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">8.1</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.4.3.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">5.2</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T6.3.1.5.4.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">#Know.</th>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.5.4.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">13.7</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.5.4.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">21.6</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.5.4.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">25.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.3.1.5.4.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">19.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.3.1.5.4.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">20.2</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S4.T6.3.1.6.5.1" style="padding-top:0.4pt;padding-bottom:0.4pt;">Speed</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.3.1.6.5.2" style="padding-top:0.4pt;padding-bottom:0.4pt;">1.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.3.1.6.5.3" style="padding-top:0.4pt;padding-bottom:0.4pt;">1.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.3.1.6.5.4" style="padding-top:0.4pt;padding-bottom:0.4pt;">1.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T6.3.1.6.5.5" style="padding-top:0.4pt;padding-bottom:0.4pt;">1.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T6.3.1.6.5.6" style="padding-top:0.4pt;padding-bottom:0.4pt;">1.8</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Relative inference speed for our method compared to the vanilla NMT in English to German multi-domain test sets. The batch size is <math alttext="32" class="ltx_Math" display="inline" id="S4.T6.2.m1.1"><semantics id="S4.T6.2.m1.1b"><mn id="S4.T6.2.m1.1.1" xref="S4.T6.2.m1.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.T6.2.m1.1c"><cn id="S4.T6.2.m1.1.1.cmml" type="integer" xref="S4.T6.2.m1.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.m1.1d">32</annotation><annotation encoding="application/x-llamapun" id="S4.T6.2.m1.1e">32</annotation></semantics></math>. #Term., #Sent., #Temp., and #Know. indicate the average number of tokens of matching terminologies, similar sentences, predicted templates and whole knowledge sequences on the target, respectively.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Effect of Similarity Threshold <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.SS4.1.m1.1"><semantics id="S4.SS4.1.m1.1b"><mi id="S4.SS4.1.m1.1.1" xref="S4.SS4.1.m1.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.1.m1.1c"><ci id="S4.SS4.1.m1.1.1.cmml" xref="S4.SS4.1.m1.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.1.m1.1d">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.1.m1.1e">italic_Î»</annotation></semantics></math>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Most worksÂ <cite class="ltx_cite ltx_citemacro_cite">Bulte and Tezcan (<a class="ltx_ref" href="#bib.bib4" title="">2019a</a>); Xu etÂ al. (<a class="ltx_ref" href="#bib.bib46" title="">2020</a>); Pham etÂ al. (<a class="ltx_ref" href="#bib.bib32" title="">2020</a>)</cite> use 0.5 or 0.6 as a similarity threshold. TableÂ <a class="ltx_ref" href="#S4.T5" title="Table 5 â€£ 4.2 Main Results â€£ 4 Experiments â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">5</span></a> shows the effect of the threshold for similar sentence
retrieval on translation quality. We find that retrieving similar sentences using a lower threshold leads to improvements. The average best performance on in-domain test sets can be achieved by our method based on the 0.4 threshold.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Inference Speed</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.2">We report the inference speed of our approach relative to the vanilla NMT in TableÂ <a class="ltx_ref" href="#S4.T6" title="Table 6 â€£ 4.3 Ablation Studies â€£ 4 Experiments â€£ Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting"><span class="ltx_text ltx_ref_tag">6</span></a>. We use the multiple types of knowledge as prefixes of the encoder and decoder of the NMT model, increasing the extra calculating time during decoding. The speed of the proposed method using beam search is 1.6<math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS5.p1.1.m1.1"><semantics id="S4.SS5.p1.1.m1.1a"><mo id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.1.m1.1d">âˆ¼</annotation></semantics></math>1.9 times slower than the vanilla NMT and mainly depends on the number of tokens of the prefixes on the target during decoding. Compared to <math alttext="k" class="ltx_Math" display="inline" id="S4.SS5.p1.2.m2.1"><semantics id="S4.SS5.p1.2.m2.1a"><mi id="S4.SS5.p1.2.m2.1.1" xref="S4.SS5.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.2.m2.1b"><ci id="S4.SS5.p1.2.m2.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.2.m2.1d">italic_k</annotation></semantics></math>NN-MT with a generation speed that is two orders of magnitude slower than the vanilla NMT, our method is more easily acceptable in terms of inference speed.
</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we propose a unified framework to integrate multi-knowledge into NMT models. We utilize multiple types of knowledge as prefixes of the encoder and decoder of NMT models, which guides the NMT modelâ€™s translation process. Especially, our approaches do not actually require the model to see the domain-specific data in training. The model has automatic domain adaption capability and can be extended to new domains without updating parameters. The experimental results on multi-domain translation tasks demonstrated that incorporating multiple types of knowledge into NMT models leads to significant improvements in both translation quality and exact match accuracy.
</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Monolingual data is valuable to improve the translation quality of NMT models. In the future, we would like to integrate monolingual knowledge into the NMT model. Furthermore, our approach can be applied for tasks where there are multiple types of knowledge, such as Question Answering and Image to Text.
</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">As with the majority of studies, the design of the current approach is subject to limitations. We integrate multiple types of knowledge as additional prefixes of NMT models and add time consumption in the training and inference stages. The experimental results show the added time cost of the proposed method is acceptable. Our approach depends on multiple types of knowledge and obtaining the knowledge may be difficult in some practical applications.
</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aharoni and Goldberg (2020)</span>
<span class="ltx_bibblock">
Roee Aharoni and Yoav Goldberg. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2004.02105" title="">Unsupervised domain
clusters in pretrained language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anastasopoulos etÂ al. (2021)</span>
<span class="ltx_bibblock">
Antonios Anastasopoulos, Laurent Besacier, James Cross, Matthias GallÃ©,
Philipp Koehn, Vassilina Nikoulina, etÂ al. 2021.

</span>
<span class="ltx_bibblock">On the evaluation of machine translation for terminology consistency.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2106.11891</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
etÂ al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Advances in neural information processing systems</em>,
33:1877â€“1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bulte and Tezcan (2019a)</span>
<span class="ltx_bibblock">
Bram Bulte and Arda Tezcan. 2019a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1175" title="">Neural fuzzy repair:
Integrating fuzzy matches into neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 1800â€“1809, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bulte and Tezcan (2019b)</span>
<span class="ltx_bibblock">
Bram Bulte and Arda Tezcan. 2019b.

</span>
<span class="ltx_bibblock">Neural fuzzy repair: Integrating fuzzy matches into neural machine
translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">57th Annual Meeting of the
Association-for-Computational-Linguistics (ACL)</em>, pages 1800â€“1809.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai etÂ al. (2021)</span>
<span class="ltx_bibblock">
Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. 2021.

</span>
<span class="ltx_bibblock">Neural machine translation with monolingual translation memory.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 7307â€“7318.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao and Xiong (2018)</span>
<span class="ltx_bibblock">
Qian Cao and Deyi Xiong. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-1340" title="">Encoding gated
translation memory into neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pages 3042â€“3047, Brussels, Belgium.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2021)</span>
<span class="ltx_bibblock">
Guanhua Chen, Yun Chen, and VictorÂ O.K. Li. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1609/aaai.v35i14.17496" title="">Lexically
constrained neural machine translation with explicit alignment guidance</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>,
35(14):12630â€“12638.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2020)</span>
<span class="ltx_bibblock">
Guanhua Chen, Yun Chen, Yong Wang, and VictorÂ O.K. Li. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.24963/ijcai.2020/496" title="">Lexical-constraint-aware neural machine translation via data augmentation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the Twenty-Ninth International Joint
Conference on Artificial Intelligence, IJCAI-20</em>, pages 3587â€“3593.
International Joint Conferences on Artificial Intelligence Organization.

</span>
<span class="ltx_bibblock">Main track.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dinu etÂ al. (2019)</span>
<span class="ltx_bibblock">
Georgiana Dinu, Prashant Mathur, Marcello Federico, and Yaser Al-Onaizan. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1294" title="">Training neural machine
translation to apply terminology constraints</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 3063â€“3068, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dougal and Lonsdale (2020)</span>
<span class="ltx_bibblock">
DuaneÂ K. Dougal and Deryle Lonsdale. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.lrec-1.593" title="">Improving NMT
quality using terminology injection</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the Twelfth Language Resources and Evaluation
Conference</em>, pages 4820â€“4827, Marseille, France. European Language Resources
Association.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Farajian etÂ al. (2017)</span>
<span class="ltx_bibblock">
M.Â Amin Farajian, Marco Turchi, Matteo Negri, and Marcello Federico. 2017.

</span>
<span class="ltx_bibblock">Multi-domain neural machine translation through unsupervised
adaptation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Conference on Machine Translation</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu etÂ al. (2018)</span>
<span class="ltx_bibblock">
Jiatao Gu, Yong Wang, Kyunghyun Cho, and VictorÂ OK Li. 2018.

</span>
<span class="ltx_bibblock">Search engine guided neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volumeÂ 32.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2021)</span>
<span class="ltx_bibblock">
Qiuxiang He, Guoping Huang, QuÂ Cui, LiÂ Li, and Lemao Liu. 2021.

</span>
<span class="ltx_bibblock">Fast and accurate neural machine translation with translation memory.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 3170â€“3180.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2019)</span>
<span class="ltx_bibblock">
Qiuxiang He, Guoping Huang, Lemao Liu, and LiÂ Li. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/978-3-030-32233-5_29" title="">Word position
aware translation memory for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Natural Language Processing and Chinese Computing: 8th CCF
International Conference, NLPCC 2019, Dunhuang, China, October 9â€“14, 2019,
Proceedings, Part I</em>, page 367â€“379, Berlin, Heidelberg. Springer-Verlag.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hokamp and Liu (2017)</span>
<span class="ltx_bibblock">
Chris Hokamp and Qun Liu. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P17-1141" title="">Lexically constrained
decoding for sequence generation using grid beam search</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1535â€“1546,
Vancouver, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2019)</span>
<span class="ltx_bibblock">
J.Â Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt
Post, and Benjamin VanÂ Durme. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1090" title="">Improved lexically
constrained decoding for translation and monolingual rewriting</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 839â€“850, Minneapolis,
Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al. (2021)</span>
<span class="ltx_bibblock">
Guoping Huang, Lemao Liu, Xing Wang, Longyue Wang, Huayang Li,
Zhaopeng Tu, Chengyan Huang, and Shuming Shi. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2105.13072" title="">TranSmart: A Practical
Interactive Machine Translation System</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv e-prints</em>, page arXiv:2105.13072.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jon etÂ al. (2021)</span>
<span class="ltx_bibblock">
Josef Jon, JoÃ£oÂ Paulo Aires, Dusan Varis, and OndÅ™ej Bojar. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.311" title="">End-to-end
lexically constrained machine translation for morphologically rich
languages</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 4019â€“4033,
Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal etÂ al. (2021)</span>
<span class="ltx_bibblock">
Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.
2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=7wCBOfJ8hJM" title="">Nearest neighbor
machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2015)</span>
<span class="ltx_bibblock">
DiederikÂ P. Kingma and Jimmy Ba. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1412.6980" title="">Adam: A method for
stochastic optimization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn and Senellart (2010)</span>
<span class="ltx_bibblock">
Philipp Koehn and Jean Senellart. 2010.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2010.jec-1.4" title="">Convergence of
translation memory and statistical machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the Second Joint EM+/CNGL Workshop: Bringing
MT to the User: Research on Integrating MT in the Translation Industry</em>,
pages 21â€“32, Denver, Colorado, USA. Association for Machine Translation in
the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levenshtein (1965)</span>
<span class="ltx_bibblock">
VladimirÂ I. Levenshtein. 1965.

</span>
<span class="ltx_bibblock">Binary codes capable of correcting deletions, insertions, and
reversals.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Soviet physics. Doklady</em>, 10:707â€“710.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2019)</span>
<span class="ltx_bibblock">
Huayang Li, Guoping Huang, Deng Cai, and Lemao Liu. 2019.

</span>
<span class="ltx_bibblock">Neural machine translation with noisy lexical constraints.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">IEEE/ACM Transactions on Audio, Speech, and Language
Processing</em>, 28:1864â€“1874.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2016)</span>
<span class="ltx_bibblock">
Chunyang Liu, Yang Liu, Maosong Sun, Huanbo Luan, and Heng Yu. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P16-1097" title="">Agreement-based
learning of parallel lexicons and phrases from non-parallel corpora</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1024â€“1033,
Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2019a)</span>
<span class="ltx_bibblock">
Yang Liu, Kun Wang, Chengqing Zong, and Keh-Yih Su. 2019a.

</span>
<span class="ltx_bibblock">A unified framework and models for integrating translation memory
into phrase-based statistical machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Computer Speech &amp; Language</em>, 54:176â€“206.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2019b)</span>
<span class="ltx_bibblock">
Yang Liu, Kun Wang, Chengqing Zong, and Keh-Yih Su. 2019b.

</span>
<span class="ltx_bibblock">A unified framework and models for integrating translation memory
into phrase-based statistical machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Comput. Speech Lang.</em>, 54:176â€“206.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan
Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00343" title="">Multilingual denoising
pre-training for neural machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Transactions of the Association for Computational Linguistics</em>,
8:726â€“742.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma etÂ al. (2011)</span>
<span class="ltx_bibblock">
Yanjun Ma, Yifan He, Andy Way, and Josef van Genabith. 2011.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/P11-1124" title="">Consistent translation
using discriminative learning - a translation memory-inspired approach</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Technologies</em>, pages
1239â€“1248, Portland, Oregon, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manning etÂ al. (2014)</span>
<span class="ltx_bibblock">
Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard,
and David McClosky. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/v1/P14-5010" title="">The Stanford
CoreNLP natural language processing toolkit</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of 52nd Annual Meeting of the Association for
Computational Linguistics: System Demonstrations</em>, pages 55â€“60, Baltimore,
Maryland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ott etÂ al. (2019)</span>
<span class="ltx_bibblock">
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
David Grangier, and Michael Auli. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-4009" title="">fairseq: A fast,
extensible toolkit for sequence modeling</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics (Demonstrations)</em>,
pages 48â€“53, Minneapolis, Minnesota. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pham etÂ al. (2020)</span>
<span class="ltx_bibblock">
MinhÂ Quang Pham, Jitao Xu, Josep-Maria Crego, Jean Senellart, and
FranÃ§ois Yvon. 2020.

</span>
<span class="ltx_bibblock">Priming neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Conference on Machine Translation</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6319" title="">A call for clarity in
reporting BLEU scores</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the Third Conference on Machine Translation:
Research Papers</em>, pages 186â€“191, Brussels, Belgium. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post and Vilar (2018)</span>
<span class="ltx_bibblock">
Matt Post and David Vilar. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N18-1119" title="">Fast lexically
constrained decoding with dynamic beam allocation for neural machine
translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers)</em>, pages 1314â€“1324, New Orleans,
Louisiana. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich etÂ al. (2016)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P16-1162" title="">Neural machine
translation of rare words with subword units</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1715â€“1725,
Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shang etÂ al. (2021)</span>
<span class="ltx_bibblock">
Wei Shang, Chong Feng, Tianfu Zhang, and DaÂ Xu. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/IJCNN52387.2021.9533734" title="">Guiding
neural machine translation with retrieved translation template</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">2021 International Joint Conference on Neural Networks
(IJCNN)</em>, pages 1â€“7.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song etÂ al. (2019)</span>
<span class="ltx_bibblock">
Kai Song, Yue Zhang, Heng Yu, Weihua Luo, Kun Wang, and Min Zhang. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1044" title="">Code-switching for
enhancing NMT with pre-specified translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 449â€“459, Minneapolis,
Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Susanto etÂ al. (2020)</span>
<span class="ltx_bibblock">
RaymondÂ Hendy Susanto, Shamil Chollampatt, and Liling Tan. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.325" title="">Lexically
constrained neural machine translation with Levenshtein transformer</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 3536â€“3543, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al. (2016)</span>
<span class="ltx_bibblock">
Yaohua Tang, Fandong Meng, Zhengdong Lu, Hang Li, and P.Â Yu. 2016.

</span>
<span class="ltx_bibblock">Neural machine translation with external phrase memory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">ArXiv</em>, abs/1606.01792.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian etÂ al. (2014)</span>
<span class="ltx_bibblock">
Liang Tian, DerekÂ F. Wong, LidiaÂ S. Chao, Paulo Quaresma, Francisco Oliveira,
YiÂ Lu, Shuo Li, Yiming Wang, and Longyue Wang. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/774_Paper.pdf" title="">UM-corpus: A large English-Chinese parallel corpus for statistical
machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the Ninth International Conference on
Language Resources and Evaluation (LRECâ€™14)</em>, pages 1837â€“1842, Reykjavik,
Iceland. European Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
AidanÂ N. Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 31st International Conference on Neural
Information Processing Systems</em>, NIPSâ€™17, page 6000â€“6010, Red Hook, NY,
USA. Curran Associates Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2013)</span>
<span class="ltx_bibblock">
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2013.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/P13-1002" title="">Integrating translation
memory into phrase-based machine translation during decoding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 11â€“21, Sofia,
Bulgaria. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Shuo Wang, Peng Li, Zhixing Tan, Zhaopeng Tu, Maosong Sun, and Yang Liu.
2022a.

</span>
<span class="ltx_bibblock">A template-based method for constrained neural machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">ArXiv</em>, abs/2205.11255.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Shuo Wang, Zhixing Tan, and Yang Liu. 2022b.

</span>
<span class="ltx_bibblock">Integrating vectorized lexical constraints for neural machine
translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2203.12210</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia etÂ al. (2019)</span>
<span class="ltx_bibblock">
Mengzhou Xia, Guoping Huang, Lemao Liu, and Shuming Shi. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1609/aaai.v33i01.33017297" title="">Graph based
translation memory for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the Thirty-Third AAAI Conference on
Artificial Intelligence and Thirty-First Innovative Applications of
Artificial Intelligence Conference and Ninth AAAI Symposium on Educational
Advances in Artificial Intelligence</em>, AAAIâ€™19/IAAIâ€™19/EAAIâ€™19. AAAI Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Jitao Xu, Josep Crego, and Jean Senellart. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.144" title="">Boosting
neural machine translation with similar translations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 1580â€“1590, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yamada (2011)</span>
<span class="ltx_bibblock">
Masaru Yamada. 2011.

</span>
<span class="ltx_bibblock">The effect of translation memory databases on productivity.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2020)</span>
<span class="ltx_bibblock">
Jian Yang, Shuming Ma, Dongdong Zhang, Zhoujun Li, and Ming Zhou. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.531" title="">Improving
neural machine translation with soft template prediction</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 5979â€“5989, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2018a)</span>
<span class="ltx_bibblock">
Jiacheng Zhang, Yang Liu, Huanbo Luan, Jingfang Xu, and Maosong Sun.
2018a.

</span>
<span class="ltx_bibblock">Prior knowledge integration for neural machine translation using
posterior regularization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:1811.01100</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2021)</span>
<span class="ltx_bibblock">
Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang Xu, and Yang
Liu. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TASLP.2021.3057831" title="">Neural machine
translation with explicit phrase alignment</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">IEEE/ACM Transactions on Audio, Speech, and Language
Processing</em>, 29:1001â€“1010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2018b)</span>
<span class="ltx_bibblock">
Jingyi Zhang, Masao Utiyama, Eiichro Sumita, Graham Neubig, and Satoshi
Nakamura. 2018b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N18-1120" title="">Guiding neural machine
translation with retrieved translation pieces</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers)</em>, pages 1325â€“1335, New Orleans,
Louisiana. Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Dec  8 02:46:35 2023 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
