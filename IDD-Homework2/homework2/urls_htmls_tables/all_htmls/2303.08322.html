<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.08322] Optimization Design for Federated Learning in Heterogeneous 6G Networks</title><meta property="og:description" content="With the rapid advancement of 5G networks, billions of smart Internet of Things (IoT) devices along with an enormous amount of data are generated at the network edge.
While still at an early age, it is expected that th…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Optimization Design for Federated Learning in Heterogeneous 6G Networks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Optimization Design for Federated Learning in Heterogeneous 6G Networks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.08322">

<!--Generated on Thu Feb 29 20:07:52 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Optimization Design for Federated Learning in Heterogeneous 6G Networks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bing Luo, Xiaomin Ouyang, Peng Sun, Pengchao Han, Ningning Ding, and Jianwei Huang
</span><span class="ltx_author_notes">
This work is supported by the National Natural Science Foundation of China (Project 62271434 and 62102337), Shenzhen Science and Technology Program (Project JCYJ20210324120011032), Guangdong Basic and Applied Basic Research Foundation (Project 2021B1515120008), Shenzhen Key Lab of Crowd Intelligence Empowered Low-Carbon Energy Network (No. ZDSYS20220606100601002), and the Shenzhen Institute of Artificial Intelligence and Robotics for Society. (Corresponding author: Jianwei Huang)
Bing Luo is with the Data Science Research Center and the Division of Natural and Applied Sciences, Duke Kunshan
University, Kunshan, Jiangsu, China. (e-mail: bing.luo@dukekunshan.edu.cn)
Xiaomin Ouyang is with the department of Information Engineering, The Chinese University of Hong Kong, Hong Kong SAR, China (email:xmouyang@link.cuhk.edu.hk)
Peng Sun is with the College of Computer Science and Electronic Engineering, Hunan University, Changsha 410082, China. (email: psun@hnu.edu.cn)
Pengchao Han and Jianwei Huang are with the School of Science and Engineering, The Chinese University of Hong Kong,
Shenzhen, China, and the Shenzhen Institute of Artificial Intelligence and
Robotics for Society, Shenzhen, China. (emails: hanpengchao@cuhk.edu.cn; jianweihuang@cuhk.edu.cn)
Ningning Ding is with the Department of Electrical and Computer Engineering, Northwestern University, Evanston, IL 60208 USA (email: ningning.ding@northwestern.edu). </span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">With the rapid advancement of 5G networks, billions of smart Internet of Things (IoT) devices along with an enormous amount of data are generated at the network edge.
While still at an early age, it is expected that the evolving 6G network will adopt advanced artificial intelligence (AI) technologies to collect, transmit, and learn this valuable data for innovative applications and intelligent services.
However, traditional machine learning (ML) approaches require centralizing the training data in the data center or cloud, raising serious user-privacy concerns. Federated learning, as an emerging distributed AI paradigm with privacy-preserving nature, is anticipated to be a key enabler for achieving ubiquitous
AI in 6G networks.
However, there are several system and statistical heterogeneity challenges for effective and efficient FL implementation in 6G networks.
In this article, we investigate the optimization approaches that can effectively address the challenging heterogeneity issues from three aspects: incentive mechanism design,
network resource management, and personalized model
optimization. We also present some open problems and promising directions for future research.
</p>
</div>
<section id="S1" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With the rapid applications of the Internet of Things (IoT), autonomous driving, industry 4.0, and metaverse, a massive volume of data is expected to generate at the network edge. The valuable data has great potential to power intelligent applications for our daily lives. While still in its infancy, it is generally believed that the Sixth Generation (6G) systems will be established on ubiquitous artificial intelligence (AI) technologies, to enable such data-driven machine learning (ML) applications and services <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
However, traditional ML techniques normally collect the training data in a centralized data center, which raises severe privacy concerns (e.g., risk of data misuse and leakage of data owners) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To address the above challenge, federated learning (FL) has emerged as an attractive distributed learning paradigm (shown in Fig. 1). It enables network edge entities (clients) to collaboratively train a shared model under the coordination of a central server, while keeping the raw training data private <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. In FL, each client exploits its local dataset to compute a local model update, and the server periodically aggregates these
local model updates to obtain a global model. FL has demonstrated its success in many mobile applications (e.g., Goggle’s Gboard and Apple’s Siri), which makes it a high-potential enabler for AI-empowered 6G technology.
</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2303.08322/assets/figure/1.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="330" height="222" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">An illustration of FL in heterogeneous 6G networks.</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, the implementation of FL will face severe heterogeneity challenges <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> in 6G networks. This is because unlike 5G networks that aim to improve the network performance (e.g., peak data rate and service coverage), 6G networks will be able to tailor customized services to guarantee everyone’s quality of experience (QoE).
To achieve this goal, the 6G betwork AI architecture needs to utilize data from every user’s device, and integrate heterogeneous network resources and ubiquitous intelligence from the cloud to the edge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Although such a cloud-edge 6G Network AI Architecture can natively incorporate FL to support user-centric AI, the individual customized and multi-dimensional service requirements will bring critical heterogeneity challenges.
For example, massive participating users will be with highly diverse system resources (e.g., computation, communication, and storage). This can cause diverse on-device local model training latency when deploying FL, which negatively affects the application for delay-sensitive services in 6G networks, such as interactive VR/AR games. Moreover, due to highly heterogeneous data, the demand for personalized FL model optimization will be even higher in 6G networks in order to guarantee individual’s quality of experience.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The above characteristics and challenges will lead to effectiveness and efficiency issues for 6G FL optimization design, which can be concluded into three aspects that we proceed to discuss in this article: (i) incentive mechanism design,
(ii) network resource management, and (iii) personalized model optimization.
</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p"><em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">Incentive Mechanism Design</em>. To facilitate the optimization design of FL in heterogeneous 6G networks, we first need to design proper incentive mechanisms to stimulate sufficient client participation and contribution. Specifically, on one hand, clients participating in FL tasks incur various system costs. For example, clients sustain computation costs when computing local model updates using local CPU/GPU resources, and have communication costs when uploading locally updated model parameters or intermediate gradients. On the other hand, clients involved in FL are still susceptible to privacy threats. For example, adversaries or an honest but curious central server can infer data owners’ private information from their shared intermediate gradients or model parameters. Therefore, self-interested clients may be reluctant to participate in FL without sufficient economic compensation, which necessitates a well-designed incentive mechanism.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><em id="S1.p6.1.1" class="ltx_emph ltx_font_italic">Network Resource Management</em>. In FL, since iterative local model computation and information communications between clients and the central server can be both time and energy-consuming, it is necessary and important to analyze the incurred cost for resource-constrained edge clients. In particular, the number of participating clients is comparably large while the accessed wireless system bandwidth is limited in 6G networks. In this case, clients may suffer from a high transmission latency, which results in an unsatisfactory user quality of experience (QoE) for delay-sensitive applications. Therefore, proper network resource management is crucial in achieving cost-effective FL in resource constrained 6G networks. </p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p"><em id="S1.p7.1.1" class="ltx_emph ltx_font_italic">Personalized Model Optimization</em>.
In a canonical FL framework (e.g., FedAvg), a central server aggregates model weights from all clients iteratively until converging to a global model. However, in real-world applications under 6G networks (e.g., smart city), the data of different clients is usually highly heterogeneous due to issues such as different user habits and physical environments. In this case, such a single model learning paradigm often suffers poor accuracy performance on the data of clients that have non-IID distributions. Moreover, there is an increasing need to improve the model accuracy on a specific user/client in federated learning. Therefore, designing a <em id="S1.p7.1.2" class="ltx_emph ltx_font_italic">personalized FL paradigm</em> that can customize different models for clients with heterogeneous data during federated learning is of great significance.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">In this article, we first outline the main research challenges in the above three design aspects. Then, we propose several optimization approaches and algorithms for efficient FL deployment in 6G networks for each aspect.
Experimental results through simulated environments and hardware prototypes are also provided to validate the effectiveness of some typical research works. Finally,
we identify several future research directions along with key open problems to inspire future FL research in heterogeneous 6G networks.
</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2303.08322/assets/x1.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="236" height="177" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.3.2" class="ltx_text" style="font-size:90%;">Server’s cost of the proposed incentive mechanism on the CIFAR-10 dataset for incomplete information.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Incentive Mechanism Design for FL</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The mechanism design for incentivizing clients’ participation in FL in heterogeneous 6G networks involves several challenges. First, edge clients usually have multi-dimensional private information such as transmission delay and training costs, which are generally <em id="S2.p1.1.1" class="ltx_emph ltx_font_italic">unknown</em> by the server. Thus, it is highly nontrivial for the server to selectively incentivize desirable users’ participation to enhance training efficiency and effectiveness. Second, it is challenging to simultaneously account for the privacy preservation and incentive design for clients while ensuring good FL model training performance. The reason is that there exists an intrinsic tradeoff between privacy and model performance, which is hard to analyze. Third, it is difficult to evaluate each client’s contribution in a fair and efficient manner, since the FL paradigm does not allow direct access to each client’s local data. In this section, we introduce some typical works that can well address the above challenges when designing incentive mechanisms for FL in heterogeneous 6G networks.</p>
</div>
<section id="S2.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Incentive with Multi-Dimensional Private Information</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Edge clients in heterogeneous 6G networks usually have multi-dimensional private information (e.g., heterogeneous training costs and communication delay). It is necessary for the server to design an incentive mechanism to stimulate clients’ participation, encourage honest behaviors, and enhance training efficiency in the presence of clients’ multi-dimensional private information. Ding <em id="S2.SS1.p1.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> presented an analytical study on the optimal incentive mechanism design for FL in such cases. They employed a multi-dimensional contract-theoretic approach, which summarizes clients’ multi-dimensional private information into a one-dimensional criterion that allows a complete order of clients. They further implemented the optimal contract design under three typical information scenarios (i.e., complete information scenario, weakly incomplete information scenario, and strongly incomplete information scenario), to reveal the impact of information asymmetry levels on server’s optimal strategy and minimum cost (consisting of expected accuracy loss of the global model and the total payment to users). As shown in Fig. <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ Optimization Design for Federated Learning in Heterogeneous 6G Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, they showed that the proposed optimal incentive mechanism ALC has a much better performance compared with state-of-the-art baselines designed for non-IID data (i.e., RMA, OUC, and SBG proposed in other literature). The maximum cost reduction of ALC compared with benchmarks RMA, OUC, and SBG can reach 33.62%, 36.17%, and 37.01%, respectively.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Incentive with Privacy Preservation</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">To jointly deal with clients’ privacy protection and incentive issues while ensuring satisfactory FL model training performance, researchers have developed several privacy-preserving incentives for FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. For example, Sun <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> incorporated differential privacy (DP) into FL to preserve clients’ privacy. Furthermore, considering that clients under DP protection (with moderate privacy budgets) still sustain a certain degree of potential privacy disclosure and incur some privacy costs, they designed a contract-theoretic personalized privacy-preserving incentive for FL, named Pain-FL.
The basic idea of Pain-FL is that the server customizes a contract item for each client, which specifies a kind of privacy-preserving level (PPL) measured by the privacy budget in DP and the corresponding payment. In each round of FL with DP, each client perturbs her calculated stochastic gradients with the specified PPL in her chosen contract item in exchange for the corresponding payment. They analytically derived a set of optimal contract items under both complete and incomplete information scenarios. They further empirically show that the designed incentive mechanism outperforms the uniform payment baselines in terms of the convergence error performance of the finally learned global model.
</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Efficient and Fair Contribution Measurement</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Clients usually make heterogeneous contributions to FL model training due to factors like different training data quantity and quality (e.g., the non-IID degree of local training data). Therefore, it is crucial to accurately measure each client’s contribution for fair reward allocation. Specifically, each client in FL should get corresponding rewards based on its contribution to the federation rather than the same reward, which promotes the sustainable operation of the federation. Compared to the existing contribution measurement methods that consume intensive computing resources and operate offline, Yan <em id="S2.SS3.p1.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> proposed a real-time contribution measurement method (FedCM) for clients in FL. FedCM defines the impact of each client, and comprehensively considers the current and previous rounds to obtain the contribution rate of each client with attention aggregation. Moreover, FedCM updates contribution aligned with FL, which enables it to implement in real-time. The authors conducted extensive experiments to evaluate FedCM, and the results show that it is more sensitive to data quantity and data quality under the premise of real-time than the state-of-the-art methods.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Network Resource Management for FL</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The resource cost for FL in 6G networks mainly occurs at edge clients’ iterative local model training for computing model updates and wireless communications for transmitting model parameters, which involves both learning time and energy consumption.
However, as the participating clients in 6G networks usually have different computational powers and wireless communications speeds, standard FL algorithm (e.g., FedAvg) may cause inefficient resource cost for achieving the required model performance, especially when the clients’ data are highly non-IID and unbalanced. In this section, we present some typical network resource management methods that can efficiently address the heterogeneity challenges in 6G networks for achieving cost-effective FL design.
</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2303.08322/assets/figure/hd_new.jpg" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="314" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.6.3.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.4.2" class="ltx_text" style="font-size:90%;">Heterogeneous wireless federated learning testbed with <math id="S3.F3.3.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S3.F3.3.1.m1.1b"><mn id="S3.F3.3.1.m1.1.1" xref="S3.F3.3.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.F3.3.1.m1.1c"><cn type="integer" id="S3.F3.3.1.m1.1.1.cmml" xref="S3.F3.3.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.3.1.m1.1d">20</annotation></semantics></math> CPU-based Raspberry Pis (version 4) and <math id="S3.F3.4.2.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.F3.4.2.m2.1b"><mn id="S3.F3.4.2.m2.1.1" xref="S3.F3.4.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.F3.4.2.m2.1c"><cn type="integer" id="S3.F3.4.2.m2.1.1.cmml" xref="S3.F3.4.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.4.2.m2.1d">10</annotation></semantics></math> GPU-based Jetson Nanos.</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Adaptive Parameter Control</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Considering limited communication bandwidth and large communication overhead, the de facto FedAvg algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> usually performs <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">multiple local iterations</em> in parallel
on <em id="S3.SS1.p1.1.2" class="ltx_emph ltx_font_italic">a fraction of randomly sampled clients</em>. These essential parameters play an important role in computation and communication resource consumption. In line with this, Luo <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> studied how to design adaptive FL in wireless networks that optimally chooses these essential control variables to minimize the total cost while ensuring the required model performance. The authors first analytically established the relationship between the total resource cost and the control variables with the convergence upper bound. Then, to efficiently solve the cost minimization problem, they developed a low-cost sampling-based algorithm to estimate the convergence-related unknown parameters. Different from most existing FL works based on computer simulations, they implement their algorithm in an actual hardware prototype with resource-constrained devices, as shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ III Network Resource Management for FL ‣ Optimization Design for Federated Learning in Heterogeneous 6G Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The developed on-device model training and real wireless communications testbed can effectively capture real heterogeneous system operation time in terms of computation and communication, which provided the design principles for FL algorithms in optimizing client sampling percentage and local iteration steps.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Importance-based Client Sampling</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Existing works on the convergence analysis of FL mainly focused on
sampling schemes that are uniformly at random or proportional
to the clients’ data sizes, which often suffer from slow error
convergence with respect to wall-clock time due
to high degrees of the system and statistical heterogeneity. To this end, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> proposed an adaptive
client sampling approach
that tackles the heterogeneity challenges to minimize the wall-clock convergence time. With an
unbiased model aggregation design, they obtained a tractable convergence upper bound for FL algorithms with arbitrary client sampling probabilities. This allows the authors to establish the analytical relationship
between the total learning time and client sampling
probabilities and formulate a non-convex training time
minimization problem. Their solution characterizes the impact of heterogeneous computation, communication, and data distribution on the optimal client sampling probability. They also conduct experiments on a hardware prototype to validate the effectiveness of their optimized client sampling algorithm. In particular, Fig. <a href="#S4.F4" title="Figure 4 ‣ IV Personalized Model Optimization for FL ‣ Optimization Design for Federated Learning in Heterogeneous 6G Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows that for the EMNIST dataset their proposed optimal client sampling spends at least 70% less time than other baseline sampling schemes for achieving the same target accuracy.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Adaptive Gradient Compression</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.4" class="ltx_p">Transmitting model parameters between clients and users in FL may lead to high communication overhead, especially for deep neural networks with millions of parameters in 6G heterogeneous networks with limited communication resources. Gradient sparsification can alleviate the communication burden of FL in 6G heterogeneous networks by only communicating a small subset of important elements of the model gradient, i.e., the top <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">k</annotation></semantics></math> gradients with higher absolute values. In this regard,
Han <span id="S3.SS3.p1.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> proposed an adaptive gradient sparsification approach for improving the efficiency of FL towards heterogeneous communication resources. First, to ensure all clients contribute equally to the global model update in each round of communication, the authors design a fairness-aware bidirectional top-<math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">k</annotation></semantics></math> gradient sparsification method. Then, for given communication resource availability, the authors formulate the overall training time minimization problem to automatically determine the optimal degree of gradient sparsity (i.e., <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">k</annotation></semantics></math>). Minimizing the overall training contributes to achieving the trade-off between computation and communication. Since the system can only reveal the training time after applying a deterministic <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mi id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><ci id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">k</annotation></semantics></math>, the authors propose an online learning approach to find the optimal degree of gradient sparsity using an estimated sign of the derivative of the objective function.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Personalized Model Optimization for FL</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Most FL approaches aim to learn a single model for all users,
which often suffers poor accuracy performance on heterogeneous user data in real-world applications under 6G networks. The goal of personalized model optimization in FL is to customize different models for clients with heterogeneous data to improve model accuracy. There are two major challenges in personalized FL. First, there is a trade-off between the generalization and personalization ability of the learned models during federated learning, which largely affects the model accuracy. Second, federated learning on the non-IID data of clients often exhibits many convergence issues due to the divergence of model updates, which will incur significant training delays and system overhead. In this section, we introduce several representative personalized FL approaches that can customize different models for heterogeneous clients in FL.
</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2303.08322/assets/x2.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="177" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Testing accuracy over the wall-clock time for the EMNIST dataset on the hardware prototype using the logistic regression model.</span></figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Clustering-based Multi-task Learning</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To train personalized models while enabling collaborative learning among similar nodes, Ouyang <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> proposed a clustering-based federated multi-task learning approach named ClusterFL.
The design of ClusterFL is motivated by the key observation that the data distributions of some clients share spatial-temporal similarity in a wide range of applications with 6G networks, which can be exploited to improve the model accuracy in FL. Specifically, ClusterFL features a novel clustered multi-task federated learning formulation by introducing a cluster indicator matrix indicating the similarity of users, which minimizes the empirical loss of learned models while automatically capturing the intrinsic cluster structure among different users.
In ClusterFL, the model weights and the cluster indicator matrix are alternatively optimized until convergence while keeping the data locality of nodes. The authors also provide theoretical analysis for achieving convergence with general non-convex and strongly convex local models.
Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-B Regularized Local Training ‣ IV Personalized Model Optimization for FL ‣ Optimization Design for Federated Learning in Heterogeneous 6G Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> compares the accuracy performance of ClusterFL with different learning paradigms when involving different numbers of nodes. In all configurations, ClusterFL outperforms the decentralized baselines, and its accuracy even exceeds centralized learning for 60 and 90 nodes. Moreover, ClusterFL has a significantly smaller variation of accuracy among nodes, which means that ClusterFL can improve model accuracy for most nodes.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Regularized Local Training</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Another type of personalized model optimization approach in FL is based on regularized local training. Specifically, regularization techniques are applied to limit the impact of local updates, which can provide more robust convergence performance and better-personalized models. For example, pFedMe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> adds a regularization term to the local loss function of clients, which measures the distance between the global model and the clients’ local model. Then the global model is averaged by all the local models at the central server. This approach helps decouple personalized model optimization from global model learning, thus achieving a good convergence performance. The results show that pFedMe can capture the statistical diversity of clients’ data and achieves a sublinear speedup of order 2/3 for smooth non-convex objectives.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2303.08322/assets/figure/clusterfl-accuracy.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="330" height="208" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Accuracy performance of ClusterFL with different numbers of nodes. The results are obtained by using a large-scale dataset that collected the activity data of 121 subjects and a new FL testbed with 10 Nvidia edge devices.</span></figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Global Model Post-training</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">There are also personalized FL approaches based on post-training of the global model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. The training process in these approaches includes two steps, federated averaging and local adaptation. Specifically, the clients will first collaboratively train a global model through federated averaging. Then the single global model learned by FL will be adapted to different clients based on their own data. The techniques for adapting the global model to clients include model fine-tuning and knowledge distillation. Therefore, the clients can individually improve the quality of their local models without re-designing the FL framework or involving other participants. However, there is a lack of theoretical analysis on how to achieve the balance of local personalization and learning from other clients for such a post-training personalized FL approach.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Future Challenges and Open Issues</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In addition to the proposed optimization methods and algorithms which address the heterogeneity issues, we further outline several future challenges and open problems in future 6G networks.</p>
</div>
<div id="S5.p2" class="ltx_para">
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p"><em id="S5.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">Incentive Mechanism for Randomized Participation.</em>
Most existing incentive mechanism for FL usually assumes that all clients participate in all training rounds (known as full client participation). This assumption is generally impractical
in 6G networks due to clients’ intermittent availability (e.g., unstable wireless communications or out of battery). Therefore, it is more meaningful to design a practical incentive mechanism for FL with partial client participation. However, the challenge for incentivizing partial clients is that the resulting model can be severely biased as the data on the incentivized clients
may not be representative of all clients’ data. In this case, the mechanism may fail to
converge to the optimal model that would be obtained if all
the clients participate in training. Therefore, how to design an unbiased and convergence-guaranteed incentive mechanism requires further investigation.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p"><em id="S5.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">Incentive Mechanism with Punishment Design.</em> Existing incentive mechanisms for FL mainly focus on how to reward clients based on their contributions. However, they have largely neglected the security issues in incentive design. Specifically, as an open and decentralized system, clients in FL can be easily compromised by external adversaries. These compromised clients can then launch Byzantine attacks via data or model poisoning to mislead the FL process and degrade the FL model performance. In such problem settings, existing incentive design will not only lead to a waste of money but also a deteriorated or even useless global model. Therefore, in order to create and maintain a benign and sustainable FL ecosystem, we may need to design incentive mechanisms that explicitly consider punishing malicious clients accordingly and induce them to behave faithfully. This is a fundamental and promising direction that is worth further investigation.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p"><em id="S5.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">FL with Time-varying Data Distribution</em>.
In 6G networks, the clients in FL will encounter dynamic wireless channel conditions due to mobility, which leads to unstable communication rates. This problem has been widely studied in wireless communications via flexible scheduling and resource allocation algorithms. However, in addition to system dynamics, the main dynamic challenge in FL also comes from data dynamics, where clients’ local training datasets <em id="S5.I1.i3.p1.1.2" class="ltx_emph ltx_font_italic">vary over time</em>, e.g., climate data in sensor nodes and trajectory data in autonomous cars. In the literature,
existing FL optimization design usually assumes a fixed data distribution among
clients throughout all training rounds, which may not hold in 6G networks. The difficulty in addressing the time-varying challenge lies in the unpredictable distribution of future data, which requires new effective and robust FL learning algorithms.</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.1" class="ltx_p"><em id="S5.I1.i4.p1.1.1" class="ltx_emph ltx_font_italic">Federated Knowledge Distillation</em>.
The 6G networks feature enormous number of intelligent users with customized service and QoS demands <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The devices are normmaly with different computation capacities with limited communication resources. Dealing with these users’ QoS requirements over heterogeneous infrastructures relies on a more flexible federated learning framework, including diverse model structures, different privacy protection levels, and higher model accuracy. It is possible to leverage Federated knowledge distillation (FedKD) to deal with the heterogeneous models of users and efficient communication desire, although FedKD still undergoes challenges for robustness and security design to meet the 6G version.
First, FedKD is rigid in treating all clients equally while ignoring the non-IID data of clients. It is important yet challenging to find an efficient way for knowledge transfer among clients with non-IID data, other than simply taking an average of client model outputs for knowledge distillation. Furthermore, sharing the model outputs of clients to the sever faces the risk of privacy leakage. FedKD requires an additional privacy protection mechanism, which has not been stressed in the literature.
</p>
</div>
</li>
<li id="S5.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i5.p1" class="ltx_para">
<p id="S5.I1.i5.p1.1" class="ltx_p"><em id="S5.I1.i5.p1.1.1" class="ltx_emph ltx_font_italic">Federated Multimodal Learning</em>. Most of the current studies in FL assume that there is only single-modality data on the clients. However, in many real-world applications under 6G networks such as human-computer interaction and autonomous diving, the local data on clients are usually generated from multiple modalities. Integrating information from different clients’ data in federated multimodal learning has several major challenges. First,
the local data of clients may come from multiple modalities or only a single modality due to resource limitations or sensor faults. Therefore, a scalable federated multimodal learning framework is needed to effectively aggregate the multimodal and unimodal models of clients. Second, it is more challenging to deal with the non-IID data distributions of clients in federated multimodal learning. The reason is that current personalized FL solutions are developed for unimodal settings where all clients train models with the same architecture, thus cannot be directly applied to federated multimodal learning. Therefore, it is important to design new algorithms that can improve the model accuracy of FL clients with both heterogeneous data distributions and data modalities.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S6" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this article, we have investigated
three key optimization design aspects to address the heterogeneity challenges for FL in 6G networks. For each design aspect, we outline the main challenges and present several optimization approaches
to optimize the effectiveness and efficiency of FL.
Both simulation and hardware prototype experiments are provided to demonstrate the effectiveness of our
proposed research works.
Finally, we identify several future research directions along with
key challenges to inspire the future FL research.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
H. Yang, A. Alphones, Z. Xiong, D. Niyato, J. Zhao, and K. Wu,
“Artificial-intelligence-enabled intelligent 6g networks,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE
Network</em>, vol. 34, no. 6, pp. 272–280, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Y. Sun, J. Liu, J. Wang, Y. Cao, and N. Kato, “When machine learning meets
privacy in 6g: A survey,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</em>,
vol. 22, no. 4, pp. 2694–2724, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence and
Statistics</em>, 2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith,
“Federated optimization in heterogeneous networks,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of
Machine Learning and Systems (MLSys)</em>, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Y. Yang, M. Ma, H. Wu, Q. Yu, P. Zhang, X. You, J. Wu, C. Peng, T.-S. P. Yum,
S. Shen <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “6g network ai architecture for everyone-centric
customized services,” <em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.09944</em>, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
N. Ding, Z. Fang, and J. Huang, “Optimal contract design for efficient
federated learning with multi-dimensional private information,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE
Journal on Selected Areas in Communications</em>, vol. 39, no. 1, pp. 186–200,
2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
P. Sun, X. Chen, G. Liao, and J. Huang, “A profit-maximizing model marketplace
with differentially private federated learning,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE INFOCOM
2022-IEEE Conference on Computer Communications</em>.   IEEE, 2022, pp. 1439–1448.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
P. Sun, H. Che, Z. Wang, Y. Wang, T. Wang, L. Wu, and H. Shao, “Pain-FL:
Personalized privacy-preserving incentive for federated learning,”
<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected Areas in Communications</em>, vol. 39, no. 12, pp.
3805–3820, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
B. Yan, B. Liu, L. Wang, Y. Zhou, Z. Liang, M. Liu, and C.-Z. Xu, “Fedcm: A
real-time contribution measurement method for participants in federated
learning,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2021 International Joint Conference on Neural Networks
(IJCNN)</em>.   IEEE, 2021, pp. 1–8.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
B. Luo, X. Li, S. Wang, J. Huang, and L. Tassiulas, “Cost-effective federated
learning in mobile edge networks,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected Areas in
Communications</em>, vol. 39, no. 12, pp. 3606–3621, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
B. Luo, W. Xiao, S. Wang, J. Huang, and L. Tassiulas, “Tackling system and
statistical heterogeneity for federated learning with adaptive client
sampling,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE INFOCOM 2022 - IEEE Conference on Computer
Communications</em>, 2022, pp. 1739–1748.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
P. Han, S. Wang, and K. K. Leung, “Adaptive gradient sparsification for
efficient federated learning: An online learning approach,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2020
IEEE 40th International Conference on Distributed Computing Systems (ICDCS)</em>,
2020, pp. 300–310.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
X. Ouyang, Z. Xie, J. Zhou, J. Huang, and G. Xing, “Clusterfl: a
similarity-aware federated learning system for human activity recognition,”
in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 19th Annual International Conference on Mobile
Systems, Applications, and Services</em>, 2021, pp. 54–66.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
C. T Dinh, N. Tran, and J. Nguyen, “Personalized federated learning with
moreau envelopes,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
vol. 33, pp. 21 394–21 405, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
T. Yu, E. Bagdasaryan, and V. Shmatikov, “Salvaging federated learning by
local adaptation,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.04758</em>, 2020.

</span>
</li>
</ul>
</section>
<figure id="tab1" class="ltx_float biography">
<table id="tab1.1" class="ltx_tabular">
<tr id="tab1.1.1" class="ltx_tr">
<td id="tab1.1.1.1" class="ltx_td">
<span id="tab1.1.1.1.1" class="ltx_inline-block">
<span id="tab1.1.1.1.1.1" class="ltx_p"><span id="tab1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Bing Luo</span> 
received the Ph.D. degree from The University of Melbourne, Australia. He is currently an Assistant Professor of Data and Computational Science at Duke Kunshan University, China. His research interests are federated learning and analytics, network optimization, game theory, and 5G/6G wireless communications and energy harvesting systems.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab2" class="ltx_float biography">
<table id="tab2.1" class="ltx_tabular">
<tr id="tab2.1.1" class="ltx_tr">
<td id="tab2.1.1.1" class="ltx_td">
<span id="tab2.1.1.1.1" class="ltx_inline-block">
<span id="tab2.1.1.1.1.1" class="ltx_p"><span id="tab2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Xiaomin Ouyang</span>  received the B.E. degree from Xiamen University, China, in 2019. She is currently pursuing the Ph.D. degree in the Department of Information Engineering, The Chinese University of Hong Kong. Her research interests include Artificial Intelligence for Internet of Things, Smart Health, and Mobile Computing.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab3" class="ltx_float biography">
<table id="tab3.1" class="ltx_tabular">
<tr id="tab3.1.1" class="ltx_tr">
<td id="tab3.1.1.1" class="ltx_td">
<span id="tab3.1.1.1.1" class="ltx_inline-block">
<span id="tab3.1.1.1.1.1" class="ltx_p"><span id="tab3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Peng Sun</span>  received his Ph.D degree in control science and engineering from Zhejiang University, China, in 2020. He is currently an Associate Professor with the College of Computer Science and Electronic Engineering, Hunan University, China. His research interests include Internet of Things, mobile crowdsensing, and federated learning.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab4" class="ltx_float biography">
<table id="tab4.1" class="ltx_tabular">
<tr id="tab4.1.1" class="ltx_tr">
<td id="tab4.1.1.1" class="ltx_td">
<span id="tab4.1.1.1.1" class="ltx_inline-block">
<span id="tab4.1.1.1.1.1" class="ltx_p"><span id="tab4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Pengchao Han</span> 
received the Ph.D. degree in communication and information systems at Northeastern University, China. She is currently a Postdoc research associate at The Chinese University of Hong Kong, Shenzhen, China. Her research interests include wireless and optical networks, mobile edge computing, federated learning, and knowledge distillation.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab5" class="ltx_float biography">
<table id="tab5.1" class="ltx_tabular">
<tr id="tab5.1.1" class="ltx_tr">
<td id="tab5.1.1.1" class="ltx_td">
<span id="tab5.1.1.1.1" class="ltx_inline-block">
<span id="tab5.1.1.1.1.1" class="ltx_p"><span id="tab5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Ningning Ding</span>  is a Post-Doctoral Fellow with the Department of Electrical and Computer Engineering, Northwestern University, USA. Her primary research interests are in the interdisciplinary area between network economics and machine learning, with current emphasis on pricing and incentive design for federated learning, distributed coded machine learning, and IoT systems.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab6" class="ltx_float biography">
<table id="tab6.1" class="ltx_tabular">
<tr id="tab6.1.1" class="ltx_tr">
<td id="tab6.1.1.1" class="ltx_td">
<span id="tab6.1.1.1.1" class="ltx_inline-block">
<span id="tab6.1.1.1.1.1" class="ltx_p"><span id="tab6.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Jianwei Huang</span>  is a Presidential Chair Professor and Associate Vice President at the Chinese University of Hong Kong, Shenzhen. He has won multiple Best Paper Awards, including the 2011 IEEE Marconi Prize Paper Award. He is an IEEE Fellow and a Clarivate Web of Science Highly Cited Researcher, and currently serves as Editor-in-Chief of IEEE Transactions on Network Science and Engineering.</span>
</span>
</td>
</tr>
</table>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.08321" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.08322" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2303.08322">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.08322" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.08323" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 20:07:52 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
