<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2112.02829] SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection</title><meta property="og:description" content="With the emergence of deep learning in the last years, new opportunities arose in Earth observation research. Nevertheless, they also brought with them new challenges. The data-hungry training processes of deep learnin…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2112.02829">

<!--Generated on Fri Mar  1 15:53:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">(cvpr)                Package cvpr Warning: Single column document - CVPR requires papers to have two-column layout. Please load document class ‘article’ with ‘twocolumn’ option

(cvpr)                Package cvpr Warning: Incorrect paper size - CVPR uses paper size ‘letter’. Please load document class ‘article’ with ‘letterpaper’ option</p>
</div>
<h1 class="ltx_title ltx_title_document">SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Preprint, submitted 23. Aug. 2021
<br class="ltx_break">
<br class="ltx_break">Thorsten Hoeser<sup id="id6.6.id1" class="ltx_sup"><span id="id6.6.id1.1" class="ltx_text ltx_font_italic">1,∗</span></sup>  Claudia Kuenzer<sup id="id7.7.id2" class="ltx_sup"><span id="id7.7.id2.1" class="ltx_text ltx_font_italic">1,2</span></sup> 
<br class="ltx_break">
<br class="ltx_break"><sup id="id8.8.id3" class="ltx_sup">1</sup>German Remote Sensing Data Center (DFD), German Aerospace Center (DLR) 
<br class="ltx_break"><sup id="id9.9.id4" class="ltx_sup">2</sup>Department of Remote Sensing, Institute of Geography and Geology, University of Wuerzburg
<br class="ltx_break">
<br class="ltx_break"><sup id="id10.10.id5" class="ltx_sup"><span id="id10.10.id5.1" class="ltx_text ltx_font_italic" style="font-size:90%;">∗</span></sup><span id="id11.11.id6" class="ltx_text" style="font-size:90%;">Corresponding author, <span id="id11.11.id6.1" class="ltx_text ltx_font_typewriter">thorsten.hoeser@dlr.de</span>
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id12.id1" class="ltx_p">With the emergence of deep learning in the last years, new opportunities arose in Earth observation research. Nevertheless, they also brought with them new challenges. The data-hungry training processes of deep learning models demand large, resource expensive, annotated data sets and partly replaced knowledge-driven approaches so that model behaviour and the final prediction process became a black box. The proposed SyntEO approach enables Earth observation researchers to automatically generate large deep learning ready data sets by merging existing and procedural data. SyntEO does this by including expert knowledge in the data generation process in a highly structured manner to control the automatic image and label generation by employing an ontology. In this way, fully controllable experiment environments are set up, which support insights in the model training on the synthetic data sets. Thus, SyntEO makes the learning process approachable, which is an important cornerstone for explainable machine learning. We demonstrate the SyntEO approach by predicting offshore wind farms in Sentinel-1 images on two of the worlds largest offshore wind energy production sites. The largest generated data set has 90,000 training examples. A basic convolutional neural network for object detection, that is only trained on this synthetic data, confidently detects offshore wind farms by minimising false detections in challenging environments. In addition, four sequential data sets are generated, demonstrating how the SyntEO approach can precisely define the data set structure and influence the training process. SyntEO is thus a hybrid approach that creates an interface between expert knowledge and data-driven image analysis.</p>
</div>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Over the last decades, Earth observation faced a massive increase in data availability. Today, petabytes of public and commercial remote sensing archives are accessible, which provide data on a global scale with increasing variance in modality. In order to harness this steadily growing amount of data, machine learning became a fundamental toolset in Earth observation. Lately, with the emergence of deep learning and especially its development in computer vision for image analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, it became possible to extract complex spatio-temporal features from large amounts of remote sensing data. As a consequence, deep learning eventually established itself as an essential tool in Earth observation and gave new perspectives to geoscientific research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The successful implementation of deep learning at a time of increasing data availability, not just in Earth observation, is no coincidence but rather a self-reinforcing effect. Deep learning needs huge and precisely labelled training data in order to learn the underlying complex features. At the same time, a deep learning model is highly efficient in processing large amounts of unseen data once trained. Hence, deep learning is both an approach that is capable of processing large data archives as well as an approach that highly depends on large training data sets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The latter brings up a major drawback of deep learning: The creation of large, precisely labelled training data sets is a resource-expensive task. Multiple approaches have been developed to cope with the problem that deep learning demands large annotated training data sets. Few-shot learning is a meta-learning approach that trains a model to perform a task, such as a general separation of classes instead of predicting a specific class. That way, a large training data set can be used to train a task which can then be applied to a small support set of target data which alone would be too small to optimise a deep learning model. However, an initial large training data set, prior knowledge or a combination of both is needed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. In weakly supervised learning, large data sets are used, which have limited labels like inaccurate labels or labels with high-level information compared to the task at hand. That way, annotating data is less time consuming, and more data can be acquired at the same time or even be automatically generated. During the training process, limited labels are refined iteratively to receive sufficiently labelled training examples. That way, tasks can be learned on limited labels, which from a <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">strong</span> supervised perspective would not be sufficient <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. In self-supervised learning, models start learning representations in a data set without using a single label. Instead, they produce their own supervision signal during training. To do so, so-called pretext-tasks have to be defined, on which the model can generate its own supervision signal. For example, such a task can be to remove noise from an automatically generated noisy image, where the image is the raw training data and the noise generated on the fly and therewith known to the model. By solving this pretext-task, the model learns representations within the data set. After that, the model can be trained on the target task, like object detection with a smaller amount of labelled data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">All of these approaches have one thing in common. At some point, they need a large amount of unlabelled or imperfect labelled images, which can become a problem in the Earth observation domain. Depending on the sensor and target of interest, even today, the number of usable images can quickly get too small to optimise a deep learning model. Therefore, we focus on synthetic training data in this study, where images and labels are generated automatically. Thus, the raw data limitation and annotation problems are solved simultaneously, and the commonly employed supervised learning approach can still be used without adapting well-established training schemes.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The successful application of synthetic generated data sets in other domains like computer vision, bioinformatics, or robotics demonstrate the potential of this approach in combination with deep learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. However, in Earth observation, the number of studies that incorporate synthetic data into their deep learning workflows are vanishingly small compared to the surge of publications that apply deep learning methods in the last years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. In addition to generating large training data sets, synthetic data opens up new opportunities towards a sophisticated human-machine interaction that supports an understanding of the model’s reasoning. The latter is part of the recently growing eXplainable AI (XAI) field, intending to open the black box of artificial agents to make models interpretable <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In order to promote the usage of synthetic data sets in Earth observation, we introduce SyntEO, an approach for synthetic data set generation with a particular focus on data from the Earth observation domain. Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarises how a deep learning data set is commonly created and compares it to a process that applies SyntEO. In common data set creation, a domain expert visually examines a limited set of sensor acquisitions and then annotates the images by hand. Each image-annotation pair is then stored as a training example, which results in a limited data set with as many training examples as acquisitions are available. The sole annotation by a domain expert occupies the most resources but will result in a relatively high annotation accuracy. To redistribute the workload and free resources, the domain expert can formulate an annotation guideline to instruct domain amateurs if the complexity of data annotated and topic allows this knowledge transfer. The resulting data set annotated by the domain amateurs will be technically the same, a limited amount of image-annotation pairs. However, annotation accuracy might be lower with a wider variance than the same data set only labelled by a single domain expert.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">On the right-hand side of figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> a synthetic data set generation process that uses the SyntEO approach is depicted. A domain expert has to explicitly formulate machine-readable numeric knowledge, which describes how the domain expert perceives characteristics from single entities and their interrelations in the data. This is done by formulating an ontology (see section <a href="#S3.SS1" title="3.1 Ontology Formulation ‣ 3 Introduction to SyntEO ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>) that provides dimensions of numeric knowledge for the observed characteristics. Upon this ontology, an artificial data set generator can interpret the knowledge representation and compose a synthetic scene of all entities described by the expert. A synthetic image is generated by adding texture to the composed scene, whereas the synthetic scene composition can be used to derive different types of annotations that will always be accurate without any variation. Nevertheless, the quality of the represented task and remote sensing scene depends on the expert knowledge in the ontology. Since the artificial generator relies on a complex description of potential characteristics and randomised value selection, the number of possible training examples is to be assumed infinite.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">Therewith, SyntEO has two major goals:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">To enable domain experts to create large deep learning ready data sets specifically designed for Earth observation research and remote sensing data by automatic image and annotation generation.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">To provide an interface that can be used for human-machine interaction, where an initial change in parameters during the data generation process triggers different behaviours in the machine learning model, which learns from the generated data. That way, an interactive loop is established where the user and model react to each other’s outputs.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2112.02829/assets/figures/common_synteo_150.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="608" height="719" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Comparison of two data set creation processes. Left: The common way of manual annotation which creates fixed and limited data sets. Right: The proposed approach of synthetic data set generation with fully automatic annotation and dynamic data sets in size and content.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">In this introduction of the approach, first, the theoretical background is presented by discussing the general motivation and core concepts of SyntEO. Following that, the first application of SyntEO for offshore wind farm detection in Sentinel-1 imagery demonstrates a hands-on example for the efficient creation of a large data set to solve a real-world problem. This example gives an intuition on how SyntEO provides an experimental environment to get insights into the learning process and model behaviour and how to adapt the data set creation process to accomplish accurate, spatially transferable offshore wind farm detection by minimised false detections.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Research</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Synthetic Data sets in Earth Observation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Especially in computer vision, synthetic data sets are an established tool to train neural networks for tasks like predicting optical flow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> or image segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. To create these data sets, different strategies have been used, like adding virtual models of objects in front of random backgrounds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> up to generating entire 3D environments and using virtual cameras to take pictures of them <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Thus, synthetic training data can be seen as a recomposition of already existing template data as well as fully synthetically generated data.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">In Earth observation, the number of studies that combine synthetic data sets and deep learning is a rarity despite its successful implementation in other research domains. Of those studies which combine a synthetic data set and deep learning approach, RGB images of very high resolution with less than 1 m are the majority. Their applications are aircraft or vehicle detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> and building footprint detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Two more synthetic data sets focus on speckle noise reduction in radar imagery <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> and one publication uses fully synthetic training data for river network extraction from single band MNDWI images of Landsat 8 acquisitions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">For data set generation Isikdogan et al.<span id="S2.SS1.p3.1.1" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and Kong et al.<span id="S2.SS1.p3.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> used fully synthetic approaches. Especially Kong et al.<span id="S2.SS1.p3.1.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> preserved spatial context accurately by generating complete 3D environments in which objects are located not randomly but in spatial relationships similar to real-world environments. In contrast, Weber et al.<span id="S2.SS1.p3.1.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> demonstrated that 2D models of vehicles that were placed in front of random backgrounds and therewith dismissing spatial context are still detectable in real-world overhead images with a very high spatial resolution of 10 cm. However, since such feature richness of the object of interest is not guaranteed across application domains and especially spaceborne sensor spatial resolution, we argue that spatio-temporal context of target objects and their environment is a major characteristic of Earth observation data and has to be an important aspect in synthetic data generation.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">One common feature shared by all the studies mentioned is the training of the convolutional neural network (CNN) deep learning model with the synthetically generated data. Since CNNs are the most widely used deep learning models in Earth observation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> and also used in this study, we briefly outline the development and characteristics of CNNs.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Convolutional Neural Networks in Earth Observation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.5" class="ltx_p">When in 2012 CNNs became popular with the introduction of AlexNet, stacked convolutional layers with trained kernel functions were used to predict single labels for entire images on relatively small inputs with a dimension of <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mrow id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mn id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p1.1.m1.1.1.1" xref="S2.SS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><times id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">224</cn><cn type="integer" id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">224\times 224</annotation></semantics></math> pixels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. The so-called patch-based approach used the same general architecture designs with even smaller input sizes for image segmentation or object detection. In order to find objects or to segment an image, the image was split into patches of <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="8\times 8" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><mrow id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><mn id="S2.SS2.p1.2.m2.1.1.2" xref="S2.SS2.p1.2.m2.1.1.2.cmml">8</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p1.2.m2.1.1.1" xref="S2.SS2.p1.2.m2.1.1.1.cmml">×</mo><mn id="S2.SS2.p1.2.m2.1.1.3" xref="S2.SS2.p1.2.m2.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><times id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S2.SS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2">8</cn><cn type="integer" id="S2.SS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">8\times 8</annotation></semantics></math> to <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="16\times 16" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><mrow id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml"><mn id="S2.SS2.p1.3.m3.1.1.2" xref="S2.SS2.p1.3.m3.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p1.3.m3.1.1.1" xref="S2.SS2.p1.3.m3.1.1.1.cmml">×</mo><mn id="S2.SS2.p1.3.m3.1.1.3" xref="S2.SS2.p1.3.m3.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><apply id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1"><times id="S2.SS2.p1.3.m3.1.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1.1"></times><cn type="integer" id="S2.SS2.p1.3.m3.1.1.2.cmml" xref="S2.SS2.p1.3.m3.1.1.2">16</cn><cn type="integer" id="S2.SS2.p1.3.m3.1.1.3.cmml" xref="S2.SS2.p1.3.m3.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">16\times 16</annotation></semantics></math> pixels. For each patch, a prediction was made if the patch or patch centre pixel shows an object or belongs to a semantic class. The aggregation of all predictions on each patch was then the result for a given image. It is important to mention that at this early stage in the evolution of CNNs, the small spatial range of <math id="S2.SS2.p1.4.m4.1" class="ltx_Math" alttext="8\times 8" display="inline"><semantics id="S2.SS2.p1.4.m4.1a"><mrow id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml"><mn id="S2.SS2.p1.4.m4.1.1.2" xref="S2.SS2.p1.4.m4.1.1.2.cmml">8</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p1.4.m4.1.1.1" xref="S2.SS2.p1.4.m4.1.1.1.cmml">×</mo><mn id="S2.SS2.p1.4.m4.1.1.3" xref="S2.SS2.p1.4.m4.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><apply id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1"><times id="S2.SS2.p1.4.m4.1.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1.1"></times><cn type="integer" id="S2.SS2.p1.4.m4.1.1.2.cmml" xref="S2.SS2.p1.4.m4.1.1.2">8</cn><cn type="integer" id="S2.SS2.p1.4.m4.1.1.3.cmml" xref="S2.SS2.p1.4.m4.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">8\times 8</annotation></semantics></math> to <math id="S2.SS2.p1.5.m5.1" class="ltx_Math" alttext="16\times 16" display="inline"><semantics id="S2.SS2.p1.5.m5.1a"><mrow id="S2.SS2.p1.5.m5.1.1" xref="S2.SS2.p1.5.m5.1.1.cmml"><mn id="S2.SS2.p1.5.m5.1.1.2" xref="S2.SS2.p1.5.m5.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p1.5.m5.1.1.1" xref="S2.SS2.p1.5.m5.1.1.1.cmml">×</mo><mn id="S2.SS2.p1.5.m5.1.1.3" xref="S2.SS2.p1.5.m5.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.1b"><apply id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1"><times id="S2.SS2.p1.5.m5.1.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1.1"></times><cn type="integer" id="S2.SS2.p1.5.m5.1.1.2.cmml" xref="S2.SS2.p1.5.m5.1.1.2">16</cn><cn type="integer" id="S2.SS2.p1.5.m5.1.1.3.cmml" xref="S2.SS2.p1.5.m5.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.1c">16\times 16</annotation></semantics></math> pixels can only extract limited spatial features. In an Earth observation context, this limitation was interpreted to be close to a pixel-based approach because single pixel information still dominates features generated from such spatially small patches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. However, convolutional feature extractors have developed fast and at least since the introduction of the Fully Convolutional Network (FCN) for image segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> or Region based-CNN (R-CNN), especially Faster R-CNN for object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, new end to end trainable CNN architectures with considerable larger input sizes became available. With the development of graphics processing units (GPUs) with increased memory, the input size and the capability of CNNs to learn large-scale spatial features from the training data became more sophisticated. An important reaction to this development was the creation of new building blocks for CNN architectures. These new architectures specifically search for spatial features on multiple scales in order to combine small and large scale features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Overall, object detection and image segmentation with CNNs changed significantly over the last decade and are today capable of extracting features from large input sizes on multiple scales by taking spatial context into account. This development is especially important for the successful application in Earth observation since fine-grained features on multiple scales are essential characteristics of remote sensing data. For an in-depth review of the evolution and application of CNNs in the context of Earth observation, we refer to Hoeser and Kuenzer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and Hoeser et al.<span id="S2.SS2.p2.1.1" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Human-Machine Interaction</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">With the rise of artificial intelligence in the last decades, humans have gotten used to interacting with machines daily. Smartphones guide our cameras by recognising faces and providing optimal settings that the user can see and select. Digital assistants like Alexa and Siri understand questions and commands due to their speech recognition modules and perform tasks like turning lights on or buying a product. Humans are getting used to these intelligent agents and, to some extent, trust that they will perform the task for which they were designed without knowing how they were designed.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">However, in science, it is of great importance to interact with models and understand how they make their decisions. When a user applies a decision tree based on expert knowledge to analyse a remote sensing scene, the outcome can be understood in its entirety since the model is fully interpretable. Instead, deep learning models like the CNN are categorised as black-box models due to the vast amount of parameters. A user who applies such a model typically accepts the output of the model without knowing how the model came up with the prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Thus, using a standard CNN, there is limited interaction between the user and the artificial agent here a machine learning model. The user requests a prediction, which the artificial agent delivers, but the user can not request an understandable explanation.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Metrics that communicate the model performance are a first step to building confidence in the prediction. However, they do not explain how the model works, and a model can successfully solve a task, but it might do so based on completely different features than those assumed by the user <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. This can be problematic since a user who thinks to understand the reasoning of the model might draw wrong conclusions from the model output <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Also, the model’s competence is unknown to the user. That means that the model’s performance can drop drastically when it operates in a new situation. However, the user does not necessarily know what a new situation looks like to the model. Therefore, knowing a models competence is of high value since a user can better decide when the limits of the model are reached, and the model needs adaptations or, if adaptations are not possible, stop relying on it <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">Since deep learning based artificial agents are challenging to explain, let alone make them completely interpretable <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, the possibility to increase interaction with the model in fully controllable experiment environments can help to formulate a hypothesis about their reasoning and competence. Moreover, increasing interaction and gaining insights into model behaviour can be used to adapt models to enhance their competencies. This application of eXplainable AI (XAI) is described as ”explain to control” by Adadi and Berrada <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> p. (52143). With the emergence of artificial agents over the last decade in science, industry, healthcare, security and day to day life, XAI gains increasing importance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and was lately promoted to be of high interest in Earth observation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Introduction to SyntEO</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Considering the limited amount of studies that apply synthetic data in Earth observation with deep learning, a guideline is necessary to encourage researchers to explore the opportunities of synthetic training data. Such a workflow should enable users to automatically generate large amounts of labelled training data upon their expert knowledge, existing template data, and newly generated data. During the data generation process, spatio-temporal context on multiple scales should be considered to simulate the inherent characteristics of Earth observation data. Furthermore, the workflow should support to add and modify parameters in order to expand the complexity of the synthetic data sets. Thereby, stable experiment environments are established that researchers can use to explore which changes in the data set influence the training process and model behaviour.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">To that end, we introduce SyntEO, a synthetic data set generation approach specifically designed for investigations of Earth observation data and deep learning methods. To make this introduction more intuitive, we will use examples of the later investigated offshore wind farm detection to illustrate our explanations. Nevertheless, the SyntEO approach is by itself a generic approach, and the discussed examples are only chosen to explain the otherwise abstract and technical SyntEO workflow.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">The SyntEO workflow consists of four steps pictured in figure <a href="#S3.F2" title="Figure 2 ‣ 3 Introduction to SyntEO ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. First, an expert formulates an ontology in which perceptual knowledge from observing real-world examples and remote sensing data as well as expert knowledge is formalised to become machine-readable, numeric knowledge. Using an ontology, human-understandable semantic in the knowledge description is preserved and accessible as machine-readable numeric knowledge and logics. To formalise expert knowledge about offshore wind farms in the SyntEO ontology, an Earth observation expert would start by describing the outer boundary as <span id="S3.p3.1.1" class="ltx_text ltx_font_italic">large</span> or <span id="S3.p3.1.2" class="ltx_text ltx_font_italic">small</span> polygons with specific shape parameters along with numerical values in square meters and number of edges. Likewise, the internal structure of how wind turbines are distributed in these bounds is described and finally, a single turbine. Thus, the ontology holds nested information of entities related to each other.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">The second step defines general configurations for the data generation process. Hereby, the synthetically generated training data dimensions are defined as the synthetic scene and image extent. They are important to solve trade-offs between characteristics of the spatial extent of the target, the granularity of characteristic features and constraints given by hardware capacities or the deep learning model architecture. When this trade-off is solved to maximise potential feature representation in the synthetic training data, the machine-readable ontology and the general configurations are passed to an artificial data generator. An example configuration would take into account that an entire offshore wind farm appears on a large spatial scale but has small scale information represented by single wind turbines simultaneously. Thus, a scene and image extent have to be found, in which the wind farm entity can appear entirely at a pixel resolution that also captures small scale features of single turbines without exceeding the memory of the hardware, which is later used for training a deep learning model.</p>
</div>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2112.02829/assets/figures/synteo_workflow_150.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="628" height="403" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Overview of the SyntEO workflow. First, the domain expert formulates a machine-readable ontology and defines general configurations. Then, an artificial data set generator uses this information to generate a complex data set from template and procedural data. The resulting image-annotation pairs are then transformed into deep learning ready data sets.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">The artificial data generator uses the synthetic scene extent to initiate boundaries of a virtual environment. By following the ontology, values are selected, which describe characteristics of entities to generate geometries of single scene elements and finally compose them to a two or three-dimensional discrete description of a synthetic scene. The single geometries in this composition are then filled with class-specific texture to simulate a sensor acquisition and derive task-specific annotations. The artificial data generator can be imagined as a cluster of user-defined modules that can query databases to add existing template data to the scene composition or generate fully synthetic data, which we call procedural data.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">The last step is the export of the generated data to a training example like an image and annotation pair and a metadata file that describes the selected values of the ontology by the artificial data generator for each example. All generated training examples are summarised in a database and finally optimised as a deep learning ready data set.</p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p">The initial motivation of the SyntEO workflow is to systematically generate Earth observation data that simulates spatio-temporal features and context. Technically this is done by the artificial data generator, which employs the ontology in order to generate a contextual scene composition of hierarchically nested targets and non-targets on multiple scales <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. Therewith, SyntEO is based on the assumption that space and spatial features are inherent characteristics of Earth observation data. For spatial features to appear in Earth observation data, it is necessary that the sensed objects are significantly larger than the spatial resolution of the sensor. This relation is described by the H-resolution remote sensing scene model proposed by Strahler et al.<span id="S3.p7.1.1" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. A remote sensing scene is described by Strahler et al.<span id="S3.p7.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> ”as the spatial and temporal distribution of matter and energy fluxes […]” (p. 122) and furthermore to be ”[…] not chaotic or random, but manifest spatial and temporal order” (p. 123). Derived from this, a synthetic scene in SyntEO is the spatial and temporal non-chaotic composition of scene elements which are an abstract representation of matter and energy fluxes. Hereby, scene elements can be two or three-dimensional targets and non-targets that describe the spatial location, size, distribution and shape of, e.g. land cover, land use classes, atmospheric conditions, natural or artificial landscape and objects. Targets and non-targets are both scene elements. Thereby, a target is an element of interest that is to be predicted by a model. Whereas non-targets are scene elements that naturally appear or specifically do not appear within a spatial range of targets. In particular, the term background is not used in SyntEO since we argue that a remote sensing scene is a composition of spatially distributed information that as entirety can communicate more features which help to make accurate predictions on a target due to their spatial arrangement together with non-targets compared to a target in front of a random background.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Ontology Formulation</h3>

<div class="ltx_pagination ltx_role_end_2_columns"></div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2112.02829/assets/figures/ontology_150.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="628" height="516" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.4.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.5.2" class="ltx_text" style="font-size:90%;">Overview of the core classes in the SyntEO ontology. Red classes are used by the domain expert to include knowledge starting at an <span id="S3.F3.5.2.1" class="ltx_text ltx_font_italic">Observation</span> in the ontology. Green classes belong to the artificial data generator, which ingests the represented knowledge in the ontology by accessing it at the <span id="S3.F3.5.2.2" class="ltx_text ltx_font_italic">DataGeneration</span> class to create training examples. Red and green classes are shared by both sides to establish a human-machine dialogue.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In order to automatically generate training data with nested, multi-scale scene elements and compose them to a harmonic, synthetic scene that takes spatio-temporal context into account, a well-structured description of properties has to be used as a general basis. Furthermore, this description has to be approachable by a human expert as well as an artificial data generator. The expert must be able to include expert knowledge that describes the properties of single scene elements and their relationships. At the same time, the artificial data generator must be able to ingest this information to generate synthetic training data. Therefore, semantics and numeric and logical expressions are essential to this structured knowledge representation. For these reasons, an ontology is used in the SyntEO approach.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In SyntEO, an ontology is defined from an artificial intelligence and knowledge engineering perspective to be a formal, non-ambiguous representation of knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. More specifically, an ontology in SyntEO stores information about nested entities which together describe the properties of a remote sensing scene. The goal is to make perceptual and expert knowledge explicit, structured and finally machine-readable. Therewith, an ontology in SyntEO follows the definition of Gruber <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> to be a ”simplified view of the world that we wish to represent for some purpose” (p. 908). Where in SyntEO, the purpose is to create synthetic data upon this ontology generically. Ontologies have already been used in GIScience and remote sensing, often for the purpose of adding semantics to large geo- or remote sensing databases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> or directly for image analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. For an in-depth introduction to ontologies and their applications in GIScience, Earth observation and ecology, we refer to Agrawal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, Arvor et al.<span id="S3.SS1.p2.1.1" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and Madin et al.<span id="S3.SS1.p2.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The ontology in SyntEO can be approached from two perspectives: A domain expert who defines entities and their characteristics, and the artificial data generator which uses this description to generate synthetic examples. Thereby, the domain expert’s perspective has to be a description of dimensions in which the characteristics of entities can possibly appear. In contrast, the data generator selects single values of these dimensions to generate unique compositions of entities, resulting in synthetic training examples. Therefore, it is important that the dimensions of entity characteristics are related to guarantee coordinated value selection by the data generator.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">The Extensible Observation Ontology (OBOE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> was chosen as starting point and adapted for the SyntEO approach. Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Ontology Formulation ‣ 3 Introduction to SyntEO ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the core classes and scheme of a single entity description in a typical SyntEO ontology. The domain expert’s perspective, depicted in red, starts at an <span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_italic">Observation</span> of an <span id="S3.SS1.p4.1.2" class="ltx_text ltx_font_italic">Entity</span>. Each <span id="S3.SS1.p4.1.3" class="ltx_text ltx_font_italic">Observation</span> can have multiple <span id="S3.SS1.p4.1.4" class="ltx_text ltx_font_italic">Measurements</span> where each <span id="S3.SS1.p4.1.5" class="ltx_text ltx_font_italic">Measurement</span> is of a specific <span id="S3.SS1.p4.1.6" class="ltx_text ltx_font_italic">Characteristic</span>. Since the domain expert has to describe <span id="S3.SS1.p4.1.7" class="ltx_text ltx_font_italic">Dimensions</span> instead of single values, each <span id="S3.SS1.p4.1.8" class="ltx_text ltx_font_italic">Measurement</span> has a <span id="S3.SS1.p4.1.9" class="ltx_text ltx_font_italic">Dimension</span> which again has <span id="S3.SS1.p4.1.10" class="ltx_text ltx_font_italic">Values</span> which describe the <span id="S3.SS1.p4.1.11" class="ltx_text ltx_font_italic">Characteristic</span>. The <span id="S3.SS1.p4.1.12" class="ltx_text ltx_font_italic">Values</span> in the <span id="S3.SS1.p4.1.13" class="ltx_text ltx_font_italic">Dimension</span> can take on a wide variety of formats like, statistical distributions with limiting ranges, a set of possible predefined choices or template information coming from linked databases.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">To better illustrate this scheme, let us assume the formal description of the size of offshore wind farms (OWF). First, a domain expert makes <span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_italic">Observations</span> of the <span id="S3.SS1.p5.1.2" class="ltx_text ltx_font_italic">Entity</span> offshore wind farm. The <span id="S3.SS1.p5.1.3" class="ltx_text ltx_font_italic">Measurement</span> of <span id="S3.SS1.p5.1.4" class="ltx_text ltx_font_italic">Characteristic</span> ‘size’ has a <span id="S3.SS1.p5.1.5" class="ltx_text ltx_font_italic">Dimension</span> which is then described by the domain expert with three predefined possible choices with the <span id="S3.SS1.p5.1.6" class="ltx_text ltx_font_italic">Values</span> ‘small:5 km’, ‘medium:10 km’ and ‘large:17 km’. The additional usage of the vague terms (small, medium and large) is an example of how to make numeric knowledge better understandable for a human interpreter.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">After the domain expert has included this information in the ontology and made it explicit, the artificial data generator uses it for its purpose of <span id="S3.SS1.p6.1.1" class="ltx_text ltx_font_italic">DataGeneration</span>. For each <span id="S3.SS1.p6.1.2" class="ltx_text ltx_font_italic">DataGeneration</span> of an <span id="S3.SS1.p6.1.3" class="ltx_text ltx_font_italic">Entity</span> and its <span id="S3.SS1.p6.1.4" class="ltx_text ltx_font_italic">Characteristics</span> the data generator selects a <span id="S3.SS1.p6.1.5" class="ltx_text ltx_font_italic">SceneElementSpecification</span>. This single <span id="S3.SS1.p6.1.6" class="ltx_text ltx_font_italic">Value</span> originates from the <span id="S3.SS1.p6.1.7" class="ltx_text ltx_font_italic">Values</span> in the <span id="S3.SS1.p6.1.8" class="ltx_text ltx_font_italic">Dimension</span> which were defined by the domain expert. In case of the OWF size, the artificial data generator would start a <span id="S3.SS1.p6.1.9" class="ltx_text ltx_font_italic">DataGeneration</span> for a single OWF and its <span id="S3.SS1.p6.1.10" class="ltx_text ltx_font_italic">SceneElementSpecification ofCharacteristic</span> ‘size’. From the predefined <span id="S3.SS1.p6.1.11" class="ltx_text ltx_font_italic">Values</span> in the <span id="S3.SS1.p6.1.12" class="ltx_text ltx_font_italic">Dimension</span>, it then selects a single <span id="S3.SS1.p6.1.13" class="ltx_text ltx_font_italic">Value</span> e.g. ‘small:5 km’.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">Since the artificial data generator has to harmonise all selected single scene elements and their <span id="S3.SS1.p7.1.1" class="ltx_text ltx_font_italic">SceneElementSpecifications</span> to become a coherent scene composition, <span id="S3.SS1.p7.1.2" class="ltx_text ltx_font_italic">Context</span> and <span id="S3.SS1.p7.1.3" class="ltx_text ltx_font_italic">Relationships</span> are added. <span id="S3.SS1.p7.1.4" class="ltx_text ltx_font_italic">Context</span> is used to link <span id="S3.SS1.p7.1.5" class="ltx_text ltx_font_italic">Observations</span> and <span id="S3.SS1.p7.1.6" class="ltx_text ltx_font_italic">DataGeneration</span> of different <span id="S3.SS1.p7.1.7" class="ltx_text ltx_font_italic">Entities</span> as reactive bindings in nested hierarchies. For example, a ‘small:5 km’ OWF size causes a higher wind turbine density, which properties are described in an other <span id="S3.SS1.p7.1.8" class="ltx_text ltx_font_italic">Entity</span> that is linked via the <span id="S3.SS1.p7.1.9" class="ltx_text ltx_font_italic">Context</span> attribution. The class <span id="S3.SS1.p7.1.10" class="ltx_text ltx_font_italic">Relationship</span> represents the spatio-temporal connections between <span id="S3.SS1.p7.1.11" class="ltx_text ltx_font_italic">Entities</span>. Here, the <span id="S3.SS1.p7.1.12" class="ltx_text ltx_font_italic">Relationship ofType Topology</span> are used to represent spatial conditions, for example, a wind turbine <span id="S3.SS1.p7.1.13" class="ltx_text ltx_font_italic">Entity MustBeCoincidentWith</span> an offshore wind farm <span id="S3.SS1.p7.1.14" class="ltx_text ltx_font_italic">Entity</span> which again <span id="S3.SS1.p7.1.15" class="ltx_text ltx_font_italic">MustNotOverlap</span> with a land <span id="S3.SS1.p7.1.16" class="ltx_text ltx_font_italic">Entity</span>. By implementing <span id="S3.SS1.p7.1.17" class="ltx_text ltx_font_italic">Context</span> and <span id="S3.SS1.p7.1.18" class="ltx_text ltx_font_italic">Relationships</span>, a nested structure can be represented in the ontology and from an artificial perspective <span id="S3.SS1.p7.1.19" class="ltx_text ltx_font_italic">Entities</span> can now be handled as sets. For the given <span id="S3.SS1.p7.1.20" class="ltx_text ltx_font_italic">Relationship</span> example the transitive relation can be derived that an off shore wind turbine can not appear on land, without that the <span id="S3.SS1.p7.1.21" class="ltx_text ltx_font_italic">Entities</span> of off shore wind turbine and land were directly connected in the ontology.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Synthetic Scene and Image Extent</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">General configurations must be defined before the artificial data generator can utilise the ontology for composing a synthetic scene. These configurations describe the extent of the synthetic environment in which the synthetic scene is composed as well as the extent and resolution of the synthetic image which is generated. The synthetic scene extent is important in order to initialise a data generation environment that is aligned to the information within the ontology about the extent of single scene elements and their relationships. The synthetic image extent and the synthetic sensor resolution control the maximum size and granularity of the features represented in the image. Especially by defining the synthetic image extent, the available hardware and deep learning model architecture are to be considered. Finally, synthetic scene and image extent have to be coordinated with each other. In order to define optimal synthetic scene and image extent, the following aspects have to be considered:</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">The spatial or temporal extent of potential features which can be derived from the extent of single scene elements and their relationships.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">The synthetic sensor resolution which will finally represent the feature’s occurrence in the training data. Thereby, the synthetic sensor resolution should be the same as of the sensor that acquires the data which the trained model will predict on in production.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">The deep learning model architecture.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">The specification of the hardware which will be used for training.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">For a better intuition of synthetic scene and synthetic image extent, let us consider the following example. A synthetic scene has a maximum extent which the creator defines. This extent can be infinite or finite, whereas the maximum synthetic image extent is always finite. Hence, the scene extent can be the same size or larger as the maximum image extent. Thus, from a composed, single synthetic scene, one or multiple images can be generated. For the sake of simplicity in this introduction, the maximum scene extent is considered to be equal to the maximum image extent.</p>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2112.02829/assets/figures/scene_ext_150.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="471" height="464" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">Three examples of how to define the maximum virtual scene and image extent under given constraints in feature extent, virtual sensor resolution, hardware and DL model architecture. The constraints for the first example allow using the full synthetic scene at full sensor resolution, that no trade-off between small or large features is necessary. In examples two and three, the given constraints demand smaller image dimensions, which in (2) leads to downscaling of the full image and loss in fine-grained features. In (3), multiple images are taken from the same scene to maintain small features at the cost of large features in the training image.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.6" class="ltx_p">In order to define the maximum scene extent, the creator must decide on which spatial scale the largest feature can occur potentially. Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2 Synthetic Scene and Image Extent ‣ 3 Introduction to SyntEO ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows an example that assumes a maximum feature extent (depicted as a blue polygon) of 20 km <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mo id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><times id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\times</annotation></semantics></math> 20 km and a synthetic sensor resolution of 10 m <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><mo id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><times id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">\times</annotation></semantics></math> 10 m. The following three examples show how trade-offs can be balanced in order to maximise feature representation in the synthetic image. Since maximum feature extent defines the space in which a scene must be composed, the maximum scene extent is aligned to the largest potential feature size. Because the final result is an image, the synthetic sensor resolution is considered to refine the scene extent to a technically suitable value. Here, maximum feature extent and synthetic sensor resolution would result in an image of 2000 <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><mo id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><times id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">\times</annotation></semantics></math> 2000 pixels. Since <math id="S3.SS2.p4.4.m4.1" class="ltx_Math" alttext="2^{n}" display="inline"><semantics id="S3.SS2.p4.4.m4.1a"><msup id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml"><mn id="S3.SS2.p4.4.m4.1.1.2" xref="S3.SS2.p4.4.m4.1.1.2.cmml">2</mn><mi id="S3.SS2.p4.4.m4.1.1.3" xref="S3.SS2.p4.4.m4.1.1.3.cmml">n</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><apply id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m4.1.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">superscript</csymbol><cn type="integer" id="S3.SS2.p4.4.m4.1.1.2.cmml" xref="S3.SS2.p4.4.m4.1.1.2">2</cn><ci id="S3.SS2.p4.4.m4.1.1.3.cmml" xref="S3.SS2.p4.4.m4.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">2^{n}</annotation></semantics></math> are common dimensions in CNNs, the final synthetic scene extent is adjusted to be 20,480 m <math id="S3.SS2.p4.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p4.5.m5.1a"><mo id="S3.SS2.p4.5.m5.1.1" xref="S3.SS2.p4.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m5.1b"><times id="S3.SS2.p4.5.m5.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m5.1c">\times</annotation></semantics></math> 20,480 m so that an image of 2048 <math id="S3.SS2.p4.6.m6.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p4.6.m6.1a"><mo id="S3.SS2.p4.6.m6.1.1" xref="S3.SS2.p4.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m6.1b"><times id="S3.SS2.p4.6.m6.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m6.1c">\times</annotation></semantics></math> 2048 pixels can be taken of that synthetic scene.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.8" class="ltx_p">Looking at figure <a href="#S3.F4" title="Figure 4 ‣ 3.2 Synthetic Scene and Image Extent ‣ 3 Introduction to SyntEO ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, one can see that changes in GPU memory and deep learning architecture can lead to situations in which the entire synthetic scene can not appear in a single synthetic image by maintaining the highest resolution. The first example shows a situation where GPU memory is large enough to optimise a deep learning model with an input size of 2048 <math id="S3.SS2.p5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p5.1.m1.1a"><mo id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><times id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">\times</annotation></semantics></math> 2048 pixels. Thus, no trade-off has to be balanced, and the synthetic scene with features of large and small sizes can appear in original sensor resolution in a single synthetic image. In examples two and three, the trade-off balancing is discussed in the context of a decrease in available GPU memory, which forces the deep learning model to use a smaller input dimension of 1024 <math id="S3.SS2.p5.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p5.2.m2.1a"><mo id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><times id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">\times</annotation></semantics></math> 1024 pixels to avoid a memory overflow during model optimisation. Note that the GPU memory decrease is exemplary. Each of the other aspects can also induce the need to balance trade-offs. For scenario two, the domain expert decides that large scale features (blue polygon) are more important than small scale features (green polygons). In order to ensure that large scale features appear entirely in a single training example, a synthetic image with 10 m <math id="S3.SS2.p5.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p5.3.m3.1a"><mo id="S3.SS2.p5.3.m3.1.1" xref="S3.SS2.p5.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.1b"><times id="S3.SS2.p5.3.m3.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">\times</annotation></semantics></math> 10 m resolution and a size of 2048 <math id="S3.SS2.p5.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p5.4.m4.1a"><mo id="S3.SS2.p5.4.m4.1.1" xref="S3.SS2.p5.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.4.m4.1b"><times id="S3.SS2.p5.4.m4.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.4.m4.1c">\times</annotation></semantics></math> 2048 pixels is first taken from the synthetic scene. Before training, this image is downscaled to the input size of 1024 <math id="S3.SS2.p5.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p5.5.m5.1a"><mo id="S3.SS2.p5.5.m5.1.1" xref="S3.SS2.p5.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.5.m5.1b"><times id="S3.SS2.p5.5.m5.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.5.m5.1c">\times</annotation></semantics></math> 1024 pixels. The downsizing maintains the appearance of the large feature in one image. However, since the pixel size is artificially decreased to 20 m <math id="S3.SS2.p5.6.m6.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p5.6.m6.1a"><mo id="S3.SS2.p5.6.m6.1.1" xref="S3.SS2.p5.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.6.m6.1b"><times id="S3.SS2.p5.6.m6.1.1.cmml" xref="S3.SS2.p5.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.6.m6.1c">\times</annotation></semantics></math> 20 m, a small-scale feature representation loss has to be accepted. In the third example, the domain expert decides that small scale features are more important than large scale features. Thus, the higher spatial sensor resolution of 10 m <math id="S3.SS2.p5.7.m7.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p5.7.m7.1a"><mo id="S3.SS2.p5.7.m7.1.1" xref="S3.SS2.p5.7.m7.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.7.m7.1b"><times id="S3.SS2.p5.7.m7.1.1.cmml" xref="S3.SS2.p5.7.m7.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.7.m7.1c">\times</annotation></semantics></math> 10 m has to be unchanged to ensure that fine-grained features appear. Four synthetic images of size 1024 <math id="S3.SS2.p5.8.m8.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p5.8.m8.1a"><mo id="S3.SS2.p5.8.m8.1.1" xref="S3.SS2.p5.8.m8.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.8.m8.1b"><times id="S3.SS2.p5.8.m8.1.1.cmml" xref="S3.SS2.p5.8.m8.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.8.m8.1c">\times</annotation></semantics></math> 1024 pixels are taken from one synthetic scene, splitting the large scale feature and downgrading its representation in the training data.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">In all of the three scenarios, the synthetic scene remains the same. This is important since it allows to compose a synthetic scene coherently for small and large scale features, even when the trade-off focus later shifts to e.g. small scale features. That way, both features still appear together in a meaningful relationship in the synthetic image, even when the larger scene element is not entirely included in the final synthetic image but in the synthetic scene.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Artificial Data Generator</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The artificial data generator can create synthetic training examples with the formulated ontology and defined general configurations. In a first step, a discrete scene composition is generated within a predefined synthetic environment which describes where or when data of a specific scene element has to appear in relation to other scene elements. The texture is added to the discrete geometries to simulate the sensor’s measurements in a second step.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The data generator can be understood as a cluster of modules connected to the ontology that either query and rearrange existing data or process completely new synthetic data. Thereby, each module uses the parameters which are provided by the ontology. The way the information is stored in the ontology can be different for each scene element and if it is a geometry or texture. For example, vector databases like OpenStreetMap (OSM) can be queried and selected by the artificial data generator to receive geometries for specific scene elements. The same is possible for texture upon the predefined geometry. An Earth observation archive can be searched, and the raster data included for that specific scene element. Therefore, the databases are linked in the ontology as <span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_italic">Values</span> of the <span id="S3.SS3.p2.1.2" class="ltx_text ltx_font_italic">Entity’s Dimension</span>. In SyntEO we refer to data within <span id="S3.SS3.p2.1.3" class="ltx_text ltx_font_italic">Dimensions</span> which already exists as <span id="S3.SS3.p2.1.4" class="ltx_text ltx_font_italic">template data</span>. On the opposite are fully synthetic generation processes, in which geometries and texture are generated procedurally. Such data is referred to as <span id="S3.SS3.p2.1.5" class="ltx_text ltx_font_italic">procedural data</span>. For instance, processes from the computer graphics domain and their implementation in free tools such as Blender <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> can be connected to the ontology as modules that generate fully synthetic geometries and texture. However, more specialised physical and statistical models which describe scene elements in specific research domains and sensor specifications can also be plugged into the data generation process. Tools like RaySAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, or spectral libraries <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> are then used to generate the data described by the ontology. The modular design of the data generator and its close interaction with the ontology enables to link between existing data archives and existing synthetic approaches and models and novel generator modules tailored to generate scene elements for specific research projects.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">After the last step in the data generation process, a virtual image is taken from the generated data when the texture was added to the scene composition. Together with the synthetic image, the necessary annotation is extracted from the final scene composition. The discrete geometries provide ground truth information in the same depth as formulated in the ontology and deriving segmentation masks or bounding boxes is easily possible.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">In this example, the generated data, a 2D image, and annotation are both saved to separate files. To support efficient access to the training examples, translating the data to binary file formats like the TFRecord format for the TensorFlow deep learning framework is optional but recommended. In addition to the training data, the SyntEO approach allows saving a snapshot of the sampled ontology from the data generator perspective as a detailed description of all parameters which were used to generate the scene elements and their composition. The accumulation of all snapshots is an important source for a controllable experiment environment and a strong tool in ablation studies to gain insights into the model training. The default file format for an ontology snapshot is the same as for the origin ontology, a .xml file.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Wrap-up and Potential Applications</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">For simplicity, SyntEO can be reduced to two core functionalities: The ontology formulation and the artificial data generator. The formulated ontology can be seen as a complex parameter file with nested information which describes a remote sensing scene from the big picture to tiny details. The artificial data generator is the computational backend that ingests this description and builds images based on it. The ontology can be understood by humans and machines and maps expert knowledge to technical modules that compose the synthetic remote sensing scenes. The technical interface between domain experts and artificial data generator can be a .xml or .json file or any format, which supports to store information of nested entities.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">The image processing backend has to realise the description embedded in the ontology. For example a simple ontology could describe a single band binary image with a circle of random radius at the centre of a raster of a specific extent. A straightforward processing backend can be a python script, which constructs a circle in vector space and uses the provided raster dimension to rasterise this circle. The final output would then be a binary image showing the circle and a label which, for example, holds the circle radius information. The upcoming hands on example of offshore wind farm extraction is more complex. Here the processing back end are mainly two modules: A remote sensing image database from which texture examples are queried and a python script that uses 2D numpy arrays to construct geometries of specific shapes and create procedural texture information. Together, they use the parameter file defined by the ontology to create single entities independently before they are combined, again by following the description in the parameter file.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">Since the synthetically generated radar images will be a single band image with a 10 m resolution, the open question is if SyntEO can be used for more complex data like multi-channel images with a very high resolution. The following example should provide an intuition about the motivation and transferability of SyntEO. Here, the ontology describes the generation of an RGB remote sensing scene in 3D space, the following technical description of the pipeline uses parameters that are defined in the ontology.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">Figure <a href="#S3.F5" title="Figure 5 ‣ 3.4 Wrap-up and Potential Applications ‣ 3 Introduction to SyntEO ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows how the open source software Blender <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> is used to generate a complex high resolution RGB remote sensing scene. Initially, a procedural terrain is generated by using Perlin noise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. Two crops from atmospherically corrected and pansharped IKONOS RGB images with a spatial resolution of 80 cm are used as texture and mixed using characteristics from the underlying procedural terrain. For demonstration, the target objects in this example are trees. Therefore a 3D model of a tree is randomly changed and distributed on the procedural terrain. For tree placement, a particle system uses a weight map that relies on terrain properties like height, surface convexity and pointiness, and a fixed offset to the edges of the scene extent to simulate a reasonable distribution. The output is an overhead image of the scene to simulate the sensor look angle.</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<p id="S3.SS4.p5.1" class="ltx_p">The lightning of a synthetic scene can also be controlled to simulate shadow properties, which might be important in cities with higher buildings or in areas with steep terrain. The tree labels are derived from the particle system, which provides coordinates for each tree location and a radius depending on the randomly selected scale of the 3D parent object.</p>
</div>
<div id="S3.SS4.p6" class="ltx_para">
<p id="S3.SS4.p6.1" class="ltx_p">SyntEO should be understood as a system to structure expert knowledge with the goal to make it machine readable and inject it to a processing backend which then can compose synthetic scenes and finally provide a synthetically generated remote sensing image with corresponding labels. New tasks, sensors or drastically changing environmental conditions need to update the formalised expert knowledge and possibly a new processing backend. However, the longer SyntEO is in use, the more descriptions following the SyntEO ontology will exist and therewith, building new scenes will become faster and easier over time by fusing already used functionalities with new developments.</p>
</div>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2112.02829/assets/figures/synteo_rgb_150.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="628" height="793" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.3.2" class="ltx_text" style="font-size:90%;">Procedural generation of a high resolution RGB scene in Blender. a) IKONOS texture is merged based on characteristics of the generated terrain. b) Tree placement by employing a weight map and particle system; the final virtual remote sensing image from an overhead perspective. c) oblique view of the generated scene in 3D space.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Offshore Wind Farm Detection with SyntEO</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Offshore wind farms (OWF) are a crucial cornerstone for carbon-neutral strategies, which will be realised in the upcoming decades. For example, the European Union is planning to increase its annual energy generation with OWFs from today 12 GW to at least 60 GW in 2030 and 300 GW in 2050 with a financial investment of EUR 800 billion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. However, OWFs are on the interface of climate protection, human intervention in sensitive ecosystems, economic interests and state legislative <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. In order to support maritime spatial planning, the development of new OWF sites which are aligned to exclusion zones for natural conservation or fishing industry, the monitoring of OWF deployment and future studies which investigate the impacts of large scale OWFs, an independent, high resolution, spatial-temporal monitoring of OWFs is necessary. Since the deployment of OWFs is a global issue, the monitoring has to be globally too. Thus, Earth observation imagery is the most promising data source to provide a global, independent, constant and automatic OWF monitoring product in time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>. To process a global data set frequently, efficient processing is indispensable, and at the same time, the model’s spatial transferability must be guaranteed to avoid high false positive or false negative rates.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">In this introduction to the SyntEO approach, a first hands-on example is demonstrated: The detection of offshore wind farms on two of the worlds largest offshore wind energy production sites, the North Sea basin and the East Chinese Sea. Furthermore, the model’s performance on two additional non-OWF sites is taken into account to demonstrate the sensitivity for false detections in challenging environments and the ability of SyntEO to create large-scale data sets that can be used to train spatial transferable deep learning models.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Studysite and Data</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Four study sites were selected due to their different characteristics to investigate the capabilities of the SyntEO approach. Most of the OWFs in the North Sea Basin are located with considerable distance to the coast and few of them are under construction. In comparison, OWFs in the East China Sea show different characteristics. Especially in the Hangzhou Bay near Shanghai or areas with strong tidal influence like the Jiangsu Rudong Offshore Intertidal Demonstration Wind Farm in the south of Jiangsu, OWFs appear in highly dynamic, near coast environments with many OWFs under construction. Hence the East China Sea is considered to be more challenging than the North Sea Basin concerning OWF detection. The other two study sites are the Persian Gulf and the Sea of Azov, both selected due to potentially high false positive rates resulting from offshore rigs for oil and methane extraction or rectangular agriculture fields with a specific size and geometric patterns, respectively.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The remote sensing imagery used during the SyntEO workflow and for prediction of the four study sites is based on C-band synthetical aperture radar (SAR) from Sentinel-1. Specifically, a stack of all acquisitions in three months from July to September 2020 of VH polarised IW GRD products from both ascending and descending orbits is reduced to a single band of median values. Furthermore, instead of the measurements in decibel, which are available as 16 bit floating-point numbers, the radiometric resolution is downscaled to 8 bit and the range of the values rescaled to a range of 0 to 255. Even when this results in a loss of information in each pixel, the spatial representation is only insignificantly influenced. Therewith, we follow a core idea of SyntEO that single pixel values are less important than spatial patterns, which are still distinct in 8 bit and a spatial resolution of 10 m <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mo id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><times id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\times</annotation></semantics></math> 10 m of the final S1-median product.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">GoogleEarthEngine (GEE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> was used to query the preprocessed Sentinel-1 collection and create the 8 bit median stacks. In order to handle the query and download process on the GEE a 200 km buffer around the global coastline provided by OSM was used as area of interest. This polygon was structured in a database with tiles of 1.8° <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mo id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><times id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">\times</annotation></semantics></math> 1.8°, see figure <a href="#S4.F6" title="Figure 6 ‣ 4.1 Studysite and Data ‣ 4 Offshore Wind Farm Detection with SyntEO ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. The finally downloaded Sentinel-1 median global data set for the third quarter of 2020 was split into two groups, one with potential OWF locations, like the mentioned areas above and another with scenes showing a mixture of land, coast and the open sea. The second group will later be used as texture template data in the SyntEO workflow.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">For the North Sea Basin and the East China Sea, all OWFs were manually labelled to generate a ground truth test data set for the final evaluation of the SyntEO performances. Therefore, all completely deployed offshore wind turbines until the third quarter of 2020 were labelled with points. Upon these points, clusters were aggregated to receive the ground truth bounding boxes of offshore wind farms. In the North Sea Basin, this resulted in 3,787 offshore wind turbines within 42 OFWs and in the East China Sea, 1,587 offshore wind turbines in 25 OWFs.</p>
</div>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2112.02829/assets/figures/stusi_150.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="628" height="806" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Studysite location with examples of S-1 median images in 2020Q3. a) Locations of the four study sites and the defined S-1 data grid of the global coastline. b) Closeup of the North Sea Basin and OWFs in the German Bight. c) Closeup of the East China Sea, OWFs at the coast of Jiangsu, China. d) Closeup of the Persian Gulf, rigs in open water near Khaji, Saudi Arabia. e) Closeup of the Sea of Azov, agriculture fields used as false positive examples due to their grid like pattern near Yeysk, Russia.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>SyntEO Offshore Wind Farm Data Set Generation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">For a demonstration of SyntEO in a real-world application, a synthetic data set for OWF detection in multi-temporal Sentinel-1 images is created. In order to get the best impression of the creation and performance of the final data set, three preliminary data sets are forked during the data set generation. Thus, four consecutive synthetic data sets are produced with increasing complexity of the included synthetic training examples. For each data set, full from-scratch training of a well established CNN object detector, the Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> with adjacent performance analyses on the ground truth data set is carried out.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">When looking at the OWFs shown in figure <a href="#S4.F6" title="Figure 6 ‣ 4.1 Studysite and Data ‣ 4 Offshore Wind Farm Detection with SyntEO ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> the received perception can be formulated as follow: ”<span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_italic">Offshore wind farms are located in the sea but can appear in coastal areas on tidal flats. Smaller wind farms are closer to the coast than larger wind farms. The wind turbine density decreases by increasing OWF size. Wind turbines are organised in a regular grid-like pattern with individual but consistent, systematic changes to the grid structure for each wind farm. The typical outer shape of the entire wind farm is a polygon with four to five sides</span>.”</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">With the above given semantic description, an expert can now formulate an ontology to structure this perceptual knowledge and translate it to numeric knowledge to make it machine-readable. For a better intuition, this example will focus on the target scene element <span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_italic">WindFarm</span> as an example of how the formulated ontology can be used to generate this specific scene element.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">A graphical representation of the subset of <span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_italic">WindFarm</span> related ontology is shown in figure <a href="#S4.F7" title="Figure 7 ‣ 4.2 SyntEO Offshore Wind Farm Data Set Generation ‣ 4 Offshore Wind Farm Detection with SyntEO ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. When following the structure of the ontology, the above given semantic description is incorporated in the visualisation of the ontology to show which point of the ontology represents the perceptual knowledge. The starting point of the present scenario is a scene composition with a predefined maximum scene extent of 20,480 m <math id="S4.SS2.p4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.p4.1.m1.1a"><mo id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><times id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">\times</annotation></semantics></math> 20,480 m. The artificial data generator has already included the non-target scene elements <span id="S4.SS2.p4.1.2" class="ltx_text ltx_font_italic">Sea, Coast</span> and <span id="S4.SS2.p4.1.3" class="ltx_text ltx_font_italic">Land</span> to the scene composition by using the ontology. The following task is to generate an OWF and integrate it into the existing scene composition.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">Starting at a) in figure <a href="#S4.F7" title="Figure 7 ‣ 4.2 SyntEO Offshore Wind Farm Data Set Generation ‣ 4 Offshore Wind Farm Detection with SyntEO ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, first, the size of the OWF is taken from three values (large, medium and small), which represent scale factors as numeric values as well as semantics for better human understanding and to use the words as keys for subsequent choices of contextual <span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_italic">Observations</span>. Since the <span id="S4.SS2.p5.1.2" class="ltx_text ltx_font_italic">Entity Land</span> is within distance of potential OWF locations in the scene composition, the data generator selects the <span id="S4.SS2.p5.1.3" class="ltx_text ltx_font_italic">Value</span> <span id="S4.SS2.p5.1.4" class="ltx_text ltx_font_italic">small:5 km</span> for the <span id="S4.SS2.p5.1.5" class="ltx_text ltx_font_italic">WindFarm Characteristic size</span>. After that, the artificial data generator generates the internal structure and outer boundary of the OWF. In b), the internal structure is modelled as a regular grid, where each crossing is a potential wind turbine location. The density of the grid is the number of lines in a unit square. This number is chosen randomly upon two discrete uniform distributions for the x and y-axis. The limits of these distributions are determined by the <span id="S4.SS2.p5.1.6" class="ltx_text ltx_font_italic">size Value</span> to represent the increasing wind turbine density with decreasing OWF size. The result is a regular, orthogonal grid in a unit square. To introduce c) ”<span id="S4.SS2.p5.1.7" class="ltx_text ltx_font_italic">individual but consistent, systematic changes to the grid structure</span>”, a deformation function is chosen randomly and applied to the generated regular grid.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p">In d), the outer boundary is defined to be a polygon of a specific number of vertices and distance to each other. Like the turbine density, the outer boundary <span id="S4.SS2.p6.1.1" class="ltx_text ltx_font_italic">hasContext</span> with the <span id="S4.SS2.p6.1.2" class="ltx_text ltx_font_italic">size</span> of the OWF. By using the <span id="S4.SS2.p6.1.3" class="ltx_text ltx_font_italic">Value small</span> as key, numeric values for <span id="S4.SS2.p6.1.4" class="ltx_text ltx_font_italic">NumberOfVertices</span> and <span id="S4.SS2.p6.1.5" class="ltx_text ltx_font_italic">MinVerticesDistance</span> are selected and passed to a random polygon generator, which again uses a unit square to construct the polygon as the outer boundary. Due to the <span id="S4.SS2.p6.1.6" class="ltx_text ltx_font_italic">Relationship ofType Topology</span> which determines that the <span id="S4.SS2.p6.1.7" class="ltx_text ltx_font_italic">GriddedWindfarmLayout MustBeInside WindfarmBoundary</span>, the potential wind turbine locations are spatially subsetted. The locations selected in this way finally constitute the internal and external structure of the OWF.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p">To further integrate the generated OWF into the scene composition, it is scaled by the initially selected value <span id="S4.SS2.p7.1.1" class="ltx_text ltx_font_italic">small:5 km</span>. Now in e), the <span id="S4.SS2.p7.1.2" class="ltx_text ltx_font_italic">Relationship ofType Topology</span> to Sea, Land and Coast can be used to find a location where all topologies are valid. Upon the final scene composition, the texture is added to receive the synthetic image in f). Therefore two approaches are applied. Sea, coast and land texture are queried from a template database. Since the geometries of these three entities rely on OSM coastline data, the same geometries can be used to query the prepared Sentinel-1 data grid of the median image. Thus, the shapes are filled with the corresponding pixel values.</p>
</div>
<div id="S4.SS2.p8" class="ltx_para">
<p id="S4.SS2.p8.1" class="ltx_p">On the other hand, the texture for wind turbines is completely procedural. The parameters for its automatic generation are also defined in the ontology in the <span id="S4.SS2.p8.1.1" class="ltx_text ltx_font_italic">Entity WindTurbine</span>. The generator module is a numpy array implementation that uses these parameters to create a stack of 2D kernels to generate each wind turbine’s texture. In order to guarantee smooth gradients between sea and turbine texture, the values of the turbine texture are generated after the sea texture was added. This way, the local sea texture can be taken into account during procedural turbine texture generation. Therewith, geometries for scene composition and the corresponding texture are a combination of template data and procedural data provided by modules that ingest the input parameters from the ontology to compose and harmonise scene elements in the scene composition and texture adding process.</p>
</div>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2112.02829/assets/figures/example_ontology_150.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="981" height="626" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.4.2" class="ltx_text" style="font-size:90%;">Overview of the subset for <span id="S4.F7.4.2.1" class="ltx_text ltx_font_italic">WindFarm</span> generation of the SyntEO ontology for OWF data set generation. A semantic description of perceptual knowledge in blue leads through the ontology outlined. a) describes the size selection for the wind farm; b) and c) define the internal structure of the wind farm by potential turbine locations; d) describes the outer boundary of the wind farm and combines it with the potential turbine locations; e) relates the generated wind farm geometry to other scene elements by using topologies, resulting in the scene composition, f) shows how the scene composition is filled with texture and the derived bounding box annotation.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div id="S4.SS2.p9" class="ltx_para">
<p id="S4.SS2.p9.1" class="ltx_p">Finally, the corresponding label for the generated image, a bounding box of the OWF, is derived from the <span id="S4.SS2.p9.1.1" class="ltx_text ltx_font_italic">WindFarm</span> scene element and its final location in the scene composition. The image is exported to a single band .png file, whereas the generated label is saved to a .xml file following the PASCAL-VOC convention for object detection annotation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S4.SS2.p10" class="ltx_para">
<p id="S4.SS2.p10.1" class="ltx_p">With this introduced ontology, a data set can be created that only contains examples that show target information. However, it is good praxis in a large deep learning data set to include non-target examples, especially when non-targets exist that are known to be similar to the targets. In the case of OWFs these are, for example, oil rigs. As shown in figure <a href="#S4.F6" title="Figure 6 ‣ 4.1 Studysite and Data ‣ 4 Offshore Wind Farm Detection with SyntEO ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, single oil rigs are similar to single wind turbines, bright spots in front of a dark sea. However, the critical difference is that on a small scale, a single oil rig looks different from an offshore wind turbine and on a larger scale, oil rig clusters appear unstructured and random, whereas wind turbines in an OWF are structured. A description of oil rigs was also included in the ontology. To give an intuition about the generation process, figure <a href="#S4.F8" title="Figure 8 ‣ 4.2 SyntEO Offshore Wind Farm Data Set Generation ‣ 4 Offshore Wind Farm Detection with SyntEO ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> describes the procedural generation of oil rigs.</p>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2112.02829/assets/figures/rigs_150.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="550" height="415" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S4.F8.3.2" class="ltx_text" style="font-size:90%;">Overview of the generation process of synthetic oil rig training examples. With a threshold on Simplex noise, generic shapes are generated. Their combination wit random locations for oil rigs is then filled with texture to generate the final non-target training example.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div id="S4.SS2.p11" class="ltx_para">
<p id="S4.SS2.p11.1" class="ltx_p">Therefore, simplex noise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> generates a 2D raster with values of ranges of [-1,1]. By applying a threshold to this raster, random organic shapes are created. Another module generates random locations with x and y coordinates, drawn from a uniform distribution in the same synthetic scene extent as the simplex noise. The rig locations are set up by applying a topology that only allows locations within the generated polygons. For adding texture to the rigs, the same approach as for the turbines is used. A 2D kernel simulates a single rig, whereby the kernel for a rig is a single Gaussian variant. Sea texture is again template data selected from the median Sentinel-1 data grid.</p>
</div>
<div id="S4.SS2.p12" class="ltx_para">
<p id="S4.SS2.p12.1" class="ltx_p">That way, non-target training images can be generated. However, the corresponding label file has to be created as specifically empty. During training, the optimiser recognises that explicitly no target is in the image and therewith, each prediction made during training by the model results in a false positive optimisation signal.</p>
</div>
<div id="S4.SS2.p13" class="ltx_para">
<p id="S4.SS2.p13.1" class="ltx_p">This demonstration of the SyntEO workflow shows how the perception by observation of real-world examples can be used to formulate a semantic description which is further developed to be an explicit description and finally machine-readable numeric knowledge. With the core classes of the SyntEO ontology, a structure can be created which, on one hand, allows to define <span id="S4.SS2.p13.1.1" class="ltx_text ltx_font_italic">Dimensions</span> of <span id="S4.SS2.p13.1.2" class="ltx_text ltx_font_italic">Values</span> to describe many <span id="S4.SS2.p13.1.3" class="ltx_text ltx_font_italic">Obeservations</span> and on the other hand, to give access to an artificial data generator which follows the same structure to generically compose complex scene compositions by selecting a single <span id="S4.SS2.p13.1.4" class="ltx_text ltx_font_italic">Value</span> for each <span id="S4.SS2.p13.1.5" class="ltx_text ltx_font_italic">Characteristic</span> it has to express.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Synthetic Data Set Evolution</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">With the previously described workflow, four sequential data sets were created. This was done to demonstrate the human-machine interaction that is possible by using the SyntEO approach. It shows how different synthetic data sets and changes to their generating parameters trigger different training results in a deep learning model. That way, an optimal ontology can be formulated by iterating through a dialogue between the human domain expert and the machine learning model.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The successive data sets each share the characteristics of the previous one and progressively increase in complexity. The data sets 1-3 and 3+, as shown in figure <a href="#S4.F9" title="Figure 9 ‣ 4.3 Synthetic Data Set Evolution ‣ 4 Offshore Wind Farm Detection with SyntEO ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, are created by stepwise enabling parts of the formulated ontology. Data set-1 shows only OWF of small size and near a land scene element without a coast scene element between land and sea. Also, the sea texture is not template data from the median Sentinel-1 data grid but a single constant value drawn from a uniform distribution. The overall size of the data set is 45,000 images. The handling of land without coast and a single value for sea data is maintained in data set-2. The changes for data set-2 were made by introducing the two additional size scales medium and large, to the OWF scene element. The overall size of data set-2 is 90,000 images with a 1/4-2/4-1/4 split for the size classes small, medium and large, respectively.</p>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
</div>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2112.02829/assets/figures/ds_evo_150.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="628" height="829" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S4.F9.3.2" class="ltx_text" style="font-size:90%;">Overview of the sequential synthetic data set evolution. From single sized (1) to multi-sized (2) OWFs and finally multi-sized OWFs in complex environments with template data for sea, land and coast plus non-target examples to minimise false detections (3,4). (1bc) show examples of procedural wind turbine textures used in data set 1-3, (4c) is a specific texture example for turbines on tidal flats, which are introduced in data set-3+.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Data set-3 shifts the focus from target characteristics to non-target characteristics to make a model trained on the data set sensible for false positives. In data set-3, the non-target scene element coast is included, and texture data for the scene element sea is no longer a single value but template data from the median Sentinel-1 data grid. Furthermore, specific non-target examples are generated which show no OWF but scene elements that, on different scales, share specific characteristics with the target OWF. These non-targets are scenes with rectangular fields and road networks that show a regular gird like pattern generated with template data. The other are examples that show rigs for e.g. oil and methane extraction as irregular clusters with a diameter of 200 m to 20 km of bright spots in front of darker sea texture.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">The last data set-3+ has the same scene elements and texture as data set-3 but an additional texture for a specific offshore wind turbine type. In the data sets 1-3 two turbine texture types were used, see figure <a href="#S4.F9" title="Figure 9 ‣ 4.3 Synthetic Data Set Evolution ‣ 4 Offshore Wind Farm Detection with SyntEO ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> 1b) and 1c). Both types show wind turbines in the open sea, whereas the X like pattern of 1c) is found in regions with a balanced number of Sentinel-1 acquisitions of ascending and descending orbits, like in the North Sea Basin. Similar numbers of acquisitions from both orbits are necessary for this particular pattern, which originates from Doppler effects that are clearly visible over open water. Whereas in the East China Sea, the 1b) version is more frequently found, which means acquisitions of one orbit dominate the median signal. However, the 4c) variant specially designed for data set-3+ shows a specific type of wind turbine that normally occurs near the coast in areas with a strong tidal influence. Due to the tidal flats, some acquisitions in the three-month temporal stack have images without water. These acquisitions dampen the occurrence of the Doppler effect to be visible in the image since over the tidal flats, the radar backscatter properties drastically change compared to open water. Therewith, the unique fingerprint of a typical offshore wind turbine is less distinct for examples in such areas. To represent these special cases, they were included in the data set-3+. For both data sets, 3 and 3+, the overall size is 90,000 training examples with a split of 1/6-1/3-1/6-1/6-1/6 for the OWF sizes small, medium and large and the non-targets rigs and land, respectively.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>CNN Model and Training</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The Faster R-CNN object detector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> with a ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> convolutional feature extractor was used as the only CNN architecture in each training. This specifically basic and well-established object detector in Earth observation was chosen to demonstrate that with a suitable data set, basic deep learning architectures can achieve sound results on the ground truth data. Therewith, in this study, we purposely shift the focus away from details in variations of architectures and training schemes towards different variants of the training data set. Thus the ability of SyntEO to support stable experiment environments by having control over the training data set becomes more obvious in this demonstration.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">The only difference in the Faster R-CNN configuration between the different trainings is the initial anchor size and ratio of the Region Proposal Network (RPN), which is characteristic of the Faster R-CNN architecture. Briefly described, the RPN is a submodule of the Faster R-CNN, which connects the ResNet-50 feature extractor with the Faster R-CNN object detector head. The RPN uses fixed anchor boxes to check for each anchor if it contains a potential object. Thereby, the anchor boxes can be imagined as uniformly distributed boxes over the last feature map that comes from the ResNet-50 backbone. Since potential objects might occur on different scales and with different aspects, for each anchor box location, <math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><mi id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><ci id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">n</annotation></semantics></math> anchor boxes are processed with different scales and aspect-ratios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. Normally, the number of boxes per anchor is defined by an expert due to an analysis of the training data set. In the case of SyntEO, the necessary information can directly be derived by consulting the ontology and synthetic image extent.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.6" class="ltx_p">In this example, the maximum scene extent is 20,480 m <math id="S4.SS4.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.p3.1.m1.1a"><mo id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><times id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">\times</annotation></semantics></math> 20,480 m, with a synthetic sensor resolution of 10 m, equal to the ground sampling distance of Sentinel-1 IW GRD products, each synthetic image has a dimension of 2048 <math id="S4.SS4.p3.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.p3.2.m2.1a"><mo id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><times id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">\times</annotation></semantics></math> 2048 pixels. The available hardware for this study is a NVIDIA RTX-2080Ti GPU with 11 GB memory. Since training on the full resolution of 2048 <math id="S4.SS4.p3.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.p3.3.m3.1a"><mo id="S4.SS4.p3.3.m3.1.1" xref="S4.SS4.p3.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.3.m3.1b"><times id="S4.SS4.p3.3.m3.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.3.m3.1c">\times</annotation></semantics></math> 2048 pixels would result in a memory overflow, the earlier discussed second variant of defining the synthetic image extent for training was chosen. The reason is that large OWFs often have a dimension of 17 km in diameter and should appear in a single training example. Nevertheless, since 2048 <math id="S4.SS4.p3.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.p3.4.m4.1a"><mo id="S4.SS4.p3.4.m4.1.1" xref="S4.SS4.p3.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.4.m4.1b"><times id="S4.SS4.p3.4.m4.1.1.cmml" xref="S4.SS4.p3.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.4.m4.1c">\times</annotation></semantics></math> 2048 pixels are too large for the given hardware constraints, the images are getting downscaled before training and therewith accepting the loss of small scale features. Finally, the image extent of a training example and the CNN model’s input dimension are 1024 <math id="S4.SS4.p3.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.p3.5.m5.1a"><mo id="S4.SS4.p3.5.m5.1.1" xref="S4.SS4.p3.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.5.m5.1b"><times id="S4.SS4.p3.5.m5.1.1.cmml" xref="S4.SS4.p3.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.5.m5.1c">\times</annotation></semantics></math> 1024 pixels, which cover the full synthetic scene extent of 20,480 m <math id="S4.SS4.p3.6.m6.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.p3.6.m6.1a"><mo id="S4.SS4.p3.6.m6.1.1" xref="S4.SS4.p3.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.6.m6.1b"><times id="S4.SS4.p3.6.m6.1.1.cmml" xref="S4.SS4.p3.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.6.m6.1c">\times</annotation></semantics></math> 20,480 m.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.2" class="ltx_p">By looking into the ontology, three maximum [5 km, 10 km, 17 km] and two minimum [1200 m, 2400 m] sizes can be extracted as major OWF extents. With a spatial resolution of 10 m <math id="S4.SS4.p4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.p4.1.m1.1a"><mo id="S4.SS4.p4.1.m1.1.1" xref="S4.SS4.p4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.1.m1.1b"><times id="S4.SS4.p4.1.m1.1.1.cmml" xref="S4.SS4.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.1.m1.1c">\times</annotation></semantics></math> 10 m and to allow easier calculation in this example, the scale values get slightly adjusted to [128, 256, 512, 1024, 1792]. Since these pixel dimensions describe the size of the targets in the synthetic image but not in the feature map which enters the RPN, the downscaling factor of the ResNet-50 backbone, the stride, must be taken into account. For the given ResNet-50 architecture, the stride is 16. Furthermore, the size of a scale one anchor box is set to 4 <math id="S4.SS4.p4.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.p4.2.m2.1a"><mo id="S4.SS4.p4.2.m2.1.1" xref="S4.SS4.p4.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.2.m2.1b"><times id="S4.SS4.p4.2.m2.1.1.cmml" xref="S4.SS4.p4.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.2.m2.1c">\times</annotation></semantics></math> 4 pixels in this example. Now, the following equation can be used to calculate scale factors for each object size derived from the ontology:</p>
</div>
<div id="S4.SS4.p5" class="ltx_para">
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.1" class="ltx_Math" alttext="\mathcal{A}_{\mathrm{scale}}=\sqrt{\mathcal{T}_{h}\mathcal{T}_{w}\frac{\mathcal{M}_{h}\mathcal{M}_{w}}{\mathcal{I}_{h}\mathcal{I}_{w}}}\times\frac{1}{s\sqrt{\mathcal{A}_{h}\mathcal{A}_{w}}}" display="block"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml"><msub id="S4.E1.m1.1.1.2" xref="S4.E1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.2.2" xref="S4.E1.m1.1.1.2.2.cmml">𝒜</mi><mi id="S4.E1.m1.1.1.2.3" xref="S4.E1.m1.1.1.2.3.cmml">scale</mi></msub><mo id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.cmml">=</mo><mrow id="S4.E1.m1.1.1.3" xref="S4.E1.m1.1.1.3.cmml"><msqrt id="S4.E1.m1.1.1.3.2" xref="S4.E1.m1.1.1.3.2.cmml"><mrow id="S4.E1.m1.1.1.3.2.2" xref="S4.E1.m1.1.1.3.2.2.cmml"><msub id="S4.E1.m1.1.1.3.2.2.2" xref="S4.E1.m1.1.1.3.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.3.2.2.2.2" xref="S4.E1.m1.1.1.3.2.2.2.2.cmml">𝒯</mi><mi id="S4.E1.m1.1.1.3.2.2.2.3" xref="S4.E1.m1.1.1.3.2.2.2.3.cmml">h</mi></msub><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.2.2.1" xref="S4.E1.m1.1.1.3.2.2.1.cmml">​</mo><msub id="S4.E1.m1.1.1.3.2.2.3" xref="S4.E1.m1.1.1.3.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.3.2.2.3.2" xref="S4.E1.m1.1.1.3.2.2.3.2.cmml">𝒯</mi><mi id="S4.E1.m1.1.1.3.2.2.3.3" xref="S4.E1.m1.1.1.3.2.2.3.3.cmml">w</mi></msub><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.2.2.1a" xref="S4.E1.m1.1.1.3.2.2.1.cmml">​</mo><mfrac id="S4.E1.m1.1.1.3.2.2.4" xref="S4.E1.m1.1.1.3.2.2.4.cmml"><mrow id="S4.E1.m1.1.1.3.2.2.4.2" xref="S4.E1.m1.1.1.3.2.2.4.2.cmml"><msub id="S4.E1.m1.1.1.3.2.2.4.2.2" xref="S4.E1.m1.1.1.3.2.2.4.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.3.2.2.4.2.2.2" xref="S4.E1.m1.1.1.3.2.2.4.2.2.2.cmml">ℳ</mi><mi id="S4.E1.m1.1.1.3.2.2.4.2.2.3" xref="S4.E1.m1.1.1.3.2.2.4.2.2.3.cmml">h</mi></msub><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.2.2.4.2.1" xref="S4.E1.m1.1.1.3.2.2.4.2.1.cmml">​</mo><msub id="S4.E1.m1.1.1.3.2.2.4.2.3" xref="S4.E1.m1.1.1.3.2.2.4.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.3.2.2.4.2.3.2" xref="S4.E1.m1.1.1.3.2.2.4.2.3.2.cmml">ℳ</mi><mi id="S4.E1.m1.1.1.3.2.2.4.2.3.3" xref="S4.E1.m1.1.1.3.2.2.4.2.3.3.cmml">w</mi></msub></mrow><mrow id="S4.E1.m1.1.1.3.2.2.4.3" xref="S4.E1.m1.1.1.3.2.2.4.3.cmml"><msub id="S4.E1.m1.1.1.3.2.2.4.3.2" xref="S4.E1.m1.1.1.3.2.2.4.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.3.2.2.4.3.2.2" xref="S4.E1.m1.1.1.3.2.2.4.3.2.2.cmml">ℐ</mi><mi id="S4.E1.m1.1.1.3.2.2.4.3.2.3" xref="S4.E1.m1.1.1.3.2.2.4.3.2.3.cmml">h</mi></msub><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.2.2.4.3.1" xref="S4.E1.m1.1.1.3.2.2.4.3.1.cmml">​</mo><msub id="S4.E1.m1.1.1.3.2.2.4.3.3" xref="S4.E1.m1.1.1.3.2.2.4.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.3.2.2.4.3.3.2" xref="S4.E1.m1.1.1.3.2.2.4.3.3.2.cmml">ℐ</mi><mi id="S4.E1.m1.1.1.3.2.2.4.3.3.3" xref="S4.E1.m1.1.1.3.2.2.4.3.3.3.cmml">w</mi></msub></mrow></mfrac></mrow></msqrt><mo lspace="0.222em" rspace="0.222em" id="S4.E1.m1.1.1.3.1" xref="S4.E1.m1.1.1.3.1.cmml">×</mo><mfrac id="S4.E1.m1.1.1.3.3" xref="S4.E1.m1.1.1.3.3.cmml"><mn id="S4.E1.m1.1.1.3.3.2" xref="S4.E1.m1.1.1.3.3.2.cmml">1</mn><mrow id="S4.E1.m1.1.1.3.3.3" xref="S4.E1.m1.1.1.3.3.3.cmml"><mi id="S4.E1.m1.1.1.3.3.3.2" xref="S4.E1.m1.1.1.3.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.3.3.1" xref="S4.E1.m1.1.1.3.3.3.1.cmml">​</mo><msqrt id="S4.E1.m1.1.1.3.3.3.3" xref="S4.E1.m1.1.1.3.3.3.3.cmml"><mrow id="S4.E1.m1.1.1.3.3.3.3.2" xref="S4.E1.m1.1.1.3.3.3.3.2.cmml"><msub id="S4.E1.m1.1.1.3.3.3.3.2.2" xref="S4.E1.m1.1.1.3.3.3.3.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.3.3.3.3.2.2.2" xref="S4.E1.m1.1.1.3.3.3.3.2.2.2.cmml">𝒜</mi><mi id="S4.E1.m1.1.1.3.3.3.3.2.2.3" xref="S4.E1.m1.1.1.3.3.3.3.2.2.3.cmml">h</mi></msub><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.3.3.3.2.1" xref="S4.E1.m1.1.1.3.3.3.3.2.1.cmml">​</mo><msub id="S4.E1.m1.1.1.3.3.3.3.2.3" xref="S4.E1.m1.1.1.3.3.3.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.3.3.3.3.2.3.2" xref="S4.E1.m1.1.1.3.3.3.3.2.3.2.cmml">𝒜</mi><mi id="S4.E1.m1.1.1.3.3.3.3.2.3.3" xref="S4.E1.m1.1.1.3.3.3.3.2.3.3.cmml">w</mi></msub></mrow></msqrt></mrow></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"><eq id="S4.E1.m1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"></eq><apply id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.2">subscript</csymbol><ci id="S4.E1.m1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.2.2">𝒜</ci><ci id="S4.E1.m1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.2.3">scale</ci></apply><apply id="S4.E1.m1.1.1.3.cmml" xref="S4.E1.m1.1.1.3"><times id="S4.E1.m1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.3.1"></times><apply id="S4.E1.m1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.3.2"><root id="S4.E1.m1.1.1.3.2a.cmml" xref="S4.E1.m1.1.1.3.2"></root><apply id="S4.E1.m1.1.1.3.2.2.cmml" xref="S4.E1.m1.1.1.3.2.2"><times id="S4.E1.m1.1.1.3.2.2.1.cmml" xref="S4.E1.m1.1.1.3.2.2.1"></times><apply id="S4.E1.m1.1.1.3.2.2.2.cmml" xref="S4.E1.m1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.2.2.2.1.cmml" xref="S4.E1.m1.1.1.3.2.2.2">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.2.2.2.cmml" xref="S4.E1.m1.1.1.3.2.2.2.2">𝒯</ci><ci id="S4.E1.m1.1.1.3.2.2.2.3.cmml" xref="S4.E1.m1.1.1.3.2.2.2.3">ℎ</ci></apply><apply id="S4.E1.m1.1.1.3.2.2.3.cmml" xref="S4.E1.m1.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.2.2.3.1.cmml" xref="S4.E1.m1.1.1.3.2.2.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.2.3.2.cmml" xref="S4.E1.m1.1.1.3.2.2.3.2">𝒯</ci><ci id="S4.E1.m1.1.1.3.2.2.3.3.cmml" xref="S4.E1.m1.1.1.3.2.2.3.3">𝑤</ci></apply><apply id="S4.E1.m1.1.1.3.2.2.4.cmml" xref="S4.E1.m1.1.1.3.2.2.4"><divide id="S4.E1.m1.1.1.3.2.2.4.1.cmml" xref="S4.E1.m1.1.1.3.2.2.4"></divide><apply id="S4.E1.m1.1.1.3.2.2.4.2.cmml" xref="S4.E1.m1.1.1.3.2.2.4.2"><times id="S4.E1.m1.1.1.3.2.2.4.2.1.cmml" xref="S4.E1.m1.1.1.3.2.2.4.2.1"></times><apply id="S4.E1.m1.1.1.3.2.2.4.2.2.cmml" xref="S4.E1.m1.1.1.3.2.2.4.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.2.2.4.2.2.1.cmml" xref="S4.E1.m1.1.1.3.2.2.4.2.2">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.2.4.2.2.2.cmml" xref="S4.E1.m1.1.1.3.2.2.4.2.2.2">ℳ</ci><ci id="S4.E1.m1.1.1.3.2.2.4.2.2.3.cmml" xref="S4.E1.m1.1.1.3.2.2.4.2.2.3">ℎ</ci></apply><apply id="S4.E1.m1.1.1.3.2.2.4.2.3.cmml" xref="S4.E1.m1.1.1.3.2.2.4.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.2.2.4.2.3.1.cmml" xref="S4.E1.m1.1.1.3.2.2.4.2.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.2.4.2.3.2.cmml" xref="S4.E1.m1.1.1.3.2.2.4.2.3.2">ℳ</ci><ci id="S4.E1.m1.1.1.3.2.2.4.2.3.3.cmml" xref="S4.E1.m1.1.1.3.2.2.4.2.3.3">𝑤</ci></apply></apply><apply id="S4.E1.m1.1.1.3.2.2.4.3.cmml" xref="S4.E1.m1.1.1.3.2.2.4.3"><times id="S4.E1.m1.1.1.3.2.2.4.3.1.cmml" xref="S4.E1.m1.1.1.3.2.2.4.3.1"></times><apply id="S4.E1.m1.1.1.3.2.2.4.3.2.cmml" xref="S4.E1.m1.1.1.3.2.2.4.3.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.2.2.4.3.2.1.cmml" xref="S4.E1.m1.1.1.3.2.2.4.3.2">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.2.4.3.2.2.cmml" xref="S4.E1.m1.1.1.3.2.2.4.3.2.2">ℐ</ci><ci id="S4.E1.m1.1.1.3.2.2.4.3.2.3.cmml" xref="S4.E1.m1.1.1.3.2.2.4.3.2.3">ℎ</ci></apply><apply id="S4.E1.m1.1.1.3.2.2.4.3.3.cmml" xref="S4.E1.m1.1.1.3.2.2.4.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.2.2.4.3.3.1.cmml" xref="S4.E1.m1.1.1.3.2.2.4.3.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.2.4.3.3.2.cmml" xref="S4.E1.m1.1.1.3.2.2.4.3.3.2">ℐ</ci><ci id="S4.E1.m1.1.1.3.2.2.4.3.3.3.cmml" xref="S4.E1.m1.1.1.3.2.2.4.3.3.3">𝑤</ci></apply></apply></apply></apply></apply><apply id="S4.E1.m1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.3.3"><divide id="S4.E1.m1.1.1.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3"></divide><cn type="integer" id="S4.E1.m1.1.1.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.2">1</cn><apply id="S4.E1.m1.1.1.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.3"><times id="S4.E1.m1.1.1.3.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3.3.1"></times><ci id="S4.E1.m1.1.1.3.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.3.2">𝑠</ci><apply id="S4.E1.m1.1.1.3.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.3.3"><root id="S4.E1.m1.1.1.3.3.3.3a.cmml" xref="S4.E1.m1.1.1.3.3.3.3"></root><apply id="S4.E1.m1.1.1.3.3.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.3.3.2"><times id="S4.E1.m1.1.1.3.3.3.3.2.1.cmml" xref="S4.E1.m1.1.1.3.3.3.3.2.1"></times><apply id="S4.E1.m1.1.1.3.3.3.3.2.2.cmml" xref="S4.E1.m1.1.1.3.3.3.3.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.3.3.3.2.2.1.cmml" xref="S4.E1.m1.1.1.3.3.3.3.2.2">subscript</csymbol><ci id="S4.E1.m1.1.1.3.3.3.3.2.2.2.cmml" xref="S4.E1.m1.1.1.3.3.3.3.2.2.2">𝒜</ci><ci id="S4.E1.m1.1.1.3.3.3.3.2.2.3.cmml" xref="S4.E1.m1.1.1.3.3.3.3.2.2.3">ℎ</ci></apply><apply id="S4.E1.m1.1.1.3.3.3.3.2.3.cmml" xref="S4.E1.m1.1.1.3.3.3.3.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.3.3.3.2.3.1.cmml" xref="S4.E1.m1.1.1.3.3.3.3.2.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.3.3.3.2.3.2.cmml" xref="S4.E1.m1.1.1.3.3.3.3.2.3.2">𝒜</ci><ci id="S4.E1.m1.1.1.3.3.3.3.2.3.3.cmml" xref="S4.E1.m1.1.1.3.3.3.3.2.3.3">𝑤</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">\mathcal{A}_{\mathrm{scale}}=\sqrt{\mathcal{T}_{h}\mathcal{T}_{w}\frac{\mathcal{M}_{h}\mathcal{M}_{w}}{\mathcal{I}_{h}\mathcal{I}_{w}}}\times\frac{1}{s\sqrt{\mathcal{A}_{h}\mathcal{A}_{w}}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS4.p6" class="ltx_para">
<p id="S4.SS4.p6.8" class="ltx_p">Where <math id="S4.SS4.p6.1.m1.1" class="ltx_Math" alttext="\mathcal{A}_{\mathrm{scale}}" display="inline"><semantics id="S4.SS4.p6.1.m1.1a"><msub id="S4.SS4.p6.1.m1.1.1" xref="S4.SS4.p6.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p6.1.m1.1.1.2" xref="S4.SS4.p6.1.m1.1.1.2.cmml">𝒜</mi><mi id="S4.SS4.p6.1.m1.1.1.3" xref="S4.SS4.p6.1.m1.1.1.3.cmml">scale</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p6.1.m1.1b"><apply id="S4.SS4.p6.1.m1.1.1.cmml" xref="S4.SS4.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p6.1.m1.1.1.1.cmml" xref="S4.SS4.p6.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p6.1.m1.1.1.2.cmml" xref="S4.SS4.p6.1.m1.1.1.2">𝒜</ci><ci id="S4.SS4.p6.1.m1.1.1.3.cmml" xref="S4.SS4.p6.1.m1.1.1.3">scale</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p6.1.m1.1c">\mathcal{A}_{\mathrm{scale}}</annotation></semantics></math> is the anchor box scale factor, <sub id="S4.SS4.p6.8.1" class="ltx_sub"><span id="S4.SS4.p6.8.1.1" class="ltx_text ltx_font_italic">h</span></sub> and <sub id="S4.SS4.p6.8.2" class="ltx_sub"><span id="S4.SS4.p6.8.2.1" class="ltx_text ltx_font_italic">w</span></sub> are height and width in pixel, <math id="S4.SS4.p6.4.m4.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="S4.SS4.p6.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p6.4.m4.1.1" xref="S4.SS4.p6.4.m4.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p6.4.m4.1b"><ci id="S4.SS4.p6.4.m4.1.1.cmml" xref="S4.SS4.p6.4.m4.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p6.4.m4.1c">\mathcal{T}</annotation></semantics></math> is the size of a target object, <math id="S4.SS4.p6.5.m5.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S4.SS4.p6.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p6.5.m5.1.1" xref="S4.SS4.p6.5.m5.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p6.5.m5.1b"><ci id="S4.SS4.p6.5.m5.1.1.cmml" xref="S4.SS4.p6.5.m5.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p6.5.m5.1c">\mathcal{M}</annotation></semantics></math> the input size of the CNN model, <math id="S4.SS4.p6.6.m6.1" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics id="S4.SS4.p6.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p6.6.m6.1.1" xref="S4.SS4.p6.6.m6.1.1.cmml">ℐ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p6.6.m6.1b"><ci id="S4.SS4.p6.6.m6.1.1.cmml" xref="S4.SS4.p6.6.m6.1.1">ℐ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p6.6.m6.1c">\mathcal{I}</annotation></semantics></math> the size of the synthetic image, <math id="S4.SS4.p6.7.m7.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S4.SS4.p6.7.m7.1a"><mi id="S4.SS4.p6.7.m7.1.1" xref="S4.SS4.p6.7.m7.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p6.7.m7.1b"><ci id="S4.SS4.p6.7.m7.1.1.cmml" xref="S4.SS4.p6.7.m7.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p6.7.m7.1c">s</annotation></semantics></math> the stride of the CNN feature extractor and <math id="S4.SS4.p6.8.m8.1" class="ltx_Math" alttext="\mathcal{A}_{h}\mathcal{A}_{w}" display="inline"><semantics id="S4.SS4.p6.8.m8.1a"><mrow id="S4.SS4.p6.8.m8.1.1" xref="S4.SS4.p6.8.m8.1.1.cmml"><msub id="S4.SS4.p6.8.m8.1.1.2" xref="S4.SS4.p6.8.m8.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p6.8.m8.1.1.2.2" xref="S4.SS4.p6.8.m8.1.1.2.2.cmml">𝒜</mi><mi id="S4.SS4.p6.8.m8.1.1.2.3" xref="S4.SS4.p6.8.m8.1.1.2.3.cmml">h</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS4.p6.8.m8.1.1.1" xref="S4.SS4.p6.8.m8.1.1.1.cmml">​</mo><msub id="S4.SS4.p6.8.m8.1.1.3" xref="S4.SS4.p6.8.m8.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p6.8.m8.1.1.3.2" xref="S4.SS4.p6.8.m8.1.1.3.2.cmml">𝒜</mi><mi id="S4.SS4.p6.8.m8.1.1.3.3" xref="S4.SS4.p6.8.m8.1.1.3.3.cmml">w</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p6.8.m8.1b"><apply id="S4.SS4.p6.8.m8.1.1.cmml" xref="S4.SS4.p6.8.m8.1.1"><times id="S4.SS4.p6.8.m8.1.1.1.cmml" xref="S4.SS4.p6.8.m8.1.1.1"></times><apply id="S4.SS4.p6.8.m8.1.1.2.cmml" xref="S4.SS4.p6.8.m8.1.1.2"><csymbol cd="ambiguous" id="S4.SS4.p6.8.m8.1.1.2.1.cmml" xref="S4.SS4.p6.8.m8.1.1.2">subscript</csymbol><ci id="S4.SS4.p6.8.m8.1.1.2.2.cmml" xref="S4.SS4.p6.8.m8.1.1.2.2">𝒜</ci><ci id="S4.SS4.p6.8.m8.1.1.2.3.cmml" xref="S4.SS4.p6.8.m8.1.1.2.3">ℎ</ci></apply><apply id="S4.SS4.p6.8.m8.1.1.3.cmml" xref="S4.SS4.p6.8.m8.1.1.3"><csymbol cd="ambiguous" id="S4.SS4.p6.8.m8.1.1.3.1.cmml" xref="S4.SS4.p6.8.m8.1.1.3">subscript</csymbol><ci id="S4.SS4.p6.8.m8.1.1.3.2.cmml" xref="S4.SS4.p6.8.m8.1.1.3.2">𝒜</ci><ci id="S4.SS4.p6.8.m8.1.1.3.3.cmml" xref="S4.SS4.p6.8.m8.1.1.3.3">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p6.8.m8.1c">\mathcal{A}_{h}\mathcal{A}_{w}</annotation></semantics></math> height and width of the anchor box of scale 1. By applying (<a href="#S4.E1" title="Equation 1 ‣ 4.4 CNN Model and Training ‣ 4 Offshore Wind Farm Detection with SyntEO ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) for the above given target object sizes, data set-1 has the scale factors [0.25, 0.5, 1] and all other data sets have the scale factors [0.25, 0.5, 1, 2, 3.5], the aspect ratios are same for all [0.5, 1, 2].</p>
</div>
<div id="S4.SS4.p7" class="ltx_para">
<p id="S4.SS4.p7.2" class="ltx_p">The training was done on four parallel NVIDIA RTX-2080Ti GPUs by using the NVIDIA docker implementation of the TensorFlow deep learning framework. The .png images and .xml annotation files were parsed to the TFRecord binary format to enable TensorFlow’s data API. The images in the TFRecord files have the size of 2048 <math id="S4.SS4.p7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.p7.1.m1.1a"><mo id="S4.SS4.p7.1.m1.1.1" xref="S4.SS4.p7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p7.1.m1.1b"><times id="S4.SS4.p7.1.m1.1.1.cmml" xref="S4.SS4.p7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p7.1.m1.1c">\times</annotation></semantics></math> 2048 pixels and are downscaled to an input size of 1024 <math id="S4.SS4.p7.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.p7.2.m2.1a"><mo id="S4.SS4.p7.2.m2.1.1" xref="S4.SS4.p7.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p7.2.m2.1b"><times id="S4.SS4.p7.2.m2.1.1.cmml" xref="S4.SS4.p7.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p7.2.m2.1c">\times</annotation></semantics></math> 1024 pixels on the fly during training. Eight training shards in TFRecord format hold 95% of all examples, whereas the single validation shard contains the remaining 5% for each synthetic data set. The test data set is not part of the synthetic data set but is the independent, manually labelled ground truth data set of the four introduced test sites. Further hyperparameter settings during training are a momentum optimiser with a momentum of 0.9 and a learning rate with a cosine decay <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> with a base of 0.01 over 10,687 or 21,375 training steps and a batch size of 4 for data set-1 and all other data sets, respectively.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Test Site Prediction and Metrics</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.2" class="ltx_p">After training, each model was exported to predict the ground truth (GT) data set. Therefore, the GT tiles were split into chips of size 2048 <math id="S4.SS5.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS5.p1.1.m1.1a"><mo id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><times id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">\times</annotation></semantics></math> 2048 with a 50% overlap, which are again resized to 1024 <math id="S4.SS5.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS5.p1.2.m2.1a"><mo id="S4.SS5.p1.2.m2.1.1" xref="S4.SS5.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.2.m2.1b"><times id="S4.SS5.p1.2.m2.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.2.m2.1c">\times</annotation></semantics></math> 1024 during prediction. A non-maximum suppression with a threshold of 0.8 on the predicted score was applied for all bounding boxes. Since some OWF clusters are very close in the ground truth data and spread over multiple image chips, the remaining bounding boxes were aggregated by converting their coordinates to the geographic reference system WGS84 and merging them in a cascading manner by starting with the highest prediction score and a minimum IoU of 0.333, see (<a href="#S4.E2" title="Equation 2 ‣ 4.5 Test Site Prediction and Metrics ‣ 4 Offshore Wind Farm Detection with SyntEO ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), with the next bounding box. That way, an accurate description of the estimated location and shape by the trained model over multiple images on the ground truth data is maintained. The polygons created by this approach are the final result of the test data. They are saved as .geojson file and used to calculate metrics for the accuracy assessment and model performance investigation.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">For model evaluation and comparison, the following metrics where calculated:</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.2" class="ltx_Math" alttext="\textrm{IoU}=\frac{\textrm{area}(y_{\textrm{gt}}\cap y_{\textrm{pred}})}{\textrm{area}(y_{\textrm{gt}}\cup y_{\textrm{pred}})}" display="block"><semantics id="S4.E2.m1.2a"><mrow id="S4.E2.m1.2.3" xref="S4.E2.m1.2.3.cmml"><mtext id="S4.E2.m1.2.3.2" xref="S4.E2.m1.2.3.2a.cmml">IoU</mtext><mo id="S4.E2.m1.2.3.1" xref="S4.E2.m1.2.3.1.cmml">=</mo><mfrac id="S4.E2.m1.2.2" xref="S4.E2.m1.2.2.cmml"><mrow id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.cmml"><mtext id="S4.E2.m1.1.1.1.3" xref="S4.E2.m1.1.1.1.3a.cmml">area</mtext><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.2" xref="S4.E2.m1.1.1.1.2.cmml">​</mo><mrow id="S4.E2.m1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E2.m1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E2.m1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.cmml"><msub id="S4.E2.m1.1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.2.2" xref="S4.E2.m1.1.1.1.1.1.1.2.2.cmml">y</mi><mtext id="S4.E2.m1.1.1.1.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.1.1.2.3a.cmml">gt</mtext></msub><mo id="S4.E2.m1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.cmml">∩</mo><msub id="S4.E2.m1.1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.1.3.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.3.2" xref="S4.E2.m1.1.1.1.1.1.1.3.2.cmml">y</mi><mtext id="S4.E2.m1.1.1.1.1.1.1.3.3" xref="S4.E2.m1.1.1.1.1.1.1.3.3a.cmml">pred</mtext></msub></mrow><mo stretchy="false" id="S4.E2.m1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mrow id="S4.E2.m1.2.2.2" xref="S4.E2.m1.2.2.2.cmml"><mtext id="S4.E2.m1.2.2.2.3" xref="S4.E2.m1.2.2.2.3a.cmml">area</mtext><mo lspace="0em" rspace="0em" id="S4.E2.m1.2.2.2.2" xref="S4.E2.m1.2.2.2.2.cmml">​</mo><mrow id="S4.E2.m1.2.2.2.1.1" xref="S4.E2.m1.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S4.E2.m1.2.2.2.1.1.2" xref="S4.E2.m1.2.2.2.1.1.1.cmml">(</mo><mrow id="S4.E2.m1.2.2.2.1.1.1" xref="S4.E2.m1.2.2.2.1.1.1.cmml"><msub id="S4.E2.m1.2.2.2.1.1.1.2" xref="S4.E2.m1.2.2.2.1.1.1.2.cmml"><mi id="S4.E2.m1.2.2.2.1.1.1.2.2" xref="S4.E2.m1.2.2.2.1.1.1.2.2.cmml">y</mi><mtext id="S4.E2.m1.2.2.2.1.1.1.2.3" xref="S4.E2.m1.2.2.2.1.1.1.2.3a.cmml">gt</mtext></msub><mo id="S4.E2.m1.2.2.2.1.1.1.1" xref="S4.E2.m1.2.2.2.1.1.1.1.cmml">∪</mo><msub id="S4.E2.m1.2.2.2.1.1.1.3" xref="S4.E2.m1.2.2.2.1.1.1.3.cmml"><mi id="S4.E2.m1.2.2.2.1.1.1.3.2" xref="S4.E2.m1.2.2.2.1.1.1.3.2.cmml">y</mi><mtext id="S4.E2.m1.2.2.2.1.1.1.3.3" xref="S4.E2.m1.2.2.2.1.1.1.3.3a.cmml">pred</mtext></msub></mrow><mo stretchy="false" id="S4.E2.m1.2.2.2.1.1.3" xref="S4.E2.m1.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.2b"><apply id="S4.E2.m1.2.3.cmml" xref="S4.E2.m1.2.3"><eq id="S4.E2.m1.2.3.1.cmml" xref="S4.E2.m1.2.3.1"></eq><ci id="S4.E2.m1.2.3.2a.cmml" xref="S4.E2.m1.2.3.2"><mtext id="S4.E2.m1.2.3.2.cmml" xref="S4.E2.m1.2.3.2">IoU</mtext></ci><apply id="S4.E2.m1.2.2.cmml" xref="S4.E2.m1.2.2"><divide id="S4.E2.m1.2.2.3.cmml" xref="S4.E2.m1.2.2"></divide><apply id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"><times id="S4.E2.m1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.2"></times><ci id="S4.E2.m1.1.1.1.3a.cmml" xref="S4.E2.m1.1.1.1.3"><mtext id="S4.E2.m1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.3">area</mtext></ci><apply id="S4.E2.m1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1"><intersect id="S4.E2.m1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1"></intersect><apply id="S4.E2.m1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.2">𝑦</ci><ci id="S4.E2.m1.1.1.1.1.1.1.2.3a.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.3"><mtext mathsize="70%" id="S4.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2.3">gt</mtext></ci></apply><apply id="S4.E2.m1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.2">𝑦</ci><ci id="S4.E2.m1.1.1.1.1.1.1.3.3a.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.3"><mtext mathsize="70%" id="S4.E2.m1.1.1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3.3">pred</mtext></ci></apply></apply></apply><apply id="S4.E2.m1.2.2.2.cmml" xref="S4.E2.m1.2.2.2"><times id="S4.E2.m1.2.2.2.2.cmml" xref="S4.E2.m1.2.2.2.2"></times><ci id="S4.E2.m1.2.2.2.3a.cmml" xref="S4.E2.m1.2.2.2.3"><mtext id="S4.E2.m1.2.2.2.3.cmml" xref="S4.E2.m1.2.2.2.3">area</mtext></ci><apply id="S4.E2.m1.2.2.2.1.1.1.cmml" xref="S4.E2.m1.2.2.2.1.1"><union id="S4.E2.m1.2.2.2.1.1.1.1.cmml" xref="S4.E2.m1.2.2.2.1.1.1.1"></union><apply id="S4.E2.m1.2.2.2.1.1.1.2.cmml" xref="S4.E2.m1.2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.2.2.2.1.1.1.2.1.cmml" xref="S4.E2.m1.2.2.2.1.1.1.2">subscript</csymbol><ci id="S4.E2.m1.2.2.2.1.1.1.2.2.cmml" xref="S4.E2.m1.2.2.2.1.1.1.2.2">𝑦</ci><ci id="S4.E2.m1.2.2.2.1.1.1.2.3a.cmml" xref="S4.E2.m1.2.2.2.1.1.1.2.3"><mtext mathsize="70%" id="S4.E2.m1.2.2.2.1.1.1.2.3.cmml" xref="S4.E2.m1.2.2.2.1.1.1.2.3">gt</mtext></ci></apply><apply id="S4.E2.m1.2.2.2.1.1.1.3.cmml" xref="S4.E2.m1.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.2.2.2.1.1.1.3.1.cmml" xref="S4.E2.m1.2.2.2.1.1.1.3">subscript</csymbol><ci id="S4.E2.m1.2.2.2.1.1.1.3.2.cmml" xref="S4.E2.m1.2.2.2.1.1.1.3.2">𝑦</ci><ci id="S4.E2.m1.2.2.2.1.1.1.3.3a.cmml" xref="S4.E2.m1.2.2.2.1.1.1.3.3"><mtext mathsize="70%" id="S4.E2.m1.2.2.2.1.1.1.3.3.cmml" xref="S4.E2.m1.2.2.2.1.1.1.3.3">pred</mtext></ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.2c">\textrm{IoU}=\frac{\textrm{area}(y_{\textrm{gt}}\cap y_{\textrm{pred}})}{\textrm{area}(y_{\textrm{gt}}\cup y_{\textrm{pred}})}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.1" class="ltx_p">Upon the IoU, the threshold <math id="S4.SS5.p4.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S4.SS5.p4.1.m1.1a"><mi id="S4.SS5.p4.1.m1.1.1" xref="S4.SS5.p4.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p4.1.m1.1b"><ci id="S4.SS5.p4.1.m1.1.1.cmml" xref="S4.SS5.p4.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p4.1.m1.1c">\tau</annotation></semantics></math> of 0.33 is applied to define whether a prediction is true positive (TP) or false positive (FP). The threshold reflects the technical difference of ground truth bounding boxes and predicted shapes. Where predictions are tight polygons with irregular shapes around the OWFs, ground truth are rectangular bounding boxes which were automatically derived from the single turbine point clusters. In dynamic areas like the East China Sea, these bounding boxes cover a significantly larger area than the predicted polygons. Thus, a lower IoU is better suited to balance the different shapes of predictions and labels. Now, the performance can be measured by precision Pr, recall Rc and F1 score:</p>
</div>
<div id="S4.SS5.p5" class="ltx_para">
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.1" class="ltx_Math" alttext="\mathrm{Pr}=\frac{\mathrm{TP}}{\mathrm{TP+FP}}" display="block"><semantics id="S4.E3.m1.1a"><mrow id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml"><mi id="S4.E3.m1.1.1.2" xref="S4.E3.m1.1.1.2.cmml">Pr</mi><mo id="S4.E3.m1.1.1.1" xref="S4.E3.m1.1.1.1.cmml">=</mo><mfrac id="S4.E3.m1.1.1.3" xref="S4.E3.m1.1.1.3.cmml"><mi id="S4.E3.m1.1.1.3.2" xref="S4.E3.m1.1.1.3.2.cmml">TP</mi><mrow id="S4.E3.m1.1.1.3.3" xref="S4.E3.m1.1.1.3.3.cmml"><mi id="S4.E3.m1.1.1.3.3.2" xref="S4.E3.m1.1.1.3.3.2.cmml">TP</mi><mo id="S4.E3.m1.1.1.3.3.1" xref="S4.E3.m1.1.1.3.3.1.cmml">+</mo><mi id="S4.E3.m1.1.1.3.3.3" xref="S4.E3.m1.1.1.3.3.3.cmml">FP</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.1b"><apply id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1"><eq id="S4.E3.m1.1.1.1.cmml" xref="S4.E3.m1.1.1.1"></eq><ci id="S4.E3.m1.1.1.2.cmml" xref="S4.E3.m1.1.1.2">Pr</ci><apply id="S4.E3.m1.1.1.3.cmml" xref="S4.E3.m1.1.1.3"><divide id="S4.E3.m1.1.1.3.1.cmml" xref="S4.E3.m1.1.1.3"></divide><ci id="S4.E3.m1.1.1.3.2.cmml" xref="S4.E3.m1.1.1.3.2">TP</ci><apply id="S4.E3.m1.1.1.3.3.cmml" xref="S4.E3.m1.1.1.3.3"><plus id="S4.E3.m1.1.1.3.3.1.cmml" xref="S4.E3.m1.1.1.3.3.1"></plus><ci id="S4.E3.m1.1.1.3.3.2.cmml" xref="S4.E3.m1.1.1.3.3.2">TP</ci><ci id="S4.E3.m1.1.1.3.3.3.cmml" xref="S4.E3.m1.1.1.3.3.3">FP</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.1c">\mathrm{Pr}=\frac{\mathrm{TP}}{\mathrm{TP+FP}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS5.p6" class="ltx_para">
<table id="S4.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E4.m1.1" class="ltx_Math" alttext="\mathrm{Rc}=\frac{\mathrm{TP}}{\mathrm{TP+FN}}" display="block"><semantics id="S4.E4.m1.1a"><mrow id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml"><mi id="S4.E4.m1.1.1.2" xref="S4.E4.m1.1.1.2.cmml">Rc</mi><mo id="S4.E4.m1.1.1.1" xref="S4.E4.m1.1.1.1.cmml">=</mo><mfrac id="S4.E4.m1.1.1.3" xref="S4.E4.m1.1.1.3.cmml"><mi id="S4.E4.m1.1.1.3.2" xref="S4.E4.m1.1.1.3.2.cmml">TP</mi><mrow id="S4.E4.m1.1.1.3.3" xref="S4.E4.m1.1.1.3.3.cmml"><mi id="S4.E4.m1.1.1.3.3.2" xref="S4.E4.m1.1.1.3.3.2.cmml">TP</mi><mo id="S4.E4.m1.1.1.3.3.1" xref="S4.E4.m1.1.1.3.3.1.cmml">+</mo><mi id="S4.E4.m1.1.1.3.3.3" xref="S4.E4.m1.1.1.3.3.3.cmml">FN</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.1b"><apply id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1"><eq id="S4.E4.m1.1.1.1.cmml" xref="S4.E4.m1.1.1.1"></eq><ci id="S4.E4.m1.1.1.2.cmml" xref="S4.E4.m1.1.1.2">Rc</ci><apply id="S4.E4.m1.1.1.3.cmml" xref="S4.E4.m1.1.1.3"><divide id="S4.E4.m1.1.1.3.1.cmml" xref="S4.E4.m1.1.1.3"></divide><ci id="S4.E4.m1.1.1.3.2.cmml" xref="S4.E4.m1.1.1.3.2">TP</ci><apply id="S4.E4.m1.1.1.3.3.cmml" xref="S4.E4.m1.1.1.3.3"><plus id="S4.E4.m1.1.1.3.3.1.cmml" xref="S4.E4.m1.1.1.3.3.1"></plus><ci id="S4.E4.m1.1.1.3.3.2.cmml" xref="S4.E4.m1.1.1.3.3.2">TP</ci><ci id="S4.E4.m1.1.1.3.3.3.cmml" xref="S4.E4.m1.1.1.3.3.3">FN</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.1c">\mathrm{Rc}=\frac{\mathrm{TP}}{\mathrm{TP+FN}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS5.p7" class="ltx_para">
<table id="S4.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E5.m1.1" class="ltx_Math" alttext="\mathrm{F1}=2\times\frac{\mathrm{Pr}\times\mathrm{Rc}}{\mathrm{Pr}+\mathrm{Rc}}" display="block"><semantics id="S4.E5.m1.1a"><mrow id="S4.E5.m1.1.1" xref="S4.E5.m1.1.1.cmml"><mi id="S4.E5.m1.1.1.2" xref="S4.E5.m1.1.1.2.cmml">F1</mi><mo id="S4.E5.m1.1.1.1" xref="S4.E5.m1.1.1.1.cmml">=</mo><mrow id="S4.E5.m1.1.1.3" xref="S4.E5.m1.1.1.3.cmml"><mn id="S4.E5.m1.1.1.3.2" xref="S4.E5.m1.1.1.3.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S4.E5.m1.1.1.3.1" xref="S4.E5.m1.1.1.3.1.cmml">×</mo><mfrac id="S4.E5.m1.1.1.3.3" xref="S4.E5.m1.1.1.3.3.cmml"><mrow id="S4.E5.m1.1.1.3.3.2" xref="S4.E5.m1.1.1.3.3.2.cmml"><mi id="S4.E5.m1.1.1.3.3.2.2" xref="S4.E5.m1.1.1.3.3.2.2.cmml">Pr</mi><mo lspace="0.222em" rspace="0.222em" id="S4.E5.m1.1.1.3.3.2.1" xref="S4.E5.m1.1.1.3.3.2.1.cmml">×</mo><mi id="S4.E5.m1.1.1.3.3.2.3" xref="S4.E5.m1.1.1.3.3.2.3.cmml">Rc</mi></mrow><mrow id="S4.E5.m1.1.1.3.3.3" xref="S4.E5.m1.1.1.3.3.3.cmml"><mi id="S4.E5.m1.1.1.3.3.3.2" xref="S4.E5.m1.1.1.3.3.3.2.cmml">Pr</mi><mo id="S4.E5.m1.1.1.3.3.3.1" xref="S4.E5.m1.1.1.3.3.3.1.cmml">+</mo><mi id="S4.E5.m1.1.1.3.3.3.3" xref="S4.E5.m1.1.1.3.3.3.3.cmml">Rc</mi></mrow></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E5.m1.1b"><apply id="S4.E5.m1.1.1.cmml" xref="S4.E5.m1.1.1"><eq id="S4.E5.m1.1.1.1.cmml" xref="S4.E5.m1.1.1.1"></eq><ci id="S4.E5.m1.1.1.2.cmml" xref="S4.E5.m1.1.1.2">F1</ci><apply id="S4.E5.m1.1.1.3.cmml" xref="S4.E5.m1.1.1.3"><times id="S4.E5.m1.1.1.3.1.cmml" xref="S4.E5.m1.1.1.3.1"></times><cn type="integer" id="S4.E5.m1.1.1.3.2.cmml" xref="S4.E5.m1.1.1.3.2">2</cn><apply id="S4.E5.m1.1.1.3.3.cmml" xref="S4.E5.m1.1.1.3.3"><divide id="S4.E5.m1.1.1.3.3.1.cmml" xref="S4.E5.m1.1.1.3.3"></divide><apply id="S4.E5.m1.1.1.3.3.2.cmml" xref="S4.E5.m1.1.1.3.3.2"><times id="S4.E5.m1.1.1.3.3.2.1.cmml" xref="S4.E5.m1.1.1.3.3.2.1"></times><ci id="S4.E5.m1.1.1.3.3.2.2.cmml" xref="S4.E5.m1.1.1.3.3.2.2">Pr</ci><ci id="S4.E5.m1.1.1.3.3.2.3.cmml" xref="S4.E5.m1.1.1.3.3.2.3">Rc</ci></apply><apply id="S4.E5.m1.1.1.3.3.3.cmml" xref="S4.E5.m1.1.1.3.3.3"><plus id="S4.E5.m1.1.1.3.3.3.1.cmml" xref="S4.E5.m1.1.1.3.3.3.1"></plus><ci id="S4.E5.m1.1.1.3.3.3.2.cmml" xref="S4.E5.m1.1.1.3.3.3.2">Pr</ci><ci id="S4.E5.m1.1.1.3.3.3.3.cmml" xref="S4.E5.m1.1.1.3.3.3.3">Rc</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.1c">\mathrm{F1}=2\times\frac{\mathrm{Pr}\times\mathrm{Rc}}{\mathrm{Pr}+\mathrm{Rc}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS5.p8" class="ltx_para">
<p id="S4.SS5.p8.4" class="ltx_p">To calculate the average precision AP a monotonic precision-recall curve, <math id="S4.SS5.p8.1.m1.1" class="ltx_Math" alttext="\mathrm{Pr}_{\mathrm{interp}}" display="inline"><semantics id="S4.SS5.p8.1.m1.1a"><msub id="S4.SS5.p8.1.m1.1.1" xref="S4.SS5.p8.1.m1.1.1.cmml"><mi id="S4.SS5.p8.1.m1.1.1.2" xref="S4.SS5.p8.1.m1.1.1.2.cmml">Pr</mi><mi id="S4.SS5.p8.1.m1.1.1.3" xref="S4.SS5.p8.1.m1.1.1.3.cmml">interp</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p8.1.m1.1b"><apply id="S4.SS5.p8.1.m1.1.1.cmml" xref="S4.SS5.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS5.p8.1.m1.1.1.1.cmml" xref="S4.SS5.p8.1.m1.1.1">subscript</csymbol><ci id="S4.SS5.p8.1.m1.1.1.2.cmml" xref="S4.SS5.p8.1.m1.1.1.2">Pr</ci><ci id="S4.SS5.p8.1.m1.1.1.3.cmml" xref="S4.SS5.p8.1.m1.1.1.3">interp</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p8.1.m1.1c">\mathrm{Pr}_{\mathrm{interp}}</annotation></semantics></math> is constructed by sorting all predictions indexed with <math id="S4.SS5.p8.2.m2.4" class="ltx_Math" alttext="k=1,2,...,K" display="inline"><semantics id="S4.SS5.p8.2.m2.4a"><mrow id="S4.SS5.p8.2.m2.4.5" xref="S4.SS5.p8.2.m2.4.5.cmml"><mi id="S4.SS5.p8.2.m2.4.5.2" xref="S4.SS5.p8.2.m2.4.5.2.cmml">k</mi><mo id="S4.SS5.p8.2.m2.4.5.1" xref="S4.SS5.p8.2.m2.4.5.1.cmml">=</mo><mrow id="S4.SS5.p8.2.m2.4.5.3.2" xref="S4.SS5.p8.2.m2.4.5.3.1.cmml"><mn id="S4.SS5.p8.2.m2.1.1" xref="S4.SS5.p8.2.m2.1.1.cmml">1</mn><mo id="S4.SS5.p8.2.m2.4.5.3.2.1" xref="S4.SS5.p8.2.m2.4.5.3.1.cmml">,</mo><mn id="S4.SS5.p8.2.m2.2.2" xref="S4.SS5.p8.2.m2.2.2.cmml">2</mn><mo id="S4.SS5.p8.2.m2.4.5.3.2.2" xref="S4.SS5.p8.2.m2.4.5.3.1.cmml">,</mo><mi mathvariant="normal" id="S4.SS5.p8.2.m2.3.3" xref="S4.SS5.p8.2.m2.3.3.cmml">…</mi><mo id="S4.SS5.p8.2.m2.4.5.3.2.3" xref="S4.SS5.p8.2.m2.4.5.3.1.cmml">,</mo><mi id="S4.SS5.p8.2.m2.4.4" xref="S4.SS5.p8.2.m2.4.4.cmml">K</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p8.2.m2.4b"><apply id="S4.SS5.p8.2.m2.4.5.cmml" xref="S4.SS5.p8.2.m2.4.5"><eq id="S4.SS5.p8.2.m2.4.5.1.cmml" xref="S4.SS5.p8.2.m2.4.5.1"></eq><ci id="S4.SS5.p8.2.m2.4.5.2.cmml" xref="S4.SS5.p8.2.m2.4.5.2">𝑘</ci><list id="S4.SS5.p8.2.m2.4.5.3.1.cmml" xref="S4.SS5.p8.2.m2.4.5.3.2"><cn type="integer" id="S4.SS5.p8.2.m2.1.1.cmml" xref="S4.SS5.p8.2.m2.1.1">1</cn><cn type="integer" id="S4.SS5.p8.2.m2.2.2.cmml" xref="S4.SS5.p8.2.m2.2.2">2</cn><ci id="S4.SS5.p8.2.m2.3.3.cmml" xref="S4.SS5.p8.2.m2.3.3">…</ci><ci id="S4.SS5.p8.2.m2.4.4.cmml" xref="S4.SS5.p8.2.m2.4.4">𝐾</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p8.2.m2.4c">k=1,2,...,K</annotation></semantics></math> in descending order of their score. For each <math id="S4.SS5.p8.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS5.p8.3.m3.1a"><mi id="S4.SS5.p8.3.m3.1.1" xref="S4.SS5.p8.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p8.3.m3.1b"><ci id="S4.SS5.p8.3.m3.1.1.cmml" xref="S4.SS5.p8.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p8.3.m3.1c">k</annotation></semantics></math> the subsequent precision-recall pair is calculated. Thereby, <math id="S4.SS5.p8.4.m4.1" class="ltx_Math" alttext="\mathrm{Pr}_{\mathrm{interp}}" display="inline"><semantics id="S4.SS5.p8.4.m4.1a"><msub id="S4.SS5.p8.4.m4.1.1" xref="S4.SS5.p8.4.m4.1.1.cmml"><mi id="S4.SS5.p8.4.m4.1.1.2" xref="S4.SS5.p8.4.m4.1.1.2.cmml">Pr</mi><mi id="S4.SS5.p8.4.m4.1.1.3" xref="S4.SS5.p8.4.m4.1.1.3.cmml">interp</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p8.4.m4.1b"><apply id="S4.SS5.p8.4.m4.1.1.cmml" xref="S4.SS5.p8.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS5.p8.4.m4.1.1.1.cmml" xref="S4.SS5.p8.4.m4.1.1">subscript</csymbol><ci id="S4.SS5.p8.4.m4.1.1.2.cmml" xref="S4.SS5.p8.4.m4.1.1.2">Pr</ci><ci id="S4.SS5.p8.4.m4.1.1.3.cmml" xref="S4.SS5.p8.4.m4.1.1.3">interp</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p8.4.m4.1c">\mathrm{Pr}_{\mathrm{interp}}</annotation></semantics></math> always corresponds to the next maximum in the ordered recall values, by using the all-point interpolation approach, see Padilla et al.<span id="S4.SS5.p8.4.1" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> for a detailed explanation:</p>
</div>
<div id="S4.SS5.p9" class="ltx_para">
<table id="S4.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E6.m1.1" class="ltx_Math" alttext="\mathrm{Pr}_{\mathrm{interp}}=\underset{\tilde{\mathrm{Rc}}\colon\tilde{\mathrm{Rc}}\geq\mathrm{Rc}}{\mathrm{max}}\ \mathrm{Pr}_{\mathrm{interp}}(\tilde{\mathrm{Rc}})" display="block"><semantics id="S4.E6.m1.1a"><mrow id="S4.E6.m1.1.2" xref="S4.E6.m1.1.2.cmml"><msub id="S4.E6.m1.1.2.2" xref="S4.E6.m1.1.2.2.cmml"><mi id="S4.E6.m1.1.2.2.2" xref="S4.E6.m1.1.2.2.2.cmml">Pr</mi><mi id="S4.E6.m1.1.2.2.3" xref="S4.E6.m1.1.2.2.3.cmml">interp</mi></msub><mo id="S4.E6.m1.1.2.1" xref="S4.E6.m1.1.2.1.cmml">=</mo><mrow id="S4.E6.m1.1.2.3" xref="S4.E6.m1.1.2.3.cmml"><munder accentunder="true" id="S4.E6.m1.1.2.3.2" xref="S4.E6.m1.1.2.3.2.cmml"><mi id="S4.E6.m1.1.2.3.2.2" xref="S4.E6.m1.1.2.3.2.2.cmml">max</mi><mrow id="S4.E6.m1.1.2.3.2.1" xref="S4.E6.m1.1.2.3.2.1.cmml"><mover accent="true" id="S4.E6.m1.1.2.3.2.1.2" xref="S4.E6.m1.1.2.3.2.1.2.cmml"><mi id="S4.E6.m1.1.2.3.2.1.2.2" xref="S4.E6.m1.1.2.3.2.1.2.2.cmml">Rc</mi><mo id="S4.E6.m1.1.2.3.2.1.2.1" xref="S4.E6.m1.1.2.3.2.1.2.1.cmml">~</mo></mover><mo lspace="0.278em" rspace="0.278em" id="S4.E6.m1.1.2.3.2.1.1" xref="S4.E6.m1.1.2.3.2.1.1.cmml">:</mo><mrow id="S4.E6.m1.1.2.3.2.1.3" xref="S4.E6.m1.1.2.3.2.1.3.cmml"><mover accent="true" id="S4.E6.m1.1.2.3.2.1.3.2" xref="S4.E6.m1.1.2.3.2.1.3.2.cmml"><mi id="S4.E6.m1.1.2.3.2.1.3.2.2" xref="S4.E6.m1.1.2.3.2.1.3.2.2.cmml">Rc</mi><mo id="S4.E6.m1.1.2.3.2.1.3.2.1" xref="S4.E6.m1.1.2.3.2.1.3.2.1.cmml">~</mo></mover><mo id="S4.E6.m1.1.2.3.2.1.3.1" xref="S4.E6.m1.1.2.3.2.1.3.1.cmml">≥</mo><mi id="S4.E6.m1.1.2.3.2.1.3.3" xref="S4.E6.m1.1.2.3.2.1.3.3.cmml">Rc</mi></mrow></mrow></munder><mo lspace="0em" rspace="0em" id="S4.E6.m1.1.2.3.1" xref="S4.E6.m1.1.2.3.1.cmml">​</mo><msub id="S4.E6.m1.1.2.3.3" xref="S4.E6.m1.1.2.3.3.cmml"><mi id="S4.E6.m1.1.2.3.3.2" xref="S4.E6.m1.1.2.3.3.2.cmml">Pr</mi><mi id="S4.E6.m1.1.2.3.3.3" xref="S4.E6.m1.1.2.3.3.3.cmml">interp</mi></msub><mo lspace="0em" rspace="0em" id="S4.E6.m1.1.2.3.1a" xref="S4.E6.m1.1.2.3.1.cmml">​</mo><mrow id="S4.E6.m1.1.2.3.4.2" xref="S4.E6.m1.1.1.cmml"><mo stretchy="false" id="S4.E6.m1.1.2.3.4.2.1" xref="S4.E6.m1.1.1.cmml">(</mo><mover accent="true" id="S4.E6.m1.1.1" xref="S4.E6.m1.1.1.cmml"><mi id="S4.E6.m1.1.1.2" xref="S4.E6.m1.1.1.2.cmml">Rc</mi><mo id="S4.E6.m1.1.1.1" xref="S4.E6.m1.1.1.1.cmml">~</mo></mover><mo stretchy="false" id="S4.E6.m1.1.2.3.4.2.2" xref="S4.E6.m1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E6.m1.1b"><apply id="S4.E6.m1.1.2.cmml" xref="S4.E6.m1.1.2"><eq id="S4.E6.m1.1.2.1.cmml" xref="S4.E6.m1.1.2.1"></eq><apply id="S4.E6.m1.1.2.2.cmml" xref="S4.E6.m1.1.2.2"><csymbol cd="ambiguous" id="S4.E6.m1.1.2.2.1.cmml" xref="S4.E6.m1.1.2.2">subscript</csymbol><ci id="S4.E6.m1.1.2.2.2.cmml" xref="S4.E6.m1.1.2.2.2">Pr</ci><ci id="S4.E6.m1.1.2.2.3.cmml" xref="S4.E6.m1.1.2.2.3">interp</ci></apply><apply id="S4.E6.m1.1.2.3.cmml" xref="S4.E6.m1.1.2.3"><times id="S4.E6.m1.1.2.3.1.cmml" xref="S4.E6.m1.1.2.3.1"></times><apply id="S4.E6.m1.1.2.3.2.cmml" xref="S4.E6.m1.1.2.3.2"><apply id="S4.E6.m1.1.2.3.2.1.cmml" xref="S4.E6.m1.1.2.3.2.1"><ci id="S4.E6.m1.1.2.3.2.1.1.cmml" xref="S4.E6.m1.1.2.3.2.1.1">:</ci><apply id="S4.E6.m1.1.2.3.2.1.2.cmml" xref="S4.E6.m1.1.2.3.2.1.2"><ci id="S4.E6.m1.1.2.3.2.1.2.1.cmml" xref="S4.E6.m1.1.2.3.2.1.2.1">~</ci><ci id="S4.E6.m1.1.2.3.2.1.2.2.cmml" xref="S4.E6.m1.1.2.3.2.1.2.2">Rc</ci></apply><apply id="S4.E6.m1.1.2.3.2.1.3.cmml" xref="S4.E6.m1.1.2.3.2.1.3"><geq id="S4.E6.m1.1.2.3.2.1.3.1.cmml" xref="S4.E6.m1.1.2.3.2.1.3.1"></geq><apply id="S4.E6.m1.1.2.3.2.1.3.2.cmml" xref="S4.E6.m1.1.2.3.2.1.3.2"><ci id="S4.E6.m1.1.2.3.2.1.3.2.1.cmml" xref="S4.E6.m1.1.2.3.2.1.3.2.1">~</ci><ci id="S4.E6.m1.1.2.3.2.1.3.2.2.cmml" xref="S4.E6.m1.1.2.3.2.1.3.2.2">Rc</ci></apply><ci id="S4.E6.m1.1.2.3.2.1.3.3.cmml" xref="S4.E6.m1.1.2.3.2.1.3.3">Rc</ci></apply></apply><ci id="S4.E6.m1.1.2.3.2.2.cmml" xref="S4.E6.m1.1.2.3.2.2">max</ci></apply><apply id="S4.E6.m1.1.2.3.3.cmml" xref="S4.E6.m1.1.2.3.3"><csymbol cd="ambiguous" id="S4.E6.m1.1.2.3.3.1.cmml" xref="S4.E6.m1.1.2.3.3">subscript</csymbol><ci id="S4.E6.m1.1.2.3.3.2.cmml" xref="S4.E6.m1.1.2.3.3.2">Pr</ci><ci id="S4.E6.m1.1.2.3.3.3.cmml" xref="S4.E6.m1.1.2.3.3.3">interp</ci></apply><apply id="S4.E6.m1.1.1.cmml" xref="S4.E6.m1.1.2.3.4.2"><ci id="S4.E6.m1.1.1.1.cmml" xref="S4.E6.m1.1.1.1">~</ci><ci id="S4.E6.m1.1.1.2.cmml" xref="S4.E6.m1.1.1.2">Rc</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m1.1c">\mathrm{Pr}_{\mathrm{interp}}=\underset{\tilde{\mathrm{Rc}}\colon\tilde{\mathrm{Rc}}\geq\mathrm{Rc}}{\mathrm{max}}\ \mathrm{Pr}_{\mathrm{interp}}(\tilde{\mathrm{Rc}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS5.p10" class="ltx_para">
<p id="S4.SS5.p10.1" class="ltx_p">In order to summarise the characteristics of the precision-recall curve in a single metric, the average precision AP is reported by calculating the Riemann integral of the precision-recall curve:</p>
</div>
<div id="S4.SS5.p11" class="ltx_para">
<table id="S4.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E7.m1.3" class="ltx_Math" alttext="\mathrm{AP}=\sum\limits_{k=1}^{K}({\mathrm{Rc}}(k)-{\mathrm{Rc}}(k-1))\times\mathrm{Pr}_{\mathrm{interp}}(\mathrm{Rc}(k))," display="block"><semantics id="S4.E7.m1.3a"><mrow id="S4.E7.m1.3.3.1" xref="S4.E7.m1.3.3.1.1.cmml"><mrow id="S4.E7.m1.3.3.1.1" xref="S4.E7.m1.3.3.1.1.cmml"><mi id="S4.E7.m1.3.3.1.1.4" xref="S4.E7.m1.3.3.1.1.4.cmml">AP</mi><mo rspace="0.111em" id="S4.E7.m1.3.3.1.1.3" xref="S4.E7.m1.3.3.1.1.3.cmml">=</mo><mrow id="S4.E7.m1.3.3.1.1.2" xref="S4.E7.m1.3.3.1.1.2.cmml"><munderover id="S4.E7.m1.3.3.1.1.2.3" xref="S4.E7.m1.3.3.1.1.2.3.cmml"><mo movablelimits="false" rspace="0em" id="S4.E7.m1.3.3.1.1.2.3.2.2" xref="S4.E7.m1.3.3.1.1.2.3.2.2.cmml">∑</mo><mrow id="S4.E7.m1.3.3.1.1.2.3.2.3" xref="S4.E7.m1.3.3.1.1.2.3.2.3.cmml"><mi id="S4.E7.m1.3.3.1.1.2.3.2.3.2" xref="S4.E7.m1.3.3.1.1.2.3.2.3.2.cmml">k</mi><mo id="S4.E7.m1.3.3.1.1.2.3.2.3.1" xref="S4.E7.m1.3.3.1.1.2.3.2.3.1.cmml">=</mo><mn id="S4.E7.m1.3.3.1.1.2.3.2.3.3" xref="S4.E7.m1.3.3.1.1.2.3.2.3.3.cmml">1</mn></mrow><mi id="S4.E7.m1.3.3.1.1.2.3.3" xref="S4.E7.m1.3.3.1.1.2.3.3.cmml">K</mi></munderover><mrow id="S4.E7.m1.3.3.1.1.2.2" xref="S4.E7.m1.3.3.1.1.2.2.cmml"><mrow id="S4.E7.m1.3.3.1.1.1.1.1" xref="S4.E7.m1.3.3.1.1.1.1.1.cmml"><mrow id="S4.E7.m1.3.3.1.1.1.1.1.1.1" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E7.m1.3.3.1.1.1.1.1.1.1.2" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mrow id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.3.2" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml">Rc</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.3.1" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mrow id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.3.3.2" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.3.3.2.1" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.3.cmml">(</mo><mi id="S4.E7.m1.1.1" xref="S4.E7.m1.1.1.cmml">k</mi><mo stretchy="false" id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.3.3.2.2" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.3.cmml">)</mo></mrow></mrow><mo id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">−</mo><mrow id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">Rc</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">k</mi><mo id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mn id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo rspace="0.055em" stretchy="false" id="S4.E7.m1.3.3.1.1.1.1.1.1.1.3" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S4.E7.m1.3.3.1.1.1.1.1.2" xref="S4.E7.m1.3.3.1.1.1.1.1.2.cmml">×</mo><msub id="S4.E7.m1.3.3.1.1.1.1.1.3" xref="S4.E7.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S4.E7.m1.3.3.1.1.1.1.1.3.2" xref="S4.E7.m1.3.3.1.1.1.1.1.3.2.cmml">Pr</mi><mi id="S4.E7.m1.3.3.1.1.1.1.1.3.3" xref="S4.E7.m1.3.3.1.1.1.1.1.3.3.cmml">interp</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S4.E7.m1.3.3.1.1.2.2.3" xref="S4.E7.m1.3.3.1.1.2.2.3.cmml">​</mo><mrow id="S4.E7.m1.3.3.1.1.2.2.2.1" xref="S4.E7.m1.3.3.1.1.2.2.2.1.1.cmml"><mo stretchy="false" id="S4.E7.m1.3.3.1.1.2.2.2.1.2" xref="S4.E7.m1.3.3.1.1.2.2.2.1.1.cmml">(</mo><mrow id="S4.E7.m1.3.3.1.1.2.2.2.1.1" xref="S4.E7.m1.3.3.1.1.2.2.2.1.1.cmml"><mi id="S4.E7.m1.3.3.1.1.2.2.2.1.1.2" xref="S4.E7.m1.3.3.1.1.2.2.2.1.1.2.cmml">Rc</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.3.3.1.1.2.2.2.1.1.1" xref="S4.E7.m1.3.3.1.1.2.2.2.1.1.1.cmml">​</mo><mrow id="S4.E7.m1.3.3.1.1.2.2.2.1.1.3.2" xref="S4.E7.m1.3.3.1.1.2.2.2.1.1.cmml"><mo stretchy="false" id="S4.E7.m1.3.3.1.1.2.2.2.1.1.3.2.1" xref="S4.E7.m1.3.3.1.1.2.2.2.1.1.cmml">(</mo><mi id="S4.E7.m1.2.2" xref="S4.E7.m1.2.2.cmml">k</mi><mo stretchy="false" id="S4.E7.m1.3.3.1.1.2.2.2.1.1.3.2.2" xref="S4.E7.m1.3.3.1.1.2.2.2.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E7.m1.3.3.1.1.2.2.2.1.3" xref="S4.E7.m1.3.3.1.1.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S4.E7.m1.3.3.1.2" xref="S4.E7.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E7.m1.3b"><apply id="S4.E7.m1.3.3.1.1.cmml" xref="S4.E7.m1.3.3.1"><eq id="S4.E7.m1.3.3.1.1.3.cmml" xref="S4.E7.m1.3.3.1.1.3"></eq><ci id="S4.E7.m1.3.3.1.1.4.cmml" xref="S4.E7.m1.3.3.1.1.4">AP</ci><apply id="S4.E7.m1.3.3.1.1.2.cmml" xref="S4.E7.m1.3.3.1.1.2"><apply id="S4.E7.m1.3.3.1.1.2.3.cmml" xref="S4.E7.m1.3.3.1.1.2.3"><csymbol cd="ambiguous" id="S4.E7.m1.3.3.1.1.2.3.1.cmml" xref="S4.E7.m1.3.3.1.1.2.3">superscript</csymbol><apply id="S4.E7.m1.3.3.1.1.2.3.2.cmml" xref="S4.E7.m1.3.3.1.1.2.3"><csymbol cd="ambiguous" id="S4.E7.m1.3.3.1.1.2.3.2.1.cmml" xref="S4.E7.m1.3.3.1.1.2.3">subscript</csymbol><sum id="S4.E7.m1.3.3.1.1.2.3.2.2.cmml" xref="S4.E7.m1.3.3.1.1.2.3.2.2"></sum><apply id="S4.E7.m1.3.3.1.1.2.3.2.3.cmml" xref="S4.E7.m1.3.3.1.1.2.3.2.3"><eq id="S4.E7.m1.3.3.1.1.2.3.2.3.1.cmml" xref="S4.E7.m1.3.3.1.1.2.3.2.3.1"></eq><ci id="S4.E7.m1.3.3.1.1.2.3.2.3.2.cmml" xref="S4.E7.m1.3.3.1.1.2.3.2.3.2">𝑘</ci><cn type="integer" id="S4.E7.m1.3.3.1.1.2.3.2.3.3.cmml" xref="S4.E7.m1.3.3.1.1.2.3.2.3.3">1</cn></apply></apply><ci id="S4.E7.m1.3.3.1.1.2.3.3.cmml" xref="S4.E7.m1.3.3.1.1.2.3.3">𝐾</ci></apply><apply id="S4.E7.m1.3.3.1.1.2.2.cmml" xref="S4.E7.m1.3.3.1.1.2.2"><times id="S4.E7.m1.3.3.1.1.2.2.3.cmml" xref="S4.E7.m1.3.3.1.1.2.2.3"></times><apply id="S4.E7.m1.3.3.1.1.1.1.1.cmml" xref="S4.E7.m1.3.3.1.1.1.1.1"><times id="S4.E7.m1.3.3.1.1.1.1.1.2.cmml" xref="S4.E7.m1.3.3.1.1.1.1.1.2"></times><apply id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1"><minus id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.2"></minus><apply id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.3"><times id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.3.1"></times><ci id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.3.2">Rc</ci><ci id="S4.E7.m1.1.1.cmml" xref="S4.E7.m1.1.1">𝑘</ci></apply><apply id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1"><times id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.2"></times><ci id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.3">Rc</ci><apply id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1"><minus id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1"></minus><ci id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑘</ci><cn type="integer" id="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E7.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3">1</cn></apply></apply></apply><apply id="S4.E7.m1.3.3.1.1.1.1.1.3.cmml" xref="S4.E7.m1.3.3.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E7.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S4.E7.m1.3.3.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E7.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S4.E7.m1.3.3.1.1.1.1.1.3.2">Pr</ci><ci id="S4.E7.m1.3.3.1.1.1.1.1.3.3.cmml" xref="S4.E7.m1.3.3.1.1.1.1.1.3.3">interp</ci></apply></apply><apply id="S4.E7.m1.3.3.1.1.2.2.2.1.1.cmml" xref="S4.E7.m1.3.3.1.1.2.2.2.1"><times id="S4.E7.m1.3.3.1.1.2.2.2.1.1.1.cmml" xref="S4.E7.m1.3.3.1.1.2.2.2.1.1.1"></times><ci id="S4.E7.m1.3.3.1.1.2.2.2.1.1.2.cmml" xref="S4.E7.m1.3.3.1.1.2.2.2.1.1.2">Rc</ci><ci id="S4.E7.m1.2.2.cmml" xref="S4.E7.m1.2.2">𝑘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E7.m1.3c">\mathrm{AP}=\sum\limits_{k=1}^{K}({\mathrm{Rc}}(k)-{\mathrm{Rc}}(k-1))\times\mathrm{Pr}_{\mathrm{interp}}(\mathrm{Rc}(k)),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS5.p12" class="ltx_para">
<p id="S4.SS5.p12.1" class="ltx_p">with <math id="S4.SS5.p12.1.m1.1" class="ltx_Math" alttext="\mathrm{Rc}(0)=0" display="inline"><semantics id="S4.SS5.p12.1.m1.1a"><mrow id="S4.SS5.p12.1.m1.1.2" xref="S4.SS5.p12.1.m1.1.2.cmml"><mrow id="S4.SS5.p12.1.m1.1.2.2" xref="S4.SS5.p12.1.m1.1.2.2.cmml"><mi id="S4.SS5.p12.1.m1.1.2.2.2" xref="S4.SS5.p12.1.m1.1.2.2.2.cmml">Rc</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p12.1.m1.1.2.2.1" xref="S4.SS5.p12.1.m1.1.2.2.1.cmml">​</mo><mrow id="S4.SS5.p12.1.m1.1.2.2.3.2" xref="S4.SS5.p12.1.m1.1.2.2.cmml"><mo stretchy="false" id="S4.SS5.p12.1.m1.1.2.2.3.2.1" xref="S4.SS5.p12.1.m1.1.2.2.cmml">(</mo><mn id="S4.SS5.p12.1.m1.1.1" xref="S4.SS5.p12.1.m1.1.1.cmml">0</mn><mo stretchy="false" id="S4.SS5.p12.1.m1.1.2.2.3.2.2" xref="S4.SS5.p12.1.m1.1.2.2.cmml">)</mo></mrow></mrow><mo id="S4.SS5.p12.1.m1.1.2.1" xref="S4.SS5.p12.1.m1.1.2.1.cmml">=</mo><mn id="S4.SS5.p12.1.m1.1.2.3" xref="S4.SS5.p12.1.m1.1.2.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p12.1.m1.1b"><apply id="S4.SS5.p12.1.m1.1.2.cmml" xref="S4.SS5.p12.1.m1.1.2"><eq id="S4.SS5.p12.1.m1.1.2.1.cmml" xref="S4.SS5.p12.1.m1.1.2.1"></eq><apply id="S4.SS5.p12.1.m1.1.2.2.cmml" xref="S4.SS5.p12.1.m1.1.2.2"><times id="S4.SS5.p12.1.m1.1.2.2.1.cmml" xref="S4.SS5.p12.1.m1.1.2.2.1"></times><ci id="S4.SS5.p12.1.m1.1.2.2.2.cmml" xref="S4.SS5.p12.1.m1.1.2.2.2">Rc</ci><cn type="integer" id="S4.SS5.p12.1.m1.1.1.cmml" xref="S4.SS5.p12.1.m1.1.1">0</cn></apply><cn type="integer" id="S4.SS5.p12.1.m1.1.2.3.cmml" xref="S4.SS5.p12.1.m1.1.2.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p12.1.m1.1c">\mathrm{Rc}(0)=0</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The result section focuses on presenting the performance of SyntEO in the real world example OWF detection to give a good intuition on how SyntEO can be used in order to investigate the training process and model behaviour. An in-depth discussion about SyntEO itself is given in section <a href="#S6" title="6 Discussion ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
<figure id="S5.F10" class="ltx_figure"><img src="/html/2112.02829/assets/figures/prcurve_150.png" id="S5.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="471" height="375" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S5.F10.3.2" class="ltx_text" style="font-size:90%;">Interpolated precision-recall curves for the four trained models and their performance on the ground truth data set. The corresponding AP values are: Model-1 (0.21), Model-2 (0.842), Model-3 (0.901), Model-3+ (0.904).</span></figcaption>
</figure>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.17.2.1" class="ltx_text" style="font-size:113%;">Table 1</span>: </span><span id="S5.T1.2.1" class="ltx_text" style="font-size:113%;">Overview of all metrics for all models on the ground truth (GT) data. For each model, the metrics are presented <span id="S5.T1.2.1.1" class="ltx_text ltx_font_bold">Combined</span> for all test sites together and separated for each test site. For the <span id="S5.T1.2.1.2" class="ltx_text ltx_font_bold">Combined</span> performance, the <span id="S5.T1.2.1.3" class="ltx_text ltx_framed ltx_framed_underline">best metrics are underlined</span>. True positive (TP); false positive (FP); false negative (FN); recall (Rc), precision (Pr); average precision (AP). Metrics with <sub id="S5.T1.2.1.4" class="ltx_sub"><span id="S5.T1.2.1.4.1" class="ltx_text ltx_font_italic">WT</span></sub> refer to the model performance on a wind turbine level.</span></figcaption>
<table id="S5.T1.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.5.3" class="ltx_tr">
<th id="S5.T1.5.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T1.5.3.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Studysite</span></th>
<th id="S5.T1.5.3.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T1.5.3.5.1" class="ltx_text" style="font-size:80%;">GT</span></th>
<th id="S5.T1.5.3.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T1.5.3.6.1" class="ltx_text" style="font-size:80%;">TP</span></th>
<th id="S5.T1.5.3.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T1.5.3.7.1" class="ltx_text" style="font-size:80%;">FP</span></th>
<th id="S5.T1.5.3.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T1.5.3.8.1" class="ltx_text" style="font-size:80%;">FN</span></th>
<th id="S5.T1.5.3.9" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T1.5.3.9.1" class="ltx_text" style="font-size:80%;">Rc</span></th>
<th id="S5.T1.5.3.10" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T1.5.3.10.1" class="ltx_text" style="font-size:80%;">Pr</span></th>
<th id="S5.T1.5.3.11" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T1.5.3.11.1" class="ltx_text" style="font-size:80%;">F1</span></th>
<th id="S5.T1.5.3.12" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T1.5.3.12.1" class="ltx_text" style="font-size:80%;">AP</span></th>
<th id="S5.T1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T1.3.1.1.1" class="ltx_text" style="font-size:80%;">GT</span><math id="S5.T1.3.1.1.m1.1" class="ltx_Math" alttext="{}_{\textrm{WT}}" display="inline"><semantics id="S5.T1.3.1.1.m1.1a"><msub id="S5.T1.3.1.1.m1.1.1" xref="S5.T1.3.1.1.m1.1.1.cmml"><mi id="S5.T1.3.1.1.m1.1.1a" xref="S5.T1.3.1.1.m1.1.1.cmml"></mi><mtext mathsize="80%" id="S5.T1.3.1.1.m1.1.1.1" xref="S5.T1.3.1.1.m1.1.1.1a.cmml">WT</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T1.3.1.1.m1.1b"><apply id="S5.T1.3.1.1.m1.1.1.cmml" xref="S5.T1.3.1.1.m1.1.1"><ci id="S5.T1.3.1.1.m1.1.1.1a.cmml" xref="S5.T1.3.1.1.m1.1.1.1"><mtext mathsize="56%" id="S5.T1.3.1.1.m1.1.1.1.cmml" xref="S5.T1.3.1.1.m1.1.1.1">WT</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.1.1.m1.1c">{}_{\textrm{WT}}</annotation></semantics></math>
</th>
<th id="S5.T1.4.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T1.4.2.2.1" class="ltx_text" style="font-size:80%;">TP</span><math id="S5.T1.4.2.2.m1.1" class="ltx_Math" alttext="{}_{\textrm{WT}}" display="inline"><semantics id="S5.T1.4.2.2.m1.1a"><msub id="S5.T1.4.2.2.m1.1.1" xref="S5.T1.4.2.2.m1.1.1.cmml"><mi id="S5.T1.4.2.2.m1.1.1a" xref="S5.T1.4.2.2.m1.1.1.cmml"></mi><mtext mathsize="80%" id="S5.T1.4.2.2.m1.1.1.1" xref="S5.T1.4.2.2.m1.1.1.1a.cmml">WT</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T1.4.2.2.m1.1b"><apply id="S5.T1.4.2.2.m1.1.1.cmml" xref="S5.T1.4.2.2.m1.1.1"><ci id="S5.T1.4.2.2.m1.1.1.1a.cmml" xref="S5.T1.4.2.2.m1.1.1.1"><mtext mathsize="56%" id="S5.T1.4.2.2.m1.1.1.1.cmml" xref="S5.T1.4.2.2.m1.1.1.1">WT</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.2.2.m1.1c">{}_{\textrm{WT}}</annotation></semantics></math>
</th>
<th id="S5.T1.5.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><math id="S5.T1.5.3.3.m1.1" class="ltx_Math" alttext="\mathrm{Rc}_{\mathrm{WT}}" display="inline"><semantics id="S5.T1.5.3.3.m1.1a"><msub id="S5.T1.5.3.3.m1.1.1" xref="S5.T1.5.3.3.m1.1.1.cmml"><mi mathsize="80%" id="S5.T1.5.3.3.m1.1.1.2" xref="S5.T1.5.3.3.m1.1.1.2.cmml">Rc</mi><mi mathsize="80%" id="S5.T1.5.3.3.m1.1.1.3" xref="S5.T1.5.3.3.m1.1.1.3.cmml">WT</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T1.5.3.3.m1.1b"><apply id="S5.T1.5.3.3.m1.1.1.cmml" xref="S5.T1.5.3.3.m1.1.1"><csymbol cd="ambiguous" id="S5.T1.5.3.3.m1.1.1.1.cmml" xref="S5.T1.5.3.3.m1.1.1">subscript</csymbol><ci id="S5.T1.5.3.3.m1.1.1.2.cmml" xref="S5.T1.5.3.3.m1.1.1.2">Rc</ci><ci id="S5.T1.5.3.3.m1.1.1.3.cmml" xref="S5.T1.5.3.3.m1.1.1.3">WT</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.3.3.m1.1c">\mathrm{Rc}_{\mathrm{WT}}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.5.4.1" class="ltx_tr">
<td id="S5.T1.5.4.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T1.5.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model - 1</span></td>
<td id="S5.T1.5.4.1.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.4.1.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.4.1.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.4.1.5" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.4.1.6" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.4.1.7" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.4.1.8" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.4.1.9" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.4.1.10" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.4.1.11" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.4.1.12" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T1.5.5.2" class="ltx_tr">
<td id="S5.T1.5.5.2.1" class="ltx_td ltx_align_left"><span id="S5.T1.5.5.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Combined</span></td>
<td id="S5.T1.5.5.2.2" class="ltx_td ltx_align_left"><span id="S5.T1.5.5.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">67</span></td>
<td id="S5.T1.5.5.2.3" class="ltx_td ltx_align_left"><span id="S5.T1.5.5.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">31</span></td>
<td id="S5.T1.5.5.2.4" class="ltx_td ltx_align_left"><span id="S5.T1.5.5.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">252</span></td>
<td id="S5.T1.5.5.2.5" class="ltx_td ltx_align_left"><span id="S5.T1.5.5.2.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">36</span></td>
<td id="S5.T1.5.5.2.6" class="ltx_td ltx_align_left"><span id="S5.T1.5.5.2.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.463</span></td>
<td id="S5.T1.5.5.2.7" class="ltx_td ltx_align_left"><span id="S5.T1.5.5.2.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.11</span></td>
<td id="S5.T1.5.5.2.8" class="ltx_td ltx_align_left"><span id="S5.T1.5.5.2.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.177</span></td>
<td id="S5.T1.5.5.2.9" class="ltx_td ltx_align_left"><span id="S5.T1.5.5.2.9.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.21</span></td>
<td id="S5.T1.5.5.2.10" class="ltx_td ltx_align_left"><span id="S5.T1.5.5.2.10.1" class="ltx_text ltx_font_bold" style="font-size:80%;">5374</span></td>
<td id="S5.T1.5.5.2.11" class="ltx_td ltx_align_left"><span id="S5.T1.5.5.2.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">863</span></td>
<td id="S5.T1.5.5.2.12" class="ltx_td ltx_align_left"><span id="S5.T1.5.5.2.12.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.161</span></td>
</tr>
<tr id="S5.T1.5.6.3" class="ltx_tr">
<td id="S5.T1.5.6.3.1" class="ltx_td ltx_align_left"><span id="S5.T1.5.6.3.1.1" class="ltx_text" style="font-size:80%;">North Sea Basin</span></td>
<td id="S5.T1.5.6.3.2" class="ltx_td ltx_align_left"><span id="S5.T1.5.6.3.2.1" class="ltx_text" style="font-size:80%;">42</span></td>
<td id="S5.T1.5.6.3.3" class="ltx_td ltx_align_left"><span id="S5.T1.5.6.3.3.1" class="ltx_text" style="font-size:80%;">19</span></td>
<td id="S5.T1.5.6.3.4" class="ltx_td ltx_align_left"><span id="S5.T1.5.6.3.4.1" class="ltx_text" style="font-size:80%;">85</span></td>
<td id="S5.T1.5.6.3.5" class="ltx_td ltx_align_left"><span id="S5.T1.5.6.3.5.1" class="ltx_text" style="font-size:80%;">23</span></td>
<td id="S5.T1.5.6.3.6" class="ltx_td ltx_align_left"><span id="S5.T1.5.6.3.6.1" class="ltx_text" style="font-size:80%;">0.452</span></td>
<td id="S5.T1.5.6.3.7" class="ltx_td ltx_align_left"><span id="S5.T1.5.6.3.7.1" class="ltx_text" style="font-size:80%;">0.183</span></td>
<td id="S5.T1.5.6.3.8" class="ltx_td ltx_align_left"><span id="S5.T1.5.6.3.8.1" class="ltx_text" style="font-size:80%;">0.260</span></td>
<td id="S5.T1.5.6.3.9" class="ltx_td ltx_align_left"><span id="S5.T1.5.6.3.9.1" class="ltx_text" style="font-size:80%;">0.217</span></td>
<td id="S5.T1.5.6.3.10" class="ltx_td ltx_align_left"><span id="S5.T1.5.6.3.10.1" class="ltx_text" style="font-size:80%;">3787</span></td>
<td id="S5.T1.5.6.3.11" class="ltx_td ltx_align_left"><span id="S5.T1.5.6.3.11.1" class="ltx_text" style="font-size:80%;">687</span></td>
<td id="S5.T1.5.6.3.12" class="ltx_td ltx_align_left"><span id="S5.T1.5.6.3.12.1" class="ltx_text" style="font-size:80%;">0.181</span></td>
</tr>
<tr id="S5.T1.5.7.4" class="ltx_tr">
<td id="S5.T1.5.7.4.1" class="ltx_td ltx_align_left"><span id="S5.T1.5.7.4.1.1" class="ltx_text" style="font-size:80%;">East Chinese Sea</span></td>
<td id="S5.T1.5.7.4.2" class="ltx_td ltx_align_left"><span id="S5.T1.5.7.4.2.1" class="ltx_text" style="font-size:80%;">25</span></td>
<td id="S5.T1.5.7.4.3" class="ltx_td ltx_align_left"><span id="S5.T1.5.7.4.3.1" class="ltx_text" style="font-size:80%;">12</span></td>
<td id="S5.T1.5.7.4.4" class="ltx_td ltx_align_left"><span id="S5.T1.5.7.4.4.1" class="ltx_text" style="font-size:80%;">67</span></td>
<td id="S5.T1.5.7.4.5" class="ltx_td ltx_align_left"><span id="S5.T1.5.7.4.5.1" class="ltx_text" style="font-size:80%;">13</span></td>
<td id="S5.T1.5.7.4.6" class="ltx_td ltx_align_left"><span id="S5.T1.5.7.4.6.1" class="ltx_text" style="font-size:80%;">0.48</span></td>
<td id="S5.T1.5.7.4.7" class="ltx_td ltx_align_left"><span id="S5.T1.5.7.4.7.1" class="ltx_text" style="font-size:80%;">0.152</span></td>
<td id="S5.T1.5.7.4.8" class="ltx_td ltx_align_left"><span id="S5.T1.5.7.4.8.1" class="ltx_text" style="font-size:80%;">0.231</span></td>
<td id="S5.T1.5.7.4.9" class="ltx_td ltx_align_left"><span id="S5.T1.5.7.4.9.1" class="ltx_text" style="font-size:80%;">0.282</span></td>
<td id="S5.T1.5.7.4.10" class="ltx_td ltx_align_left"><span id="S5.T1.5.7.4.10.1" class="ltx_text" style="font-size:80%;">1587</span></td>
<td id="S5.T1.5.7.4.11" class="ltx_td ltx_align_left"><span id="S5.T1.5.7.4.11.1" class="ltx_text" style="font-size:80%;">176</span></td>
<td id="S5.T1.5.7.4.12" class="ltx_td ltx_align_left"><span id="S5.T1.5.7.4.12.1" class="ltx_text" style="font-size:80%;">0.111</span></td>
</tr>
<tr id="S5.T1.5.8.5" class="ltx_tr">
<td id="S5.T1.5.8.5.1" class="ltx_td ltx_align_left"><span id="S5.T1.5.8.5.1.1" class="ltx_text" style="font-size:80%;">Persian Gulf</span></td>
<td id="S5.T1.5.8.5.2" class="ltx_td ltx_align_left"><span id="S5.T1.5.8.5.2.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S5.T1.5.8.5.3" class="ltx_td"></td>
<td id="S5.T1.5.8.5.4" class="ltx_td ltx_align_left"><span id="S5.T1.5.8.5.4.1" class="ltx_text" style="font-size:80%;">100</span></td>
<td id="S5.T1.5.8.5.5" class="ltx_td"></td>
<td id="S5.T1.5.8.5.6" class="ltx_td"></td>
<td id="S5.T1.5.8.5.7" class="ltx_td"></td>
<td id="S5.T1.5.8.5.8" class="ltx_td"></td>
<td id="S5.T1.5.8.5.9" class="ltx_td"></td>
<td id="S5.T1.5.8.5.10" class="ltx_td"></td>
<td id="S5.T1.5.8.5.11" class="ltx_td"></td>
<td id="S5.T1.5.8.5.12" class="ltx_td"></td>
</tr>
<tr id="S5.T1.5.9.6" class="ltx_tr">
<td id="S5.T1.5.9.6.1" class="ltx_td ltx_align_left"><span id="S5.T1.5.9.6.1.1" class="ltx_text" style="font-size:80%;">Sea of Azov</span></td>
<td id="S5.T1.5.9.6.2" class="ltx_td ltx_align_left"><span id="S5.T1.5.9.6.2.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S5.T1.5.9.6.3" class="ltx_td"></td>
<td id="S5.T1.5.9.6.4" class="ltx_td ltx_align_left"><span id="S5.T1.5.9.6.4.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S5.T1.5.9.6.5" class="ltx_td"></td>
<td id="S5.T1.5.9.6.6" class="ltx_td"></td>
<td id="S5.T1.5.9.6.7" class="ltx_td"></td>
<td id="S5.T1.5.9.6.8" class="ltx_td"></td>
<td id="S5.T1.5.9.6.9" class="ltx_td"></td>
<td id="S5.T1.5.9.6.10" class="ltx_td"></td>
<td id="S5.T1.5.9.6.11" class="ltx_td"></td>
<td id="S5.T1.5.9.6.12" class="ltx_td"></td>
</tr>
<tr id="S5.T1.5.10.7" class="ltx_tr">
<td id="S5.T1.5.10.7.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T1.5.10.7.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model - 2</span></td>
<td id="S5.T1.5.10.7.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.10.7.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.10.7.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.10.7.5" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.10.7.6" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.10.7.7" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.10.7.8" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.10.7.9" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.10.7.10" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.10.7.11" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.10.7.12" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T1.5.11.8" class="ltx_tr">
<td id="S5.T1.5.11.8.1" class="ltx_td ltx_align_left"><span id="S5.T1.5.11.8.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Combined</span></td>
<td id="S5.T1.5.11.8.2" class="ltx_td ltx_align_left"><span id="S5.T1.5.11.8.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">67</span></td>
<td id="S5.T1.5.11.8.3" class="ltx_td ltx_align_left"><span id="S5.T1.5.11.8.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">59</span></td>
<td id="S5.T1.5.11.8.4" class="ltx_td ltx_align_left"><span id="S5.T1.5.11.8.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">80</span></td>
<td id="S5.T1.5.11.8.5" class="ltx_td ltx_align_left"><span id="S5.T1.5.11.8.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">8</span></td>
<td id="S5.T1.5.11.8.6" class="ltx_td ltx_align_left"><span id="S5.T1.5.11.8.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.881</span></td>
<td id="S5.T1.5.11.8.7" class="ltx_td ltx_align_left"><span id="S5.T1.5.11.8.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.424</span></td>
<td id="S5.T1.5.11.8.8" class="ltx_td ltx_align_left"><span id="S5.T1.5.11.8.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.573</span></td>
<td id="S5.T1.5.11.8.9" class="ltx_td ltx_align_left"><span id="S5.T1.5.11.8.9.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.842</span></td>
<td id="S5.T1.5.11.8.10" class="ltx_td ltx_align_left"><span id="S5.T1.5.11.8.10.1" class="ltx_text ltx_font_bold" style="font-size:80%;">5374</span></td>
<td id="S5.T1.5.11.8.11" class="ltx_td ltx_align_left"><span id="S5.T1.5.11.8.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">5263</span></td>
<td id="S5.T1.5.11.8.12" class="ltx_td ltx_align_left"><span id="S5.T1.5.11.8.12.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.979</span></td>
</tr>
<tr id="S5.T1.5.12.9" class="ltx_tr">
<td id="S5.T1.5.12.9.1" class="ltx_td ltx_align_left"><span id="S5.T1.5.12.9.1.1" class="ltx_text" style="font-size:80%;">North Sea Basin</span></td>
<td id="S5.T1.5.12.9.2" class="ltx_td ltx_align_left"><span id="S5.T1.5.12.9.2.1" class="ltx_text" style="font-size:80%;">42</span></td>
<td id="S5.T1.5.12.9.3" class="ltx_td ltx_align_left"><span id="S5.T1.5.12.9.3.1" class="ltx_text" style="font-size:80%;">40</span></td>
<td id="S5.T1.5.12.9.4" class="ltx_td ltx_align_left"><span id="S5.T1.5.12.9.4.1" class="ltx_text" style="font-size:80%;">3</span></td>
<td id="S5.T1.5.12.9.5" class="ltx_td ltx_align_left"><span id="S5.T1.5.12.9.5.1" class="ltx_text" style="font-size:80%;">2</span></td>
<td id="S5.T1.5.12.9.6" class="ltx_td ltx_align_left"><span id="S5.T1.5.12.9.6.1" class="ltx_text" style="font-size:80%;">0.952</span></td>
<td id="S5.T1.5.12.9.7" class="ltx_td ltx_align_left"><span id="S5.T1.5.12.9.7.1" class="ltx_text" style="font-size:80%;">0.93</span></td>
<td id="S5.T1.5.12.9.8" class="ltx_td ltx_align_left"><span id="S5.T1.5.12.9.8.1" class="ltx_text" style="font-size:80%;">0.941</span></td>
<td id="S5.T1.5.12.9.9" class="ltx_td ltx_align_left"><span id="S5.T1.5.12.9.9.1" class="ltx_text" style="font-size:80%;">0.952</span></td>
<td id="S5.T1.5.12.9.10" class="ltx_td ltx_align_left"><span id="S5.T1.5.12.9.10.1" class="ltx_text" style="font-size:80%;">3787</span></td>
<td id="S5.T1.5.12.9.11" class="ltx_td ltx_align_left"><span id="S5.T1.5.12.9.11.1" class="ltx_text" style="font-size:80%;">3764</span></td>
<td id="S5.T1.5.12.9.12" class="ltx_td ltx_align_left"><span id="S5.T1.5.12.9.12.1" class="ltx_text" style="font-size:80%;">0.994</span></td>
</tr>
<tr id="S5.T1.5.13.10" class="ltx_tr">
<td id="S5.T1.5.13.10.1" class="ltx_td ltx_align_left"><span id="S5.T1.5.13.10.1.1" class="ltx_text" style="font-size:80%;">East Chinese Sea</span></td>
<td id="S5.T1.5.13.10.2" class="ltx_td ltx_align_left"><span id="S5.T1.5.13.10.2.1" class="ltx_text" style="font-size:80%;">25</span></td>
<td id="S5.T1.5.13.10.3" class="ltx_td ltx_align_left"><span id="S5.T1.5.13.10.3.1" class="ltx_text" style="font-size:80%;">19</span></td>
<td id="S5.T1.5.13.10.4" class="ltx_td ltx_align_left"><span id="S5.T1.5.13.10.4.1" class="ltx_text" style="font-size:80%;">22</span></td>
<td id="S5.T1.5.13.10.5" class="ltx_td ltx_align_left"><span id="S5.T1.5.13.10.5.1" class="ltx_text" style="font-size:80%;">6</span></td>
<td id="S5.T1.5.13.10.6" class="ltx_td ltx_align_left"><span id="S5.T1.5.13.10.6.1" class="ltx_text" style="font-size:80%;">0.76</span></td>
<td id="S5.T1.5.13.10.7" class="ltx_td ltx_align_left"><span id="S5.T1.5.13.10.7.1" class="ltx_text" style="font-size:80%;">0.46</span></td>
<td id="S5.T1.5.13.10.8" class="ltx_td ltx_align_left"><span id="S5.T1.5.13.10.8.1" class="ltx_text" style="font-size:80%;">0.576</span></td>
<td id="S5.T1.5.13.10.9" class="ltx_td ltx_align_left"><span id="S5.T1.5.13.10.9.1" class="ltx_text" style="font-size:80%;">0.712</span></td>
<td id="S5.T1.5.13.10.10" class="ltx_td ltx_align_left"><span id="S5.T1.5.13.10.10.1" class="ltx_text" style="font-size:80%;">1587</span></td>
<td id="S5.T1.5.13.10.11" class="ltx_td ltx_align_left"><span id="S5.T1.5.13.10.11.1" class="ltx_text" style="font-size:80%;">1499</span></td>
<td id="S5.T1.5.13.10.12" class="ltx_td ltx_align_left"><span id="S5.T1.5.13.10.12.1" class="ltx_text" style="font-size:80%;">0.945</span></td>
</tr>
<tr id="S5.T1.5.14.11" class="ltx_tr">
<td id="S5.T1.5.14.11.1" class="ltx_td ltx_align_left"><span id="S5.T1.5.14.11.1.1" class="ltx_text" style="font-size:80%;">Persian Gulf</span></td>
<td id="S5.T1.5.14.11.2" class="ltx_td ltx_align_left"><span id="S5.T1.5.14.11.2.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S5.T1.5.14.11.3" class="ltx_td"></td>
<td id="S5.T1.5.14.11.4" class="ltx_td ltx_align_left"><span id="S5.T1.5.14.11.4.1" class="ltx_text" style="font-size:80%;">31</span></td>
<td id="S5.T1.5.14.11.5" class="ltx_td"></td>
<td id="S5.T1.5.14.11.6" class="ltx_td"></td>
<td id="S5.T1.5.14.11.7" class="ltx_td"></td>
<td id="S5.T1.5.14.11.8" class="ltx_td"></td>
<td id="S5.T1.5.14.11.9" class="ltx_td"></td>
<td id="S5.T1.5.14.11.10" class="ltx_td"></td>
<td id="S5.T1.5.14.11.11" class="ltx_td"></td>
<td id="S5.T1.5.14.11.12" class="ltx_td"></td>
</tr>
<tr id="S5.T1.5.15.12" class="ltx_tr">
<td id="S5.T1.5.15.12.1" class="ltx_td ltx_align_left"><span id="S5.T1.5.15.12.1.1" class="ltx_text" style="font-size:80%;">Sea of Azov</span></td>
<td id="S5.T1.5.15.12.2" class="ltx_td ltx_align_left"><span id="S5.T1.5.15.12.2.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S5.T1.5.15.12.3" class="ltx_td"></td>
<td id="S5.T1.5.15.12.4" class="ltx_td ltx_align_left"><span id="S5.T1.5.15.12.4.1" class="ltx_text" style="font-size:80%;">24</span></td>
<td id="S5.T1.5.15.12.5" class="ltx_td"></td>
<td id="S5.T1.5.15.12.6" class="ltx_td"></td>
<td id="S5.T1.5.15.12.7" class="ltx_td"></td>
<td id="S5.T1.5.15.12.8" class="ltx_td"></td>
<td id="S5.T1.5.15.12.9" class="ltx_td"></td>
<td id="S5.T1.5.15.12.10" class="ltx_td"></td>
<td id="S5.T1.5.15.12.11" class="ltx_td"></td>
<td id="S5.T1.5.15.12.12" class="ltx_td"></td>
</tr>
<tr id="S5.T1.5.16.13" class="ltx_tr">
<td id="S5.T1.5.16.13.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T1.5.16.13.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model - 3</span></td>
<td id="S5.T1.5.16.13.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.16.13.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.16.13.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.16.13.5" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.16.13.6" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.16.13.7" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.16.13.8" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.16.13.9" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.16.13.10" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.16.13.11" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.16.13.12" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T1.5.17.14" class="ltx_tr">
<td id="S5.T1.5.17.14.1" class="ltx_td ltx_align_left"><span id="S5.T1.5.17.14.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Combined</span></td>
<td id="S5.T1.5.17.14.2" class="ltx_td ltx_align_left"><span id="S5.T1.5.17.14.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">67</span></td>
<td id="S5.T1.5.17.14.3" class="ltx_td ltx_align_left"><span id="S5.T1.5.17.14.3.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:80%;">61</span></td>
<td id="S5.T1.5.17.14.4" class="ltx_td ltx_align_left"><span id="S5.T1.5.17.14.4.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:80%;">11</span></td>
<td id="S5.T1.5.17.14.5" class="ltx_td ltx_align_left"><span id="S5.T1.5.17.14.5.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:80%;">6</span></td>
<td id="S5.T1.5.17.14.6" class="ltx_td ltx_align_left"><span id="S5.T1.5.17.14.6.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:80%;">0.91</span></td>
<td id="S5.T1.5.17.14.7" class="ltx_td ltx_align_left"><span id="S5.T1.5.17.14.7.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:80%;">0.847</span></td>
<td id="S5.T1.5.17.14.8" class="ltx_td ltx_align_left"><span id="S5.T1.5.17.14.8.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:80%;">0.878</span></td>
<td id="S5.T1.5.17.14.9" class="ltx_td ltx_align_left"><span id="S5.T1.5.17.14.9.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.901</span></td>
<td id="S5.T1.5.17.14.10" class="ltx_td ltx_align_left"><span id="S5.T1.5.17.14.10.1" class="ltx_text ltx_font_bold" style="font-size:80%;">5374</span></td>
<td id="S5.T1.5.17.14.11" class="ltx_td ltx_align_left"><span id="S5.T1.5.17.14.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">5185</span></td>
<td id="S5.T1.5.17.14.12" class="ltx_td ltx_align_left"><span id="S5.T1.5.17.14.12.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.965</span></td>
</tr>
<tr id="S5.T1.5.18.15" class="ltx_tr">
<td id="S5.T1.5.18.15.1" class="ltx_td ltx_align_left"><span id="S5.T1.5.18.15.1.1" class="ltx_text" style="font-size:80%;">North Sea Basin</span></td>
<td id="S5.T1.5.18.15.2" class="ltx_td ltx_align_left"><span id="S5.T1.5.18.15.2.1" class="ltx_text" style="font-size:80%;">42</span></td>
<td id="S5.T1.5.18.15.3" class="ltx_td ltx_align_left"><span id="S5.T1.5.18.15.3.1" class="ltx_text" style="font-size:80%;">40</span></td>
<td id="S5.T1.5.18.15.4" class="ltx_td ltx_align_left"><span id="S5.T1.5.18.15.4.1" class="ltx_text" style="font-size:80%;">1</span></td>
<td id="S5.T1.5.18.15.5" class="ltx_td ltx_align_left"><span id="S5.T1.5.18.15.5.1" class="ltx_text" style="font-size:80%;">2</span></td>
<td id="S5.T1.5.18.15.6" class="ltx_td ltx_align_left"><span id="S5.T1.5.18.15.6.1" class="ltx_text" style="font-size:80%;">0.952</span></td>
<td id="S5.T1.5.18.15.7" class="ltx_td ltx_align_left"><span id="S5.T1.5.18.15.7.1" class="ltx_text" style="font-size:80%;">0.976</span></td>
<td id="S5.T1.5.18.15.8" class="ltx_td ltx_align_left"><span id="S5.T1.5.18.15.8.1" class="ltx_text" style="font-size:80%;">0.964</span></td>
<td id="S5.T1.5.18.15.9" class="ltx_td ltx_align_left"><span id="S5.T1.5.18.15.9.1" class="ltx_text" style="font-size:80%;">0.952</span></td>
<td id="S5.T1.5.18.15.10" class="ltx_td ltx_align_left"><span id="S5.T1.5.18.15.10.1" class="ltx_text" style="font-size:80%;">3787</span></td>
<td id="S5.T1.5.18.15.11" class="ltx_td ltx_align_left"><span id="S5.T1.5.18.15.11.1" class="ltx_text" style="font-size:80%;">3742</span></td>
<td id="S5.T1.5.18.15.12" class="ltx_td ltx_align_left"><span id="S5.T1.5.18.15.12.1" class="ltx_text" style="font-size:80%;">0.988</span></td>
</tr>
<tr id="S5.T1.5.19.16" class="ltx_tr">
<td id="S5.T1.5.19.16.1" class="ltx_td ltx_align_left"><span id="S5.T1.5.19.16.1.1" class="ltx_text" style="font-size:80%;">East Chinese Sea</span></td>
<td id="S5.T1.5.19.16.2" class="ltx_td ltx_align_left"><span id="S5.T1.5.19.16.2.1" class="ltx_text" style="font-size:80%;">25</span></td>
<td id="S5.T1.5.19.16.3" class="ltx_td ltx_align_left"><span id="S5.T1.5.19.16.3.1" class="ltx_text" style="font-size:80%;">21</span></td>
<td id="S5.T1.5.19.16.4" class="ltx_td ltx_align_left"><span id="S5.T1.5.19.16.4.1" class="ltx_text" style="font-size:80%;">5</span></td>
<td id="S5.T1.5.19.16.5" class="ltx_td ltx_align_left"><span id="S5.T1.5.19.16.5.1" class="ltx_text" style="font-size:80%;">4</span></td>
<td id="S5.T1.5.19.16.6" class="ltx_td ltx_align_left"><span id="S5.T1.5.19.16.6.1" class="ltx_text" style="font-size:80%;">0.84</span></td>
<td id="S5.T1.5.19.16.7" class="ltx_td ltx_align_left"><span id="S5.T1.5.19.16.7.1" class="ltx_text" style="font-size:80%;">0.808</span></td>
<td id="S5.T1.5.19.16.8" class="ltx_td ltx_align_left"><span id="S5.T1.5.19.16.8.1" class="ltx_text" style="font-size:80%;">0.824</span></td>
<td id="S5.T1.5.19.16.9" class="ltx_td ltx_align_left"><span id="S5.T1.5.19.16.9.1" class="ltx_text" style="font-size:80%;">0.817</span></td>
<td id="S5.T1.5.19.16.10" class="ltx_td ltx_align_left"><span id="S5.T1.5.19.16.10.1" class="ltx_text" style="font-size:80%;">1587</span></td>
<td id="S5.T1.5.19.16.11" class="ltx_td ltx_align_left"><span id="S5.T1.5.19.16.11.1" class="ltx_text" style="font-size:80%;">1443</span></td>
<td id="S5.T1.5.19.16.12" class="ltx_td ltx_align_left"><span id="S5.T1.5.19.16.12.1" class="ltx_text" style="font-size:80%;">0.91</span></td>
</tr>
<tr id="S5.T1.5.20.17" class="ltx_tr">
<td id="S5.T1.5.20.17.1" class="ltx_td ltx_align_left"><span id="S5.T1.5.20.17.1.1" class="ltx_text" style="font-size:80%;">Persian Gulf</span></td>
<td id="S5.T1.5.20.17.2" class="ltx_td ltx_align_left"><span id="S5.T1.5.20.17.2.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S5.T1.5.20.17.3" class="ltx_td"></td>
<td id="S5.T1.5.20.17.4" class="ltx_td ltx_align_left"><span id="S5.T1.5.20.17.4.1" class="ltx_text" style="font-size:80%;">5</span></td>
<td id="S5.T1.5.20.17.5" class="ltx_td"></td>
<td id="S5.T1.5.20.17.6" class="ltx_td"></td>
<td id="S5.T1.5.20.17.7" class="ltx_td"></td>
<td id="S5.T1.5.20.17.8" class="ltx_td"></td>
<td id="S5.T1.5.20.17.9" class="ltx_td"></td>
<td id="S5.T1.5.20.17.10" class="ltx_td"></td>
<td id="S5.T1.5.20.17.11" class="ltx_td"></td>
<td id="S5.T1.5.20.17.12" class="ltx_td"></td>
</tr>
<tr id="S5.T1.5.21.18" class="ltx_tr">
<td id="S5.T1.5.21.18.1" class="ltx_td ltx_align_left"><span id="S5.T1.5.21.18.1.1" class="ltx_text" style="font-size:80%;">Sea of Azov</span></td>
<td id="S5.T1.5.21.18.2" class="ltx_td ltx_align_left"><span id="S5.T1.5.21.18.2.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S5.T1.5.21.18.3" class="ltx_td"></td>
<td id="S5.T1.5.21.18.4" class="ltx_td ltx_align_left"><span id="S5.T1.5.21.18.4.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S5.T1.5.21.18.5" class="ltx_td"></td>
<td id="S5.T1.5.21.18.6" class="ltx_td"></td>
<td id="S5.T1.5.21.18.7" class="ltx_td"></td>
<td id="S5.T1.5.21.18.8" class="ltx_td"></td>
<td id="S5.T1.5.21.18.9" class="ltx_td"></td>
<td id="S5.T1.5.21.18.10" class="ltx_td"></td>
<td id="S5.T1.5.21.18.11" class="ltx_td"></td>
<td id="S5.T1.5.21.18.12" class="ltx_td"></td>
</tr>
<tr id="S5.T1.5.22.19" class="ltx_tr">
<td id="S5.T1.5.22.19.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T1.5.22.19.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model - 3+</span></td>
<td id="S5.T1.5.22.19.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.22.19.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.22.19.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.22.19.5" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.22.19.6" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.22.19.7" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.22.19.8" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.22.19.9" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.22.19.10" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.22.19.11" class="ltx_td ltx_border_t"></td>
<td id="S5.T1.5.22.19.12" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T1.5.23.20" class="ltx_tr">
<td id="S5.T1.5.23.20.1" class="ltx_td ltx_align_left"><span id="S5.T1.5.23.20.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Combined</span></td>
<td id="S5.T1.5.23.20.2" class="ltx_td ltx_align_left"><span id="S5.T1.5.23.20.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">67</span></td>
<td id="S5.T1.5.23.20.3" class="ltx_td ltx_align_left"><span id="S5.T1.5.23.20.3.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:80%;">61</span></td>
<td id="S5.T1.5.23.20.4" class="ltx_td ltx_align_left"><span id="S5.T1.5.23.20.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">14</span></td>
<td id="S5.T1.5.23.20.5" class="ltx_td ltx_align_left"><span id="S5.T1.5.23.20.5.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:80%;">6</span></td>
<td id="S5.T1.5.23.20.6" class="ltx_td ltx_align_left"><span id="S5.T1.5.23.20.6.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:80%;">0.91</span></td>
<td id="S5.T1.5.23.20.7" class="ltx_td ltx_align_left"><span id="S5.T1.5.23.20.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.813</span></td>
<td id="S5.T1.5.23.20.8" class="ltx_td ltx_align_left"><span id="S5.T1.5.23.20.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.86</span></td>
<td id="S5.T1.5.23.20.9" class="ltx_td ltx_align_left"><span id="S5.T1.5.23.20.9.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:80%;">0.904</span></td>
<td id="S5.T1.5.23.20.10" class="ltx_td ltx_align_left"><span id="S5.T1.5.23.20.10.1" class="ltx_text ltx_font_bold" style="font-size:80%;">5374</span></td>
<td id="S5.T1.5.23.20.11" class="ltx_td ltx_align_left"><span id="S5.T1.5.23.20.11.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:80%;">5296</span></td>
<td id="S5.T1.5.23.20.12" class="ltx_td ltx_align_left"><span id="S5.T1.5.23.20.12.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:80%;">0.985</span></td>
</tr>
<tr id="S5.T1.5.24.21" class="ltx_tr">
<td id="S5.T1.5.24.21.1" class="ltx_td ltx_align_left"><span id="S5.T1.5.24.21.1.1" class="ltx_text" style="font-size:80%;">North Sea Basin</span></td>
<td id="S5.T1.5.24.21.2" class="ltx_td ltx_align_left"><span id="S5.T1.5.24.21.2.1" class="ltx_text" style="font-size:80%;">42</span></td>
<td id="S5.T1.5.24.21.3" class="ltx_td ltx_align_left"><span id="S5.T1.5.24.21.3.1" class="ltx_text" style="font-size:80%;">40</span></td>
<td id="S5.T1.5.24.21.4" class="ltx_td ltx_align_left"><span id="S5.T1.5.24.21.4.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S5.T1.5.24.21.5" class="ltx_td ltx_align_left"><span id="S5.T1.5.24.21.5.1" class="ltx_text" style="font-size:80%;">2</span></td>
<td id="S5.T1.5.24.21.6" class="ltx_td ltx_align_left"><span id="S5.T1.5.24.21.6.1" class="ltx_text" style="font-size:80%;">0.952</span></td>
<td id="S5.T1.5.24.21.7" class="ltx_td ltx_align_left"><span id="S5.T1.5.24.21.7.1" class="ltx_text" style="font-size:80%;">1</span></td>
<td id="S5.T1.5.24.21.8" class="ltx_td ltx_align_left"><span id="S5.T1.5.24.21.8.1" class="ltx_text" style="font-size:80%;">0.976</span></td>
<td id="S5.T1.5.24.21.9" class="ltx_td ltx_align_left"><span id="S5.T1.5.24.21.9.1" class="ltx_text" style="font-size:80%;">0.952</span></td>
<td id="S5.T1.5.24.21.10" class="ltx_td ltx_align_left"><span id="S5.T1.5.24.21.10.1" class="ltx_text" style="font-size:80%;">3787</span></td>
<td id="S5.T1.5.24.21.11" class="ltx_td ltx_align_left"><span id="S5.T1.5.24.21.11.1" class="ltx_text" style="font-size:80%;">3756</span></td>
<td id="S5.T1.5.24.21.12" class="ltx_td ltx_align_left"><span id="S5.T1.5.24.21.12.1" class="ltx_text" style="font-size:80%;">0.992</span></td>
</tr>
<tr id="S5.T1.5.25.22" class="ltx_tr">
<td id="S5.T1.5.25.22.1" class="ltx_td ltx_align_left"><span id="S5.T1.5.25.22.1.1" class="ltx_text" style="font-size:80%;">East Chinese Sea</span></td>
<td id="S5.T1.5.25.22.2" class="ltx_td ltx_align_left"><span id="S5.T1.5.25.22.2.1" class="ltx_text" style="font-size:80%;">25</span></td>
<td id="S5.T1.5.25.22.3" class="ltx_td ltx_align_left"><span id="S5.T1.5.25.22.3.1" class="ltx_text" style="font-size:80%;">21</span></td>
<td id="S5.T1.5.25.22.4" class="ltx_td ltx_align_left"><span id="S5.T1.5.25.22.4.1" class="ltx_text" style="font-size:80%;">4</span></td>
<td id="S5.T1.5.25.22.5" class="ltx_td ltx_align_left"><span id="S5.T1.5.25.22.5.1" class="ltx_text" style="font-size:80%;">4</span></td>
<td id="S5.T1.5.25.22.6" class="ltx_td ltx_align_left"><span id="S5.T1.5.25.22.6.1" class="ltx_text" style="font-size:80%;">0.84</span></td>
<td id="S5.T1.5.25.22.7" class="ltx_td ltx_align_left"><span id="S5.T1.5.25.22.7.1" class="ltx_text" style="font-size:80%;">0.84</span></td>
<td id="S5.T1.5.25.22.8" class="ltx_td ltx_align_left"><span id="S5.T1.5.25.22.8.1" class="ltx_text" style="font-size:80%;">0.84</span></td>
<td id="S5.T1.5.25.22.9" class="ltx_td ltx_align_left"><span id="S5.T1.5.25.22.9.1" class="ltx_text" style="font-size:80%;">0.831</span></td>
<td id="S5.T1.5.25.22.10" class="ltx_td ltx_align_left"><span id="S5.T1.5.25.22.10.1" class="ltx_text" style="font-size:80%;">1587</span></td>
<td id="S5.T1.5.25.22.11" class="ltx_td ltx_align_left"><span id="S5.T1.5.25.22.11.1" class="ltx_text" style="font-size:80%;">1540</span></td>
<td id="S5.T1.5.25.22.12" class="ltx_td ltx_align_left"><span id="S5.T1.5.25.22.12.1" class="ltx_text" style="font-size:80%;">0.97</span></td>
</tr>
<tr id="S5.T1.5.26.23" class="ltx_tr">
<td id="S5.T1.5.26.23.1" class="ltx_td ltx_align_left"><span id="S5.T1.5.26.23.1.1" class="ltx_text" style="font-size:80%;">Persian Gulf</span></td>
<td id="S5.T1.5.26.23.2" class="ltx_td ltx_align_left"><span id="S5.T1.5.26.23.2.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S5.T1.5.26.23.3" class="ltx_td"></td>
<td id="S5.T1.5.26.23.4" class="ltx_td ltx_align_left"><span id="S5.T1.5.26.23.4.1" class="ltx_text" style="font-size:80%;">10</span></td>
<td id="S5.T1.5.26.23.5" class="ltx_td"></td>
<td id="S5.T1.5.26.23.6" class="ltx_td"></td>
<td id="S5.T1.5.26.23.7" class="ltx_td"></td>
<td id="S5.T1.5.26.23.8" class="ltx_td"></td>
<td id="S5.T1.5.26.23.9" class="ltx_td"></td>
<td id="S5.T1.5.26.23.10" class="ltx_td"></td>
<td id="S5.T1.5.26.23.11" class="ltx_td"></td>
<td id="S5.T1.5.26.23.12" class="ltx_td"></td>
</tr>
<tr id="S5.T1.5.27.24" class="ltx_tr">
<td id="S5.T1.5.27.24.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T1.5.27.24.1.1" class="ltx_text" style="font-size:80%;">Sea of Azov</span></td>
<td id="S5.T1.5.27.24.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T1.5.27.24.2.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S5.T1.5.27.24.3" class="ltx_td ltx_border_bb"></td>
<td id="S5.T1.5.27.24.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T1.5.27.24.4.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S5.T1.5.27.24.5" class="ltx_td ltx_border_bb"></td>
<td id="S5.T1.5.27.24.6" class="ltx_td ltx_border_bb"></td>
<td id="S5.T1.5.27.24.7" class="ltx_td ltx_border_bb"></td>
<td id="S5.T1.5.27.24.8" class="ltx_td ltx_border_bb"></td>
<td id="S5.T1.5.27.24.9" class="ltx_td ltx_border_bb"></td>
<td id="S5.T1.5.27.24.10" class="ltx_td ltx_border_bb"></td>
<td id="S5.T1.5.27.24.11" class="ltx_td ltx_border_bb"></td>
<td id="S5.T1.5.27.24.12" class="ltx_td ltx_border_bb"></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Four data sets were generated with a minimum size of 45,000 and a maximum size of 90,000 training examples. For the largest data sets with 90,000 examples, it took a machine with 4 Intel Xeon Platinum 8260 CPUs and 2.40GHz running 190 parallel threads, 2.6 hours to generate all images along with their annotation files. Therewith, the SyntEO approach for this example shows the potential of how a large training data set can be generated on demand once an ontology is formulated and connected to an image processing backend.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">All four synthetic data sets were used to successfully optimise four deep learning models, which will further be called model-1 to 3+, corresponding to the data set. Due to the increasing complexity of the data sets, models 3 and 3+ are trained upon the most representative data sets, which results in the best performance metrics see figure <a href="#S5.F10" title="Figure 10 ‣ 5 Results ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> and table <a href="#S5.T1" title="Table 1 ‣ 5 Results ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. On a wind farm level, both models achieve recall scores of 91%. On a wind turbine level, the performance increases to 96.5% and 98.5% for models 3 and 3+, respectively. That demonstrates that the models are able to detect most wind farms confidently, and the predicted boundaries enclose almost all wind turbines of the ground truth data. At the same time, the precision values of model 3 and 3+ are well above over 80%, resulting in high F1 scores greater than 0.85 and an AP greater 0.9. Therewith, the models are not just able to securely detect OWFs but at the same time to minimise the false detection rate.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">Figure <a href="#S5.F11" title="Figure 11 ‣ 5 Results ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> shows examples of the North Sea Basin test site. The progression in model performance is clearly visible in increasing boundary refinement of the predicted OWF areas and eventually no false detection. The introduction of all size parameters in data set-2 strongly influences the model performance of model-2, resulting in less fragmented predictions than model-1, since model-2 learns to make predictions on multiple scales. The false positive detection in the South East England example, which remains until model-3, is the Triton Knoll OWF which in 2020Q3 was under construction. Magnification b2) shows a platform’s spatial pattern under construction without turbine pole and hub compared to magnification a1) of a completely deployed wind turbine. The detection is a false positive since the ground truth and synthetic training data only contain completely deployed wind turbines. The structure on a medium to large scale is already similar to a completed OWF, making it a challenging example. Interestingly, the predicted area decreases from model-2 to 3 after introducing the non-target oil rigs, which on the smallest scale look similar to a wind turbine platform under construction.</p>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
</div>
<figure id="S5.F11" class="ltx_figure"><img src="/html/2112.02829/assets/figures/nsb_res_150.png" id="S5.F11.g1" class="ltx_graphics ltx_centering ltx_img_square" width="628" height="585" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F11.2.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S5.F11.3.2" class="ltx_text" style="font-size:90%;">Model performances on the North Sea basin test site with two examples in South East England and Northern Germany. Closeup a) shows the two OWFs Horns Rev 2 and 3; closeup b) shows the Triton Knoll OWF which in 2020Q3 was under construction.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">In comparison to the North Sea Basin, the entire East China Sea test site is more complex, see figure <a href="#S5.F12" title="Figure 12 ‣ 5 Results ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>. Many OWFs are under construction with less distinct boundaries and in less homogenous areas, surrounded by harbour infrastructure, small islands and bridges. Furthermore, fewer acquisitions lead to less distinct wind turbine signatures in the S1-median images, see magnification a3). The general trend of increasing boundary refinement over all models and the onset of multi-scale detection from model-2 is the same as in the North Sea Basin test site. However, the Hangzhou Bay example of model-2 shows that when the model is trained to look on larger scales, pier structures are getting confused for OWFs. By introducing the coast scene element in data set-3, model-3 is aware of all sizes of OWF targets and pier structures as non-targets. This way, the model is now optimised to differentiate between regular structures coming from piers and OWFs. Along with the introduction of the non-target coast in data set-3, oil rigs were also introduced in this data set. Even when oil rigs do not appear in the East China Sea, these non-target training images are the reason that in the Jiangsu example, model-3 underestimates the western part of the near coast OWF cluster, see magnification b). The near coast OWF in Jiangsu is built on tidal flats, resulting in less distinct turbine signatures, as discussed earlier, see magnification 4b). These specific turbines are more similar to wind turbines under construction or oil rigs. Since model-3 has learned to differentiate between oil rigs and OWFs, these wind turbines are wrongly rejected. Interestingly, model-3 securely detects the OWF cluster beneath that area, closer to the coast and under the same tidal influence. The difference between both clusters is that the lower cluster, closer to the coast, has a stronger grid-like pattern on a medium scale, since this wind farm, the Jiangsu Rudong Offshore Intertidal Demonstration Wind Farm, is completely deployed. This observation of model behaviour supports the hypothesis that model-3 looks at features from small and medium scales. Furthermore, model-3 puts a stronger weight on a regular pattern in the medium scale as on single wind turbine features on a small scale and therefore predicts correctly on the near coast cluster and false on the less structured cluster above. Also, this is a possible explanation why model-3 wrongly includes the unfinished Triton Knoll OWF in the North Sea Basin. Finally, by including the specific wind turbine texture for turbines in tidal areas in data set-3+, the corresponding model-3+ is able to detect the western part of the OWF cluster in the Jiangsu test site.</p>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
</div>
<figure id="S5.F12" class="ltx_figure"><img src="/html/2112.02829/assets/figures/res_ecs_150.png" id="S5.F12.g1" class="ltx_graphics ltx_centering ltx_img_square" width="628" height="591" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F12.2.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="S5.F12.3.2" class="ltx_text" style="font-size:90%;">Model performances on the East Chinese Sea test site with two examples in the south of Jiangsu and the Hangzhou Bay area near Shanghai. Closeup a) shows an OWF, which is still under construction but with the most turbines completely deployed; closeup b) shows an OWF situated on tidal flats with a less distinct wind turbine signature in the Sentinel-1 median image.</span></figcaption>
</figure>
<figure id="S5.F13" class="ltx_figure"><img src="/html/2112.02829/assets/figures/pg_soa_res_150.png" id="S5.F13.g1" class="ltx_graphics ltx_centering ltx_img_square" width="628" height="541" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F13.2.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="S5.F13.3.2" class="ltx_text" style="font-size:90%;">Model performances on the Persian Gulf and Sea of Azov test sites with challenging non-targets. Closeup a) shows rectangular gridded patterns of agricultural fields and road network on the coast of the Sea of Azov; closeup b) shows oil rigs and refineries in the Persian Gulf.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">Figure <a href="#S5.F13" title="Figure 13 ‣ 5 Results ‣ SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning - Demonstrated for Offshore Wind Farm Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> shows the two non-target test sites. The Persian Gulf example shows how model-1 and 2 are unaware of non-targets similar to OWFs, since they are not included in the training data set. As in the OWF test sites, the size adaption is clearly visible from model-1 to 2. With the introduction of synthetic oil rigs as non-target training examples, the model dramatically reduces its false positive rate. However, with the introduction of the wind turbine texture for strong tidal areas, model-3+ starts to detect false positives again since the small scale feature of target class OWF is now again closer to oil rigs as in model-3. The Sea of Azov example shows how, after introducing the OWF typical grid-like patterns in medium and large scales, model-2 detects similar patterns on a similar scale within field and road networks. With the introduction of non-target inland training examples in data set-3, the models 3 and 3+ can differentiate between field-road network patterns and OWFs securely and reduce the false detection rate to 0.</p>
</div>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.1" class="ltx_p">Overall, model-3 and model-3+ are the best performing model variants, which are trained with data sets including both: a high variation in targets as well as a high variation in non-targets. This combination makes both models spatially transferable. That was demonstrated in reliable detection of OWFs at two OWF test sites, which have different characteristics in the underlying data, with less optimal conditions in the East China Sea, as well as target and non-target complexity. Furthermore, the demonstrated approach can reduce false positives by specifically including non-target training examples in the synthetic training data and thereby dramatically affecting the precision of the model performance. Besides the control over the model behaviour, the SyntEO approach also supports gathering insights in the investigated target in real-world applications. That way, the turbine type in strongly tidal areas could specifically be addressed by increasing wind turbine texture variance. Therewith, the human-machine interaction, which the domain expert started with the formulation of the ontology, continues. The model response can be interpreted as an answer that suggests that a specific anomaly in the target data was not taken into account by the human expert.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The ontology in SyntEO is a complex set of rules for a representative description of the investigated object and its common context. Hence, one could ask why this set of rules is not employed in a rule-based classification. Furthermore, the core concepts of SyntEO like the H-resolution model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, hierarchically nested scene elements <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> and the usage of auxiliary data coming from the GIS domain to investigate remote sensing images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> are together with a rule-based approach driven by expert knowledge characteristics of Geographic Object Base Image Analysis (GEOBIA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Works from this field inspired the SyntEO approach. However, compared to a typical GEOBIA workflow, SyntEO does not rely on pre-segmentation before classification since feature extraction is part of the training that also optimises for classification and unifies this in a single model. Neither does it use thresholds in a rule-based approach directly, both of which are issues of GEOBIA and often mentioned why such approaches are less spatial transferable and partly rely on heavy fine-tuning by an expert <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. In order to avoid these limitations, in SyntEO, the expert knowledge is intensively used during data set generation and strongly influences the otherwise purely data-driven deep learning approach. The deep learning model, which is optimised upon a SyntEO data set, indirectly learns a combination of expert knowledge, auxiliary data, and underlying features coming from their final spatio-temporal composition. This way, the entire SyntEO workflow and final deep learning model training is a hybrid approach of expert knowledge-driven and data-driven image analysis. SyntEO can thus be classified as an approach on the interface between rule-based image analysis on one side and deep learning and Big-EO data on the other side.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">During the demonstration of SyntEO, it could be shown how the approach can be utilised to establish human-machine interactions with fully controllable experiment environments and adjustable data set variants. Therewith, it was possible to make assumptions over the training process and model behaviour which is an important cornerstone towards explainable machine learning and interpretability of artificial intelligence, which is a current challenge in Earth observation and artificial intelligence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. The SyntEO approach starts a dialogue between machine and expert by using the ontology as a medium for exchange that both sides can interpret. Together with large scale ablation studies, there is the possibility of modelling complex systems and investigating them in order to explain the outcome of a prediction by comparing them with existing knowledge. Furthermore, such a stable and adjustable experiment environment can also be used to discover new insights like anomalies that diverge from a stable, defined training data set but are included and important characteristics in real-world data. In every application of SyntEO, the ontology serves as a basic structure that allows experts from different domains and backgrounds to set up an environment in which they can start a machine-human dialogue, regardless of whether the knowledge they bring to the ontology is based on physical models or has a purely artistic origin.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">The aforementioned advantages of the SyntEO approach are in addition to the original motivation: The generation of large training data sets that are particularly suitable for Earth observation. The example given illustrates how the approach can generate a huge data set with up to 90,000 images, which is the multiple of existing real-world examples of the target objects. Due to the synthetic data set, it is now possible to train deep learning models in a supervised manner since the number of training examples is no longer an issue. Therewith, SyntEO provides the possibility to investigate objects which are scarce but scattered over many thousands of remote sensing images by applying a synthetic data approach. This opens up new opportunities for novel applications in Earth observation with deep learning.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">However, an argument against a SyntEO data set might be that it will never be able to generate highly complex ecosystems or that the expenses are too high to build such a complex synthetic environment. The counterargument is that a hybrid approach of synthetic and hand labelled data should be considered before this red line is crossed. This way, one part of the training data is created with a less complex SyntEO approach, and another, smaller part of the training data would be annotated manually. During training, the SyntEO data set can be used as a pre-training data set to move the model’s parameters in the right direction and finally transfer learning on the small but highly complex real-world data set to fine-tune the model and learn the real complexity of the given task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">By employing the SyntEO approach, it is possible to generate large scale, deep learning ready data set. SyntEO is based on fundamental characteristics of Earth observation data and strongly takes multi-scale, spatio-temporal context during data synthesising into account. Domain experts can include their knowledge in the data generation process by making it explicit and machine-readable by embedding it in an ontology. An artificial data generator uses this knowledge representation to build the training data set. Detailled control over the generation process can be gained by deactivating and activating parts of the ontology. Thus, it is possible to establish stable and highly adjustable experiment environments. Therewith, SyntEO offers the possibility to establish human-machine interactions which provide a fundament to gather insights in the machine learning process.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">In a first hands-on example, offshore wind farms were detected in the North Sea Basin and East China Sea with a deep learning CNN model solely optimised on a SyntEO data set with the largest size of 90,000 generated images. High scores in recall and precision confirm the ability of the proposed approach to detect the target of interest by minimising false detections securely. Furthermore, conclusions about the training process and model behaviour could be made by conducting an ablation study with different data set variants. On a single local example of wind turbines in tidal flats, it was possible to show how the sound experiment design of SyntEO can be used to raise awareness of anomalies of the target object in real-world data, which are not included in the synthetic training data.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">Overall, SyntEO is a flexible approach that is built around the introduced ontology concept, which makes expert knowledge accessible for automatic data generation in Earth observation. Future studies which apply the SyntEO approach will show how it can be utilised for different application domains to get new insights into Earth observation and deep learning.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Funding</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research received no external funding.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Declaration of competing interest</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">The authors declare no conflict of interest.</p>
</div>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Author Contributions</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">Thorsten Hoeser: Conceptualisation, original manuscript writing, code development, data processing, visualisation; Claudia Kuenzer: Supervision and manuscript reviewing.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Amina Adadi and Mohammed Berrada.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Peeking inside the black-box: A survey on explainable artificial
intelligence (xai).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Access</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 6:52138–52160, 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Pragya Agarwal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Ontological considerations in giscience.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Geographical Information Science</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">,
19(5):501–536, 2005.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Peri Akiva, Matthew Purri, and Matthew J. Leotta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Self-supervised material and texture representation learning for
remote sensing tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, abs/2112.01715, 2021.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Khitem Amiri, Mohamed Farah, and Imed Riadh Farah.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Fuzzy hypergraph of concepts for semantic annotation of remotely
sensed images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 International Conference on Advanced Technologies for
Signal and Image Processing (ATSIP)</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 1–8, 2017.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Damien Arvor, Mariana Belgiu, Zoe Falomir, Isabelle Mougenot, and Laurent
Durieux.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Ontologies to interpret remote sensing images: why do we need them?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">GIScience &amp; Remote Sensing</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 56(6):911–939, 2019.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Stefan Auer, Richard Bamler, and Peter Reinartz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Raysar - 3d sar simulator: Now open source.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2016 IEEE International Geoscience and Remote Sensing
Symposium (IGARSS)</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 6730–6733, 2016.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Alice M. Baldridge, Simon J. Hook, Cindy I. Grove, and Gerardo Rivera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">The aster spectral library version 2.0.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Remote Sensing of Environment</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 113(4):711–715, 2009.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien
Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez,
Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Explainable artificial intelligence (xai): Concepts, taxonomies,
opportunities and challenges toward responsible ai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Information Fusion</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 58:82–115, 2020.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Emily E. Berkson, Jared D. VanCor, Steven Esposito, Gary Chern, and Mark Pritt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Synthetic data generation to mitigate the low/no-shot problem in
machine learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 IEEE Applied Imagery Pattern Recognition Workshop
(AIPR)</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 1–7, 2019.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Thomas Blaschke.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Object based image analysis for remote sensing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ISPRS Journal of Photogrammetry and Remote Sensing</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">,
65(1):2–16, 2010.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Thomas Blaschke, Geoffrey J. Hay, Maggi Kelly, Stefan Lang, Peter Hofmann,
Elisabeth Addink, Raul Queiroz Feitosa, Freek van der Meer, Harald van
der Werff, Frieke van Coillie, and Dirk Tiede.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Geographic object-based image analysis – towards a new paradigm.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ISPRS Journal of Photogrammetry and Remote Sensing</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">,
87:180–191, 2014.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Blender Online Community.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Blender - a 3D modelling and rendering package</span><span id="bib.bib12.3.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.4.1" class="ltx_text" style="font-size:90%;">Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Rethinking atrous convolution for semantic image segmentation, 2017.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Shrey Dabhi, Kartavya Soni, Utkarsh Patel, Priyanka Sharma, and Manojkumar
Parmar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Virtual sar: A synthetic dataset for deep learning based speckle
noise reduction algorithms, 2020.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Arun Das and Paul Rad.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Opportunities and challenges in explainable artificial intelligence
(XAI): A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, abs/2006.11371, 2020.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Häusser, Caner
Hazirbas, Vladimir Golkov, Patrick V. D. Smagt, Daneil Cremers, and Thomas.
Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Flownet: Learning optical flow with convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2015 IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">,
pages 2758–2766, 2015.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
European Commission.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">An EU strategy to harness the potential of offshore renewable
energy for a climate neutral future, 2020.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn, and
Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">The pascal visual object classes (voc) challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 88(2):303–338, June
2010.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Noel Gorelick, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, and
Rebecca Moore.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Google earth engine: Planetary-scale geospatial analysis for
everyone.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Remote Sensing of Environment</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 202:18–27, 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.5.1" class="ltx_text" style="font-size:90%;">Big Remotely Sensed Data: tools, applications and experiences.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Thomas R. Gruber.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Toward principles for the design of ontologies used for knowledge
sharing?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Human-Computer Studies</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">,
43(5):907–928, 1995.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
David Gunning and David Aha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Darpa’s explainable artificial intelligence (xai) program.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">AI Magazine</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 40(2):44–58, Jun. 2019.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
David Gunning, Mark Stefik, Jaesik Choi, Timothy Miller, Simone Stumpf, and
Guang-Zhong Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">XAI-explainable artificial intelligence.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Science Robotics</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 4(37):eaay7120, 2019.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Laura Gușatu, Stefano Menegon, Daniel Depellegrin, Christian Zuidema, Aandé
Faaij, and Claudia Yamu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Spatial and temporal analysis of cumulative environmental effects of
offshore wind farms in the north sea basin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Scientific Reports</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 11, 2021.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Sanghui Han, Alex Fafard, John Kerekes, Michael Gartley, Emmett Ientilucci,
Andreas Savakis, Charles Law, Jason Parhan, Matt Turek, Keith Fieldhouse, and
Todd Rovito.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Efficient generation of image chips for training deep learning
algorithms.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In Firooz A. Sadjadi and Abhijit Mahalanobis, editors, </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Automatic
Target Recognition XXVII</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, volume 10202, pages 15 – 23. International
Society for Optics and Photonics, SPIE, 2017.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Thorsten Hoeser, Felix Bachofer, and Claudia Kuenzer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Object detection and image segmentation with deep learning on earth
observation data: A review—part II: Applications.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Remote Sensing</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 12(18), 2020.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Thorsten Hoeser and Claudia Kuenzer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Object detection and image segmentation with deep learning on earth
observation data: A review-part I: Evolution and recent trends.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Remote Sensing</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, 12(10), 2020.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Furkan Isikdogan, Alan Bovik, and Paola Passalacqua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Learning a river network extractor using an adaptive loss function.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Geoscience and Remote Sensing Letters</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 15(6):813–817,
2018.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Samin Khan, Buu Phan, Rick Salay, and Krzysztof Czarnecki.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Procsy: Procedural synthetic dataset generation towards influence
factor studies of semantic segmentation networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) Workshops</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 88–96, June 2019.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Fanjie Kong, Bohao Huang, Kyle Bradbury, and Jordan M. Malof.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">The synthinel-1 dataset: a collection of high resolution synthetic
overhead imagery for building segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2020 IEEE Winter Conference on Applications of Computer
Vision (WACV)</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages 1803–1812, 2020.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Imagenet classification with deep convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger,
editors, </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems 25</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages
1097–1105. Curran Associates, Inc., 2012.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Stefan Lang and Thomas Blaschke.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Bridging remote sensing and gis - what are the main supportive
pillars?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In Elisabeth Schöpfer Stefan Lang, Thomas Blaschke, editor, </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">1st
International Conference on Object-based Image Analysis (OBIA 2006)</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">. ISPRS,
June-July 2006.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Stefan Lang, Geoffrey J. Hay, Andrea Baraldi, Dirk Tiede, and Thomas Blaschke.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Geobia achievements and spatial opportunities in the era of big earth
observation data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ISPRS International Journal of Geo-Information</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 8(11), 2019.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Deep learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nature</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, 521(7553):436–444, 2015.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Piotr Dollár, Ross B. Girshick, Kaiming He, Bharath
Hariharan, and Serge J. Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Feature pyramid networks for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, pages 936–944, 2016.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Ilya Loshchilov and Frank Hutter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Sgdr: Stochastic gradient descent with warm restarts, 2017.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Lei Ma, Yu Liu, Xueliang Zhang, Yuanxin Ye, Gaofei Yin, and Brian Alan Johnson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Deep learning in remote sensing applications: A meta-analysis and
review.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ISPRS Journal of Photogrammetry and Remote Sensing</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">, 152:166 –
177, 2019.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Joshua Madin, Shawn Bowers, Mark Schildhauer, Sergeui Krivov, Deana Pennington,
and Ferdinando Villa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">An ontology for describing and synthesizing ecological observation
data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Ecological Informatics</span><span id="bib.bib38.4.2" class="ltx_text" style="font-size:90%;">, 2(3):279–296, 2007.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.5.1" class="ltx_text" style="font-size:90%;">Meta-information systems and ontologies. A Special Feature from the
5th International Conference on Ecological Informatics ISEI5, Santa Barbara,
CA, Dec. 4–7, 2006.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Codescu Mihai, Gregor Horsinka, Oliver Kutz, Till Mossakowski, and Rafaela Rau.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Osmonto-an ontology of openstreetmap tags.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Paper presented at the State of the Map Europe (SOTM-EU)
Conference</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, July 2011.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Niklas Moran, Simon Nieland, Gregor Tintrup gen. Suntrup, and Birgit
Kleinschmit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Combining machine learning and ontological data handling for
multi-source classification of nature conservation areas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Applied Earth Observation and
Geoinformation</span><span id="bib.bib40.4.2" class="ltx_text" style="font-size:90%;">, 54:124–133, 2017.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Bochra Nasri, Hafedh Nefzi, and Mohamed Farah.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Towards a hybrid approach for remote sensing ontology construction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 4th International Conference on Advanced Technologies
for Signal and Image Processing (ATSIP)</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, pages 1–5, 2018.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Sergey I. Nikolenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Synthetic Data for Deep Learning</span><span id="bib.bib42.3.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.4.1" class="ltx_text" style="font-size:90%;">Springer International Publishing, Cham, 2021.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Rafael Padilla, Wesley L. Passos, Thadeu L. B. Dias, Sergio L. Netto, and
Eduardo A. B. da Silva.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">A comparative analysis of object detection metrics with a companion
open-source toolkit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Electronics</span><span id="bib.bib43.4.2" class="ltx_text" style="font-size:90%;">, 10(3), 2021.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Ken Perlin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">An image synthesizer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">SIGGRAPH Comput. Graph.</span><span id="bib.bib44.4.2" class="ltx_text" style="font-size:90%;">, 19(3):287–296, jul 1985.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Ken Perlin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Chapter 2, noise hardware.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">SIGGRAPH 2001</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, pages 2–1 – 2–24, New York, NY, USA, 2001.
Association for Computing Machinery.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Markus Reichstein, Gustau Camps-Valls, Bjorn Stevens, Martin Jung, Joachim
Denzler, Nuno Carvalhais, and Prabhat.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Deep learning and process understanding for data-driven earth system
science.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nature</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 566:195–204, 2019.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib47.4.2" class="ltx_text" style="font-size:90%;">,
39:1137–1149, 2015.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Evan Shelhamer, Jonathan Long, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Fully convolutional networks for semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib48.4.2" class="ltx_text" style="font-size:90%;">,
39:640–651, 2014.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Jacob Shermeyer, Thomas Hossler, Adam Van Etten, Daniel Hogan, Ryan Lewis, and
Daeil Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Rareplanes: Synthetic data takes flight.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision (WACV)</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, pages 207–217, January 2021.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Alan H. Strahler, Curtis E. Woodcock, and James A. Smith.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">On the nature of models in remote sensing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Remote Sensing of Environment</span><span id="bib.bib50.4.2" class="ltx_text" style="font-size:90%;">, 20(2):121–139, 1986.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Xian Sun, Bing Wang, Zhirui Wang, Hao Li, Hengchao Li, and Kun Fu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Research progress on few-shot learning for remote sensing image
interpretation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Journal of Selected Topics in Applied Earth Observations
and Remote Sensing</span><span id="bib.bib51.4.2" class="ltx_text" style="font-size:90%;">, 14:2387–2402, 2021.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Chao Tao, Ji Qi, Weipeng Lu, Hao Wang, and Haifeng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Remote sensing image scene classification with self-supervised
paradigm under limited labeled samples.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Geoscience and Remote Sensing Letters</span><span id="bib.bib52.4.2" class="ltx_text" style="font-size:90%;">, 19:1–5, 2022.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Tremblay, Thang To, and Stan Birchfield.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Falling things: A synthetic dataset for 3d object detection and pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops (CVPRW)</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, pages 2119–21193, 2018.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Devis Tuia, Ribana Roscher, Jan Dirk Wegner, Nathan Jacobs, Xiaoxiang Zhu, and
Gustau Camps-Valls.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Toward a collective agenda on ai for earth science data analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Geoscience and Remote Sensing Magazine</span><span id="bib.bib54.4.2" class="ltx_text" style="font-size:90%;">, 9(2):88–104,
2021.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Sergio Vitale, Giampaolo Ferraioli, and Vito Pascazio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Analysis on the building of training dataset for deep learning sar
despeckling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Geoscience and Remote Sensing Letters</span><span id="bib.bib55.4.2" class="ltx_text" style="font-size:90%;">, pages 1–5, 2021.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Immanuel Weber, Jens Bongartz, and Ribana Roscher.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Artificial and beneficial – exploiting artificial images for aerial
vehicle detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ISPRS Journal of Photogrammetry and Remote Sensing</span><span id="bib.bib56.4.2" class="ltx_text" style="font-size:90%;">,
175:158–170, 2021.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Jianguo Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Hierarchy and scaling: Extrapolating information along a scaling
ladder.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Canadian Journal of Remote Sensing</span><span id="bib.bib57.4.2" class="ltx_text" style="font-size:90%;">, 25(4):367–380, 1999.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">How transferable are features in deep neural networks?, 2014.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Dingwen Zhang, Junwei Han, Gong Cheng, Zhenbao Liu, Shuhui Bu, and Lei Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">Weakly supervised learning for target detection in remote sensing
images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Geoscience and Remote Sensing Letters</span><span id="bib.bib59.4.2" class="ltx_text" style="font-size:90%;">, 12(4):701–705,
2015.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Ting Zhang, Bo Tian, Dhritiraj Sengupta, Lei Zhang, and Yali Si.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Global offshore wind turbine dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Scientific Data</span><span id="bib.bib60.4.2" class="ltx_text" style="font-size:90%;">, 8(1):1–12, 2021.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
Xiao X. Zhu, Devis Tuia, Lichao Mou, Gui-Song Xia, Liangpei Zhang,
Feng Xu, and Friedrich Fraundorfer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">Deep learning in remote sensing: A comprehensive review and list of
resources.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Geoscience and Remote Sensing Magazine</span><span id="bib.bib61.4.2" class="ltx_text" style="font-size:90%;">, 5(4):8–36, 2017.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2112.02828" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2112.02829" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2112.02829">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2112.02829" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2112.02830" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 15:53:18 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
