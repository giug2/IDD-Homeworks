<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>An Empirical Study of In-context Learning in LLMs for Machine Translation</title>
<!--Generated on Tue Jun  4 19:34:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2401.12097v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#S1" title="In An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_serif">1</span> </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#S2" title="In An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_serif">2</span> </span>Related Works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#S3" title="In An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_serif">3</span> </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#S3.SS1" title="In 3 Methodology ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_serif">3.1</span> </span>Instruction Variations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#S3.SS2" title="In 3 Methodology ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_serif">3.2</span> </span>Demonstration Perturbations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#S3.SS3" title="In 3 Methodology ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_serif">3.3</span> </span>Role of Demonstration Attributes</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#S4" title="In An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_serif">4</span> </span>Experimental setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#S5" title="In An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_serif">5</span> </span>Results and Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#S5.SS1" title="In 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_serif">5.1</span> </span>Sensitivity to Instruction Variations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#S5.SS2" title="In 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_serif">5.2</span> </span>Example Perturbations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#S5.SS2.SSS1" title="In 5.2 Example Perturbations ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_serif">5.2.1</span> </span>Homogeneous Perturbations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#S5.SS2.SSS2" title="In 5.2 Example Perturbations ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_serif">5.2.2</span> </span>Heterogeneous Perturbations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#S5.SS2.SSS3" title="In 5.2 Example Perturbations ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_serif">5.2.3</span> </span>Demonstrations from Allied Tasks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#S5.SS3" title="In 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_serif">5.3</span> </span>Misalignment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#S5.SS4" title="In 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_serif">5.4</span> </span>Directionality of ICL Demonstrations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#S6" title="In An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_serif">6</span> </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#S7" title="In An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_serif">7</span> </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#S8" title="In An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_serif">8</span> </span>Ethical Considerations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#S9" title="In An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_serif">9</span> </span>Acknowledgements</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#A1" title="In An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">A</span> </span><span class="ltx_text ltx_font_typewriter">Fine-tuning BLOOM 7B on MT</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#A2" title="In An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">B</span> </span><span class="ltx_text ltx_font_typewriter">Use of model-based metrics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#A3" title="In An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">C</span> </span><span class="ltx_text ltx_font_typewriter">Prompt Templates</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#A4" title="In An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">D</span> </span><span class="ltx_text ltx_font_typewriter">Instruction Templates</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#A5" title="In An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">E</span> </span><span class="ltx_text ltx_font_typewriter">Examples of Perturbation Variants</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#A6" title="In An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">F</span> </span><span class="ltx_text ltx_font_typewriter">Languages and Directions Considered</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#A7" title="In An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">G</span> </span><span class="ltx_text ltx_font_typewriter">Additional results</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_font_typewriter ltx_title_document">An Empirical Study of In-context Learning 
<br class="ltx_break"/>in LLMs for Machine Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="id1.1.1">Pranjal A. Chitale<sup class="ltx_sup" id="id1.1.1.1"><span class="ltx_text ltx_font_serif ltx_font_medium ltx_font_italic" id="id1.1.1.1.1">1,2</span></sup></span><span class="ltx_text ltx_font_typewriter" id="id8.8.id1">    </span><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="id2.2.2">Jay Gala <sup class="ltx_sup" id="id2.2.2.1"><span class="ltx_text ltx_font_serif ltx_font_medium ltx_font_italic" id="id2.2.2.1.1">3∗</span></sup></span><span class="ltx_text ltx_font_typewriter" id="id9.9.id2">  </span><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="id3.3.3">Raj Dabre<sup class="ltx_sup" id="id3.3.3.1"><span class="ltx_text ltx_font_serif ltx_font_medium ltx_font_italic" id="id3.3.3.1.1">4</span></sup></span><span class="ltx_text ltx_font_typewriter" id="id10.10.id3">
</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id11.11.id4">
</span><sup class="ltx_sup" id="id12.12.id5"><span class="ltx_text ltx_font_italic" id="id12.12.id5.1">1</span></sup><span class="ltx_text ltx_font_typewriter" id="id13.13.id6">Nilekani Centre at AI4Bharat  </span><sup class="ltx_sup" id="id14.14.id7"><span class="ltx_text ltx_font_italic" id="id14.14.id7.1">2</span></sup><span class="ltx_text ltx_font_typewriter" id="id15.15.id8">IIT Madras </span>
<br class="ltx_break"/><sup class="ltx_sup" id="id16.16.id9"><span class="ltx_text ltx_font_italic" id="id16.16.id9.1">3</span></sup><span class="ltx_text ltx_font_typewriter" id="id17.17.id10">Mohamed bin Zayed University of Artificial Intelligence </span>
<br class="ltx_break"/><sup class="ltx_sup" id="id18.18.id11"><span class="ltx_text ltx_font_italic" id="id18.18.id11.1">4</span></sup><span class="ltx_text ltx_font_typewriter" id="id19.19.id12">National Institute of Information and Communications Technology, Kyoto, Japan
</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id20.20.id13">
cs21s022@cse.iitm.ac.in, jay.gala@mbzuai.ac.ae, prajdabre@nict.go.jp</span>
</span><span class="ltx_author_notes"><span class="ltx_text ltx_font_typewriter" id="id21.21.id1">  Equal contribution</span><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="id22.22.id1"> Work done at Nilekani Centre at AI4Bharat</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id23.id1"><span class="ltx_text ltx_font_typewriter" id="id23.id1.1">Recent interest has surged in employing Large Language Models (LLMs) for machine translation (MT) via in-context learning (ICL) </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="id23.id1.2.1">(</span><span class="ltx_text ltx_font_typewriter">Vilar et al.</span><span class="ltx_text ltx_font_typewriter" id="id23.id1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib47" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="id23.id1.4.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="id23.id1.5">. Most prior studies primarily focus on optimizing translation quality, with limited attention to understanding the specific aspects of ICL that influence the said quality. To this end, we perform the first of its kind, an exhaustive study of in-context learning for machine translation. We first establish that ICL is primarily example-driven and not instruction-driven. Following this, we conduct an extensive exploration of various aspects of the examples to understand their influence on downstream performance. Our analysis includes factors such as quality and quantity of demonstrations, spatial proximity, and source versus target originality. Further, we also investigate challenging scenarios involving indirectness and misalignment of examples to understand the limits of ICL. While we establish the significance of the quality of the target distribution over the source distribution of demonstrations, we further observe that perturbations sometimes act as regularizers, resulting in performance improvements. Surprisingly, ICL does not necessitate examples from the same task, and a related task with the same target distribution proves sufficient. We hope that our study acts as a guiding resource for considerations in utilizing ICL for MT. Our code is available on </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/PranjalChitale/in-context-mt-analysis" title="">https://github.com/PranjalChitale/in-context-mt-analysis</a><span class="ltx_text ltx_font_typewriter" id="id23.id1.6">.</span></p>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\newmdenv</span>
<p class="ltx_p" id="p1.2">[backgroundcolor=gray!10,
linecolor=gray!70,
linewidth=1pt,
roundcorner=5pt,
skipabove=10pt,
skipbelow=10pt,
font=<span class="ltx_text ltx_font_typewriter" id="p1.2.1">,
]promptbox
<span class="ltx_ERROR undefined" id="p1.2.1.1">\newmdenv</span>[
backgroundcolor=blue!5,
linecolor=blue!70,
linewidth=1pt,
roundcorner=5pt,
skipabove=10pt,
skipbelow=10pt,
font=,
]keytakeaway



</span></p>
</div>
<div class="ltx_para ltx_noindent" id="p2">
<div class="ltx_block ltx_align_bottom" id="p2.7">
<p class="ltx_p" id="p2.7.8"><span class="ltx_text ltx_font_bold" id="p2.7.8.1">An Empirical Study of In-context Learning 
<br class="ltx_break"/>in LLMs for Machine Translation</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p2.7.7" style="width:433.6pt;"><span class="ltx_text ltx_font_typewriter" id="p2.7.7.8"></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" id="p2.7.7.7" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p2.7.7.7.7">
<span class="ltx_tbody">
<span class="ltx_tr" id="p2.3.3.3.3.3">
<span class="ltx_td ltx_align_center" id="p2.3.3.3.3.3.3" style="padding-bottom:4.30554pt;"><span class="ltx_text ltx_font_bold" id="p2.3.3.3.3.3.3.3">Pranjal A. Chitale<sup class="ltx_sup" id="p2.3.3.3.3.3.3.3.1"><span class="ltx_text ltx_font_serif ltx_font_medium ltx_font_italic" id="p2.3.3.3.3.3.3.3.1.1">1,2</span></sup><span class="ltx_note ltx_role_thanks" id="p2.3.3.3.3.3.3.3.2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span><span class="ltx_text ltx_font_serif" id="p2.3.3.3.3.3.3.3.2.1">  Equal contribution</span></span></span></span><span class="ltx_text ltx_font_serif" id="p2.3.3.3.3.3.3.3.3">    </span>Jay Gala <sup class="ltx_sup" id="p2.3.3.3.3.3.3.3.4"><span class="ltx_text ltx_font_serif ltx_font_medium ltx_font_italic" id="p2.3.3.3.3.3.3.3.4.1">3∗</span></sup><span class="ltx_note ltx_role_thanks" id="p2.3.3.3.3.3.3.3.5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span> Work done at Nilekani Centre at AI4Bharat</span></span></span><span class="ltx_text ltx_font_serif" id="p2.3.3.3.3.3.3.3.6">  </span>Raj Dabre<sup class="ltx_sup" id="p2.3.3.3.3.3.3.3.7"><span class="ltx_text ltx_font_serif ltx_font_medium ltx_font_italic" id="p2.3.3.3.3.3.3.3.7.1">4</span></sup></span></span></span>
<span class="ltx_tr" id="p2.5.5.5.5.5">
<span class="ltx_td ltx_align_center" id="p2.5.5.5.5.5.2"><sup class="ltx_sup" id="p2.5.5.5.5.5.2.1"><span class="ltx_text ltx_font_serif ltx_font_italic" id="p2.5.5.5.5.5.2.1.1">1</span></sup>Nilekani Centre at AI4Bharat  <sup class="ltx_sup" id="p2.5.5.5.5.5.2.2"><span class="ltx_text ltx_font_serif ltx_font_italic" id="p2.5.5.5.5.5.2.2.1">2</span></sup>IIT Madras</span></span>
<span class="ltx_tr" id="p2.6.6.6.6.6">
<span class="ltx_td ltx_align_center" id="p2.6.6.6.6.6.1"><sup class="ltx_sup" id="p2.6.6.6.6.6.1.1"><span class="ltx_text ltx_font_serif ltx_font_italic" id="p2.6.6.6.6.6.1.1.1">3</span></sup>Mohamed bin Zayed University of Artificial Intelligence</span></span>
<span class="ltx_tr" id="p2.7.7.7.7.7">
<span class="ltx_td ltx_align_center" id="p2.7.7.7.7.7.1" style="padding-bottom:4.30554pt;"><sup class="ltx_sup" id="p2.7.7.7.7.7.1.1"><span class="ltx_text ltx_font_serif ltx_font_italic" id="p2.7.7.7.7.7.1.1.1">4</span></sup>National Institute of Information and Communications Technology, Kyoto, Japan</span></span>
<span class="ltx_tr" id="p2.7.7.7.7.8.1">
<span class="ltx_td ltx_align_center" id="p2.7.7.7.7.8.1.1">cs21s022@cse.iitm.ac.in, jay.gala@mbzuai.ac.ae, prajdabre@nict.go.jp</span></span>
</span>
</span></span><span class="ltx_text ltx_font_typewriter" id="p2.7.7.9"></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_font_typewriter ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text ltx_font_serif" id="S1.1.1.1">1</span> </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1"><span class="ltx_text ltx_font_typewriter" id="S1.p1.1.1">Large Language Models leverage In-Context Learning to effectively solve diverse downstream tasks </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S1.p1.1.2.1">(</span><span class="ltx_text ltx_font_typewriter">Brown et al.</span><span class="ltx_text ltx_font_typewriter" id="S1.p1.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib5" title=""><span class="ltx_text ltx_font_typewriter">2020</span></a>; <span class="ltx_text ltx_font_typewriter">Dong et al.</span><span class="ltx_text ltx_font_typewriter" id="S1.p1.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib12" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a>; <span class="ltx_text ltx_font_typewriter">Liu et al.</span><span class="ltx_text ltx_font_typewriter" id="S1.p1.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib26" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a>; <span class="ltx_text ltx_font_typewriter">OpenAI et al.</span><span class="ltx_text ltx_font_typewriter" id="S1.p1.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib34" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a>; <span class="ltx_text ltx_font_typewriter">Chowdhery et al.</span><span class="ltx_text ltx_font_typewriter" id="S1.p1.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib9" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a><span class="ltx_text ltx_font_typewriter" id="S1.p1.1.4.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S1.p1.1.5">. This inference-only method involves conditioning the model with task-specific demonstrations consisting of input-output pairs within the prompt before the actual test example. Most recently, MT via ICL has become popular </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Vilar et al.</span> <span class="ltx_text ltx_font_typewriter" id="S1.p1.1.6.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib47" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S1.p1.1.7.2.2.1">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S1.p1.1.8">, however, there is still a limited understanding of how ICL works and its aspects in this context. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Raunak et al.</span> <span class="ltx_text ltx_font_typewriter" id="S1.p1.1.9.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib39" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S1.p1.1.10.2.2.1">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S1.p1.1.11"> make some initial explorations on partial test sets, but there are still open questions pertaining to whether ICL is example-driven or instruction-driven, whether all examples contribute equally or not, and its potential to enforce control over the generation to regulate the pre-training biases observed in models.
To answer these questions, in this paper, we conduct experiments by perturbing either the instructions or the demonstrations while keeping the other clean to assess the model’s ability to perform tasks appropriately. This allows us to understand if clear instructions can guide the model effectively even when examples are perturbed and, conversely, if clean examples can steer the model to perform the appropriate task despite the instruction being misleading.
We assess 6 models across 2 language families, spanning 12 language pairs on complete test sets across various noise thresholds by considering realistic perturbation attacks.</span></p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1"><span class="ltx_text ltx_font_typewriter" id="S1.p2.1.1">When perturbing examples, we study 1) whether ICL treats all examples equally or favors those in spatial proximity to the test example, 2) whether utilizing in-context examples from related tasks can effectively guide models in performing another task to simulate the unavailability of demonstrations for some language pairs, 3) whether various choices of auxiliary source language for the demonstrations influence downstream MT, 4) whether models can implicitly make associations based on contextual information via a transitive MT setup and finally, 5) whether smaller models are susceptible to contextual misinformation or if their pre-training biases act as safeguards against it.
Our work aims to holistically understand ICL and its driving factors for MT tasks with experiments spanning over </span><span class="ltx_text ltx_font_typewriter ltx_font_italic" id="S1.p2.1.2">25K</span><span class="ltx_text ltx_font_typewriter" id="S1.p2.1.3"> runs across various models and language directions. We observe that examples significantly influence ICL, while instructions have a limited impact. Notably, target distribution plays a more significant role than source distribution in ICL demonstrations. While perturbations might seem potentially harmful, in some cases, they can act as a form of regularization, particularly evident in the Llama 2 7B model. Spatial proximity emerges as a critical factor, implying that clean examples should be placed closer and noisy ones farther during in-context example selection for optimal downstream performance. Furthermore, our findings suggest that examples from related tasks can suffice for ICL in MT, with the constraint that the target language of the demonstrations matches the language of the test example while the choice of the source language is inconsequential. The directionality of the demonstrations has minimal impact, with both target-original and source-original demonstrations being equally effective. ICL has the potential to override semantic priors and may be exploited to misguide models or generate misinformation. We believe our study offers valuable insights for practitioners and would be widely generalizable.</span></p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_font_typewriter ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text ltx_font_serif" id="S2.1.1.1">2</span> </span>Related Works</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S2.p1.1.1">In-context Learning:</span><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.2">
Large Language Models have demonstrated strong performance in various downstream tasks through supervised fine-tuning on task-specific data </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.3.1">(</span><span class="ltx_text ltx_font_typewriter">Devlin et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.4.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib11" title=""><span class="ltx_text ltx_font_typewriter">2019</span></a>; <span class="ltx_text ltx_font_typewriter">Raffel et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.4.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib38" title=""><span class="ltx_text ltx_font_typewriter">2020</span></a>; <span class="ltx_text ltx_font_typewriter">Lewis et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.4.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib24" title=""><span class="ltx_text ltx_font_typewriter">2020</span></a>; <span class="ltx_text ltx_font_typewriter">Radford et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.4.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib37" title=""><span class="ltx_text ltx_font_typewriter">2019</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.5.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.6">. ICL </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.7.1">(</span><span class="ltx_text ltx_font_typewriter">Brown et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.8.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib5" title=""><span class="ltx_text ltx_font_typewriter">2020</span></a>; <span class="ltx_text ltx_font_typewriter">Dong et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.8.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib12" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a>; <span class="ltx_text ltx_font_typewriter">Liu et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.8.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib26" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a>; <span class="ltx_text ltx_font_typewriter">OpenAI et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.8.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib34" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a>; <span class="ltx_text ltx_font_typewriter">Chowdhery et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.8.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib9" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.9.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.10"> is a training-free method that has emerged as a cost-effective approach given the increasing size of LLMs (</span><math alttext="\geq" class="ltx_Math" display="inline" id="S2.p1.1.m1.1"><semantics id="S2.p1.1.m1.1a"><mo id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><geq id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S2.p1.1.m1.1d">≥</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.11"> 7B parameters).
ICL aims to implicitly learn the task-solving abilities through a few hand-crafted demonstrations. These emergent capabilities </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.12.1">(</span><span class="ltx_text ltx_font_typewriter">Wei et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.13.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib48" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a>; <span class="ltx_text ltx_font_typewriter">Lu et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.13.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib27" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.14.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.15"> are driven by the distribution of the pretraining data </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.16.1">(</span><span class="ltx_text ltx_font_typewriter">Chan et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.17.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib6" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a>; <span class="ltx_text ltx_font_typewriter">Briakou et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.17.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib4" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.18.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.19">.
Intuitively, ICL can be considered analogous to performing a gradient descent implicitly during the inference </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.20.1">(</span><span class="ltx_text ltx_font_typewriter">Akyürek et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.21.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib2" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a>; <span class="ltx_text ltx_font_typewriter">Li et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.21.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib25" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a>; <span class="ltx_text ltx_font_typewriter">Zhang et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.21.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib55" title=""><span class="ltx_text ltx_font_typewriter">2023b</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.22.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.23">.
</span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Vilar et al.</span> <span class="ltx_text ltx_font_typewriter" id="S2.p1.1.24.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib47" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.25.2.2.1">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.26"> explored few-shot prompting strategies for MT task on PaLM </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.27.1">(</span><span class="ltx_text ltx_font_typewriter">Chowdhery et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.28.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib9" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.29.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.30"> suggesting that the downstream MT performance is largely correlated with the quality of demonstrations. Subsequently, several recent works have also explored ICL capabilities of LLMs for the MT task </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Robinson et al.</span> <span class="ltx_text ltx_font_typewriter" id="S2.p1.1.31.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib41" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.32.2.2.1">)</span>; <span class="ltx_text ltx_font_typewriter">Zhang et al.</span> <span class="ltx_text ltx_font_typewriter" id="S2.p1.1.31.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib53" title=""><span class="ltx_text ltx_font_typewriter">2023a</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.32.2.2.1">)</span>; <span class="ltx_text ltx_font_typewriter">Zhu et al.</span> <span class="ltx_text ltx_font_typewriter" id="S2.p1.1.31.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib59" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.32.2.2.1">)</span>; <span class="ltx_text ltx_font_typewriter">Kumar et al.</span> <span class="ltx_text ltx_font_typewriter" id="S2.p1.1.31.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib23" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.32.2.2.1">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.33">. However, none of these works delve into a fine-grained examination of the underlying factors influencing the model performance; a gap that we fill with our study.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S2.p2.1.1">Factors impacting in-context learning:</span><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.2">
Several aspects of the demonstrations, such as input distribution, output distribution, or input-output alignment, can play a significant role in the ICL performance of LLMs. While </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Min et al.</span> <span class="ltx_text ltx_font_typewriter" id="S2.p2.1.3.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib30" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.4.2.2.1">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.5"> observed limited significance of label correctness in demonstrations for classification tasks, however, </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Yoo et al.</span> <span class="ltx_text ltx_font_typewriter" id="S2.p2.1.6.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib52" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.7.2.2.1">)</span>; <span class="ltx_text ltx_font_typewriter">Kossen et al.</span> <span class="ltx_text ltx_font_typewriter" id="S2.p2.1.6.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib22" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.7.2.2.1">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.8"> argued that ICL is sensitive to input-label mapping and can affect downstream performance. Furthermore, </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Wei et al.</span> <span class="ltx_text ltx_font_typewriter" id="S2.p2.1.9.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib49" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.10.2.2.1">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.11"> showed that ICL can override semantic priors, being influenced even by semantically unrelated input-label mappings, particularly in larger models. While prior works </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.12.1">(</span><span class="ltx_text ltx_font_typewriter">Min et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.13.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib30" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a>; <span class="ltx_text ltx_font_typewriter">Wei et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.13.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib49" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a>; <span class="ltx_text ltx_font_typewriter">Kossen et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.13.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib22" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.14.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.15"> have explored various aspects of ICL for NLU tasks, it is crucial to examine its applicability to NLG tasks. </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Raunak et al.</span> <span class="ltx_text ltx_font_typewriter" id="S2.p2.1.16.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib39" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.17.2.2.1">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.18">, which is most related to our work, found that syntactic perturbations to demonstrations affect MT performance, with target text distribution having more impact than source text distribution. However, their experiments were limited to GPT-3 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.19.1">(</span><span class="ltx_text ltx_font_typewriter">Brown et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.20.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib5" title=""><span class="ltx_text ltx_font_typewriter">2020</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.21.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p2.1.22"> with only 100 test examples per direction. Our study substantially expands on this by extensively experimenting across the multiple axes mentioned earlier.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S2.p3.1.1">Perturbation and Robustness:</span><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.2">
Text perturbation attacks </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.3.1">(</span><span class="ltx_text ltx_font_typewriter">Xu et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.4.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib51" title=""><span class="ltx_text ltx_font_typewriter">2021</span></a>; <span class="ltx_text ltx_font_typewriter">Moradi and Samwald</span><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.4.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib31" title=""><span class="ltx_text ltx_font_typewriter">2021</span></a>; <span class="ltx_text ltx_font_typewriter">Zhang et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.4.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib57" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a>; <span class="ltx_text ltx_font_typewriter">Niu et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.4.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib33" title=""><span class="ltx_text ltx_font_typewriter">2020</span></a>; <span class="ltx_text ltx_font_typewriter">Salinas and Morstatter</span><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.4.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib43" title=""><span class="ltx_text ltx_font_typewriter">2024</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.5.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.6"> are frequently used to assess the resilience of models to input alterations, distinguishing them from adversarial attacks </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.7.1">(</span><span class="ltx_text ltx_font_typewriter">Michel et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.8.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib29" title=""><span class="ltx_text ltx_font_typewriter">2019</span></a>; <span class="ltx_text ltx_font_typewriter">Garg and Ramakrishnan</span><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.8.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib18" title=""><span class="ltx_text ltx_font_typewriter">2020</span></a>; <span class="ltx_text ltx_font_typewriter">Zhang et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.8.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib56" title=""><span class="ltx_text ltx_font_typewriter">2021</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.9.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.10"> with their intent to simulate real-world noise rather than to explicitly deceive models. These attacks, applied at either word level </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.11.1">(</span><span class="ltx_text ltx_font_typewriter">Zhao et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.12.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib58" title=""><span class="ltx_text ltx_font_typewriter">2018</span></a>; <span class="ltx_text ltx_font_typewriter">Cheng et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.12.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib8" title=""><span class="ltx_text ltx_font_typewriter">2020</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.13.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.14"> or character level </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.15.1">(</span><span class="ltx_text ltx_font_typewriter">Belinkov and Bisk</span><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.16.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib3" title=""><span class="ltx_text ltx_font_typewriter">2018</span></a>; <span class="ltx_text ltx_font_typewriter">Ebrahimi et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.16.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib13" title=""><span class="ltx_text ltx_font_typewriter">2018</span></a>; <span class="ltx_text ltx_font_typewriter">Formento et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.16.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib16" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.17.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.18">, assess NMT model robustness through strategies like introducing random or linguistically aware perturbations, such as frequently confused characters. Additionally, perturbations may also involve random removal or addition of punctuations </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.19.1">(</span><span class="ltx_text ltx_font_typewriter">Formento et al.</span><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.20.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib16" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.21.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.22">. Closest to our work is the work by </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Moradi and Samwald</span> <span class="ltx_text ltx_font_typewriter" id="S2.p3.1.23.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib31" title=""><span class="ltx_text ltx_font_typewriter">2021</span></a><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.24.2.2.1">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S2.p3.1.25">, however, our approach differs from theirs as we only perturb demonstrations while leaving test examples as is.</span></p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_font_typewriter ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text ltx_font_serif" id="S3.1.1.1">3</span> </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1"><span class="ltx_text ltx_font_typewriter" id="S3.p1.1.1">Our experimentation first focuses on understanding whether in-context learning for MT is instruction-driven or example-driven, and then expands on the latter, exploring various aspects of examples. We categorize our experimentation along two principal axes: </span><span class="ltx_text ltx_font_typewriter ltx_font_italic" id="S3.p1.1.2">instruction perturbations</span><span class="ltx_text ltx_font_typewriter" id="S3.p1.1.3"> and </span><span class="ltx_text ltx_font_typewriter ltx_font_italic" id="S3.p1.1.4">demonstration perturbations</span><span class="ltx_text ltx_font_typewriter" id="S3.p1.1.5">.</span></p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_font_typewriter ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text ltx_font_serif" id="S3.SS1.1.1.1">3.1</span> </span>Instruction Variations</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.1">LLMs require extensive task descriptions for controlled generations and aligned outputs </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.2.1">(</span><span class="ltx_text ltx_font_typewriter">Sondos Mahmoud Bsharat</span><span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib45" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.4.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.5">. We intend to understand if instructions are really necessary or not and whether in-context exemplars can guide the model to perform the MT task in cases of suboptimal or confounding instructions. Keeping the demonstrations fixed, we experiment with the task instructions described below:</span></p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.I1.i1.p1.1.1">Standard instruction:</span><span class="ltx_text ltx_font_typewriter" id="S3.I1.i1.p1.1.2">
The baseline condition, a default instruction, indicates the MT task is used.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.I1.i2.p1.1.1">No instruction:</span><span class="ltx_text ltx_font_typewriter" id="S3.I1.i2.p1.1.2">
Only in-context exemplars are used without any explicit instructions to see if the models can infer the task implicitly.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.I1.i3.p1.1.1">Generic instruction:</span><span class="ltx_text ltx_font_typewriter" id="S3.I1.i3.p1.1.2">
A non-task-specific generic instruction is used. This is similar to the previous setting, however, we add a generic instruction to infer the task from the exemplars.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.I1.i4.p1.1.1">Constrastive instruction:</span><span class="ltx_text ltx_font_typewriter" id="S3.I1.i4.p1.1.2">
An instruction with the opposite translation direction than the in-context exemplars is used to assess the model’s susceptibility to off-target translations due to instruction changes.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.I1.i5.p1.1.1">Random instruction:</span><span class="ltx_text ltx_font_typewriter" id="S3.I1.i5.p1.1.2">
This includes a random instruction from a pool of non-translation tasks, while the in-context exemplars still reflect the translation task, to see if the model is misled into performing a different task.</span></p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_font_typewriter ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text ltx_font_serif" id="S3.SS2.1.1.1">3.2</span> </span>Demonstration Perturbations</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1"><span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.1.1">In this set of experiments, we intend to investigate the impact of perturbing in-context demonstrations on the downstream MT performance, keeping instructions fixed. Specifically, our objective is to ascertain whether the model can effectively follow clear instructions and perform the tasks or whether the inclusion of suboptimal in-context exemplars adversely influences the subsequent downstream MT performance. To achieve this, we homogeneously either perturb the source or target distribution of in-context demonstrations by introducing different types of errors. Our perturbation methods are limited by a noise budget and influence the lexical (alterations to individual words), syntactic (alterations to structure or ordering of words), and semantic (alterations to meaning) properties of the in-context demonstrations and are listed below:</span></p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.I2.i1.p1.1.1">Span Noise:</span><span class="ltx_text ltx_font_typewriter" id="S3.I2.i1.p1.1.2">
Random contiguous segments of the original text are modified by string operations such as deletion or replacement of characters within the selected spans similar to </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Maurya et al.</span> <span class="ltx_text ltx_font_typewriter" id="S3.I2.i1.p1.1.3.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib28" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S3.I2.i1.p1.1.4.2.2.1">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S3.I2.i1.p1.1.5"> and </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Joshi et al.</span> <span class="ltx_text ltx_font_typewriter" id="S3.I2.i1.p1.1.6.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib20" title=""><span class="ltx_text ltx_font_typewriter">2020</span></a><span class="ltx_text ltx_font_typewriter" id="S3.I2.i1.p1.1.7.2.2.1">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S3.I2.i1.p1.1.8">.
We consider the number of characters to be perturbed by uniformly selecting 1-3 gram spans and uniformly choosing to delete or replace with a single random character until the chosen budget is exhausted.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.I2.i2.p1.1.1">OCR:</span><span class="ltx_text ltx_font_typewriter" id="S3.I2.i2.p1.1.2">
Words from the original text are uniformly selected, and operations, such as fusion with adjacent words or splitting into two words to simulate noise, are performed, which might commonly be introduced in the pipelines involving OCR systems.
The noise percentage determines the degree of perturbation, affecting how many words are manipulated with a uniform probability of fusing consecutive words together or splitting a word into two parts.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.I2.i3.p1.1.1">Word Ordering:</span><span class="ltx_text ltx_font_typewriter" id="S3.I2.i3.p1.1.2">
The original word order of some portions of the original text is disrupted.
It is important to note that this perturbation can have different implications across different languages due to differing levels of word order flexibility.
We uniformly sample a set of words from the original text based on noise budget for reordering by generating a new order for these words.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.I2.i4.p1.1.1">Word Duplication:</span><span class="ltx_text ltx_font_typewriter" id="S3.I2.i4.p1.1.2">
Redundancy is added to the original text without altering its semantics by duplicating certain words.
We uniformly choose a set of words for duplication based on noise budget.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i5" style="list-style-type:none;">
<div class="ltx_para" id="S3.I2.i5.p1">
<p class="ltx_p" id="S3.I2.i5.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.I2.i5.p1.1.1">Punctuation:</span><span class="ltx_text ltx_font_typewriter" id="S3.I2.i5.p1.1.2">
The syntactic cues and rhythm of the original text are altered by either inserting or deleting existing punctuations present in the original text.
We uniformly add punctuation to words lacking it in addition and uniformly delete from the original text in case of deletion, given a noise budget.</span></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A5.T3" title="In Appendix E Examples of Perturbation Variants ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.1"> in </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A5" title="Appendix E Examples of Perturbation Variants ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">E</span></a><span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.2"> provides a categorization of various perturbation methods considered as a part of this study with regard to the properties it impacts.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_font_typewriter ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text ltx_font_serif" id="S3.SS3.1.1.1">3.3</span> </span>Role of Demonstration Attributes</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1"><span class="ltx_text ltx_font_typewriter" id="S3.SS3.p1.1.1">While perturbations influence exemplar quality, there are other important attributes such as source distribution, target distribution, spatial proximity of the demonstration, and alignment between in-context demonstrations and test examples. We focus on these to further delve deeper into which attributes of an in-context demonstration play a critical role in downstream performance.</span></p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<ul class="ltx_itemize" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<div class="ltx_para" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.4"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.I3.i1.p1.4.1">Heterogeneous Perturbations:</span><span class="ltx_text ltx_font_typewriter" id="S3.I3.i1.p1.4.2"> We perform heterogeneous perturbations to investigate if spatial proximity to the test example affects the downstream performance. Specifically, we consider cases involving a mix of clean and noisy demonstrations: </span><math alttext="\{(k-1\ \mathrm{c},1\ \mathrm{n}),(1\ \mathrm{c},k-1\ \mathrm{n}),(1\ \mathrm{%
n},k-1\ \mathrm{c}),(k-1\ \mathrm{n},1\ \mathrm{c})\}" class="ltx_Math" display="inline" id="S3.I3.i1.p1.1.m1.4"><semantics id="S3.I3.i1.p1.1.m1.4a"><mrow id="S3.I3.i1.p1.1.m1.4.4.4" xref="S3.I3.i1.p1.1.m1.4.4.5.cmml"><mo id="S3.I3.i1.p1.1.m1.4.4.4.5" stretchy="false" xref="S3.I3.i1.p1.1.m1.4.4.5.cmml">{</mo><mrow id="S3.I3.i1.p1.1.m1.1.1.1.1.2" xref="S3.I3.i1.p1.1.m1.1.1.1.1.3.cmml"><mo id="S3.I3.i1.p1.1.m1.1.1.1.1.2.3" stretchy="false" xref="S3.I3.i1.p1.1.m1.1.1.1.1.3.cmml">(</mo><mrow id="S3.I3.i1.p1.1.m1.1.1.1.1.1.1" xref="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.2" xref="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.2.cmml">k</mi><mo id="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.1" xref="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.3" xref="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.3.cmml"><mn id="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.3.2" xref="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.3.2.cmml">1</mn><mo id="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.3.1" lspace="0.500em" xref="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.3.3" mathvariant="normal" xref="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.3.3.cmml">c</mi></mrow></mrow><mo id="S3.I3.i1.p1.1.m1.1.1.1.1.2.4" xref="S3.I3.i1.p1.1.m1.1.1.1.1.3.cmml">,</mo><mrow id="S3.I3.i1.p1.1.m1.1.1.1.1.2.2" xref="S3.I3.i1.p1.1.m1.1.1.1.1.2.2.cmml"><mn id="S3.I3.i1.p1.1.m1.1.1.1.1.2.2.2" xref="S3.I3.i1.p1.1.m1.1.1.1.1.2.2.2.cmml">1</mn><mo id="S3.I3.i1.p1.1.m1.1.1.1.1.2.2.1" lspace="0.500em" xref="S3.I3.i1.p1.1.m1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S3.I3.i1.p1.1.m1.1.1.1.1.2.2.3" mathvariant="normal" xref="S3.I3.i1.p1.1.m1.1.1.1.1.2.2.3.cmml">n</mi></mrow><mo id="S3.I3.i1.p1.1.m1.1.1.1.1.2.5" stretchy="false" xref="S3.I3.i1.p1.1.m1.1.1.1.1.3.cmml">)</mo></mrow><mo id="S3.I3.i1.p1.1.m1.4.4.4.6" xref="S3.I3.i1.p1.1.m1.4.4.5.cmml">,</mo><mrow id="S3.I3.i1.p1.1.m1.2.2.2.2.2" xref="S3.I3.i1.p1.1.m1.2.2.2.2.3.cmml"><mo id="S3.I3.i1.p1.1.m1.2.2.2.2.2.3" stretchy="false" xref="S3.I3.i1.p1.1.m1.2.2.2.2.3.cmml">(</mo><mrow id="S3.I3.i1.p1.1.m1.2.2.2.2.1.1" xref="S3.I3.i1.p1.1.m1.2.2.2.2.1.1.cmml"><mn id="S3.I3.i1.p1.1.m1.2.2.2.2.1.1.2" xref="S3.I3.i1.p1.1.m1.2.2.2.2.1.1.2.cmml">1</mn><mo id="S3.I3.i1.p1.1.m1.2.2.2.2.1.1.1" lspace="0.500em" xref="S3.I3.i1.p1.1.m1.2.2.2.2.1.1.1.cmml">⁢</mo><mi id="S3.I3.i1.p1.1.m1.2.2.2.2.1.1.3" mathvariant="normal" xref="S3.I3.i1.p1.1.m1.2.2.2.2.1.1.3.cmml">c</mi></mrow><mo id="S3.I3.i1.p1.1.m1.2.2.2.2.2.4" xref="S3.I3.i1.p1.1.m1.2.2.2.2.3.cmml">,</mo><mrow id="S3.I3.i1.p1.1.m1.2.2.2.2.2.2" xref="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.cmml"><mi id="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.2" xref="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.2.cmml">k</mi><mo id="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.1" xref="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.1.cmml">−</mo><mrow id="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.3" xref="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.3.cmml"><mn id="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.3.2" xref="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.3.2.cmml">1</mn><mo id="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.3.1" lspace="0.500em" xref="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.3.3" mathvariant="normal" xref="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.3.3.cmml">n</mi></mrow></mrow><mo id="S3.I3.i1.p1.1.m1.2.2.2.2.2.5" stretchy="false" xref="S3.I3.i1.p1.1.m1.2.2.2.2.3.cmml">)</mo></mrow><mo id="S3.I3.i1.p1.1.m1.4.4.4.7" xref="S3.I3.i1.p1.1.m1.4.4.5.cmml">,</mo><mrow id="S3.I3.i1.p1.1.m1.3.3.3.3.2" xref="S3.I3.i1.p1.1.m1.3.3.3.3.3.cmml"><mo id="S3.I3.i1.p1.1.m1.3.3.3.3.2.3" stretchy="false" xref="S3.I3.i1.p1.1.m1.3.3.3.3.3.cmml">(</mo><mrow id="S3.I3.i1.p1.1.m1.3.3.3.3.1.1" xref="S3.I3.i1.p1.1.m1.3.3.3.3.1.1.cmml"><mn id="S3.I3.i1.p1.1.m1.3.3.3.3.1.1.2" xref="S3.I3.i1.p1.1.m1.3.3.3.3.1.1.2.cmml">1</mn><mo id="S3.I3.i1.p1.1.m1.3.3.3.3.1.1.1" lspace="0.500em" xref="S3.I3.i1.p1.1.m1.3.3.3.3.1.1.1.cmml">⁢</mo><mi id="S3.I3.i1.p1.1.m1.3.3.3.3.1.1.3" mathvariant="normal" xref="S3.I3.i1.p1.1.m1.3.3.3.3.1.1.3.cmml">n</mi></mrow><mo id="S3.I3.i1.p1.1.m1.3.3.3.3.2.4" xref="S3.I3.i1.p1.1.m1.3.3.3.3.3.cmml">,</mo><mrow id="S3.I3.i1.p1.1.m1.3.3.3.3.2.2" xref="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.cmml"><mi id="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.2" xref="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.2.cmml">k</mi><mo id="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.1" xref="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.1.cmml">−</mo><mrow id="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.3" xref="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.3.cmml"><mn id="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.3.2" xref="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.3.2.cmml">1</mn><mo id="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.3.1" lspace="0.500em" xref="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.3.1.cmml">⁢</mo><mi id="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.3.3" mathvariant="normal" xref="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.3.3.cmml">c</mi></mrow></mrow><mo id="S3.I3.i1.p1.1.m1.3.3.3.3.2.5" stretchy="false" xref="S3.I3.i1.p1.1.m1.3.3.3.3.3.cmml">)</mo></mrow><mo id="S3.I3.i1.p1.1.m1.4.4.4.8" xref="S3.I3.i1.p1.1.m1.4.4.5.cmml">,</mo><mrow id="S3.I3.i1.p1.1.m1.4.4.4.4.2" xref="S3.I3.i1.p1.1.m1.4.4.4.4.3.cmml"><mo id="S3.I3.i1.p1.1.m1.4.4.4.4.2.3" stretchy="false" xref="S3.I3.i1.p1.1.m1.4.4.4.4.3.cmml">(</mo><mrow id="S3.I3.i1.p1.1.m1.4.4.4.4.1.1" xref="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.cmml"><mi id="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.2" xref="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.2.cmml">k</mi><mo id="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.1" xref="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.1.cmml">−</mo><mrow id="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.3" xref="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.3.cmml"><mn id="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.3.2" xref="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.3.2.cmml">1</mn><mo id="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.3.1" lspace="0.500em" xref="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.3.1.cmml">⁢</mo><mi id="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.3.3" mathvariant="normal" xref="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.3.3.cmml">n</mi></mrow></mrow><mo id="S3.I3.i1.p1.1.m1.4.4.4.4.2.4" xref="S3.I3.i1.p1.1.m1.4.4.4.4.3.cmml">,</mo><mrow id="S3.I3.i1.p1.1.m1.4.4.4.4.2.2" xref="S3.I3.i1.p1.1.m1.4.4.4.4.2.2.cmml"><mn id="S3.I3.i1.p1.1.m1.4.4.4.4.2.2.2" xref="S3.I3.i1.p1.1.m1.4.4.4.4.2.2.2.cmml">1</mn><mo id="S3.I3.i1.p1.1.m1.4.4.4.4.2.2.1" lspace="0.500em" xref="S3.I3.i1.p1.1.m1.4.4.4.4.2.2.1.cmml">⁢</mo><mi id="S3.I3.i1.p1.1.m1.4.4.4.4.2.2.3" mathvariant="normal" xref="S3.I3.i1.p1.1.m1.4.4.4.4.2.2.3.cmml">c</mi></mrow><mo id="S3.I3.i1.p1.1.m1.4.4.4.4.2.5" stretchy="false" xref="S3.I3.i1.p1.1.m1.4.4.4.4.3.cmml">)</mo></mrow><mo id="S3.I3.i1.p1.1.m1.4.4.4.9" stretchy="false" xref="S3.I3.i1.p1.1.m1.4.4.5.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I3.i1.p1.1.m1.4b"><set id="S3.I3.i1.p1.1.m1.4.4.5.cmml" xref="S3.I3.i1.p1.1.m1.4.4.4"><interval closure="open" id="S3.I3.i1.p1.1.m1.1.1.1.1.3.cmml" xref="S3.I3.i1.p1.1.m1.1.1.1.1.2"><apply id="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.I3.i1.p1.1.m1.1.1.1.1.1.1"><minus id="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.1"></minus><ci id="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.2">𝑘</ci><apply id="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.3"><times id="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.3.1"></times><cn id="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.3.2.cmml" type="integer" xref="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.3.2">1</cn><ci id="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.I3.i1.p1.1.m1.1.1.1.1.1.1.3.3">c</ci></apply></apply><apply id="S3.I3.i1.p1.1.m1.1.1.1.1.2.2.cmml" xref="S3.I3.i1.p1.1.m1.1.1.1.1.2.2"><times id="S3.I3.i1.p1.1.m1.1.1.1.1.2.2.1.cmml" xref="S3.I3.i1.p1.1.m1.1.1.1.1.2.2.1"></times><cn id="S3.I3.i1.p1.1.m1.1.1.1.1.2.2.2.cmml" type="integer" xref="S3.I3.i1.p1.1.m1.1.1.1.1.2.2.2">1</cn><ci id="S3.I3.i1.p1.1.m1.1.1.1.1.2.2.3.cmml" xref="S3.I3.i1.p1.1.m1.1.1.1.1.2.2.3">n</ci></apply></interval><interval closure="open" id="S3.I3.i1.p1.1.m1.2.2.2.2.3.cmml" xref="S3.I3.i1.p1.1.m1.2.2.2.2.2"><apply id="S3.I3.i1.p1.1.m1.2.2.2.2.1.1.cmml" xref="S3.I3.i1.p1.1.m1.2.2.2.2.1.1"><times id="S3.I3.i1.p1.1.m1.2.2.2.2.1.1.1.cmml" xref="S3.I3.i1.p1.1.m1.2.2.2.2.1.1.1"></times><cn id="S3.I3.i1.p1.1.m1.2.2.2.2.1.1.2.cmml" type="integer" xref="S3.I3.i1.p1.1.m1.2.2.2.2.1.1.2">1</cn><ci id="S3.I3.i1.p1.1.m1.2.2.2.2.1.1.3.cmml" xref="S3.I3.i1.p1.1.m1.2.2.2.2.1.1.3">c</ci></apply><apply id="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.cmml" xref="S3.I3.i1.p1.1.m1.2.2.2.2.2.2"><minus id="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.1.cmml" xref="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.1"></minus><ci id="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.2">𝑘</ci><apply id="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.3.cmml" xref="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.3"><times id="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.3.1.cmml" xref="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.3.1"></times><cn id="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.3.2.cmml" type="integer" xref="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.3.2">1</cn><ci id="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.3.3.cmml" xref="S3.I3.i1.p1.1.m1.2.2.2.2.2.2.3.3">n</ci></apply></apply></interval><interval closure="open" id="S3.I3.i1.p1.1.m1.3.3.3.3.3.cmml" xref="S3.I3.i1.p1.1.m1.3.3.3.3.2"><apply id="S3.I3.i1.p1.1.m1.3.3.3.3.1.1.cmml" xref="S3.I3.i1.p1.1.m1.3.3.3.3.1.1"><times id="S3.I3.i1.p1.1.m1.3.3.3.3.1.1.1.cmml" xref="S3.I3.i1.p1.1.m1.3.3.3.3.1.1.1"></times><cn id="S3.I3.i1.p1.1.m1.3.3.3.3.1.1.2.cmml" type="integer" xref="S3.I3.i1.p1.1.m1.3.3.3.3.1.1.2">1</cn><ci id="S3.I3.i1.p1.1.m1.3.3.3.3.1.1.3.cmml" xref="S3.I3.i1.p1.1.m1.3.3.3.3.1.1.3">n</ci></apply><apply id="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.cmml" xref="S3.I3.i1.p1.1.m1.3.3.3.3.2.2"><minus id="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.1.cmml" xref="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.1"></minus><ci id="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.2.cmml" xref="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.2">𝑘</ci><apply id="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.3.cmml" xref="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.3"><times id="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.3.1.cmml" xref="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.3.1"></times><cn id="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.3.2.cmml" type="integer" xref="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.3.2">1</cn><ci id="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.3.3.cmml" xref="S3.I3.i1.p1.1.m1.3.3.3.3.2.2.3.3">c</ci></apply></apply></interval><interval closure="open" id="S3.I3.i1.p1.1.m1.4.4.4.4.3.cmml" xref="S3.I3.i1.p1.1.m1.4.4.4.4.2"><apply id="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.cmml" xref="S3.I3.i1.p1.1.m1.4.4.4.4.1.1"><minus id="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.1.cmml" xref="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.1"></minus><ci id="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.2.cmml" xref="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.2">𝑘</ci><apply id="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.3.cmml" xref="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.3"><times id="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.3.1.cmml" xref="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.3.1"></times><cn id="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.3.2.cmml" type="integer" xref="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.3.2">1</cn><ci id="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.3.3.cmml" xref="S3.I3.i1.p1.1.m1.4.4.4.4.1.1.3.3">n</ci></apply></apply><apply id="S3.I3.i1.p1.1.m1.4.4.4.4.2.2.cmml" xref="S3.I3.i1.p1.1.m1.4.4.4.4.2.2"><times id="S3.I3.i1.p1.1.m1.4.4.4.4.2.2.1.cmml" xref="S3.I3.i1.p1.1.m1.4.4.4.4.2.2.1"></times><cn id="S3.I3.i1.p1.1.m1.4.4.4.4.2.2.2.cmml" type="integer" xref="S3.I3.i1.p1.1.m1.4.4.4.4.2.2.2">1</cn><ci id="S3.I3.i1.p1.1.m1.4.4.4.4.2.2.3.cmml" xref="S3.I3.i1.p1.1.m1.4.4.4.4.2.2.3">c</ci></apply></interval></set></annotation-xml><annotation encoding="application/x-tex" id="S3.I3.i1.p1.1.m1.4c">\{(k-1\ \mathrm{c},1\ \mathrm{n}),(1\ \mathrm{c},k-1\ \mathrm{n}),(1\ \mathrm{%
n},k-1\ \mathrm{c}),(k-1\ \mathrm{n},1\ \mathrm{c})\}</annotation><annotation encoding="application/x-llamapun" id="S3.I3.i1.p1.1.m1.4d">{ ( italic_k - 1 roman_c , 1 roman_n ) , ( 1 roman_c , italic_k - 1 roman_n ) , ( 1 roman_n , italic_k - 1 roman_c ) , ( italic_k - 1 roman_n , 1 roman_c ) }</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="S3.I3.i1.p1.4.3"> where </span><math alttext="c" class="ltx_Math" display="inline" id="S3.I3.i1.p1.2.m2.1"><semantics id="S3.I3.i1.p1.2.m2.1a"><mi id="S3.I3.i1.p1.2.m2.1.1" xref="S3.I3.i1.p1.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.I3.i1.p1.2.m2.1b"><ci id="S3.I3.i1.p1.2.m2.1.1.cmml" xref="S3.I3.i1.p1.2.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I3.i1.p1.2.m2.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.I3.i1.p1.2.m2.1d">italic_c</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="S3.I3.i1.p1.4.4"> and </span><math alttext="n" class="ltx_Math" display="inline" id="S3.I3.i1.p1.3.m3.1"><semantics id="S3.I3.i1.p1.3.m3.1a"><mi id="S3.I3.i1.p1.3.m3.1.1" xref="S3.I3.i1.p1.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.I3.i1.p1.3.m3.1b"><ci id="S3.I3.i1.p1.3.m3.1.1.cmml" xref="S3.I3.i1.p1.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I3.i1.p1.3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.I3.i1.p1.3.m3.1d">italic_n</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="S3.I3.i1.p1.4.5"> indicate clean and noisy and </span><math alttext="k" class="ltx_Math" display="inline" id="S3.I3.i1.p1.4.m4.1"><semantics id="S3.I3.i1.p1.4.m4.1a"><mi id="S3.I3.i1.p1.4.m4.1.1" xref="S3.I3.i1.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.I3.i1.p1.4.m4.1b"><ci id="S3.I3.i1.p1.4.m4.1.1.cmml" xref="S3.I3.i1.p1.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I3.i1.p1.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.I3.i1.p1.4.m4.1d">italic_k</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="S3.I3.i1.p1.4.6"> denotes number of shots.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<div class="ltx_para" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.I3.i2.p1.1.1">Directionality:</span><span class="ltx_text ltx_font_typewriter" id="S3.I3.i2.p1.1.2">
While the influence of test set directionality and translationese effects on traditional MT is well-explored </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S3.I3.i2.p1.1.3.1">(</span><span class="ltx_text ltx_font_typewriter">Zhang and Toral</span><span class="ltx_text ltx_font_typewriter" id="S3.I3.i2.p1.1.4.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib54" title=""><span class="ltx_text ltx_font_typewriter">2019</span></a>; <span class="ltx_text ltx_font_typewriter">Federmann et al.</span><span class="ltx_text ltx_font_typewriter" id="S3.I3.i2.p1.1.4.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib15" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a><span class="ltx_text ltx_font_typewriter" id="S3.I3.i2.p1.1.5.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S3.I3.i2.p1.1.6">, its impact on the performance of LLMs using in-context examples remains largely unexplored </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S3.I3.i2.p1.1.7.1">(</span><span class="ltx_text ltx_font_typewriter">Raunak et al.</span><span class="ltx_text ltx_font_typewriter" id="S3.I3.i2.p1.1.8.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib39" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S3.I3.i2.p1.1.9.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S3.I3.i2.p1.1.10">. We explore this by choosing in-context exemplars from source-original and target-original sets to analyze whether leveraging target-original in-context examples is a superior choice than source-original in-context examples.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i3" style="list-style-type:none;">
<div class="ltx_para" id="S3.I3.i3.p1">
<p class="ltx_p" id="S3.I3.i3.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.I3.i3.p1.1.1">Demonstrations from Allied Tasks as Proxy:</span><span class="ltx_text ltx_font_typewriter" id="S3.I3.i3.p1.1.2">
When demonstrations for the specific task are not available during inference, a key question arises: </span><span class="ltx_text ltx_font_typewriter ltx_font_italic" id="S3.I3.i3.p1.1.3">Can demonstrations from a related task serve as a proxy for the model?</span><span class="ltx_text ltx_font_typewriter" id="S3.I3.i3.p1.1.4"> This is particularly relevant in MT, where obtaining demonstrations for every low-resource language pair might not be feasible. In this scenario, if the translation direction at test time is from language X to Y, we provide exemplars from language A to Y to see if these are sufficient to guide the model in generating language Y effectively. Furthermore, we also experiment with various auxiliary languages as the source for these in-context exemplars to determine if the choice of auxiliary language affects downstream performance. Specifically, we consider whether the auxiliary language a) matches the test time source language (baseline), b) matches the script of the test time source language and is not the test time source language, c) is English, d) is a randomly selected non-English language that differs in script from the test time source language.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i4" style="list-style-type:none;">
<div class="ltx_para" id="S3.I3.i4.p1">
<p class="ltx_p" id="S3.I3.i4.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.I3.i4.p1.1.1">Misalignment:</span><span class="ltx_text ltx_font_typewriter" id="S3.I3.i4.p1.1.2">
Effective in-context learners should extract pertinent information from the context and provide precise responses. However, this trait can be exploited by intentionally injecting misinformation into the examples. Therefore, we aim to understand if models are susceptible to such attacks. This differs from prior perturbation experiments (see </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5.SS2.SSS1" title="5.2.1 Homogeneous Perturbations ‣ 5.2 Example Perturbations ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.2.1</span></a><span class="ltx_text ltx_font_typewriter" id="S3.I3.i4.p1.1.3">), where attacks targeted either source or target distribution without introducing misinformation through alignment alterations. Our experiment emulates a pivot translation scenario. We present two in-context examples: the first in language X with its English translation and the second demonstrating the translation of the English sentence into language Y. During testing, the model is tasked with translating from language X to Y, resembling a multi-hop reasoning process. Here, we consider 4 alignment types: a) all aligned, b) pivot misaligned, c) pivot and target misaligned, d) target misaligned. Types a) and b) are unperturbed cases, while c) and d) present perturbations or misinformation.</span></p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_font_typewriter ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text ltx_font_serif" id="S4.1.1.1">4</span> </span>Experimental setup</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1"><span class="ltx_text ltx_font_typewriter" id="S4.p1.1.1">This section describes the different pre-trained models, evaluation benchmarks, in-context prompts, and decoding hyperparameters used for our current study.</span></p>
</div>
<div class="ltx_para" id="S4.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I1.i1.p1.1.1">Models:</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.2"> We conduct our experimentation on off-the-shelf open-source pre-trained LLMs like Llama 2 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.3.1">(</span><span class="ltx_text ltx_font_typewriter">Touvron et al.</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.4.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib46" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.5.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.6">, BLOOM </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.7.1">(</span><span class="ltx_text ltx_font_typewriter">Fan et al.</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.8.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib14" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.9.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.10"> and their corresponding instruction-tuned variants, ALMA </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.11.1">(</span><span class="ltx_text ltx_font_typewriter">Xu et al.</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.12.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib50" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.13.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.14"> and BLOOMZ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Muennighoff et al.</span> <span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.15.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib32" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.16.2.2.1">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.17"> respectively. Firstly, models like Llama 2 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Touvron et al.</span> <span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.18.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib46" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.19.2.2.1">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.20"> are pre-dominantly trained with English or European languages and the tokenizer has poor fertility on Indic languages. Further, </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Ahuja et al.</span> <span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.21.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib1" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.22.2.2.1">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.23"> show that Llama 2 70B </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.24.1">(</span><span class="ltx_text ltx_font_typewriter">Touvron et al.</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.25.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib46" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.26.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.27"> exhibits poor in-context MT performance on Indian languages. Hence, we evaluate Llama 2 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.28.1">(</span><span class="ltx_text ltx_font_typewriter">Touvron et al.</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.29.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib46" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.30.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.31"> and ALMA </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.32.1">(</span><span class="ltx_text ltx_font_typewriter">Xu et al.</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.33.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib50" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.34.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.35"> only on European languages and evaluate BLOOM </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.36.1">(</span><span class="ltx_text ltx_font_typewriter">Fan et al.</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.37.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib14" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.38.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.39"> and BLOOMZ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_typewriter">Muennighoff et al.</span> <span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.40.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib32" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.41.2.2.1">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.42"> on the remaining three Indian languages. For consistency in our experiments, we add a task-specific fine-tuned baseline (BLOOM 7B FT) by fine-tuning the BLOOM 7B model on a subset of BPCC-seed data </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.43.1">(</span><span class="ltx_text ltx_font_typewriter">Gala et al.</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.44.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib17" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.45.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.46"> using the same prompt structure as ALMA </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.47.1">(</span><span class="ltx_text ltx_font_typewriter">Xu et al.</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.48.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib50" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.49.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.50">. The fine-tuning hyperparameters are outlined in </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A1" title="Appendix A Fine-tuning BLOOM 7B on MT ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">A</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.51">. Due to computational constraints, we restrict our experimentation to 7B models for a fair comparison across the base and instruction-tuned variants. We adopt a greedy decoding strategy without sampling to generate a maximum of 256 tokens with early stopping to ensure reproducibility in our results.</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I1.i2.p1.1.1">Languages:</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i2.p1.1.2">
Broadly, our experimentation consists of 3 Indian languages like Bengali, Hindi, and Tamil, and 3 European languages like Czech, German, and Russian. However, we consider different sets of languages for certain experiments due to the nature of the experiment or the availability of the data. </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A6.T4" title="In Appendix F Languages and Directions Considered ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i2.p1.1.3"> in </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A6" title="Appendix F Languages and Directions Considered ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">F</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i2.p1.1.4"> provides the languages considered, and the test set for each experiment.</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I1.i3.p1.1.1">Benchmarks:</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i3.p1.1.2">
FLORES-200 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S4.I1.i3.p1.1.3.1">(</span><span class="ltx_text ltx_font_typewriter">Goyal et al.</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i3.p1.1.4.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib19" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a>; <span class="ltx_text ltx_font_typewriter">Costa-jussà et al.</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i3.p1.1.4.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib10" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i3.p1.1.5.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i3.p1.1.6"> is a N-way parallel benchmark that covers 200 languages, including various low-resource languages. However, it is important to note that data from FLORES-200 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S4.I1.i3.p1.1.7.1">(</span><span class="ltx_text ltx_font_typewriter">Goyal et al.</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i3.p1.1.8.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib19" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a>; <span class="ltx_text ltx_font_typewriter">Costa-jussà et al.</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i3.p1.1.8.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib10" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i3.p1.1.9.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i3.p1.1.10"> has been consumed in the multitask fine-tuning xP3 mixture of BLOOMZ </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S4.I1.i3.p1.1.11.1">(</span><span class="ltx_text ltx_font_typewriter">Muennighoff et al.</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i3.p1.1.12.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib32" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i3.p1.1.13.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i3.p1.1.14">, suggesting data contamination. Therefore, we do not consider FLORES-200 for the evaluation of Indian languages and instead, we use the newly released IN22-Gen </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S4.I1.i3.p1.1.15.1">(</span><span class="ltx_text ltx_font_typewriter">Gala et al.</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i3.p1.1.16.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib17" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i3.p1.1.17.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i3.p1.1.18">. IN22-Gen is a 22-way parallel benchmark for Indian languages that focuses on demography-specific content and serves as an unbiased evaluation set for both BLOOM and BLOOMZ models.</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.2"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I1.i4.p1.2.1">Prompt Details:</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i4.p1.2.2">
In order to maintain consistency in terms of evaluation, we follow the same prompt that was used for fine-tuning ALMA </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S4.I1.i4.p1.2.3.1">(</span><span class="ltx_text ltx_font_typewriter">Xu et al.</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i4.p1.2.4.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib50" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i4.p1.2.5.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i4.p1.2.6"> across all different models considered in this work (see </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A3" title="Appendix C Prompt Templates ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">C</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i4.p1.2.7">). However, we find Llama 2 Chat models do not perform well with the above standard prompt in the monolithic format, which is similar to </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Sclar et al.</span> <span class="ltx_text ltx_font_typewriter" id="S4.I1.i4.p1.2.8.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib44" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i4.p1.2.9.2.2.1">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i4.p1.2.10">. Therefore, we resort to passing each demonstration as a user-assistant turn and task description in the system prompt in the chat format. We use uniformly sampled high-quality translation pairs from the FLORES-200 dev set to compose the in-context demonstrations with </span><math alttext="k=\{1,4,8\}" class="ltx_Math" display="inline" id="S4.I1.i4.p1.1.m1.3"><semantics id="S4.I1.i4.p1.1.m1.3a"><mrow id="S4.I1.i4.p1.1.m1.3.4" xref="S4.I1.i4.p1.1.m1.3.4.cmml"><mi id="S4.I1.i4.p1.1.m1.3.4.2" xref="S4.I1.i4.p1.1.m1.3.4.2.cmml">k</mi><mo id="S4.I1.i4.p1.1.m1.3.4.1" xref="S4.I1.i4.p1.1.m1.3.4.1.cmml">=</mo><mrow id="S4.I1.i4.p1.1.m1.3.4.3.2" xref="S4.I1.i4.p1.1.m1.3.4.3.1.cmml"><mo id="S4.I1.i4.p1.1.m1.3.4.3.2.1" stretchy="false" xref="S4.I1.i4.p1.1.m1.3.4.3.1.cmml">{</mo><mn id="S4.I1.i4.p1.1.m1.1.1" xref="S4.I1.i4.p1.1.m1.1.1.cmml">1</mn><mo id="S4.I1.i4.p1.1.m1.3.4.3.2.2" xref="S4.I1.i4.p1.1.m1.3.4.3.1.cmml">,</mo><mn id="S4.I1.i4.p1.1.m1.2.2" xref="S4.I1.i4.p1.1.m1.2.2.cmml">4</mn><mo id="S4.I1.i4.p1.1.m1.3.4.3.2.3" xref="S4.I1.i4.p1.1.m1.3.4.3.1.cmml">,</mo><mn id="S4.I1.i4.p1.1.m1.3.3" xref="S4.I1.i4.p1.1.m1.3.3.cmml">8</mn><mo id="S4.I1.i4.p1.1.m1.3.4.3.2.4" stretchy="false" xref="S4.I1.i4.p1.1.m1.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i4.p1.1.m1.3b"><apply id="S4.I1.i4.p1.1.m1.3.4.cmml" xref="S4.I1.i4.p1.1.m1.3.4"><eq id="S4.I1.i4.p1.1.m1.3.4.1.cmml" xref="S4.I1.i4.p1.1.m1.3.4.1"></eq><ci id="S4.I1.i4.p1.1.m1.3.4.2.cmml" xref="S4.I1.i4.p1.1.m1.3.4.2">𝑘</ci><set id="S4.I1.i4.p1.1.m1.3.4.3.1.cmml" xref="S4.I1.i4.p1.1.m1.3.4.3.2"><cn id="S4.I1.i4.p1.1.m1.1.1.cmml" type="integer" xref="S4.I1.i4.p1.1.m1.1.1">1</cn><cn id="S4.I1.i4.p1.1.m1.2.2.cmml" type="integer" xref="S4.I1.i4.p1.1.m1.2.2">4</cn><cn id="S4.I1.i4.p1.1.m1.3.3.cmml" type="integer" xref="S4.I1.i4.p1.1.m1.3.3">8</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i4.p1.1.m1.3c">k=\{1,4,8\}</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i4.p1.1.m1.3d">italic_k = { 1 , 4 , 8 }</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="S4.I1.i4.p1.2.11"> across various noise scales with </span><math alttext="\delta=\{0,0.1,0.25,0.5,0.75\}" class="ltx_Math" display="inline" id="S4.I1.i4.p1.2.m2.5"><semantics id="S4.I1.i4.p1.2.m2.5a"><mrow id="S4.I1.i4.p1.2.m2.5.6" xref="S4.I1.i4.p1.2.m2.5.6.cmml"><mi id="S4.I1.i4.p1.2.m2.5.6.2" xref="S4.I1.i4.p1.2.m2.5.6.2.cmml">δ</mi><mo id="S4.I1.i4.p1.2.m2.5.6.1" xref="S4.I1.i4.p1.2.m2.5.6.1.cmml">=</mo><mrow id="S4.I1.i4.p1.2.m2.5.6.3.2" xref="S4.I1.i4.p1.2.m2.5.6.3.1.cmml"><mo id="S4.I1.i4.p1.2.m2.5.6.3.2.1" stretchy="false" xref="S4.I1.i4.p1.2.m2.5.6.3.1.cmml">{</mo><mn id="S4.I1.i4.p1.2.m2.1.1" xref="S4.I1.i4.p1.2.m2.1.1.cmml">0</mn><mo id="S4.I1.i4.p1.2.m2.5.6.3.2.2" xref="S4.I1.i4.p1.2.m2.5.6.3.1.cmml">,</mo><mn id="S4.I1.i4.p1.2.m2.2.2" xref="S4.I1.i4.p1.2.m2.2.2.cmml">0.1</mn><mo id="S4.I1.i4.p1.2.m2.5.6.3.2.3" xref="S4.I1.i4.p1.2.m2.5.6.3.1.cmml">,</mo><mn id="S4.I1.i4.p1.2.m2.3.3" xref="S4.I1.i4.p1.2.m2.3.3.cmml">0.25</mn><mo id="S4.I1.i4.p1.2.m2.5.6.3.2.4" xref="S4.I1.i4.p1.2.m2.5.6.3.1.cmml">,</mo><mn id="S4.I1.i4.p1.2.m2.4.4" xref="S4.I1.i4.p1.2.m2.4.4.cmml">0.5</mn><mo id="S4.I1.i4.p1.2.m2.5.6.3.2.5" xref="S4.I1.i4.p1.2.m2.5.6.3.1.cmml">,</mo><mn id="S4.I1.i4.p1.2.m2.5.5" xref="S4.I1.i4.p1.2.m2.5.5.cmml">0.75</mn><mo id="S4.I1.i4.p1.2.m2.5.6.3.2.6" stretchy="false" xref="S4.I1.i4.p1.2.m2.5.6.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i4.p1.2.m2.5b"><apply id="S4.I1.i4.p1.2.m2.5.6.cmml" xref="S4.I1.i4.p1.2.m2.5.6"><eq id="S4.I1.i4.p1.2.m2.5.6.1.cmml" xref="S4.I1.i4.p1.2.m2.5.6.1"></eq><ci id="S4.I1.i4.p1.2.m2.5.6.2.cmml" xref="S4.I1.i4.p1.2.m2.5.6.2">𝛿</ci><set id="S4.I1.i4.p1.2.m2.5.6.3.1.cmml" xref="S4.I1.i4.p1.2.m2.5.6.3.2"><cn id="S4.I1.i4.p1.2.m2.1.1.cmml" type="integer" xref="S4.I1.i4.p1.2.m2.1.1">0</cn><cn id="S4.I1.i4.p1.2.m2.2.2.cmml" type="float" xref="S4.I1.i4.p1.2.m2.2.2">0.1</cn><cn id="S4.I1.i4.p1.2.m2.3.3.cmml" type="float" xref="S4.I1.i4.p1.2.m2.3.3">0.25</cn><cn id="S4.I1.i4.p1.2.m2.4.4.cmml" type="float" xref="S4.I1.i4.p1.2.m2.4.4">0.5</cn><cn id="S4.I1.i4.p1.2.m2.5.5.cmml" type="float" xref="S4.I1.i4.p1.2.m2.5.5">0.75</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i4.p1.2.m2.5c">\delta=\{0,0.1,0.25,0.5,0.75\}</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i4.p1.2.m2.5d">italic_δ = { 0 , 0.1 , 0.25 , 0.5 , 0.75 }</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="S4.I1.i4.p1.2.12"> for evaluating LLMs.</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I1.i5.p1.1.1">Evaluation:</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i5.p1.1.2">
Following </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Sai B et al.</span> <span class="ltx_text ltx_font_typewriter" id="S4.I1.i5.p1.1.3.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib42" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i5.p1.1.4.2.2.1">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i5.p1.1.5">, we use the ChrF++ metric </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S4.I1.i5.p1.1.6.1">(</span><span class="ltx_text ltx_font_typewriter">Popović</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i5.p1.1.7.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib35" title=""><span class="ltx_text ltx_font_typewriter">2017</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i5.p1.1.8.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i5.p1.1.9">, computed using SacreBLEU</span><span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_text ltx_font_typewriter" id="footnote1.1">sacreBLEU ChrF++ signature: 
<br class="ltx_break"/>nrefs:1|case:mixed|eff:no|tok:flores200|
<br class="ltx_break"/>smooth:exp|version:2.4.0</span></span></span></span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i5.p1.1.10"> </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S4.I1.i5.p1.1.11.1">(</span><span class="ltx_text ltx_font_typewriter">Post</span><span class="ltx_text ltx_font_typewriter" id="S4.I1.i5.p1.1.12.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib36" title=""><span class="ltx_text ltx_font_typewriter">2018</span></a><span class="ltx_text ltx_font_typewriter" id="S4.I1.i5.p1.1.13.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S4.I1.i5.p1.1.14">, as the primary metric for our analysis due to its strongest correlation with human judgments among automatic metrics.</span></p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_font_typewriter ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text ltx_font_serif" id="S5.1.1.1">5</span> </span>Results and Analysis</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_typewriter" id="S5.p1.1.1">We now describe results that attempt to reveal the various factors that influence ICL for MT.</span></p>
</div>
<figure class="ltx_figure" id="S5.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="926" id="S5.F1.g1" src="x1.png" width="831"/>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_serif" id="S5.F1.3.1.1">Figure 1</span>: </span>Comparison of ChrF++ scores for En-XX (top) and XX-En (bottom) across different instruction types for BLOOM and Llama 2 model families (averaged across different languages and shots).</figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_font_typewriter ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text ltx_font_serif" id="S5.SS1.1.1.1">5.1</span> </span>Sensitivity to Instruction Variations</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5.F1" title="In 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text ltx_font_typewriter" id="S5.SS1.p1.1.1"> illustrates that base LLMs such as BLOOM-7B and Llama 2 7B exhibit less variability to instruction perturbations averaged across different shots. Furthermore, </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A7.F12" title="In Appendix G Additional results ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">12</span></a><span class="ltx_text ltx_font_typewriter" id="S5.SS1.p1.1.2"> provides detailed results depicting performance variation across different shots subjected to instruction perturbation. We observe that chat models like BLOOMZ-7B and Llama 2 7B Chat show some sensitivity to instructions, particularly in a 1-shot setting, but this sensitivity diminishes as the number of shots increases. On the other hand, task-specific fine-tuned models like ALMA and BLOOM-7B-FT might be expected to be highly sensitive to instructions due to their training on data with specific instruction formats. Surprisingly, these models remain largely unaffected by perturbed instructions, even in a 1-shot setting. Although contrastive or random instructions aim to induce off-target or irrelevant generations, we observe that all the models consistently perform the translation task in the appropriate language, albeit with slightly reduced quality, highlighting that in-context examples primarily influence task recognition and learning.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_font_typewriter ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text ltx_font_serif" id="S5.SS2.1.1.1">5.2</span> </span>Example Perturbations</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1"><span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.1.1">Having established that models are mostly immune to instruction perturbations and are influenced more by examples, we focus on the latter.</span></p>
</div>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_font_typewriter ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text ltx_font_serif" id="S5.SS2.SSS1.1.1.1">5.2.1</span> </span>Homogeneous Perturbations</h4>
<figure class="ltx_figure" id="S5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="466" id="S5.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_serif" id="S5.F2.6.1.1">Figure 2</span>: </span>Mean percent change in ChrF++ score for each attack relative to the 0 noise baseline for each model across both translation directions (En-XX and XX-En) and both perturbation directions. Scores are averaged across shots and noise percentages (<math alttext="\delta" class="ltx_Math" display="inline" id="S5.F2.2.m1.1"><semantics id="S5.F2.2.m1.1b"><mi id="S5.F2.2.m1.1.1" xref="S5.F2.2.m1.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S5.F2.2.m1.1c"><ci id="S5.F2.2.m1.1.1.cmml" xref="S5.F2.2.m1.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.2.m1.1d">\delta</annotation><annotation encoding="application/x-llamapun" id="S5.F2.2.m1.1e">italic_δ</annotation></semantics></math>). ‘‘Span’’ denotes span noise, ‘‘order’’ represents word order attack, ‘‘dup’’ signifies word duplication attack, ‘‘add’’ indicates a punctuation addition attack, and ‘‘drop’’ signifies punctuation removal attack. Positive values indicate the performance decreased post perturbation while the negative values indicate that performance increases post perturbation. Note: In certain cases, scores are bounded within minimum and maximum values for trend clarification.
</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1"><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5.F2" title="In 5.2.1 Homogeneous Perturbations ‣ 5.2 Example Perturbations ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS1.p1.1.1"> indicates that perturbing in-context examples appear to adversely impact performance despite clear instructions. It is important to note that these instructions do not suffice as substitutes for noisy in-context examples. Generally, perturbations to the target distribution have a more severe impact than those to the source distribution, however, we observe instances where the impact of the source distribution cannot be disregarded. For instance, the BLOOMZ-7B model and ALMA-7B are susceptible to source-side perturbations, necessitating dedicated experimentation to ascertain the significance of the source distribution. We explore this aspect further in </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5.SS2.SSS3" title="5.2.3 Demonstrations from Allied Tasks ‣ 5.2 Example Perturbations ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.2.3</span></a><span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS1.p1.1.2">. Span noise emerges as the most detrimental perturbation across different model families, with the exception of BLOOM-7B-FT and Llama 2 7B models.</span></p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p2">
<p class="ltx_p" id="S5.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS1.p2.1.1">Across various perturbation attacks, we observe that the task-specific fine-tuned BLOOM-7B-FT model is most robust, followed by the BLOOM-7B model, while the multitask fine-tuned BLOOMZ-7B model is highly vulnerable. Llama 2 model generally demonstrates strong robustness to different attacks, except for the task-specific fine-tuned ALMA model, which is susceptible to span noise attack on the target side. Despite the relatively mild nature of word duplication attacks, the BLOOM-7B models surprisingly show high susceptibility. BLOOMZ-7B is robust to all attacks in XX-En translation, except for the span noise attack. Surprisingly, the Llama 2 7B model shows significant performance improvements when subjected to various attacks compared to the clean baseline across both translation and perturbation directions.</span></p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p3">
<p class="ltx_p" id="S5.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS1.p3.1.1">The current experiments primarily focus on homogeneous perturbations, indicating a detrimental impact on performance when in-context examples are perturbed uniformly. To investigate the potential influence of spatial proximity between perturbed in-context examples and the test examples, we delve into heterogeneous perturbations in </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5.SS2.SSS2" title="5.2.2 Heterogeneous Perturbations ‣ 5.2 Example Perturbations ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.2.2</span></a><span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS1.p3.1.2">. To limit the number of experiments due to computational constraints, we choose to exclude punctuation add and drop attacks, as the number of punctuations in a sentence is limited.</span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_font_typewriter ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text ltx_font_serif" id="S5.SS2.SSS2.1.1.1">5.2.2</span> </span>Heterogeneous Perturbations</h4>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="466" id="S5.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_serif" id="S5.F3.6.1.1">Figure 3</span>: </span>Mean percent change in ChrF++ score across different heterogeneous cases relative to the 0 noise baseline for each model across both translation directions (En-XX and XX-En) and both perturbation directions. Scores are averaged across attack types, shots and noise percentages (<math alttext="\delta" class="ltx_Math" display="inline" id="S5.F3.2.m1.1"><semantics id="S5.F3.2.m1.1b"><mi id="S5.F3.2.m1.1.1" xref="S5.F3.2.m1.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S5.F3.2.m1.1c"><ci id="S5.F3.2.m1.1.1.cmml" xref="S5.F3.2.m1.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.2.m1.1d">\delta</annotation><annotation encoding="application/x-llamapun" id="S5.F3.2.m1.1e">italic_δ</annotation></semantics></math>). Positive values indicate the performance decreased post perturbation while the negative values indicate that performance increases post perturbation. Note: In certain cases, scores are bounded within minimum and maximum values for clarity in depicting overarching trends.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.7"><span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS2.p1.7.1">We consider four cases categorized into two pairs based on the degree of noise perturbation: </span><math alttext="\{(k-1\ \mathrm{c},1\ \mathrm{n}),(1\ \mathrm{n},k-1\ \mathrm{c})" class="ltx_math_unparsed" display="inline" id="S5.SS2.SSS2.p1.1.m1.1"><semantics id="S5.SS2.SSS2.p1.1.m1.1a"><mrow id="S5.SS2.SSS2.p1.1.m1.1b"><mo id="S5.SS2.SSS2.p1.1.m1.1.1" stretchy="false">{</mo><mrow id="S5.SS2.SSS2.p1.1.m1.1.2"><mo id="S5.SS2.SSS2.p1.1.m1.1.2.1" stretchy="false">(</mo><mi id="S5.SS2.SSS2.p1.1.m1.1.2.2">k</mi><mo id="S5.SS2.SSS2.p1.1.m1.1.2.3">−</mo><mn id="S5.SS2.SSS2.p1.1.m1.1.2.4">1</mn><mi id="S5.SS2.SSS2.p1.1.m1.1.2.5" mathvariant="normal">c</mi><mo id="S5.SS2.SSS2.p1.1.m1.1.2.6">,</mo><mn id="S5.SS2.SSS2.p1.1.m1.1.2.7">1</mn><mi id="S5.SS2.SSS2.p1.1.m1.1.2.8" mathvariant="normal">n</mi><mo id="S5.SS2.SSS2.p1.1.m1.1.2.9" stretchy="false">)</mo></mrow><mo id="S5.SS2.SSS2.p1.1.m1.1.3">,</mo><mrow id="S5.SS2.SSS2.p1.1.m1.1.4"><mo id="S5.SS2.SSS2.p1.1.m1.1.4.1" stretchy="false">(</mo><mn id="S5.SS2.SSS2.p1.1.m1.1.4.2">1</mn><mi id="S5.SS2.SSS2.p1.1.m1.1.4.3" mathvariant="normal">n</mi><mo id="S5.SS2.SSS2.p1.1.m1.1.4.4">,</mo><mi id="S5.SS2.SSS2.p1.1.m1.1.4.5">k</mi><mo id="S5.SS2.SSS2.p1.1.m1.1.4.6">−</mo><mn id="S5.SS2.SSS2.p1.1.m1.1.4.7">1</mn><mi id="S5.SS2.SSS2.p1.1.m1.1.4.8" mathvariant="normal">c</mi><mo id="S5.SS2.SSS2.p1.1.m1.1.4.9" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.1.m1.1c">\{(k-1\ \mathrm{c},1\ \mathrm{n}),(1\ \mathrm{n},k-1\ \mathrm{c})</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p1.1.m1.1d">{ ( italic_k - 1 roman_c , 1 roman_n ) , ( 1 roman_n , italic_k - 1 roman_c )</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS2.p1.7.2"> and </span><math alttext="\{(1\ \mathrm{n},k-1\ \mathrm{c}),(k-1\ \mathrm{n},1\ \mathrm{c})\}" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p1.2.m2.2"><semantics id="S5.SS2.SSS2.p1.2.m2.2a"><mrow id="S5.SS2.SSS2.p1.2.m2.2.2.2" xref="S5.SS2.SSS2.p1.2.m2.2.2.3.cmml"><mo id="S5.SS2.SSS2.p1.2.m2.2.2.2.3" stretchy="false" xref="S5.SS2.SSS2.p1.2.m2.2.2.3.cmml">{</mo><mrow id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.3.cmml"><mo id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.3" stretchy="false" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.3.cmml">(</mo><mrow id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.1.1" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.1.1.cmml"><mn id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.1.1.2" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.1.1.2.cmml">1</mn><mo id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.1.1.1" lspace="0.500em" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.1.1.3" mathvariant="normal" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.1.1.3.cmml">n</mi></mrow><mo id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.4" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.3.cmml">,</mo><mrow id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.cmml"><mi id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.2" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.2.cmml">k</mi><mo id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.1" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.1.cmml">−</mo><mrow id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.3" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.3.cmml"><mn id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.3.2" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.3.2.cmml">1</mn><mo id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.3.1" lspace="0.500em" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.3.1.cmml">⁢</mo><mi id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.3.3" mathvariant="normal" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.3.3.cmml">c</mi></mrow></mrow><mo id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.5" stretchy="false" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.3.cmml">)</mo></mrow><mo id="S5.SS2.SSS2.p1.2.m2.2.2.2.4" xref="S5.SS2.SSS2.p1.2.m2.2.2.3.cmml">,</mo><mrow id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.3.cmml"><mo id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2.3" stretchy="false" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.3.cmml">(</mo><mrow id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.cmml"><mi id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.2" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.2.cmml">k</mi><mo id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.1" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.1.cmml">−</mo><mrow id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.3" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.3.cmml"><mn id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.3.2" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.3.2.cmml">1</mn><mo id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.3.1" lspace="0.500em" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.3.3" mathvariant="normal" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.3.3.cmml">n</mi></mrow></mrow><mo id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2.4" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.3.cmml">,</mo><mrow id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2.2" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.cmml"><mn id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.2" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.2.cmml">1</mn><mo id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.1" lspace="0.500em" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.1.cmml">⁢</mo><mi id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.3" mathvariant="normal" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.3.cmml">c</mi></mrow><mo id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2.5" stretchy="false" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.3.cmml">)</mo></mrow><mo id="S5.SS2.SSS2.p1.2.m2.2.2.2.5" stretchy="false" xref="S5.SS2.SSS2.p1.2.m2.2.2.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.2.m2.2b"><set id="S5.SS2.SSS2.p1.2.m2.2.2.3.cmml" xref="S5.SS2.SSS2.p1.2.m2.2.2.2"><interval closure="open" id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.3.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2"><apply id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.1.1.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.1.1"><times id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.1.1.1.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.1.1.1"></times><cn id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.1.1.2.cmml" type="integer" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.1.1.2">1</cn><ci id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.1.1.3.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.1.1.3">n</ci></apply><apply id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2"><minus id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.1.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.1"></minus><ci id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.2.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.2">𝑘</ci><apply id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.3.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.3"><times id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.3.1.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.3.1"></times><cn id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.3.2.cmml" type="integer" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.3.2">1</cn><ci id="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.3.3.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.1.2.2.3.3">c</ci></apply></apply></interval><interval closure="open" id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.3.cmml" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2"><apply id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.cmml" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1"><minus id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.1.cmml" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.1"></minus><ci id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.2.cmml" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.2">𝑘</ci><apply id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.3.cmml" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.3"><times id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.3.1.cmml" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.3.1"></times><cn id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.3.2.cmml" type="integer" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.3.2">1</cn><ci id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.3.3.cmml" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.1.1.3.3">n</ci></apply></apply><apply id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.cmml" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2.2"><times id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.1.cmml" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.1"></times><cn id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.2.cmml" type="integer" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.2">1</cn><ci id="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.3.cmml" xref="S5.SS2.SSS2.p1.2.m2.2.2.2.2.2.2.3">c</ci></apply></interval></set></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.2.m2.2c">\{(1\ \mathrm{n},k-1\ \mathrm{c}),(k-1\ \mathrm{n},1\ \mathrm{c})\}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p1.2.m2.2d">{ ( 1 roman_n , italic_k - 1 roman_c ) , ( italic_k - 1 roman_n , 1 roman_c ) }</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS2.p1.7.3"> where </span><math alttext="c" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p1.3.m3.1"><semantics id="S5.SS2.SSS2.p1.3.m3.1a"><mi id="S5.SS2.SSS2.p1.3.m3.1.1" xref="S5.SS2.SSS2.p1.3.m3.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.3.m3.1b"><ci id="S5.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S5.SS2.SSS2.p1.3.m3.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.3.m3.1c">c</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p1.3.m3.1d">italic_c</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS2.p1.7.4"> and </span><math alttext="n" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p1.4.m4.1"><semantics id="S5.SS2.SSS2.p1.4.m4.1a"><mi id="S5.SS2.SSS2.p1.4.m4.1.1" xref="S5.SS2.SSS2.p1.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.4.m4.1b"><ci id="S5.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S5.SS2.SSS2.p1.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.4.m4.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p1.4.m4.1d">italic_n</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS2.p1.7.5"> indicates clean and noisy. We find that placing noisy examples closer to the test example has a more detrimental impact than placing them farther away (see </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5.F3" title="In 5.2.2 Heterogeneous Perturbations ‣ 5.2 Example Perturbations ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS2.p1.7.6">). Among these cases, </span><math alttext="\{(1\ \mathrm{c},k-1\ \mathrm{n})\}" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p1.5.m5.1"><semantics id="S5.SS2.SSS2.p1.5.m5.1a"><mrow id="S5.SS2.SSS2.p1.5.m5.1.1.1" xref="S5.SS2.SSS2.p1.5.m5.1.1.2.cmml"><mo id="S5.SS2.SSS2.p1.5.m5.1.1.1.2" stretchy="false" xref="S5.SS2.SSS2.p1.5.m5.1.1.2.cmml">{</mo><mrow id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.3.cmml"><mo id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.3" stretchy="false" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.3.cmml">(</mo><mrow id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.1.1" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.1.1.cmml"><mn id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.1.1.2" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.1.1.2.cmml">1</mn><mo id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.1.1.1" lspace="0.500em" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.1.1.3" mathvariant="normal" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.1.1.3.cmml">c</mi></mrow><mo id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.4" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.3.cmml">,</mo><mrow id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.cmml"><mi id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.2" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.2.cmml">k</mi><mo id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.1" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.1.cmml">−</mo><mrow id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.3" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.3.cmml"><mn id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.3.2" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.3.2.cmml">1</mn><mo id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.3.1" lspace="0.500em" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.3.1.cmml">⁢</mo><mi id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.3.3" mathvariant="normal" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.3.3.cmml">n</mi></mrow></mrow><mo id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.5" stretchy="false" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.3.cmml">)</mo></mrow><mo id="S5.SS2.SSS2.p1.5.m5.1.1.1.3" stretchy="false" xref="S5.SS2.SSS2.p1.5.m5.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.5.m5.1b"><set id="S5.SS2.SSS2.p1.5.m5.1.1.2.cmml" xref="S5.SS2.SSS2.p1.5.m5.1.1.1"><interval closure="open" id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.3.cmml" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2"><apply id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.1.1.cmml" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.1.1"><times id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.1.1.1.cmml" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.1.1.1"></times><cn id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.1.1.2.cmml" type="integer" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.1.1.2">1</cn><ci id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.1.1.3.cmml" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.1.1.3">c</ci></apply><apply id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.cmml" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2"><minus id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.1.cmml" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.1"></minus><ci id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.2.cmml" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.2">𝑘</ci><apply id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.3.cmml" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.3"><times id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.3.1.cmml" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.3.1"></times><cn id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.3.2.cmml" type="integer" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.3.2">1</cn><ci id="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.3.3.cmml" xref="S5.SS2.SSS2.p1.5.m5.1.1.1.1.2.2.3.3">n</ci></apply></apply></interval></set></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.5.m5.1c">\{(1\ \mathrm{c},k-1\ \mathrm{n})\}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p1.5.m5.1d">{ ( 1 roman_c , italic_k - 1 roman_n ) }</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS2.p1.7.7"> is identified as the most detrimental. Consistent with prior findings, we note an improvement in Llama 2 7B performance when exposed to noise, with the most significant improvement occurring when the noisy examples are furthest from the test example (optimal conditions at </span><math alttext="(k-1\ \mathrm{c},1\ \mathrm{n})" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p1.6.m6.2"><semantics id="S5.SS2.SSS2.p1.6.m6.2a"><mrow id="S5.SS2.SSS2.p1.6.m6.2.2.2" xref="S5.SS2.SSS2.p1.6.m6.2.2.3.cmml"><mo id="S5.SS2.SSS2.p1.6.m6.2.2.2.3" stretchy="false" xref="S5.SS2.SSS2.p1.6.m6.2.2.3.cmml">(</mo><mrow id="S5.SS2.SSS2.p1.6.m6.1.1.1.1" xref="S5.SS2.SSS2.p1.6.m6.1.1.1.1.cmml"><mi id="S5.SS2.SSS2.p1.6.m6.1.1.1.1.2" xref="S5.SS2.SSS2.p1.6.m6.1.1.1.1.2.cmml">k</mi><mo id="S5.SS2.SSS2.p1.6.m6.1.1.1.1.1" xref="S5.SS2.SSS2.p1.6.m6.1.1.1.1.1.cmml">−</mo><mrow id="S5.SS2.SSS2.p1.6.m6.1.1.1.1.3" xref="S5.SS2.SSS2.p1.6.m6.1.1.1.1.3.cmml"><mn id="S5.SS2.SSS2.p1.6.m6.1.1.1.1.3.2" xref="S5.SS2.SSS2.p1.6.m6.1.1.1.1.3.2.cmml">1</mn><mo id="S5.SS2.SSS2.p1.6.m6.1.1.1.1.3.1" lspace="0.500em" xref="S5.SS2.SSS2.p1.6.m6.1.1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS2.SSS2.p1.6.m6.1.1.1.1.3.3" mathvariant="normal" xref="S5.SS2.SSS2.p1.6.m6.1.1.1.1.3.3.cmml">c</mi></mrow></mrow><mo id="S5.SS2.SSS2.p1.6.m6.2.2.2.4" xref="S5.SS2.SSS2.p1.6.m6.2.2.3.cmml">,</mo><mrow id="S5.SS2.SSS2.p1.6.m6.2.2.2.2" xref="S5.SS2.SSS2.p1.6.m6.2.2.2.2.cmml"><mn id="S5.SS2.SSS2.p1.6.m6.2.2.2.2.2" xref="S5.SS2.SSS2.p1.6.m6.2.2.2.2.2.cmml">1</mn><mo id="S5.SS2.SSS2.p1.6.m6.2.2.2.2.1" lspace="0.500em" xref="S5.SS2.SSS2.p1.6.m6.2.2.2.2.1.cmml">⁢</mo><mi id="S5.SS2.SSS2.p1.6.m6.2.2.2.2.3" mathvariant="normal" xref="S5.SS2.SSS2.p1.6.m6.2.2.2.2.3.cmml">n</mi></mrow><mo id="S5.SS2.SSS2.p1.6.m6.2.2.2.5" stretchy="false" xref="S5.SS2.SSS2.p1.6.m6.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.6.m6.2b"><interval closure="open" id="S5.SS2.SSS2.p1.6.m6.2.2.3.cmml" xref="S5.SS2.SSS2.p1.6.m6.2.2.2"><apply id="S5.SS2.SSS2.p1.6.m6.1.1.1.1.cmml" xref="S5.SS2.SSS2.p1.6.m6.1.1.1.1"><minus id="S5.SS2.SSS2.p1.6.m6.1.1.1.1.1.cmml" xref="S5.SS2.SSS2.p1.6.m6.1.1.1.1.1"></minus><ci id="S5.SS2.SSS2.p1.6.m6.1.1.1.1.2.cmml" xref="S5.SS2.SSS2.p1.6.m6.1.1.1.1.2">𝑘</ci><apply id="S5.SS2.SSS2.p1.6.m6.1.1.1.1.3.cmml" xref="S5.SS2.SSS2.p1.6.m6.1.1.1.1.3"><times id="S5.SS2.SSS2.p1.6.m6.1.1.1.1.3.1.cmml" xref="S5.SS2.SSS2.p1.6.m6.1.1.1.1.3.1"></times><cn id="S5.SS2.SSS2.p1.6.m6.1.1.1.1.3.2.cmml" type="integer" xref="S5.SS2.SSS2.p1.6.m6.1.1.1.1.3.2">1</cn><ci id="S5.SS2.SSS2.p1.6.m6.1.1.1.1.3.3.cmml" xref="S5.SS2.SSS2.p1.6.m6.1.1.1.1.3.3">c</ci></apply></apply><apply id="S5.SS2.SSS2.p1.6.m6.2.2.2.2.cmml" xref="S5.SS2.SSS2.p1.6.m6.2.2.2.2"><times id="S5.SS2.SSS2.p1.6.m6.2.2.2.2.1.cmml" xref="S5.SS2.SSS2.p1.6.m6.2.2.2.2.1"></times><cn id="S5.SS2.SSS2.p1.6.m6.2.2.2.2.2.cmml" type="integer" xref="S5.SS2.SSS2.p1.6.m6.2.2.2.2.2">1</cn><ci id="S5.SS2.SSS2.p1.6.m6.2.2.2.2.3.cmml" xref="S5.SS2.SSS2.p1.6.m6.2.2.2.2.3">n</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.6.m6.2c">(k-1\ \mathrm{c},1\ \mathrm{n})</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p1.6.m6.2d">( italic_k - 1 roman_c , 1 roman_n )</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS2.p1.7.8">). Llama 2 7B Chat is fairly robust and demonstrates improvement upon perturbation, except for the most detrimental case. ALMA shows slight susceptibility across all variants, with the most severe impact observed in the most detrimental case. BLOOM-7B demonstrates robustness against source-side perturbations but is heavily affected by target-side perturbations, particularly when </span><math alttext="k-1" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p1.7.m7.1"><semantics id="S5.SS2.SSS2.p1.7.m7.1a"><mrow id="S5.SS2.SSS2.p1.7.m7.1.1" xref="S5.SS2.SSS2.p1.7.m7.1.1.cmml"><mi id="S5.SS2.SSS2.p1.7.m7.1.1.2" xref="S5.SS2.SSS2.p1.7.m7.1.1.2.cmml">k</mi><mo id="S5.SS2.SSS2.p1.7.m7.1.1.1" xref="S5.SS2.SSS2.p1.7.m7.1.1.1.cmml">−</mo><mn id="S5.SS2.SSS2.p1.7.m7.1.1.3" xref="S5.SS2.SSS2.p1.7.m7.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.7.m7.1b"><apply id="S5.SS2.SSS2.p1.7.m7.1.1.cmml" xref="S5.SS2.SSS2.p1.7.m7.1.1"><minus id="S5.SS2.SSS2.p1.7.m7.1.1.1.cmml" xref="S5.SS2.SSS2.p1.7.m7.1.1.1"></minus><ci id="S5.SS2.SSS2.p1.7.m7.1.1.2.cmml" xref="S5.SS2.SSS2.p1.7.m7.1.1.2">𝑘</ci><cn id="S5.SS2.SSS2.p1.7.m7.1.1.3.cmml" type="integer" xref="S5.SS2.SSS2.p1.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.7.m7.1c">k-1</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p1.7.m7.1d">italic_k - 1</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS2.p1.7.9"> noisy examples are closer to the test example. BLOOMZ-7B is generally affected across all settings, with the most severe impact in the most detrimental case. Conversely, BLOOM-7B-FT also demonstrates a fair amount of robustness. Our experimental results provide empirical evidence to practitioners that in settings involving a mixed pool of noisy and clean in-context examples, the optimal approach is to position noisy examples at the beginning and cleaner examples closer to the test example to maximize performance.</span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS3">
<h4 class="ltx_title ltx_font_typewriter ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text ltx_font_serif" id="S5.SS2.SSS3.1.1.1">5.2.3</span> </span>Demonstrations from Allied Tasks</h4>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="654" id="S5.F4.g1" src="x4.png" width="831"/>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_serif" id="S5.F4.3.1.1">Figure 4</span>: </span>ChrF++ score of different models averaged across shots and translation directions comparing the choice of the auxiliary source language of the demonstration.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.SSS3.p1">
<p class="ltx_p" id="S5.SS2.SSS3.p1.1"><span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS3.p1.1.1">Prior experiments (see </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5.SS1" title="5.1 Sensitivity to Instruction Variations ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Sections</span> <span class="ltx_text ltx_ref_tag">5.1</span></a><span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS3.p1.1.2"> and </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5.SS2.SSS1" title="5.2.1 Homogeneous Perturbations ‣ 5.2 Example Perturbations ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">5.2.1</span></a><span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS3.p1.1.3">) indicate that examples are more influential than instructions, and the target distribution of the example has broadly more importance than the source distribution. However, our focus has been limited to in-context examples of the same task during testing. To explore the limits of example-based learning, we aim to investigate if in-context examples from a different source language but the same target language suffice as a proxy for guiding the model in translating into the desired target language, thus probing the limits of example-based learning.</span></p>
</div>
<div class="ltx_para" id="S5.SS2.SSS3.p2">
<p class="ltx_p" id="S5.SS2.SSS3.p2.1"><span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS3.p2.1.1">Our initial experiments, selecting an auxiliary language, showed that using in-context examples from a related task can generally serve as a suitable proxy for the target task, barring a few exceptions. To delve deeper, we considered 2 additional settings: using the same script as the test time source language and using English, the predominant language models were trained on, as the auxiliary source. </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5.F4" title="In 5.2.3 Demonstrations from Allied Tasks ‣ 5.2 Example Perturbations ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS3.p2.1.2"> shows that the source distribution of in-context examples has a marginal effect on downstream MT performance. This suggests that any auxiliary source language can generally guide the model adequately, although there are a few exceptions, such as Llama 2 7B where using the same script as the auxiliary language outperforms English, and BLOOMZ-7B where a randomly chosen auxiliary language surpasses even using the same language. This suggests the limited impact of the choice of source language (source distribution). We conclude that in the absence of demonstrations from the exact pair as the test examples, demonstrations from any alternative source language can serve as a viable substitute, yielding comparable performance to that of the same source language baseline.</span></p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_font_typewriter ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text ltx_font_serif" id="S5.SS3.1.1.1">5.3</span> </span>Misalignment</h3>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="926" id="S5.F5.g1" src="x5.png" width="831"/>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_serif" id="S5.F5.3.1.1">Figure 5</span>: </span>ChrF++ score of different models averaged translation directions comparing different forms of misalignment in a pivot-translation setting.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1"><span class="ltx_text ltx_font_typewriter" id="S5.SS3.p1.1.1">In this experimental setup, we aim to understand if semantic priors in models can protect against susceptibility to misinformation presented in context, rendering them effective and robust in-context learners. It is evident that most models, except Llama 2 7B Chat, can form transitive associations to varying degrees (see </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5.F5" title="In 5.3 Misalignment ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a><span class="ltx_text ltx_font_typewriter" id="S5.SS3.p1.1.2">). Models from the BLOOM family and ALMA are particularly susceptible to extensive copy-pasting, suggesting they can be easily misled by in-context demonstrations, especially in cases of the target being misaligned. Notably, the ALMA model heavily relies on the English sentence as a pivot for these associations, and perturbing the English example also severely impacts performance despite the target translation being present in the context. Llama 2 7B exhibits poor translation quality, being unable to deduce answers from the context using transitive relations in aligned cases, and the performance further degrades in misaligned cases, indicating vulnerability to perturbations. In contrast, Llama 2 7B Chat remains largely unaffected by misinformation-based perturbations, showing robustness but poor downstream performance, indicating inadequacy in building transitive associations in the case of aligned examples. An ideal model should utilize its inherent biases to regulate its reliance on the information presented in context. Current 7B LLMs fall short in this regard, highlighting the necessity for future research to develop models that excel as in-context learners while minimizing susceptibility to misinformation within the context.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_font_typewriter ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text ltx_font_serif" id="S5.SS4.1.1.1">5.4</span> </span>Directionality of ICL Demonstrations</h3>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="753" id="S5.F6.g1" src="x6.png" width="706"/>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_serif" id="S5.F6.3.1.1">Figure 6</span>: </span>ChrF++ score of different models averaged across shots across translation directions comparing the choice of the source original and target original demonstrations.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1"><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5.SS2.SSS1" title="5.2.1 Homogeneous Perturbations ‣ 5.2 Example Perturbations ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Sections</span> <span class="ltx_text ltx_ref_tag">5.2.1</span></a><span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.1"> and </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5.SS2.SSS3" title="5.2.3 Demonstrations from Allied Tasks ‣ 5.2 Example Perturbations ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">5.2.3</span></a><span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.2"> shows the significance of target distribution; therefore, we further study if target-original demonstrations are superior to source-original demonstrations, depending on the translation direction. Focusing on Indic languages, we utilize an in-house multi-domain Indic Original set for sourcing Indic Original demonstrations and FLORES-200 dev set </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.3.1">(</span><span class="ltx_text ltx_font_typewriter">Goyal et al.</span><span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.4.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib19" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a>; <span class="ltx_text ltx_font_typewriter">Costa-jussà et al.</span><span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.4.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib10" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a><span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.5.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.6"> for English original ones. IN22-Gen </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.7.1">(</span><span class="ltx_text ltx_font_typewriter">Gala et al.</span><span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.8.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib17" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.9.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.10">, which is English-original, serves as the test set. The Indic Original set</span><span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Will be released later as a held-out set of <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www2.statmt.org/wmt24/multiindicmt-task.html" title="">https://www2.statmt.org/wmt24/multiindicmt-task.html</a></span></span></span><span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.11"> serves as the target original set for En-XX direction and the source original set for XX-En, while FLORES-200 dev set </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.12.1">(</span><span class="ltx_text ltx_font_typewriter">Goyal et al.</span><span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.13.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib19" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a>; <span class="ltx_text ltx_font_typewriter">Costa-jussà et al.</span><span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.13.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib10" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a><span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.14.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.15"> acts as the target original set for XX-En direction and source original set for En-XX direction.</span></p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1"><span class="ltx_text ltx_font_typewriter" id="S5.SS4.p2.1.1">From </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5.F6" title="In 5.4 Directionality of ICL Demonstrations ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a><span class="ltx_text ltx_font_typewriter" id="S5.SS4.p2.1.2">, across both En-XX and XX-En translation directions, we observe a minimal difference in the performance when demonstrations are chosen from the respective source original and target-original sets for all BLOOM family models considered. This suggests that the directionality of demonstrations may not have much significance on the downstream performance. However, it’s important to acknowledge that our findings are limited to experiments conducted for Indic languages and using automatic metric-based evaluations, which may not sufficiently capture whether using target-original in-context examples has any impact on the fluency of the generation or not. Further investigation through human evaluation is necessary to ascertain this, an aspect we leave for future work.</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_font_typewriter ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text ltx_font_serif" id="S6.1.1.1">6</span> </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1"><span class="ltx_text ltx_font_typewriter" id="S6.p1.1.1">Although ICL has demonstrated good performance and is widely employed across different tasks, there is limited understanding of the aspects that influence the downstream performance of ICL. In this work, we aim to bridge this gap for MT by investigating the central question of whether ICL for MT is example-driven or instruction-driven and subsequently examining the role of different aspects of ICL. We find that: (1) ICL is primarily example-driven and choice of instruction has limited impact, with target distribution of the demonstrations being the most influential (2) Perturbation to demonstrations is generally detrimental but can have a regularization effect in some cases (3) Spatial proximity to the test example is an important factor in ICL (4) Demonstrations having the same target language can serve as a reliable proxy and choice of the source language of the demonstration is inconsequential
(5) The directionality of the demonstrations has minimal impact.
(6) ICL can potentially be exploited to mislead even smaller models, highlighting the need for further research to make models robust in-context learners.</span></p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_font_typewriter ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text ltx_font_serif" id="S7.1.1.1">7</span> </span>Limitations</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1"><span class="ltx_text ltx_font_typewriter" id="S7.p1.1.1">Our perturbation experiments focused on only 5 non-linguistic perturbations, which involved random textual attacks that were mostly uniform and language-agnostic in nature. However, it would also be interesting to explore linguistically aware perturbations such as causality alternation, entity replacement, and number replacement, similar to </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_typewriter">Chen et al.</span> <span class="ltx_text ltx_font_typewriter" id="S7.p1.1.2.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib7" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="S7.p1.1.3.2.2.1">)</span></cite><span class="ltx_text ltx_font_typewriter" id="S7.p1.1.4">. This would test if the model is susceptible to more fine-grained and subtle errors. Additionally, our investigation was limited to the 7B model scale due to computational constraints, and exploring whether our current findings generalize across larger model scales would provide us a more comprehensive understanding of whether ICL capabilities models vary with scale. Furthermore, while the focus of this study was on the MT task, subsequent research should examine the transferability of these insights to diverse natural language generation tasks.</span></p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_font_typewriter ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text ltx_font_serif" id="S8.1.1.1">8</span> </span>Ethical Considerations</h2>
<div class="ltx_para ltx_noindent" id="S8.p1">
<p class="ltx_p" id="S8.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S8.p1.1.1">Potential toxic and hateful outputs:</span><span class="ltx_text ltx_font_typewriter" id="S8.p1.1.2"> Our work focuses on the robustness of models to various factors influencing MT via ICL. One aspect of our exploration is via perturbation which may lead to hallucinations as well as generate toxic and hateful content as a result of the model’s distributions being perturbed. This may even be applicable to other generative tasks but that is not the intention of our work.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S8.p2">
<p class="ltx_p" id="S8.p2.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S8.p2.1.1">Safety Circumvention and Jailbreaking:</span><span class="ltx_text ltx_font_typewriter" id="S8.p2.1.2"> Adversarial perturbations might be able to circumvent a model’s safety parameters and enable the generation of biased and harmful outputs. We plan to release our perturbation scripts and resultant perturbed data for research, and we do not intend it to be used to perform adversarial attacks in practice intended to undo the security features, also known as jailbreaking, of a model.</span></p>
</div>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_font_typewriter ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text ltx_font_serif" id="S9.1.1.1">9</span> </span>Acknowledgements</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1"><span class="ltx_text ltx_font_typewriter" id="S9.p1.1.1">We sincerely thank Varun Gumma (SCAI Fellow, Microsoft Research) for the insightful discussions and his generous review of this draft, which helped us organize our ideas more effectively and improve its overall readability.</span></p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_font_typewriter ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib1.5.5.1">Ahuja et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib1.7.1">
Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, Ishaan Watts, Ashutosh Sathe, Millicent Ochieng, Rishav Hada, Prachi Jain, Maxamed Axmed, Kalika Bali, and Sunayana Sitaram. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2311.07463" title="">Megaverse: Benchmarking large language models across languages, modalities, models and tasks</a><span class="ltx_text ltx_font_typewriter" id="bib.bib1.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib2.5.5.1">Akyürek et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib2.7.1">
Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://openreview.net/forum?id=0g0X4H8yN4I" title="">What learning algorithm is in-context learning? investigations with linear models</a><span class="ltx_text ltx_font_typewriter" id="bib.bib2.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib2.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib2.10.2">The Eleventh International Conference on Learning Representations</em><span class="ltx_text ltx_font_typewriter" id="bib.bib2.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib3.4.4.1">Belinkov and Bisk (2018)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib3.6.1">
Yonatan Belinkov and Yonatan Bisk. 2018.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://openreview.net/forum?id=BJ8vJebC-" title="">Synthetic and natural noise both break neural machine translation</a><span class="ltx_text ltx_font_typewriter" id="bib.bib3.7.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib3.8.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib3.9.2">International Conference on Learning Representations</em><span class="ltx_text ltx_font_typewriter" id="bib.bib3.10.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib4.5.5.1">Briakou et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib4.7.1">
Eleftheria Briakou, Colin Cherry, and George Foster. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/2023.acl-long.524" title="">Searching for needles in a haystack: On the role of incidental bilingualism in PaLM’s translation capability</a><span class="ltx_text ltx_font_typewriter" id="bib.bib4.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib4.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib4.10.2">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em><span class="ltx_text ltx_font_typewriter" id="bib.bib4.11.3">, pages 9432--9452, Toronto, Canada. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib5.5.5.1">Brown et al. (2020)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib5.7.1">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" title="">Language models are few-shot learners</a><span class="ltx_text ltx_font_typewriter" id="bib.bib5.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib5.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib5.10.2">Advances in Neural Information Processing Systems</em><span class="ltx_text ltx_font_typewriter" id="bib.bib5.11.3">, volume 33, pages 1877--1901. Curran Associates, Inc.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib6.5.5.1">Chan et al. (2022)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib6.7.1">
Stephanie C. Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh, Pierre H. Richemond, Jay McClelland, and Felix Hill. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2205.05055" title="">Data distributional properties drive emergent in-context learning in transformers</a><span class="ltx_text ltx_font_typewriter" id="bib.bib6.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib7.5.5.1">Chen et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib7.7.1">
Mingda Chen, Kevin Heffernan, Onur Çelebi, Alexandre Mourachko, and Holger Schwenk. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/2023.acl-short.10" title="">xSIM++: An improved proxy to bitext mining performance for low-resource languages</a><span class="ltx_text ltx_font_typewriter" id="bib.bib7.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib7.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib7.10.2">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em><span class="ltx_text ltx_font_typewriter" id="bib.bib7.11.3">, pages 101--109, Toronto, Canada. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib8.5.5.1">Cheng et al. (2020)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib8.7.1">
Minhao Cheng, Jinfeng Yi, Pin-Yu Chen, Huan Zhang, and Cho-Jui Hsieh. 2020.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.1609/AAAI.V34I04.5767" title="">Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples</a><span class="ltx_text ltx_font_typewriter" id="bib.bib8.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib8.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib8.10.2">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020</em><span class="ltx_text ltx_font_typewriter" id="bib.bib8.11.3">, pages 3601--3608. AAAI Press.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib9.5.5.1">Chowdhery et al. (2022)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib9.7.1">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2204.02311" title="">Palm: Scaling language modeling with pathways</a><span class="ltx_text ltx_font_typewriter" id="bib.bib9.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib10.5.5.1">Costa-jussà et al. (2022)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib10.7.1">
Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2207.04672" title="">No language left behind: Scaling human-centered machine translation</a><span class="ltx_text ltx_font_typewriter" id="bib.bib10.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib11.5.5.1">Devlin et al. (2019)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib11.7.1">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/N19-1423" title="">BERT: Pre-training of deep bidirectional transformers for language understanding</a><span class="ltx_text ltx_font_typewriter" id="bib.bib11.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib11.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib11.10.2">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em><span class="ltx_text ltx_font_typewriter" id="bib.bib11.11.3">, pages 4171--4186, Minneapolis, Minnesota. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib12.5.5.1">Dong et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib12.7.1">
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2301.00234" title="">A survey on in-context learning</a><span class="ltx_text ltx_font_typewriter" id="bib.bib12.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib13.5.5.1">Ebrahimi et al. (2018)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib13.7.1">
Javid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://aclanthology.org/C18-1055" title="">On adversarial examples for character-level neural machine translation</a><span class="ltx_text ltx_font_typewriter" id="bib.bib13.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib13.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib13.10.2">Proceedings of the 27th International Conference on Computational Linguistics</em><span class="ltx_text ltx_font_typewriter" id="bib.bib13.11.3">, pages 653--663, Santa Fe, New Mexico, USA. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib14.5.5.1">Fan et al. (2022)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib14.7.1">
Angela Fan, Suzana Ilic, Thomas Wolf, and Matthias Gallé, editors. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://aclanthology.org/2022.bigscience-1.0" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib14.8.1.1">Proceedings of BigScience Episode #5 -- Workshop on Challenges &amp; Perspectives in Creating Large Language Models</em></a><span class="ltx_text ltx_font_typewriter" id="bib.bib14.9.2">. Association for Computational Linguistics, virtual+Dublin.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib15.5.5.1">Federmann et al. (2022)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib15.7.1">
Christian Federmann, Tom Kocmi, and Ying Xin. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://aclanthology.org/2022.sumeval-1.4" title="">NTREX-128 -- news test references for MT evaluation of 128 languages</a><span class="ltx_text ltx_font_typewriter" id="bib.bib15.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib15.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib15.10.2">Proceedings of the First Workshop on Scaling Up Multilingual Evaluation</em><span class="ltx_text ltx_font_typewriter" id="bib.bib15.11.3">, pages 21--24, Online. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib16.5.5.1">Formento et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib16.7.1">
Brian Formento, Chuan Sheng Foo, Luu Anh Tuan, and See Kiong Ng. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/2023.findings-eacl.1" title="">Using punctuation as an adversarial attack on deep learning-based NLP systems: An empirical study</a><span class="ltx_text ltx_font_typewriter" id="bib.bib16.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib16.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib16.10.2">Findings of the Association for Computational Linguistics: EACL 2023</em><span class="ltx_text ltx_font_typewriter" id="bib.bib16.11.3">, pages 1--34, Dubrovnik, Croatia. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib17.5.5.1">Gala et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib17.7.1">
Jay Gala, Pranjal A Chitale, A K Raghavan, Varun Gumma, Sumanth Doddapaneni, Aswanth Kumar M, Janki Atul Nawale, Anupama Sujatha, Ratish Puduppully, Vivek Raghavan, Pratyush Kumar, Mitesh M Khapra, Raj Dabre, and Anoop Kunchukuttan. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://openreview.net/forum?id=vfT4YuzAYA" title="">Indictrans2: Towards high-quality and accessible machine translation models for all 22 scheduled indian languages</a><span class="ltx_text ltx_font_typewriter" id="bib.bib17.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib17.9.1">Transactions on Machine Learning Research</em><span class="ltx_text ltx_font_typewriter" id="bib.bib17.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib18.4.4.1">Garg and Ramakrishnan (2020)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib18.6.1">
Siddhant Garg and Goutham Ramakrishnan. 2020.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/2020.emnlp-main.498" title="">Bae: Bert-based adversarial examples for text classification</a><span class="ltx_text ltx_font_typewriter" id="bib.bib18.7.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib18.8.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib18.9.2">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em><span class="ltx_text ltx_font_typewriter" id="bib.bib18.10.3">. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib19.5.5.1">Goyal et al. (2022)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib19.7.1">
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.1162/tacl_a_00474" title="">The Flores-101 evaluation benchmark for low-resource and multilingual machine translation</a><span class="ltx_text ltx_font_typewriter" id="bib.bib19.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib19.9.1">Transactions of the Association for Computational Linguistics</em><span class="ltx_text ltx_font_typewriter" id="bib.bib19.10.2">, 10:522--538.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib20.5.5.1">Joshi et al. (2020)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib20.7.1">
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/1907.10529" title="">Spanbert: Improving pre-training by representing and predicting spans</a><span class="ltx_text ltx_font_typewriter" id="bib.bib20.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib21.5.5.1">Kocmi et al. (2021)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib21.7.1">
Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. 2021.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://aclanthology.org/2021.wmt-1.57" title="">To ship or not to ship: An extensive evaluation of automatic metrics for machine translation</a><span class="ltx_text ltx_font_typewriter" id="bib.bib21.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib21.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib21.10.2">Proceedings of the Sixth Conference on Machine Translation</em><span class="ltx_text ltx_font_typewriter" id="bib.bib21.11.3">, pages 478--494, Online. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib22.5.5.1">Kossen et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib22.7.1">
Jannik Kossen, Yarin Gal, and Tom Rainforth. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2307.12375" title="">In-context learning learns label relationships but is not conventional learning</a><span class="ltx_text ltx_font_typewriter" id="bib.bib22.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib23.5.5.1">Kumar et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib23.7.1">
Aswanth Kumar, Ratish Puduppully, Raj Dabre, and Anoop Kunchukuttan. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/2023.findings-emnlp.519" title="">CTQScorer: Combining multiple features for in-context example selection for machine translation</a><span class="ltx_text ltx_font_typewriter" id="bib.bib23.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib23.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib23.10.2">Findings of the Association for Computational Linguistics: EMNLP 2023</em><span class="ltx_text ltx_font_typewriter" id="bib.bib23.11.3">, pages 7736--7752, Singapore. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib24.5.5.1">Lewis et al. (2020)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib24.7.1">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/2020.acl-main.703" title="">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</a><span class="ltx_text ltx_font_typewriter" id="bib.bib24.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib24.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib24.10.2">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em><span class="ltx_text ltx_font_typewriter" id="bib.bib24.11.3">, pages 7871--7880, Online. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib25.5.5.1">Li et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib25.7.1">
Yingcong Li, M. Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2301.07067" title="">Transformers as algorithms: Generalization and stability in in-context learning</a><span class="ltx_text ltx_font_typewriter" id="bib.bib25.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib26.5.5.1">Liu et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib26.7.1">
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.1145/3560815" title="">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</a><span class="ltx_text ltx_font_typewriter" id="bib.bib26.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib26.9.1">ACM Comput. Surv.</em><span class="ltx_text ltx_font_typewriter" id="bib.bib26.10.2">, 55(9).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib27.5.5.1">Lu et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib27.7.1">
Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2309.01809" title="">Are emergent abilities in large language models just in-context learning?</a><span class="ltx_text ltx_font_typewriter" id="bib.bib27.8.1">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib28.5.5.1">Maurya et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib28.7.1">
Kaushal Kumar Maurya, Rahul Kejriwal, Maunendra Sankar Desarkar, and Anoop Kunchukuttan. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2305.05214" title="">Utilizing lexical similarity to enable zero-shot machine translation for extremely low-resource languages</a><span class="ltx_text ltx_font_typewriter" id="bib.bib28.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib29.5.5.1">Michel et al. (2019)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib29.7.1">
Paul Michel, Xian Li, Graham Neubig, and Juan Pino. 2019.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/N19-1314" title="">On evaluation of adversarial perturbations for sequence-to-sequence models</a><span class="ltx_text ltx_font_typewriter" id="bib.bib29.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib29.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib29.10.2">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em><span class="ltx_text ltx_font_typewriter" id="bib.bib29.11.3">, pages 3103--3114, Minneapolis, Minnesota. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib30.5.5.1">Min et al. (2022)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib30.7.1">
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/2022.emnlp-main.759" title="">Rethinking the role of demonstrations: What makes in-context learning work?</a><span class="ltx_text ltx_font_typewriter" id="bib.bib30.8.1">
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib30.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib30.10.2">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em><span class="ltx_text ltx_font_typewriter" id="bib.bib30.11.3">, pages 11048--11064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib31.4.4.1">Moradi and Samwald (2021)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib31.6.1">
Milad Moradi and Matthias Samwald. 2021.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/2021.emnlp-main.117" title="">Evaluating the robustness of neural language models to input perturbations</a><span class="ltx_text ltx_font_typewriter" id="bib.bib31.7.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib31.8.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib31.9.2">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em><span class="ltx_text ltx_font_typewriter" id="bib.bib31.10.3">, pages 1558--1570, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib32.5.5.1">Muennighoff et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib32.7.1">
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/2023.acl-long.891" title="">Crosslingual generalization through multitask finetuning</a><span class="ltx_text ltx_font_typewriter" id="bib.bib32.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib32.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib32.10.2">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em><span class="ltx_text ltx_font_typewriter" id="bib.bib32.11.3">, pages 15991--16111, Toronto, Canada. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib33.5.5.1">Niu et al. (2020)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib33.7.1">
Xing Niu, Prashant Mathur, Georgiana Dinu, and Yaser Al-Onaizan. 2020.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/2020.acl-main.755" title="">Evaluating robustness to input perturbations for neural machine translation</a><span class="ltx_text ltx_font_typewriter" id="bib.bib33.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib33.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib33.10.2">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em><span class="ltx_text ltx_font_typewriter" id="bib.bib33.11.3">, pages 8538--8544, Online. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib34.5.5.1">OpenAI et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib34.7.1">
OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao,
Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe,
Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder,
Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao
Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2303.08774" title="">Gpt-4 technical report</a><span class="ltx_text ltx_font_typewriter" id="bib.bib34.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib35.4.4.1">Popović (2017)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib35.6.1">
Maja Popović. 2017.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/W17-4770" title="">chrF++: words helping character n-grams</a><span class="ltx_text ltx_font_typewriter" id="bib.bib35.7.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib35.8.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib35.9.2">Proceedings of the Second Conference on Machine Translation</em><span class="ltx_text ltx_font_typewriter" id="bib.bib35.10.3">, pages 612--618, Copenhagen, Denmark. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib36.4.4.1">Post (2018)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib36.6.1">
Matt Post. 2018.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/W18-6319" title="">A call for clarity in reporting BLEU scores</a><span class="ltx_text ltx_font_typewriter" id="bib.bib36.7.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib36.8.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib36.9.2">Proceedings of the Third Conference on Machine Translation: Research Papers</em><span class="ltx_text ltx_font_typewriter" id="bib.bib36.10.3">, pages 186--191, Brussels, Belgium. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib37.5.5.1">Radford et al. (2019)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib37.7.1">
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib37.8.1">Language models are unsupervised multitask learners.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib38.5.5.1">Raffel et al. (2020)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib38.7.1">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://jmlr.org/papers/v21/20-074.html" title="">Exploring the limits of transfer learning with a unified text-to-text transformer</a><span class="ltx_text ltx_font_typewriter" id="bib.bib38.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib38.9.1">Journal of Machine Learning Research</em><span class="ltx_text ltx_font_typewriter" id="bib.bib38.10.2">, 21(140):1--67.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib39.5.5.1">Raunak et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib39.7.1">
Vikas Raunak, Arul Menezes, and Hany Awadalla. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/2023.findings-emnlp.61" title="">Dissecting in-context learning of translations in GPT-3</a><span class="ltx_text ltx_font_typewriter" id="bib.bib39.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib39.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib39.10.2">Findings of the Association for Computational Linguistics: EMNLP 2023</em><span class="ltx_text ltx_font_typewriter" id="bib.bib39.11.3">, pages 866--872, Singapore. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib40.5.5.1">Rei et al. (2022)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib40.7.1">
Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://aclanthology.org/2022.wmt-1.52" title="">COMET-22: Unbabel-IST 2022 submission for the metrics shared task</a><span class="ltx_text ltx_font_typewriter" id="bib.bib40.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib40.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib40.10.2">Proceedings of the Seventh Conference on Machine Translation (WMT)</em><span class="ltx_text ltx_font_typewriter" id="bib.bib40.11.3">, pages 578--585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib41.5.5.1">Robinson et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib41.7.1">
Nathaniel Robinson, Perez Ogayo, David R. Mortensen, and Graham Neubig. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/2023.wmt-1.40" title="">ChatGPT MT: Competitive for high- (but not low-) resource languages</a><span class="ltx_text ltx_font_typewriter" id="bib.bib41.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib41.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib41.10.2">Proceedings of the Eighth Conference on Machine Translation</em><span class="ltx_text ltx_font_typewriter" id="bib.bib41.11.3">, pages 392--418, Singapore. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib42.5.5.1">Sai B et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib42.7.1">
Ananya Sai B, Tanay Dixit, Vignesh Nagarajan, Anoop Kunchukuttan, Pratyush Kumar, Mitesh M. Khapra, and Raj Dabre. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/2023.acl-long.795" title="">IndicMT eval: A dataset to meta-evaluate machine translation metrics for Indian languages</a><span class="ltx_text ltx_font_typewriter" id="bib.bib42.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib42.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib42.10.2">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em><span class="ltx_text ltx_font_typewriter" id="bib.bib42.11.3">, pages 14210--14228, Toronto, Canada. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib43.4.4.1">Salinas and Morstatter (2024)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib43.6.1">
Abel Salinas and Fred Morstatter. 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib43.7.1">The butterfly effect of altering prompts: How small changes and jailbreaks affect large language model performance.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib43.8.1">arXiv preprint arXiv: 2401.03729</em><span class="ltx_text ltx_font_typewriter" id="bib.bib43.9.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib44.5.5.1">Sclar et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib44.7.1">
Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib44.8.1">Quantifying language models’ sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib44.9.1">arXiv preprint arXiv: 2310.11324</em><span class="ltx_text ltx_font_typewriter" id="bib.bib44.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib45.4.4.1">Sondos Mahmoud Bsharat (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib45.6.1">
Zhiqiang Shen Sondos Mahmoud Bsharat, Aidar Myrzakhan. 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib45.7.1">Principled instructions are all you need for questioning llama-1/2, gpt-3.5/4.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib45.8.1">arXiv preprint arXiv:2312.16171</em><span class="ltx_text ltx_font_typewriter" id="bib.bib45.9.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib46.5.5.1">Touvron et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib46.7.1">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2307.09288" title="">Llama 2: Open foundation and fine-tuned chat models</a><span class="ltx_text ltx_font_typewriter" id="bib.bib46.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib47.5.5.1">Vilar et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib47.7.1">
David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/2023.acl-long.859" title="">Prompting PaLM for translation: Assessing strategies and performance</a><span class="ltx_text ltx_font_typewriter" id="bib.bib47.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib47.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib47.10.2">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em><span class="ltx_text ltx_font_typewriter" id="bib.bib47.11.3">, pages 15406--15427, Toronto, Canada. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib48.5.5.1">Wei et al. (2022)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib48.7.1">
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, E. Chi, Tatsunori Hashimoto, O. Vinyals, P. Liang, J. Dean, and W. Fedus. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2206.07682" title="">Emergent abilities of large language models</a><span class="ltx_text ltx_font_typewriter" id="bib.bib48.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib48.9.1">Trans. Mach. Learn. Res.</em><span class="ltx_text ltx_font_typewriter" id="bib.bib48.10.2">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib49.5.5.1">Wei et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib49.7.1">
Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2303.03846" title="">Larger language models do in-context learning differently</a><span class="ltx_text ltx_font_typewriter" id="bib.bib49.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib50.5.5.1">Xu et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib50.7.1">
Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2309.11674" title="">A paradigm shift in machine translation: Boosting translation performance of large language models</a><span class="ltx_text ltx_font_typewriter" id="bib.bib50.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib51.5.5.1">Xu et al. (2021)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib51.7.1">
Weiwen Xu, Ai Ti Aw, Yang Ding, Kui Wu, and Shafiq Joty. 2021.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/2021.naacl-industry.11" title="">Addressing the vulnerability of NMT in input perturbations</a><span class="ltx_text ltx_font_typewriter" id="bib.bib51.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib51.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib51.10.2">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers</em><span class="ltx_text ltx_font_typewriter" id="bib.bib51.11.3">, pages 80--88, Online. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib52.5.5.1">Yoo et al. (2022)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib52.7.1">
Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/2022.emnlp-main.155" title="">Ground-truth labels matter: A deeper look into input-label demonstrations</a><span class="ltx_text ltx_font_typewriter" id="bib.bib52.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib52.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib52.10.2">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em><span class="ltx_text ltx_font_typewriter" id="bib.bib52.11.3">, pages 2422--2437, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib53.5.5.1">Zhang et al. (2023a)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib53.7.1">
Biao Zhang, Barry Haddow, and Alexandra Birch. 2023a.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib53.8.1">Prompting large language model for machine translation: A case study.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib53.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib53.10.2">Proceedings of the 40th International Conference on Machine Learning</em><span class="ltx_text ltx_font_typewriter" id="bib.bib53.11.3">, ICML’23. JMLR.org.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib54.4.4.1">Zhang and Toral (2019)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib54.6.1">
Mike Zhang and Antonio Toral. 2019.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/W19-5208" title="">The effect of translationese in machine translation test sets</a><span class="ltx_text ltx_font_typewriter" id="bib.bib54.7.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib54.8.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib54.9.2">Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</em><span class="ltx_text ltx_font_typewriter" id="bib.bib54.10.3">, pages 73--81, Florence, Italy. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib55.5.5.1">Zhang et al. (2023b)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib55.7.1">
Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. 2023b.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="http://arxiv.org/abs/2306.09927" title="">Trained transformers learn linear models in-context</a><span class="ltx_text ltx_font_typewriter" id="bib.bib55.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib56.5.5.1">Zhang et al. (2021)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib56.7.1">
Xinze Zhang, Junzhe Zhang, Zhenhua Chen, and Kun He. 2021.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/2021.acl-long.153" title="">Crafting adversarial examples for neural machine translation</a><span class="ltx_text ltx_font_typewriter" id="bib.bib56.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib56.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib56.10.2">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em><span class="ltx_text ltx_font_typewriter" id="bib.bib56.11.3">, pages 1967--1977, Online. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib57.5.5.1">Zhang et al. (2022)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib57.7.1">
Yunxiang Zhang, Liangming Pan, Samson Tan, and Min-Yen Kan. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://doi.org/10.18653/v1/2022.findings-acl.315" title="">Interpreting the robustness of neural NLP models to textual perturbations</a><span class="ltx_text ltx_font_typewriter" id="bib.bib57.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib57.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib57.10.2">Findings of the Association for Computational Linguistics: ACL 2022</em><span class="ltx_text ltx_font_typewriter" id="bib.bib57.11.3">, pages 3993--4007, Dublin, Ireland. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib58.5.5.1">Zhao et al. (2018)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib58.7.1">
Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2018.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib58.8.1">Generating natural adversarial examples.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib58.9.1">In </span><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib58.10.2">International Conference on Learning Representations (ICLR)</em><span class="ltx_text ltx_font_typewriter" id="bib.bib58.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_typewriter" id="bib.bib59.5.5.1">Zhu et al. (2023)</span></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib59.7.1">
Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_typewriter" id="bib.bib59.8.1">Multilingual machine translation with large language models: Empirical results and analysis.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_typewriter ltx_font_italic" id="bib.bib59.9.1">arXiv preprint arXiv: 2304.04675</em><span class="ltx_text ltx_font_typewriter" id="bib.bib59.10.2">.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_font_typewriter ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix"><span class="ltx_text ltx_font_serif" id="A1.1.1.1">Appendix A</span> </span>Fine-tuning BLOOM 7B on MT</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1"><span class="ltx_text ltx_font_typewriter" id="A1.p1.1.1">We fine-tune BLOOM </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="A1.p1.1.2.1">(</span><span class="ltx_text ltx_font_typewriter">Fan et al.</span><span class="ltx_text ltx_font_typewriter" id="A1.p1.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib14" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a><span class="ltx_text ltx_font_typewriter" id="A1.p1.1.4.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="A1.p1.1.5"> 7B model on a human-annotated subset of BPCC </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="A1.p1.1.6.1">(</span><span class="ltx_text ltx_font_typewriter">Gala et al.</span><span class="ltx_text ltx_font_typewriter" id="A1.p1.1.7.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib17" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="A1.p1.1.8.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="A1.p1.1.9">, namely BPCC-seed and this forms the task-specific fine-tuned baseline for the BLOOM family. </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A1.T1" title="In Appendix A Fine-tuning BLOOM 7B on MT ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text ltx_font_typewriter" id="A1.p1.1.10"> and </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A1.T2" title="In Appendix A Fine-tuning BLOOM 7B on MT ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text ltx_font_typewriter" id="A1.p1.1.11"> list the data scales and hyperparameters used for fine-tuning respectively.</span></p>
</div>
<figure class="ltx_table" id="A1.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T1.1.1.1.1"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.1.1.1">Language</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.T1.1.1.1.2"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.1.1.2.1">Bitext pairs</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T1.1.2.1.1"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.2.1.1.1">Bengali</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T1.1.2.1.2"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.2.1.2.1">50K</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T1.1.3.2.1"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.3.2.1.1">Hindi</span></th>
<td class="ltx_td ltx_align_right" id="A1.T1.1.3.2.2"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.3.2.2.1">50K</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T1.1.4.3.1"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.4.3.1.1">Marathi</span></th>
<td class="ltx_td ltx_align_right" id="A1.T1.1.4.3.2"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.4.3.2.1">50K</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T1.1.5.4.1"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.5.4.1.1">Tamil</span></th>
<td class="ltx_td ltx_align_right" id="A1.T1.1.5.4.2"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.5.4.2.1">30.6K</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.T1.1.6.5.1"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.6.5.1.1">Telugu</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.T1.1.6.5.2"><span class="ltx_text ltx_font_typewriter" id="A1.T1.1.6.5.2.1">36.9K</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_serif" id="A1.T1.8.1.1">Table 1</span>: </span>Statistics of the sample of the English-centric BPCC-Seed data <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_typewriter">Gala et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib17" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a>)</cite> used for fine-tuning the BLOOM 7B model. Note that the data was formatted in both directions (En-XX and XX-En) directions and used for joint fine-tuning.</figcaption>
</figure>
<figure class="ltx_table" id="A1.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T2.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T2.5.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T2.5.6.1.1"><span class="ltx_text ltx_font_typewriter" id="A1.T2.5.6.1.1.1">Hyperparameters</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.T2.5.6.1.2"><span class="ltx_text ltx_font_typewriter" id="A1.T2.5.6.1.2.1">Value</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T2.1.1.2"><span class="ltx_text ltx_font_typewriter" id="A1.T2.1.1.2.1">Batch size</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T2.1.1.1"><math alttext="128" class="ltx_Math" display="inline" id="A1.T2.1.1.1.m1.1"><semantics id="A1.T2.1.1.1.m1.1a"><mn id="A1.T2.1.1.1.m1.1.1" xref="A1.T2.1.1.1.m1.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="A1.T2.1.1.1.m1.1b"><cn id="A1.T2.1.1.1.m1.1.1.cmml" type="integer" xref="A1.T2.1.1.1.m1.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T2.1.1.1.m1.1c">128</annotation><annotation encoding="application/x-llamapun" id="A1.T2.1.1.1.m1.1d">128</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T2.2.2.2"><span class="ltx_text ltx_font_typewriter" id="A1.T2.2.2.2.1">Learning rate</span></th>
<td class="ltx_td ltx_align_right" id="A1.T2.2.2.1"><math alttext="5e-5" class="ltx_Math" display="inline" id="A1.T2.2.2.1.m1.1"><semantics id="A1.T2.2.2.1.m1.1a"><mrow id="A1.T2.2.2.1.m1.1.1" xref="A1.T2.2.2.1.m1.1.1.cmml"><mrow id="A1.T2.2.2.1.m1.1.1.2" xref="A1.T2.2.2.1.m1.1.1.2.cmml"><mn id="A1.T2.2.2.1.m1.1.1.2.2" xref="A1.T2.2.2.1.m1.1.1.2.2.cmml">5</mn><mo id="A1.T2.2.2.1.m1.1.1.2.1" xref="A1.T2.2.2.1.m1.1.1.2.1.cmml">⁢</mo><mi id="A1.T2.2.2.1.m1.1.1.2.3" xref="A1.T2.2.2.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="A1.T2.2.2.1.m1.1.1.1" xref="A1.T2.2.2.1.m1.1.1.1.cmml">−</mo><mn id="A1.T2.2.2.1.m1.1.1.3" xref="A1.T2.2.2.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.T2.2.2.1.m1.1b"><apply id="A1.T2.2.2.1.m1.1.1.cmml" xref="A1.T2.2.2.1.m1.1.1"><minus id="A1.T2.2.2.1.m1.1.1.1.cmml" xref="A1.T2.2.2.1.m1.1.1.1"></minus><apply id="A1.T2.2.2.1.m1.1.1.2.cmml" xref="A1.T2.2.2.1.m1.1.1.2"><times id="A1.T2.2.2.1.m1.1.1.2.1.cmml" xref="A1.T2.2.2.1.m1.1.1.2.1"></times><cn id="A1.T2.2.2.1.m1.1.1.2.2.cmml" type="integer" xref="A1.T2.2.2.1.m1.1.1.2.2">5</cn><ci id="A1.T2.2.2.1.m1.1.1.2.3.cmml" xref="A1.T2.2.2.1.m1.1.1.2.3">𝑒</ci></apply><cn id="A1.T2.2.2.1.m1.1.1.3.cmml" type="integer" xref="A1.T2.2.2.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T2.2.2.1.m1.1c">5e-5</annotation><annotation encoding="application/x-llamapun" id="A1.T2.2.2.1.m1.1d">5 italic_e - 5</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A1.T2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T2.3.3.2"><span class="ltx_text ltx_font_typewriter" id="A1.T2.3.3.2.1">Epochs</span></th>
<td class="ltx_td ltx_align_right" id="A1.T2.3.3.1"><math alttext="4" class="ltx_Math" display="inline" id="A1.T2.3.3.1.m1.1"><semantics id="A1.T2.3.3.1.m1.1a"><mn id="A1.T2.3.3.1.m1.1.1" xref="A1.T2.3.3.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A1.T2.3.3.1.m1.1b"><cn id="A1.T2.3.3.1.m1.1.1.cmml" type="integer" xref="A1.T2.3.3.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T2.3.3.1.m1.1c">4</annotation><annotation encoding="application/x-llamapun" id="A1.T2.3.3.1.m1.1d">4</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A1.T2.5.7.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T2.5.7.1.1"><span class="ltx_text ltx_font_typewriter" id="A1.T2.5.7.1.1.1">Scheduler</span></th>
<td class="ltx_td ltx_align_right" id="A1.T2.5.7.1.2"><span class="ltx_text ltx_font_typewriter" id="A1.T2.5.7.1.2.1">linear</span></td>
</tr>
<tr class="ltx_tr" id="A1.T2.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T2.4.4.2"><span class="ltx_text ltx_font_typewriter" id="A1.T2.4.4.2.1">Warmup ratio</span></th>
<td class="ltx_td ltx_align_right" id="A1.T2.4.4.1"><math alttext="0.03" class="ltx_Math" display="inline" id="A1.T2.4.4.1.m1.1"><semantics id="A1.T2.4.4.1.m1.1a"><mn id="A1.T2.4.4.1.m1.1.1" xref="A1.T2.4.4.1.m1.1.1.cmml">0.03</mn><annotation-xml encoding="MathML-Content" id="A1.T2.4.4.1.m1.1b"><cn id="A1.T2.4.4.1.m1.1.1.cmml" type="float" xref="A1.T2.4.4.1.m1.1.1">0.03</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T2.4.4.1.m1.1c">0.03</annotation><annotation encoding="application/x-llamapun" id="A1.T2.4.4.1.m1.1d">0.03</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A1.T2.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.T2.5.5.2"><span class="ltx_text ltx_font_typewriter" id="A1.T2.5.5.2.1">Weight decay</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.T2.5.5.1"><math alttext="0." class="ltx_Math" display="inline" id="A1.T2.5.5.1.m1.1"><semantics id="A1.T2.5.5.1.m1.1a"><mrow id="A1.T2.5.5.1.m1.1.2.2"><mn id="A1.T2.5.5.1.m1.1.1" xref="A1.T2.5.5.1.m1.1.1.cmml">0</mn><mo id="A1.T2.5.5.1.m1.1.2.2.1" lspace="0em">.</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.T2.5.5.1.m1.1b"><cn id="A1.T2.5.5.1.m1.1.1.cmml" type="integer" xref="A1.T2.5.5.1.m1.1.1">0</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T2.5.5.1.m1.1c">0.</annotation><annotation encoding="application/x-llamapun" id="A1.T2.5.5.1.m1.1d">0 .</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_serif" id="A1.T2.8.1.1">Table 2</span>: </span>Hyperparameters for fine-tuning the BLOOM 7B model on MT task.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_font_typewriter ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix"><span class="ltx_text ltx_font_serif" id="A2.1.1.1">Appendix B</span> </span>Use of model-based metrics</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1"><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.1">Model-based metrics such as COMET </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.2.1">(</span><span class="ltx_text ltx_font_typewriter">Rei et al.</span><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib40" title=""><span class="ltx_text ltx_font_typewriter">2022</span></a><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.4.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.5"> have demonstrated a good correlation with human judgments across several languages </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.6.1">(</span><span class="ltx_text ltx_font_typewriter">Kocmi et al.</span><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.7.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib21" title=""><span class="ltx_text ltx_font_typewriter">2021</span></a><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.8.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.9">. However, these metrics may not be well-calibrated for all the languages considered in this study </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.10.1">(</span><span class="ltx_text ltx_font_typewriter">Gala et al.</span><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.11.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib17" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.12.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.13">. Furthermore, the scale of our experiments makes it impractical to compute model-based metrics like COMET due to the additional computational overhead needed.
Consequently, we use the best string-based metric, ChrF++ </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.14.1">(</span><span class="ltx_text ltx_font_typewriter">Popović</span><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.15.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib35" title=""><span class="ltx_text ltx_font_typewriter">2017</span></a><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.16.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.17"> as outlined in </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.18.1">(</span><span class="ltx_text ltx_font_typewriter">Sai B et al.</span><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.19.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2401.12097v3#bib.bib42" title=""><span class="ltx_text ltx_font_typewriter">2023</span></a><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.20.3">)</span></cite><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.21">. For completeness, we also compute the COMET scores using the wmt22-comet-da</span><span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/Unbabel/wmt22-comet-da" title="">https://huggingface.co/Unbabel/wmt22-comet-da</a></span></span></span><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.22"> model for one of our primary experimental setups (instruction perturbation) and found the trends to be consistent with the ChrF++ metrics. This suggests that ChrF++ can serve as a reliable proxy, indicative of the overall trends. </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A2.F7" title="In Appendix B Use of model-based metrics ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.23"> shows the trends in COMET scores for the instruction perturbation experiment, clearly mirroring the trends in ChrF++ scores presented in </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5.F1" title="In 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text ltx_font_typewriter" id="A2.p1.1.24">.</span></p>
</div>
<figure class="ltx_figure" id="A2.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="926" id="A2.F7.g1" src="x7.png" width="831"/>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_serif" id="A2.F7.3.1.1">Figure 7</span>: </span>Comparison of COMET scores for En-XX (top) and XX-En (bottom) across different instruction types for BLOOM and Llama 2 model families (averaged across different languages and shots).</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_font_typewriter ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix"><span class="ltx_text ltx_font_serif" id="A3.1.1.1">Appendix C</span> </span>Prompt Templates</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1"><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.1">We use the standard prompt template (</span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A3.F8" title="In Appendix C Prompt Templates ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.2">) for most of our experiments except for experiments related to </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5.SS2.SSS3" title="5.2.3 Demonstrations from Allied Tasks ‣ 5.2 Example Perturbations ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Sections</span> <span class="ltx_text ltx_ref_tag">5.2.3</span></a><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.3"> and </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5.SS3" title="5.3 Misalignment ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">5.3</span></a><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.4">. In the case of experiments in </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5.SS2.SSS3" title="5.2.3 Demonstrations from Allied Tasks ‣ 5.2 Example Perturbations ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.2.3</span></a><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.5">, we do not specify the source language in the instruction similar to the prompt illustrated in </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A3.F9" title="In Appendix C Prompt Templates ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9</span></a><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.6">.
For the experiments in </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5.SS3" title="5.3 Misalignment ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.3</span></a><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.7">, we use the prompt template specified in </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A3.F10" title="In Appendix C Prompt Templates ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">10</span></a><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.8">.</span></p>
</div>
<figure class="ltx_figure" id="A3.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="A3.F8.3">{promptbox}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A3.F8.2"><span class="ltx_text ltx_font_typewriter" id="A3.F8.2.1">Translate this from {src_lang} into {tgt_lang}: </span>
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A3.F8.2.2">{src_lang}: {src_text} </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A3.F8.2.3">{tgt_lang}: {tgt_text} </span>
<br class="ltx_break"/>
<br class="ltx_break"/><math alttext="\cdots" class="ltx_Math" display="inline" id="A3.F8.1.m1.1"><semantics id="A3.F8.1.m1.1a"><mi id="A3.F8.1.m1.1.1" mathvariant="normal" xref="A3.F8.1.m1.1.1.cmml">⋯</mi><annotation-xml encoding="MathML-Content" id="A3.F8.1.m1.1b"><ci id="A3.F8.1.m1.1.1.cmml" xref="A3.F8.1.m1.1.1">⋯</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.F8.1.m1.1c">\cdots</annotation><annotation encoding="application/x-llamapun" id="A3.F8.1.m1.1d">⋯</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="A3.F8.2.4"> </span>
<br class="ltx_break"/><math alttext="\cdots" class="ltx_Math" display="inline" id="A3.F8.2.m2.1"><semantics id="A3.F8.2.m2.1a"><mi id="A3.F8.2.m2.1.1" mathvariant="normal" xref="A3.F8.2.m2.1.1.cmml">⋯</mi><annotation-xml encoding="MathML-Content" id="A3.F8.2.m2.1b"><ci id="A3.F8.2.m2.1.1.cmml" xref="A3.F8.2.m2.1.1">⋯</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.F8.2.m2.1c">\cdots</annotation><annotation encoding="application/x-llamapun" id="A3.F8.2.m2.1d">⋯</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="A3.F8.2.5"> </span>
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A3.F8.4"><span class="ltx_text ltx_font_typewriter" id="A3.F8.4.1">{src_lang}: {src_text} </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A3.F8.4.2">{tgt_lang}:</span></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_serif" id="A3.F8.7.1.1">Figure 8</span>: </span>Standard prompt for translation</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="A3.F9.3">{promptbox}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A3.F9.4"><span class="ltx_text ltx_font_typewriter" id="A3.F9.4.1">Translate this into {tgt_lang}: </span>
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A3.F9.4.2">{aux_lang}: {aux_text} </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A3.F9.4.3">{tgt_lang}: {tgt_text} </span>
<br class="ltx_break"/>
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A3.F9.2"><math alttext="\cdots" class="ltx_Math" display="inline" id="A3.F9.1.m1.1"><semantics id="A3.F9.1.m1.1a"><mi id="A3.F9.1.m1.1.1" mathvariant="normal" xref="A3.F9.1.m1.1.1.cmml">⋯</mi><annotation-xml encoding="MathML-Content" id="A3.F9.1.m1.1b"><ci id="A3.F9.1.m1.1.1.cmml" xref="A3.F9.1.m1.1.1">⋯</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.F9.1.m1.1c">\cdots</annotation><annotation encoding="application/x-llamapun" id="A3.F9.1.m1.1d">⋯</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="A3.F9.2.1"> </span>
<br class="ltx_break"/><math alttext="\cdots" class="ltx_Math" display="inline" id="A3.F9.2.m2.1"><semantics id="A3.F9.2.m2.1a"><mi id="A3.F9.2.m2.1.1" mathvariant="normal" xref="A3.F9.2.m2.1.1.cmml">⋯</mi><annotation-xml encoding="MathML-Content" id="A3.F9.2.m2.1b"><ci id="A3.F9.2.m2.1.1.cmml" xref="A3.F9.2.m2.1.1">⋯</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.F9.2.m2.1c">\cdots</annotation><annotation encoding="application/x-llamapun" id="A3.F9.2.m2.1d">⋯</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="A3.F9.2.2"> </span>
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A3.F9.5"><span class="ltx_text ltx_font_typewriter" id="A3.F9.5.1">{src_lang}: {src_text} </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A3.F9.5.2">{tgt_lang}:</span></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_serif" id="A3.F9.8.1.1">Figure 9</span>: </span>Prompt for Allied Task Setup</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="A3.F10.1">{promptbox}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A3.F10.2"><span class="ltx_text ltx_font_typewriter" id="A3.F10.2.1">Translate this from {src_lang} into {pivot_lang}: </span>
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A3.F10.2.2">{src_lang}: {src_text} </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A3.F10.2.3">{pivot_lang}: {pivot_text} </span>
<br class="ltx_break"/>
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A3.F10.3"><span class="ltx_text ltx_font_typewriter" id="A3.F10.3.1">Translate this from {pivot_lang} into {tgt_lang}: </span>
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A3.F10.3.2">{pivot_lang}: {pivot_text} </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A3.F10.3.3">{tgt_lang}: {tgt_text} </span>
<br class="ltx_break"/>
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A3.F10.4"><span class="ltx_text ltx_font_typewriter" id="A3.F10.4.1">Translate this from {src_lang} into {tgt_lang}: </span>
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A3.F10.4.2">{src_lang}: {src_text} </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A3.F10.4.3">{tgt_lang}:</span></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_serif" id="A3.F10.7.1.1">Figure 10</span>: </span>Misalignment prompt for translation</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_font_typewriter ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix"><span class="ltx_text ltx_font_serif" id="A4.1.1.1">Appendix D</span> </span>Instruction Templates</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1"><span class="ltx_text ltx_font_typewriter" id="A4.p1.1.1">We outline the different types of instructions considered in the instruction perturbation experiments mentioned in </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5.SS1" title="5.1 Sensitivity to Instruction Variations ‣ 5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">5.1</span></a><span class="ltx_text ltx_font_typewriter" id="A4.p1.1.2">. For each type of instruction, an example is provided in </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A3" title="Appendix C Prompt Templates ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">C</span></a><span class="ltx_text ltx_font_typewriter" id="A4.p1.1.3">. Additionally, for the random instruction type, any one of the prefix instructions is selected at random.</span></p>
</div>
<figure class="ltx_figure" id="A4.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="A4.F11.1">{promptbox}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A4.F11.2"><span class="ltx_text ltx_font_typewriter" id="A4.F11.2.1"># Standard </span>
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A4.F11.2.2" style="color:#4D4DFF;"> Translate this from {src_lang} into {tgt_lang}:</span><span class="ltx_text ltx_font_typewriter" id="A4.F11.2.3"> </span>
<br class="ltx_break"/>
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A4.F11.3"><span class="ltx_text ltx_font_typewriter" id="A4.F11.3.1"># Generic </span>
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A4.F11.3.2" style="color:#FF4D4D;"> Perform the task based on the examples provided:</span><span class="ltx_text ltx_font_typewriter" id="A4.F11.3.3"> </span>
<br class="ltx_break"/>
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A4.F11.4"><span class="ltx_text ltx_font_typewriter" id="A4.F11.4.1"># Random </span>
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A4.F11.4.2" style="color:#D24D79;"> Complete the description with an appropriate ending: 
<br class="ltx_break"/>
<br class="ltx_break"/>I am hesitating between two options. Help me choose the more likely cause or effect. 
<br class="ltx_break"/>
<br class="ltx_break"/>Generate a headline for the following article(s) as accurately as possible. 
<br class="ltx_break"/>
<br class="ltx_break"/>Predict the sentiment of the review. The possible choices for the sentiment are: ’positive’ and ’negative’. 
<br class="ltx_break"/>
<br class="ltx_break"/>Answer whether the hypothesis is more likely to be true (entailment), false (contradiction), or unknown (neutral) based on the given premise. 
<br class="ltx_break"/>
<br class="ltx_break"/>The following are multiple choice questions (with answers) about subjects. 
<br class="ltx_break"/></span><span class="ltx_text ltx_font_typewriter" id="A4.F11.4.3"> </span>
<br class="ltx_break"/>
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A4.F11.5"><span class="ltx_text ltx_font_typewriter" id="A4.F11.5.1"># Contrastive </span>
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A4.F11.5.2" style="color:#D2A679;"> Translate this from {tgt_lang} into {src_lang}:</span><span class="ltx_text ltx_font_typewriter" id="A4.F11.5.3"> </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A4.F11.5.4"></span></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_serif" id="A4.F11.8.1.1">Figure 11</span>: </span>Different types of instructions</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_font_typewriter ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix"><span class="ltx_text ltx_font_serif" id="A5.1.1.1">Appendix E</span> </span>Examples of Perturbation Variants</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1"><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A5.T3" title="In Appendix E Examples of Perturbation Variants ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text ltx_font_typewriter" id="A5.p1.1.1"> categorizes the different perturbations used in this study based on attributes they impact, along with an example.</span></p>
</div>
<figure class="ltx_table" id="A5.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A5.T3.20">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A5.T3.20.21.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A5.T3.20.21.1.1"><span class="ltx_text ltx_font_typewriter" id="A5.T3.20.21.1.1.1">Perturbation Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T3.20.21.1.2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.20.21.1.2.1">Lexical</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T3.20.21.1.3"><span class="ltx_text ltx_font_typewriter" id="A5.T3.20.21.1.3.1">Syntactic</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T3.20.21.1.4"><span class="ltx_text ltx_font_typewriter" id="A5.T3.20.21.1.4.1">Semantic</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A5.T3.20.21.1.5"><span class="ltx_text ltx_font_typewriter" id="A5.T3.20.21.1.5.1">Example</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T3.20.22.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T3.20.22.1.1" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.20.22.1.1.1">Clean</span></td>
<td class="ltx_td ltx_border_t" id="A5.T3.20.22.1.2"></td>
<td class="ltx_td ltx_border_t" id="A5.T3.20.22.1.3"></td>
<td class="ltx_td ltx_border_t" id="A5.T3.20.22.1.4"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T3.20.22.1.5"><span class="ltx_text ltx_font_typewriter" id="A5.T3.20.22.1.5.1">Wow! That place is so wonderful, and I would</span></td>
</tr>
<tr class="ltx_tr" id="A5.T3.20.23.2">
<td class="ltx_td" id="A5.T3.20.23.2.1"></td>
<td class="ltx_td" id="A5.T3.20.23.2.2"></td>
<td class="ltx_td" id="A5.T3.20.23.2.3"></td>
<td class="ltx_td ltx_align_left" id="A5.T3.20.23.2.4"><span class="ltx_text ltx_font_typewriter" id="A5.T3.20.23.2.4.1">love to go there again.</span></td>
</tr>
<tr class="ltx_tr" id="A5.T3.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T3.3.3.4" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.3.3.4.1">Span Noise</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.1.1" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.1.1.1.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="A5.T3.1.1.1.1.m1.1"><semantics id="A5.T3.1.1.1.1.m1.1a"><mi id="A5.T3.1.1.1.1.m1.1.1" mathvariant="normal" xref="A5.T3.1.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A5.T3.1.1.1.1.m1.1b"><ci id="A5.T3.1.1.1.1.m1.1.1.cmml" xref="A5.T3.1.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.1.1.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="A5.T3.1.1.1.1.m1.1d">✓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.2.2.2" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.2.2.2.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="A5.T3.2.2.2.1.m1.1"><semantics id="A5.T3.2.2.2.1.m1.1a"><mi id="A5.T3.2.2.2.1.m1.1.1" mathvariant="normal" xref="A5.T3.2.2.2.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A5.T3.2.2.2.1.m1.1b"><ci id="A5.T3.2.2.2.1.m1.1.1.cmml" xref="A5.T3.2.2.2.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.2.2.2.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="A5.T3.2.2.2.1.m1.1d">✓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.3.3.3" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.3.3.3.1"><math alttext="\times" class="ltx_Math" display="inline" id="A5.T3.3.3.3.1.m1.1"><semantics id="A5.T3.3.3.3.1.m1.1a"><mo id="A5.T3.3.3.3.1.m1.1.1" xref="A5.T3.3.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A5.T3.3.3.3.1.m1.1b"><times id="A5.T3.3.3.3.1.m1.1.1.cmml" xref="A5.T3.3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.3.3.3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A5.T3.3.3.3.1.m1.1d">×</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T3.3.3.5"><span class="ltx_text ltx_font_typewriter" id="A5.T3.3.3.5.1">Wow! That place is po wonde9l, a I uld</span></td>
</tr>
<tr class="ltx_tr" id="A5.T3.20.24.3">
<td class="ltx_td ltx_align_left" id="A5.T3.20.24.3.1"><span class="ltx_text ltx_font_typewriter" id="A5.T3.20.24.3.1.1">love to go thyre again.</span></td>
</tr>
<tr class="ltx_tr" id="A5.T3.6.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T3.6.6.4" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.6.6.4.1">OCR</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.4.4.1" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.4.4.1.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="A5.T3.4.4.1.1.m1.1"><semantics id="A5.T3.4.4.1.1.m1.1a"><mi id="A5.T3.4.4.1.1.m1.1.1" mathvariant="normal" xref="A5.T3.4.4.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A5.T3.4.4.1.1.m1.1b"><ci id="A5.T3.4.4.1.1.m1.1.1.cmml" xref="A5.T3.4.4.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.4.4.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="A5.T3.4.4.1.1.m1.1d">✓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.5.5.2" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.5.5.2.1"><math alttext="\times" class="ltx_Math" display="inline" id="A5.T3.5.5.2.1.m1.1"><semantics id="A5.T3.5.5.2.1.m1.1a"><mo id="A5.T3.5.5.2.1.m1.1.1" xref="A5.T3.5.5.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A5.T3.5.5.2.1.m1.1b"><times id="A5.T3.5.5.2.1.m1.1.1.cmml" xref="A5.T3.5.5.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.5.5.2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A5.T3.5.5.2.1.m1.1d">×</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.6.6.3" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.6.6.3.1"><math alttext="\times" class="ltx_Math" display="inline" id="A5.T3.6.6.3.1.m1.1"><semantics id="A5.T3.6.6.3.1.m1.1a"><mo id="A5.T3.6.6.3.1.m1.1.1" xref="A5.T3.6.6.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A5.T3.6.6.3.1.m1.1b"><times id="A5.T3.6.6.3.1.m1.1.1.cmml" xref="A5.T3.6.6.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.6.6.3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A5.T3.6.6.3.1.m1.1d">×</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T3.6.6.5"><span class="ltx_text ltx_font_typewriter" id="A5.T3.6.6.5.1">Wow!That place isso wonderful, and I would</span></td>
</tr>
<tr class="ltx_tr" id="A5.T3.20.25.4">
<td class="ltx_td ltx_align_left" id="A5.T3.20.25.4.1"><span class="ltx_text ltx_font_typewriter" id="A5.T3.20.25.4.1.1">lo ve to go there again .</span></td>
</tr>
<tr class="ltx_tr" id="A5.T3.9.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T3.9.9.4" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.9.9.4.1">Word Ordering</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.7.7.1" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.7.7.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="A5.T3.7.7.1.1.m1.1"><semantics id="A5.T3.7.7.1.1.m1.1a"><mo id="A5.T3.7.7.1.1.m1.1.1" xref="A5.T3.7.7.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A5.T3.7.7.1.1.m1.1b"><times id="A5.T3.7.7.1.1.m1.1.1.cmml" xref="A5.T3.7.7.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.7.7.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A5.T3.7.7.1.1.m1.1d">×</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.8.8.2" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.8.8.2.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="A5.T3.8.8.2.1.m1.1"><semantics id="A5.T3.8.8.2.1.m1.1a"><mi id="A5.T3.8.8.2.1.m1.1.1" mathvariant="normal" xref="A5.T3.8.8.2.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A5.T3.8.8.2.1.m1.1b"><ci id="A5.T3.8.8.2.1.m1.1.1.cmml" xref="A5.T3.8.8.2.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.8.8.2.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="A5.T3.8.8.2.1.m1.1d">✓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.9.9.3" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.9.9.3.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="A5.T3.9.9.3.1.m1.1"><semantics id="A5.T3.9.9.3.1.m1.1a"><mi id="A5.T3.9.9.3.1.m1.1.1" mathvariant="normal" xref="A5.T3.9.9.3.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A5.T3.9.9.3.1.m1.1b"><ci id="A5.T3.9.9.3.1.m1.1.1.cmml" xref="A5.T3.9.9.3.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.9.9.3.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="A5.T3.9.9.3.1.m1.1d">✓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T3.9.9.5"><span class="ltx_text ltx_font_typewriter" id="A5.T3.9.9.5.1">Wow! and place is so to would I That wonderful,</span></td>
</tr>
<tr class="ltx_tr" id="A5.T3.20.26.5">
<td class="ltx_td ltx_align_left" id="A5.T3.20.26.5.1"><span class="ltx_text ltx_font_typewriter" id="A5.T3.20.26.5.1.1">love go there again.</span></td>
</tr>
<tr class="ltx_tr" id="A5.T3.12.12">
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T3.12.12.4" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.12.12.4.1">Word Duplication</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.10.10.1" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.10.10.1.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="A5.T3.10.10.1.1.m1.1"><semantics id="A5.T3.10.10.1.1.m1.1a"><mi id="A5.T3.10.10.1.1.m1.1.1" mathvariant="normal" xref="A5.T3.10.10.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A5.T3.10.10.1.1.m1.1b"><ci id="A5.T3.10.10.1.1.m1.1.1.cmml" xref="A5.T3.10.10.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.10.10.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="A5.T3.10.10.1.1.m1.1d">✓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.11.11.2" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.11.11.2.1"><math alttext="\times" class="ltx_Math" display="inline" id="A5.T3.11.11.2.1.m1.1"><semantics id="A5.T3.11.11.2.1.m1.1a"><mo id="A5.T3.11.11.2.1.m1.1.1" xref="A5.T3.11.11.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A5.T3.11.11.2.1.m1.1b"><times id="A5.T3.11.11.2.1.m1.1.1.cmml" xref="A5.T3.11.11.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.11.11.2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A5.T3.11.11.2.1.m1.1d">×</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.12.12.3" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.12.12.3.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="A5.T3.12.12.3.1.m1.1"><semantics id="A5.T3.12.12.3.1.m1.1a"><mi id="A5.T3.12.12.3.1.m1.1.1" mathvariant="normal" xref="A5.T3.12.12.3.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A5.T3.12.12.3.1.m1.1b"><ci id="A5.T3.12.12.3.1.m1.1.1.cmml" xref="A5.T3.12.12.3.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.12.12.3.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="A5.T3.12.12.3.1.m1.1d">✓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T3.12.12.5"><span class="ltx_text ltx_font_typewriter" id="A5.T3.12.12.5.1">Wow! That place is is so so wonderful, and I I would</span></td>
</tr>
<tr class="ltx_tr" id="A5.T3.20.27.6">
<td class="ltx_td ltx_align_left" id="A5.T3.20.27.6.1"><span class="ltx_text ltx_font_typewriter" id="A5.T3.20.27.6.1.1">love to go there again. again.</span></td>
</tr>
<tr class="ltx_tr" id="A5.T3.16.16">
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T3.13.13.1" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.13.13.1.1">Punctuation<sub class="ltx_sub" id="A5.T3.13.13.1.1.1"><span class="ltx_text ltx_font_serif ltx_font_italic" id="A5.T3.13.13.1.1.1.1">add</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.14.14.2" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.14.14.2.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="A5.T3.14.14.2.1.m1.1"><semantics id="A5.T3.14.14.2.1.m1.1a"><mi id="A5.T3.14.14.2.1.m1.1.1" mathvariant="normal" xref="A5.T3.14.14.2.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A5.T3.14.14.2.1.m1.1b"><ci id="A5.T3.14.14.2.1.m1.1.1.cmml" xref="A5.T3.14.14.2.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.14.14.2.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="A5.T3.14.14.2.1.m1.1d">✓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.15.15.3" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.15.15.3.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="A5.T3.15.15.3.1.m1.1"><semantics id="A5.T3.15.15.3.1.m1.1a"><mi id="A5.T3.15.15.3.1.m1.1.1" mathvariant="normal" xref="A5.T3.15.15.3.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A5.T3.15.15.3.1.m1.1b"><ci id="A5.T3.15.15.3.1.m1.1.1.cmml" xref="A5.T3.15.15.3.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.15.15.3.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="A5.T3.15.15.3.1.m1.1d">✓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.16.16.4" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.16.16.4.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="A5.T3.16.16.4.1.m1.1"><semantics id="A5.T3.16.16.4.1.m1.1a"><mi id="A5.T3.16.16.4.1.m1.1.1" mathvariant="normal" xref="A5.T3.16.16.4.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A5.T3.16.16.4.1.m1.1b"><ci id="A5.T3.16.16.4.1.m1.1.1.cmml" xref="A5.T3.16.16.4.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.16.16.4.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="A5.T3.16.16.4.1.m1.1d">✓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T3.16.16.5"><span class="ltx_text ltx_font_typewriter" id="A5.T3.16.16.5.1">Wow! That% place is so wonderful, and I would"</span></td>
</tr>
<tr class="ltx_tr" id="A5.T3.20.28.7">
<td class="ltx_td ltx_align_left" id="A5.T3.20.28.7.1"><span class="ltx_text ltx_font_typewriter" id="A5.T3.20.28.7.1.1">love. to go there again.</span></td>
</tr>
<tr class="ltx_tr" id="A5.T3.20.20">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A5.T3.17.17.1" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.17.17.1.1">Punctuation<sub class="ltx_sub" id="A5.T3.17.17.1.1.1"><span class="ltx_text ltx_font_serif ltx_font_italic" id="A5.T3.17.17.1.1.1.1">drop</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A5.T3.18.18.2" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.18.18.2.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="A5.T3.18.18.2.1.m1.1"><semantics id="A5.T3.18.18.2.1.m1.1a"><mi id="A5.T3.18.18.2.1.m1.1.1" mathvariant="normal" xref="A5.T3.18.18.2.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A5.T3.18.18.2.1.m1.1b"><ci id="A5.T3.18.18.2.1.m1.1.1.cmml" xref="A5.T3.18.18.2.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.18.18.2.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="A5.T3.18.18.2.1.m1.1d">✓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A5.T3.19.19.3" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.19.19.3.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="A5.T3.19.19.3.1.m1.1"><semantics id="A5.T3.19.19.3.1.m1.1a"><mi id="A5.T3.19.19.3.1.m1.1.1" mathvariant="normal" xref="A5.T3.19.19.3.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A5.T3.19.19.3.1.m1.1b"><ci id="A5.T3.19.19.3.1.m1.1.1.cmml" xref="A5.T3.19.19.3.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.19.19.3.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="A5.T3.19.19.3.1.m1.1d">✓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A5.T3.20.20.4" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A5.T3.20.20.4.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="A5.T3.20.20.4.1.m1.1"><semantics id="A5.T3.20.20.4.1.m1.1a"><mi id="A5.T3.20.20.4.1.m1.1.1" mathvariant="normal" xref="A5.T3.20.20.4.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="A5.T3.20.20.4.1.m1.1b"><ci id="A5.T3.20.20.4.1.m1.1.1.cmml" xref="A5.T3.20.20.4.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.T3.20.20.4.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="A5.T3.20.20.4.1.m1.1d">✓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T3.20.20.5"><span class="ltx_text ltx_font_typewriter" id="A5.T3.20.20.5.1">Wow That place is so wonderful, and I would</span></td>
</tr>
<tr class="ltx_tr" id="A5.T3.20.29.8">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A5.T3.20.29.8.1"><span class="ltx_text ltx_font_typewriter" id="A5.T3.20.29.8.1.1">love to go there again</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_serif" id="A5.T3.23.1.1">Table 3</span>: </span>Categorization of different perturbation methods for the different attributes.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_font_typewriter ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix"><span class="ltx_text ltx_font_serif" id="A6.1.1.1">Appendix F</span> </span>Languages and Directions Considered</h2>
<div class="ltx_para" id="A6.p1">
<p class="ltx_p" id="A6.p1.1"><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A6.T4" title="In Appendix F Languages and Directions Considered ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text ltx_font_typewriter" id="A6.p1.1.1"> describes the languages considered and respective benchmarks used for each experiment conducted as a part of this study.</span></p>
</div>
<figure class="ltx_table" id="A6.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A6.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A6.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A6.T4.1.1.1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A6.T4.1.1.1.1.1">Experiment</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A6.T4.1.1.1.2"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A6.T4.1.1.1.2.1">Models</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A6.T4.1.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="A6.T4.1.1.1.3.1">
<tr class="ltx_tr" id="A6.T4.1.1.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.1.1.3.1.1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A6.T4.1.1.1.3.1.1.1.1">Translation</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.1.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.1.1.3.1.2.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A6.T4.1.1.1.3.1.2.1.1">direction</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A6.T4.1.1.1.4"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A6.T4.1.1.1.4.1">Test set</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A6.T4.1.1.1.5"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A6.T4.1.1.1.5.1">In-context set</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A6.T4.1.1.1.6"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A6.T4.1.1.1.6.1">Languages</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A6.T4.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A6.T4.1.2.1.1" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.2.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="A6.T4.1.2.1.1.1.1">
<span class="ltx_tr" id="A6.T4.1.2.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.2.1.1.1.1.1.1">Instruction</span></span>
<span class="ltx_tr" id="A6.T4.1.2.1.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.2.1.1.1.1.2.1">variation</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T4.1.2.1.2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.2.1.2.1">Llama 2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T4.1.2.1.3" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.2.1.3.1">En-X</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T4.1.2.1.4"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.2.1.4.1">Flores200</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T4.1.2.1.5"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.2.1.5.1">Flores200</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A6.T4.1.2.1.6"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.2.1.6.1">ces_Latn, deu_Latn, rus_Cyrl</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.3.2">
<td class="ltx_td ltx_align_center" id="A6.T4.1.3.2.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.3.2.1.1">BLOOM</span></td>
<td class="ltx_td ltx_align_center" id="A6.T4.1.3.2.2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.3.2.2.1">IN22-Gen</span></td>
<td class="ltx_td ltx_align_center" id="A6.T4.1.3.2.3"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.3.2.3.1">Flores200</span></td>
<td class="ltx_td ltx_align_left" id="A6.T4.1.3.2.4"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.3.2.4.1">ben_Beng, hin_Deva, tam_Taml</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="A6.T4.1.4.3.1" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.4.3.1.1">
<span class="ltx_tabular ltx_align_middle" id="A6.T4.1.4.3.1.1.1">
<span class="ltx_tr" id="A6.T4.1.4.3.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.4.3.1.1.1.1.1">Demonstration</span></span>
<span class="ltx_tr" id="A6.T4.1.4.3.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.4.3.1.1.1.2.1">perturbation</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T4.1.4.3.2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.4.3.2.1">Llama 2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T4.1.4.3.3" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.4.3.3.1">En-X</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T4.1.4.3.4"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.4.3.4.1">Flores200</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T4.1.4.3.5"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.4.3.5.1">Flores200</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A6.T4.1.4.3.6"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.4.3.6.1">ces_Latn, deu_Latn, rus_Cyrl</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.5.4">
<td class="ltx_td ltx_align_center" id="A6.T4.1.5.4.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.5.4.1.1">BLOOM</span></td>
<td class="ltx_td ltx_align_center" id="A6.T4.1.5.4.2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.5.4.2.1">IN22-Gen</span></td>
<td class="ltx_td ltx_align_center" id="A6.T4.1.5.4.3"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.5.4.3.1">Flores200</span></td>
<td class="ltx_td ltx_align_left" id="A6.T4.1.5.4.4"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.5.4.4.1">ben_Beng, hin_Deva, tam_Taml</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="A6.T4.1.6.5.1" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.6.5.1.1">Directionality</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T4.1.6.5.2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.6.5.2.1">BLOOM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T4.1.6.5.3" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.6.5.3.1">En-X</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T4.1.6.5.4"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.6.5.4.1">IN22-Gen</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T4.1.6.5.5">
<table class="ltx_tabular ltx_align_middle" id="A6.T4.1.6.5.5.1">
<tr class="ltx_tr" id="A6.T4.1.6.5.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.6.5.5.1.1.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.6.5.5.1.1.1.1">Flores200</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.6.5.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.6.5.5.1.2.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.6.5.5.1.2.1.1">IndicOG set</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A6.T4.1.6.5.6"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.6.5.6.1">ben_Beng, guj_Gujr, hin_Deva, tel_Telu</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.7.6">
<td class="ltx_td ltx_align_center" id="A6.T4.1.7.6.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.7.6.1.1">BLOOM</span></td>
<td class="ltx_td ltx_align_center" id="A6.T4.1.7.6.2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.7.6.2.1">IN22-Gen</span></td>
<td class="ltx_td ltx_align_center" id="A6.T4.1.7.6.3">
<table class="ltx_tabular ltx_align_middle" id="A6.T4.1.7.6.3.1">
<tr class="ltx_tr" id="A6.T4.1.7.6.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.7.6.3.1.1.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.7.6.3.1.1.1.1">Flores200</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.7.6.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.7.6.3.1.2.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.7.6.3.1.2.1.1">IndicOG set</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left" id="A6.T4.1.7.6.4"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.7.6.4.1">ben_Beng, guj_Gujr, hin_Deva, tel_Telu</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="A6.T4.1.8.7.1" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.8.7.1.1">
<span class="ltx_tabular ltx_align_middle" id="A6.T4.1.8.7.1.1.1">
<span class="ltx_tr" id="A6.T4.1.8.7.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.8.7.1.1.1.1.1">Demonstrations from</span></span>
<span class="ltx_tr" id="A6.T4.1.8.7.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.8.7.1.1.1.2.1">allied task as proxy</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T4.1.8.7.2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.8.7.2.1">Llama 2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T4.1.8.7.3" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.8.7.3.1">
<span class="ltx_tabular ltx_align_middle" id="A6.T4.1.8.7.3.1.1">
<span class="ltx_tr" id="A6.T4.1.8.7.3.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.8.7.3.1.1.1.1">En-X</span></span>
<span class="ltx_tr" id="A6.T4.1.8.7.3.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.8.7.3.1.1.2.1">X-Y</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T4.1.8.7.4"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.8.7.4.1">Flores200</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T4.1.8.7.5"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.8.7.5.1">Flores200</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A6.T4.1.8.7.6">
<table class="ltx_tabular ltx_align_middle" id="A6.T4.1.8.7.6.1">
<tr class="ltx_tr" id="A6.T4.1.8.7.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.8.7.6.1.1.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.8.7.6.1.1.1.1">ces_Latn - rus_Cyrl</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.8.7.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.8.7.6.1.2.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.8.7.6.1.2.1.1">(deu_Latn, eng_Latn, hin_Deva)</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.8.7.6.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.8.7.6.1.3.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.8.7.6.1.3.1.1">deu_Latn - rus_Cyrl</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.8.7.6.1.4">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.8.7.6.1.4.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.8.7.6.1.4.1.1">(ces_Latn, eng_Latn, hin_Deva)</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.8.7.6.1.5">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.8.7.6.1.5.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.8.7.6.1.5.1.1">srp_Cyrl - deu_Latn</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.8.7.6.1.6">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.8.7.6.1.6.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.8.7.6.1.6.1.1">(rus_Cyrl, eng_Latn, hin_Deva)</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.8.7.6.1.7">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.8.7.6.1.7.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.8.7.6.1.7.1.1">srp_Cyrl - ces_Latn</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.8.7.6.1.8">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.8.7.6.1.8.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.8.7.6.1.8.1.1">(rus_Cyrl, eng_Latn, hin_Deva)</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.9.8">
<td class="ltx_td ltx_align_center" id="A6.T4.1.9.8.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.9.8.1.1">BLOOM</span></td>
<td class="ltx_td ltx_align_center" id="A6.T4.1.9.8.2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.9.8.2.1">IN22-Gen</span></td>
<td class="ltx_td ltx_align_center" id="A6.T4.1.9.8.3"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.9.8.3.1">Flores200</span></td>
<td class="ltx_td ltx_align_left" id="A6.T4.1.9.8.4">
<table class="ltx_tabular ltx_align_middle" id="A6.T4.1.9.8.4.1">
<tr class="ltx_tr" id="A6.T4.1.9.8.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.9.8.4.1.1.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.9.8.4.1.1.1.1">mar_Deva - tam_Taml</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.9.8.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.9.8.4.1.2.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.9.8.4.1.2.1.1">(hin_Deva, eng_Latn, ben_Beng)</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.9.8.4.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.9.8.4.1.3.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.9.8.4.1.3.1.1">asm_Beng - hin_Deva</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.9.8.4.1.4">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A6.T4.1.9.8.4.1.4.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.9.8.4.1.4.1.1">(ben_Beng, eng_Latn, tam_Taml)</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.10.9">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A6.T4.1.10.9.1" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.10.9.1.1">Transitivity</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T4.1.10.9.2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.10.9.2.1">Llama 2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A6.T4.1.10.9.3" rowspan="2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.10.9.3.1">X-Y</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T4.1.10.9.4"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.10.9.4.1">Flores200</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T4.1.10.9.5"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.10.9.5.1">Flores200</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A6.T4.1.10.9.6"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.10.9.6.1">ces_Latn, deu_Latn, rus_Cyrl</span></td>
</tr>
<tr class="ltx_tr" id="A6.T4.1.11.10">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A6.T4.1.11.10.1"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.11.10.1.1">BLOOM</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A6.T4.1.11.10.2"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.11.10.2.1">IN22-Gen</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A6.T4.1.11.10.3"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.11.10.3.1">Flores200</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A6.T4.1.11.10.4"><span class="ltx_text ltx_font_typewriter" id="A6.T4.1.11.10.4.1">ben_Beng, hin_Deva, tam_Taml</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_serif" id="A6.T4.4.1.1">Table 4</span>: </span>Details about the models, test sets, in-context sets and languages considered for different experiments. LLama2 family indicates Llama2-7B, Llama2-Chat-7B, ALMA while BLOOM family indicates BLOOM-7B, BLOOMZ-7B and a task-specific fine-tuned BLOOM model on MT. En-X in the translation direction indicates English-centric evaluation, while X-Y indicates non-English-centric evaluation. FLORES200 in the test set column indicates the FLORES200 devtest set, while in the in-context set column indicates FLORES200 dev set.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_font_typewriter ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix"><span class="ltx_text ltx_font_serif" id="A7.1.1.1">Appendix G</span> </span>Additional results</h2>
<div class="ltx_para" id="A7.p1">
<p class="ltx_p" id="A7.p1.1"><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A7.F12" title="In Appendix G Additional results ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Figures</span> <span class="ltx_text ltx_ref_tag">12</span></a><span class="ltx_text ltx_font_typewriter" id="A7.p1.1.1">, </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A7.F13" title="Figure 13 ‣ Appendix G Additional results ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">13</span></a><span class="ltx_text ltx_font_typewriter" id="A7.p1.1.2">, </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A7.F14" title="Figure 14 ‣ Appendix G Additional results ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">14</span></a><span class="ltx_text ltx_font_typewriter" id="A7.p1.1.3">, </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A7.F15" title="Figure 15 ‣ Appendix G Additional results ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">15</span></a><span class="ltx_text ltx_font_typewriter" id="A7.p1.1.4">, </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A7.F16" title="Figure 16 ‣ Appendix G Additional results ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">16</span></a><span class="ltx_text ltx_font_typewriter" id="A7.p1.1.5">, </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A7.F17" title="Figure 17 ‣ Appendix G Additional results ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">17</span></a><span class="ltx_text ltx_font_typewriter" id="A7.p1.1.6"> and </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#A7.F18" title="Figure 18 ‣ Appendix G Additional results ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">18</span></a><span class="ltx_text ltx_font_typewriter" id="A7.p1.1.7"> illustrate fine-grained details such as shot-wise and case-wise trends for the aspects outlined in </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S3" title="3 Methodology ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text ltx_font_typewriter" id="A7.p1.1.8">. The overall trends have been described in </span><a class="ltx_ref ltx_font_typewriter" href="https://arxiv.org/html/2401.12097v3#S5" title="5 Results and Analysis ‣ An Empirical Study of In-context Learning in LLMs for Machine Translation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5</span></a><span class="ltx_text ltx_font_typewriter" id="A7.p1.1.9">.</span></p>
</div>
<figure class="ltx_figure" id="A7.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="590" id="A7.F12.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_serif" id="A7.F12.3.1.1">Figure 12</span>: </span>Shot-wise comparison of ChrF++ scores for En-XX (left) and XX-En (right) across different instruction types for BLOOM and Llama 2 model families (averaged across different languages).</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="A7.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="487" id="A7.F13.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_serif" id="A7.F13.3.1.1">Figure 13</span>: </span>Mean percent change in ChrF++ score across heterogeneous case I relative to the 0 noise baseline for each model across both translation directions (En-XX and XX-En) and both perturbation directions. Scores are averaged across attack types, shots, and noise percentages. Positive values indicate the performance decreased post perturbation while the negative values indicate that performance increases post perturbation. Note: In certain cases, scores are bounded within minimum and maximum values for clarity in depicting overarching trends.</figcaption>
</figure>
<figure class="ltx_figure" id="A7.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="487" id="A7.F14.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_serif" id="A7.F14.3.1.1">Figure 14</span>: </span>Mean percent change in ChrF++ score across heterogeneous case II relative to the 0 noise baseline for each model across both translation directions (En-XX and XX-En) and both perturbation directions. Scores are averaged across attack types, shots, and noise percentages. Positive values indicate the performance decreased post perturbation while the negative values indicate that performance increases post perturbation. Note: In certain cases, scores are bounded within minimum and maximum values for clarity in depicting overarching trends.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="A7.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="487" id="A7.F15.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_serif" id="A7.F15.3.1.1">Figure 15</span>: </span>Mean percent change in ChrF++ score across heterogeneous case III relative to the 0 noise baseline for each model across both translation directions (En-XX and XX-En) and both perturbation directions. Scores are averaged across attack types, shots, and noise percentages. Positive values indicate the performance decreased post perturbation while the negative values indicate that performance increases post perturbation. Note: In certain cases, scores are bounded within minimum and maximum values for clarity in depicting overarching trends.</figcaption>
</figure>
<figure class="ltx_figure" id="A7.F16"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="487" id="A7.F16.g1" src="x12.png" width="830"/>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_serif" id="A7.F16.3.1.1">Figure 16</span>: </span>Mean percent change in ChrF++ score across heterogeneous case IV relative to the 0 noise baseline for each model across both translation directions (En-XX and XX-En) and both perturbation directions. Scores are averaged across attack types, shots, and noise percentages. Positive values indicate the performance decreased post perturbation while the negative values indicate that performance increases post perturbation. Note: In certain cases, scores are bounded within minimum and maximum values for clarity in depicting overarching trends.</figcaption>
</figure>
<figure class="ltx_figure" id="A7.F17"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="835" id="A7.F17.g1" src="x13.png" width="581"/>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_serif" id="A7.F17.3.1.1">Figure 17</span>: </span>Shot-wise comparison of ChrF++ score of different models averaged across translation directions comparing the choice of the auxiliary source language of demonstrations.</figcaption>
</figure>
<figure class="ltx_figure" id="A7.F18"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="620" id="A7.F18.g1" src="x14.png" width="581"/>
<figcaption class="ltx_caption ltx_centering ltx_font_typewriter"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_serif" id="A7.F18.3.1.1">Figure 18</span>: </span>Shot-wise comparison of ChrF++ score of different models averaged across translation directions, comparing the choice of the source original and target original demonstrations</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun  4 19:34:34 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
