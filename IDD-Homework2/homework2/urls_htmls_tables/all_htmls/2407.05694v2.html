<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>On the Limitations of Compute Thresholds as a Governance Strategy.</title>
<!--Generated on Tue Jul 30 02:55:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.05694v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S1" title="In On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Understanding Risk</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S2" title="In On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>The Uncertain Relationship Between Compute and Risk.</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S2.SS1" title="In 2 The Uncertain Relationship Between Compute and Risk. ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>A shift in the relationship between compute and performance</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S2.SS1.SSS1" title="In 2.1 A shift in the relationship between compute and performance ‣ 2 The Uncertain Relationship Between Compute and Risk. ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Data quality reduces reliance on compute.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S2.SS1.SSS2" title="In 2.1 A shift in the relationship between compute and performance ‣ 2 The Uncertain Relationship Between Compute and Risk. ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Optimization breakthroughs compensate for compute.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S2.SS1.SSS3" title="In 2.1 A shift in the relationship between compute and performance ‣ 2 The Uncertain Relationship Between Compute and Risk. ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.3 </span>Architecture plays a significant role in determining scalability</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S3" title="In On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Avoiding a FLOP FLOP</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S3.SS1" title="In 3 Avoiding a FLOP FLOP ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Challenges of using FLOP as a metric</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S3.SS1.SSS1" title="In 3.1 Challenges of using FLOP as a metric ‣ 3 Avoiding a FLOP FLOP ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Training FLOP doesn’t account for post-training leaps in performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S3.SS1.SSS2" title="In 3.1 Challenges of using FLOP as a metric ‣ 3 Avoiding a FLOP FLOP ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Difficulty Tracking FLOP across model lifecycle.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S3.SS1.SSS3" title="In 3.1 Challenges of using FLOP as a metric ‣ 3 Avoiding a FLOP FLOP ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.3 </span>How to handle Mixture of Experts (MoEs) and classic ensembling?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S3.SS1.SSS4" title="In 3.1 Challenges of using FLOP as a metric ‣ 3 Avoiding a FLOP FLOP ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.4 </span>FLOP only accounts for a single model, but does not capture risk of the overall system.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S3.SS1.SSS5" title="In 3.1 Challenges of using FLOP as a metric ‣ 3 Avoiding a FLOP FLOP ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.5 </span>FLOP varies dramatically across different modalities.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S4" title="In On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>We are not very good at predicting the relationship between compute and risk</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S4.SS0.SSS1" title="In 4 We are not very good at predicting the relationship between compute and risk ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.0.1 </span>Limitations of scaling laws.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S4.SS0.SSS2" title="In 4 We are not very good at predicting the relationship between compute and risk ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.0.2 </span>Critical to specify the time horizon of interest.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S5" title="In On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>The Way Forward</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S5.SS1" title="In 5 The Way Forward ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Moving Away from Hard Coded Compute Thresholds</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S5.SS2" title="In 5 The Way Forward ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>The case for dynamic thresolds.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S5.SS3" title="In 5 The Way Forward ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Compute should not be used alone as a proxy for risk.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S5.SS4" title="In 5 The Way Forward ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Parting Thoughts</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S6" title="In On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Acknowledgments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#A1" title="In On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Technical Challenges of Measuring FLOP</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#A2" title="In On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>A wider view of what determines return on compute</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#A3" title="In On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Energy Requirements of AI Workloads over Time</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">On the Limitations of Compute Thresholds as a Governance Strategy.</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
name=<span class="ltx_text" id="id1.1.id1" style="font-size:173%;">Sara Hooker</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">At face value, this essay is about understanding a fairly esoteric governance tool called compute thresholds. However, in order to grapple with whether these thresholds will achieve anything, we must first understand how they came to be. To do so, we need to engage with a decades-old debate at the heart of computer science progress, namely, is <span class="ltx_text ltx_font_italic" id="id2.id1.1">bigger always better?</span> Does a certain inflection point of compute result in changes to the risk profile of a model? Hence, this essay may be of interest not only to policymakers and the wider public but also to computer scientists interested in understanding the role of compute in unlocking breakthroughs. This discussion is timely given the wide adoption of compute thresholds in both the White House Executive Orders on AI Safety (EO) and the EU AI Act to identify more risky systems. A key conclusion of this essay is that compute thresholds as currently implemented <span class="ltx_text ltx_font_bold" id="id2.id1.2">are shortsighted and likely to fail to mitigate risk</span>. The relationship between compute and risk is highly uncertain and rapidly changing. Relying upon compute thresholds overestimates our ability to predict what abilities emerge at different scales. This essay ends with recommendations for a better way forward.</p>
</div>
<section class="ltx_section" id="S1" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Understanding Risk</h2>
<div class="ltx_para" id="S1.p1">
<blockquote class="ltx_quote ltx_epigraph" id="S1.p1.1" style="width:281.85pt; margin-left:auto;;">
<div class="ltx_block ltx_epigraph_text" id="S1.p1.1.1" style="text-align:left; ;">
<p class="ltx_p" id="S1.p1.1.1.1"><span class="ltx_text" id="S1.p1.1.1.1.1" style="font-size:90%;">It’s hard to predict — especially the future.</span></p>
</div>
<div class="ltx_block ltx_epigraph_source" id="S1.p1.1.2" style="border-top:solid 0.4pt; text-align:right; ;">
<p class="ltx_p" id="S1.p1.1.2.1"><span class="ltx_text ltx_font_italic" id="S1.p1.1.2.1.1" style="font-size:90%;">Niels Bohr</span></p>
</div>
</blockquote>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Inherent to the human experience is our desire to limit risk. We avoid walking down dark streets at night; we wear sunscreen to reduce the risk of skin damage; we use seatbelts when driving. Seeking to proactively control risk is one of the key differentiators of modern society. As the historian Peter Bernstein said, “The ability to define what may happen in the future and to choose among alternatives lies at the heart of contemporary societies.”</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="440" id="S1.F1.sf1.g1" src="extracted/5762747/images/The_Plague_Belgium.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.sf1.3.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text ltx_font_bold" id="S1.F1.sf1.4.2" style="font-size:90%;">The plague.<span class="ltx_text ltx_font_medium" id="S1.F1.sf1.4.2.1"> A lack of medical knowledge precluded an understanding of what levers amplified risk.
</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="348" id="S1.F1.sf2.g1" src="extracted/5762747/images/Great_Fire_London.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.sf2.3.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text ltx_font_bold" id="S1.F1.sf2.4.2" style="font-size:90%;">The great fire of London.<span class="ltx_text ltx_font_medium" id="S1.F1.sf2.4.2.1"> Here, the sources of risk were known yet the proportional response was inadequate.</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.4.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.5.2" style="font-size:90%;">Effective governance requires both 1) estimating the level and origins of risk to society (see <span class="ltx_text ltx_font_bold" id="S1.F1.5.2.1">Right</span>) and 2) aligning on a proportionate response (see <span class="ltx_text ltx_font_bold" id="S1.F1.5.2.2">Left</span>). History is replete with examples where one or both of these stages fail. This note applies this lens to understand the viability of policies aimed at mitigating the risks introduced by a new era of Generative AI models. We ask whether 1) we have correctly estimated the role of compute in amplifying generative AI model risk, and 2) are hard-coded compute thresholds a meaningful tool for mitigating risk?</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Risk is a particularly challenging concept to formulate an effective governance response to, because it requires both <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">1)</span> <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">a successful estimate of the level and origins of risk to society</span> and <span class="ltx_text ltx_font_bold" id="S1.p3.1.3">2)</span> <span class="ltx_text ltx_font_italic" id="S1.p3.1.4">aligning on a proportionate response</span>. History is replete with examples where one or both requirements fail. For example, the large human toll incurred by the black death is a good example of the difficulty of estimating what vectors amplify risk, where inadequate medical knowledge in the 1300s led to a failure to identify rats as one of the main carriers of the disease <cite class="ltx_cite ltx_citemacro_citep">(Benedictow, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib27" title="">2004</a>)</cite>. In other cases, the risk is well known yet the response is inadequate. In 1666, the famous London fire swept through the city and devastated over half of all buildings. This risk was well known by authorities, as London had experienced several major fires before 1666. However, hesitation from authorities to act quickly to contain the blaze doomed the city <cite class="ltx_cite ltx_citemacro_citep">(Peter, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib165" title="">2002</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Few areas pose as significant a headache to policymakers as new technological breakthroughs. The historian Arthur Schlesinger aptly said, “Science and technology revolutionize our lives, but memory, tradition and myth frame our response.” Arthur’s point is that new technology must interact with the social fabric of our past and present, and be shaped by our humanity. Policymakers are often the first to grapple with what this means in practice. Here, the two-pronged objective of estimating and mitigating risks introduced by new technology is particularly tricky because breakthroughs are by definition hard to predict, so our response is almost always reactionary.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">This is almost certainly true for Generative AI, where a combination of deep neural networks, transformers, and ever-larger amounts of compute and data have changed overnight the realm of what is possible. Most language models prior to 2017 were focused on mastering narrow tasks that tested whether a model could learn linguistic properties such as logic or entailment <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib232" title="">2019</a>; Winograd, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib240" title="">1980</a>)</cite>. These models couldn’t generate long, fluid sequences and were rarely used outside of the realm of research conferences. In contrast, we now have machines that produce text indistinguishable from that produced by humans. Our current models can produce usable code, reason about the steps involved in solving a math problem, amuse humans with creativity, and accelerate productivity.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">With more powerful tools comes more possibility for misuse. This includes known harms including hallucinations <cite class="ltx_cite ltx_citemacro_citep">(HAI, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib89" title="">2023</a>; Economist, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib65" title="">2023</a>; Kossen et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib118" title="">2024</a>)</cite>, disinformation and misinformation <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib254" title="">2023</a>; Zellers et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib252" title="">2019</a>; Goldstein et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib86" title="">2023</a>; Musser, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib150" title="">2023</a>; Buchanan et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib36" title="">2021</a>)</cite>, bias <cite class="ltx_cite ltx_citemacro_citep">(Wiggers, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib239" title="">2023</a>; Hao, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib90" title="">2024</a>)</cite> and toxicity <cite class="ltx_cite ltx_citemacro_citep">(Pozzobon et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib167" title="">2023a</a>; Üstün et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib255" title="">2024</a>; Gehman et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib84" title="">2020</a>)</cite>. However, it also includes unknown risks incurred by further developing this technology, with researchers concerned by national security risks like biorisk <cite class="ltx_cite ltx_citemacro_citep">(AISI, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib10" title="">2024</a>; Mouton et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib148" title="">2024</a>; OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib157" title="">2024</a>)</cite>, cybersecurity threats <cite class="ltx_cite ltx_citemacro_citep">(NCSC, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib151" title="">2024</a>; Barrett et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib24" title="">2024</a>; Fang et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib75" title="">2024a</a>; Lohn &amp; Musser, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib135" title="">2022</a>)</cite> and loss of control <cite class="ltx_cite ltx_citemacro_citep">(UK Government, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib228" title="">2021</a>)</cite>. Partly, the difficulty we face is how to balance this portfolio of risks and how to allocate limited resources between mitigation of both present and future possible harms.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.2">A surprisingly popular approach to target and mitigate risk has been to equate the amount of compute used to <span class="ltx_text ltx_font_italic" id="S1.p7.2.2">train</span> a model with its propensity for harm. The implication that scale is a key lever for estimating risk pervades frameworks like responsible <span class="ltx_text ltx_font_italic" id="S1.p7.2.3">scaling policies</span> released by key industry players like Anthropic <cite class="ltx_cite ltx_citemacro_citep">(Anthropic, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib15" title="">2023</a>)</cite> and Open AI <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib156" title="">2023</a>)</cite>. It is also core to the motivation of compute thresholds which have influenced some of the first national and transnational policy governing Generative AI systems such as the White House Executive Order <cite class="ltx_cite ltx_citemacro_citep">(The White House, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib221" title="">2023</a>)</cite> (EO) and the EU AI Act <cite class="ltx_cite ltx_citemacro_citep">(European Union, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib72" title="">2024</a>)</cite> as well as ongoing legislation in China <cite class="ltx_cite ltx_citemacro_citep">(Linghan et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib133" title="">2024</a>)</cite>, California <cite class="ltx_cite ltx_citemacro_citep">(Senate, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib191" title="">2024</a>)</cite> and Bills focused on export controls <cite class="ltx_cite ltx_citemacro_citep">(on Foreign Affairs, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib155" title="">2024</a>; Reuters, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib179" title="">2024</a>)</cite>. Both the White House Executive Order and the EU AI Act differentiate models into different tiers of risk based upon a hard coded threshold; models above the threshold are considered more risky and require additional reporting steps and scrutiny. Namely, both use a static total number of FLOP or <span class="ltx_text ltx_font_italic" id="S1.p7.2.4">floating-point operations</span> to identify highly performant systems that require additional scrutiny. For the White House Executive Order this is set as any model that was <span class="ltx_text ltx_font_italic" id="S1.p7.1.1">trained using a quantity of computing power greater than <math alttext="10^{26}" class="ltx_Math" display="inline" id="S1.p7.1.1.m1.1"><semantics id="S1.p7.1.1.m1.1a"><msup id="S1.p7.1.1.m1.1.1" xref="S1.p7.1.1.m1.1.1.cmml"><mn id="S1.p7.1.1.m1.1.1.2" xref="S1.p7.1.1.m1.1.1.2.cmml">10</mn><mn id="S1.p7.1.1.m1.1.1.3" xref="S1.p7.1.1.m1.1.1.3.cmml">26</mn></msup><annotation-xml encoding="MathML-Content" id="S1.p7.1.1.m1.1b"><apply id="S1.p7.1.1.m1.1.1.cmml" xref="S1.p7.1.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p7.1.1.m1.1.1.1.cmml" xref="S1.p7.1.1.m1.1.1">superscript</csymbol><cn id="S1.p7.1.1.m1.1.1.2.cmml" type="integer" xref="S1.p7.1.1.m1.1.1.2">10</cn><cn id="S1.p7.1.1.m1.1.1.3.cmml" type="integer" xref="S1.p7.1.1.m1.1.1.3">26</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.1.1.m1.1c">10^{26}</annotation><annotation encoding="application/x-llamapun" id="S1.p7.1.1.m1.1d">10 start_POSTSUPERSCRIPT 26 end_POSTSUPERSCRIPT</annotation></semantics></math> integer or floating-point operations</span>, whereas in the EU AI Act a more stringent threshold is chosen as any model trained with more than <math alttext="10^{25}" class="ltx_Math" display="inline" id="S1.p7.2.m1.1"><semantics id="S1.p7.2.m1.1a"><msup id="S1.p7.2.m1.1.1" xref="S1.p7.2.m1.1.1.cmml"><mn id="S1.p7.2.m1.1.1.2" xref="S1.p7.2.m1.1.1.2.cmml">10</mn><mn id="S1.p7.2.m1.1.1.3" xref="S1.p7.2.m1.1.1.3.cmml">25</mn></msup><annotation-xml encoding="MathML-Content" id="S1.p7.2.m1.1b"><apply id="S1.p7.2.m1.1.1.cmml" xref="S1.p7.2.m1.1.1"><csymbol cd="ambiguous" id="S1.p7.2.m1.1.1.1.cmml" xref="S1.p7.2.m1.1.1">superscript</csymbol><cn id="S1.p7.2.m1.1.1.2.cmml" type="integer" xref="S1.p7.2.m1.1.1.2">10</cn><cn id="S1.p7.2.m1.1.1.3.cmml" type="integer" xref="S1.p7.2.m1.1.1.3">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.2.m1.1c">10^{25}</annotation><annotation encoding="application/x-llamapun" id="S1.p7.2.m1.1d">10 start_POSTSUPERSCRIPT 25 end_POSTSUPERSCRIPT</annotation></semantics></math> FLOP.</p>
</div>
<figure class="ltx_figure ltx_align_center" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="395" id="S1.F2.1.g1" src="extracted/5762747/images/1rjq425hb8501.jpg" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F2.4.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S1.F2.5.2" style="font-size:90%;">Bytes Magazine Cover, Volume 2, 1977.<span class="ltx_text ltx_font_medium" id="S1.F2.5.2.1"> A key characteristic of modern societies is our ability to choose amongst future alternatives by controlling for risk. One of the challenges is how to balance future unknown risks and risks of harm presented today. Compute thresholds as currently implemented are an example of precautionary policy – few models currently deployed in the wild fulfill the current criteria. This implies that the emphasis is not on auditing risks incurred by current models – but rather based upon the belief that future levels of compute will introduce new unforeseen risks.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">In this essay, we will ask what at first glance is a series of straightforward questions: <span class="ltx_text ltx_font_bold" id="S1.p8.1.1">1)</span> <span class="ltx_text ltx_font_italic" id="S1.p8.1.2">is compute as measured by FLOP a meaningful metric to estimate model risk?</span> and <span class="ltx_text ltx_font_bold" id="S1.p8.1.3">2)</span> <span class="ltx_text ltx_font_italic" id="S1.p8.1.4">are hard-coded thresholds an effective response to mitigate this risk?</span> A key conclusion of this work is that compute thresholds as currently implemented <span class="ltx_text ltx_font_bold" id="S1.p8.1.5">are shortsighted and likely to fail to mitigate risk</span>. Governance that relies on compute fails to understand that the relationship between compute and risk is highly uncertain and rapidly changing. We are observing a bifurcation in compute trends. On the one hand, at least in the short term systems are likely to continue to get bigger. On the other hand, the relationship between compute and performance is increasingly strained and hard to predict <cite class="ltx_cite ltx_citemacro_citep">(Niu et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib152" title="">2024</a>)</cite>. While the trend over the last 10 years involves more and more compute, a clear counter-trend has emerged with smaller models showcasing extremely high levels of performance.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.4">There is not a clear justification for any of the compute thresholds proposed to date. Indeed, the choice of <math alttext="10^{26}" class="ltx_Math" display="inline" id="S1.p9.1.m1.1"><semantics id="S1.p9.1.m1.1a"><msup id="S1.p9.1.m1.1.1" xref="S1.p9.1.m1.1.1.cmml"><mn id="S1.p9.1.m1.1.1.2" xref="S1.p9.1.m1.1.1.2.cmml">10</mn><mn id="S1.p9.1.m1.1.1.3" xref="S1.p9.1.m1.1.1.3.cmml">26</mn></msup><annotation-xml encoding="MathML-Content" id="S1.p9.1.m1.1b"><apply id="S1.p9.1.m1.1.1.cmml" xref="S1.p9.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p9.1.m1.1.1.1.cmml" xref="S1.p9.1.m1.1.1">superscript</csymbol><cn id="S1.p9.1.m1.1.1.2.cmml" type="integer" xref="S1.p9.1.m1.1.1.2">10</cn><cn id="S1.p9.1.m1.1.1.3.cmml" type="integer" xref="S1.p9.1.m1.1.1.3">26</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p9.1.m1.1c">10^{26}</annotation><annotation encoding="application/x-llamapun" id="S1.p9.1.m1.1d">10 start_POSTSUPERSCRIPT 26 end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="10^{25}" class="ltx_Math" display="inline" id="S1.p9.2.m2.1"><semantics id="S1.p9.2.m2.1a"><msup id="S1.p9.2.m2.1.1" xref="S1.p9.2.m2.1.1.cmml"><mn id="S1.p9.2.m2.1.1.2" xref="S1.p9.2.m2.1.1.2.cmml">10</mn><mn id="S1.p9.2.m2.1.1.3" xref="S1.p9.2.m2.1.1.3.cmml">25</mn></msup><annotation-xml encoding="MathML-Content" id="S1.p9.2.m2.1b"><apply id="S1.p9.2.m2.1.1.cmml" xref="S1.p9.2.m2.1.1"><csymbol cd="ambiguous" id="S1.p9.2.m2.1.1.1.cmml" xref="S1.p9.2.m2.1.1">superscript</csymbol><cn id="S1.p9.2.m2.1.1.2.cmml" type="integer" xref="S1.p9.2.m2.1.1.2">10</cn><cn id="S1.p9.2.m2.1.1.3.cmml" type="integer" xref="S1.p9.2.m2.1.1.3">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p9.2.m2.1c">10^{25}</annotation><annotation encoding="application/x-llamapun" id="S1.p9.2.m2.1d">10 start_POSTSUPERSCRIPT 25 end_POSTSUPERSCRIPT</annotation></semantics></math> rather than a number smaller or larger has not been justified in any of the policies implementing compute thresholds as a governance strategy. We do know that model scale amplifies certain risks – larger models tend to produce more toxic text and harmful associations <cite class="ltx_cite ltx_citemacro_citep">(Birhane et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib29" title="">2023</a>)</cite> and increases privacy risk because the propensity to memorize rare artifacts can increase the likelihood of data leakage <cite class="ltx_cite ltx_citemacro_citep">(Panda et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib160" title="">2024</a>; Kandpal et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib112" title="">2022</a>; Carlini et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib39" title="">2023</a>)</cite>. However, these relationships hold in compute settings far below <math alttext="10^{25}" class="ltx_Math" display="inline" id="S1.p9.3.m3.1"><semantics id="S1.p9.3.m3.1a"><msup id="S1.p9.3.m3.1.1" xref="S1.p9.3.m3.1.1.cmml"><mn id="S1.p9.3.m3.1.1.2" xref="S1.p9.3.m3.1.1.2.cmml">10</mn><mn id="S1.p9.3.m3.1.1.3" xref="S1.p9.3.m3.1.1.3.cmml">25</mn></msup><annotation-xml encoding="MathML-Content" id="S1.p9.3.m3.1b"><apply id="S1.p9.3.m3.1.1.cmml" xref="S1.p9.3.m3.1.1"><csymbol cd="ambiguous" id="S1.p9.3.m3.1.1.1.cmml" xref="S1.p9.3.m3.1.1">superscript</csymbol><cn id="S1.p9.3.m3.1.1.2.cmml" type="integer" xref="S1.p9.3.m3.1.1.2">10</cn><cn id="S1.p9.3.m3.1.1.3.cmml" type="integer" xref="S1.p9.3.m3.1.1.3">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p9.3.m3.1c">10^{25}</annotation><annotation encoding="application/x-llamapun" id="S1.p9.3.m3.1d">10 start_POSTSUPERSCRIPT 25 end_POSTSUPERSCRIPT</annotation></semantics></math> or <math alttext="10^{26}" class="ltx_Math" display="inline" id="S1.p9.4.m4.1"><semantics id="S1.p9.4.m4.1a"><msup id="S1.p9.4.m4.1.1" xref="S1.p9.4.m4.1.1.cmml"><mn id="S1.p9.4.m4.1.1.2" xref="S1.p9.4.m4.1.1.2.cmml">10</mn><mn id="S1.p9.4.m4.1.1.3" xref="S1.p9.4.m4.1.1.3.cmml">26</mn></msup><annotation-xml encoding="MathML-Content" id="S1.p9.4.m4.1b"><apply id="S1.p9.4.m4.1.1.cmml" xref="S1.p9.4.m4.1.1"><csymbol cd="ambiguous" id="S1.p9.4.m4.1.1.1.cmml" xref="S1.p9.4.m4.1.1">superscript</csymbol><cn id="S1.p9.4.m4.1.1.2.cmml" type="integer" xref="S1.p9.4.m4.1.1.2">10</cn><cn id="S1.p9.4.m4.1.1.3.cmml" type="integer" xref="S1.p9.4.m4.1.1.3">26</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p9.4.m4.1c">10^{26}</annotation><annotation encoding="application/x-llamapun" id="S1.p9.4.m4.1d">10 start_POSTSUPERSCRIPT 26 end_POSTSUPERSCRIPT</annotation></semantics></math> FLOP and are present in many models far smaller than the current threshold. What is striking about the choice of compute thresholds to date is that many are examples of precautionary policy <cite class="ltx_cite ltx_citemacro_citep">(Ricci &amp; Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib180" title="">2011</a>)</cite> – no models currently deployed in the wild fulfill the current criteria set by US Executive order. Only a handful of models will be impacted by the EU AI Act when it comes into effect <cite class="ltx_cite ltx_citemacro_citep">(Epoch AI, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib68" title="">2023</a>)</cite>. This implies that the emphasis is <em class="ltx_emph ltx_font_italic" id="S1.p9.4.1">not</em> on auditing the risks incurred by currently deployed models in the wild but rather is based upon the belief that future levels of compute will introduce unforeseen new risks that demand a higher level of scrutiny. Across this essay, several recommendations will emerge from our deep dive into the relationship between compute and risk:</p>
</div>
<div class="ltx_para" id="S1.p10">
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">The relationship between compute and risk is rapidly changing</span> While the last decade has involved ever larger amounts of compute, increasingly smaller models are more performant due to optimization which happens outside of traditional training. <span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.2">Training compute</span> fails to account for “inference-time compute” enhancements which can dramatically change risk profile of the model. In Section <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S2" title="2 The Uncertain Relationship Between Compute and Risk. ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_tag">2</span></a> we explore what is known about the relationship between compute and performance and find that much of the gains in risk over the last few years can be attributed to optimization strategies and high quality data, rather than pure FLOP. Year over year, smaller models are showcasing extremely high levels of performance.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Evidence to-date suggests we are not good at predicting what abilities emerge at different scales.</span> The choice of where compute thresholds are set will have far-ranging implications – too low and too many models will be selected for additional scrutiny and reporting each year. In contrast, if it is set too high, not enough models will subject to reporting requirements, and the threshold risks become decorative rather than a meaningful indicator of risk. In Section <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S4" title="4 We are not very good at predicting the relationship between compute and risk ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_tag">4</span></a> we take stock of our track record predicting performance at different levels of compute and find that our track record to date is wanting. Put simply, we are not good at predicting the relationship between scale and downstream metrics. Despite considerable effort and a large body of literature, our ability to predict the emergence of specific downstream capabilities with scale remains elusive <cite class="ltx_cite ltx_citemacro_citep">(Schaeffer et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib185" title="">2024a</a>)</cite>. This calls into question the viability of any choice of training compute threshold – it is hard to tell if we have set the number of FLOP correctly.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">FLOP has to be better specified as a metric to be meaningful.</span> Existing policies do not specify key details around FLOP measurement that are necessary to ensure fair reporting. In Section <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S3" title="3 Avoiding a FLOP FLOP ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_tag">3</span></a>, we show how an under-specified threshold on FLOP presents many loopholes that are easy to exploit. As currently detailed, the lack of specification is a lesson in <span class="ltx_text ltx_font_italic" id="S1.I1.i3.p1.1.2">Goodhart’s Law</span>: “When a measure becomes a target, it ceases to be a good measure.” Preventing FLOP from becoming merely decorative requires clear and consistent guidance across jurisdictions. Using compute thresholds should be done with caution, and having clear understanding of the limitations and standardized reporting is critical for avoiding manipulation of the metric.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i4.p1.1.1">Governments should be transparent about what risks they are concerned about and where they are allocating limited resources.</span> Current compute thresholds do not apply to almost all models currently deployed in the wild. However, currently deployed models present considerable risk. Governments should articulate what future risks motivate a focus on forward-looking scrutiny. There is currently a severe shortage of technical staff with AI experience within government <cite class="ltx_cite ltx_citemacro_citep">(Zakrzewski, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib251" title="">2024</a>; Aitken et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib11" title="">2022</a>; Engstrom et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib67" title="">2020</a>)</cite> and capacity issues which might limit the ability of governments to implement effective policies <cite class="ltx_cite ltx_citemacro_citep">(Marchant, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib140" title="">2011</a>; Reuel et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib178" title="">2024</a>)</cite>. With limited resources, it is even more paramount that governance goals are transparent with the public. Without being explicit about the risks compute thresholds hope to mitigate, it is hard to weigh the likelihood of successful mitigation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i5.p1.1.1">Applying hard coded thresholds to a quickly changing distribution is likely to fail.</span> We show throughout this essay that one of the most misbehaved and rapidly changing distributions is the relationship between compute and performance. When a data distribution is rapidly changing, it is risky to use a hard-coded threshold precisely because it is hard to know exactly where to place it. In Section <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S5.SS1" title="5.1 Moving Away from Hard Coded Compute Thresholds ‣ 5 The Way Forward ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_tag">5.1</span></a>, we recommend instead using a dynamic threshold which automatically self-adjusts to a percentile of the distribution of model properties released that year. We also recommend moving away from using compute as a sole indicate to tier models, and instead using a risk index composed of several measures of performance. This avoids putting <span class="ltx_text ltx_font_italic" id="S1.I1.i5.p1.1.2">all eggs in one basket.</span></p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p11">
<p class="ltx_p" id="S1.p11.1">To first understand how thresholds came to be, we need to delve into a decades-old debate at the heart of compute science progress, namely, is <span class="ltx_text ltx_font_italic" id="S1.p11.1.1">scaling always better</span>. For the last decade, computer science progress has been caught by our own Moore’s law <cite class="ltx_cite ltx_citemacro_citep">(Schaller, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib187" title="">1997</a>)</cite> of a painfully simple formula for innovation by <span class="ltx_text ltx_font_italic" id="S1.p11.1.2">adding more model parameters and data</span>. Yet, this essay will posit it is far from clear that future innovation or indeed amplified levels of risk will come from compute alone. As we will see in the next section, the relationship between compute and performance is far from straightforward and far from settled. <span class="ltx_text ltx_font_italic" id="S1.p11.1.3">Compute is changing rapidly, as fast as the technology that it serves.</span></p>
</div>
</section>
<section class="ltx_section" id="S2" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>The Uncertain Relationship Between Compute and Risk.</h2>
<div class="ltx_para" id="S2.p1">
<blockquote class="ltx_quote ltx_epigraph" id="S2.p1.1" style="width:281.85pt; margin-left:auto;;">
<div class="ltx_block ltx_epigraph_text" id="S2.p1.1.1" style="text-align:left; ;">
<p class="ltx_p" id="S2.p1.1.1.1"><span class="ltx_text" id="S2.p1.1.1.1.1" style="font-size:90%;">“Well Babbage what are you dreaming about?” to which I replied, “I am thinking that all these tables might be calculated by machinery.”</span></p>
</div>
<div class="ltx_block ltx_epigraph_source" id="S2.p1.1.2" style="border-top:solid 0.4pt; text-align:right; ;">
<p class="ltx_p" id="S2.p1.1.2.1"><span class="ltx_text ltx_font_italic" id="S2.p1.1.2.1.1" style="font-size:90%;">Charles Babbage</span></p>
</div>
</blockquote>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Many inventions are re-purposed for means unintended by their designers. Initially, the magnetron tube was developed for radar technology during World War II. In 1945, a self-taught American engineer, Percy Spencer, noticed that a chocolate bar melted in his pocket whenever he was close to a radar set. This innocuous discovery resulted in the patent for the first microwave <cite class="ltx_cite ltx_citemacro_citep">(Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib253" title="">2017</a>)</cite>. In a similar vein, deep neural networks only began to work when an existing technology was unexpectedly re-purposed. A graphical processing unit (GPU) was originally introduced in the 1970s as a specialized accelerator for video games and for developing graphics for movies and animation. In the 2000s, like the magnetron tube, GPUs were re-purposed for an entirely unimagined use case – to train deep neural networks <cite class="ltx_cite ltx_citemacro_citep">(Chellapilla et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib40" title="">2006</a>; Hooker, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib100" title="">2021</a>; Oh &amp; Jung, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib154" title="">2004</a>; Payne et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib163" title="">2005</a>)</cite>. GPUs had one critical advantage over CPUs - they were far better at parallelizing matrix multiples <cite class="ltx_cite ltx_citemacro_citep">(Brodtkorb et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib35" title="">2013</a>; Dettmers, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib57" title="">2023</a>)</cite>, a mathemetical operation which dominates the definition of deep neural network layers <cite class="ltx_cite ltx_citemacro_citep">(Fawzi et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib77" title="">2022</a>; Davies et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib54" title="">2024</a>)</cite>. This higher number of floating operation points per second (FLOP/s) combined with the clever distribution of training between GPUs unblocked the training of deeper networks. The depth of the network turned out to be critical. Performance on ImageNet jumped with ever deeper networks in 2011 <cite class="ltx_cite ltx_citemacro_citep">(Ciresan et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib44" title="">2011</a>)</cite>, 2012 <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib120" title="">2012</a>)</cite> and 2015 <cite class="ltx_cite ltx_citemacro_citep">(Szegedy et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib211" title="">2014</a>)</cite>. A striking example of this jump in compute is a comparison of the now famous 2012 Google paper which used 16,000 CPU cores to classify cats <cite class="ltx_cite ltx_citemacro_citep">(Le et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib121" title="">2012</a>)</cite> to a paper published a mere year later that solved the same task with only two CPU cores and four GPUs <cite class="ltx_cite ltx_citemacro_citep">(Coates et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib45" title="">2013</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="443" id="S2.F3.sf1.g1" src="x1.png" width="580"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S2.F3.sf1.3.2" style="font-size:90%;">Open Leaderboard Scores For Small Models (&lt;13B) Over Time
</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="443" id="S2.F3.sf2.g1" src="x2.png" width="580"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S2.F3.sf2.3.2" style="font-size:90%;">Large models (&gt;13B) that perform worse than Small Models (&lt;13B)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.4.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S2.F3.5.2" style="font-size:90%;">The changing relationship between compute and performance. Smaller models are becoming increasingly performant and routinely now outperform much larger models. <span class="ltx_text ltx_font_bold" id="S2.F3.5.2.1">Right:</span> Plot of the best daily 13B or smaller model submitted to the Open LLM leaderboard over time. Even amongst comparable small sized models, performance has been growing rapidly. <span class="ltx_text ltx_font_bold" id="S2.F3.5.2.2">Left:</span> The best small models submitted to the Open LLM leaderboard easily outperform far larger models. We observe that over time there have been more and more large models which are easily out-competed by small &lt;13B models. In the left plot, scatter plot is sized by number of parameters to give a sense of proportion of each model submitted.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">This would ignite a rush for compute which has led to a bigger-is-better race in the number of model parameters over the last decade <cite class="ltx_cite ltx_citemacro_citep">(Canziani et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib38" title="">2016</a>; Strubell et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib206" title="">2019b</a>; Rae et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib173" title="">2021</a>; Raffel et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib175" title="">2020</a>; Bommasani et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib30" title="">2021</a>; Bender et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib26" title="">2021</a>)</cite>. The computer scientist Ken Thompson famously said <span class="ltx_text ltx_font_italic" id="S2.p3.1.1">“When in doubt, use brute force.”</span> This was formalized as the “bitter lesson” by Rich Sutton who posited that computer science history tells us that throwing more compute at a problem has consistently outperformed all attempts to leverage human knowledge of a domain to teach a model <cite class="ltx_cite ltx_citemacro_citep">(Sutton, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib210" title="">2019</a>)</cite>. In a punch to the ego of every computer scientist out there, what Sutton is saying is that symbolic methods that codify human knowledge have not worked as well as letting a model learn patterns for itself coupled with ever-vaster amounts of compute.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.p4.1.1">Is Sutton right?</span> Certainly, he is correct that scaling has been a widely favored formula because it has provided persuasive gains in overall performance – size is the most de-risked tool we have to unlock new gains. As the computer scientist Michael Jordan quipped <span class="ltx_text ltx_font_italic" id="S2.p4.1.2">“Today we can’t think without holding a piece of metal.”</span> Increasing compute also conveniently fits into the cadence of quarterly industry planning, it is less risky to propose training a bigger model than it is to propose an alternative optimization technique. However, relying on compute alone misses a critical shift that is underway in the relationship between compute and performance. It is not always the case that bigger models result in better performance. The bitter lesson doesn’t explain why Falcon 180B <cite class="ltx_cite ltx_citemacro_citep">(Almazrouei et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib14" title="">2023</a>)</cite> is easily outperformed by far smaller open weights models such as Llama-3 8B <cite class="ltx_cite ltx_citemacro_citep">(AI@Meta, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib9" title="">2024</a>)</cite>, Command R 35B <cite class="ltx_cite ltx_citemacro_citep">(Cohere &amp; Team, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib46" title="">2024</a>)</cite>, Gemma 27B <cite class="ltx_cite ltx_citemacro_citep">(Team, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib216" title="">2024</a>)</cite>. It also doesn’t explain why Aya 23 8B <cite class="ltx_cite ltx_citemacro_citep">(Aryabumi et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib20" title="">2024</a>)</cite> easily outperforms BLOOM 176 B <cite class="ltx_cite ltx_citemacro_citep">(Workshop et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib241" title="">2023</a>)</cite> despite having only 4.5% of the parameters.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">These are not isolated examples, but rather indicative of an overall trend where there is no guarantee larger models consistently outperform smaller models. Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S2.F3.sf2" title="Figure 3(b) ‣ Figure 3 ‣ 2 The Uncertain Relationship Between Compute and Risk. ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_tag">3(b)</span></a> plots the scores of models submitted to the Open LLM Leaderboard <cite class="ltx_cite ltx_citemacro_citep">(Beeching et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib25" title="">2023</a>)</cite> over the last two years. Here, we plot <span class="ltx_text ltx_font_italic" id="S2.p5.1.1">large models</span> with more than 13 billion parameters whose leaderboard score is less than the top performing <span class="ltx_text ltx_font_italic" id="S2.p5.1.2">small model</span> with less than 13 billion parameters. We observe that over time, more and more large models have been submitted that are outperformed by the best small model daily submission.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">To understand why this is the case, we must understand what key variables have been driving gains in performance over the last decade. In an era where there are diminishing returns for the amount of compute available <cite class="ltx_cite ltx_citemacro_citep">(Lohn &amp; Musser, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib135" title="">2022</a>; Thompson et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib222" title="">2020</a>)</cite>, optimization and architecture breakthroughs define the rate of return for a given unit of compute. <span class="ltx_text ltx_font_bold" id="S2.p6.1.1">It is this rate of return which is most critical to the pace of progress and to the level of risk incurred by additional compute</span>.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>A shift in the relationship between compute and performance</h3>
<div class="ltx_para" id="S2.SS1.p1">
<blockquote class="ltx_quote ltx_epigraph" id="S2.SS1.p1.1" style="width:281.85pt; margin-left:auto;;">
<div class="ltx_block ltx_epigraph_text" id="S2.SS1.p1.1.1" style="text-align:left; ;">
<p class="ltx_p" id="S2.SS1.p1.1.1.1"><span class="ltx_text" id="S2.SS1.p1.1.1.1.1" style="font-size:90%;">The world has changed less since Jesus Christ than it has in the last 30 years.</span></p>
</div>
<div class="ltx_block ltx_epigraph_source" id="S2.SS1.p1.1.2" style="border-top:solid 0.4pt; text-align:right; ;">
<p class="ltx_p" id="S2.SS1.p1.1.2.1"><span class="ltx_text" id="S2.SS1.p1.1.2.1.1" style="font-size:90%;">Charles Peguy, 1913</span></p>
</div>
</blockquote>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">In complex systems, it is challenging to manipulate one variable in isolation and foresee all implications. Throughout the 20th century doctors recommended removing tonsils in response to any swelling or infection, but research has recently shown the removal may lead to higher incidence of throat cancer <cite class="ltx_cite ltx_citemacro_citep">(Liang et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib129" title="">2023</a>)</cite>. Early televised drug prevention advertisements in the 2000s led to increased drug use <cite class="ltx_cite ltx_citemacro_citep">(Terry-McElrath et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib218" title="">2011</a>)</cite>. In a similar vein, the belief that more compute equates with more risk belies a far more complex picture that requires re-examining the relationship between performance and compute. A key limitation of simply throwing more scale at a task is that the relationship between additional compute and generalization remains poorly understood. A growing body of research suggests that the relationship between compute and performance is far more complex. Empirical evidence suggests that small models are rapidly becoming more performant and riskier.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Data quality reduces reliance on compute.</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">Models trained on better data do not require as much compute. A large body of work has emerged which shows that efforts to better curate training corpus, including de-duping <cite class="ltx_cite ltx_citemacro_citep">(Taylor et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib214" title="">2022</a>; Kocetkov et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib117" title="">2022</a>)</cite>, data pruning <cite class="ltx_cite ltx_citemacro_citep">(Marion et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib142" title="">2023</a>; Singh et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib200" title="">2024a</a>; Sorscher et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib202" title="">2023</a>; Albalak et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib13" title="">2024</a>; Tirumala et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib223" title="">2023</a>; Chimoto et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib43" title="">2024</a>)</cite> or data prioritization <cite class="ltx_cite ltx_citemacro_citep">(Boubdir et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib33" title="">2023</a>; Thakkar et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib220" title="">2023</a>)</cite> can compensate for more weights. This suggests that the number of learnable parameters is not definitively the constraint on improving performance; investments in better data quality mitigate the need for more weights <cite class="ltx_cite ltx_citemacro_citep">(Singh et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib200" title="">2024a</a>; Penedo et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib164" title="">2023</a>; Raffel et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib175" title="">2020</a>; Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib125" title="">2022</a>)</cite>. If the size of a training dataset can be reduced without impacting performance <cite class="ltx_cite ltx_citemacro_citep">(Marion et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib142" title="">2023</a>)</cite>, training time is reduced. This directly impacts the number of training FLOP and means less compute is needed.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Optimization breakthroughs compensate for compute.</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">Progress over the last few years has been as much due to optimization improvements as it has been due to compute. This includes extending pre-training with instruction finetuning to teach models instruction following <cite class="ltx_cite ltx_citemacro_citep">(Singh et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib201" title="">2024b</a>)</cite>, model distillation using synthetic data from larger more performant "teachers" to train highly capable, smaller "students" <cite class="ltx_cite ltx_citemacro_citep">(Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib217" title="">2024b</a>; Aryabumi et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib20" title="">2024</a>)</cite>, chain-of-thought reasoning <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib237" title="">2023</a>; Hsieh et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib103" title="">2023</a>)</cite>, increased context-length <cite class="ltx_cite ltx_citemacro_citep">(Xiong et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib245" title="">2023</a>)</cite>, enabled tool-use <cite class="ltx_cite ltx_citemacro_citep">(Qin et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib169" title="">2023</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib233" title="">2023a</a>)</cite>, retrieval augmented generation <cite class="ltx_cite ltx_citemacro_citep">(Pozzobon et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib168" title="">2023b</a>; Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib127" title="">2020</a>)</cite>, and preference training to align models with human feedback <cite class="ltx_cite ltx_citemacro_citep">(Dang et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib52" title="">2024</a>; Ahmadian et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib8" title="">2024</a>; Ouyang et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib158" title="">2022</a>; Bai et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib23" title="">2022</a>; Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib123" title="">2023</a>; Tunstall et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib227" title="">2023</a>; Khalifa et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib115" title="">2021</a>; Rafailov et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib174" title="">2023</a>; Azar et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib21" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p2">
<p class="ltx_p" id="S2.SS1.SSS2.p2.1">All these techniques compensate for the need for weights or expensive prolonged training <cite class="ltx_cite ltx_citemacro_citep">(Ho et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib98" title="">2024b</a>)</cite>.All things equal, these have been shown to dramatically improve model performance relative to a model trained without these optimization tricks given the same level of compute <cite class="ltx_cite ltx_citemacro_citep">(Davidson et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib53" title="">2023</a>; Hernandez &amp; Brown, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib95" title="">2020</a>; Erdil &amp; Besiroglu, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib70" title="">2023</a>; METR Team, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib145" title="">2023</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib134" title="">2024</a>)</cite>. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S2.F3.sf1" title="Figure 3(a) ‣ Figure 3 ‣ 2 The Uncertain Relationship Between Compute and Risk. ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_tag">3(a)</span></a>, we plot the best daily 13B or smaller model submitted to the <a class="ltx_ref ltx_href" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" title="">Open LLM Leaderboard</a> over time. In a mere span of 2 years, the best-performing daily scores from small model went from an average of 38.59% across to an average of 77.15% across 2024 submissions. The takeaway is clear – smaller models with the same amount of capacity are becoming more and more performant.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Architecture plays a significant role in determining scalability</h4>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">The introduction of a new architecture design can fundamentally change the relationship between compute and performance <cite class="ltx_cite ltx_citemacro_citep">(Tay et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib213" title="">2022</a>; Sevilla et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib193" title="">2022a</a>; Ho et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib97" title="">2024a</a>)</cite> and render any compute threshold that is set irrelevant. For example, the key breakthroughs in AI adoption around the world were the introduction of architectures like convolutional neural networks (CNNs) for vision <cite class="ltx_cite ltx_citemacro_citep">(Ciresan et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib44" title="">2011</a>; Krizhevsky et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib120" title="">2012</a>; Szegedy et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib211" title="">2014</a>)</cite> and Transformers for language modeling <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib230" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p2">
<p class="ltx_p" id="S2.SS1.SSS3.p2.1">While deep neural networks represent a huge step forward in performance for a given level of compute, what is often missed is that our architectures also represent the ceiling in what is achievable through scaling.</p>
</div>
<figure class="ltx_figure ltx_align_floatleft" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="382" id="S2.F4.g1" src="extracted/5762747/images/computers_labatory.jpg" width="281"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.4.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text ltx_font_bold" id="S2.F4.5.2" style="font-size:90%;">Bytes Magazine Cover, Volume 5, 1980.<span class="ltx_text ltx_font_medium" id="S2.F4.5.2.1"> Compute is rarely the only determinant of progress. Data quality, instruction-finetuning, preference training, retrieval augmented networks, enabled tool use, chain-of-thought reasoning, increased context-length are all examples of optimization techniques which add little or no <span class="ltx_text ltx_font_italic" id="S2.F4.5.2.1.1">training</span> FLOP but result in significant gains in performance.
</span></span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.SSS3.p3">
<p class="ltx_p" id="S2.SS1.SSS3.p3.1">While progress has revolved around deep neural networks for the last decade, there is much to suggest that the next significant gain in efficiency will require an entirely different architecture. Deep neural networks remain very inefficient as an algorithm. Our typical training regimes require that all examples are shown the same number of times during the training <cite class="ltx_cite ltx_citemacro_citep">(Xue et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib246" title="">2023</a>)</cite>. All modern networks are trained based upon minimization of average error <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib87" title="">2016</a>)</cite>. This means that learning rare artifacts requires far more training time or capacity due to the diluted signal of infrequent attributes relative to the most frequent patterns in the dataset <cite class="ltx_cite ltx_citemacro_citep">(Achille et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib4" title="">2017</a>; Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib110" title="">2020</a>; Mangalam &amp; Prabhu, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib139" title="">2019</a>; Faghri et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib74" title="">2020</a>; Frankle et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib80" title="">2020</a>; Arpit et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib19" title="">2017</a>)</cite>. Small models are already good at learning the most frequent features, and most easy features and common patterns are learned early on training with much harder rare features learned in later stages <cite class="ltx_cite ltx_citemacro_citep">(Agarwal &amp; Hooker, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib6" title="">2020</a>; Paul et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib162" title="">2021</a>; Mangalam &amp; Prabhu, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib139" title="">2019</a>; Siddiqui et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib199" title="">2022</a>; Abbe et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib3" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p4">
<p class="ltx_p" id="S2.SS1.SSS3.p4.1">When we radically scale the size of a model, we show the most gains in performance are on rare and underrepresented attributes in the dataset – the long-tail <cite class="ltx_cite ltx_citemacro_citep">(Hooker et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib101" title="">2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib102" title="">2020</a>)</cite>. Put differently, scaling is being used to inefficiently learn a very small fraction of the overall training dataset. Our reliance on global updates also results in catastrophic forgetting, where performance deteriorates on the original task because the new information interferes with previously learned behavior <cite class="ltx_cite ltx_citemacro_citep">(Mcclelland et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib144" title="">1995</a>; Pozzobon et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib168" title="">2023b</a>)</cite>. All this suggests that our current architecture choices are probably not final and key disruptions lie ahead. This is likely to radically change any scaling relationships, in the same way it has done in the last decade. For example, it is unlikely any prediction of how compute scales based upon architectures before deep neural networks holds true post-2012 after the introduction of convolutional neural networks.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Avoiding a FLOP FLOP</h2>
<div class="ltx_para" id="S3.p1">
<blockquote class="ltx_quote ltx_epigraph" id="S3.p1.1" style="width:281.85pt; margin-left:auto;;">
<div class="ltx_block ltx_epigraph_text" id="S3.p1.1.1" style="text-align:left; ;">
<p class="ltx_p" id="S3.p1.1.1.1"><span class="ltx_text" id="S3.p1.1.1.1.1" style="font-size:90%;">Any statistical relationship will break down when used for policy purposes.</span></p>
</div>
<div class="ltx_block ltx_epigraph_source" id="S3.p1.1.2" style="border-top:solid 0.4pt; text-align:right; ;">
<p class="ltx_p" id="S3.p1.1.2.1"><span class="ltx_text ltx_font_italic" id="S3.p1.1.2.1.1" style="font-size:90%;">Jon Danielsson</span></p>
</div>
</blockquote>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_italic" id="S3.p2.1.1">Are FLOP a reliable proxy for overall compute?</span> Even if the relationship between compute and generalization were stable – there are difficulties operationalizing FLOP as a metric. FLOP <cite class="ltx_cite ltx_citemacro_citep">(Goldberg, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib85" title="">1991</a>)</cite> refers to <span class="ltx_text ltx_font_italic" id="S3.p2.1.2">floating-point operations</span>, and has a fairly straightforward definition: sum up all the math operations in floating point (such as addition, subtraction, multiplication, and division). In the 1950s and 1960s, as computers were becoming more prevalent, the need for a standard measure of performance arose. FLOP are particularly useful in fields that require floating-point calculations, such as scientific computations, advanced analytics, and 3D graphics processing. This is because all these areas are dominated by simple primitive mathematical operations – for example, FLOP tend to be closely associated with the size of models because deep neural network layers are dominated by a single operation – matrix multiplies – which can be decomposed into a set of floating point operations <cite class="ltx_cite ltx_citemacro_citep">(Fawzi et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib77" title="">2022</a>; Davies et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib54" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.p3.1.1">We first begin by noting there are some reasons FLOP are attractive as a policy measure.</span> The primary one is that FLOP provides a standardized way to compare across different hardware and software stacks. FLOP counts don’t change across hardware – the number of mathematical operations is the same no matter what hardware you train a model on. In a world where hardware is increasingly heterogeneous <cite class="ltx_cite ltx_citemacro_citep">(Hooker, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib100" title="">2021</a>)</cite> and it is hard to replicate the exact training setting due to a lack of software portability <cite class="ltx_cite ltx_citemacro_citep">(Mince et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib147" title="">2023</a>)</cite>, it is attractive to use a metric that doesn’t depend on replicating exact infrastructure. It also neatly sidesteps reporting issues that could occur if relying only on the number of hardware devices used to train a model. The rapidly increasing performance of new hardware generations <cite class="ltx_cite ltx_citemacro_citep">(Hobbhahn et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib99" title="">2023</a>)</cite>, as well as engineering investments in training infrastructure <cite class="ltx_cite ltx_citemacro_citep">(Yoo et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib247" title="">2022</a>; Lepikhin et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib126" title="">2020</a>)</cite>, mean that over time much larger models will be trained using the same number of devices. FLOP is also a metric which could potentially be inferred by cloud providers. Given most machine learning workloads are run by a few key cloud providers, this may make administering such a measure effectively easier <cite class="ltx_cite ltx_citemacro_citep">(Heim et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib94" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">A key conundrum posed by FLOP thresholds is that policymakers are using FLOP as a proxy for risk, but FLOP doesn’t say anything about end performance of a model — only about the number of operations applied to the data. For example, if you compare two models trained for the same number of FLOP but one has had safety alignment during post-training <cite class="ltx_cite ltx_citemacro_citep">(Aakanksha et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib2" title="">2024</a>; Bai et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib23" title="">2022</a>)</cite> and the other has none – these two models will still be accorded the same level of risk according to number of FLOP but one will present a far lower risk to society because of safety alignment.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">Another key hurdle governance which adopts compute threshold will have to overcome is the lack of clear guidance in all the policy to-date about how FLOP will actually be measured in practice. This ambiguity risks FLOP as a metric being irrelevant or at the very least easy to manipulate. Developing principled standards for measuring any metric of interest is essential for ensuring that safety measures are applied in a proportionate and appropriate way. In the followings Section, we specify some of the key ways in which it is easy to manipulate FLOP if it is left underspecified as a metric.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Challenges of using FLOP as a metric</h3>
<div class="ltx_para" id="S3.SS1.p1">
<blockquote class="ltx_quote ltx_epigraph" id="S3.SS1.p1.1" style="width:281.85pt; margin-left:auto;;">
<div class="ltx_block ltx_epigraph_text" id="S3.SS1.p1.1.1" style="text-align:left; ;">
<p class="ltx_p" id="S3.SS1.p1.1.1.1"><span class="ltx_text" id="S3.SS1.p1.1.1.1.1" style="font-size:90%;">If you cannot measure it, you cannot improve it.</span></p>
</div>
<div class="ltx_block ltx_epigraph_source" id="S3.SS1.p1.1.2" style="border-top:solid 0.4pt; text-align:right; ;">
<p class="ltx_p" id="S3.SS1.p1.1.2.1"><span class="ltx_text" id="S3.SS1.p1.1.2.1.1" style="font-size:90%;">Lord Kelvin</span></p>
</div>
</blockquote>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Training FLOP doesn’t account for post-training leaps in performance</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">Applying scrutiny and regulation based upon training FLOP ignores that a lot of compute can be spent outside of training to improve performance of a model. This can be grouped under “inference-time compute” and can result in large performance gains that dramatically increase the risk profile of a model. The limited work to-date which has evaluated a subset of ‘inference-time compute” improvements estimates these can impart gains between 5x and 20x of base level post-training performance <cite class="ltx_cite ltx_citemacro_citep">(Davidson et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib53" title="">2023</a>)</cite>.“Inference-time compute” includes best-of-n sampling techniques <cite class="ltx_cite ltx_citemacro_citep">(Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib215" title="">2024a</a>)</cite>, chain-of-thought reasoning <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib237" title="">2023</a>; Hsieh et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib103" title="">2023</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib235" title="">2023c</a>)</cite> and model distillation using synthetic data <cite class="ltx_cite ltx_citemacro_citep">(Aryabumi et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib20" title="">2024</a>; Shimabucoro et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib198" title="">2024</a>; Üstün et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib255" title="">2024</a>; Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib215" title="">2024a</a>)</cite>. All these techniques require more compute at test-time because of the need to perform more forward passes of the model to generate additional samples. However, these are not reflected in training time costs and indeed can often <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p1.1.1">reduce</span> the compute needed during training. For example, smaller, more performant models are often trained on smaller amounts of synthetic data from a highly performant teacher <cite class="ltx_cite ltx_citemacro_citep">(Villalobos &amp; Atkinson, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib231" title="">2023</a>; Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib105" title="">2022</a>)</cite>. These improvements dramatically improve performance but are currently completely ignored by compute thresholds since they don’t contribute to <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p1.1.2">training</span> FLOP.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">Increasing the context-length <cite class="ltx_cite ltx_citemacro_citep">(Xiong et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib245" title="">2023</a>)</cite> and retrieval augmented systems <cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib124" title="">2024</a>; Pozzobon et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib168" title="">2023b</a>; Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib127" title="">2020</a>)</cite> are additional examples of introducing additional computational overhead at test-time by increasing the number of tokens to process. Retrieval augmented models (RAG) have become a mainstay of state-of-art models yet are often introduced after training. Most RAG systems are critical for keeping models up-to-date with knowledge yet contribute minimal or no FLOP. Retrieval augmented models are particularly good at supplementing models with search capabilities or external knowledge, which can enhances risks which depend on up-to-date knowledge such as biorisk and cybersecurity threats.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.1">Additionally increasing the context length often requires minimal FLOP but can dramatically increase performance of a model. Entire books can be passed in at test time dramatically improving model performance on specialized tasks (Gemini has 2M context window) <cite class="ltx_cite ltx_citemacro_citep">(Xiong et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib245" title="">2023</a>)</cite>. This can make the number of FLOP irrelevant if sensitive biological data can be passed at inference time in a long-context window.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Difficulty Tracking FLOP across model lifecycle.</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">Increasingly, training a model falls into distinct stages that all confer different properties. For example, unsupervised pre-training dominates compute costs because the volume of data is typically in the trillions of tokens <cite class="ltx_cite ltx_citemacro_citep">(Cottier, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib48" title="">2023</a>; Heim, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib93" title="">2023</a>)</cite>. Following this, there is instruction finetuning, which confers the model the ability to follow instructions <cite class="ltx_cite ltx_citemacro_citep">(Singh et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib200" title="">2024a</a>)</cite> and then preference training <cite class="ltx_cite ltx_citemacro_citep">(Aakanksha et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib2" title="">2024</a>; Ahmadian et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib8" title="">2024</a>; Bai et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib23" title="">2022</a>; Ouyang et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib158" title="">2022</a>; Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib123" title="">2023</a>; Tunstall et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib227" title="">2023</a>; Khalifa et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib115" title="">2021</a>; Rafailov et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib174" title="">2023</a>; Azar et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib21" title="">2023</a>)</cite>, which aligns model performance with human values. Between each of these steps models are often released publicly <cite class="ltx_cite ltx_citemacro_citep">(Üstün et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib255" title="">2024</a>; Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib224" title="">2023</a>; Aryabumi et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib20" title="">2024</a>)</cite>, meaning that developers can take a model from a different developer and continue optimizing. The models with the most downloads on platforms like HuggingFace are base models which are most conducive for continued pre-training. As sharing of models at different stages of the life-cycle becomes more common, so will difficulties in tallying FLOP across the entire model life-cycle. Furthermore, it may simply be infeasible to trace federated, decentralized training of models where hardware often belongs to many different participants and training is conducted in a privacy-preserving manner <cite class="ltx_cite ltx_citemacro_citep">(Don-Yehiya et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib62" title="">2023</a>; Borzunov et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib32" title="">2023</a>; Yuan et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib248" title="">2023</a>; Qin et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib170" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>How to handle Mixture of Experts (MoEs) and classic ensembling?</h4>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">MoEs <cite class="ltx_cite ltx_citemacro_citep">(Zadouri et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib249" title="">2023</a>; Shazeer et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib196" title="">2018</a>; Riquelme et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib181" title="">2021</a>; Du et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib63" title="">2022</a>; Fedus et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib78" title="">2022</a>; Tan et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib212" title="">2024</a>)</cite> are examples of adaptive compute – where examples are routed to different parts of a model. This type of architecture can often provide powerful efficiency gains, as despite a much larger overall architecture, only a subset of weights are activated for a given example. Current policy frameworks do clearly not specify how to handle Mixture of Experts (MoEs), which constitute some of the most highly performant systems currently deployed, such as Mixtral <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib109" title="">2024</a>)</cite> and the Gemini family of models <cite class="ltx_cite ltx_citemacro_citep">(Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib215" title="">2024a</a>)</cite>. However, this raises important questions – should the compute for each expert be counted towards total FLOP, or only the FLOP used to train the subset of experts that are active at inference time? Given final performance depends on all experts in an MoE, a recommendation should be to include all FLOP in the final consideration, but this is currently under-specified. It also raises the question of how to treat new <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS3.p1.1.1">hybrid techniques</em> which train several specialized experts and then both average parameters and utilize routing <cite class="ltx_cite ltx_citemacro_citep">(Sukhbaatar et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib209" title="">2024</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.sf1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.sf1.3.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text ltx_font_bold" id="S3.F5.sf1.4.2" style="font-size:90%;">All ML Models 2010-24<span class="ltx_text ltx_font_medium" id="S3.F5.sf1.4.2.1">
</span></span></figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="435" id="S3.F5.sf1.g1" src="x3.png" width="730"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.sf2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.sf2.3.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text ltx_font_bold" id="S3.F5.sf2.4.2" style="font-size:90%;">Notable ML Models 2010-24</span></figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="429" id="S3.F5.sf2.g1" src="x4.png" width="759"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.4.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.5.2" style="font-size:90%;">Different modalities have very different compute requirements <span class="ltx_text ltx_font_bold" id="S3.F5.5.2.1">Right:</span> A plot of all models tracked in the Epoch AI database. While model size has grown overall, some domains are far more prone to scaling such as language. <span class="ltx_text ltx_font_bold" id="S3.F5.5.2.2">Left:</span> We also plot the boxplot distribution for systems that Epoch AI classifies as notable for the same period of time (2010-24) and see pronounced differences in the distributions between modalities. Language models have many training compute outliers, whereas notable systems from vision, biology, and image generation models tend to be characterized by models that require far fewer training FLOP <cite class="ltx_cite ltx_citemacro_citep">(Epoch AI, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib69" title="">2024</a>)</cite></span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S3.F5.6">.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S3.SS1.SSS3.p2">
<p class="ltx_p" id="S3.SS1.SSS3.p2.1">Classical <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS3.p2.1.1">simple ensembling techniques</em> dominate production systems in the real world <cite class="ltx_cite ltx_citemacro_citep">(Ko et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib116" title="">2023</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib128" title="">2024</a>)</cite> and have been shown to heavily outperform a single model. Unlike MoEs which are jointly optimized or trained using a router, classic ensembles are often only combined at inference time using simple averaging of weights. Given the ensemble is never trained together, it is unclear whether FLOP should reflect the compute of the single final model or the sum of all the training compute across models that were averaged. If it only reflects the FLOP of the final model, this may underestimate risk given ensembling is known to improve performance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.4 </span>FLOP only accounts for a single model, but does not capture risk of the overall system.</h4>
<div class="ltx_para" id="S3.SS1.SSS4.p1">
<p class="ltx_p" id="S3.SS1.SSS4.p1.1">The emphasis on compute thresholds as an indicator of risk also implies that risk is the property of a single model rather than the system in which it is deployed. In the real-world, impact and risk are rarely attributable to a single model but are a facet of the entire system a model sits in and the way it interacts with its environment <cite class="ltx_cite ltx_citemacro_citep">(Zaharia et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib250" title="">2024</a>; Sculley et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib189" title="">2015</a>; Jatho et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib108" title="">2023</a>; Raji et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib176" title="">2020</a>)</cite>. Many real-world production systems are made up of cascading models where the final output is produced as a results of inputs being processed by multiple algorithms in sequence <cite class="ltx_cite ltx_citemacro_citep">(Paleyes et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib159" title="">2022</a>; Forum, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib79" title="">2023</a>; Sculley et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib189" title="">2015</a>; Shankar et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib195" title="">2022</a>)</cite>. There has yet to be guidance on whether the FLOP threshold is specific to a single model or whether all models that constitute an end-to-end system contribute to the final tally. This has significant implications for model providers – a cascade system is often made up of models which are not individually very powerful or risky – yet the overall system may exceed the FLOP threshold.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS4.p2">
<p class="ltx_p" id="S3.SS1.SSS4.p2.1">There is also no specification as to how to treat model agents which may interact with both each other and/or use tools. End performance of the agents is undoubtedly due to the interactions with other agents and access to tools <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib128" title="">2024</a>)</cite>, yet is unlikely to be considered a single model. It has already been shown that models which are enabled with tool use, or can interact with a wider environment outperform a single model on its own <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib234" title="">2023b</a>; Anwar et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib16" title="">2024a</a>; Mialon et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib146" title="">2023</a>)</cite>. These are far from edge cases; the reality is that most technology deployed in the wild is rarely just an algorithm is isolation. Typically, interdependent models feed into a user experience and interact with a set of choices about design and delivery that impact the overall level of risk.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.5 </span>FLOP varies dramatically across different modalities.</h4>
<div class="ltx_para" id="S3.SS1.SSS5.p1">
<p class="ltx_p" id="S3.SS1.SSS5.p1.1">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S3.F5.sf2" title="Figure 5(b) ‣ Figure 5 ‣ 3.1.3 How to handle Mixture of Experts (MoEs) and classic ensembling? ‣ 3.1 Challenges of using FLOP as a metric ‣ 3 Avoiding a FLOP FLOP ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_tag">5(b)</span></a>, we plot the FLOP requirements over time of models grouped according to modality and downstream use case (model FLOP data from <cite class="ltx_cite ltx_citemacro_citet">Epoch AI (<a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib69" title="">2024</a>)</cite>). It is easy to observe that the compute requirements have not increased at the same rate across modalities. For example, code models typically require less compute <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib131" title="">2024b</a>)</cite>, as do biological models <cite class="ltx_cite ltx_citemacro_citep">(Maug et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib143" title="">2024</a>)</cite>.
Multilingual models <cite class="ltx_cite ltx_citemacro_citep">(Üstün et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib255" title="">2024</a>; Aryabumi et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib20" title="">2024</a>)</cite> tend to require more compute for each additional language covered. This is often referred to as the <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS5.p1.1.1">curse of multilinguality</span> <cite class="ltx_cite ltx_citemacro_citep">(Üstün et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib255" title="">2024</a>; Arivazhagan et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib18" title="">2019</a>; Conneau et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib47" title="">2019</a>; Pfeiffer et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib166" title="">2022</a>)</cite>, where capacity is split between more languages such that performance on any given language suffers relative to a monolingual (single language) model of the same size. These differing compute needs mean that a single threshold may penalize some types of models and reward others. For example, thresholds may penalize multilingual models that attempt to serve many languages and improve access to technology <cite class="ltx_cite ltx_citemacro_citep">(Üstün et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib255" title="">2024</a>; Aryabumi et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib20" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS5.p2">
<p class="ltx_p" id="S3.SS1.SSS5.p2.1">One way to address differences in modalities is to maintain different compute thresholds for each modality. While at first glance this is an attractive solution, it also imposes more technical overhead on governments who must correctly set a hard-coded benchmark for each modality.</p>
</div>
<figure class="ltx_figure ltx_align_floatleft" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="362" id="S3.F6.g1" src="extracted/5762747/images/1985_02_BYTE_10-02_Computing_and_the_Sciences_0000.jpg" width="281"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.4.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F6.5.2" style="font-size:90%;">Bytes Magazine Cover, Volume 10, 1985.<span class="ltx_text ltx_font_medium" id="S3.F6.5.2.1"> A key difficulty setting compute thresholds is that different domains and downstream tasks (language, vision, biology) demand very different levels of training compute, and so one compute threshold is not suitable to <span class="ltx_text ltx_font_italic" id="S3.F6.5.2.1.1">rule them all</span>. This imposes more technical overhead on governments who must correctly set a hard-coded benchmark for each area. Only one domain specific compute threshold has been set to-date, by the EO for biological models. However, it has already been surpassed by several models that do not clearly present more risk than previous generations so may have been set too low.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS5.p3">
<p class="ltx_p" id="S3.SS1.SSS5.p3.2">For example, it is interesting to note that the US Executive Order already has at least one modality-specific caveat to the compute thresholds by carving out a separate compute threshold for biological models. It is set lower for models trained for biological sequence data at <math alttext="10^{23}" class="ltx_Math" display="inline" id="S3.SS1.SSS5.p3.1.m1.1"><semantics id="S3.SS1.SSS5.p3.1.m1.1a"><msup id="S3.SS1.SSS5.p3.1.m1.1.1" xref="S3.SS1.SSS5.p3.1.m1.1.1.cmml"><mn id="S3.SS1.SSS5.p3.1.m1.1.1.2" xref="S3.SS1.SSS5.p3.1.m1.1.1.2.cmml">10</mn><mn id="S3.SS1.SSS5.p3.1.m1.1.1.3" xref="S3.SS1.SSS5.p3.1.m1.1.1.3.cmml">23</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS5.p3.1.m1.1b"><apply id="S3.SS1.SSS5.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS5.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS5.p3.1.m1.1.1.1.cmml" xref="S3.SS1.SSS5.p3.1.m1.1.1">superscript</csymbol><cn id="S3.SS1.SSS5.p3.1.m1.1.1.2.cmml" type="integer" xref="S3.SS1.SSS5.p3.1.m1.1.1.2">10</cn><cn id="S3.SS1.SSS5.p3.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS5.p3.1.m1.1.1.3">23</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS5.p3.1.m1.1c">10^{23}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS5.p3.1.m1.1d">10 start_POSTSUPERSCRIPT 23 end_POSTSUPERSCRIPT</annotation></semantics></math>. However, since the threshold was set, models like xTrimoPGLM <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib41" title="">2024</a>)</cite> already exceed the biological threshold set at <math alttext="1e23" class="ltx_Math" display="inline" id="S3.SS1.SSS5.p3.2.m2.1"><semantics id="S3.SS1.SSS5.p3.2.m2.1a"><mrow id="S3.SS1.SSS5.p3.2.m2.1.1" xref="S3.SS1.SSS5.p3.2.m2.1.1.cmml"><mn id="S3.SS1.SSS5.p3.2.m2.1.1.2" xref="S3.SS1.SSS5.p3.2.m2.1.1.2.cmml">1</mn><mo id="S3.SS1.SSS5.p3.2.m2.1.1.1" xref="S3.SS1.SSS5.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS1.SSS5.p3.2.m2.1.1.3" xref="S3.SS1.SSS5.p3.2.m2.1.1.3.cmml">e</mi><mo id="S3.SS1.SSS5.p3.2.m2.1.1.1a" xref="S3.SS1.SSS5.p3.2.m2.1.1.1.cmml">⁢</mo><mn id="S3.SS1.SSS5.p3.2.m2.1.1.4" xref="S3.SS1.SSS5.p3.2.m2.1.1.4.cmml">23</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS5.p3.2.m2.1b"><apply id="S3.SS1.SSS5.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS5.p3.2.m2.1.1"><times id="S3.SS1.SSS5.p3.2.m2.1.1.1.cmml" xref="S3.SS1.SSS5.p3.2.m2.1.1.1"></times><cn id="S3.SS1.SSS5.p3.2.m2.1.1.2.cmml" type="integer" xref="S3.SS1.SSS5.p3.2.m2.1.1.2">1</cn><ci id="S3.SS1.SSS5.p3.2.m2.1.1.3.cmml" xref="S3.SS1.SSS5.p3.2.m2.1.1.3">𝑒</ci><cn id="S3.SS1.SSS5.p3.2.m2.1.1.4.cmml" type="integer" xref="S3.SS1.SSS5.p3.2.m2.1.1.4">23</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS5.p3.2.m2.1c">1e23</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS5.p3.2.m2.1d">1 italic_e 23</annotation></semantics></math> operations by a factor of 6x <cite class="ltx_cite ltx_citemacro_citep">(Maug et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib143" title="">2024</a>)</cite>. Many models <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib132" title="">2023</a>; Elnaggar et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib66" title="">2020</a>; Dalla-Torre et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib51" title="">2023</a>)</cite> are currently within a factor of 10x the Executive Order’s reporting threshold <cite class="ltx_cite ltx_citemacro_citep">(Maug et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib143" title="">2024</a>)</cite>. These models do not appear to present a decidedly different risk profile from previous generations, so if the goal of the thresholds is to be an inflection point for amplified risk it is unclear if it has been set successfully.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS5.p4">
<p class="ltx_p" id="S3.SS1.SSS5.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS5.p4.1.1">Specifying separate thresholds for different modalities risks inviting gamification.</span> For example, to avoid a lower threshold for scrutiny for biological models one loophole is to preserve biology specific training data at less than 50%. According to current guidance the model would no-longer qualify as a “biological” model and would only be subject to the higher general purpose compute thresholds. Galactica-120B <cite class="ltx_cite ltx_citemacro_citep">(Taylor et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib214" title="">2022</a>)</cite> and Llama-molinst-protein-7b <cite class="ltx_cite ltx_citemacro_citep">(Fang et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib76" title="">2024b</a>)</cite> are both examples of models with capabilities for biological sequence modeling without primarily being trained on biological sequence data. Despite both presenting biological capabilities, neither is likely to be considered “biological” under the current Executive Order requirements <cite class="ltx_cite ltx_citemacro_citep">(Maug et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib143" title="">2024</a>)</cite>. This highlights the fundamental tension of relying on compute alone – since it is not anchored to the risk metric that is of primary concern, it may be possible to sidestep in many creative ways while still presenting high-risk capabilities.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS5.p5">
<p class="ltx_p" id="S3.SS1.SSS5.p5.1">In Appendix <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#A1" title="Appendix A Technical Challenges of Measuring FLOP ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_tag">A</span></a>, we also present some more technical aspects of the difficulty of measuring FLOP in practice, such as the difference between theoretical and hardware FLOP, and how to handle difference in quantization. Developing principled standards for measuring FLOP is essential for ensuring that safety measures are applied in a proportionate and appropriate way.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>We are not very good at predicting the relationship between compute and risk</h2>
<div class="ltx_para" id="S4.p1">
<blockquote class="ltx_quote ltx_epigraph" id="S4.p1.1" style="width:281.85pt; margin-left:auto;;">
<div class="ltx_block ltx_epigraph_text" id="S4.p1.1.1" style="text-align:left; ;">
<p class="ltx_p" id="S4.p1.1.1.1"><span class="ltx_text" id="S4.p1.1.1.1.1" style="font-size:90%;">In theory, there is no difference between theory and practice. But, in practice, there is.</span></p>
</div>
<div class="ltx_block ltx_epigraph_source" id="S4.p1.1.2" style="border-top:solid 0.4pt; text-align:right; ;">
<p class="ltx_p" id="S4.p1.1.2.1"><span class="ltx_text ltx_font_italic" id="S4.p1.1.2.1.1" style="font-size:90%;">Walter J. Savitch</span></p>
</div>
</blockquote>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">The choice of where compute thresholds are set will have far-ranging implications – too low and too many models will be selected for additional auditing and benchmarking each year. In contrast, if it is set too high, not enough models will be audited for risk, and the threshold risks become decorative rather than a meaningful indicator of risk. None of the policies to date have provided justification about where they have set their thresholds, or why it excludes almost all models deployed in the wild today. In Section <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S2.SS1" title="2.1 A shift in the relationship between compute and performance ‣ 2 The Uncertain Relationship Between Compute and Risk. ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_tag">2.1</span></a>, we grappled with the changing overall relationship between compute and performance. However, scientific justification for a threshold requires predicting how downstream risk scales with additional compute. Indeed, ideally the choice of hard coded threshold reflects scientific consensus as to when particular risk factors are expected to emerge due to scale. Hence, it is worth considering our success to date in estimating how different model properties change with scale.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Warren Buffet once said <span class="ltx_text ltx_font_italic" id="S4.p3.1.1">“Don’t ask the barber if you need a haircut.”</span> In the same vein, don’t ask a computer scientist or economist whether you can predict the future. The temptation to say yes often overrides a necessary humility about what can and cannot be predicted accurately. One such area where hubris has overridden common sense is attempts to predict the relationship between scale and performance in the form of <span class="ltx_text ltx_font_italic" id="S4.p3.1.2">scaling laws</span> <cite class="ltx_cite ltx_citemacro_citep">(Kaplan et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib113" title="">2020</a>; Hernandez et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib96" title="">2021</a>; Dhariwal et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib61" title="">2021</a>)</cite> which either try and predict how a model’s pre-training loss scales <cite class="ltx_cite ltx_citemacro_citep">(Bowman, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib34" title="">2023</a>)</cite> or how downstream properties emerge with scale. It is the latter task which is urgently needed by policymakers in order to anticipate the emergence of unsafe capabilities and inform restrictions (such as compute thresholds) at inflection points where risk increases with scale <cite class="ltx_cite ltx_citemacro_citep">(Anthropic, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib15" title="">2023</a>; OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib156" title="">2023</a>; Kaminski, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib111" title="">2023</a>)</cite>.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS0.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.0.1 </span>Limitations of scaling laws.</h4>
<div class="ltx_para" id="S4.SS0.SSS1.p1">
<p class="ltx_p" id="S4.SS0.SSS1.p1.1">One of the biggest limitations of scaling laws is that they have only been shown to hold when predicting a model’s pre-training test loss <cite class="ltx_cite ltx_citemacro_citep">(Bowman, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib34" title="">2023</a>)</cite>, which measures the model’s ability to correctly predict how an incomplete piece of text will be continued. Indeed, when actual performance on downstream tasks is used, the results are often murky or inconsistent <cite class="ltx_cite ltx_citemacro_citep">(Ganguli et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib83" title="">2022</a>; Schaeffer et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib184" title="">2023</a>; Anwar et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib17" title="">2024b</a>; Ganguli et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib83" title="">2022</a>; Schaeffer et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib186" title="">2024b</a>; Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib104" title="">2024</a>)</cite>. Indeed, the term <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS1.p1.1.1">emerging properties</span> is often used to describe this discrepancy <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib236" title="">2022</a>; Srivastava et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib203" title="">2023</a>)</cite>: a property that appears “suddenly” as the complexity of the system increases and cannot be predicted. Emergent properties imply that scaling laws don’t hold when you try to predict downstream performance instead of predicting test loss for the next word token.</p>
</div>
<div class="ltx_para" id="S4.SS0.SSS1.p2">
<p class="ltx_p" id="S4.SS0.SSS1.p2.1">Even when limited to predicting test loss, there have been issues with replicability of scaling results under slightly different assumptions about the distribution <cite class="ltx_cite ltx_citemacro_citep">(Besiroglu et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib28" title="">2024</a>; Anwar et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib16" title="">2024a</a>)</cite>. Research has also increasingly found that many downstream capabilities display irregular scaling curves <cite class="ltx_cite ltx_citemacro_citep">(Srivastava et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib203" title="">2023</a>)</cite> or non power-law scaling <cite class="ltx_cite ltx_citemacro_citep">(Caballero et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib37" title="">2023</a>)</cite>. For complex systems that require projecting into the future, small errors end up accumulating due to time step dependencies being modelled. This makes accurate predictions of when risks will emerge inherently hard, which is compounded by the small samples sizes often available for analysis. each data point is a model, and computation cost means scaling “laws” are frequently based upon analysis of less than 100 data points <cite class="ltx_cite ltx_citemacro_citep">(Ruan et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib182" title="">2024</a>)</cite>). This means many reported power law relationships can lack statistical support and power <cite class="ltx_cite ltx_citemacro_citep">(Stumpf &amp; Porter, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib208" title="">2012</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS0.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.0.2 </span>Critical to specify the time horizon of interest.</h4>
<div class="ltx_para" id="S4.SS0.SSS2.p1">
<p class="ltx_p" id="S4.SS0.SSS2.p1.1">One immediate recommendation is that the accuracy of scaling laws and predictions of emerging risk can be greatly improved by more guidance from policymakers about what range is of interest and specifying the risks that policymakers are concerned about <cite class="ltx_cite ltx_citemacro_citep">(Stumpf &amp; Porter, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib208" title="">2012</a>)</cite>. For example, there is a big difference between using scaling laws to optimize for the correct amount of training data in your next large-scale run versus attempting to extrapolate trends several orders of magnitude out. Typically, policy use cases demand high precision over a longer time horizon, which is exactly the type of extrapolation we are currently worst at. Specifying which risks are of interest will also benefit precision; scaling laws tend to have high variance in precision between tasks. For example, code-generation has shown fairly predictable power law scaling across 10 orders of magnitude of compute <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib104" title="">2024</a>; Anwar et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib17" title="">2024b</a>)</cite>. However, other capabilities have been far shown to scale far more erratically <cite class="ltx_cite ltx_citemacro_citep">(Srivastava et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib203" title="">2023</a>; Caballero et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib37" title="">2023</a>)</cite>. Perhaps as important, policymakers should be aware that accurately predicting the impact of scaling is currently far from feasible. Hence, there is currently limited scientific support for using exact thresholds of compute alone to triage different risk levels.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>The Way Forward</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Moving Away from Hard Coded Compute Thresholds</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.2">Compute thresholds to date propose a single number (<math alttext="10^{26}" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><msup id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mn id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">10</mn><mn id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml">26</mn></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">superscript</csymbol><cn id="S5.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S5.SS1.p1.1.m1.1.1.2">10</cn><cn id="S5.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S5.SS1.p1.1.m1.1.1.3">26</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">10^{26}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">10 start_POSTSUPERSCRIPT 26 end_POSTSUPERSCRIPT</annotation></semantics></math> or <math alttext="10^{25}" class="ltx_Math" display="inline" id="S5.SS1.p1.2.m2.1"><semantics id="S5.SS1.p1.2.m2.1a"><msup id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml"><mn id="S5.SS1.p1.2.m2.1.1.2" xref="S5.SS1.p1.2.m2.1.1.2.cmml">10</mn><mn id="S5.SS1.p1.2.m2.1.1.3" xref="S5.SS1.p1.2.m2.1.1.3.cmml">25</mn></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><apply id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">superscript</csymbol><cn id="S5.SS1.p1.2.m2.1.1.2.cmml" type="integer" xref="S5.SS1.p1.2.m2.1.1.2">10</cn><cn id="S5.SS1.p1.2.m2.1.1.3.cmml" type="integer" xref="S5.SS1.p1.2.m2.1.1.3">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">10^{25}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.2.m2.1d">10 start_POSTSUPERSCRIPT 25 end_POSTSUPERSCRIPT</annotation></semantics></math>) to distinguish risky systems which merit more scrutiny. This hard-coding of a single threshold reflects a philosophy of absolutism, a legal and philosophical view that at <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.2.1">least some truths in the relevant domain apply to all times, places or social and cultural frameworks.</span> From a data-centric perspective, absolutism makes sense as a governance philosophy when the data distribution is well known and follows a predictable statistical pattern. For example, the use thresholds in medicine for classifying diabetes detection <cite class="ltx_cite ltx_citemacro_citep">(Saudek et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib183" title="">2008</a>)</cite> or for allocating additional care to infants based upon birth weight <cite class="ltx_cite ltx_citemacro_citep">(Cutland et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib50" title="">2017</a>; Seri &amp; Evans, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib192" title="">2008</a>)</cite>. These hard-coded thresholds have stood the test of time because these data distributions tend to be well-behaved and predictable.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">In your introduction to machine learning class, this type of bell-shaped distribution was introduced to you as a <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.1">normal distribution</span>. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S5.F8" title="Figure 8 ‣ 5.2 The case for dynamic thresolds. ‣ 5 The Way Forward ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_tag">8</span></a>, we plot some very common examples of close to normal distributions found in the wild. Unlike other distributions, the normal distribution is well-behaved and remarkably symmetrical, with an equal number of outliers on each side. Normal distributions in the real world also tend to coincide with distributions that don’t change much over time. For example, the distribution of baby weights is unlikely to change tomorrow or even in the next 10 years. For these type of stable distributions where the data is well behaved hard thresholds make sense as a governance tool. The stability of these distributions make it easy to determine outliers and have confidence that a set threshold will have longevity and not have to change every year. There are successful examples of governments setting hard thresholds when they designate speed limits <cite class="ltx_cite ltx_citemacro_citep">(US Department of Transportation, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib229" title="">2020</a>)</cite> or limits for blood alcohol to determine drinking under the influence <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib242" title="">World Health Organization, </a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">In contrast, we know from Section <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S2.SS1" title="2.1 A shift in the relationship between compute and performance ‣ 2 The Uncertain Relationship Between Compute and Risk. ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_tag">2.1</span></a> that one of the most misbehaved and rapidly changing distributions is the relationship between compute and performance. The plots in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S5.F7" title="Figure 7 ‣ 5.2 The case for dynamic thresolds. ‣ 5 The Way Forward ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_tag">7</span></a> show that if we plot any proxy variable for compute – parameters, FLOP, training dataset size, training time – we are confronted with a distribution that is far from the perfect bell-shaped curve that characterize the kinds of problems that hard-coded thresholds are successfully applied to. <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Some other examples of misbehaved real-world distributions include the amount of information on the internet <cite class="ltx_cite ltx_citemacro_citep">(Lyman &amp; Varian, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib138" title="">2003</a>)</cite>, CEO salaries <cite class="ltx_cite ltx_citemacro_citep">(Frydman &amp; Molloy, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib82" title="">2007</a>)</cite>, the size of clouds <cite class="ltx_cite ltx_citemacro_citep">(DeWitt et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib60" title="">2024</a>)</cite>, or internet searches for certain keywords, actors or movies over time <cite class="ltx_cite ltx_citemacro_citep">(Adamic &amp; Huberman, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib5" title="">2001</a>)</cite></span></span></span> Perhaps more dangerous, these non-normal distributions are also more likely to rapidly shift over time. For these distributions, applying a hard-coded threshold is a bad policy as there is a much higher likelihood that the threshold will be placed incorrectly. As quoted by the Mathematician David Orrell, <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.1">Orthodox tools based on a normal distribution therefore fail exactly where they are most needed, at the extremes.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>The case for dynamic thresolds.</h3>
<div class="ltx_para" id="S5.SS2.p1">
<blockquote class="ltx_quote ltx_epigraph" id="S5.SS2.p1.1" style="width:281.85pt; margin-left:auto;;">
<div class="ltx_block ltx_epigraph_text" id="S5.SS2.p1.1.1" style="text-align:left; ;">
<p class="ltx_p" id="S5.SS2.p1.1.1.1"><span class="ltx_text" id="S5.SS2.p1.1.1.1.1" style="font-size:90%;">A measurement is not an absolute thing, but only relates one entity to another.</span></p>
</div>
<div class="ltx_block ltx_epigraph_source" id="S5.SS2.p1.1.2" style="border-top:solid 0.4pt; text-align:right; ;">
<p class="ltx_p" id="S5.SS2.p1.1.2.1"><span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.2.1.1" style="font-size:90%;">H.T. Pledge</span></p>
</div>
</blockquote>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Compute thresholds could be much improved by moving to dynamic instread of static thresholds. An unpredictable relationship between compute and performance means that there will likely be false negatives when a hard threshold is set. That is, as smaller models become more performant, models which should be audited because of the risk they present avoid doing so because they fall underneath the threshold. Furthermore, it is likely that policymakers will constantly have to revisit and redefine a sensible threshold, which imposes technical overhead and creates issues with credibility.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">Sophist Protagora (c. 485-410 B.C.) said <span class="ltx_text ltx_font_italic" id="S5.SS2.p3.1.1">Man is the measure of all things</span>, implying that most of how we arrive at judgement is based upon relative perception. Instead of leveraging hard-coded thresholds, in the face of unknown distributions, it is more sensible to have relative approaches for auditing that are easier to adapt over time <cite class="ltx_cite ltx_citemacro_citep">(Reuel &amp; Undheim, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib177" title="">2024</a>)</cite>. In practice, there are plenty of historical examples where government policy defaults to dynamic automatically adjusting tools to address rapidly changing distributions. For example, the U.S. government adjusts the dollar threshold for exempt consumer credit transactions annually based on the Consumer Price Index for Urban Wage Earners and Clerical Workers (CPI-W). There are also dynamic thresholds for identifying systemic banking crises using ratios <cite class="ltx_cite ltx_citemacro_citep">(Bordley, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib31" title="">2014</a>)</cite>, including credit-to-GDP. <cite class="ltx_cite ltx_citemacro_citep">(Lund-Jensen, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib137" title="">2012</a>)</cite>. The European Union avoids hardcoding definitions of poverty by instead defining an at-risk-of-poverty threshold at 60% of the median equivalized disposable income <cite class="ltx_cite ltx_citemacro_citep">(Office, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib153" title="">2024</a>)</cite>. This allows it to adjust as wages grow dynamically over time. A dynamic threshold for compute could focus auditing resources on the top 5-10 percentile of models ranked according to an index of metrics (consisting of more than compute) that serve as a proxy for risk.</p>
</div>
<figure class="ltx_figure" id="S5.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F7.sf1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S5.F7.sf1.3.2" style="font-size:90%;">Compute</span></figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="380" id="S5.F7.sf1.g1" src="x5.png" width="551"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F7.sf2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S5.F7.sf2.3.2" style="font-size:90%;">Dataset Size</span></figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="380" id="S5.F7.sf2.g1" src="x6.png" width="551"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F7.sf3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S5.F7.sf3.3.2" style="font-size:90%;">Train Time</span></figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="380" id="S5.F7.sf3.g1" src="x7.png" width="551"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F7.sf4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.sf4.2.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S5.F7.sf4.3.2" style="font-size:90%;">Model Size</span></figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="380" id="S5.F7.sf4.g1" src="x8.png" width="551"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.3.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S5.F7.4.2" style="font-size:90%;">The distribution of model attributes from models designated as notable AI systems by Epoch AI <cite class="ltx_cite ltx_citemacro_citep">(Epoch AI, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib69" title="">2024</a>)</cite>. All of these properties are heavily skewed, with a non-normal distribution. Note the histogram axis is set to log scale. This skew and the rapidly changing nature of these properties over time, makes it hard to apply a hard-coded threshold with confidence. <span class="ltx_text ltx_font_bold" id="S5.F7.4.2.1">For rapidly changing distributions, dynamic automatically adjusting thresholds have historically been more successful as a policy tool.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="S5.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F8.sf1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F8.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S5.F8.sf1.3.2" style="font-size:90%;">Chest</span></figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="41" id="S5.F8.sf1.g1" src="x9.png" width="68"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F8.sf2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F8.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S5.F8.sf2.3.2" style="font-size:90%;">Blood Pressure</span></figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="41" id="S5.F8.sf2.g1" src="x10.png" width="68"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F8.sf3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F8.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S5.F8.sf3.3.2" style="font-size:90%;">Heart Rate</span></figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="224" id="S5.F8.sf3.g1" src="x11.png" width="374"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F8.sf4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F8.sf4.2.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S5.F8.sf4.3.2" style="font-size:90%;">Baby Weights</span></figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="41" id="S5.F8.sf4.g1" src="x12.png" width="68"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S5.F8.3.2" style="font-size:90%;">Many natural phenomena follow reasonably close to a normal distribution, such as chest circumference <cite class="ltx_cite ltx_citemacro_citep">(Quetelet, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib172" title="">1817</a>)</cite>, diastolic blood pressure <cite class="ltx_cite ltx_citemacro_citep">(Musameh et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib149" title="">2017</a>)</cite>, resting heart rate <cite class="ltx_cite ltx_citemacro_citep">(Quer et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib171" title="">2020</a>)</cite>, distribution of baby weights <cite class="ltx_cite ltx_citemacro_citep">(Shen et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib197" title="">2014</a>)</cite>. Historically, hard coded static thresholds work well with normal distributions because the data is well-behaved and predictable.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">Switching to dynamic thresholds would also mean current harms are not neglected</span>. Using a percentile threshold based upon annual reporting would also ensure a guaranteed number of models with relatively higher estimated risk receive additional scrutiny every year. This would ensure that thresholds don’t become decorative and only applied to future models, but also apply to models currently deployed that are outliers relative to their peer group. Having a predictable number of models that receive additional scrutiny also helps build up needed technical muscle within recently created safety institutes around the world that have varying levels of technical expertise <cite class="ltx_cite ltx_citemacro_citep">(Zakrzewski, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib251" title="">2024</a>; Aitken et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib11" title="">2022</a>; Engstrom et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib67" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p5.1.1">Percentile based measures can also take into account differences in modality</span> Given the large variance in compute FLOP across modalities, AI regulators should also look to the rich body of work on reference class forecasting <cite class="ltx_cite ltx_citemacro_citep">(Baerenbold, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib22" title="">2023</a>)</cite>, where forecasts are only made relative to similar basket of goods. For example, if you wanted to predict how long it takes to read a history textbook, it is less informative to take the average reading time for all books in the world and likely more precise to restrict to similar history books. This is already done when setting property prices (takes into account local neighborhoods) and assessing risk on financial assets. In turn, policymakers could consider grouping models by whether they are general purpose in intent or domain-specialized (biological model for example). This should again be combined with additional metrics as FLOP is insufficient and be implemented as a dynamic threshold to avoid the technical overhead of continual adjustments of several hard coded thresholds.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Compute should not be used alone as a proxy for risk.</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">In 1928, the Soviet Union embarked on a set of 5-year plans where the government set specific targets for industrial output, agricultural production, and other economic indicators <cite class="ltx_cite ltx_citemacro_citep">(Erlich, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib71" title="">1967</a>)</cite>. The metrics for success where defined almost entirely by the <em class="ltx_emph ltx_font_italic" id="S5.SS3.p1.1.1">quantity</em> of goods built, rather than the <em class="ltx_emph ltx_font_italic" id="S5.SS3.p1.1.2">quality</em>. This under-specification led to decades of commendable success in growth of production, but extremely low quality output which was often immediately discarded <cite class="ltx_cite ltx_citemacro_citep">(Duda, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib64" title="">2023</a>)</cite>. In the same vein, a clear takeaway is that compute cannot be used as the only indicator of risk.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Even if we limit our purview to future risks like cyber- and bio-risk, it is unclear compute thresholds are viable. This is both because we are not good at predicting what capabilities emerge with scaling (Section <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S4" title="4 We are not very good at predicting the relationship between compute and risk ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_tag">4</span></a>) and because the relationship is fundamentally changing between training compute and performance (Section <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S2.SS1" title="2.1 A shift in the relationship between compute and performance ‣ 2 The Uncertain Relationship Between Compute and Risk. ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_tag">2.1</span></a>). Dynamic compute thresholds will not resolve all these limitations. One recommendation is that any threshold is done based upon a basket of metrics that inform an index of risk. Here, policymakers being transparent about what risks are of concern helps inform more precise selection of benchmarks. For example, if concern about future risks like bio-risk is indeed top-of-mind, then specialized benchmarks that capture these risks are far more useful. Additionally, one could imagine complementing this index with some measure of general performance such as ranking by quality of open-ended responses <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib42" title="">2024</a>)</cite>. This dilutes reliance on the limitations on single metric – another recommendation is that the index be allowed to evolve over time to account for changes in risks governments are concerned about.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p3.1.1">FLOP as a metric has to be better specified to be meaningful</span> Even if compute as measured by FLOP remains one metric in an overall index to profile risk, it has to be better specified to be meaningful. The existing legislation does not specify key details around FLOP – how to deal with quantized weights, mixture of expert models, fractured pre-training. This will increasingly pose issues as these inference time optimizations result in gains in performance without any associated increase in FLOP. The use of FLOP can be greatly strengthened by standardizing technical specifications.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Parting Thoughts</h3>
<div class="ltx_para" id="S5.SS4.p1">
<blockquote class="ltx_quote ltx_epigraph" id="S5.SS4.p1.1" style="width:281.85pt; margin-left:auto;;">
<div class="ltx_block ltx_epigraph_text" id="S5.SS4.p1.1.1" style="text-align:left; ;">
<p class="ltx_p" id="S5.SS4.p1.1.1.1"><span class="ltx_text" id="S5.SS4.p1.1.1.1.1" style="font-size:90%;">Our knowledge of the ways things work, in society or nature, comes trailing clouds of vagueness. Vast ills have followed a belief in certainty.</span></p>
</div>
<div class="ltx_block ltx_epigraph_source" id="S5.SS4.p1.1.2" style="border-top:solid 0.4pt; text-align:right; ;">
<p class="ltx_p" id="S5.SS4.p1.1.2.1"><span class="ltx_text ltx_font_italic" id="S5.SS4.p1.1.2.1.1" style="font-size:90%;">Kenneth Arrow</span></p>
</div>
</blockquote>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.2">It is very hard to trace how compute thresholds gained such traction in a short amount of time over national and international governance of AI. Compute thresholds are striking because they have emerged with no clear scientific support for either the thresholds chosen at <math alttext="10^{26}" class="ltx_Math" display="inline" id="S5.SS4.p2.1.m1.1"><semantics id="S5.SS4.p2.1.m1.1a"><msup id="S5.SS4.p2.1.m1.1.1" xref="S5.SS4.p2.1.m1.1.1.cmml"><mn id="S5.SS4.p2.1.m1.1.1.2" xref="S5.SS4.p2.1.m1.1.1.2.cmml">10</mn><mn id="S5.SS4.p2.1.m1.1.1.3" xref="S5.SS4.p2.1.m1.1.1.3.cmml">26</mn></msup><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.1.m1.1b"><apply id="S5.SS4.p2.1.m1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS4.p2.1.m1.1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1">superscript</csymbol><cn id="S5.SS4.p2.1.m1.1.1.2.cmml" type="integer" xref="S5.SS4.p2.1.m1.1.1.2">10</cn><cn id="S5.SS4.p2.1.m1.1.1.3.cmml" type="integer" xref="S5.SS4.p2.1.m1.1.1.3">26</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.1.m1.1c">10^{26}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p2.1.m1.1d">10 start_POSTSUPERSCRIPT 26 end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="10^{25}" class="ltx_Math" display="inline" id="S5.SS4.p2.2.m2.1"><semantics id="S5.SS4.p2.2.m2.1a"><msup id="S5.SS4.p2.2.m2.1.1" xref="S5.SS4.p2.2.m2.1.1.cmml"><mn id="S5.SS4.p2.2.m2.1.1.2" xref="S5.SS4.p2.2.m2.1.1.2.cmml">10</mn><mn id="S5.SS4.p2.2.m2.1.1.3" xref="S5.SS4.p2.2.m2.1.1.3.cmml">25</mn></msup><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.2.m2.1b"><apply id="S5.SS4.p2.2.m2.1.1.cmml" xref="S5.SS4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS4.p2.2.m2.1.1.1.cmml" xref="S5.SS4.p2.2.m2.1.1">superscript</csymbol><cn id="S5.SS4.p2.2.m2.1.1.2.cmml" type="integer" xref="S5.SS4.p2.2.m2.1.1.2">10</cn><cn id="S5.SS4.p2.2.m2.1.1.3.cmml" type="integer" xref="S5.SS4.p2.2.m2.1.1.3">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.2.m2.1c">10^{25}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p2.2.m2.1d">10 start_POSTSUPERSCRIPT 25 end_POSTSUPERSCRIPT</annotation></semantics></math>, and largely only apply to future models. One key recommendation that emerges from this essay is that we should be transparent about what risks we are concerned about. This is both to allow everyday citizens to weigh in on how government resources are allocated and also to allow for needed scientific scrutiny as to whether compute thresholds are a successful protocol for estimating and mitigating risk.</p>
</div>
<div class="ltx_para" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.2"><span class="ltx_text ltx_font_bold" id="S5.SS4.p3.2.1">Any recommendation of compute as a metric to triage risk should be technically motivated by scientific evidence.</span> When policy is introduced, it is often hard to change. The initial values chosen by the Executive Order, as described by the Computer Scientist Suresh Venkatasubramanian had huge “signaling power” <cite class="ltx_cite ltx_citemacro_citep">(Karen Hao, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib114" title="">2023</a>)</cite> and likely influenced the default framing of discussion in the European Union that informed the EU Act. Given this intertia, it is even more critical that governance strategies like thresholds are motivated by scientific evidence. The choice of <math alttext="10^{26}" class="ltx_Math" display="inline" id="S5.SS4.p3.1.m1.1"><semantics id="S5.SS4.p3.1.m1.1a"><msup id="S5.SS4.p3.1.m1.1.1" xref="S5.SS4.p3.1.m1.1.1.cmml"><mn id="S5.SS4.p3.1.m1.1.1.2" xref="S5.SS4.p3.1.m1.1.1.2.cmml">10</mn><mn id="S5.SS4.p3.1.m1.1.1.3" xref="S5.SS4.p3.1.m1.1.1.3.cmml">26</mn></msup><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.1.m1.1b"><apply id="S5.SS4.p3.1.m1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS4.p3.1.m1.1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1">superscript</csymbol><cn id="S5.SS4.p3.1.m1.1.1.2.cmml" type="integer" xref="S5.SS4.p3.1.m1.1.1.2">10</cn><cn id="S5.SS4.p3.1.m1.1.1.3.cmml" type="integer" xref="S5.SS4.p3.1.m1.1.1.3">26</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.1.m1.1c">10^{26}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p3.1.m1.1d">10 start_POSTSUPERSCRIPT 26 end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="10^{25}" class="ltx_Math" display="inline" id="S5.SS4.p3.2.m2.1"><semantics id="S5.SS4.p3.2.m2.1a"><msup id="S5.SS4.p3.2.m2.1.1" xref="S5.SS4.p3.2.m2.1.1.cmml"><mn id="S5.SS4.p3.2.m2.1.1.2" xref="S5.SS4.p3.2.m2.1.1.2.cmml">10</mn><mn id="S5.SS4.p3.2.m2.1.1.3" xref="S5.SS4.p3.2.m2.1.1.3.cmml">25</mn></msup><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.2.m2.1b"><apply id="S5.SS4.p3.2.m2.1.1.cmml" xref="S5.SS4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS4.p3.2.m2.1.1.1.cmml" xref="S5.SS4.p3.2.m2.1.1">superscript</csymbol><cn id="S5.SS4.p3.2.m2.1.1.2.cmml" type="integer" xref="S5.SS4.p3.2.m2.1.1.2">10</cn><cn id="S5.SS4.p3.2.m2.1.1.3.cmml" type="integer" xref="S5.SS4.p3.2.m2.1.1.3">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.2.m2.1c">10^{25}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p3.2.m2.1d">10 start_POSTSUPERSCRIPT 25 end_POSTSUPERSCRIPT</annotation></semantics></math> rather than a number smaller or larger has not been justified in any of the policies implementing compute thresholds as a governance strategy. To motivate a compute threshold we should be able to articulate what risks we believe will be mitigated by investing in scrutiny of models at that threshold.</p>
</div>
<div class="ltx_para" id="S5.SS4.p4">
<p class="ltx_p" id="S5.SS4.p4.1">Given the wide adoption of compute thresholds across governance structures, scientific support seems necessary in the same way precautionary policies that aim to present harm from climate change <cite class="ltx_cite ltx_citemacro_citep">(448 U.S. 607, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib1" title="">1980</a>)</cite> or policies to improve public health <cite class="ltx_cite ltx_citemacro_citep">(Krimsky, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib119" title="">2005</a>)</cite> are justified after weighing the scientific evidence. Governments should invite technical reports from a variety of experts before adopting thresholds. If hard thresholds are chosen as part of national or international governance, they should be motivated by scientific consensus.</p>
</div>
<div class="ltx_para" id="S5.SS4.p5">
<p class="ltx_p" id="S5.SS4.p5.1">Policymakers face a formidable task ahead of them. What is humbling and, at times, overwhelming to ponder is that computer science as a discipline is incredibly young – it has been a mere 68 years since the Dartmouth workshop where the term <span class="ltx_text ltx_font_italic" id="S5.SS4.p5.1.1">Artificial Intelligence</span> was coined. Much remains to be discovered, and new tools will pose formidable risks and benefits. Perhaps one of the key takeaways of this essay, is that we must have necessary humility about our ability to predict the future. Compute thresholds are currently presented as a very rigid governance tool because of the emphasis on a single static number to tier risk. These types of estimates are prone to failure precisely because of how rapidly the landscape is changing. Instead, we should focus on flexible tools for monitoring risk that are not tied to static numbers. Furthermore, FLOP as a measure can be greatly improved by standardizing reporting and closing possible loopholes. In the previous Section <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S5.SS1" title="5.1 Moving Away from Hard Coded Compute Thresholds ‣ 5 The Way Forward ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_tag">5.1</span></a>, we discussed some of these recommendations. As to what comes next, the only certain thing is that something will come next. Perhaps fitting to conclude with a quote from Alan Turing <span class="ltx_text ltx_font_italic" id="S5.SS4.p5.1.2">“We can only see a short distance ahead, but we can see plenty there that needs to be done.”</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S6" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgments</h2>
<div class="ltx_para" id="S6.p1">
<blockquote class="ltx_quote ltx_epigraph" id="S6.p1.1" style="width:281.85pt; margin-left:auto;;">
<div class="ltx_block ltx_epigraph_text" id="S6.p1.1.1" style="text-align:left; ;">
<p class="ltx_p" id="S6.p1.1.1.1"><span class="ltx_text" id="S6.p1.1.1.1.1" style="font-size:90%;">Wisdom is like a baobab tree; no one individual can embrace it.</span></p>
</div>
<div class="ltx_block ltx_epigraph_source" id="S6.p1.1.2" style="border-top:solid 0.4pt; text-align:right; ;">
<p class="ltx_p" id="S6.p1.1.2.1"><span class="ltx_text ltx_font_italic" id="S6.p1.1.2.1.1" style="font-size:90%;">Ewe Proverb</span></p>
</div>
</blockquote>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Thank you to many of my wonderful colleagues and peers who took time to provide valuable feedback on earlier versions of this essay. I do not have much time to write or think deeply in isolation about a topic these days. My time is increasingly spent helping others create breakthroughs. However, I have greatly enjoyed the small parcels of time I have spent on this essay wrestling with these ideas. This essay felt important to write because it requires grappling with several topics that are timely: the changing relationship we have with compute, how we navigate the risks introduced by the technology we have helped build and how science should inform policy. We are in an interesting time; it is rare to see research progress that is adopted overnight. Computer science ideas do not just resonate in conference halls anymore, but profoundly impact the world around us. This merits accountability, evidence and care as we navigate this impact.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Thanks for valuable feedback from several colleagues across several drafts of this essay (in no particular order): <span class="ltx_text ltx_font_bold" id="S6.p3.1.1">Usman Anwar, Neil Thompson, Sanmi Koyejo, Helen Toner, Lennard Heim, Irene Solaiman, Shayne Longpre, Leshem Choshen, Sasha Luccioni, Stephen Casper, Jaime Sevilla, Nitarshan Rajkumar, Patrick Lewis, Aaron Courville, Nick Frosst, Rishi Bommasani, Gary Marcus, Thomas Diettrich, Margaret Jennings, Marzieh Fadaee, Ahmet Ustun, Aidan Peppin, Arash Ahmadian, Yoshua Bengio, Ivan Zhang, Markus Anderljung, Alexander Popper</span>. Perhaps unusually, I regularly try and stress test ideas by seeking to understand the strongest counterarguments. I typically learn more from those who hold different viewpoints, and for this essay I have tried to invite input from colleagues with a varied set of stances on compute thresholds. No need to identify these worthy critics, but a huge thank you to everyone who engaged fully with this piece by providing very meaningful and rich feedback that greatly improved it. Many thanks to <span class="ltx_text ltx_font_bold" id="S6.p3.1.2">Aidan Peppin</span> for additional valuable proofreading. An additional thanks to <span class="ltx_text ltx_font_bold" id="S6.p3.1.3">Linus Chui</span> for visual input on the normal distribution plots in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#S5.F8" title="Figure 8 ‣ 5.2 The case for dynamic thresolds. ‣ 5 The Way Forward ‣ On the Limitations of Compute Thresholds as a Governance Strategy."><span class="ltx_text ltx_ref_tag">8</span></a>. Many thanks to <span class="ltx_text ltx_font_bold" id="S6.p3.1.4">Shivalika Singh</span> for putting together the associated website to make this essay more accessible to those beyond the academic community.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">448 U.S. 607 (1980)</span>
<span class="ltx_bibblock">
448 U.S. 607.

</span>
<span class="ltx_bibblock">Industrial union department v. American Petroleum Institute, June 1980.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aakanksha et al. (2024)</span>
<span class="ltx_bibblock">
Aakanksha, Arash Ahmadian, Beyza Ermis, Seraphina Goldfarb-Tarrant, Julia Kreutzer, Marzieh Fadaee, and Sara Hooker.

</span>
<span class="ltx_bibblock">The multilingual alignment prism: Aligning global and local preferences to reduce harm, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.18682" title="">https://arxiv.org/abs/2406.18682</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abbe et al. (2021)</span>
<span class="ltx_bibblock">
Emmanuel Abbe, Enric Boix-Adsera, Matthew Brennan, Guy Bresler, and Dheeraj Nagaraj.

</span>
<span class="ltx_bibblock">The staircase property: How hierarchical structure can guide deep learning, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2108.10573" title="">https://arxiv.org/abs/2108.10573</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achille et al. (2017)</span>
<span class="ltx_bibblock">
Alessandro Achille, Matteo Rovere, and Stefano Soatto.

</span>
<span class="ltx_bibblock">Critical learning periods in deep neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">ArXiv</em>, abs/1711.08856, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adamic &amp; Huberman (2001)</span>
<span class="ltx_bibblock">
Lada Adamic and Bernardo Huberman.

</span>
<span class="ltx_bibblock">Zipf’s law and the internet.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Glottometrics</em>, 3, 11 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agarwal &amp; Hooker (2020)</span>
<span class="ltx_bibblock">
Chirag Agarwal and Sara Hooker.

</span>
<span class="ltx_bibblock">Estimating example difficulty using variance of gradients, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahmadian et al. (2023)</span>
<span class="ltx_bibblock">
Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, Zhen Stephen Gou, Phil Blunsom, Ahmet Üstün, and Sara Hooker.

</span>
<span class="ltx_bibblock">Intriguing properties of quantization at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=IYe8j7Gy8f" title="">https://openreview.net/forum?id=IYe8j7Gy8f</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahmadian et al. (2024)</span>
<span class="ltx_bibblock">
Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker.

</span>
<span class="ltx_bibblock">Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI@Meta (2024)</span>
<span class="ltx_bibblock">
AI@Meta.

</span>
<span class="ltx_bibblock">Llama 3 model card, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md" title="">https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AISI (2024)</span>
<span class="ltx_bibblock">
AISI.

</span>
<span class="ltx_bibblock">Advanced ai evaluations: May update, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.aisi.gov.uk/work/advanced-ai-evaluations-may-update" title="">https://www.aisi.gov.uk/work/advanced-ai-evaluations-may-update</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aitken et al. (2022)</span>
<span class="ltx_bibblock">
Murray Aitken, David Leslie, Fabian Ostmann, Joseph Pratt, Helen Margetts, and Cristina Dorobantu.

</span>
<span class="ltx_bibblock">Common regulatory capacity for AI.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">The Alan Turing Institute</em>, 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.5281/zenodo.6838946" title="">10.5281/zenodo.6838946</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.5281/zenodo.6838946" title="">https://doi.org/10.5281/zenodo.6838946</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aji &amp; Heafield (2020)</span>
<span class="ltx_bibblock">
Alham Fikri Aji and Kenneth Heafield.

</span>
<span class="ltx_bibblock">Compressing Neural Machine Translation Models with 4-bit Precision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the Fourth Workshop on Neural Generation and Translation</em>, pp.  35–42, Online, July 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.aclweb.org/anthology/2020.ngt-1.4" title="">https://www.aclweb.org/anthology/2020.ngt-1.4</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Albalak et al. (2024)</span>
<span class="ltx_bibblock">
Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, and William Yang Wang.

</span>
<span class="ltx_bibblock">A survey on data selection for language models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Almazrouei et al. (2023)</span>
<span class="ltx_bibblock">
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo.

</span>
<span class="ltx_bibblock">The falcon series of open language models, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2311.16867" title="">https://arxiv.org/abs/2311.16867</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2023)</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Responsible scaling of ai, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www-cdn.anthropic.com/1adf000c8f675958c2ee23805d91aaade1cd4613/responsible-scaling-policy.pdf" title="">https://www-cdn.anthropic.com/1adf000c8f675958c2ee23805d91aaade1cd4613/responsible-scaling-policy.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anwar et al. (2024a)</span>
<span class="ltx_bibblock">
Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario Günther, Anton Korinek, Jose Hernandez-Orallo, Lewis Hammond, Eric Bigelow, Alexander Pan, Lauro Langosco, Tomasz Korbak, Heidi Zhang, Ruiqi Zhong, Seán Ó hÉigeartaigh, Gabriel Recchia, Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Yoshua Bengio, Danqi Chen, Samuel Albanie, Tegan Maharaj, Jakob Foerster, Florian Tramer, He He, Atoosa Kasirzadeh, Yejin Choi, and David Krueger.

</span>
<span class="ltx_bibblock">Foundational challenges in assuring alignment and safety of large language models, 2024a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.09932" title="">https://arxiv.org/abs/2404.09932</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anwar et al. (2024b)</span>
<span class="ltx_bibblock">
Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario Günther, Anton Korinek, Jose Hernandez-Orallo, Lewis Hammond, Eric Bigelow, Alexander Pan, Lauro Langosco, Tomasz Korbak, Heidi Zhang, Ruiqi Zhong, Seán Ó hÉigeartaigh, Gabriel Recchia, Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Yoshua Bengio, Danqi Chen, Samuel Albanie, Tegan Maharaj, Jakob Foerster, Florian Tramer, He He, Atoosa Kasirzadeh, Yejin Choi, and David Krueger.

</span>
<span class="ltx_bibblock">Foundational challenges in assuring alignment and safety of large language models, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arivazhagan et al. (2019)</span>
<span class="ltx_bibblock">
Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al.

</span>
<span class="ltx_bibblock">Massively multilingual neural machine translation in the wild: Findings and challenges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:1907.05019</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arpit et al. (2017)</span>
<span class="ltx_bibblock">
Devansh Arpit, Stanisław Jastrzębski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien.

</span>
<span class="ltx_bibblock">A closer look at memorization in deep networks.

</span>
<span class="ltx_bibblock">In Doina Precup and Yee Whye Teh (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 34th International Conference on Machine Learning</em>, volume 70 of <em class="ltx_emph ltx_font_italic" id="bib.bib19.2.2">Proceedings of Machine Learning Research</em>, pp.  233–242. PMLR, 06–11 Aug 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v70/arpit17a.html" title="">https://proceedings.mlr.press/v70/arpit17a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aryabumi et al. (2024)</span>
<span class="ltx_bibblock">
Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Kelly Marchisio, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Phil Blunsom, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker.

</span>
<span class="ltx_bibblock">Aya 23: Open weight releases to further multilingual progress, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azar et al. (2023)</span>
<span class="ltx_bibblock">
Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos.

</span>
<span class="ltx_bibblock">A general theoretical paradigm to understand learning from human preferences, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baerenbold (2023)</span>
<span class="ltx_bibblock">
Rebekka Baerenbold.

</span>
<span class="ltx_bibblock">Reducing risks in megaprojects: The potential of reference class forecasting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Project Leadership and Society</em>, 4:100103, 2023.

</span>
<span class="ltx_bibblock">ISSN 2666-7215.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/https://doi.org/10.1016/j.plas.2023.100103" title="">https://doi.org/10.1016/j.plas.2023.100103</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S2666721523000248" title="">https://www.sciencedirect.com/science/article/pii/S2666721523000248</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2022)</span>
<span class="ltx_bibblock">
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan.

</span>
<span class="ltx_bibblock">Constitutional ai: Harmlessness from ai feedback, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrett et al. (2024)</span>
<span class="ltx_bibblock">
Anthony M. Barrett, Krystal Jackson, Evan R. Murphy, Nada Madkour, and Jessica Newman.

</span>
<span class="ltx_bibblock">Benchmark early and red team often: A framework for assessing and managing dual-use hazards of ai foundation models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beeching et al. (2023)</span>
<span class="ltx_bibblock">
Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf.

</span>
<span class="ltx_bibblock">Open llm leaderboard.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard" title="">https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bender et al. (2021)</span>
<span class="ltx_bibblock">
Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell.

</span>
<span class="ltx_bibblock">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, FAccT ’21, pp.  610–623, New York, NY, USA, 2021. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3442188.3445922" title="">https://doi.org/10.1145/3442188.3445922</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Benedictow (2004)</span>
<span class="ltx_bibblock">
O.J. Benedictow.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">The Black Death, 1346-1353: The Complete History</em>.

</span>
<span class="ltx_bibblock">Boydell Press, 2004.

</span>
<span class="ltx_bibblock">ISBN 9781843832140.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://books.google.com/books?id=KjLHAOE7irsC" title="">https://books.google.com/books?id=KjLHAOE7irsC</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Besiroglu et al. (2024)</span>
<span class="ltx_bibblock">
Tamay Besiroglu, Ege Erdil, Matthew Barnett, and Josh You.

</span>
<span class="ltx_bibblock">Chinchilla scaling: A replication attempt, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Birhane et al. (2023)</span>
<span class="ltx_bibblock">
Abeba Birhane, Vinay Prabhu, Sang Han, and Vishnu Naresh Boddeti.

</span>
<span class="ltx_bibblock">On hate scaling laws for data-swamps, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bommasani et al. (2021)</span>
<span class="ltx_bibblock">
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.

</span>
<span class="ltx_bibblock">On the Opportunities and Risks of Foundation Models, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bordley (2014)</span>
<span class="ltx_bibblock">
Robert F. Bordley.

</span>
<span class="ltx_bibblock">Reference class forecasting: Resolving its challenge to statistical modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">The American Statistician</em>, 68(4):221–229, 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1080/00031305.2014.937544" title="">10.1080/00031305.2014.937544</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1080/00031305.2014.937544" title="">https://doi.org/10.1080/00031305.2014.937544</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borzunov et al. (2023)</span>
<span class="ltx_bibblock">
Alexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem Chumachenko, Pavel Samygin, and Colin Raffel.

</span>
<span class="ltx_bibblock">Petals: Collaborative inference and fine-tuning of large models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boubdir et al. (2023)</span>
<span class="ltx_bibblock">
Meriem Boubdir, Edward Kim, Beyza Ermis, Marzieh Fadaee, and Sara Hooker.

</span>
<span class="ltx_bibblock">Which prompts make the difference? data prioritization for efficient human llm evaluation, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bowman (2023)</span>
<span class="ltx_bibblock">
Samuel R. Bowman.

</span>
<span class="ltx_bibblock">Eight things to know about large language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brodtkorb et al. (2013)</span>
<span class="ltx_bibblock">
André R. Brodtkorb, Trond R. Hagen, and Martin L. Sætra.

</span>
<span class="ltx_bibblock">Graphics processing unit (gpu) programming strategies and trends in gpu computing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Journal of Parallel and Distributed Computing</em>, 73(1):4 – 13, 2013.

</span>
<span class="ltx_bibblock">ISSN 0743-7315.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/https://doi.org/10.1016/j.jpdc.2012.04.003" title="">https://doi.org/10.1016/j.jpdc.2012.04.003</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.sciencedirect.com/science/article/pii/S0743731512000998" title="">http://www.sciencedirect.com/science/article/pii/S0743731512000998</a>.

</span>
<span class="ltx_bibblock">Metaheuristics on GPUs.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buchanan et al. (2021)</span>
<span class="ltx_bibblock">
Ben Buchanan, Andrew Lohn, Micah Musser, and Katerina Sedova.

</span>
<span class="ltx_bibblock">Truth, lies, and automation: How language models could change disinformation, May 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caballero et al. (2023)</span>
<span class="ltx_bibblock">
Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger.

</span>
<span class="ltx_bibblock">Broken neural scaling laws, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Canziani et al. (2016)</span>
<span class="ltx_bibblock">
Alfredo Canziani, Adam Paszke, and Eugenio Culurciello.

</span>
<span class="ltx_bibblock">An Analysis of Deep Neural Network Models for Practical Applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv e-prints</em>, pp.  arXiv:1605.07678, May 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et al. (2023)</span>
<span class="ltx_bibblock">
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang.

</span>
<span class="ltx_bibblock">Quantifying memorization across neural language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chellapilla et al. (2006)</span>
<span class="ltx_bibblock">
Kumar Chellapilla, Sidd Puri, and Patrice Simard.

</span>
<span class="ltx_bibblock">High performance convolutional neural networks for document processing, 10 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Bo Chen, Xingyi Cheng, Pan Li, Yangli ao Geng, Jing Gong, Shen Li, Zhilei Bei, Xu Tan, Boyan Wang, Xin Zeng, Chiming Liu, Aohan Zeng, Yuxiao Dong, Jie Tang, and Le Song.

</span>
<span class="ltx_bibblock">xtrimopglm: Unified 100b-scale pre-trained transformer for deciphering the language of protein, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al. (2024)</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica.

</span>
<span class="ltx_bibblock">Chatbot arena: An open platform for evaluating llms by human preference, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chimoto et al. (2024)</span>
<span class="ltx_bibblock">
Everlyn Asiko Chimoto, Jay Gala, Orevaoghene Ahia, Julia Kreutzer, Bruce A. Bassett, and Sara Hooker.

</span>
<span class="ltx_bibblock">Critical learning periods: Leveraging early training dynamics for efficient data pruning, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ciresan et al. (2011)</span>
<span class="ltx_bibblock">
Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber.

</span>
<span class="ltx_bibblock">Flexible, high performance convolutional neural networks for image classification., 07 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coates et al. (2013)</span>
<span class="ltx_bibblock">
Adam Coates, Brody Huval, Tao Wang, David Wu, Bryan Catanzaro, and Ng Andrew.

</span>
<span class="ltx_bibblock">Deep learning with COTS HPC systems.

</span>
<span class="ltx_bibblock">In Sanjoy Dasgupta and David McAllester (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the 30th International Conference on Machine Learning</em>, volume 28 of <em class="ltx_emph ltx_font_italic" id="bib.bib45.2.2">Proceedings of Machine Learning Research</em>, pp.  1337–1345, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://proceedings.mlr.press/v28/coates13.html" title="">http://proceedings.mlr.press/v28/coates13.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohere &amp; Team (2024)</span>
<span class="ltx_bibblock">
Cohere and Cohere For AI Team.

</span>
<span class="ltx_bibblock">C4ai command r+, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/CohereForAI/c4ai-command-r-plus" title="">https://huggingface.co/CohereForAI/c4ai-command-r-plus</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-06-30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2019)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">Unsupervised cross-lingual representation learning at scale, July 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.acl-main.747" title="">https://aclanthology.org/2020.acl-main.747</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cottier (2023)</span>
<span class="ltx_bibblock">
Ben Cottier.

</span>
<span class="ltx_bibblock">Trends in the dollar training cost of machine learning systems, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems" title="">https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-05-28.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Courbariaux et al. (2014)</span>
<span class="ltx_bibblock">
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.

</span>
<span class="ltx_bibblock">Training deep neural networks with low precision multiplications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv e-prints</em>, art. arXiv:1412.7024, Dec 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cutland et al. (2017)</span>
<span class="ltx_bibblock">
Claire L. Cutland, Emily M. Lackritz, Tanis Mallett-Moore, Anna Bardají, Ramakrishnan Chandrasekaran, Cagri Lahariya, Muhammad Imran Nisar, Martin D. Tapia, Jay Pathirana, Sumita Kochhar, Francisco M. Muñoz, and Brighton Collaboration Low Birth Weight Working Group.

</span>
<span class="ltx_bibblock">Low birth weight: Case definition &amp; guidelines for data collection, analysis, and presentation of maternal immunization safety data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Vaccine</em>, 35(48 Pt A):6492–6500, 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1016/j.vaccine.2017.01.049" title="">10.1016/j.vaccine.2017.01.049</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dalla-Torre et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Dalla-Torre, Liam Gonzalez, Javier Mendoza Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Hassan Sirelkhatim, Guillaume Richard, Marcin Skwark, Karim Beguir, Marie Lopez, and Thomas Pierrot.

</span>
<span class="ltx_bibblock">The nucleotide transformer: Building and evaluating robust foundation models for human genomics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">bioRxiv</em>, 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1101/2023.01.11.523679" title="">10.1101/2023.01.11.523679</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.biorxiv.org/content/early/2023/01/15/2023.01.11.523679" title="">https://www.biorxiv.org/content/early/2023/01/15/2023.01.11.523679</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dang et al. (2024)</span>
<span class="ltx_bibblock">
John Dang, Arash Ahmadian, Kelly Marchisio, Julia Kreutzer, Ahmet Üstün, and Sara Hooker.

</span>
<span class="ltx_bibblock">Rlhf can speak many languages: Unlocking multilingual preference optimization for llms, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.02552" title="">https://arxiv.org/abs/2407.02552</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Davidson et al. (2023)</span>
<span class="ltx_bibblock">
Tom Davidson, Jean-Stanislas Denain, Pablo Villalobos, and Guillem Bas.

</span>
<span class="ltx_bibblock">Ai capabilities can be significantly improved without expensive retraining, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Davies et al. (2024)</span>
<span class="ltx_bibblock">
Michael Davies, Ian McDougall, Selvaraj Anandaraj, Deep Machchhar, Rithik Jain, and Karthikeyan Sankaralingam.

</span>
<span class="ltx_bibblock">A journey of a 1,000 kernels begins with a single step: A retrospective of deep learning on gpus.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2</em>, ASPLOS ’24, pp.  20–36, New York, NY, USA, 2024. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9798400703850.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1145/3620665.3640367" title="">10.1145/3620665.3640367</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3620665.3640367" title="">https://doi.org/10.1145/3620665.3640367</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dehghani et al. (2021)</span>
<span class="ltx_bibblock">
Mostafa Dehghani, Anurag Arnab, Lucas Beyer, Ashish Vaswani, and Yi Tay.

</span>
<span class="ltx_bibblock">The efficiency misnomer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">CoRR</em>, abs/2110.12894, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2110.12894" title="">https://arxiv.org/abs/2110.12894</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Derczynski (2020)</span>
<span class="ltx_bibblock">
Leon Derczynski.

</span>
<span class="ltx_bibblock">Power Consumption Variation over Activation Functions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2006.07237v1</em>, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2006.07237v1" title="">https://arxiv.org/abs/2006.07237v1</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers (2023)</span>
<span class="ltx_bibblock">
Tim Dettmers.

</span>
<span class="ltx_bibblock">Which gpu for deep learning in 2023?, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/" title="">https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et al. (2022)</span>
<span class="ltx_bibblock">
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Llm. int8 (): 8-bit matrix multiplication for transformers at scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">arXiv preprint arXiv:2208.07339</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et al. (2023)</span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Qlora: Efficient finetuning of quantized llms, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.14314" title="">https://arxiv.org/abs/2305.14314</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeWitt et al. (2024)</span>
<span class="ltx_bibblock">
Tristan D. DeWitt, Timothy J. Garrett, Katherine N. Rees, Christophe Bois, Scott K. Krueger, and Nicolas Ferlay.

</span>
<span class="ltx_bibblock">Climatologically invariant scale invariance seen in distributions of cloud horizontal sizes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Atmospheric Chemistry and Physics</em>, 24:109–122, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.5194/acp-24-109-2024" title="">10.5194/acp-24-109-2024</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhariwal et al. (2021)</span>
<span class="ltx_bibblock">
Prafulla Dhariwal, Girish Sastry, Mark Chen, Dan I. Moldovan, Alex, Beutel, and Jonathan Deaton.

</span>
<span class="ltx_bibblock">Data and parameter scaling laws for neural machine translation, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:235415752" title="">https://api.semanticscholar.org/CorpusID:235415752</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Don-Yehiya et al. (2023)</span>
<span class="ltx_bibblock">
Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen.

</span>
<span class="ltx_bibblock">Cold fusion: Collaborative descent for distributed multitask finetuning, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. (2022)</span>
<span class="ltx_bibblock">
Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, and Claire Cui.

</span>
<span class="ltx_bibblock">Glam: Efficient scaling of language models with mixture-of-experts, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duda (2023)</span>
<span class="ltx_bibblock">
Kathryn Duda.

</span>
<span class="ltx_bibblock">The soviet imaginary: Industrialization and collectivization, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.lib.uchicago.edu/collex/exhibits/soviet-imaginary/technology/industrialization-and-collectivization/" title="">https://www.lib.uchicago.edu/collex/exhibits/soviet-imaginary/technology/industrialization-and-collectivization/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Economist (2023)</span>
<span class="ltx_bibblock">
The Economist.

</span>
<span class="ltx_bibblock">Why ai models make stuff up, and how hallucinations can be controlled, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.economist.com/science-and-technology/2023/02/28/ai-models-make-stuff-up-how-can-hallucinations-be-controlled" title="">https://www.economist.com/science-and-technology/2023/02/28/ai-models-make-stuff-up-how-can-hallucinations-be-controlled</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elnaggar et al. (2020)</span>
<span class="ltx_bibblock">
Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, and Burkhard Rost.

</span>
<span class="ltx_bibblock">Prottrans: Towards cracking the language of life’s code through self-supervised deep learning and high performance computing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">CoRR</em>, abs/2007.06225, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2007.06225" title="">https://arxiv.org/abs/2007.06225</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Engstrom et al. (2020)</span>
<span class="ltx_bibblock">
David Freeman Engstrom, Daniel E. Ho, Catherine M. Sharkey, and Mariano-Florentino Cuellar.

</span>
<span class="ltx_bibblock">Government by algorithm: Artificial intelligence in federal administrative agencies.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">NYU School of Law, Public Law Research Paper</em>, February 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.2139/ssrn.3551505" title="">10.2139/ssrn.3551505</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ssrn.com/abstract=3551505" title="">https://ssrn.com/abstract=3551505</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Epoch AI (2023)</span>
<span class="ltx_bibblock">
Epoch AI.

</span>
<span class="ltx_bibblock">Key trends and figures in machine learning, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://epochai.org/trends" title="">https://epochai.org/trends</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-05-19.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Epoch AI (2024)</span>
<span class="ltx_bibblock">
Epoch AI.

</span>
<span class="ltx_bibblock">Parameter, compute and data trends in machine learning, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://epochai.org/data/epochdb/visualization" title="">https://epochai.org/data/epochdb/visualization</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-05-27.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Erdil &amp; Besiroglu (2023)</span>
<span class="ltx_bibblock">
Ege Erdil and Tamay Besiroglu.

</span>
<span class="ltx_bibblock">Algorithmic progress in computer vision, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Erlich (1967)</span>
<span class="ltx_bibblock">
Alexander Erlich.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">Development Strategy and Planning: The Soviet Experience</em>, pp.  233–278.

</span>
<span class="ltx_bibblock">NBER, 1967.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.nber.org/chapters/c1425" title="">http://www.nber.org/chapters/c1425</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">European Union (2024)</span>
<span class="ltx_bibblock">
European Union.

</span>
<span class="ltx_bibblock">Eu artificial intelligence act, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://artificialintelligenceact.eu/the-act/" title="">https://artificialintelligenceact.eu/the-act/</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-06-30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Evci et al. (2019)</span>
<span class="ltx_bibblock">
Utku Evci, Fabian Pedregosa, Aidan Gomez, and Erich Elsen.

</span>
<span class="ltx_bibblock">The difficulty of training sparse neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">arXiv preprint arXiv:1906.10732</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Faghri et al. (2020)</span>
<span class="ltx_bibblock">
Fartash Faghri, David Duvenaud, David J. Fleet, and Jimmy Ba.

</span>
<span class="ltx_bibblock">A Study of Gradient Variance in Deep Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">arXiv e-prints</em>, art. arXiv:2007.04532, July 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. (2024a)</span>
<span class="ltx_bibblock">
Richard Fang, Rohan Bindu, Akul Gupta, and Daniel Kang.

</span>
<span class="ltx_bibblock">Llm agents can autonomously exploit one-day vulnerabilities, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. (2024b)</span>
<span class="ltx_bibblock">
Yin Fang, Ningyu Zhang, Zhuo Chen, Lingbing Guo, Xiaohui Fan, and Huajun Chen.

</span>
<span class="ltx_bibblock">Domain-agnostic molecular generation with chemical feedback, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fawzi et al. (2022)</span>
<span class="ltx_bibblock">
Ali Fawzi, Miklos Balog, Alex Huang, Ziwei Song, Yang Song, and Oriol Vinyals.

</span>
<span class="ltx_bibblock">Discovering faster matrix multiplication algorithms with reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">Nature</em>, 610:47–53, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fedus et al. (2022)</span>
<span class="ltx_bibblock">
William Fedus, Barret Zoph, and Noam Shazeer.

</span>
<span class="ltx_bibblock">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Forum (2023)</span>
<span class="ltx_bibblock">
Frontier Model Forum.

</span>
<span class="ltx_bibblock">Issue brief: Measuring training compute.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/" title="">https://www.frontiermodelforum.org/updates/issue-brief-measuring-training-compute/</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frankle et al. (2020)</span>
<span class="ltx_bibblock">
Jonathan Frankle, David J. Schwab, and Ari S. Morcos.

</span>
<span class="ltx_bibblock">The early phase of neural network training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">CoRR</em>, abs/2002.10365, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2002.10365" title="">https://arxiv.org/abs/2002.10365</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frantar et al. (2022)</span>
<span class="ltx_bibblock">
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.

</span>
<span class="ltx_bibblock">GPTQ: Accurate post-training compression for generative pretrained transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">arXiv preprint arXiv:2210.17323</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frydman &amp; Molloy (2007)</span>
<span class="ltx_bibblock">
Carola Frydman and Raven Molloy.

</span>
<span class="ltx_bibblock">Historical trends in executive compensation, 1936-2003.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">Journal of Economic History</em>, 67, 01 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ganguli et al. (2022)</span>
<span class="ltx_bibblock">
Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, Sheer El Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Scott Johnston, Andy Jones, Nicholas Joseph, Jackson Kernian, Shauna Kravec, Ben Mann, Neel Nanda, Kamal Ndousse, Catherine Olsson, Daniela Amodei, Tom Brown, Jared Kaplan, Sam McCandlish, Christopher Olah, Dario Amodei, and Jack Clark.

</span>
<span class="ltx_bibblock">Predictability and surprise in large generative models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">2022 ACM Conference on Fairness, Accountability, and Transparency</em>, FAccT ’22. ACM, June 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1145/3531146.3533229" title="">10.1145/3531146.3533229</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1145/3531146.3533229" title="">http://dx.doi.org/10.1145/3531146.3533229</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gehman et al. (2020)</span>
<span class="ltx_bibblock">
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith.

</span>
<span class="ltx_bibblock">Realtoxicityprompts: Evaluating neural toxic degeneration in language models, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2009.11462" title="">https://arxiv.org/abs/2009.11462</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldberg (1991)</span>
<span class="ltx_bibblock">
David Goldberg.

</span>
<span class="ltx_bibblock">What every computer scientist should know about floating-point arithmetic.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">ACM Comput. Surv.</em>, 23(1):5–48, mar 1991.

</span>
<span class="ltx_bibblock">ISSN 0360-0300.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1145/103162.103163" title="">10.1145/103162.103163</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/103162.103163" title="">https://doi.org/10.1145/103162.103163</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldstein et al. (2023)</span>
<span class="ltx_bibblock">
Josh A. Goldstein, Girish Sastry, Micah Musser, Renee DiResta, Matthew Gentzel, and Katerina Sedova.

</span>
<span class="ltx_bibblock">Generative language models and automated influence operations: Emerging threats and potential mitigations, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow et al. (2016)</span>
<span class="ltx_bibblock">
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">Deep Learning</em>.

</span>
<span class="ltx_bibblock">MIT Press, 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.deeplearningbook.org" title="">http://www.deeplearningbook.org</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al. (2015)</span>
<span class="ltx_bibblock">
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.

</span>
<span class="ltx_bibblock">Deep Learning with Limited Numerical Precision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">CoRR</em>, abs/1502.02551, 2015.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1502.02551" title="">http://arxiv.org/abs/1502.02551</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">HAI (2023)</span>
<span class="ltx_bibblock">
Stanford HAI.

</span>
<span class="ltx_bibblock">Hallucinating the law: Legal mistakes in large language models are pervasive, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://hai.stanford.edu/news/hallucinating-law-legal-mistakes-large-language-models-are-pervasive" title="">https://hai.stanford.edu/news/hallucinating-law-legal-mistakes-large-language-models-are-pervasive</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao (2024)</span>
<span class="ltx_bibblock">
Karen Hao.

</span>
<span class="ltx_bibblock">Llms become more “covertly racist” with human intervention, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.technologyreview.com/2024/03/11/1089683/llms-become-more-covertly-racist-with-human-intervention/" title="">https://www.technologyreview.com/2024/03/11/1089683/llms-become-more-covertly-racist-with-human-intervention/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassibi et al. (1993a)</span>
<span class="ltx_bibblock">
B. Hassibi, D. G. Stork, and G. J. Wolff.

</span>
<span class="ltx_bibblock">Optimal Brain Surgeon and general network pruning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">IEEE International Conference on Neural Networks</em>, pp.  293–299 vol.1, March 1993a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassibi et al. (1993b)</span>
<span class="ltx_bibblock">
Babak Hassibi, David G. Stork, and Stork Crc. Ricoh. Com.

</span>
<span class="ltx_bibblock">Second order derivatives for network pruning: Optimal brain surgeon.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">Advances in Neural Information Processing Systems 5</em>, pp.  164–171. Morgan Kaufmann, 1993b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heim (2023)</span>
<span class="ltx_bibblock">
Lennart Heim.

</span>
<span class="ltx_bibblock">Estimating palm’s training cost, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.heim.xyz/palm-training-cost/" title="">https://blog.heim.xyz/palm-training-cost/</a>.

</span>
<span class="ltx_bibblock">Accessed: 2023-08-10.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heim et al. (2024)</span>
<span class="ltx_bibblock">
Lennart Heim, Tim Fist, Janet Egan, Sihao Huang, Stephen Zekany, Robert Trager, Michael A Osborne, and Noa Zilberman.

</span>
<span class="ltx_bibblock">Governing through the cloud: The intermediary role of compute providers in ai regulation, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hernandez &amp; Brown (2020)</span>
<span class="ltx_bibblock">
Danny Hernandez and Tom B. Brown.

</span>
<span class="ltx_bibblock">Measuring the algorithmic efficiency of neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">CoRR</em>, abs/2005.04305, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2005.04305" title="">https://arxiv.org/abs/2005.04305</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hernandez et al. (2021)</span>
<span class="ltx_bibblock">
Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish.

</span>
<span class="ltx_bibblock">Scaling laws for transfer, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. (2024a)</span>
<span class="ltx_bibblock">
Anson Ho, Tamay Besiroglu, Ege Erdil, David Owen, Robi Rahman, Zifan Carl Guo, David Atkinson, Neil Thompson, and Jaime Sevilla.

</span>
<span class="ltx_bibblock">Algorithmic progress in language models, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. (2024b)</span>
<span class="ltx_bibblock">
Anson Ho, Tamay Besiroglu, Ege Erdil, David Owen, Robi Rahman, Zifan Carl Guo, David Atkinson, Neil Thompson, and Jaime Sevilla.

</span>
<span class="ltx_bibblock">Algorithmic progress in language models, 2024b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.05812" title="">https://arxiv.org/abs/2403.05812</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hobbhahn et al. (2023)</span>
<span class="ltx_bibblock">
Marius Hobbhahn, Lennart Heim, and Gökçe Aydos.

</span>
<span class="ltx_bibblock">Trends in machine learning hardware, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://epochai.org/blog/trends-in-machine-learning-hardware" title="">https://epochai.org/blog/trends-in-machine-learning-hardware</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-05-28.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hooker (2021)</span>
<span class="ltx_bibblock">
Sara Hooker.

</span>
<span class="ltx_bibblock">The hardware lottery.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">Commun. ACM</em>, 64(12):58–65, nov 2021.

</span>
<span class="ltx_bibblock">ISSN 0001-0782.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1145/3467017" title="">10.1145/3467017</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3467017" title="">https://doi.org/10.1145/3467017</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hooker et al. (2019)</span>
<span class="ltx_bibblock">
Sara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, and Andrea Frome.

</span>
<span class="ltx_bibblock">What do compressed deep neural networks forget?, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hooker et al. (2020)</span>
<span class="ltx_bibblock">
Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton.

</span>
<span class="ltx_bibblock">Characterising Bias in Compressed Models, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsieh et al. (2023)</span>
<span class="ltx_bibblock">
Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.

</span>
<span class="ltx_bibblock">Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2024)</span>
<span class="ltx_bibblock">
Shengding Hu, Xin Liu, Xu Han, Xinrong Zhang, Chaoqun He, Weilin Zhao, Yankai Lin, Ning Ding, Zebin Ou, Guoyang Zeng, Zhiyuan Liu, and Maosong Sun.

</span>
<span class="ltx_bibblock">Predicting emergent abilities with infinite resolution evaluation, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.03262" title="">https://arxiv.org/abs/2310.03262</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2022)</span>
<span class="ltx_bibblock">
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han.

</span>
<span class="ltx_bibblock">Large language models can self-improve, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hubara et al. (2016)</span>
<span class="ltx_bibblock">
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Quantized neural networks: Training neural networks with low precision weights and activations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">CoRR</em>, abs/1609.07061, 2016.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1609.07061" title="">http://arxiv.org/abs/1609.07061</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jacob et al. (2018)</span>
<span class="ltx_bibblock">
Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko.

</span>
<span class="ltx_bibblock">Quantization and training of neural networks for efficient integer-arithmetic-only inference.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, Jun 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1109/cvpr.2018.00286" title="">10.1109/cvpr.2018.00286</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1109/CVPR.2018.00286" title="">http://dx.doi.org/10.1109/CVPR.2018.00286</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jatho et al. (2023)</span>
<span class="ltx_bibblock">
Edgar W. Jatho, Logan O. Mailloux, Eugene D. Williams, Patrick McClure, and Joshua A. Kroll.

</span>
<span class="ltx_bibblock">Concrete safety for ml problems: System safety for ml development and assessment, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2302.02972" title="">https://arxiv.org/abs/2302.02972</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2024)</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al.

</span>
<span class="ltx_bibblock">Mixtral of experts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">arXiv preprint arXiv:2401.04088</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2020)</span>
<span class="ltx_bibblock">
Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael C Mozer.

</span>
<span class="ltx_bibblock">Exploring the memorization-generalization continuum in deep learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">arXiv preprint arXiv:2002.03206</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaminski (2023)</span>
<span class="ltx_bibblock">
Margot E. Kaminski.

</span>
<span class="ltx_bibblock">Regulating the Risks of AI.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">Boston University Law Review</em>, 103(1347), 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.2139/ssrn.4195066" title="">10.2139/ssrn.4195066</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ssrn.com/abstract=4195066" title="">https://ssrn.com/abstract=4195066</a>.

</span>
<span class="ltx_bibblock">U of Colorado Law Legal Studies Research Paper No. 22-21.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kandpal et al. (2022)</span>
<span class="ltx_bibblock">
Nikhil Kandpal, Eric Wallace, and Colin Raffel.

</span>
<span class="ltx_bibblock">Deduplicating training data mitigates privacy risks in language models, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et al. (2020)</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karen Hao (2023)</span>
<span class="ltx_bibblock">
Matteo Wong Karen Hao.

</span>
<span class="ltx_bibblock">The white house is preparing for an ai-dominated future, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.theatlantic.com/technology/archive/2023/10/biden-white-house-ai-executive-order/675837/" title="">https://www.theatlantic.com/technology/archive/2023/10/biden-white-house-ai-executive-order/675837/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khalifa et al. (2021)</span>
<span class="ltx_bibblock">
Muhammad Khalifa, Hady Elsahar, and Marc Dymetman.

</span>
<span class="ltx_bibblock">A distributional approach to controlled text generation, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ko et al. (2023)</span>
<span class="ltx_bibblock">
Wei-Yin Ko, Daniel D’souza, Karina Nguyen, Randall Balestriero, and Sara Hooker.

</span>
<span class="ltx_bibblock">Fair-ensemble: When fairness naturally emerges from deep ensembling, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocetkov et al. (2022)</span>
<span class="ltx_bibblock">
Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries.

</span>
<span class="ltx_bibblock">The stack: 3 tb of permissively licensed source code, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kossen et al. (2024)</span>
<span class="ltx_bibblock">
Jannik Kossen, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth Malik, and Yarin Gal.

</span>
<span class="ltx_bibblock">Semantic entropy probes: Robust and cheap hallucination detection in llms, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.15927" title="">https://arxiv.org/abs/2406.15927</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krimsky (2005)</span>
<span class="ltx_bibblock">
Sheldon Krimsky.

</span>
<span class="ltx_bibblock">The weight of scientific evidence in policy and law.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">American Journal of Public Health</em>, 95(Suppl 1):S129–S136, 2005.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.2105/AJPH.2004.044727" title="">10.2105/AJPH.2004.044727</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky et al. (2012)</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">Communications of the ACM</em>, 60(6):84–90, 2012.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1145/3091627" title="">10.1145/3091627</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3091627" title="">https://doi.org/10.1145/3091627</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le et al. (2012)</span>
<span class="ltx_bibblock">
Quoc V. Le, Marc’Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, and Andrew Y. Ng.

</span>
<span class="ltx_bibblock">Building high-level features using large scale unsupervised learning, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun et al. (1990)</span>
<span class="ltx_bibblock">
Yann LeCun, John S. Denker, and Sara A. Solla.

</span>
<span class="ltx_bibblock">Optimal Brain Damage.

</span>
<span class="ltx_bibblock">In David Touretzky (ed.), <em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">Advances in Neural Information Processing Systems</em>, volume 2, pp.  598–605. Morgan Kaufmann, 1990.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2023)</span>
<span class="ltx_bibblock">
Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash.

</span>
<span class="ltx_bibblock">Rlaif: Scaling reinforcement learning from human feedback with ai feedback, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2024)</span>
<span class="ltx_bibblock">
Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, Sébastien M. R. Arnold, Vincent Perot, Siddharth Dalmia, Hexiang Hu, Xudong Lin, Panupong Pasupat, Aida Amini, Jeremy R. Cole, Sebastian Riedel, Iftekhar Naim, Ming-Wei Chang, and Kelvin Guu.

</span>
<span class="ltx_bibblock">Can long-context language models subsume retrieval, rag, sql, and more?, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2022)</span>
<span class="ltx_bibblock">
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini.

</span>
<span class="ltx_bibblock">Deduplicating training data makes language models better, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lepikhin et al. (2020)</span>
<span class="ltx_bibblock">
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.

</span>
<span class="ltx_bibblock">Gshard: Scaling giant models with conditional computation and automatic sharding, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock">In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib127.1.1">Advances in Neural Information Processing Systems</em>, volume 33, pp.  9459–9474. Curran Associates, Inc., 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, and Deheng Ye.

</span>
<span class="ltx_bibblock">More agents is all you need, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2023)</span>
<span class="ltx_bibblock">
Jinfeng Liang, Yi Huang, Li Yin, Fatemeh Sadeghi, Yanping Yang, Xue Xiao, Hans-Olov Adami, Weimin Ye, Zhe Zhang, and Fang Fang.

</span>
<span class="ltx_bibblock">Cancer risk following surgical removal of tonsils and adenoids — a population-based, sibling-controlled cohort study in sweden.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib129.1.1">BMC Medicine</em>, 21, 05 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1186/s12916-023-02902-x" title="">10.1186/s12916-023-02902-x</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2024a)</span>
<span class="ltx_bibblock">
Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han.

</span>
<span class="ltx_bibblock">Awq: Activation-aware weight quantization for llm compression and acceleration.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib130.1.1">MLSys</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2024b)</span>
<span class="ltx_bibblock">
Jiayi Lin, Hande Dong, Yutao Xie, and Lei Zhang.

</span>
<span class="ltx_bibblock">Scaling laws behind code understanding model, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2023)</span>
<span class="ltx_bibblock">
Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, and Alexander Rives.

</span>
<span class="ltx_bibblock">Evolutionary-scale prediction of atomic-level protein structure with a language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib132.1.1">Science</em>, 379(6637):1123–1130, 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1126/science.ade2574" title="">10.1126/science.ade2574</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.science.org/doi/abs/10.1126/science.ade2574" title="">https://www.science.org/doi/abs/10.1126/science.ade2574</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Linghan et al. (2024)</span>
<span class="ltx_bibblock">
Zhang Linghan, Yang Jianjun, Cheng Ying, Zhao Jingwu, Han Xuzhi, Zheng Zhifeng, and Xu Xiaoben.

</span>
<span class="ltx_bibblock">Artificial intelligence law of the people’s republic of china (draft for suggestions from scholars), 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cset.georgetown.edu/publication/china-ai-law-draft/" title="">https://cset.georgetown.edu/publication/china-ai-law-draft/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024)</span>
<span class="ltx_bibblock">
Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma.

</span>
<span class="ltx_bibblock">Sophia: A scalable stochastic second-order optimizer for language model pre-training, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lohn &amp; Musser (2022)</span>
<span class="ltx_bibblock">
Andrew J. Lohn and Micah Musser.

</span>
<span class="ltx_bibblock">Ai and compute: How much longer can computing power drive artificial intelligence progress?, January 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.51593/2021CA009" title="">https://doi.org/10.51593/2021CA009</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Louizos et al. (2017)</span>
<span class="ltx_bibblock">
Christos Louizos, Max Welling, and Diederik P. Kingma.

</span>
<span class="ltx_bibblock">Learning Sparse Neural Networks through <math alttext="L_{0}" class="ltx_Math" display="inline" id="bib.bib136.1.m1.1"><semantics id="bib.bib136.1.m1.1a"><msub id="bib.bib136.1.m1.1.1" xref="bib.bib136.1.m1.1.1.cmml"><mi id="bib.bib136.1.m1.1.1.2" xref="bib.bib136.1.m1.1.1.2.cmml">L</mi><mn id="bib.bib136.1.m1.1.1.3" xref="bib.bib136.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="bib.bib136.1.m1.1b"><apply id="bib.bib136.1.m1.1.1.cmml" xref="bib.bib136.1.m1.1.1"><csymbol cd="ambiguous" id="bib.bib136.1.m1.1.1.1.cmml" xref="bib.bib136.1.m1.1.1">subscript</csymbol><ci id="bib.bib136.1.m1.1.1.2.cmml" xref="bib.bib136.1.m1.1.1.2">𝐿</ci><cn id="bib.bib136.1.m1.1.1.3.cmml" type="integer" xref="bib.bib136.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib136.1.m1.1c">L_{0}</annotation><annotation encoding="application/x-llamapun" id="bib.bib136.1.m1.1d">italic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> Regularization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib136.2.1">ArXiv e-prints</em>, December 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lund-Jensen (2012)</span>
<span class="ltx_bibblock">
Kasper Lund-Jensen.

</span>
<span class="ltx_bibblock">Monitoring systemic risk based on dynamic thresholds.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib137.1.1">IMF Working Papers</em>, 12, 06 2012.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.5089/9781475504576.001" title="">10.5089/9781475504576.001</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyman &amp; Varian (2003)</span>
<span class="ltx_bibblock">
Peter Lyman and Hal R Varian.

</span>
<span class="ltx_bibblock">How much information?, 2003.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.sims.berkeley.edu/how-much-info2003" title="">http://www.sims.berkeley.edu/how-much-info2003</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mangalam &amp; Prabhu (2019)</span>
<span class="ltx_bibblock">
Karttikeya Mangalam and Vinay Uday Prabhu.

</span>
<span class="ltx_bibblock">Do deep neural networks learn shallow learnable examples first, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marchant (2011)</span>
<span class="ltx_bibblock">
Gary E. Marchant.

</span>
<span class="ltx_bibblock">The growing gap between emerging technologies and the law.

</span>
<span class="ltx_bibblock">In Gary Marchant, Braden Allenby, and Joseph Herkert (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib140.1.1">The Growing Gap Between Emerging Technologies and Legal-Ethical Oversight</em>, volume 7 of <em class="ltx_emph ltx_font_italic" id="bib.bib140.2.2">The International Library of Ethics, Law and Technology</em>. Springer, 2011.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1007/978-94-007-1356-7_2" title="">10.1007/978-94-007-1356-7_2</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-94-007-1356-7_2" title="">https://doi.org/10.1007/978-94-007-1356-7_2</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marchisio et al. (2024)</span>
<span class="ltx_bibblock">
Kelly Marchisio, Saurabh Dash, Hongyu Chen, Dennis Aumiller, Ahmet Üstün, Sara Hooker, and Sebastian Ruder.

</span>
<span class="ltx_bibblock">How does quantization affect multilingual llms?, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marion et al. (2023)</span>
<span class="ltx_bibblock">
Max Marion, Ahmet Üstün, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker.

</span>
<span class="ltx_bibblock">When less is more: Investigating data pruning for pretraining llms at scale, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maug et al. (2024)</span>
<span class="ltx_bibblock">
Nicole Maug, Aidan O’Gara, and Tamay Besiroglu.

</span>
<span class="ltx_bibblock">Biological sequence models in the context of the ai directives, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://epochai.org/blog/biological-sequence-models-in-the-context-of-the-ai-directives" title="">https://epochai.org/blog/biological-sequence-models-in-the-context-of-the-ai-directives</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-05-19.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mcclelland et al. (1995)</span>
<span class="ltx_bibblock">
James Mcclelland, Bruce Mcnaughton, and Randall O’Reilly.

</span>
<span class="ltx_bibblock">Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib144.1.1">Psychological review</em>, 102:419–57, 08 1995.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1037/0033-295X.102.3.419" title="">10.1037/0033-295X.102.3.419</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">METR Team (2023)</span>
<span class="ltx_bibblock">
METR Team.

</span>
<span class="ltx_bibblock">Elicitation gap, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://metr.github.io/autonomy-evals-guide/elicitation-gap/" title="">https://metr.github.io/autonomy-evals-guide/elicitation-gap/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mialon et al. (2023)</span>
<span class="ltx_bibblock">
Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom.

</span>
<span class="ltx_bibblock">Augmented language models: a survey, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2302.07842" title="">https://arxiv.org/abs/2302.07842</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mince et al. (2023)</span>
<span class="ltx_bibblock">
Fraser Mince, Dzung Dinh, Jonas Kgomo, Neil Thompson, and Sara Hooker.

</span>
<span class="ltx_bibblock">The grand illusion: The myth of software portability and implications for ml progress.

</span>
<span class="ltx_bibblock">In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib147.1.1">Advances in Neural Information Processing Systems</em>, volume 36, pp.  21217–21229. Curran Associates, Inc., 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/42c40aff7814e9796266e12053b1c610-Paper-Conference.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2023/file/42c40aff7814e9796266e12053b1c610-Paper-Conference.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mouton et al. (2024)</span>
<span class="ltx_bibblock">
Christopher A. Mouton, Caleb Lucas, and Ella Guest.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib148.1.1">The Operational Risks of AI in Large-Scale Biological Attacks: Results of a Red-Team Study</em>.

</span>
<span class="ltx_bibblock">RAND Corporation, Santa Monica, CA, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.7249/RRA2977-2" title="">10.7249/RRA2977-2</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Musameh et al. (2017)</span>
<span class="ltx_bibblock">
Mohammad Musameh, Christopher Nelson, Jonathan Gracey, Jessica Davies, Richard Davies, Denise Francis, Adrian Hughes, Gregory Y H Lip, Helen Mcnamara, Alison Mccarthy, et al.

</span>
<span class="ltx_bibblock">Determinants of day–night difference in blood pressure, a comparison with determinants of daytime and night-time blood pressure.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib149.1.1">Journal of human hypertension</em>, 31(1):43–48, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Musser (2023)</span>
<span class="ltx_bibblock">
Micah Musser.

</span>
<span class="ltx_bibblock">A cost analysis of generative language models and influence operations, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NCSC (2024)</span>
<span class="ltx_bibblock">
NCSC.

</span>
<span class="ltx_bibblock">The impact of ai on the cyber threat, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ncsc.gov.uk/report/impact-of-ai-on-cyber-threat" title="">https://www.ncsc.gov.uk/report/impact-of-ai-on-cyber-threat</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Niu et al. (2024)</span>
<span class="ltx_bibblock">
Xueyan Niu, Bo Bai, Lei Deng, and Wei Han.

</span>
<span class="ltx_bibblock">Beyond scaling laws: Understanding transformer performance with associative memory, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Office (2024)</span>
<span class="ltx_bibblock">
Swiss Federal Statistical Office.

</span>
<span class="ltx_bibblock">Risk of poverty, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bfs.admin.ch/bfs/en/home/statistics/economic-social-situation-population/economic-and-social-situequation-of-the-population/poverty-deprivation/risk-poverty.html" title="">https://www.bfs.admin.ch/bfs/en/home/statistics/economic-social-situation-population/economic-and-social-situequation-of-the-population/poverty-deprivation/risk-poverty.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oh &amp; Jung (2004)</span>
<span class="ltx_bibblock">
Kyoung-Su Oh and Keechul Jung.

</span>
<span class="ltx_bibblock">GPU implementation of neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib154.1.1">Pattern Recognition</em>, 37(6):1311–1314, 2004.

</span>
<span class="ltx_bibblock">ISSN 0031-3203.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.sciencedirect.com/science/article/pii/S0031320304000524" title="">http://www.sciencedirect.com/science/article/pii/S0031320304000524</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">on Foreign Affairs (2024)</span>
<span class="ltx_bibblock">
U.S. House Committee on Foreign Affairs.

</span>
<span class="ltx_bibblock">H.r.8315 - enhancing national frameworks for overseas restriction of critical exports act or enforce act., 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://foreignaffairs.house.gov/wp-content/uploads/2024/05/HR-8315.pdf" title="">https://foreignaffairs.house.gov/wp-content/uploads/2024/05/HR-8315.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Our approach to frontier risk, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/global-affairs/our-approach-to-frontier-risk/" title="">https://openai.com/global-affairs/our-approach-to-frontier-risk/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Building an early warning system for llm-aided biological threat creation, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/building-an-early-warning-system-for-llm-aided-biological-threat-creation/" title="">https://openai.com/index/building-an-early-warning-system-for-llm-aided-biological-threat-creation/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paleyes et al. (2022)</span>
<span class="ltx_bibblock">
Andrei Paleyes, Raoul-Gabriel Urma, and Neil D. Lawrence.

</span>
<span class="ltx_bibblock">Challenges in deploying machine learning: A survey of case studies.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib159.1.1">ACM Comput. Surv.</em>, 55(6), dec 2022.

</span>
<span class="ltx_bibblock">ISSN 0360-0300.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1145/3533378" title="">10.1145/3533378</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3533378" title="">https://doi.org/10.1145/3533378</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panda et al. (2024)</span>
<span class="ltx_bibblock">
Ashwinee Panda, Christopher A. Choquette-Choo, Zhengming Zhang, Yaoqing Yang, and Prateek Mittal.

</span>
<span class="ltx_bibblock">Teach llms to phish: Stealing private information from language models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patterson et al. (2021)</span>
<span class="ltx_bibblock">
David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean.

</span>
<span class="ltx_bibblock">Carbon Emissions and Large Neural Network Training, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paul et al. (2021)</span>
<span class="ltx_bibblock">
Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite.

</span>
<span class="ltx_bibblock">Deep learning on a data diet: Finding important examples early in training, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Payne et al. (2005)</span>
<span class="ltx_bibblock">
Bryson R. Payne, Saeid O. Belkasim, G. Scott Owen, Michael C. Weeks, and Ying Zhu.

</span>
<span class="ltx_bibblock">Accelerated 2D Image Processing on GPUs.

</span>
<span class="ltx_bibblock">In Vaidy S. Sunderam, Geert Dick van Albada, Peter M. A. Sloot, and Jack J. Dongarra (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib163.1.1">Computational Science – ICCS 2005</em>, pp.  256–264, Berlin, Heidelberg, 2005. Springer Berlin Heidelberg.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Penedo et al. (2023)</span>
<span class="ltx_bibblock">
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.

</span>
<span class="ltx_bibblock">The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peter (2002)</span>
<span class="ltx_bibblock">
John Peter.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib165.1.1">The Great Fire of London: In That Apocalyptic Year, 1666</em>.

</span>
<span class="ltx_bibblock">John Wiley, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfeiffer et al. (2022)</span>
<span class="ltx_bibblock">
Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe.

</span>
<span class="ltx_bibblock">Lifting the curse of multilinguality by pre-training modular transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib166.1.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pp.  3479–3495, Seattle, United States, July 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2022.naacl-main.255" title="">10.18653/v1/2022.naacl-main.255</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.naacl-main.255" title="">https://aclanthology.org/2022.naacl-main.255</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pozzobon et al. (2023a)</span>
<span class="ltx_bibblock">
Luiza Pozzobon, Beyza Ermis, Patrick Lewis, and Sara Hooker.

</span>
<span class="ltx_bibblock">Goodtriever: Adaptive toxicity mitigation with retrieval-augmented models.

</span>
<span class="ltx_bibblock">In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib167.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pp.  5108–5125, Singapore, December 2023a. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2023.findings-emnlp.339" title="">10.18653/v1/2023.findings-emnlp.339</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.findings-emnlp.339" title="">https://aclanthology.org/2023.findings-emnlp.339</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pozzobon et al. (2023b)</span>
<span class="ltx_bibblock">
Luiza Pozzobon, Beyza Ermis, Patrick Lewis, and Sara Hooker.

</span>
<span class="ltx_bibblock">Goodtriever: Adaptive toxicity mitigation with retrieval-augmented models, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. (2023)</span>
<span class="ltx_bibblock">
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun.

</span>
<span class="ltx_bibblock">Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. (2024)</span>
<span class="ltx_bibblock">
Zhen Qin, Daoyuan Chen, Bingchen Qian, Bolin Ding, Yaliang Li, and Shuiguang Deng.

</span>
<span class="ltx_bibblock">Federated full-parameter tuning of billion-sized language models with communication cost under 18 kilobytes, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.06353" title="">https://arxiv.org/abs/2312.06353</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Quer et al. (2020)</span>
<span class="ltx_bibblock">
Giorgio Quer, Pishoy Gouda, Michael Galarnyk, Eric Topol, and Steven Steinhubl.

</span>
<span class="ltx_bibblock">Inter- and intraindividual variability in daily resting heart rate and its associations with age, sex, sleep, bmi, and time of year: Retrospective, longitudinal cohort study of 92,457 adults.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib171.1.1">PLOS ONE</em>, 15:e0227709, 02 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1371/journal.pone.0227709" title="">10.1371/journal.pone.0227709</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Quetelet (1817)</span>
<span class="ltx_bibblock">
A. Quetelet.

</span>
<span class="ltx_bibblock">Statement of the sizes of men in different counties of scotland, taken from the local militia.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib172.1.1">Edinburgh Medical and Surgical Journal</em>, 13:260–264, 1817.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et al. (2021)</span>
<span class="ltx_bibblock">
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones,
James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving.

</span>
<span class="ltx_bibblock">Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et al. (2023)</span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a reward model, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.

</span>
<span class="ltx_bibblock">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raji et al. (2020)</span>
<span class="ltx_bibblock">
Inioluwa Deborah Raji, Andrew Smart, Rebecca N. White, Margaret Mitchell, Timnit Gebru, Ben Hutchinson, Jamila Smith-Loud, Daniel Theron, and Parker Barnes.

</span>
<span class="ltx_bibblock">Closing the ai accountability gap: Defining an end-to-end framework for internal algorithmic auditing, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2001.00973" title="">https://arxiv.org/abs/2001.00973</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reuel &amp; Undheim (2024)</span>
<span class="ltx_bibblock">
Anka Reuel and Trond Arne Undheim.

</span>
<span class="ltx_bibblock">Generative ai needs adaptive governance, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:270357796" title="">https://api.semanticscholar.org/CorpusID:270357796</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reuel et al. (2024)</span>
<span class="ltx_bibblock">
Anka Reuel, Lisa Soder, Benjamin Bucknall, and Trond Arne Undheim.

</span>
<span class="ltx_bibblock">Position: Technical research and talent is needed for effective AI governance.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib178.1.1">Forty-first International Conference on Machine Learning</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Be2B6f0ps1" title="">https://openreview.net/forum?id=Be2B6f0ps1</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reuters (2024)</span>
<span class="ltx_bibblock">
Reuters.

</span>
<span class="ltx_bibblock">U.s. eyes curbs on china’s access to ai software behind apps like chatgpt, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.reuters.com/technology/us-eyes-curbs-chinas-access-ai-software-behind-apps-like-chatgpt-2024-05-08/" title="">https://www.reuters.com/technology/us-eyes-curbs-chinas-access-ai-software-behind-apps-like-chatgpt-2024-05-08/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ricci &amp; Zhang (2011)</span>
<span class="ltx_bibblock">
P.F. Ricci and J. Zhang.

</span>
<span class="ltx_bibblock">Benefits and limitations of the precautionary principle.

</span>
<span class="ltx_bibblock">In J.O. Nriagu (ed.), <em class="ltx_emph ltx_font_italic" id="bib.bib180.1.1">Encyclopedia of Environmental Health</em>, pp.  276–285. Elsevier, Burlington, 2011.

</span>
<span class="ltx_bibblock">ISBN 978-0-444-52272-6.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/https://doi.org/10.1016/B978-0-444-52272-6.00230-0" title="">https://doi.org/10.1016/B978-0-444-52272-6.00230-0</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/B9780444522726002300" title="">https://www.sciencedirect.com/science/article/pii/B9780444522726002300</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Riquelme et al. (2021)</span>
<span class="ltx_bibblock">
Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, and Neil Houlsby.

</span>
<span class="ltx_bibblock">Scaling vision with sparse mixture of experts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib181.1.1">Advances in Neural Information Processing Systems</em>, 34:8583–8595, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruan et al. (2024)</span>
<span class="ltx_bibblock">
Yangjun Ruan, Chris J. Maddison, and Tatsunori Hashimoto.

</span>
<span class="ltx_bibblock">Observational scaling laws and the predictability of language model performance, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2405.10938" title="">https://arxiv.org/abs/2405.10938</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saudek et al. (2008)</span>
<span class="ltx_bibblock">
Christopher D. Saudek, William H. Herman, David B. Sacks, Richard M. Bergenstal, David Edelman, and Mayer B. Davidson.

</span>
<span class="ltx_bibblock">A new look at screening and diagnosing diabetes mellitus.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib183.1.1">The Journal of Clinical Endocrinology</em>, 93(7):2447–2453, 07 2008.

</span>
<span class="ltx_bibblock">ISSN 0021-972X.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1210/jc.2007-2174" title="">10.1210/jc.2007-2174</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1210/jc.2007-2174" title="">https://doi.org/10.1210/jc.2007-2174</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schaeffer et al. (2023)</span>
<span class="ltx_bibblock">
Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo.

</span>
<span class="ltx_bibblock">Are emergent abilities of large language models a mirage?, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schaeffer et al. (2024a)</span>
<span class="ltx_bibblock">
Rylan Schaeffer, Hailey Schoelkopf, Brando Miranda, Gabriel Mukobi, Varun Madan, Adam Ibrahim, Herbie Bradley, Stella Biderman, and Sanmi Koyejo.

</span>
<span class="ltx_bibblock">Why has predicting downstream capabilities of frontier ai models with scale remained elusive?, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schaeffer et al. (2024b)</span>
<span class="ltx_bibblock">
Rylan Schaeffer, Hailey Schoelkopf, Brando Miranda, Gabriel Mukobi, Varun Madan, Adam Ibrahim, Herbie Bradley, Stella Biderman, and Sanmi Koyejo.

</span>
<span class="ltx_bibblock">Why has predicting downstream capabilities of frontier ai models with scale remained elusive?, 2024b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.04391" title="">https://arxiv.org/abs/2406.04391</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schaller (1997)</span>
<span class="ltx_bibblock">
R.R. Schaller.

</span>
<span class="ltx_bibblock">Moore’s law: past, present and future.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib187.1.1">IEEE Spectrum</em>, 34(6):52–59, 1997.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1109/6.591665" title="">10.1109/6.591665</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwartz et al. (2020)</span>
<span class="ltx_bibblock">
Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni.

</span>
<span class="ltx_bibblock">Green AI.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib188.1.1">Communications of the ACM (CACM)</em>, 63(12):54–63, November 2020.

</span>
<span class="ltx_bibblock">ISSN 0001-0782.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1145/3381831" title="">10.1145/3381831</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3381831" title="">https://doi.org/10.1145/3381831</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sculley et al. (2015)</span>
<span class="ltx_bibblock">
D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-François Crespo, and Dan Dennison.

</span>
<span class="ltx_bibblock">Hidden technical debt in machine learning systems.

</span>
<span class="ltx_bibblock">In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib189.1.1">Advances in Neural Information Processing Systems</em>, volume 28. Curran Associates, Inc., 2015.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">See et al. (2016)</span>
<span class="ltx_bibblock">
Abigail See, Minh-Thang Luong, and Christopher D. Manning.

</span>
<span class="ltx_bibblock">Compression of Neural Machine Translation Models via Pruning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib190.1.1">arXiv e-prints</em>, pp.  arXiv:1606.09274, Jun 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Senate (2024)</span>
<span class="ltx_bibblock">
California Senate.

</span>
<span class="ltx_bibblock">Senate bill 1047: Safe and secure innovation for frontier artificial intelligence models act., 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://legiscan.com/CA/text/SB1047/id/2999979" title="">https://legiscan.com/CA/text/SB1047/id/2999979</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seri &amp; Evans (2008)</span>
<span class="ltx_bibblock">
Irit Seri and Jonathan Evans.

</span>
<span class="ltx_bibblock">Limits of viability: definition of the gray zone.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib192.1.1">Journal of Perinatology</em>, 28 Suppl 1:S4–8, May 2008.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1038/jp.2008.42" title="">10.1038/jp.2008.42</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sevilla et al. (2022a)</span>
<span class="ltx_bibblock">
Jaime Sevilla, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos.

</span>
<span class="ltx_bibblock">Compute trends across three eras of machine learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib193.1.1">2022 International Joint Conference on Neural Networks (IJCNN)</em>. IEEE, July 2022a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1109/ijcnn55064.2022.9891914" title="">10.1109/ijcnn55064.2022.9891914</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1109/IJCNN55064.2022.9891914" title="">http://dx.doi.org/10.1109/IJCNN55064.2022.9891914</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sevilla et al. (2022b)</span>
<span class="ltx_bibblock">
Jaime Sevilla, Lennart Heim, Marius Hobbhahn, Tamay Besiroglu, Anson Ho, and Pablo Villalobos.

</span>
<span class="ltx_bibblock">Estimating training compute of deep learning models, 2022b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://epochai.org/blog/estimating-training-compute" title="">https://epochai.org/blog/estimating-training-compute</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-05-19.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shankar et al. (2022)</span>
<span class="ltx_bibblock">
Shreya Shankar, Rolando Garcia, Joseph M. Hellerstein, and Aditya G. Parameswaran.

</span>
<span class="ltx_bibblock">Operationalizing machine learning: An interview study, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer et al. (2018)</span>
<span class="ltx_bibblock">
Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and Blake Hechtman.

</span>
<span class="ltx_bibblock">Mesh-tensorflow: Deep learning for supercomputers, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2014)</span>
<span class="ltx_bibblock">
Ye Shen, Chao Li, Aura Heimonen, Jukka Meurman, Martha Nunn, Donald Miller, Thomas Van Dyke, Prashanti Bollu, Risto Kaaja, and Sok-Ja Janket.

</span>
<span class="ltx_bibblock">A pilot study on maternal oral health and birth weight of twins.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib197.1.1">Open Journal of Epidemiology</em>, 04:7–13, 01 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.4236/ojepi.2014.41002" title="">10.4236/ojepi.2014.41002</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shimabucoro et al. (2024)</span>
<span class="ltx_bibblock">
Luísa Shimabucoro, Sebastian Ruder, Julia Kreutzer, Marzieh Fadaee, and Sara Hooker.

</span>
<span class="ltx_bibblock">Llm see, llm do: Guiding data generation to target non-differentiable objectives, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.01490" title="">https://arxiv.org/abs/2407.01490</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siddiqui et al. (2022)</span>
<span class="ltx_bibblock">
Shoaib Ahmed Siddiqui, Nitarshan Rajkumar, Tegan Maharaj, David Krueger, and Sara Hooker.

</span>
<span class="ltx_bibblock">Metadata archaeology: Unearthing data subsets by leveraging training dynamics, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2024a)</span>
<span class="ltx_bibblock">
Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje F. Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzemiński, Hakimeh Fadaei, Irem Ergün, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Vu Minh Chien, Sebastian Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, Ahmet Üstün, Marzieh Fadaee, and Sara Hooker.

</span>
<span class="ltx_bibblock">Aya dataset: An open-access collection for multilingual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib200.1.1">arXiv preprint arXiv:2402.06619</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2024b)</span>
<span class="ltx_bibblock">
Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje F. Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzemiński, Hakimeh Fadaei, Irem Ergün, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Vu Minh Chien, Sebastian Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, Ahmet Üstün, Marzieh Fadaee, and Sara Hooker.

</span>
<span class="ltx_bibblock">Aya dataset: An open-access collection for multilingual instruction tuning, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sorscher et al. (2023)</span>
<span class="ltx_bibblock">
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S. Morcos.

</span>
<span class="ltx_bibblock">Beyond neural scaling laws: beating power law scaling via data pruning, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et al. (2023)</span>
<span class="ltx_bibblock">
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, and Abubakar Abid et al.

</span>
<span class="ltx_bibblock">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et al. (2014)</span>
<span class="ltx_bibblock">
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.

</span>
<span class="ltx_bibblock">Dropout: A simple way to prevent neural networks from overfitting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib204.1.1">Journal of Machine Learning Research</em>, 15(56):1929–1958, 2014.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://jmlr.org/papers/v15/srivastava14a.html" title="">http://jmlr.org/papers/v15/srivastava14a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Strubell et al. (2019a)</span>
<span class="ltx_bibblock">
Emma Strubell, Ananya Ganesh, and Andrew McCallum.

</span>
<span class="ltx_bibblock">Energy and Policy Considerations for Deep Learning in NLP.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib205.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pp.  3645–3650, Florence, Italy, July 2019a. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/P19-1355" title="">10.18653/v1/P19-1355</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P19-1355" title="">https://aclanthology.org/P19-1355</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib206">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Strubell et al. (2019b)</span>
<span class="ltx_bibblock">
Emma Strubell, Ananya Ganesh, and Andrew McCallum.

</span>
<span class="ltx_bibblock">Energy and policy considerations for deep learning in nlp, 2019b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib207">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ström (1997)</span>
<span class="ltx_bibblock">
Nikko Ström.

</span>
<span class="ltx_bibblock">Sparse connection and pruning in large dynamic artificial neural networks, 1997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib208">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stumpf &amp; Porter (2012)</span>
<span class="ltx_bibblock">
Michael P. H. Stumpf and Mason A. Porter.

</span>
<span class="ltx_bibblock">Critical truths about power laws.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib208.1.1">Science</em>, 335(6069):665–666, 2012.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1126/science.1216142" title="">10.1126/science.1216142</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.science.org/doi/abs/10.1126/science.1216142" title="">https://www.science.org/doi/abs/10.1126/science.1216142</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib209">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sukhbaatar et al. (2024)</span>
<span class="ltx_bibblock">
Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu, Xi Victoria Lin, Baptiste Rozière, Jacob Kahn, Daniel Li, Wen tau Yih, Jason Weston, and Xian Li.

</span>
<span class="ltx_bibblock">Branch-train-mix: Mixing expert llms into a mixture-of-experts llm, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib210">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sutton (2019)</span>
<span class="ltx_bibblock">
Richard Sutton.

</span>
<span class="ltx_bibblock">The bitter lesson, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" title="">http://www.incompleteideas.net/IncIdeas/BitterLesson.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib211">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Szegedy et al. (2014)</span>
<span class="ltx_bibblock">
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.

</span>
<span class="ltx_bibblock">Going deeper with convolutions, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib212">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al. (2024)</span>
<span class="ltx_bibblock">
Shawn Tan, Yikang Shen, Rameswar Panda, and Aaron Courville.

</span>
<span class="ltx_bibblock">Scattered mixture-of-experts implementation, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib213">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et al. (2022)</span>
<span class="ltx_bibblock">
Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q. Tran, Dani Yogatama, and Donald Metzler.

</span>
<span class="ltx_bibblock">Scaling laws vs model architectures: How does inductive bias influence scaling?, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib214">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taylor et al. (2022)</span>
<span class="ltx_bibblock">
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic.

</span>
<span class="ltx_bibblock">Galactica: A large language model for science, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib215">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al. (2024a)</span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, and Jean-Baptiste Alayrac et al.

</span>
<span class="ltx_bibblock">Gemini: A family of highly capable multimodal models, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib216">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team (2024)</span>
<span class="ltx_bibblock">
Gemma Team.

</span>
<span class="ltx_bibblock">Gemma, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.kaggle.com/m/3301" title="">https://www.kaggle.com/m/3301</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib217">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al. (2024b)</span>
<span class="ltx_bibblock">
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem,
Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy.

</span>
<span class="ltx_bibblock">Gemma: Open models based on gemini research and technology, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib218">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Terry-McElrath et al. (2011)</span>
<span class="ltx_bibblock">
Yvonne M. Terry-McElrath, Sherry Emery, Gery Szczypka, and Lloyd D. Johnston.

</span>
<span class="ltx_bibblock">Potential exposure to anti-drug advertising and drug-related attitudes, beliefs, and behaviors among united states youth, 1995-2006.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib218.1.1">Addictive Behaviors</em>, 36(1-2):116–124, 2011.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1016/j.addbeh.2010.09.005" title="">10.1016/j.addbeh.2010.09.005</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib219">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tessera et al. (2021)</span>
<span class="ltx_bibblock">
Kale-ab Tessera, Sara Hooker, and Benjamin Rosman.

</span>
<span class="ltx_bibblock">Keep the Gradients Flowing: Using Gradient Flow to Study Sparse Network Optimization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib219.1.1">arXiv e-prints</em>, art. arXiv:2102.01670, February 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib220">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thakkar et al. (2023)</span>
<span class="ltx_bibblock">
Megh Thakkar, Tolga Bolukbasi, Sriram Ganapathy, Shikhar Vashishth, Sarath Chandar, and Partha Talukdar.

</span>
<span class="ltx_bibblock">Self-influence guided data reweighting for language model pre-training, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib221">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">The White House (2023)</span>
<span class="ltx_bibblock">
The White House.

</span>
<span class="ltx_bibblock">Executive order on the safe, secure, and trustworthy development and use of artificial intelligence, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/" title="">https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib222">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thompson et al. (2020)</span>
<span class="ltx_bibblock">
Neil C. Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F. Manso.

</span>
<span class="ltx_bibblock">The Computational Limits of Deep Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib222.1.1">arXiv e-prints</em>, art. arXiv:2007.05558, July 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib223">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tirumala et al. (2023)</span>
<span class="ltx_bibblock">
Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari S. Morcos.

</span>
<span class="ltx_bibblock">D4: Improving llm pretraining via document de-duplication and diversification, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib224">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib225">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Treviso et al. (2023a)</span>
<span class="ltx_bibblock">
Marcos Treviso, Ji-Ung Lee, Tianchu Ji, Betty van Aken, Qingqing Cao, Manuel R. Ciosici, Michael Hassid, Kenneth Heafield, Sara Hooker, Colin Raffel, Pedro H. Martins, André F. T. Martins, Jessica Zosa Forde, Peter Milder, Edwin Simpson, Noam Slonim, Jesse Dodge, Emma Strubell, Niranjan Balasubramanian, Leon Derczynski, Iryna Gurevych, and Roy Schwartz.

</span>
<span class="ltx_bibblock">Efficient Methods for Natural Language Processing: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib225.1.1">Transactions of the Association for Computational Linguistics</em>, 11:826–860, 07 2023a.

</span>
<span class="ltx_bibblock">ISSN 2307-387X.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1162/tacl_a_00577" title="">10.1162/tacl_a_00577</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1162/tacl_a_00577" title="">https://doi.org/10.1162/tacl_a_00577</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib226">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Treviso et al. (2023b)</span>
<span class="ltx_bibblock">
Marcos Treviso, Ji-Ung Lee, Tianchu Ji, Betty van Aken, Qingqing Cao, Manuel R. Ciosici, Michael Hassid, Kenneth Heafield, Sara Hooker, Colin Raffel, Pedro H. Martins, André F. T. Martins, Jessica Zosa Forde, Peter Milder, Edwin Simpson, Noam Slonim, Jesse Dodge, Emma Strubell, Niranjan Balasubramanian, Leon Derczynski, Iryna Gurevych, and Roy Schwartz.

</span>
<span class="ltx_bibblock">Efficient Methods for Natural Language Processing: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib226.1.1">Transactions of the Association for Computational Linguistics</em>, 11:826–860, 07 2023b.

</span>
<span class="ltx_bibblock">ISSN 2307-387X.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1162/tacl_a_00577" title="">10.1162/tacl_a_00577</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1162/tacl_a_00577" title="">https://doi.org/10.1162/tacl_a_00577</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib227">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tunstall et al. (2023)</span>
<span class="ltx_bibblock">
Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf.

</span>
<span class="ltx_bibblock">Zephyr: Direct distillation of lm alignment, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib228">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">UK Government (2021)</span>
<span class="ltx_bibblock">
UK Government.

</span>
<span class="ltx_bibblock">International scientific report on the safety of advanced AI, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai" title="">https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib229">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">US Department of Transportation (2020)</span>
<span class="ltx_bibblock">
US Department of Transportation.

</span>
<span class="ltx_bibblock">Chapter 6: Government roles and related considerations, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://highways.dot.gov/safety/speed-management/speed-concepts-informational-guide/chapter-6-government-roles-related" title="">https://highways.dot.gov/safety/speed-management/speed-concepts-informational-guide/chapter-6-government-roles-related</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib230">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2023)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib231">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Villalobos &amp; Atkinson (2023)</span>
<span class="ltx_bibblock">
Pablo Villalobos and David Atkinson.

</span>
<span class="ltx_bibblock">Trading off compute in training and inference, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://epochai.org/blog/trading-off-compute-in-training-and-inference" title="">https://epochai.org/blog/trading-off-compute-in-training-and-inference</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-05-28.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib232">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2019)</span>
<span class="ltx_bibblock">
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.

</span>
<span class="ltx_bibblock">Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib233">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023a)</span>
<span class="ltx_bibblock">
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar.

</span>
<span class="ltx_bibblock">Voyager: An open-ended embodied agent with large language models, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib234">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023b)</span>
<span class="ltx_bibblock">
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar.

</span>
<span class="ltx_bibblock">Voyager: An open-ended embodied agent with large language models, 2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.16291" title="">https://arxiv.org/abs/2305.16291</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib235">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023c)</span>
<span class="ltx_bibblock">
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.

</span>
<span class="ltx_bibblock">Self-consistency improves chain of thought reasoning in language models, 2023c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib236">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus.

</span>
<span class="ltx_bibblock">Emergent Abilities of Large Language Models, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2206.07682" title="">https://arxiv.org/abs/2206.07682</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib237">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2023)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib238">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et al. (2016)</span>
<span class="ltx_bibblock">
W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li.

</span>
<span class="ltx_bibblock">Learning Structured Sparsity in Deep Neural Networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib238.1.1">ArXiv e-prints</em>, August 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib239">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wiggers (2023)</span>
<span class="ltx_bibblock">
Kyle Wiggers.

</span>
<span class="ltx_bibblock">Large language models exhibit significant western cultural bias, study finds, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://venturebeat.com/ai/large-language-models-exhibit-significant-western-cultural-bias-study-finds/" title="">https://venturebeat.com/ai/large-language-models-exhibit-significant-western-cultural-bias-study-finds/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib240">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Winograd (1980)</span>
<span class="ltx_bibblock">
Terry Winograd.

</span>
<span class="ltx_bibblock">What does it mean to understand language?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib240.1.1">Cognitive Science</em>, 4(3):209–241, 1980.

</span>
<span class="ltx_bibblock">ISSN 0364-0213.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/https://doi.org/10.1016/S0364-0213(80)80003-6" title="">https://doi.org/10.1016/S0364-0213(80)80003-6</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0364021380800036" title="">https://www.sciencedirect.com/science/article/pii/S0364021380800036</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib241">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Workshop et al. (2023)</span>
<span class="ltx_bibblock">
BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, and Suzana Ilić et al.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2211.05100" title="">https://arxiv.org/abs/2211.05100</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib242">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(242)</span>
<span class="ltx_bibblock">
World Health Organization.

</span>
<span class="ltx_bibblock">Global health observatory (gho) - data.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://apps.who.int/gho/data/view.main.54600" title="">https://apps.who.int/gho/data/view.main.54600</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib243">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2022)</span>
<span class="ltx_bibblock">
Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, Michael Gschwind, Anurag Gupta, Myle Ott, Anastasia Melnikov, Salvatore Candido, David Brooks, Geeta Chauhan, Benjamin Lee, Hsien-Hsin Lee, Bugra Akyildiz, Maximilian Balandat, Joe Spisak, Ravi Jain, Mike Rabbat, and Kim Hazelwood.

</span>
<span class="ltx_bibblock">Sustainable AI: Environmental Implications, Challenges and Opportunities.

</span>
<span class="ltx_bibblock">In D. Marculescu, Y. Chi, and C. Wu (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib243.1.1">Proceedings of Machine Learning and Systems</em>, volume 4, pp.  795–813, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlsys.org/paper/2022/file/ed3d2c21991e3bef5e069713af9fa6ca-Paper.pdf" title="">https://proceedings.mlsys.org/paper/2022/file/ed3d2c21991e3bef5e069713af9fa6ca-Paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib244">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. (2023)</span>
<span class="ltx_bibblock">
Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.

</span>
<span class="ltx_bibblock">SmoothQuant: Accurate and efficient post-training quantization for large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib244.1.1">Proceedings of the 40th International Conference on Machine Learning</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib245">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et al. (2023)</span>
<span class="ltx_bibblock">
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma.

</span>
<span class="ltx_bibblock">Effective long-context scaling of foundation models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib246">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al. (2023)</span>
<span class="ltx_bibblock">
Fuzhao Xue, Valerii Likhosherstov, Anurag Arnab, Neil Houlsby, Mostafa Dehghani, and Yang You.

</span>
<span class="ltx_bibblock">Adaptive computation with elastic input sequence, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib247">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoo et al. (2022)</span>
<span class="ltx_bibblock">
Joanna Yoo, Kuba Perlin, Siddhartha Rao Kamalakara, and João G. M. Araújo.

</span>
<span class="ltx_bibblock">Scalable training of language models using jax pjit and tpuv4, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib248">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2023)</span>
<span class="ltx_bibblock">
Binhang Yuan, Yongjun He, Jared Quincy Davis, Tianyi Zhang, Tri Dao, Beidi Chen, Percy Liang, Christopher Re, and Ce Zhang.

</span>
<span class="ltx_bibblock">Decentralized training of foundation models in heterogeneous environments, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2206.01288" title="">https://arxiv.org/abs/2206.01288</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib249">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zadouri et al. (2023)</span>
<span class="ltx_bibblock">
Ted Zadouri, Ahmet Üstün, Arash Ahmadian, Beyza Ermiş, Acyr Locatelli, and Sara Hooker.

</span>
<span class="ltx_bibblock">Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib250">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zaharia et al. (2024)</span>
<span class="ltx_bibblock">
Matei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Davis, Heather Miller, Chris Potts, James Zou, Michael Carbin, Jonathan Frankle, Naveen Rao, and Ali Ghodsi.

</span>
<span class="ltx_bibblock">The shift from models to compound ai systems.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/" title="">https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib251">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zakrzewski (2024)</span>
<span class="ltx_bibblock">
Cat Zakrzewski.

</span>
<span class="ltx_bibblock">The nation’s top ai safety lab is decaying from within, scientists say, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.washingtonpost.com/technology/2024/03/06/nist-ai-safety-lab-decaying/" title="">https://www.washingtonpost.com/technology/2024/03/06/nist-ai-safety-lab-decaying/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib252">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et al. (2019)</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi.

</span>
<span class="ltx_bibblock">Defending against neural fake news.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib252.1.1">CoRR</em>, abs/1905.12616, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1905.12616" title="">http://arxiv.org/abs/1905.12616</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib253">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang (2017)</span>
<span class="ltx_bibblock">
Hua Zhang.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib253.1.1">The History of Microwave Heating</em>.

</span>
<span class="ltx_bibblock">01 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib254">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023)</span>
<span class="ltx_bibblock">
Jiawei Zhou, Yixuan Zhang, Qianni Luo, Andrea G. Parker, and Munmun De Choudhury.

</span>
<span class="ltx_bibblock">Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions.

</span>
<span class="ltx_bibblock">In Albrecht Schmidt 0001, Kaisa Väänänen, Tesh Goyal, Per Ola Kristensson, Anicia Peters, Stefanie Mueller 0001, Julie R. Williamson, and Max L. Wilson 0001 (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib254.1.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI 2023, Hamburg, Germany, April 23-28, 2023</em>. ACM, 2023.

</span>
<span class="ltx_bibblock">ISBN 978-1-4503-9421-5.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1145/3544548.3581318" title="">10.1145/3544548.3581318</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3544548.3581318" title="">https://doi.org/10.1145/3544548.3581318</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib255">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Üstün et al. (2024)</span>
<span class="ltx_bibblock">
Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D’souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker.

</span>
<span class="ltx_bibblock">Aya model: An instruction finetuned open-access multilingual language model, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Technical Challenges of Measuring FLOP</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.p1.1.1">How to handle quantized models?</span> Models are often quantized during training to reduce memory requirements <cite class="ltx_cite ltx_citemacro_citep">(Ahmadian et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib7" title="">2023</a>; Marchisio et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib141" title="">2024</a>; Frantar et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib81" title="">2022</a>; Xiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib244" title="">2023</a>; Dettmers et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib59" title="">2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib58" title="">2022</a>; Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib130" title="">2024a</a>)</cite>. Increasingly modern networks are robust to higher level of quantization and can trained with weights at different levels of precision, such as FP16, FP8, INT8 and INT4. While the US Executive Order acknowledges the widespread use of quantization by applying the same compute threshold of <math alttext="10^{26}" class="ltx_Math" display="inline" id="A1.p1.1.m1.1"><semantics id="A1.p1.1.m1.1a"><msup id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml"><mn id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2.cmml">10</mn><mn id="A1.p1.1.m1.1.1.3" xref="A1.p1.1.m1.1.1.3.cmml">26</mn></msup><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1">superscript</csymbol><cn id="A1.p1.1.m1.1.1.2.cmml" type="integer" xref="A1.p1.1.m1.1.1.2">10</cn><cn id="A1.p1.1.m1.1.1.3.cmml" type="integer" xref="A1.p1.1.m1.1.1.3">26</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">10^{26}</annotation><annotation encoding="application/x-llamapun" id="A1.p1.1.m1.1d">10 start_POSTSUPERSCRIPT 26 end_POSTSUPERSCRIPT</annotation></semantics></math> to integer operations, the EU AI Act fails to specify how to handle integer operations. Both end up failing to handle quantized models in a meaningful way. In the case of the US Executive Order, setting the same threshold for integers and floating points makes no sense because typically lower precision operations impacts performance significantly <cite class="ltx_cite ltx_citemacro_citep">(Ahmadian et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib7" title="">2023</a>)</cite>. Hence, a quantized model will not present the same risk profile as a non-quantized model with the same number of FLOP. However, the EU AI Act risks completely ignoring any model with quantized operations and hence creates a loophole for application of compute thresholds.</p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1"><span class="ltx_text ltx_font_bold" id="A1.p2.1.1">Difference between theoretical and practical FLOP</span> The current legislation also fails to specify whether theoretical or practical FLOP will serve as the unit of measurement. Theoretical FLOP refers to the maximum number of FLOP a computer or processor can do based on its architecture and specifications. Measured FLOP, on the other hand, represents the actual computational performance observed during real-world applications. Theoretical FLOP are easier to measure because of the difficulty of consistently measuring FLOP across very different types of hardware <cite class="ltx_cite ltx_citemacro_citep">(Sevilla et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib194" title="">2022b</a>)</cite>.</p>
</div>
<div class="ltx_para" id="A1.p3">
<p class="ltx_p" id="A1.p3.1">Note that theoretical FLOP ignores practical factors, such as which parts of the model can be parallelized or hardware-related details like the cost of a memory access <cite class="ltx_cite ltx_citemacro_citep">(Dehghani et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib55" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="A1.p4">
<p class="ltx_p" id="A1.p4.1"><span class="ltx_text ltx_font_bold" id="A1.p4.1.1">Theoretical FLOP decreases with drop-out and sparsity.</span> Theoretical FLOP can be minimized by using drop-out and sparsity despite these models having comparable or even superior performance to fully dense models. For example, unstructured pruning <cite class="ltx_cite ltx_citemacro_citep">(Louizos et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib136" title="">2017</a>; Wen et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib238" title="">2016</a>; LeCun et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib122" title="">1990</a>; Hassibi et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib91" title="">1993a</a>; Ström, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib207" title="">1997</a>; Hassibi et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib92" title="">1993b</a>; See et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib190" title="">2016</a>; Evci et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib73" title="">2019</a>; Tessera et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib219" title="">2021</a>)</cite> and weight-specific quantization <cite class="ltx_cite ltx_citemacro_citep">(Jacob et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib107" title="">2018</a>; Courbariaux et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib49" title="">2014</a>; Hubara et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib106" title="">2016</a>; Gupta et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib88" title="">2015</a>; Aji &amp; Heafield, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib12" title="">2020</a>; Ahmadian et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib7" title="">2023</a>)</cite> are very successful compression techniques in deep neural networks. This keeps the overall structure of the original model, while significantly reducing the FLOP of the most expensive operations. Dropout <cite class="ltx_cite ltx_citemacro_citep">(Srivastava et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib204" title="">2014</a>)</cite> is a popular regularization strategy during pre-training, where weights are temporarily set to zero, but all weights are fully utilized during inference. However, both these techniques minimize the theoretical FLOP feasible.</p>
</div>
<figure class="ltx_figure" id="A1.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F9.sf1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F9.sf1.3.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text ltx_font_bold" id="A1.F9.sf1.4.2" style="font-size:90%;">Notable Systems 2010-24</span></figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="435" id="A1.F9.sf1.g1" src="x13.png" width="756"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F9.sf2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F9.sf2.3.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text ltx_font_bold" id="A1.F9.sf2.4.2" style="font-size:90%;">All Systems 2010-24</span></figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="429" id="A1.F9.sf2.g1" src="x14.png" width="753"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="A1.F9.3.2" style="font-size:90%;">In the main body, we plot all systems for the scatter plot, and notable systems for the box plot. Here we include the full set, with the equivalent scatter plot for notable models and a box plot of the distribution for all systems. Similar trends hold, with clearly notable differences between domains.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A1.F9.4">.</p>
</div>
</div>
</figure>
</section>
<section class="ltx_appendix" id="A2" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>A wider view of what determines return on compute</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1"><span class="ltx_text ltx_font_bold" id="A2.p1.1.1">Additional details on why convolutional and transformers unlock new patterns on scaling.</span> The introduction of a new architecture design can fundamentally change the relationship between compute and performance <cite class="ltx_cite ltx_citemacro_citep">(Tay et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib213" title="">2022</a>; Sevilla et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib193" title="">2022a</a>; Ho et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib97" title="">2024a</a>)</cite> and render any compute threshold that is set irrelevant. For example, the key breakthroughs in AI adoption around the world were the introduction of architectures like convolutional neural networks (CNNs) for vision <cite class="ltx_cite ltx_citemacro_citep">(Ciresan et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib44" title="">2011</a>; Krizhevsky et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib120" title="">2012</a>; Szegedy et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib211" title="">2014</a>)</cite> and Transformers for language modeling <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib230" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="A2.p2">
<p class="ltx_p" id="A2.p2.1">Both of these architectures have design details that make the search space for learning a good representation much more efficient. For example, convolutional neural networks apply the same set of filters across different regions of the input image. This assumes that the same feature can appear at different locations in the input image – for example “sky” can be in different parts of an image across a dataset. This local connectivity and weight sharing exploit the inherent spatial structure and local correlations present in natural images. It is also incredibly efficient, leading to a significant reduction in the number of parameters compared to fully connected neural networks; advantageous for vision problems, where the input data (images) tends to be high-dimensional <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib87" title="">2016</a>)</cite>. Transformers can handle variable-length input sequences efficiently and are highly scalable. The self-attention mechanism allows for parallel computation, enabling faster training and inference compared to sequential models like RNNs <cite class="ltx_cite ltx_citemacro_citep">(Treviso et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib225" title="">2023a</a>)</cite>. Our learning from both computer vision and language architectures highlights a crucial point – the architecture plays an enormous role at determining the overall rate of return in performance given a unit of compute. It also plays a crucial role in determining the ceiling of gains from compute.</p>
</div>
</section>
<section class="ltx_appendix" id="A3" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Energy Requirements of AI Workloads over Time</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">It is important to make a distinction between the shifting trends between compute and performance, and overall computational overhead of AI as a whole. While we will see ever smaller, more performant models – AI workloads will also be deployed in many more settings. This means that this essay should not be taken as a position that the overall environmental impact and energy cost of AI is not a formidable problem. This essay does not speak to the overall energy requirements of AI workloads over time. It only speaks to the bifurcation of trends where individual workloads are smaller and more performant. This caveate is important to make, because typically most energy requirements of AI workloads is not in training, but instead in deploying at test time. This means even if model size is trending smaller, overall energy requirements may still grow by AI be used in more and more places. While in the long run, smaller models help with efficiency and energy management, the widespread adoption of AI means overall energy requirements will likely continue to rise and is non-negligible  <cite class="ltx_cite ltx_citemacro_citep">(Strubell et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib205" title="">2019a</a>; Schwartz et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib188" title="">2020</a>; Derczynski, <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib56" title="">2020</a>; Patterson et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib161" title="">2021</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib243" title="">2022</a>; Treviso et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.05694v2#bib.bib226" title="">2023b</a>)</cite>. More work is needed to understand the intersection of these two dynamics, and how it impacts overall energy needs.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jul 30 02:55:47 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
