<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Controllable Synthetic Clinical Note Generation with Privacy Guarantees</title>
<!--Generated on Thu Sep 12 07:37:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.07809v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S1" title="In Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S2" title="In Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S2.SS1" title="In 2 Related Work ‣ Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Differential Privacy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S2.SS2" title="In 2 Related Work ‣ Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Differentially Private Stochastic Gradient Descent (DP-SGD)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S2.SS3" title="In 2 Related Work ‣ Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Training Language Models with Differential Privacy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S2.SS4" title="In 2 Related Work ‣ Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Clinical Structuring of Unstructured Text</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S2.SS5" title="In 2 Related Work ‣ Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Automatically Generated Instruction Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S3" title="In Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S4" title="In Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S4.SS1" title="In 4 Results ‣ Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Privacy-Utility Trade-off in Trained Language Models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S4.SS1.SSS1" title="In 4.1 Privacy-Utility Trade-off in Trained Language Models ‣ 4 Results ‣ Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Metrics for Privacy Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S4.SS1.SSS2" title="In 4.1 Privacy-Utility Trade-off in Trained Language Models ‣ 4 Results ‣ Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Privacy Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S4.SS2" title="In 4 Results ‣ Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Utility Test</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S5" title="In Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#A1" title="In Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix: Compute Used Overview</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Controllable Synthetic Clinical Note Generation with Privacy Guarantees</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Tal Baumel<sup class="ltx_sup" id="id10.10.id1"><span class="ltx_text ltx_font_italic" id="id10.10.id1.1">1</span></sup>, Andre Manoel<sup class="ltx_sup" id="id11.11.id2"><span class="ltx_text ltx_font_italic" id="id11.11.id2.1">2</span></sup>, Daniel Jones<sup class="ltx_sup" id="id12.12.id3"><span class="ltx_text ltx_font_italic" id="id12.12.id3.1">1</span></sup>, Shize Su<sup class="ltx_sup" id="id13.13.id4"><span class="ltx_text ltx_font_italic" id="id13.13.id4.1">1</span></sup>, 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id7.7.3">Huseyin Inan<sup class="ltx_sup" id="id7.7.3.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id7.7.3.1.1">1</span></sup>, Aaron (Ari) Bornstein<sup class="ltx_sup" id="id7.7.3.2"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id7.7.3.2.1">1</span></sup> and Robert Sim<sup class="ltx_sup" id="id7.7.3.3"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id7.7.3.3.1">1</span></sup></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id14.14.id5">1</sup>Microsoft Corporation 
<br class="ltx_break"/><sup class="ltx_sup" id="id15.15.id6">2</sup>Gretel.ai (Work done while at Microsoft)
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id16.16.id7">{tabaumel, shizs, jonesdaniel, huseyininan,
<br class="ltx_break"/>aribornstein, rsim}@microsoft.com</span> <span class="ltx_text ltx_font_typewriter" id="id17.17.id8">andremanoel@gmail.com</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id18.id1">In the field of machine learning, domain-specific annotated data is an invaluable resource for training effective models. However, in the medical domain, this data often includes Personal Health Information (PHI), raising significant privacy concerns. The stringent regulations surrounding PHI limit the availability and sharing of medical datasets, which poses a substantial challenge for researchers and practitioners aiming to develop advanced machine learning models. In this paper, we introduce a novel method to "clone" datasets containing PHI. Our approach ensures that the cloned datasets retain the essential characteristics and utility of the original data without compromising patient privacy. By leveraging differential-privacy techniques and a novel fine-tuning task, our method produces datasets that are free from identifiable information while preserving the statistical properties necessary for model training. We conduct utility testing to evaluate the performance of machine learning models trained on the cloned datasets. The results demonstrate that our cloned datasets not only uphold privacy standards but also enhance model performance compared to those trained on traditional anonymized datasets. This work offers a viable solution for the ethical and effective utilization of sensitive medical data in machine learning, facilitating progress in medical research and the development of robust predictive models.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.9">
<p class="ltx_p" id="p1.9.10"><span class="ltx_text ltx_font_bold" id="p1.9.10.1">Controllable Synthetic Clinical Note Generation with Privacy Guarantees</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.9.9" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.9.9.9" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.9.9.9.9">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.4.4.4.4.4">
<span class="ltx_td ltx_align_center" id="p1.4.4.4.4.4.4"><span class="ltx_text ltx_font_bold" id="p1.4.4.4.4.4.4.4">
Tal Baumel<sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.4.4.4.4.4.4.4.1.1">1</span></sup>, Andre Manoel<sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.2"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.4.4.4.4.4.4.4.2.1">2</span></sup>, Daniel Jones<sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.3"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.4.4.4.4.4.4.4.3.1">1</span></sup>, Shize Su<sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.4"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.4.4.4.4.4.4.4.4.1">1</span></sup>,</span></span></span>
<span class="ltx_tr" id="p1.7.7.7.7.7">
<span class="ltx_td ltx_align_center" id="p1.7.7.7.7.7.3"><span class="ltx_text ltx_font_bold" id="p1.7.7.7.7.7.3.3">Huseyin Inan<sup class="ltx_sup" id="p1.7.7.7.7.7.3.3.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.7.7.7.7.7.3.3.1.1">1</span></sup>, Aaron (Ari) Bornstein<sup class="ltx_sup" id="p1.7.7.7.7.7.3.3.2"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.7.7.7.7.7.3.3.2.1">1</span></sup> and Robert Sim<sup class="ltx_sup" id="p1.7.7.7.7.7.3.3.3"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.7.7.7.7.7.3.3.3.1">1</span></sup></span></span></span>
<span class="ltx_tr" id="p1.8.8.8.8.8">
<span class="ltx_td ltx_align_center" id="p1.8.8.8.8.8.1"><sup class="ltx_sup" id="p1.8.8.8.8.8.1.1">1</sup>Microsoft Corporation</span></span>
<span class="ltx_tr" id="p1.9.9.9.9.9">
<span class="ltx_td ltx_align_center" id="p1.9.9.9.9.9.1"><sup class="ltx_sup" id="p1.9.9.9.9.9.1.1">2</sup>Gretel.ai (Work done while at Microsoft)</span></span>
<span class="ltx_tr" id="p1.9.9.9.9.10.1">
<span class="ltx_td ltx_align_center" id="p1.9.9.9.9.10.1.1"><span class="ltx_text ltx_font_typewriter" id="p1.9.9.9.9.10.1.1.1">{tabaumel, shizs, jonesdaniel, huseyininan,</span></span></span>
<span class="ltx_tr" id="p1.9.9.9.9.11.2">
<span class="ltx_td ltx_align_center" id="p1.9.9.9.9.11.2.1"><span class="ltx_text ltx_font_typewriter" id="p1.9.9.9.9.11.2.1.1">aribornstein, rsim}@microsoft.com</span> <span class="ltx_text ltx_font_typewriter" id="p1.9.9.9.9.11.2.1.2">andremanoel@gmail.com</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Training Healthcare Language Models for downstream tasks requires quality clinical data that represents the real-world distribution of clinical texts. Acquiring such clinical data at scale is expensive and comes with many restrictions to protect patient privacy including siloing (data cannot leave the boundaries of health care providers), de-identification (Personal Identifiable Information (PII) and PHI needs to be removed), and data retention restrictions (patients and health care providers reserve the right to revoke access to training data at any time).
Data retention restrictions prevent the reproducibility of machine learning methods, as well as limit the value of expensive annotation of this data. We propose a novel method for generating controlled synthetic clinical data leveraging semantically informed instruction tuning datasets that preserve the style of siloed health care data and can be used for training Healthcare LLMs while preserving clinical accuracy, data privacy, and downstream modeling performance without violating data retention restrictions.
To resolve the healthcare data retention limitations generative models must generate outputs that meet the following conditions.
Output must be:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Stylistically akin to the original data.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Clinically accurate.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Privacy Preserving of the patients mentioned and not expose PII/PHI from the original data.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Diverse enough to be useful for modeling downstream tasks.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this paper we introduce a process that decouples the limitations of data retention while preserving if not improving, patient privacy.
By creating synthetic clinical notes in the style of a given dataset users can freely use the generated documents without any limitations. For example, if we are limited to a year of use there is a strong incentive to not invest in annotating the data since the annotations will have no utility once the terms of use expire. By generating synthetic data, we can annotate it without time restrictions and use the data freely for model training.
The process can be used in the future to create an expert language model trained on data from a specific domain, that was never available before without manually annotating massive amounts of data with instructions.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Differential Privacy</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Our approach employs differential privacy which enables model training that is relatively invariant to the presence or absence of a single patient. The following is a formal definition of differential privacy:</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="Thmdefinition1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="Thmdefinition1.1.1.1">Definition 1</span></span><span class="ltx_text ltx_font_bold" id="Thmdefinition1.2.2"> </span>(Differential Privacy (DP) <cite class="ltx_cite ltx_citemacro_cite">Dwork et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib11" title="">2006b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib10" title="">a</a>)</cite>)<span class="ltx_text ltx_font_bold" id="Thmdefinition1.3.3">.</span>
</h6>
<div class="ltx_para" id="Thmdefinition1.p1">
<p class="ltx_p" id="Thmdefinition1.p1.7"><span class="ltx_text ltx_font_italic" id="Thmdefinition1.p1.7.7">A randomized algorithm <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.1.1.m1.1"><semantics id="Thmdefinition1.p1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.1.1.m1.1.1" xref="Thmdefinition1.p1.1.1.m1.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.1.1.m1.1b"><ci id="Thmdefinition1.p1.1.1.m1.1.1.cmml" xref="Thmdefinition1.p1.1.1.m1.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.1.1.m1.1c">\mathcal{A}</annotation><annotation encoding="application/x-llamapun" id="Thmdefinition1.p1.1.1.m1.1d">caligraphic_A</annotation></semantics></math> is (<math alttext="\epsilon" class="ltx_Math" display="inline" id="Thmdefinition1.p1.2.2.m2.1"><semantics id="Thmdefinition1.p1.2.2.m2.1a"><mi id="Thmdefinition1.p1.2.2.m2.1.1" xref="Thmdefinition1.p1.2.2.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.2.2.m2.1b"><ci id="Thmdefinition1.p1.2.2.m2.1.1.cmml" xref="Thmdefinition1.p1.2.2.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.2.2.m2.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="Thmdefinition1.p1.2.2.m2.1d">italic_ϵ</annotation></semantics></math>,<math alttext="\delta" class="ltx_Math" display="inline" id="Thmdefinition1.p1.3.3.m3.1"><semantics id="Thmdefinition1.p1.3.3.m3.1a"><mi id="Thmdefinition1.p1.3.3.m3.1.1" xref="Thmdefinition1.p1.3.3.m3.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.3.3.m3.1b"><ci id="Thmdefinition1.p1.3.3.m3.1.1.cmml" xref="Thmdefinition1.p1.3.3.m3.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.3.3.m3.1c">\delta</annotation><annotation encoding="application/x-llamapun" id="Thmdefinition1.p1.3.3.m3.1d">italic_δ</annotation></semantics></math>)-differentially private if for any two neighboring datasets <math alttext="D" class="ltx_Math" display="inline" id="Thmdefinition1.p1.4.4.m4.1"><semantics id="Thmdefinition1.p1.4.4.m4.1a"><mi id="Thmdefinition1.p1.4.4.m4.1.1" xref="Thmdefinition1.p1.4.4.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.4.4.m4.1b"><ci id="Thmdefinition1.p1.4.4.m4.1.1.cmml" xref="Thmdefinition1.p1.4.4.m4.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.4.4.m4.1c">D</annotation><annotation encoding="application/x-llamapun" id="Thmdefinition1.p1.4.4.m4.1d">italic_D</annotation></semantics></math> and <math alttext="D^{\prime}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.5.5.m5.1"><semantics id="Thmdefinition1.p1.5.5.m5.1a"><msup id="Thmdefinition1.p1.5.5.m5.1.1" xref="Thmdefinition1.p1.5.5.m5.1.1.cmml"><mi id="Thmdefinition1.p1.5.5.m5.1.1.2" xref="Thmdefinition1.p1.5.5.m5.1.1.2.cmml">D</mi><mo id="Thmdefinition1.p1.5.5.m5.1.1.3" xref="Thmdefinition1.p1.5.5.m5.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.5.5.m5.1b"><apply id="Thmdefinition1.p1.5.5.m5.1.1.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1"><csymbol cd="ambiguous" id="Thmdefinition1.p1.5.5.m5.1.1.1.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1">superscript</csymbol><ci id="Thmdefinition1.p1.5.5.m5.1.1.2.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1.2">𝐷</ci><ci id="Thmdefinition1.p1.5.5.m5.1.1.3.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.5.5.m5.1c">D^{\prime}</annotation><annotation encoding="application/x-llamapun" id="Thmdefinition1.p1.5.5.m5.1d">italic_D start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>, which differ in exactly the data pertaining to a single user, and for all sets <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.6.6.m6.1"><semantics id="Thmdefinition1.p1.6.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.6.6.m6.1.1" xref="Thmdefinition1.p1.6.6.m6.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.6.6.m6.1b"><ci id="Thmdefinition1.p1.6.6.m6.1.1.cmml" xref="Thmdefinition1.p1.6.6.m6.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.6.6.m6.1c">\mathcal{S}</annotation><annotation encoding="application/x-llamapun" id="Thmdefinition1.p1.6.6.m6.1d">caligraphic_S</annotation></semantics></math> of possible outputs:
<math alttext="\textstyle{\Pr[\mathcal{A}(D)\in\mathcal{S}]\leq e^{\epsilon}\Pr[\mathcal{A}(D%
^{\prime})\in\mathcal{S}]+\delta}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.7.7.m7.5"><semantics id="Thmdefinition1.p1.7.7.m7.5a"><mrow id="Thmdefinition1.p1.7.7.m7.5.5" xref="Thmdefinition1.p1.7.7.m7.5.5.cmml"><mrow id="Thmdefinition1.p1.7.7.m7.4.4.1.1" xref="Thmdefinition1.p1.7.7.m7.4.4.1.2.cmml"><mi id="Thmdefinition1.p1.7.7.m7.2.2" xref="Thmdefinition1.p1.7.7.m7.2.2.cmml">Pr</mi><mo id="Thmdefinition1.p1.7.7.m7.4.4.1.1a" xref="Thmdefinition1.p1.7.7.m7.4.4.1.2.cmml">⁡</mo><mrow id="Thmdefinition1.p1.7.7.m7.4.4.1.1.1" xref="Thmdefinition1.p1.7.7.m7.4.4.1.2.cmml"><mo id="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.2" stretchy="false" xref="Thmdefinition1.p1.7.7.m7.4.4.1.2.cmml">[</mo><mrow id="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1" xref="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.cmml"><mrow id="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.2" xref="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.2.2" xref="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.2.2.cmml">𝒜</mi><mo id="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.2.1" xref="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.2.1.cmml">⁢</mo><mrow id="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.2.3.2" xref="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.2.cmml"><mo id="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.2.3.2.1" stretchy="false" xref="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.2.cmml">(</mo><mi id="Thmdefinition1.p1.7.7.m7.1.1" xref="Thmdefinition1.p1.7.7.m7.1.1.cmml">D</mi><mo id="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.2.3.2.2" stretchy="false" xref="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.1" xref="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.3" xref="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.3.cmml">𝒮</mi></mrow><mo id="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.3" stretchy="false" xref="Thmdefinition1.p1.7.7.m7.4.4.1.2.cmml">]</mo></mrow></mrow><mo id="Thmdefinition1.p1.7.7.m7.5.5.3" xref="Thmdefinition1.p1.7.7.m7.5.5.3.cmml">≤</mo><mrow id="Thmdefinition1.p1.7.7.m7.5.5.2" xref="Thmdefinition1.p1.7.7.m7.5.5.2.cmml"><mrow id="Thmdefinition1.p1.7.7.m7.5.5.2.1" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.cmml"><msup id="Thmdefinition1.p1.7.7.m7.5.5.2.1.3" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.3.cmml"><mi id="Thmdefinition1.p1.7.7.m7.5.5.2.1.3.2" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.3.2.cmml">e</mi><mi id="Thmdefinition1.p1.7.7.m7.5.5.2.1.3.3" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.3.3.cmml">ϵ</mi></msup><mo id="Thmdefinition1.p1.7.7.m7.5.5.2.1.2" lspace="0.167em" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.2.cmml">⁢</mo><mrow id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.2.cmml"><mi id="Thmdefinition1.p1.7.7.m7.3.3" xref="Thmdefinition1.p1.7.7.m7.3.3.cmml">Pr</mi><mo id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1a" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.2.cmml">⁡</mo><mrow id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.2.cmml"><mo id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.2" stretchy="false" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.2.cmml">[</mo><mrow id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.cmml"><mrow id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.3" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.3.cmml">𝒜</mi><mo id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.2" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1.1.cmml"><mo id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1.2" stretchy="false" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><msup id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1.1" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1.1.2" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1.1.2.cmml">D</mi><mo id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1.1.3" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1.1.3.cmml">′</mo></msup><mo id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1.3" stretchy="false" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.2" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.2.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.3" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.3.cmml">𝒮</mi></mrow><mo id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.3" stretchy="false" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.2.cmml">]</mo></mrow></mrow></mrow><mo id="Thmdefinition1.p1.7.7.m7.5.5.2.2" xref="Thmdefinition1.p1.7.7.m7.5.5.2.2.cmml">+</mo><mi id="Thmdefinition1.p1.7.7.m7.5.5.2.3" xref="Thmdefinition1.p1.7.7.m7.5.5.2.3.cmml">δ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.7.7.m7.5b"><apply id="Thmdefinition1.p1.7.7.m7.5.5.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5"><leq id="Thmdefinition1.p1.7.7.m7.5.5.3.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.3"></leq><apply id="Thmdefinition1.p1.7.7.m7.4.4.1.2.cmml" xref="Thmdefinition1.p1.7.7.m7.4.4.1.1"><ci id="Thmdefinition1.p1.7.7.m7.2.2.cmml" xref="Thmdefinition1.p1.7.7.m7.2.2">Pr</ci><apply id="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.cmml" xref="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1"><in id="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.1.cmml" xref="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.1"></in><apply id="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.2.cmml" xref="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.2"><times id="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.2.1.cmml" xref="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.2.1"></times><ci id="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.2.2.cmml" xref="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.2.2">𝒜</ci><ci id="Thmdefinition1.p1.7.7.m7.1.1.cmml" xref="Thmdefinition1.p1.7.7.m7.1.1">𝐷</ci></apply><ci id="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.3.cmml" xref="Thmdefinition1.p1.7.7.m7.4.4.1.1.1.1.3">𝒮</ci></apply></apply><apply id="Thmdefinition1.p1.7.7.m7.5.5.2.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2"><plus id="Thmdefinition1.p1.7.7.m7.5.5.2.2.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2.2"></plus><apply id="Thmdefinition1.p1.7.7.m7.5.5.2.1.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1"><times id="Thmdefinition1.p1.7.7.m7.5.5.2.1.2.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.2"></times><apply id="Thmdefinition1.p1.7.7.m7.5.5.2.1.3.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.3"><csymbol cd="ambiguous" id="Thmdefinition1.p1.7.7.m7.5.5.2.1.3.1.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.3">superscript</csymbol><ci id="Thmdefinition1.p1.7.7.m7.5.5.2.1.3.2.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.3.2">𝑒</ci><ci id="Thmdefinition1.p1.7.7.m7.5.5.2.1.3.3.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.3.3">italic-ϵ</ci></apply><apply id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.2.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1"><ci id="Thmdefinition1.p1.7.7.m7.3.3.cmml" xref="Thmdefinition1.p1.7.7.m7.3.3">Pr</ci><apply id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1"><in id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.2.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.2"></in><apply id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1"><times id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.2.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.2"></times><ci id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.3.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.3">𝒜</ci><apply id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1.1.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1.1.2">𝐷</ci><ci id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.1.1.1.1.3">′</ci></apply></apply><ci id="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.3.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2.1.1.1.1.1.3">𝒮</ci></apply></apply></apply><ci id="Thmdefinition1.p1.7.7.m7.5.5.2.3.cmml" xref="Thmdefinition1.p1.7.7.m7.5.5.2.3">𝛿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.7.7.m7.5c">\textstyle{\Pr[\mathcal{A}(D)\in\mathcal{S}]\leq e^{\epsilon}\Pr[\mathcal{A}(D%
^{\prime})\in\mathcal{S}]+\delta}</annotation><annotation encoding="application/x-llamapun" id="Thmdefinition1.p1.7.7.m7.5d">roman_Pr [ caligraphic_A ( italic_D ) ∈ caligraphic_S ] ≤ italic_e start_POSTSUPERSCRIPT italic_ϵ end_POSTSUPERSCRIPT roman_Pr [ caligraphic_A ( italic_D start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) ∈ caligraphic_S ] + italic_δ</annotation></semantics></math>.</span></p>
</div>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Differentially Private Stochastic Gradient Descent (DP-SGD)</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Differentially Private Stochastic Gradient Descent (DP-SGD) <cite class="ltx_cite ltx_citemacro_citep">(Song et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib21" title="">2013</a>; Bassily et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib3" title="">2014</a>; Abadi et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib1" title="">2016</a>)</cite> is a variant of the traditional SGD algorithm, tailored to ensure differential privacy during the training process. The key idea is to add controlled noise to the gradients, thereby masking the contribution of any single data point.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">The DP-SGD algorithm works as follows:</p>
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">Gradient Computation</span>: Compute the gradient of the loss function with respect to each data point in a randomly selected mini-batch.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Gradient Clipping</span>: Clip the gradients to ensure they have bounded norm. This prevents any single gradient from having too much influence.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.2"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.2.1">Noise Addition</span>: Add noise, typically drawn from a Gaussian distribution, to the clipped gradients. The scale of the noise is calibrated to the desired level of privacy, controlled by the parameters <math alttext="\epsilon" class="ltx_Math" display="inline" id="S2.I1.i3.p1.1.m1.1"><semantics id="S2.I1.i3.p1.1.m1.1a"><mi id="S2.I1.i3.p1.1.m1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.1.m1.1b"><ci id="S2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.1.m1.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.1.m1.1d">italic_ϵ</annotation></semantics></math> and <math alttext="\delta" class="ltx_Math" display="inline" id="S2.I1.i3.p1.2.m2.1"><semantics id="S2.I1.i3.p1.2.m2.1a"><mi id="S2.I1.i3.p1.2.m2.1.1" xref="S2.I1.i3.p1.2.m2.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.2.m2.1b"><ci id="S2.I1.i3.p1.2.m2.1.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.2.m2.1c">\delta</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.2.m2.1d">italic_δ</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i4.p1.1.1">Parameter Update</span>: Update the model parameters using the noisy gradients.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Training Language Models with Differential Privacy</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Differential privacy can be leveraged to train language models in a manner that protects the privacy of the underlying training data. When training a language model, the goal is to learn the statistical properties of the language without memorizing specific details about any individual data point.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Using DP-SGD, we can ensure that the model updates do not inadvertently leak information about any single training example. This is particularly important when training on sensitive datasets, such as those containing Personal Health Information or proprietary text.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Clinical Structuring of Unstructured Text</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">In this work, we extract structure using Text Analytics for Health (TA4H)<cite class="ltx_cite ltx_citemacro_cite">Bitran (<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib4" title="">2022</a>)</cite>. TA4H is a tool for extracting clinical named entities (e.g., medication, symptoms), linking the entities to anthologies such as UMLS <cite class="ltx_cite ltx_citemacro_cite">Bodenreider (<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib5" title="">2004</a>)</cite>, relationships between entities (e.g., medication dosage, body part direction), and assertions (e.g., negation, temporality).</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Automatically Generated Instruction Datasets</h3>
<div class="ltx_para" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">Instruction tuning is a technique designed to enhance the usefulness of language models by training them on Instruction/Response Tuples, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S2.F1" title="Figure 1 ‣ 2.5 Automatically Generated Instruction Datasets ‣ 2 Related Work ‣ Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_tag">1</span></a> (example taken from OpenAI grade school math dataset <cite class="ltx_cite ltx_citemacro_cite">Cobbe et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib7" title="">2021</a>)</cite>).</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="106" id="S2.F1.g1" src="extracted/5848271/figures/instruction_example.png" width="287"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Instruction dataset example taken from OpenAI
grade-school-math dataset
</figcaption>
</figure>
<div class="ltx_para" id="S2.SS5.p2">
<p class="ltx_p" id="S2.SS5.p2.1">This method has demonstrated impressive capabilities across a variety of NLP tasks <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib20" title="">2020</a>)</cite>. In previous work, such as T5 <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib20" title="">2020</a>)</cite>, an instruction dataset is automatically crafted by leveraging various NLP tasks, including Sentiment Analysis, Paraphrasing, Natural Language Inference (NLI), Co-reference Resolution, Word Sense Disambiguation, and more. These tasks are converted into an Instruction/Answer format. For example:</p>
</div>
<div class="ltx_para" id="S2.SS5.p3">
<p class="ltx_p" id="S2.SS5.p3.1"><span class="ltx_text ltx_font_italic" id="S2.SS5.p3.1.1">“summarize: state authorities dispatched emergency crews Tuesday to survey the damage after an onslaught of severe weather in Mississippi…” –&gt; “six people hospitalized after a storm in Attala County.”</span></p>
</div>
<div class="ltx_para" id="S2.SS5.p4">
<p class="ltx_p" id="S2.SS5.p4.1">By assigning predetermined templates to each NLP task, such datasets can be efficiently generated.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We developed the following novel process to enable the leveraging of siloed data so we can train a generative model to generate data in the style of the original siloed data for downstream tasks:</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="110" id="S3.F2.g1" src="extracted/5848271/figures/notes_generation.png" width="287"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Tuning model over examples from the instruct dataset and various use scenarios</figcaption>
</figure>
<div class="ltx_para" id="S3.p2">
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Construct reusable generation instruction templates that leverage clinical structuring types (entities, relations, assertions) that can be mapped to real clinical data. For example:
<span class="ltx_text ltx_font_typewriter" id="S3.I1.i1.p1.1.1">“
<br class="ltx_break"/>*** ENTITIES ***
<br class="ltx_break"/>- Symptom or sign: scarring
<br class="ltx_break"/>- Examination name: CT chest
<br class="ltx_break"/>
<br class="ltx_break"/>*** CLINICAL NOTE OF TYPE CXR ***
<br class="ltx_break"/>"
<br class="ltx_break"/></span>
We used MIMIC-III documents <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib15" title="">2016</a>)</cite>, 100,000 docs for training and 5,000 for training.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">The instructions and suitable completions are processed and populated automatically within a siloed security boundary, for example corresponding to a single medical provider’s compute environment leveraging the above templates and using clinical language understanding tools (such as text analytics for health) to map entities, relations, and assertions without exposing any siloed data (see figure <a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S3.F3" title="Figure 3 ‣ 3 Method ‣ Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">With the siloed instruction dataset and completion pairs, we can train LLM LoRA heads <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib13" title="">2021</a>)</cite> securely using the siloed instruction dataset, differential privacy, and anonymization methods to eliminate any privacy risks. The use of LoRA enables modeling and generation of data in the style of each silo using one base LLM model without the overhead of maintaining a new LLM model for each silo.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">Leveraging diverse sample prompts across a wide range of relations, entities, and assertions, a new dataset is created in the style of the original siloed data using the fine-tuned LoRA heads.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1">Through training a model on this synthetic data, we can demonstrate that it provides similar performance metrics, when compared to a model trained on the original siloed data.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S3.p2.1">This process decouples the limitations of data retention while preserving if not improving, patient privacy (see figure <a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S3.F2" title="Figure 2 ‣ 3 Method ‣ Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="S3.F3.g1" src="extracted/5848271/figures/instruct_dataset_gen.png" width="287"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Instruct dataset generation
</figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Privacy-Utility Trade-off in Trained Language Models</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">When training language models with Differential Privacy Stochastic Gradient Descent (DP-SGD), it is essential to evaluate the trade-off between privacy and utility. This section discusses the methods and metrics used to measure this trade-off.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Metrics for Privacy Evaluation</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">To quantify the level of privacy achieved by a trained language model, several metrics can be considered:</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Epsilon (<math alttext="\epsilon" class="ltx_Math" display="inline" id="S4.I1.i1.p1.1.1.m1.1"><semantics id="S4.I1.i1.p1.1.1.m1.1a"><mi id="S4.I1.i1.p1.1.1.m1.1.1" xref="S4.I1.i1.p1.1.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.1.1.m1.1b"><ci id="S4.I1.i1.p1.1.1.m1.1.1.cmml" xref="S4.I1.i1.p1.1.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.1.1.m1.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i1.p1.1.1.m1.1d">italic_ϵ</annotation></semantics></math>) Budget</span>: Measures the maximum allowable privacy loss. Smaller values indicate stronger privacy guarantees.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.2"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Delta (<math alttext="\delta" class="ltx_Math" display="inline" id="S4.I1.i2.p1.1.1.m1.1"><semantics id="S4.I1.i2.p1.1.1.m1.1a"><mi id="S4.I1.i2.p1.1.1.m1.1.1" xref="S4.I1.i2.p1.1.1.m1.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.1.1.m1.1b"><ci id="S4.I1.i2.p1.1.1.m1.1.1.cmml" xref="S4.I1.i2.p1.1.1.m1.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.1.m1.1c">\delta</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.1.1.m1.1d">italic_δ</annotation></semantics></math>) Parameter</span>: Quantifies the probability of failing to achieve <math alttext="\epsilon" class="ltx_Math" display="inline" id="S4.I1.i2.p1.2.m1.1"><semantics id="S4.I1.i2.p1.2.m1.1a"><mi id="S4.I1.i2.p1.2.m1.1.1" xref="S4.I1.i2.p1.2.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.2.m1.1b"><ci id="S4.I1.i2.p1.2.m1.1.1.cmml" xref="S4.I1.i2.p1.2.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.2.m1.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.2.m1.1d">italic_ϵ</annotation></semantics></math>-differential privacy.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">Utility Metrics</span>: Various metrics to assess the utility of the model, such as accuracy, perplexity, or task-specific performance.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.2">To evaluate the impact of privacy on model utility, we experimented with varying levels of differential privacy, selecting <math alttext="\epsilon" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.1.m1.1"><semantics id="S4.SS1.SSS1.p3.1.m1.1a"><mi id="S4.SS1.SSS1.p3.1.m1.1.1" xref="S4.SS1.SSS1.p3.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.1.m1.1b"><ci id="S4.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p3.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.1.m1.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.1.m1.1d">italic_ϵ</annotation></semantics></math> values of 2, 4, and 8, along with a non-private baseline. These <math alttext="\epsilon" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.2.m2.1"><semantics id="S4.SS1.SSS1.p3.2.m2.1a"><mi id="S4.SS1.SSS1.p3.2.m2.1.1" xref="S4.SS1.SSS1.p3.2.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.2.m2.1b"><ci id="S4.SS1.SSS1.p3.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p3.2.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.2.m2.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.2.m2.1d">italic_ϵ</annotation></semantics></math> values are commonly used in practice, striking a balance between privacy and utility; smaller values, such as 2, offer strong privacy guarantees, while larger values like 8 allow for more utility, making them suitable for comparing the trade-offs in practical applications.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Privacy Evaluation</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">Recent advances in membership inference attacks (MIAs) have demonstrated the effectiveness of the Robust Membership Inference Attack (rMIA), which significantly enhances privacy auditing capabilities while maintaining low computational costs. The rMIA method, introduced by Zarifzadeh et al. <cite class="ltx_cite ltx_citemacro_cite">Zarifzadeh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib22" title="">2024</a>)</cite>, leverages both reference models and population data to improve the likelihood ratio test used for detecting membership. In our work, due to constraints related to fine-tuning models on clinical notes, we adopted a setup similar to the rMIA’s low-cost scenario, utilizing only a single target model and a single reference model (an untrained version of the target model). On a dataset of 1000 in-distribution samples, with 50% randomly included in training, we employed black-box access to relative probabilities for calculating rMIA scores</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1">However, this setup did not provide sufficient signal to discern a clear trend between models with no privacy and those with added privacy, as all results generated an AUC around 0.5.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p3">
<p class="ltx_p" id="S4.SS1.SSS2.p3.1">These results can be attributed to several factors: the reference model used was the pre-trained model, which does not closely reflect the neighboring model expected by Differential Privacy (DP) assumptions. Furthermore, our canaries were sampled directly from the dataset distribution, as we opted against using synthetic canaries. Consequently, the constraints of our analysis precluded the deployment of a stronger adversary model that could extract more definitive signals. While the empirical privacy evaluation didn’t yield clear results, we are relying on prior work proving the validity of DP-transformers<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_text ltx_font_typewriter" id="footnote1.1">https://github.com/microsoft/dp-transformers</span></span></span></span>, which builds upon the well-established Opacus PyTorch library, ensuring strong differential privacy guarantees and making our approach theoretically sound.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.3.4.1.1">MLM data Model</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_t" id="S4.T1.3.4.1.2">privacy settings</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.3.4.1.3">i2b2_2009</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.3.4.1.4">n2c2_2022</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.3.4.1.5">BC5CDR</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.3.4.1.6">NCBI-disease</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.3.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" id="S4.T1.3.5.1.1">No MLM train</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_tt" id="S4.T1.3.5.1.2">N/A</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T1.3.5.1.3">0.86818</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T1.3.5.1.4">0.81984</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T1.3.5.1.5">0.81391</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T1.3.5.1.6">0.86835</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.6.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T1.3.6.2.1">Vanilla GPT-3</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr" id="S4.T1.3.6.2.2">N/A</th>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.6.2.3">0.86603</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.6.2.4">0.82577</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.6.2.5">0.81722</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.6.2.6">0.87042</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.7.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T1.3.7.3.1">Original MIMIC data</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr" id="S4.T1.3.7.3.2">N/A</th>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.7.3.3"><span class="ltx_text ltx_font_bold" id="S4.T1.3.7.3.3.1">0.87517</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.7.3.4"><span class="ltx_text ltx_font_bold" id="S4.T1.3.7.3.4.1">0.82873</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.7.3.5"><span class="ltx_text ltx_font_bold" id="S4.T1.3.7.3.5.1">0.82198</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.7.3.6"><span class="ltx_text ltx_font_bold" id="S4.T1.3.7.3.6.1">0.88047</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.8.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.3.8.4.1">GPT-3</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_t" id="S4.T1.3.8.4.2">DP</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.8.4.3">0.86854</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.8.4.4">0.82380</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.8.4.5">0.81782</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.8.4.6"><span class="ltx_text ltx_font_bold" id="S4.T1.3.8.4.6.1">0.87751</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.9.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T1.3.9.5.1">GPT-3</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr" id="S4.T1.3.9.5.2">w/o DP</th>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.9.5.3"><span class="ltx_text ltx_font_bold" id="S4.T1.3.9.5.3.1">0.87287</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.9.5.4"><span class="ltx_text ltx_font_bold" id="S4.T1.3.9.5.4.1">0.83157</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.9.5.5"><span class="ltx_text ltx_font_bold" id="S4.T1.3.9.5.5.1">0.82078</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.9.5.6">0.87412</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.2">PHI-2</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_t" id="S4.T1.1.1.1">DP <math alttext="\epsilon=2" class="ltx_Math" display="inline" id="S4.T1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.m1.1a"><mrow id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml"><mi id="S4.T1.1.1.1.m1.1.1.2" xref="S4.T1.1.1.1.m1.1.1.2.cmml">ϵ</mi><mo id="S4.T1.1.1.1.m1.1.1.1" xref="S4.T1.1.1.1.m1.1.1.1.cmml">=</mo><mn id="S4.T1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1"><eq id="S4.T1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1.1"></eq><ci id="S4.T1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.m1.1.1.2">italic-ϵ</ci><cn id="S4.T1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S4.T1.1.1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\epsilon=2</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.m1.1d">italic_ϵ = 2</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.1.3">0.86931</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.1.4">0.82640</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.1.5">0.81939</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.1.6">0.86318</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T1.2.2.2">PHI-2</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr" id="S4.T1.2.2.1">DP <math alttext="\epsilon=4" class="ltx_Math" display="inline" id="S4.T1.2.2.1.m1.1"><semantics id="S4.T1.2.2.1.m1.1a"><mrow id="S4.T1.2.2.1.m1.1.1" xref="S4.T1.2.2.1.m1.1.1.cmml"><mi id="S4.T1.2.2.1.m1.1.1.2" xref="S4.T1.2.2.1.m1.1.1.2.cmml">ϵ</mi><mo id="S4.T1.2.2.1.m1.1.1.1" xref="S4.T1.2.2.1.m1.1.1.1.cmml">=</mo><mn id="S4.T1.2.2.1.m1.1.1.3" xref="S4.T1.2.2.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.1.m1.1b"><apply id="S4.T1.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1"><eq id="S4.T1.2.2.1.m1.1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1.1"></eq><ci id="S4.T1.2.2.1.m1.1.1.2.cmml" xref="S4.T1.2.2.1.m1.1.1.2">italic-ϵ</ci><cn id="S4.T1.2.2.1.m1.1.1.3.cmml" type="integer" xref="S4.T1.2.2.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.1.m1.1c">\epsilon=4</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.1.m1.1d">italic_ϵ = 4</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.2.3">0.87441</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.2.4">0.82239</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.2.5">0.81972</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.2.2.6">0.87092</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T1.3.3.2">PHI-2</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr" id="S4.T1.3.3.1">DP <math alttext="\epsilon=8" class="ltx_Math" display="inline" id="S4.T1.3.3.1.m1.1"><semantics id="S4.T1.3.3.1.m1.1a"><mrow id="S4.T1.3.3.1.m1.1.1" xref="S4.T1.3.3.1.m1.1.1.cmml"><mi id="S4.T1.3.3.1.m1.1.1.2" xref="S4.T1.3.3.1.m1.1.1.2.cmml">ϵ</mi><mo id="S4.T1.3.3.1.m1.1.1.1" xref="S4.T1.3.3.1.m1.1.1.1.cmml">=</mo><mn id="S4.T1.3.3.1.m1.1.1.3" xref="S4.T1.3.3.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.1.m1.1b"><apply id="S4.T1.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1"><eq id="S4.T1.3.3.1.m1.1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1.1"></eq><ci id="S4.T1.3.3.1.m1.1.1.2.cmml" xref="S4.T1.3.3.1.m1.1.1.2">italic-ϵ</ci><cn id="S4.T1.3.3.1.m1.1.1.3.cmml" type="integer" xref="S4.T1.3.3.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.1.m1.1c">\epsilon=8</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.1.m1.1d">italic_ϵ = 8</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.3.3">0.87001</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.3.4"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.4.1">0.82673</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.3.5">0.81749</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.3.6">0.86477</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.10.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S4.T1.3.10.6.1">PHI-2</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_rr" id="S4.T1.3.10.6.2">w/o DP</th>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S4.T1.3.10.6.3"><span class="ltx_text ltx_font_bold" id="S4.T1.3.10.6.3.1">0.87600</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S4.T1.3.10.6.4">0.82577</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S4.T1.3.10.6.5"><span class="ltx_text ltx_font_bold" id="S4.T1.3.10.6.5.1">0.82826</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S4.T1.3.10.6.6"><span class="ltx_text ltx_font_bold" id="S4.T1.3.10.6.6.1">0.87434</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>F1 values for NER tasks after training encoder models generated data, base MLM model is RoBERTa</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Utility Test</h3>
<figure class="ltx_figure" id="S4.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="239" id="S4.F4.g1" src="x1.png" width="398"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="173" id="S4.F4.g2" src="extracted/5848271/figures/phi_ppl.png" width="287"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Perplexity over time of RoBERTa MLM train on each document set calculate against the original MIMIC texts</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="174" id="S4.F5.g1" src="extracted/5848271/figures/train_loss.png" width="287"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Train loss over time of RoBERTa MLM train on each document set</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In order to evaluate the utility of various candidate models we use the following setup to measure how well their generated data can contribute to downstream tasks. (see figure <a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S4.F6" title="Figure 6 ‣ 4.2 Utility Test ‣ 4 Results ‣ Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_tag">6</span></a>):</p>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1">Generate a document set using the instructions prompts created from MIMIC documents, we use a different generation LLM for each experiment</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1">Fine-tune RoBERTa <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib17" title="">2019</a>)</cite> masked language model (MLM) on the generated document set</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1">Train the fine-tuned RoBERTa model for NER tasks in the medical domain:</p>
<ul class="ltx_itemize" id="S4.I2.i3.I1">
<li class="ltx_item" id="S4.I2.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.I1.i1.1.1.1">–</span></span>
<div class="ltx_para" id="S4.I2.i3.I1.i1.p1">
<p class="ltx_p" id="S4.I2.i3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.I1.i1.p1.1.1">i2b2 2009<cite class="ltx_cite ltx_citemacro_cite">Patrick and Li (<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib19" title="">2010</a>)</cite></span>: shared task to extract medication-related information from narrative patient records
training (696 records) and test (533 records, 7,096 entities)</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.I1.i2.1.1.1">–</span></span>
<div class="ltx_para" id="S4.I2.i3.I1.i2.p1">
<p class="ltx_p" id="S4.I2.i3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.I1.i2.p1.1.1">n2c2 2022<cite class="ltx_cite ltx_citemacro_cite">Mahajan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib18" title="">2023</a>)</cite></span>: Social History Annotation Corpus (SHAC), this task explores the extraction of Social Determinants of Health (SDOH) from clinical notes, including substance use (alcohol, drug, tobacco), employment, and living status information [train (1,316 notes), validation (188 notes) and test set (373 notes, 3,855 entities)]</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.I1.i3.1.1.1">–</span></span>
<div class="ltx_para" id="S4.I2.i3.I1.i3.p1">
<p class="ltx_p" id="S4.I2.i3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.I1.i3.p1.1.1">BC5CDR<cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib16" title="">2016</a>)</cite></span>: PubMed articles with annotated chemicals, diseases and chemical-disease interactions.
train (5228 instances), validation (5330 instances) and test set (5865 instances, 14,177 entities)</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.I1.i4.1.1.1">–</span></span>
<div class="ltx_para" id="S4.I2.i3.I1.i4.p1">
<p class="ltx_p" id="S4.I2.i3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.I1.i4.p1.1.1">NCBI-disease<cite class="ltx_cite ltx_citemacro_cite">Doğan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib8" title="">2014</a>)</cite></span>: PubMed abstracts annotated with disease mentions.
train (5433 instances), validation (924 instances) and test set (941 instances, 2,055 entities)</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">We used several language models to generate the document set: GPT-3 <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib6" title="">2020</a>)</cite> model (<span class="ltx_text ltx_font_typewriter" id="S4.SS2.p2.1.1">code-cushman-002</span>) trained with DP<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Due to training costs of the GPT-3 model we didn’t train the model with different <math alttext="\epsilon" class="ltx_Math" display="inline" id="footnote2.m1.1"><semantics id="footnote2.m1.1b"><mi id="footnote2.m1.1.1" xref="footnote2.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="footnote2.m1.1c"><ci id="footnote2.m1.1.1.cmml" xref="footnote2.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m1.1d">\epsilon</annotation><annotation encoding="application/x-llamapun" id="footnote2.m1.1e">italic_ϵ</annotation></semantics></math> values</span></span></span>, GPT-3 model trained without DP, Vanilla GPT-3 model, PHI-2 <cite class="ltx_cite ltx_citemacro_cite">Gunasekar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib12" title="">2023</a>)</cite> (chosen due to it’s relatively small size) model trained with DP (with <math alttext="\epsilon" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">italic_ϵ</annotation></semantics></math> values of 2, 4 and 8) and PHI-2 model trained without DP) using a sample of instructions generated from MIMIC documents, we a compared the outputs to the original MIMIC documents as a test set for the MLM training. For future work we will evaluate generation with newer open source models such as Llama 3 <cite class="ltx_cite ltx_citemacro_cite">Dubey et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib9" title="">2024</a>)</cite>, Mistral <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib14" title="">2023</a>)</cite> and newer versions of Phi <cite class="ltx_cite ltx_citemacro_cite">Abdin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#bib.bib2" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">The hypothesis of this test is that the RoBERTa <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We choose RoBeRTA as an easy to train encoder model that will enable us to show our contribution</span></span></span> model without any MLM training will yield the worst results since it was not adapted to the clinical domain and the best preforming model will be the one tuned on the original MIMIC document since they best represent the domain. The question we want to determine is how well our method will bridge this gap.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="118" id="S4.F6.g1" src="extracted/5848271/figures/utility_test_process.png" width="287"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Process to obtain NER model for utility testing</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">Train loss from MLM training process (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S4.F5" title="Figure 5 ‣ 4.2 Utility Test ‣ 4 Results ‣ Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_tag">5</span></a>) illustrates how similar is the document set to the RoBERTa model expectations, while lower train loss corresponding with similarity. The most similar document set is the one generated from the vanilla GPT-3 model while the target MIMIC document set yields the highest train loss. All training process seems to converge in a similar manner over their respectful train sets.</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">Perplexity scores from the MLM training process (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S4.F4" title="Figure 4 ‣ 4.2 Utility Test ‣ 4 Results ‣ Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_tag">4</span></a>) show that training on documents generated by vanilla GPT-3 results in increased perplexity over the MIMIC test set, whereas all other models show a decrease. This demonstrates that our method, even with differential privacy (DP), learns to produce documents similar to the original MIMIC documents.</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.2">In table <a class="ltx_ref" href="https://arxiv.org/html/2409.07809v1#S4.T1" title="Table 1 ‣ 4.1.2 Privacy Evaluation ‣ 4.1 Privacy-Utility Trade-off in Trained Language Models ‣ 4 Results ‣ Controllable Synthetic Clinical Note Generation with Privacy Guarantees"><span class="ltx_text ltx_ref_tag">1</span></a> we present the weighted F1 scores of each experiment. Using the original MIMIC data to domain adapt the RoBERTa model yields the best performance while not adapting at yields the worst results. The utility of domain adapting the both GPT-3 and PHI-2 is clear while the use of DP causes only a slight drop and even improvement over w/o DP. In our setup decreasing the <math alttext="\epsilon" class="ltx_Math" display="inline" id="S4.SS2.p6.1.m1.1"><semantics id="S4.SS2.p6.1.m1.1a"><mi id="S4.SS2.p6.1.m1.1.1" xref="S4.SS2.p6.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.1.m1.1b"><ci id="S4.SS2.p6.1.m1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.1.m1.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p6.1.m1.1d">italic_ϵ</annotation></semantics></math> value seems to yield stable utility results where the best preforming <math alttext="\epsilon" class="ltx_Math" display="inline" id="S4.SS2.p6.2.m2.1"><semantics id="S4.SS2.p6.2.m2.1a"><mi id="S4.SS2.p6.2.m2.1.1" xref="S4.SS2.p6.2.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.2.m2.1b"><ci id="S4.SS2.p6.2.m2.1.1.cmml" xref="S4.SS2.p6.2.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.2.m2.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p6.2.m2.1d">italic_ϵ</annotation></semantics></math> value on average is 4.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this work, we introduce a novel method for generating ‘clone’ datasets using domain adapted entity extraction and templated instruction tuning designed to responsibly handle ephemeral constraints. We illustrate the balance between privacy and utility achieved through our approach in the creation of such datasets. Our experiments show that while the DP training causes minor drops in performance vs the original dataset it significantly improves downstream results over not using any of the protected data and its performance is stable for different <math alttext="\epsilon" class="ltx_Math" display="inline" id="S5.p1.1.m1.1"><semantics id="S5.p1.1.m1.1a"><mi id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><ci id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.m1.1d">italic_ϵ</annotation></semantics></math> values.

</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadi et al. (2016)</span>
<span class="ltx_bibblock">
Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016.

</span>
<span class="ltx_bibblock">Deep learning with differential privacy.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 2016 ACM Conference on Computer and Communications Security</em>, CCS ’16, pages 308–318. ACM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdin et al. (2024)</span>
<span class="ltx_bibblock">
Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Qin Cai, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Yen-Chun Chen, Yi-Ling Chen, Parul Chopra, Xiyang Dai, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Victor Fragoso, Dan Iter, Mei Gao, Min Gao, Jianfeng Gao, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Ce Liu, Mengchen Liu, Weishung Liu, Eric Lin, Zeqi Lin, Chong Luo, Piyush Madan, Matt Mazzola, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang
Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Xin Wang, Lijuan Wang, Chunyu Wang, Yu Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Haiping Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang, Jianwei Yang, Ziyi Yang, Yifan Yang, Donghan Yu, Lu Yuan, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2404.14219" title="">Phi-3 technical report: A highly capable language model locally on your phone</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bassily et al. (2014)</span>
<span class="ltx_bibblock">
Raef Bassily, Adam Smith, and Abhradeep Thakurta. 2014.

</span>
<span class="ltx_bibblock">Private empirical risk minimization: Efficient algorithms and tight error bounds.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 55th Annual IEEE Symposium on Foundations of Computer Science</em>, FOCS ’14, pages 464–473. IEEE Computer Society.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bitran (2022)</span>
<span class="ltx_bibblock">
Hadas Bitran. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://azure.microsoft.com/en-us/blog/expanding-ai-technology-for-unstructured-text-beyond-english/" title="">Expanding ai technology for unstructured biomedical text beyond english</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bodenreider (2004)</span>
<span class="ltx_bibblock">
Olivier Bodenreider. 2004.

</span>
<span class="ltx_bibblock">The unified medical language system (umls): integrating biomedical terminology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Nucleic acids research</em>, 32(suppl_1):D267–D270.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Advances in neural information processing systems</em>, 33:1877–1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et al. (2021)</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2110.14168</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doğan et al. (2014)</span>
<span class="ltx_bibblock">
Rezarta Islamaj Doğan, Robert Leaman, and Zhiyong Lu. 2014.

</span>
<span class="ltx_bibblock">Ncbi disease corpus: a resource for disease name recognition and concept normalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Journal of biomedical informatics</em>, 47:1–10.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al. (2024)</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,
Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier
Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu,
Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl
Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James
Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini
Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta,
Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vítor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2407.21783" title="">The llama 3 herd of models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork et al. (2006a)</span>
<span class="ltx_bibblock">
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. 2006a.

</span>
<span class="ltx_bibblock">Our data, ourselves: Privacy via distributed noise generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 24th Annual International Conference on the Theory and Applications of Cryptographic Techniques, EUROCRYPT ’06</em>, pages 486–503, Berlin, Heidelberg. Springer.

</span>
<span class="ltx_bibblock">2006a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork et al. (2006b)</span>
<span class="ltx_bibblock">
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006b.

</span>
<span class="ltx_bibblock">Calibrating noise to sensitivity in private data analysis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 3rd Conference on Theory of Cryptography, TCC ’06</em>, pages 265–284, Berlin, Heidelberg. Springer.

</span>
<span class="ltx_bibblock">2006b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gunasekar et al. (2023)</span>
<span class="ltx_bibblock">
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. 2023.

</span>
<span class="ltx_bibblock">Textbooks are all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2306.11644</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2021)</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2106.09685</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.06825" title="">Mistral 7b</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2016)</span>
<span class="ltx_bibblock">
Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. 2016.

</span>
<span class="ltx_bibblock">Mimic-iii, a freely accessible critical care database.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Scientific data</em>, 3(1):1–9.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2016)</span>
<span class="ltx_bibblock">
Jiao Li, Yueping Sun, Robin J. Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J. Mattingly, Thomas C. Wiegers, and Zhiyong Lu. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1093/database/baw068" title="">Biocreative V CDR task corpus: a resource for chemical disease relation extraction</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Database J. Biol. Databases Curation</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:1907.11692</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahajan et al. (2023)</span>
<span class="ltx_bibblock">
Diwakar Mahajan, Jennifer J Liang, Ching-Huei Tsou, and Özlem Uzuner. 2023.

</span>
<span class="ltx_bibblock">Overview of the 2022 n2c2 shared task on contextualized medication event extraction in clinical notes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Journal of Biomedical Informatics</em>, 144:104432.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patrick and Li (2010)</span>
<span class="ltx_bibblock">
Jon Patrick and Min Li. 2010.

</span>
<span class="ltx_bibblock">High accuracy information extraction of medication information from clinical notes: 2009 i2b2 medication extraction challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Journal of the American Medical Informatics Association</em>, 17(5):524–527.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://jmlr.org/papers/v21/20-074.html" title="">Exploring the limits of transfer learning with a unified text-to-text transformer</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Journal of Machine Learning Research</em>, 21(140):1–67.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2013)</span>
<span class="ltx_bibblock">
Shuang Song, Kamalika Chaudhuri, and Anand D. Sarwate. 2013.

</span>
<span class="ltx_bibblock">Stochastic gradient descent with differentially private updates.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 2013 IEEE Global Conference on Signal and Information Processing</em>, GlobalSIP ’13, pages 245–248. IEEE Computer Society.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zarifzadeh et al. (2024)</span>
<span class="ltx_bibblock">
Sajjad Zarifzadeh, Philippe Liu, and Reza Shokri. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2312.03262" title="">Low-cost high-power membership inference attacks</a>.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix: Compute Used Overview</h2>
<div class="ltx_para" id="A1.p1">
<ol class="ltx_enumerate" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1">For training GPT-3 models we used proprietary Microsoft infrastructure and cannot disclose the exact details of the compute.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1">For PHI-2 models we use a single compute node with 8 Nvidia V100 GPU, running for 60 hours.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.1">For MLM training of RoBERTa models we used a single node with 4 Nvidia v100 running for 30 minutes.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="A1.I1.i4.p1">
<p class="ltx_p" id="A1.I1.i4.p1.1">For tuned RoBERTa model training on NER task we used a single node with 4 Nvidia v100 running for 20 minutes.</p>
</div>
</li>
</ol>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 12 07:37:59 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
