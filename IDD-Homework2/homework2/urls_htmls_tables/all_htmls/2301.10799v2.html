<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2301.10799] Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering</title><meta property="og:description" content="The field of visual question answering (VQA) has recently seen a surge in research focused on providing explanations for predicted answers.
However, current systems mostly rely on separate models to predict answers and…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2301.10799">

<!--Generated on Fri Mar  1 05:39:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards a Unified Model for Generating Answers and Explanations 
<br class="ltx_break">in Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chenxi Whitehouse
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Tillman Weyde
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Pranava Madhyastha 
<br class="ltx_break">City
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> University of London 
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter">{chenxi.whitehouse, t.e.weyde, pranava.madhyastha}@city.ac.uk</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.1" class="ltx_p">The field of visual question answering (VQA) has recently seen a surge in research focused on providing explanations for predicted answers.
However, current systems mostly rely on separate models to predict answers and generate explanations, leading to less grounded and frequently inconsistent results.
To address this, we propose a multitask learning approach towards a <span id="id1.1.1" class="ltx_text ltx_font_bold">U</span>nified <span id="id1.1.2" class="ltx_text ltx_font_bold">M</span>odel for
<span id="id1.1.3" class="ltx_text ltx_font_bold">A</span>nswer and <span id="id1.1.4" class="ltx_text ltx_font_bold">E</span>xplanation generation (UMAE).
Our approach involves the addition of artificial prompt tokens to training data and fine-tuning a multimodal encoder-decoder model on a variety of VQA-related tasks.
In our experiments, UMAE models surpass the prior state-of-the-art answer accuracy on A-OKVQA by 10<math id="id1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><csymbol cd="latexml" id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\sim</annotation></semantics></math>15%, show competitive results on OK-VQA, achieve new state-of-the-art explanation scores on A-OKVQA and VCR, and demonstrate promising out-of-domain performance on VQA-X.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Code is available at: <a target="_blank" href="https://github.com/chenxwh/UMAE" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/chenxwh/UMAE</a>.</span></span></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Contemporary models for visual question answering (VQA) and commonsense reasoning are typically trained discriminatively to select the best answers from Multiple-Choice questions or to classify single-word answers to a predetermined vocabulary <cite class="ltx_cite ltx_citemacro_citep">(e.g. Anderson et al., <a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>.
Such settings often lead to limitations such as encouraging models to find superficial correlations <cite class="ltx_cite ltx_citemacro_cite">Ye and Kovashka (<a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite> or penalising model performance even when the answers are plausible (e.g. synonyms and multi-word expressions, and morphological variations are not considered correct).
Most current explanation generation models are trained independently of the QA model and the explanations are usually generated after the QA model has provided an answer.
As a result, these explanation models lack access to the process that generated the answer and thus the grounding of the explanation is limited to the answer text.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2301.10799/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="125" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Illustration of UMAE:
we train a multimodal encoder-decoder model on the mix of VQA tasks for jointly optimising answer and explanation, where we distinguish the training instances and target output with artificial prompt tokens (e.g. <span id="S1.F1.2.1" class="ltx_text ltx_font_typewriter">&lt;#AOKA#&gt;</span>).
The top and bottom examples are from A-OKVQA and VCR, respectively.
</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">We posit that a unified model that simultaneously performs answer prediction and explanation generation is a more effective and consistent approach for VQA.
Generative models, such as GPT-3 <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite>, T5 <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a href="#bib.bib23" title="" class="ltx_ref">2020</a>)</cite>, or OFA <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022a</a>)</cite>, have been shown to be successful at rapidly adapting to downstream tasks and generating high-quality open-ended text, and hence are suitable candidates for this unified approach.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We propose a multitask learning approach for multimodal transformer-based encoder-decoder models, towards a United Model for Answer and Explanation generation (UMAE).
In addition to the current trend of separate answer prediction and explanation generation based on the answers, our approach adds the capability of jointly generating answers and explanations together.
Inspired by the success of artificial prompt tokens in Neural Machine Translation (NMT) <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite>, we extend and demonstrate the efficacy of the artificial prompt-based method for VQA in a multitask setup.
We augment training instances with artificial prompt tokens, enabling the model to distinguish different tasks while learning shared semantic features.
Experiments on a combination of three knowledge-intensive VQA datasets, OK-VQA <cite class="ltx_cite ltx_citemacro_cite">Marino et al. (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>, A-OKVQA <cite class="ltx_cite ltx_citemacro_cite">Schwenk et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>, and VCR <cite class="ltx_cite ltx_citemacro_cite">Zellers et al. (<a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite>, show that the UMAE models achieve a new state-of-the-art (SOTA) answer accuracy on A-OKVQA, new SOTA explanation score on VCR, and competitive out-of-domain performance on VQA-X <cite class="ltx_cite ltx_citemacro_cite">Park et al. (<a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite>.
UMAE supports the generation of the answer to a question, the explanation for a given question and answer, and both together jointly, making the model efficient and flexible.
An illustration of the training setup is shown in <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our main contributions are:
a) the UMAE framework where answers and explanations can be generated by a single unified model (§<a href="#S3.SS1" title="3.1 Multitask Learning with Artificial Prompt ‣ 3 Methodology ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>);
b) a simple and efficient training approach that uses multitask learning with artificial prompts and demonstrates its ability to generalise across domains (§<a href="#S4" title="4 Experimental Setup ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>);
c) a method to map generated answers to Multiple-Choice options via evaluating the perplexity of the generation (§<a href="#S3.SS2" title="3.2 Perplexity as Multiple Choice Metric ‣ 3 Methodology ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>);
d) new SOTA results by UMAE, particularly for explanation generation and promising out-of-domain performance (§<a href="#S5" title="5 Results and Discussion ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Multimodal Transformer-based Models</span> achieve SOTA performance on various vision-language tasks <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>); Li et al. (<a href="#bib.bib15" title="" class="ltx_ref">2020</a>); Cho et al. (<a href="#bib.bib8" title="" class="ltx_ref">2021</a>); Wang et al. (<a href="#bib.bib29" title="" class="ltx_ref">2022c</a>); Zhang et al. (<a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite>.
They showcase the possibility of capturing richer multimodal semantic coherence than discriminatively trained models and are further capable of generating self-explanations.
Pretrained on multitask settings with natural language instructions, e.g. <span id="S2.p1.1.2" class="ltx_text ltx_font_italic">“what does the region describe?”</span>, models like OFA <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022a</a>)</cite> are claimed to have the capability to transfer to unseen tasks and domains via similar instructions. However, contrary to these claims, we observe that pretrained OFA is incapable of generating valid explanations through simple natural language instructions (<a href="#S5" title="5 Results and Discussion ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Artificial Prompt Tokens</span> have previously been explored for NMT by <cite class="ltx_cite ltx_citemacro_citet">Johnson et al. (<a href="#bib.bib13" title="" class="ltx_ref">2017</a>); Mitzalis et al. (<a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite>.
They propose a single model with the traditional NMT model architecture (usually for one language pair) and jointly train on different language pairs with added artificial prompts, e.g. <span id="S2.p2.1.2" class="ltx_text ltx_font_typewriter">2es</span> to distinguish the target language.
This approach has been found to foster implicit cross-lingual bridging and exhibit zero-shot translation capability.
In this paper, we exploit a similar approach with artificial prompts for answer and explanation generation in VQA with a united model.
This enables the model to learn shared features among tasks and datasets in various domains.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Explanation Generation for VQA</span> has gained growing interest in research.
However, most recent approaches use separate models to predict answers and generate explanations <cite class="ltx_cite ltx_citemacro_cite">Dua et al. (<a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Wu and Mooney (<a href="#bib.bib30" title="" class="ltx_ref">2019</a>)</cite> generate explanations with an object detector and a GRU unit for text embedding, then train on a subset of VQA-X in which the explanations contain the objects most attended to by the model.
<cite class="ltx_cite ltx_citemacro_citet">Kayser et al. (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite> develop an e-UG model combining UNITER <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite> for processing multimodal input and GPT-2 <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite> for generation.
In contrast, in this paper, we propose using a single united model for more grounded answer and explanation generation.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:356.2pt;height:95.3pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-59.4pt,15.8pt) scale(0.75,0.75) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="3"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">Model</span></th>
<th id="S3.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.1.2.1" class="ltx_text ltx_font_smallcaps">Ok-vqa</span></th>
<th id="S3.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5"><span id="S3.T1.1.1.1.1.3.1" class="ltx_text ltx_font_smallcaps">A-okvqa</span></th>
<th id="S3.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S3.T1.1.1.1.1.4.1" class="ltx_text ltx_font_smallcaps">Vcr</span></th>
</tr>
<tr id="S3.T1.1.1.2.2" class="ltx_tr">
<th id="S3.T1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.T1.1.1.2.2.1.1" class="ltx_text ltx_font_italic">direct answer</span></th>
<th id="S3.T1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="3"><span id="S3.T1.1.1.2.2.2.1" class="ltx_text ltx_font_italic">multiple choice</span></th>
<th id="S3.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2"><span id="S3.T1.1.1.2.2.3.1" class="ltx_text ltx_font_italic">direct answer</span></th>
<th id="S3.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.T1.1.1.2.2.4.1" class="ltx_text ltx_font_italic">multiple choice</span></th>
<th id="S3.T1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.T1.1.1.2.2.5.1" class="ltx_text ltx_font_smallcaps">bertscore</span></th>
</tr>
<tr id="S3.T1.1.1.3.3" class="ltx_tr">
<th id="S3.T1.1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.T1.1.1.3.3.1.1" class="ltx_text ltx_font_smallcaps">test</span></th>
<th id="S3.T1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span id="S3.T1.1.1.3.3.2.1" class="ltx_text ltx_font_smallcaps">val</span> (<span id="S3.T1.1.1.3.3.2.2" class="ltx_text ltx_font_italic">ppl</span>)</th>
<th id="S3.T1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span id="S3.T1.1.1.3.3.3.1" class="ltx_text ltx_font_smallcaps">val</span> (<span id="S3.T1.1.1.3.3.3.2" class="ltx_text ltx_font_italic">GloVe</span>)</th>
<th id="S3.T1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.T1.1.1.3.3.4.1" class="ltx_text ltx_font_smallcaps">test</span></th>
<th id="S3.T1.1.1.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.T1.1.1.3.3.5.1" class="ltx_text ltx_font_smallcaps">val</span></th>
<th id="S3.T1.1.1.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.T1.1.1.3.3.6.1" class="ltx_text ltx_font_smallcaps">test</span></th>
<th id="S3.T1.1.1.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span id="S3.T1.1.1.3.3.7.1" class="ltx_text ltx_font_smallcaps">val</span> (<span id="S3.T1.1.1.3.3.7.2" class="ltx_text ltx_font_italic">ppl</span>)</th>
<th id="S3.T1.1.1.3.3.8" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.T1.1.1.3.3.8.1" class="ltx_text ltx_font_smallcaps">val</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.4.1" class="ltx_tr">
<th id="S3.T1.1.1.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T1.1.1.4.1.1.1" class="ltx_text ltx_font_smallcaps">ofa*</span></th>
<td id="S3.T1.1.1.4.1.2" class="ltx_td ltx_align_center ltx_border_t">40.40</td>
<td id="S3.T1.1.1.4.1.3" class="ltx_td ltx_align_center ltx_border_t">24.54</td>
<td id="S3.T1.1.1.4.1.4" class="ltx_td ltx_align_center ltx_border_t">56.19</td>
<td id="S3.T1.1.1.4.1.5" class="ltx_td ltx_align_center ltx_border_t">47.40</td>
<td id="S3.T1.1.1.4.1.6" class="ltx_td ltx_align_center ltx_border_t">48.09</td>
<td id="S3.T1.1.1.4.1.7" class="ltx_td ltx_align_center ltx_border_t">39.77</td>
<td id="S3.T1.1.1.4.1.8" class="ltx_td ltx_align_center ltx_border_t">33.55</td>
<td id="S3.T1.1.1.4.1.9" class="ltx_td ltx_align_center ltx_border_t">64.55</td>
</tr>
<tr id="S3.T1.1.1.5.2" class="ltx_tr">
<th id="S3.T1.1.1.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.1.5.2.1.1" class="ltx_text ltx_font_smallcaps">ofa<sub id="S3.T1.1.1.5.2.1.1.1" class="ltx_sub">q-&gt;a</sub></span></th>
<td id="S3.T1.1.1.5.2.2" class="ltx_td ltx_align_center">49.93</td>
<td id="S3.T1.1.1.5.2.3" class="ltx_td ltx_align_center">74.32</td>
<td id="S3.T1.1.1.5.2.4" class="ltx_td ltx_align_center">65.30</td>
<td id="S3.T1.1.1.5.2.5" class="ltx_td ltx_align_center">61.71</td>
<td id="S3.T1.1.1.5.2.6" class="ltx_td ltx_align_center">63.00</td>
<td id="S3.T1.1.1.5.2.7" class="ltx_td ltx_align_center">53.91</td>
<td id="S3.T1.1.1.5.2.8" class="ltx_td ltx_align_center">54.89</td>
<td id="S3.T1.1.1.5.2.9" class="ltx_td ltx_align_center">83.85</td>
</tr>
<tr id="S3.T1.1.1.6.3" class="ltx_tr">
<th id="S3.T1.1.1.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.1.6.3.1.1" class="ltx_text ltx_font_smallcaps">umae<sub id="S3.T1.1.1.6.3.1.1.1" class="ltx_sub">all</sub></span></th>
<td id="S3.T1.1.1.6.3.2" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.6.3.2.1" class="ltx_text ltx_font_bold">51.77</span></td>
<td id="S3.T1.1.1.6.3.3" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.6.3.3.1" class="ltx_text ltx_font_bold">74.59</span></td>
<td id="S3.T1.1.1.6.3.4" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.6.3.4.1" class="ltx_text ltx_font_bold">65.67</span></td>
<td id="S3.T1.1.1.6.3.5" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.6.3.5.1" class="ltx_text ltx_font_bold">63.26</span></td>
<td id="S3.T1.1.1.6.3.6" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.6.3.6.1" class="ltx_text ltx_font_bold">63.29</span></td>
<td id="S3.T1.1.1.6.3.7" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.6.3.7.1" class="ltx_text ltx_font_bold">56.14</span></td>
<td id="S3.T1.1.1.6.3.8" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.6.3.8.1" class="ltx_text ltx_font_bold">56.66</span></td>
<td id="S3.T1.1.1.6.3.9" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.6.3.9.1" class="ltx_text ltx_font_bold">85.97</span></td>
</tr>
<tr id="S3.T1.1.1.7.4" class="ltx_tr">
<th id="S3.T1.1.1.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t"><span id="S3.T1.1.1.7.4.1.1" class="ltx_text ltx_font_smallcaps">Prior-best</span></th>
<td id="S3.T1.1.1.7.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">54.41</td>
<td id="S3.T1.1.1.7.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">–</td>
<td id="S3.T1.1.1.7.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">60.30</td>
<td id="S3.T1.1.1.7.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">53.70</td>
<td id="S3.T1.1.1.7.4.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">48.60</td>
<td id="S3.T1.1.1.7.4.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">40.70</td>
<td id="S3.T1.1.1.7.4.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">(77.10)<span id="S3.T1.1.1.7.4.8.1" class="ltx_text ltx_inline-block" style="width:0.0pt;"><sup id="S3.T1.1.1.7.4.8.1.1" class="ltx_sup">†</sup></span>
</td>
<td id="S3.T1.1.1.7.4.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">–</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance of models for answer generation.
Better results are in bold.
<span id="S3.T1.3.1" class="ltx_text ltx_font_smallcaps">ofa</span>* refers to the pretrained OFA.
Prior-best results
for the three datasets
are from <cite class="ltx_cite ltx_citemacro_citet">Gui et al. (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Schwenk et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a href="#bib.bib28" title="" class="ltx_ref">2022b</a>)</cite>, respectively.
† is from a discriminative model and thus not comparable
<cite class="ltx_cite ltx_citemacro_citep">(see Ye and Kovashka, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>.
</figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:388.1pt;height:149.3pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-64.7pt,24.8pt) scale(0.75,0.75) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="S3.T2.1.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">Dataset</span></td>
<td id="S3.T2.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T2.1.1.1.1.2.1" class="ltx_text ltx_font_smallcaps">Model</span></td>
<td id="S3.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">e<span id="S3.T2.1.1.1.1.3.1" class="ltx_text ltx_font_smallcaps">-V</span>i<span id="S3.T2.1.1.1.1.3.2" class="ltx_text ltx_font_smallcaps">l Scores</span>
</td>
<td id="S3.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="5"><span id="S3.T2.1.1.1.1.4.1" class="ltx_text ltx_font_smallcaps">N-gram Scores</span></td>
<td id="S3.T2.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.1.1.1.1.5.1" class="ltx_text ltx_font_smallcaps">Learnt Score</span></td>
</tr>
<tr id="S3.T2.1.1.2.2" class="ltx_tr">
<td id="S3.T2.1.1.2.2.1" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.2.2.1.1" class="ltx_text ltx_font_italic">S<sub id="S3.T2.1.1.2.2.1.1.1" class="ltx_sub"><span id="S3.T2.1.1.2.2.1.1.1.1" class="ltx_text ltx_font_smallcaps">O</span></sub></span></td>
<td id="S3.T2.1.1.2.2.2" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.2.2.2.1" class="ltx_text ltx_font_italic">S<sub id="S3.T2.1.1.2.2.2.1.1" class="ltx_sub"><span id="S3.T2.1.1.2.2.2.1.1.1" class="ltx_text ltx_font_smallcaps">T</span></sub></span></td>
<td id="S3.T2.1.1.2.2.3" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.2.2.3.1" class="ltx_text ltx_font_italic">S<sub id="S3.T2.1.1.2.2.3.1.1" class="ltx_sub"><span id="S3.T2.1.1.2.2.3.1.1.1" class="ltx_text ltx_font_smallcaps">E</span></sub></span></td>
<td id="S3.T2.1.1.2.2.4" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.2.2.4.1" class="ltx_text ltx_font_smallcaps">bleu4</span></td>
<td id="S3.T2.1.1.2.2.5" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.2.2.5.1" class="ltx_text ltx_font_smallcaps">rouge-l</span></td>
<td id="S3.T2.1.1.2.2.6" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.2.2.6.1" class="ltx_text ltx_font_smallcaps">meteor</span></td>
<td id="S3.T2.1.1.2.2.7" class="ltx_td ltx_align_right">
<span id="S3.T2.1.1.2.2.7.1" class="ltx_text ltx_font_smallcaps">cide</span>r</td>
<td id="S3.T2.1.1.2.2.8" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.2.2.8.1" class="ltx_text ltx_font_smallcaps">spice</span></td>
<td id="S3.T2.1.1.2.2.9" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.2.2.9.1" class="ltx_text ltx_font_smallcaps">bertscore</span></td>
</tr>
<tr id="S3.T2.1.1.3.3" class="ltx_tr">
<td id="S3.T2.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="4"><span id="S3.T2.1.1.3.3.1.1" class="ltx_text ltx_font_smallcaps">A-okvqa</span></td>
<td id="S3.T2.1.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.1.1.3.3.2.1" class="ltx_text ltx_font_smallcaps">ofa*</span></td>
<td id="S3.T2.1.1.3.3.3" class="ltx_td ltx_align_right ltx_border_t">4.44</td>
<td id="S3.T2.1.1.3.3.4" class="ltx_td ltx_align_right ltx_border_t">56.19</td>
<td id="S3.T2.1.1.3.3.5" class="ltx_td ltx_align_right ltx_border_t">7.90</td>
<td id="S3.T2.1.1.3.3.6" class="ltx_td ltx_align_right ltx_border_t">0.30</td>
<td id="S3.T2.1.1.3.3.7" class="ltx_td ltx_align_right ltx_border_t">4.45</td>
<td id="S3.T2.1.1.3.3.8" class="ltx_td ltx_align_right ltx_border_t">3.26</td>
<td id="S3.T2.1.1.3.3.9" class="ltx_td ltx_align_right ltx_border_t">4.82</td>
<td id="S3.T2.1.1.3.3.10" class="ltx_td ltx_align_right ltx_border_t">4.62</td>
<td id="S3.T2.1.1.3.3.11" class="ltx_td ltx_align_center ltx_border_t">68.64</td>
</tr>
<tr id="S3.T2.1.1.4.4" class="ltx_tr">
<td id="S3.T2.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S3.T2.1.1.4.4.1.1" class="ltx_text ltx_font_smallcaps">ofa<sub id="S3.T2.1.1.4.4.1.1.1" class="ltx_sub">q-&gt;a</sub>+ofa<sub id="S3.T2.1.1.4.4.1.1.2" class="ltx_sub">qa-&gt;e</sub></span></td>
<td id="S3.T2.1.1.4.4.2" class="ltx_td ltx_align_right">35.82</td>
<td id="S3.T2.1.1.4.4.3" class="ltx_td ltx_align_right">74.32</td>
<td id="S3.T2.1.1.4.4.4" class="ltx_td ltx_align_right">48.29</td>
<td id="S3.T2.1.1.4.4.5" class="ltx_td ltx_align_right">22.18</td>
<td id="S3.T2.1.1.4.4.6" class="ltx_td ltx_align_right">48.51</td>
<td id="S3.T2.1.1.4.4.7" class="ltx_td ltx_align_right">23.56</td>
<td id="S3.T2.1.1.4.4.8" class="ltx_td ltx_align_right">86.76</td>
<td id="S3.T2.1.1.4.4.9" class="ltx_td ltx_align_right">22.46</td>
<td id="S3.T2.1.1.4.4.10" class="ltx_td ltx_align_center">85.96</td>
</tr>
<tr id="S3.T2.1.1.5.5" class="ltx_tr">
<td id="S3.T2.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S3.T2.1.1.5.5.1.1" class="ltx_text ltx_font_smallcaps">umae<sub id="S3.T2.1.1.5.5.1.1.1" class="ltx_sub">a-okvqa</sub></span></td>
<td id="S3.T2.1.1.5.5.2" class="ltx_td ltx_align_right">37.10</td>
<td id="S3.T2.1.1.5.5.3" class="ltx_td ltx_align_right">73.97</td>
<td id="S3.T2.1.1.5.5.4" class="ltx_td ltx_align_right">50.15</td>
<td id="S3.T2.1.1.5.5.5" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.5.5.5.1" class="ltx_text ltx_font_bold">27.61</span></td>
<td id="S3.T2.1.1.5.5.6" class="ltx_td ltx_align_right">52.23</td>
<td id="S3.T2.1.1.5.5.7" class="ltx_td ltx_align_right">24.06</td>
<td id="S3.T2.1.1.5.5.8" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.5.5.8.1" class="ltx_text ltx_font_bold">104.39</span></td>
<td id="S3.T2.1.1.5.5.9" class="ltx_td ltx_align_right">22.88</td>
<td id="S3.T2.1.1.5.5.10" class="ltx_td ltx_align_center">87.86</td>
</tr>
<tr id="S3.T2.1.1.6.6" class="ltx_tr">
<td id="S3.T2.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S3.T2.1.1.6.6.1.1" class="ltx_text ltx_font_smallcaps">umae<sub id="S3.T2.1.1.6.6.1.1.1" class="ltx_sub">all</sub></span></td>
<td id="S3.T2.1.1.6.6.2" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.6.6.2.1" class="ltx_text ltx_font_bold">37.91</span></td>
<td id="S3.T2.1.1.6.6.3" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.6.6.3.1" class="ltx_text ltx_font_bold">74.59</span></td>
<td id="S3.T2.1.1.6.6.4" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.6.6.4.1" class="ltx_text ltx_font_bold">50.82</span></td>
<td id="S3.T2.1.1.6.6.5" class="ltx_td ltx_align_right">27.35</td>
<td id="S3.T2.1.1.6.6.6" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.6.6.6.1" class="ltx_text ltx_font_bold">52.56</span></td>
<td id="S3.T2.1.1.6.6.7" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.6.6.7.1" class="ltx_text ltx_font_bold">24.83</span></td>
<td id="S3.T2.1.1.6.6.8" class="ltx_td ltx_align_right">101.09</td>
<td id="S3.T2.1.1.6.6.9" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.6.6.9.1" class="ltx_text ltx_font_bold">23.33</span></td>
<td id="S3.T2.1.1.6.6.10" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.6.6.10.1" class="ltx_text ltx_font_bold">88.21</span></td>
</tr>
<tr id="S3.T2.1.1.7.7" class="ltx_tr">
<td id="S3.T2.1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="3"><span id="S3.T2.1.1.7.7.1.1" class="ltx_text ltx_font_smallcaps">Vcr</span></td>
<td id="S3.T2.1.1.7.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">e<span id="S3.T2.1.1.7.7.2.1" class="ltx_text ltx_font_smallcaps">-ug</span>
</td>
<td id="S3.T2.1.1.7.7.3" class="ltx_td ltx_align_right ltx_border_t">19.30</td>
<td id="S3.T2.1.1.7.7.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T2.1.1.7.7.4.1" class="ltx_text ltx_font_bold">69.80</span></td>
<td id="S3.T2.1.1.7.7.5" class="ltx_td ltx_align_right ltx_border_t">27.60</td>
<td id="S3.T2.1.1.7.7.6" class="ltx_td ltx_align_right ltx_border_t">4.30</td>
<td id="S3.T2.1.1.7.7.7" class="ltx_td ltx_align_right ltx_border_t">22.50</td>
<td id="S3.T2.1.1.7.7.8" class="ltx_td ltx_align_right ltx_border_t">11.80</td>
<td id="S3.T2.1.1.7.7.9" class="ltx_td ltx_align_right ltx_border_t">32.70</td>
<td id="S3.T2.1.1.7.7.10" class="ltx_td ltx_align_right ltx_border_t">12.60</td>
<td id="S3.T2.1.1.7.7.11" class="ltx_td ltx_align_center ltx_border_t">79.00</td>
</tr>
<tr id="S3.T2.1.1.8.8" class="ltx_tr">
<td id="S3.T2.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S3.T2.1.1.8.8.1.1" class="ltx_text ltx_font_smallcaps">umae<sub id="S3.T2.1.1.8.8.1.1.1" class="ltx_sub">vcr</sub></span></td>
<td id="S3.T2.1.1.8.8.2" class="ltx_td ltx_align_right">22.57</td>
<td id="S3.T2.1.1.8.8.3" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.8.8.3.1" class="ltx_text ltx_font_bold">56.68</span></td>
<td id="S3.T2.1.1.8.8.4" class="ltx_td ltx_align_right">39.82</td>
<td id="S3.T2.1.1.8.8.5" class="ltx_td ltx_align_right">12.25</td>
<td id="S3.T2.1.1.8.8.6" class="ltx_td ltx_align_right">28.87</td>
<td id="S3.T2.1.1.8.8.7" class="ltx_td ltx_align_right">16.67</td>
<td id="S3.T2.1.1.8.8.8" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.8.8.8.1" class="ltx_text ltx_font_bold">48.14</span></td>
<td id="S3.T2.1.1.8.8.9" class="ltx_td ltx_align_right">27.36</td>
<td id="S3.T2.1.1.8.8.10" class="ltx_td ltx_align_center">81.77</td>
</tr>
<tr id="S3.T2.1.1.9.9" class="ltx_tr">
<td id="S3.T2.1.1.9.9.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S3.T2.1.1.9.9.1.1" class="ltx_text ltx_font_smallcaps">umae<sub id="S3.T2.1.1.9.9.1.1.1" class="ltx_sub">all</sub></span></td>
<td id="S3.T2.1.1.9.9.2" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.9.9.2.1" class="ltx_text ltx_font_bold">22.82</span></td>
<td id="S3.T2.1.1.9.9.3" class="ltx_td ltx_align_right">56.66</td>
<td id="S3.T2.1.1.9.9.4" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.9.9.4.1" class="ltx_text ltx_font_bold">40.27</span></td>
<td id="S3.T2.1.1.9.9.5" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.9.9.5.1" class="ltx_text ltx_font_bold">13.44</span></td>
<td id="S3.T2.1.1.9.9.6" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.9.9.6.1" class="ltx_text ltx_font_bold">29.53</span></td>
<td id="S3.T2.1.1.9.9.7" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.9.9.7.1" class="ltx_text ltx_font_bold">17.54</span></td>
<td id="S3.T2.1.1.9.9.8" class="ltx_td ltx_align_right">47.33</td>
<td id="S3.T2.1.1.9.9.9" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.9.9.9.1" class="ltx_text ltx_font_bold">26.45</span></td>
<td id="S3.T2.1.1.9.9.10" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.9.9.10.1" class="ltx_text ltx_font_bold">81.91</span></td>
</tr>
<tr id="S3.T2.1.1.10.10" class="ltx_tr">
<td id="S3.T2.1.1.10.10.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="2"><span id="S3.T2.1.1.10.10.1.1" class="ltx_text ltx_font_smallcaps">Vqa-x</span></td>
<td id="S3.T2.1.1.10.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">e<span id="S3.T2.1.1.10.10.2.1" class="ltx_text ltx_font_smallcaps">-ug</span>
</td>
<td id="S3.T2.1.1.10.10.3" class="ltx_td ltx_align_right ltx_border_t">36.50</td>
<td id="S3.T2.1.1.10.10.4" class="ltx_td ltx_align_right ltx_border_t">80.50</td>
<td id="S3.T2.1.1.10.10.5" class="ltx_td ltx_align_right ltx_border_t">45.40</td>
<td id="S3.T2.1.1.10.10.6" class="ltx_td ltx_align_right ltx_border_t">23.20</td>
<td id="S3.T2.1.1.10.10.7" class="ltx_td ltx_align_right ltx_border_t">45.70</td>
<td id="S3.T2.1.1.10.10.8" class="ltx_td ltx_align_right ltx_border_t">22.10</td>
<td id="S3.T2.1.1.10.10.9" class="ltx_td ltx_align_right ltx_border_t">74.10</td>
<td id="S3.T2.1.1.10.10.10" class="ltx_td ltx_align_right ltx_border_t">20.10</td>
<td id="S3.T2.1.1.10.10.11" class="ltx_td ltx_align_center ltx_border_t">87.00</td>
</tr>
<tr id="S3.T2.1.1.11.11" class="ltx_tr">
<td id="S3.T2.1.1.11.11.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="background-color:#DDEBF7;"><span id="S3.T2.1.1.11.11.1.1" class="ltx_text ltx_font_smallcaps" style="background-color:#DDEBF7;">umae<sub id="S3.T2.1.1.11.11.1.1.1" class="ltx_sub">all</sub></span></td>
<td id="S3.T2.1.1.11.11.2" class="ltx_td ltx_align_right ltx_border_bb" style="background-color:#DDEBF7;"><span id="S3.T2.1.1.11.11.2.1" class="ltx_text" style="background-color:#DDEBF7;">31.58</span></td>
<td id="S3.T2.1.1.11.11.3" class="ltx_td ltx_align_right ltx_border_bb" style="background-color:#DDEBF7;"><span id="S3.T2.1.1.11.11.3.1" class="ltx_text" style="background-color:#DDEBF7;">77.65</span></td>
<td id="S3.T2.1.1.11.11.4" class="ltx_td ltx_align_right ltx_border_bb" style="background-color:#DDEBF7;"><span id="S3.T2.1.1.11.11.4.1" class="ltx_text" style="background-color:#DDEBF7;">40.67</span></td>
<td id="S3.T2.1.1.11.11.5" class="ltx_td ltx_align_right ltx_border_bb" style="background-color:#DDEBF7;"><span id="S3.T2.1.1.11.11.5.1" class="ltx_text" style="background-color:#DDEBF7;">14.63</span></td>
<td id="S3.T2.1.1.11.11.6" class="ltx_td ltx_align_right ltx_border_bb" style="background-color:#DDEBF7;"><span id="S3.T2.1.1.11.11.6.1" class="ltx_text" style="background-color:#DDEBF7;">35.12</span></td>
<td id="S3.T2.1.1.11.11.7" class="ltx_td ltx_align_right ltx_border_bb" style="background-color:#DDEBF7;"><span id="S3.T2.1.1.11.11.7.1" class="ltx_text" style="background-color:#DDEBF7;">20.29</span></td>
<td id="S3.T2.1.1.11.11.8" class="ltx_td ltx_align_right ltx_border_bb" style="background-color:#DDEBF7;"><span id="S3.T2.1.1.11.11.8.1" class="ltx_text" style="background-color:#DDEBF7;">50.35</span></td>
<td id="S3.T2.1.1.11.11.9" class="ltx_td ltx_align_right ltx_border_bb" style="background-color:#DDEBF7;"><span id="S3.T2.1.1.11.11.9.1" class="ltx_text" style="background-color:#DDEBF7;">19.13</span></td>
<td id="S3.T2.1.1.11.11.10" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;"><span id="S3.T2.1.1.11.11.10.1" class="ltx_text" style="background-color:#DDEBF7;">85.40</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Explanation Scores.
<span id="S3.T2.4.1" class="ltx_text ltx_font_smallcaps">ofa</span>* is the pretrained OFA, showing the transferability of OFA for generating explanations with natural language instructions.
Results with e-UG are from <cite class="ltx_cite ltx_citemacro_citet">Kayser et al. (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>.
We show the best results of A-OKVQA and VCR in bold.
The last row in blue shade shows <span id="S3.T2.5.2" class="ltx_text ltx_font_italic">out-of-domain</span> performance.
</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Multitask Learning with Artificial Prompt</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.3" class="ltx_p">We formulate three generation settings: <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">Q<math id="S3.SS1.p1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS1.p1.1.1.m1.1a"><mo stretchy="false" id="S3.SS1.p1.1.1.m1.1.1" xref="S3.SS1.p1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.1.m1.1b"><ci id="S3.SS1.p1.1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.1.m1.1c">\rightarrow</annotation></semantics></math>A</span>: answer prediction; <span id="S3.SS1.p1.2.2" class="ltx_text ltx_font_typewriter">QA<math id="S3.SS1.p1.2.2.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS1.p1.2.2.m1.1a"><mo stretchy="false" id="S3.SS1.p1.2.2.m1.1.1" xref="S3.SS1.p1.2.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.2.m1.1b"><ci id="S3.SS1.p1.2.2.m1.1.1.cmml" xref="S3.SS1.p1.2.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.2.m1.1c">\rightarrow</annotation></semantics></math>E</span>: explanation generation conditioned on the answer; and <span id="S3.SS1.p1.3.3" class="ltx_text ltx_font_typewriter">Q<math id="S3.SS1.p1.3.3.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS1.p1.3.3.m1.1a"><mo stretchy="false" id="S3.SS1.p1.3.3.m1.1.1" xref="S3.SS1.p1.3.3.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.3.m1.1b"><ci id="S3.SS1.p1.3.3.m1.1.1.cmml" xref="S3.SS1.p1.3.3.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.3.m1.1c">\rightarrow</annotation></semantics></math>AE</span>:
<span id="S3.SS1.p1.3.4" class="ltx_text ltx_font_italic">joint</span> answer and explanation generation for a given question.
We hypothesise that by training the model to generate both the answer and its explanation <em id="S3.SS1.p1.3.5" class="ltx_emph ltx_font_italic">simultaneously</em>, the result answer and explanation will be more grounded and consistent.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We use a pretrained multimodal encoder-decoder transformer as our base model (here we build on the openly released version of OFA as a strong baseline), and finetune the model on a mix of VQA datasets from different domains.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Different from OFA, for each image in the VQA datasets, we first extract objects and attributes using a bottom-up top-down attention-based model, which is crucial for open-domain VQA tasks <cite class="ltx_cite ltx_citemacro_cite">Anderson et al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>.
We then add artificial prompt tokens at the beginning of the textual input to signal the generation task (answer, explanation, or both) and the dataset<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Artificial prompt tokens are added as special tokens to the tokenizer to avoid bias in the pretrained embeddings. However, we note that these tokens may be biased w.r.t their association with specific tasks after training, which is an intended effect.</span></span></span>.
For <span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_typewriter">Q<math id="S3.SS1.p3.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS1.p3.1.1.m1.1a"><mo stretchy="false" id="S3.SS1.p3.1.1.m1.1.1" xref="S3.SS1.p3.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.1.m1.1b"><ci id="S3.SS1.p3.1.1.m1.1.1.cmml" xref="S3.SS1.p3.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.1.m1.1c">\rightarrow</annotation></semantics></math>AE</span>, we concatenate answers and explanations with a separator in between.
Finally, we mix all training instances, each consisting of an image (processed in patches), objects and attributes, and textual input with artificial prompts.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Perplexity as Multiple Choice Metric</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.8" class="ltx_p">To map the generated output to Multiple-Choice options, in previous work the predictions are loosely matched with options or gold answers using embedding-based methods, such as GloVe embedding similarity <cite class="ltx_cite ltx_citemacro_citep">(Schwenk et al., <a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>.
In contrast to these approaches, we propose to evaluate each option as a <span id="S3.SS2.p1.8.1" class="ltx_text ltx_font_italic">text generation</span> task, by feeding the model the information that was used to generate the answer as a prompt, and calculating the likelihood of each option being generated.
Formally, given an option <math id="S3.SS2.p1.1.m1.4" class="ltx_Math" alttext="Y=(y_{1},y_{2},...,y_{t})" display="inline"><semantics id="S3.SS2.p1.1.m1.4a"><mrow id="S3.SS2.p1.1.m1.4.4" xref="S3.SS2.p1.1.m1.4.4.cmml"><mi id="S3.SS2.p1.1.m1.4.4.5" xref="S3.SS2.p1.1.m1.4.4.5.cmml">Y</mi><mo id="S3.SS2.p1.1.m1.4.4.4" xref="S3.SS2.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S3.SS2.p1.1.m1.4.4.3.3" xref="S3.SS2.p1.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS2.p1.1.m1.4.4.3.3.4" xref="S3.SS2.p1.1.m1.4.4.3.4.cmml">(</mo><msub id="S3.SS2.p1.1.m1.2.2.1.1.1" xref="S3.SS2.p1.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.2.2.1.1.1.2" xref="S3.SS2.p1.1.m1.2.2.1.1.1.2.cmml">y</mi><mn id="S3.SS2.p1.1.m1.2.2.1.1.1.3" xref="S3.SS2.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.p1.1.m1.4.4.3.3.5" xref="S3.SS2.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS2.p1.1.m1.3.3.2.2.2" xref="S3.SS2.p1.1.m1.3.3.2.2.2.cmml"><mi id="S3.SS2.p1.1.m1.3.3.2.2.2.2" xref="S3.SS2.p1.1.m1.3.3.2.2.2.2.cmml">y</mi><mn id="S3.SS2.p1.1.m1.3.3.2.2.2.3" xref="S3.SS2.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS2.p1.1.m1.4.4.3.3.6" xref="S3.SS2.p1.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">…</mi><mo id="S3.SS2.p1.1.m1.4.4.3.3.7" xref="S3.SS2.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS2.p1.1.m1.4.4.3.3.3" xref="S3.SS2.p1.1.m1.4.4.3.3.3.cmml"><mi id="S3.SS2.p1.1.m1.4.4.3.3.3.2" xref="S3.SS2.p1.1.m1.4.4.3.3.3.2.cmml">y</mi><mi id="S3.SS2.p1.1.m1.4.4.3.3.3.3" xref="S3.SS2.p1.1.m1.4.4.3.3.3.3.cmml">t</mi></msub><mo stretchy="false" id="S3.SS2.p1.1.m1.4.4.3.3.8" xref="S3.SS2.p1.1.m1.4.4.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.4b"><apply id="S3.SS2.p1.1.m1.4.4.cmml" xref="S3.SS2.p1.1.m1.4.4"><eq id="S3.SS2.p1.1.m1.4.4.4.cmml" xref="S3.SS2.p1.1.m1.4.4.4"></eq><ci id="S3.SS2.p1.1.m1.4.4.5.cmml" xref="S3.SS2.p1.1.m1.4.4.5">𝑌</ci><vector id="S3.SS2.p1.1.m1.4.4.3.4.cmml" xref="S3.SS2.p1.1.m1.4.4.3.3"><apply id="S3.SS2.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1.1.2">𝑦</ci><cn type="integer" id="S3.SS2.p1.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS2.p1.1.m1.3.3.2.2.2.cmml" xref="S3.SS2.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS2.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.p1.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS2.p1.1.m1.3.3.2.2.2.2">𝑦</ci><cn type="integer" id="S3.SS2.p1.1.m1.3.3.2.2.2.3.cmml" xref="S3.SS2.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">…</ci><apply id="S3.SS2.p1.1.m1.4.4.3.3.3.cmml" xref="S3.SS2.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.4.4.3.3.3.1.cmml" xref="S3.SS2.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.SS2.p1.1.m1.4.4.3.3.3.2.cmml" xref="S3.SS2.p1.1.m1.4.4.3.3.3.2">𝑦</ci><ci id="S3.SS2.p1.1.m1.4.4.3.3.3.3.cmml" xref="S3.SS2.p1.1.m1.4.4.3.3.3.3">𝑡</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.4c">Y=(y_{1},y_{2},...,y_{t})</annotation></semantics></math> with <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">t</annotation></semantics></math> tokens, we calculate the probability of each token <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">y</mi><mi id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">𝑦</ci><ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">y_{i}</annotation></semantics></math> being generated by feeding the image, objects, and question, as well as the first <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="i-1" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mrow id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">i</mi><mo id="S3.SS2.p1.4.m4.1.1.1" xref="S3.SS2.p1.4.m4.1.1.1.cmml">−</mo><mn id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><minus id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1"></minus><ci id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">𝑖</ci><cn type="integer" id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">i-1</annotation></semantics></math> tokens from <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mi id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><ci id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">Y</annotation></semantics></math> to the model <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="p_{\theta}" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><msub id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><mi id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml">p</mi><mi id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2">𝑝</ci><ci id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">p_{\theta}</annotation></semantics></math>.
The perplexity is then calculated with: <math id="S3.SS2.p1.7.m7.2" class="ltx_Math" alttext="\mathrm{PPL}(Y)=\mathrm{exp}\left\{-\frac{1}{t}\sum_{i}^{t}\mathrm{log}\,p_{\theta}\left(y_{i}|y_{&lt;i}\right)\right\}" display="inline"><semantics id="S3.SS2.p1.7.m7.2a"><mrow id="S3.SS2.p1.7.m7.2.2" xref="S3.SS2.p1.7.m7.2.2.cmml"><mrow id="S3.SS2.p1.7.m7.2.2.3" xref="S3.SS2.p1.7.m7.2.2.3.cmml"><mi id="S3.SS2.p1.7.m7.2.2.3.2" xref="S3.SS2.p1.7.m7.2.2.3.2.cmml">PPL</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.7.m7.2.2.3.1" xref="S3.SS2.p1.7.m7.2.2.3.1.cmml">​</mo><mrow id="S3.SS2.p1.7.m7.2.2.3.3.2" xref="S3.SS2.p1.7.m7.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.p1.7.m7.2.2.3.3.2.1" xref="S3.SS2.p1.7.m7.2.2.3.cmml">(</mo><mi id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">Y</mi><mo stretchy="false" id="S3.SS2.p1.7.m7.2.2.3.3.2.2" xref="S3.SS2.p1.7.m7.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p1.7.m7.2.2.2" xref="S3.SS2.p1.7.m7.2.2.2.cmml">=</mo><mrow id="S3.SS2.p1.7.m7.2.2.1" xref="S3.SS2.p1.7.m7.2.2.1.cmml"><mi id="S3.SS2.p1.7.m7.2.2.1.3" xref="S3.SS2.p1.7.m7.2.2.1.3.cmml">exp</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.7.m7.2.2.1.2" xref="S3.SS2.p1.7.m7.2.2.1.2.cmml">​</mo><mrow id="S3.SS2.p1.7.m7.2.2.1.1.1" xref="S3.SS2.p1.7.m7.2.2.1.1.2.cmml"><mo id="S3.SS2.p1.7.m7.2.2.1.1.1.2" xref="S3.SS2.p1.7.m7.2.2.1.1.2.cmml">{</mo><mrow id="S3.SS2.p1.7.m7.2.2.1.1.1.1" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.cmml"><mo id="S3.SS2.p1.7.m7.2.2.1.1.1.1a" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.cmml">−</mo><mrow id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.cmml"><mfrac id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.3" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.3.cmml"><mn id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.3.2" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.3.2.cmml">1</mn><mi id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.3.3" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.3.3.cmml">t</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.2" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.cmml"><msubsup id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2.cmml"><mo id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2.2.2" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mi id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2.2.3" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2.2.3.cmml">i</mi><mi id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2.3" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2.3.cmml">t</mi></msubsup><mrow id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.3" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.3.cmml">log</mi><mo lspace="0.170em" rspace="0em" id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.2" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.2.cmml">​</mo><msub id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.4" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.4.cmml"><mi id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.4.2" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.4.2.cmml">p</mi><mi id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.4.3" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.4.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.2a" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.2.2.cmml">y</mi><mi id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo fence="false" id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.2.cmml">y</mi><mrow id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml"></mi><mo id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><mo id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.SS2.p1.7.m7.2.2.1.1.1.3" xref="S3.SS2.p1.7.m7.2.2.1.1.2.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.2b"><apply id="S3.SS2.p1.7.m7.2.2.cmml" xref="S3.SS2.p1.7.m7.2.2"><eq id="S3.SS2.p1.7.m7.2.2.2.cmml" xref="S3.SS2.p1.7.m7.2.2.2"></eq><apply id="S3.SS2.p1.7.m7.2.2.3.cmml" xref="S3.SS2.p1.7.m7.2.2.3"><times id="S3.SS2.p1.7.m7.2.2.3.1.cmml" xref="S3.SS2.p1.7.m7.2.2.3.1"></times><ci id="S3.SS2.p1.7.m7.2.2.3.2.cmml" xref="S3.SS2.p1.7.m7.2.2.3.2">PPL</ci><ci id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">𝑌</ci></apply><apply id="S3.SS2.p1.7.m7.2.2.1.cmml" xref="S3.SS2.p1.7.m7.2.2.1"><times id="S3.SS2.p1.7.m7.2.2.1.2.cmml" xref="S3.SS2.p1.7.m7.2.2.1.2"></times><ci id="S3.SS2.p1.7.m7.2.2.1.3.cmml" xref="S3.SS2.p1.7.m7.2.2.1.3">exp</ci><set id="S3.SS2.p1.7.m7.2.2.1.1.2.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1"><apply id="S3.SS2.p1.7.m7.2.2.1.1.1.1.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1"><minus id="S3.SS2.p1.7.m7.2.2.1.1.1.1.2.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1"></minus><apply id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1"><times id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.2"></times><apply id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.3"><divide id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.3"></divide><cn type="integer" id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.3.2">1</cn><ci id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.3.3">𝑡</ci></apply><apply id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1"><apply id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2.2.1.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2.2.2.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2.2.2"></sum><ci id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2.2.3.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2.2.3">𝑖</ci></apply><ci id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1"><times id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.2"></times><ci id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.3">log</ci><apply id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.4.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.4.1.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.4.2.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.4.2">𝑝</ci><ci id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.4.3.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.4.3">𝜃</ci></apply><apply id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.2.2">𝑦</ci><ci id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.2">𝑦</ci><apply id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.3"><lt id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.3.2">absent</csymbol><ci id="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.SS2.p1.7.m7.2.2.1.1.1.1.1.1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></apply></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.2c">\mathrm{PPL}(Y)=\mathrm{exp}\left\{-\frac{1}{t}\sum_{i}^{t}\mathrm{log}\,p_{\theta}\left(y_{i}|y_{&lt;i}\right)\right\}</annotation></semantics></math>, which reflects the probability of option <math id="S3.SS2.p1.8.m8.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.SS2.p1.8.m8.1a"><mi id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><ci id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">Y</annotation></semantics></math> being generated by the model.
Finally, the option with the lowest perplexity is chosen as the answer.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We also compare the performance of our approach, using perplexity as the metric, with GloVe embedding similarity for A-OKVQA (see <a href="#S3.T1" title="Table 1 ‣ 3 Methodology ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>).</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2301.10799/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="179" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of generated explanations from <span id="S3.F2.2.1" class="ltx_text ltx_font_smallcaps">umae<sub id="S3.F2.2.1.1" class="ltx_sub">all</sub></span> model with different decoding strategies.
The two examples on the left are from A-OKVQA and the two on the right are from VCR.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We primarily evaluated our proposed UMAE approach using pretrained OFA<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/OFA-Sys/OFA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/OFA-Sys/OFA</a></span></span></span> as the base model on three knowledge-intensive VQA datasets:
OK-VQA, A-OKVQA and VCR<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>See <a href="#A1" title="Appendix A Datasets ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix A</span></a> for datasets details.</span></span></span>.
We split the original train set into train and validation set (95%-5%) for all three datasets. Since the test set is not publicly available for A-OKVQA and VCR, we use the original validation set for experimental analyses.
We prepare training instances<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Specifically, we add <span id="footnote5.1" class="ltx_text ltx_font_typewriter">&lt;#OKA#&gt;</span> for OK-VQA (only answers are available), <span id="footnote5.2" class="ltx_text ltx_font_typewriter">&lt;#A#&gt;</span>, <span id="footnote5.3" class="ltx_text ltx_font_typewriter">&lt;#E#&gt;</span>, <span id="footnote5.4" class="ltx_text ltx_font_typewriter">&lt;#AE#&gt;</span> for VCR, and <span id="footnote5.5" class="ltx_text ltx_font_typewriter">&lt;#AOKA#&gt;</span>, <span id="footnote5.6" class="ltx_text ltx_font_typewriter">&lt;#AOKE#&gt;</span>, <span id="footnote5.7" class="ltx_text ltx_font_typewriter">&lt;#AOKAE#&gt;</span> for A-OKVQA.</span></span></span> as introduced in <a href="#S3.SS1" title="3.1 Multitask Learning with Artificial Prompt ‣ 3 Methodology ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>.
Additionally, for VCR, we draw coloured highlights around the referenced entity on the images, following <cite class="ltx_cite ltx_citemacro_citet">Zellers et al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite> (<a href="#A1" title="Appendix A Datasets ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix A</span></a>).
To account for the imbalance in size among the datasets, we up-sample
instances in OK-VQA and A-OKVQA, and shuffle all instances to train the <span id="S4.p1.1.1" class="ltx_text ltx_font_smallcaps">umae<sub id="S4.p1.1.1.1" class="ltx_sub">all</sub></span> model.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">For ablation studies, we finetune OFA for separate answer prediction (<span id="S4.p2.1.1" class="ltx_text ltx_font_smallcaps">ofa<sub id="S4.p2.1.1.1" class="ltx_sub">q-&gt;a</sub></span>) and explanation generation conditioned on answers (<span id="S4.p2.1.2" class="ltx_text ltx_font_smallcaps">ofa<sub id="S4.p2.1.2.1" class="ltx_sub">qa-&gt;e</sub></span>).
To better understand the impact of mixing datasets from different domains, we also train <span id="S4.p2.1.3" class="ltx_text ltx_font_smallcaps">umae<sub id="S4.p2.1.3.1" class="ltx_sub">a-okvqa</sub></span> and <span id="S4.p2.1.4" class="ltx_text ltx_font_smallcaps">umae<sub id="S4.p2.1.4.1" class="ltx_sub">vcr</sub></span>, focusing on all three answer and explanation generation tasks but only using data from a single dataset: either with A-OKVQA or with VCR.
Details of training parameters are included in <a href="#A2" title="Appendix B Hyper-Parameters and Training ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix B</span></a>.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">We use beam search for generating answers and additionally experiment with different decoding methods including top-k sampling, Nucleus sampling <cite class="ltx_cite ltx_citemacro_cite">Holtzman et al. (<a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>, and Typical sampling <cite class="ltx_cite ltx_citemacro_cite">Meister et al. (<a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite>, for generating explanations.
We evaluate answer accuracy as well as explanation quality with automatic NLG metrics and e-ViL scores <cite class="ltx_cite ltx_citemacro_cite">Kayser et al. (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>.
e-ViL scores consist of <span id="S4.p3.1.1" class="ltx_text ltx_font_typewriter">S<sub id="S4.p3.1.1.1" class="ltx_sub">T</sub></span> (task/answer accuracy),
<span id="S4.p3.1.2" class="ltx_text ltx_font_typewriter">S<sub id="S4.p3.1.2.1" class="ltx_sub">E</sub></span> (explanation score), and overall <span id="S4.p3.1.3" class="ltx_text ltx_font_typewriter">S<sub id="S4.p3.1.3.1" class="ltx_sub">O</sub></span> (product of <span id="S4.p3.1.4" class="ltx_text ltx_font_typewriter">S<sub id="S4.p3.1.4.1" class="ltx_sub">T</sub></span> and <span id="S4.p3.1.5" class="ltx_text ltx_font_typewriter">S<sub id="S4.p3.1.5.1" class="ltx_sub">E</sub></span>), where <span id="S4.p3.1.6" class="ltx_text ltx_font_typewriter">S<sub id="S4.p3.1.6.1" class="ltx_sub">E</sub></span> is the harmonic mean of NGRAMScore (the harmonic mean of n-gram scores ROUGE-L <cite class="ltx_cite ltx_citemacro_cite">Lin and Och (<a href="#bib.bib16" title="" class="ltx_ref">2004</a>)</cite>, METEOR <cite class="ltx_cite ltx_citemacro_cite">Banerjee and Lavie (<a href="#bib.bib4" title="" class="ltx_ref">2005</a>)</cite>, CIDEr <cite class="ltx_cite ltx_citemacro_cite">Vedantam et al. (<a href="#bib.bib26" title="" class="ltx_ref">2015</a>)</cite>, and SPICE <cite class="ltx_cite ltx_citemacro_cite">Anderson et al. (<a href="#bib.bib1" title="" class="ltx_ref">2016</a>)</cite>) and additionally the BERTScore <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib35" title="" class="ltx_ref">2020</a>)</cite>, a learned similarity metric over contextual representations of sentences.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Answer Accuracy</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p"><a href="#S3.T1" title="Table 1 ‣ 3 Methodology ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 1</span></a> presents our observations for answer accuracy on <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">Q-&gt;A</span> task over the three datasets.
We also evaluate VCR answers using BERTScore as the answers for VCR are usually sentences.
We observe that <span id="S5.SS1.p1.1.2" class="ltx_text ltx_font_smallcaps">umae<sub id="S5.SS1.p1.1.2.1" class="ltx_sub">all</sub></span> outperforms <span id="S5.SS1.p1.1.3" class="ltx_text ltx_font_smallcaps">ofa<sub id="S5.SS1.p1.1.3.1" class="ltx_sub">q-&gt;a</sub></span> on all datasets, improves the prior SOTA on A-OKVQA by 10<math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mo id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><csymbol cd="latexml" id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\sim</annotation></semantics></math>15%, and achieves competitive results on OK-VQA.
For models that are finetuned on A-OKVQA, we also see a salient improvement (+9%) with the proposed mapping of options by perplexity in Multiple-Choice, instead of GloVe embeddings similarity<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Preliminary experiments with NLG metrics (BERTScore and BLEU) for selecting the options given generation were sub-optimal.</span></span></span>.
We conducted several ablation studies on the dependency of the modality for the answer accuracy in A-OKVQA, where we find the visual encoder is crucial for performance.
Details are included in <a href="#A3" title="Appendix C Ablations on Modality Dependency ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix C</span></a>.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Explanation Evaluation</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p"><a href="#S3.T2" title="Table 2 ‣ 3 Methodology ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 2</span></a> shows e-ViL sores (<a href="#S4" title="4 Experimental Setup ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">4</span></a>) for explanations using automatic NLG metrics<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Nucleus sampling shows best results and is reported.
Detailed scores
with different decoding methods are shown in <a href="#A4" title="Appendix D More Explanation Scores ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix D</span></a>.</span></span></span>.
Following the same setup as in <cite class="ltx_cite ltx_citemacro_citet">Kayser et al. (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>, an explanation is evaluated only if the answer predicted by the system is correct<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>A limitation of evaluating all explanations is that explanations of wrong answers may get high scores with n-gram metrics, even though they are justifying wrong answers and should be penalised.</span></span></span>.
We observe that pretrained OFA with natural language prompts, e.g. <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">“what is the explanation for the answer?”</span> or <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_italic">“this is because”</span> performs poorly, as most
generated explanations are words (<span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_italic">“yes/no”</span>) or short-phrases<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>BERTScore in not representative of the validity of outputs from <span id="footnote9.1" class="ltx_text ltx_font_smallcaps">OFA</span>*. We refer the reader to an exposition of the problems associated with NLG metrics in <cite class="ltx_cite ltx_citemacro_citet">Caglayan et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>.</span></span></span>.
We compare UMAE models (on all and individual datasets) with prior best results from e-UG (see <a href="#S2" title="2 Related Work ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">2</span></a>), and standard separated trained baselines (<span id="S5.SS2.p1.1.4" class="ltx_text ltx_font_smallcaps">ofa<sub id="S5.SS2.p1.1.4.1" class="ltx_sub">q-&gt;a</sub></span>+<span id="S5.SS2.p1.1.5" class="ltx_text ltx_font_smallcaps">ofa<sub id="S5.SS2.p1.1.5.1" class="ltx_sub">qa-&gt;e</sub></span>).
<span id="S5.SS2.p1.1.6" class="ltx_text ltx_font_smallcaps">umae<sub id="S5.SS2.p1.1.6.1" class="ltx_sub">all</sub></span> achieves better results across all datasets, showing the advantage of mixing tasks and datasets in different domains.
For out-of-domain evaluation on VQA-X, <span id="S5.SS2.p1.1.7" class="ltx_text ltx_font_smallcaps">umae<sub id="S5.SS2.p1.1.7.1" class="ltx_sub">all</sub></span> also shows mostly competitive results.
Examples of explanation generation are shown in <a href="#S3.F2" title="Figure 2 ‣ 3.2 Perplexity as Multiple Choice Metric ‣ 3 Methodology ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a> and <a href="#A5" title="Appendix E Examples of Generated Explanations ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix E</span></a>.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Since e-ViL only evaluates an explanation if a model generates the correct answer, the subset of explanations evaluated varies by model.
To <span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_italic">fairly</span> compare explanations on the same subset, we propose only using the subset of samples where all models provide correct answers for explanation prediction.
<a href="#S5.T3" title="Table 3 ‣ 5.2 Explanation Evaluation ‣ 5 Results and Discussion ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 3</span></a> shows the results on A-OKVQA with such a subset of 770 candidates, where <span id="S5.SS2.p2.1.2" class="ltx_text ltx_font_smallcaps">umae<sub id="S5.SS2.p2.1.2.1" class="ltx_sub">all</sub></span> shows an even higher explanation score. This highlights that <span id="S5.SS2.p2.1.3" class="ltx_text ltx_font_smallcaps">umae<sub id="S5.SS2.p2.1.3.1" class="ltx_sub">all</sub></span> generates explanations that overlap significantly better with gold explanations.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<div id="S5.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:196.7pt;height:50.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-42.2pt,10.8pt) scale(0.7,0.7) ;">
<table id="S5.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S5.T3.1.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">Model</span></th>
<th id="S5.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S5.T3.1.1.1.1.2.1" class="ltx_text ltx_font_italic">S<sub id="S5.T3.1.1.1.1.2.1.1" class="ltx_sub"><span id="S5.T3.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_smallcaps">E</span></sub></span></th>
<th id="S5.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S5.T3.1.1.1.1.3.1" class="ltx_text ltx_font_smallcaps">bleu4</span></th>
<th id="S5.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S5.T3.1.1.1.1.4.1" class="ltx_text ltx_font_smallcaps">r-l</span></th>
<th id="S5.T3.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S5.T3.1.1.1.1.5.1" class="ltx_text ltx_font_smallcaps">met.</span></th>
<th id="S5.T3.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="S5.T3.1.1.1.1.6.1" class="ltx_text ltx_font_smallcaps">cide</span>r</th>
<th id="S5.T3.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S5.T3.1.1.1.1.7.1" class="ltx_text ltx_font_smallcaps">spice</span></th>
<th id="S5.T3.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="S5.T3.1.1.1.1.8.1" class="ltx_text ltx_font_smallcaps">berts</span>c.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.1.2.1" class="ltx_tr">
<th id="S5.T3.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S5.T3.1.1.2.1.1.1" class="ltx_text ltx_font_smallcaps">ofa<sub id="S5.T3.1.1.2.1.1.1.1" class="ltx_sub">q-&gt;a</sub>+ofa<sub id="S5.T3.1.1.2.1.1.1.2" class="ltx_sub">qa-&gt;e</sub></span></th>
<td id="S5.T3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">42.4</td>
<td id="S5.T3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">20.0</td>
<td id="S5.T3.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">44.2</td>
<td id="S5.T3.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">19.3</td>
<td id="S5.T3.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">66.7</td>
<td id="S5.T3.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">19.1</td>
<td id="S5.T3.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">85.1</td>
</tr>
<tr id="S5.T3.1.1.3.2" class="ltx_tr">
<th id="S5.T3.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S5.T3.1.1.3.2.1.1" class="ltx_text ltx_font_smallcaps">umae<sub id="S5.T3.1.1.3.2.1.1.1" class="ltx_sub">a-okvqa</sub></span></th>
<td id="S5.T3.1.1.3.2.2" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">45.8</td>
<td id="S5.T3.1.1.3.2.3" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">23.6</td>
<td id="S5.T3.1.1.3.2.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">47.9</td>
<td id="S5.T3.1.1.3.2.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">21.7</td>
<td id="S5.T3.1.1.3.2.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">78.0</td>
<td id="S5.T3.1.1.3.2.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">20.5</td>
<td id="S5.T3.1.1.3.2.8" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">86.9</td>
</tr>
<tr id="S5.T3.1.1.4.3" class="ltx_tr">
<th id="S5.T3.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S5.T3.1.1.4.3.1.1" class="ltx_text ltx_font_smallcaps">umae<sub id="S5.T3.1.1.4.3.1.1.1" class="ltx_sub">all</sub></span></th>
<td id="S5.T3.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S5.T3.1.1.4.3.2.1" class="ltx_text ltx_font_bold">46.8</span></td>
<td id="S5.T3.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S5.T3.1.1.4.3.3.1" class="ltx_text ltx_font_bold">24.9</span></td>
<td id="S5.T3.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S5.T3.1.1.4.3.4.1" class="ltx_text ltx_font_bold">49.5</span></td>
<td id="S5.T3.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S5.T3.1.1.4.3.5.1" class="ltx_text ltx_font_bold">22.3</span></td>
<td id="S5.T3.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S5.T3.1.1.4.3.6.1" class="ltx_text ltx_font_bold">84.1</span></td>
<td id="S5.T3.1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S5.T3.1.1.4.3.7.1" class="ltx_text ltx_font_bold">20.8</span></td>
<td id="S5.T3.1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S5.T3.1.1.4.3.8.1" class="ltx_text ltx_font_bold">87.3</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>
Explanation scores on the same subset of A-OKVQA.
</figcaption>
</figure>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">In summary, our experiments demonstrate that the UMAE model leads to improved answer and explanation generation and allows for the flexibility to generate different types of outputs, including answers, explanations, or both. We observe that UMAE exhibits promising results in jointly generating both the answer and explanation.
We further provide a comparative evaluation in <a href="#A6" title="Appendix F Joint Generation Performance ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix F</span></a> as a first step towards comparison as there is currently no standard evaluation setup for the joint answer and explanation evaluation.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Error Analysis</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">To better understand the generated answers and errors, we randomly sample 50 errors in OK-VQA and A-OKVQA.
Our analysis reveals the following main error types, where the first three are related to model performance:
(1) <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_italic">Knowledge</span>:
the implicit knowledge learned by the model is insufficient for answering some of the knowledge-intensive questions, such as questions asking <em id="S5.SS3.p1.1.2" class="ltx_emph ltx_font_italic">when</em> a certain sport was invented;
(2) <span id="S5.SS3.p1.1.3" class="ltx_text ltx_font_italic">Visual</span>:
the model fails to identify the visual attributes correctly, such as questions about <em id="S5.SS3.p1.1.4" class="ltx_emph ltx_font_italic">recognising object shape or material</em>;
(3) <span id="S5.SS3.p1.1.5" class="ltx_text ltx_font_italic">Semantic disassociation</span>:
the model misinterprets questions or fails to match the intended semantic meaning. For example, it may answer what <em id="S5.SS3.p1.1.6" class="ltx_emph ltx_font_italic">an object is</em> instead of a more complex question such as <em id="S5.SS3.p1.1.7" class="ltx_emph ltx_font_italic">what is commonly packed in it</em> (e.g. answering "suitcase" instead of "clothes");
(4) <span id="S5.SS3.p1.1.8" class="ltx_text ltx_font_italic">Metric</span>:
the evaluation metric may penalise some of the plausible answers, especially when searching for exact match answers (mostly due to the difference of singular/plural or phrases with/without space in between);
and
(5) <span id="S5.SS3.p1.1.9" class="ltx_text ltx_font_italic">Dataset</span>:
errors due to issues in the datasets themselves. We discuss prominent issues in dataset quality briefly in <a href="#A7" title="Appendix G Datasets Quality and Issues ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix G</span></a> and further present the distribution of error types in <a href="#S5.F3" title="Figure 3 ‣ 5.3 Error Analysis ‣ 5 Results and Discussion ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2301.10799/assets/x3.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="170" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Error type distribution in 100 random samples from A-OKVQA and OK-VQA.
</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work, we propose UMAE, a unified model that generates answers and explanations in VQA using a multitask learning approach for multimodal encoder-decoder models, where artificial prompt tokens are added to distinguish different tasks while learning shared semantics.
Evaluation of our approach on various VQA tasks shows that UMAE outperforms prior best models and separately trained baselines in both answer and explanation scores, where we also demonstrate the benefit of using perplexity as the metric for mapping generated answers to Multiple-Choice options.
Additionally, UMAE offers flexibility in output and can generate explanations for datasets without explanations for training, e.g. OK-VQA, while also improving answer quality.
Through case studies and error analysis, we identify potential areas for future improvement, including dataset quality.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We discuss the limitations of our work in the following two aspects.
Firstly, the experiments with our proposed framework and finetuning approach are primarily on the OFA model.
We believe our approach applies to any multimodal generative model, however, it would also provide insights to experiment with more models.
Secondly, regarding the evaluation of our proposed joint framework, to better evaluate the generated explanation quality, especially to evaluate the difference between explanations generated jointly with answers and generated conditioned on the answers, human judgement would be an important criterion compared to automatic NLG metrics.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">We acknowledge the support of Apoorv Khandelwal from AI2 for providing us with results for the evaluation of our model predictions over a hidden test set. This was valuable for our earlier draft of the paper. We would like to thank the anonymous reviewers who provided valuable feedback on the previous draft of our paper.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al. (2016)</span>
<span class="ltx_bibblock">
Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2016.

</span>
<span class="ltx_bibblock">Spice: Semantic Propositional Image Caption Evaluation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pages 382–398.
Springer.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al. (2018)</span>
<span class="ltx_bibblock">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang. 2018.

</span>
<span class="ltx_bibblock">Bottom-up and Top-down Attention for Image Captioning and Cisual
Question Answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 6077–6086.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">Vqa: Visual Question Answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 2425–2433.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee and Lavie (2005)</span>
<span class="ltx_bibblock">
Satanjeev Banerjee and Alon Lavie. 2005.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/W05-0909" title="" class="ltx_ref ltx_href">METEOR: An automatic
metric for MT evaluation with improved correlation with human judgments</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for Machine Translation and/or Summarization</em>, pages
65–72, Ann Arbor, Michigan. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et al. 2020.

</span>
<span class="ltx_bibblock">Language Models are Few-shot Learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
33:1877–1901.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caglayan et al. (2020)</span>
<span class="ltx_bibblock">
Ozan Caglayan, Pranava Madhyastha, and Lucia Specia. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.coling-main.210" title="" class="ltx_ref ltx_href">Curious
case of language generation evaluation metrics: A cautionary tale</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th International Conference on
Computational Linguistics</em>, pages 2322–2328, Barcelona, Spain (Online).
International Committee on Computational Linguistics.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,
Yu Cheng, and Jingjing Liu. 2020.

</span>
<span class="ltx_bibblock">Uniter: Universal Image-Text Representation Learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pages 104–120.
Springer.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al. (2021)</span>
<span class="ltx_bibblock">
Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021.

</span>
<span class="ltx_bibblock">Unifying Vision-and-Language Tasks via Text Generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
1931–1942. PMLR.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dua et al. (2021)</span>
<span class="ltx_bibblock">
Radhika Dua, Sai Srinivas Kancheti, and Vineeth N Balasubramanian. 2021.

</span>
<span class="ltx_bibblock">Beyond vqa: Generating Multi-Word Answers and Rationales to Visual
Questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 1623–1632.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
2017.

</span>
<span class="ltx_bibblock">Making the V in VQA Matter: Elevating the Role of Image
Understanding in Visual Question Answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 6904–6913.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gui et al. (2022)</span>
<span class="ltx_bibblock">
Liangke Gui, Borui Wang, Qiuyuan Huang, Alexander Hauptmann, Yonatan Bisk, and
Jianfeng Gao. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.naacl-main.70" title="" class="ltx_ref ltx_href">KAT: A
knowledge augmented transformer for vision-and-language</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 956–968, Seattle, United States. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holtzman et al. (2020)</span>
<span class="ltx_bibblock">
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=rygGQyrFvH" title="" class="ltx_ref ltx_href">The Curious Case
of Neural Text Degeneration</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2017)</span>
<span class="ltx_bibblock">
Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng
Chen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00065" title="" class="ltx_ref ltx_href">Google’s
multilingual neural machine translation system: Enabling zero-shot
translation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
5:339–351.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kayser et al. (2021)</span>
<span class="ltx_bibblock">
Maxime Kayser, Oana-Maria Camburu, Leonard Salewski, Cornelius Emde, Virginie
Do, Zeynep Akata, and Thomas Lukasiewicz. 2021.

</span>
<span class="ltx_bibblock">e-ViL: A Dataset and Benchmark for Natural Language Explanations in
Vision-Language Tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 1244–1254.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020)</span>
<span class="ltx_bibblock">
Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan
Wang, Houdong Hu, Li Dong, Furu Wei, et al. 2020.

</span>
<span class="ltx_bibblock">Oscar: Object-Semantics Aligned Pre-training for Vision-Language
Tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 121–137.
Springer.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin and Och (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin and Franz Josef Och. 2004.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3115/1218955.1219032" title="" class="ltx_ref ltx_href">Automatic evaluation
of machine translation quality using longest common subsequence and
skip-bigram statistics</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 42nd Annual Meeting of the Association
for Computational Linguistics (ACL-04)</em>, pages 605–612, Barcelona, Spain.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014.

</span>
<span class="ltx_bibblock">Microsoft Coco: Common Objects in Context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pages 740–755.
Springer.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino et al. (2019)</span>
<span class="ltx_bibblock">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019.

</span>
<span class="ltx_bibblock">OK-VQA: A Visual Question Answering Benchmark Requiring External
Knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/cvf conference on computer vision
and pattern recognition</em>, pages 3195–3204.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meister et al. (2022)</span>
<span class="ltx_bibblock">
Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. 2022.

</span>
<span class="ltx_bibblock">Typical Decoding for Natural Language Generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.00666</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitzalis et al. (2021)</span>
<span class="ltx_bibblock">
Faidon Mitzalis, Ozan Caglayan, Pranava Madhyastha, and Lucia Specia. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.acl-long.503" title="" class="ltx_ref ltx_href">BERTGen:
Multi-task generation through BERT</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 6440–6455,
Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2018)</span>
<span class="ltx_bibblock">
Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele,
Trevor Darrell, and Marcus Rohrbach. 2018.

</span>
<span class="ltx_bibblock">Multimodal Explanations: Justifying Decisions and Pointing to the
Evidence.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 8779–8788.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" title="" class="ltx_ref ltx_href">Language models are unsupervised multitask learners</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">OpenAI Technical Report</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://jmlr.org/papers/v21/20-074.html" title="" class="ltx_ref ltx_href">Exploring the Limits
of Transfer Learning with a Unified Text-to-Text Transformer</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 21(140):1–67.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rohrbach et al. (2017)</span>
<span class="ltx_bibblock">
Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Pal,
Hugo Larochelle, Aaron Courville, and Bernt Schiele. 2017.

</span>
<span class="ltx_bibblock">Movie Description.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 123(1):94–120.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwenk et al. (2022)</span>
<span class="ltx_bibblock">
Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and
Roozbeh Mottaghi. 2022.

</span>
<span class="ltx_bibblock">A-OKVQA: A Benchmark for Visual Question Answering using World
Knowledge.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2206.01718.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vedantam et al. (2015)</span>
<span class="ltx_bibblock">
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">Cider: Consensus-based Image Description Evaluation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 4566–4575.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022a)</span>
<span class="ltx_bibblock">
Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma,
Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v162/wang22al.html" title="" class="ltx_ref ltx_href">OFA:
Unifying architectures, tasks, and modalities through a simple
sequence-to-sequence learning framework</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 39th International Conference on Machine
Learning</em>, volume 162 of <em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>,
pages 23318–23340. PMLR.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022b)</span>
<span class="ltx_bibblock">
Yanan Wang, Michihiro Yasunaga, Hongyu Ren, Shinya Wada, and Jure Leskovec.
2022b.

</span>
<span class="ltx_bibblock">VQA-GNN: Reasoning with Multimodal Semantic Graph for Visual
Question Answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.11501</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022c)</span>
<span class="ltx_bibblock">
Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao.
2022c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=GUrhfTuf_3" title="" class="ltx_ref ltx_href">SimVLM: Simple
visual language model pretraining with weak supervision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Mooney (2019)</span>
<span class="ltx_bibblock">
Jialin Wu and Raymond Mooney. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/W19-4812" title="" class="ltx_ref ltx_href">Faithful multimodal
explanation for visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing
and Interpreting Neural Networks for NLP</em>, pages 103–112, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye and Kovashka (2021)</span>
<span class="ltx_bibblock">
Keren Ye and Adriana Kovashka. 2021.

</span>
<span class="ltx_bibblock">A Case study of the Shortcut Effects in Visual Commonsense
Reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial
intelligence</em>, volume 35, pages 3181–3189.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et al. (2019)</span>
<span class="ltx_bibblock">
Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock">From Recognition to Cognition: Visual Commonsense Reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">The IEEE Conference on Computer Vision and Pattern
Recognition</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et al. (2021)</span>
<span class="ltx_bibblock">
Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao,
Ali Farhadi, and Yejin Choi. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2021/file/c6d4eb15f1e84a36eff58eca3627c82e-Paper.pdf" title="" class="ltx_ref ltx_href">MERLOT: Multimodal Neural Script Knowledge Models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volume 34, pages 23634–23651. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2021)</span>
<span class="ltx_bibblock">
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang,
Yejin Choi, and Jianfeng Gao. 2021.

</span>
<span class="ltx_bibblock">Vinvl: Revisiting Visual Representations in Vision-Language Models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 5579–5588.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi.
2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=SkeHuCVFDr" title="" class="ltx_ref ltx_href">BERTScore:
Evaluating Text Generation with BERT</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Datasets</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">The datasets used in the paper are as follows:</p>
</div>
<section id="A1.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">OK-VQA</h4>

<div id="A1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Marino et al. (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> is a knowledge-based VQA dataset that requires outside knowledge beyond the images to answer the questions.
It has train and test splits of size 9,009 and 5,046.
Each question is provided answers by five annotators.
To use the VQA <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib3" title="" class="ltx_ref">2015</a>)</cite> metric, each annotated answer is then repeated twice to form a gold answer set with 10 answers.
Since no explanation is provided, we only train <span id="A1.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_typewriter">Q<math id="A1.SS0.SSS0.Px1.p1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="A1.SS0.SSS0.Px1.p1.1.1.m1.1a"><mo stretchy="false" id="A1.SS0.SSS0.Px1.p1.1.1.m1.1.1" xref="A1.SS0.SSS0.Px1.p1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px1.p1.1.1.m1.1b"><ci id="A1.SS0.SSS0.Px1.p1.1.1.m1.1.1.cmml" xref="A1.SS0.SSS0.Px1.p1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px1.p1.1.1.m1.1c">\rightarrow</annotation></semantics></math>A</span> task on OK-VQA.</p>
</div>
</section>
<section id="A1.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">A-OKVQA</h4>

<div id="A1.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px2.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Schwenk et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite> is currently the largest knowledge-based VQA dataset split into 17.1K, 1.1K, and 6.7K for train, validation, and test, respectively.
The questions cover four knowledge types: visual, commonsense, knowledge bases, and physical.
For each question, it provides both multiple-choice answers and 10 free-form answers (annotated by 10 different people), as well as three explanations.
Images in both OK-VQA and A-OKVQA are from MSCOCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib17" title="" class="ltx_ref">2014</a>)</cite>, and answers in both datasets are in single words or short phrases.</p>
</div>
</section>
<section id="A1.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">VCR</h4>

<div id="A1.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px3.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Zellers et al. (<a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite> is a large multiple-choice dataset for Visual Commonsense Reasoning.
The train, validation, and test splits have 191.6k, 21.3k, and 26.5k instances, respectively.
Each question has four answer options in sentences, and the correct answer is further provided with four explanation options.
Images in VCR are from movie clips <cite class="ltx_cite ltx_citemacro_cite">Rohrbach et al. (<a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite>.
Bounding boxes of entities are provided associated with mentions such as <span id="A1.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_typewriter">Person1</span> in questions, answers and explanations.
We follow <cite class="ltx_cite ltx_citemacro_citet">Zellers et al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite> and draw coloured highlights around the referenced entity on the images, where entity names and the coloured highlights are consistent in the entire dataset, expecting the model to learn the association between the coloured bounding box and the entity.</p>
</div>
</section>
<section id="A1.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">VQA-X</h4>

<div id="A1.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px4.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Park et al. (<a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite> contains a subset from the VQAv2 <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib10" title="" class="ltx_ref">2017</a>)</cite> dataset and further provides three explanations for each question.
The image-question pairs are split into train, validation, and test with 29.5k, 1.5k, and 2k instances, respectively.
We only use the original test set to evaluate the zero-shot performance of the trained models.</p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Hyper-Parameters and Training</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">We begin with the pretrained weights from the original OFA-large<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a target="_blank" href="https://github.com/OFA-Sys/OFA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/OFA-Sys/OFA</a></span></span></span>, which is trained on vision-only tasks including Image Classification,
language-only tasks including Sentence Classification, Text Summarisation, as well as various vision-language tasks including Image Captioning, Visual Question Answering and Visual Entailment.
Adam is used as the optimizer and cross-entropy is the loss function.
We set the learning rate to <math id="A2.p1.1.m1.1" class="ltx_Math" alttext="10^{-5}" display="inline"><semantics id="A2.p1.1.m1.1a"><msup id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml"><mn id="A2.p1.1.m1.1.1.2" xref="A2.p1.1.m1.1.1.2.cmml">10</mn><mrow id="A2.p1.1.m1.1.1.3" xref="A2.p1.1.m1.1.1.3.cmml"><mo id="A2.p1.1.m1.1.1.3a" xref="A2.p1.1.m1.1.1.3.cmml">−</mo><mn id="A2.p1.1.m1.1.1.3.2" xref="A2.p1.1.m1.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.1b"><apply id="A2.p1.1.m1.1.1.cmml" xref="A2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.p1.1.m1.1.1.1.cmml" xref="A2.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="A2.p1.1.m1.1.1.2.cmml" xref="A2.p1.1.m1.1.1.2">10</cn><apply id="A2.p1.1.m1.1.1.3.cmml" xref="A2.p1.1.m1.1.1.3"><minus id="A2.p1.1.m1.1.1.3.1.cmml" xref="A2.p1.1.m1.1.1.3"></minus><cn type="integer" id="A2.p1.1.m1.1.1.3.2.cmml" xref="A2.p1.1.m1.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.1.m1.1c">10^{-5}</annotation></semantics></math>, the warm-up ratio to 0.4, and the patch image size to 480.
We shuffle all the training examples and use batch size 16.
Due to the large size of VCR, we train for 30 epochs on models involving VCR (<span id="A2.p1.1.1" class="ltx_text ltx_font_smallcaps">ofa<sub id="A2.p1.1.1.1" class="ltx_sub">q-&gt;a</sub></span> for VCR, <span id="A2.p1.1.2" class="ltx_text ltx_font_smallcaps">umae<sub id="A2.p1.1.2.1" class="ltx_sub">vcr</sub></span> and <span id="A2.p1.1.3" class="ltx_text ltx_font_smallcaps">umae<sub id="A2.p1.1.3.1" class="ltx_sub">all</sub></span>), and up to 100 epochs for other models.
We report the empirical performance with checkpoints that perform best on the validation set (the 5% split from the original train set).
For A-OKVQA, we additionally report the answer accuracy on the original test set.</p>
</div>
<figure id="A2.T4" class="ltx_table">
<div id="A2.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:161.0pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-20.1pt,9.0pt) scale(0.8,0.8) ;">
<table id="A2.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T4.1.1.1.1" class="ltx_tr">
<th id="A2.T4.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.T4.1.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">Question</span></th>
<th id="A2.T4.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.T4.1.1.1.1.2.1" class="ltx_text ltx_font_smallcaps">Objects</span></th>
<th id="A2.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="A2.T4.1.1.1.1.3.1" class="ltx_text ltx_font_smallcaps">Images</span></th>
<th id="A2.T4.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.T4.1.1.1.1.4.1" class="ltx_text ltx_font_smallcaps">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T4.1.1.2.1" class="ltx_tr">
<td id="A2.T4.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T4.1.1.2.1.1.1" class="ltx_text ltx_font_smallcaps">✓</span></td>
<td id="A2.T4.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="A2.T4.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">original</td>
<td id="A2.T4.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">50.39</td>
</tr>
<tr id="A2.T4.1.1.3.2" class="ltx_tr">
<td id="A2.T4.1.1.3.2.1" class="ltx_td ltx_align_center"><span id="A2.T4.1.1.3.2.1.1" class="ltx_text ltx_font_smallcaps">✓</span></td>
<td id="A2.T4.1.1.3.2.2" class="ltx_td ltx_align_center">✗</td>
<td id="A2.T4.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">✗</td>
<td id="A2.T4.1.1.3.2.4" class="ltx_td ltx_align_center">39.16</td>
</tr>
<tr id="A2.T4.1.1.4.3" class="ltx_tr">
<td id="A2.T4.1.1.4.3.1" class="ltx_td ltx_align_center"><span id="A2.T4.1.1.4.3.1.1" class="ltx_text ltx_font_smallcaps">✓</span></td>
<td id="A2.T4.1.1.4.3.2" class="ltx_td ltx_align_center">✗</td>
<td id="A2.T4.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">random</td>
<td id="A2.T4.1.1.4.3.4" class="ltx_td ltx_align_center">33.48</td>
</tr>
<tr id="A2.T4.1.1.5.4" class="ltx_tr">
<td id="A2.T4.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T4.1.1.5.4.1.1" class="ltx_text ltx_font_smallcaps">✓</span></td>
<td id="A2.T4.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="A2.T4.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">✗</td>
<td id="A2.T4.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb">33.28</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Ablation on the modality dependency for answer accuracy of A-OKVQA.
</figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Ablations on Modality Dependency</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">We conduct several ablation studies to investigate the dependency of object features and images on the performance of our model <span id="A3.p1.1.1" class="ltx_text ltx_font_smallcaps">UMAE<sub id="A3.p1.1.1.1" class="ltx_sub">all</sub></span> for answer accuracy of A-OKVQA, where we removed images, replaced them with random images, and removed extracted attributes and features.
Results in <a href="#A2.T4" title="Table 4 ‣ Appendix B Hyper-Parameters and Training ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 4</span></a> show that the visual encoder is crucial for performance and that visual objects alone are not sufficient for answer prediction.
Using a random image would introduce noise and therefore performs worse than not including the image at all.
We did not test removing the question because we believe the model needs the questions to be able to provide answers.</p>
</div>
<figure id="A3.T5" class="ltx_table">
<div id="A3.T5.9" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.2pt;height:189.8pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-65.0pt,31.5pt) scale(0.75,0.75) ;">
<table id="A3.T5.9.9" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A3.T5.9.9.10.1" class="ltx_tr">
<th id="A3.T5.9.9.10.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="A3.T5.9.9.10.1.1.1" class="ltx_text ltx_font_smallcaps">Dataset</span></th>
<th id="A3.T5.9.9.10.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="A3.T5.9.9.10.1.2.1" class="ltx_text ltx_font_smallcaps">Decoding</span></th>
<th id="A3.T5.9.9.10.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">e<span id="A3.T5.9.9.10.1.3.1" class="ltx_text ltx_font_smallcaps">-V</span>i<span id="A3.T5.9.9.10.1.3.2" class="ltx_text ltx_font_smallcaps">l</span>
</th>
<th id="A3.T5.9.9.10.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="8"><span id="A3.T5.9.9.10.1.4.1" class="ltx_text ltx_font_smallcaps">N-gram Scores</span></th>
<th id="A3.T5.9.9.10.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.10.1.5.1" class="ltx_text ltx_font_smallcaps">Learnt Sc.</span></th>
</tr>
<tr id="A3.T5.9.9.11.2" class="ltx_tr">
<th id="A3.T5.9.9.11.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.11.2.1.1" class="ltx_text ltx_font_italic">S<sub id="A3.T5.9.9.11.2.1.1.1" class="ltx_sub"><span id="A3.T5.9.9.11.2.1.1.1.1" class="ltx_text ltx_font_smallcaps">E</span></sub></span></th>
<th id="A3.T5.9.9.11.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.11.2.2.1" class="ltx_text ltx_font_smallcaps">bleu1</span></th>
<th id="A3.T5.9.9.11.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.11.2.3.1" class="ltx_text ltx_font_smallcaps">bleu2</span></th>
<th id="A3.T5.9.9.11.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.11.2.4.1" class="ltx_text ltx_font_smallcaps">bleu3</span></th>
<th id="A3.T5.9.9.11.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.11.2.5.1" class="ltx_text ltx_font_smallcaps">bleu4</span></th>
<th id="A3.T5.9.9.11.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.11.2.6.1" class="ltx_text ltx_font_smallcaps">rouge-l</span></th>
<th id="A3.T5.9.9.11.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.11.2.7.1" class="ltx_text ltx_font_smallcaps">meteor</span></th>
<th id="A3.T5.9.9.11.2.8" class="ltx_td ltx_align_right ltx_th ltx_th_column" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="A3.T5.9.9.11.2.8.1" class="ltx_text ltx_font_smallcaps">cide</span>r</th>
<th id="A3.T5.9.9.11.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.11.2.9.1" class="ltx_text ltx_font_smallcaps">spice</span></th>
<th id="A3.T5.9.9.11.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.11.2.10.1" class="ltx_text ltx_font_smallcaps">bertscore</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A3.T5.9.9.12.1" class="ltx_tr">
<td id="A3.T5.9.9.12.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="4"><span id="A3.T5.9.9.12.1.1.1" class="ltx_text ltx_font_smallcaps">A-okvqa</span></td>
<td id="A3.T5.9.9.12.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.12.1.2.1" class="ltx_text ltx_font_smallcaps">beamsearch</span></td>
<td id="A3.T5.9.9.12.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">44.71</td>
<td id="A3.T5.9.9.12.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">52.01</td>
<td id="A3.T5.9.9.12.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">36.69</td>
<td id="A3.T5.9.9.12.1.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">26.72</td>
<td id="A3.T5.9.9.12.1.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">19.88</td>
<td id="A3.T5.9.9.12.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">40.39</td>
<td id="A3.T5.9.9.12.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">22.06</td>
<td id="A3.T5.9.9.12.1.10" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">68.48</td>
<td id="A3.T5.9.9.12.1.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">20.94</td>
<td id="A3.T5.9.9.12.1.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">86.05</td>
</tr>
<tr id="A3.T5.1.1.1" class="ltx_tr">
<td id="A3.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="A3.T5.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">top-k</span> (<math id="A3.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="k=100" display="inline"><semantics id="A3.T5.1.1.1.1.m1.1a"><mrow id="A3.T5.1.1.1.1.m1.1.1" xref="A3.T5.1.1.1.1.m1.1.1.cmml"><mi id="A3.T5.1.1.1.1.m1.1.1.2" xref="A3.T5.1.1.1.1.m1.1.1.2.cmml">k</mi><mo id="A3.T5.1.1.1.1.m1.1.1.1" xref="A3.T5.1.1.1.1.m1.1.1.1.cmml">=</mo><mn id="A3.T5.1.1.1.1.m1.1.1.3" xref="A3.T5.1.1.1.1.m1.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.T5.1.1.1.1.m1.1b"><apply id="A3.T5.1.1.1.1.m1.1.1.cmml" xref="A3.T5.1.1.1.1.m1.1.1"><eq id="A3.T5.1.1.1.1.m1.1.1.1.cmml" xref="A3.T5.1.1.1.1.m1.1.1.1"></eq><ci id="A3.T5.1.1.1.1.m1.1.1.2.cmml" xref="A3.T5.1.1.1.1.m1.1.1.2">𝑘</ci><cn type="integer" id="A3.T5.1.1.1.1.m1.1.1.3.cmml" xref="A3.T5.1.1.1.1.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T5.1.1.1.1.m1.1c">k=100</annotation></semantics></math>)</td>
<td id="A3.T5.1.1.1.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">44.34</td>
<td id="A3.T5.1.1.1.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">52.56</td>
<td id="A3.T5.1.1.1.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">37.06</td>
<td id="A3.T5.1.1.1.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">27.06</td>
<td id="A3.T5.1.1.1.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">19.72</td>
<td id="A3.T5.1.1.1.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">44.45</td>
<td id="A3.T5.1.1.1.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">21.58</td>
<td id="A3.T5.1.1.1.9" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">73.44</td>
<td id="A3.T5.1.1.1.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">19.38</td>
<td id="A3.T5.1.1.1.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">86.27</td>
</tr>
<tr id="A3.T5.2.2.2" class="ltx_tr">
<td id="A3.T5.2.2.2.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="A3.T5.2.2.2.1.1" class="ltx_text ltx_font_smallcaps">nucleus</span> (<math id="A3.T5.2.2.2.1.m1.1" class="ltx_Math" alttext="p=0.4" display="inline"><semantics id="A3.T5.2.2.2.1.m1.1a"><mrow id="A3.T5.2.2.2.1.m1.1.1" xref="A3.T5.2.2.2.1.m1.1.1.cmml"><mi id="A3.T5.2.2.2.1.m1.1.1.2" xref="A3.T5.2.2.2.1.m1.1.1.2.cmml">p</mi><mo id="A3.T5.2.2.2.1.m1.1.1.1" xref="A3.T5.2.2.2.1.m1.1.1.1.cmml">=</mo><mn id="A3.T5.2.2.2.1.m1.1.1.3" xref="A3.T5.2.2.2.1.m1.1.1.3.cmml">0.4</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.T5.2.2.2.1.m1.1b"><apply id="A3.T5.2.2.2.1.m1.1.1.cmml" xref="A3.T5.2.2.2.1.m1.1.1"><eq id="A3.T5.2.2.2.1.m1.1.1.1.cmml" xref="A3.T5.2.2.2.1.m1.1.1.1"></eq><ci id="A3.T5.2.2.2.1.m1.1.1.2.cmml" xref="A3.T5.2.2.2.1.m1.1.1.2">𝑝</ci><cn type="float" id="A3.T5.2.2.2.1.m1.1.1.3.cmml" xref="A3.T5.2.2.2.1.m1.1.1.3">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T5.2.2.2.1.m1.1c">p=0.4</annotation></semantics></math>)</td>
<td id="A3.T5.2.2.2.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.2.2.2.2.1" class="ltx_text ltx_font_bold">50.82</span></td>
<td id="A3.T5.2.2.2.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">58.92</td>
<td id="A3.T5.2.2.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">44.66</td>
<td id="A3.T5.2.2.2.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">35.06</td>
<td id="A3.T5.2.2.2.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">27.35</td>
<td id="A3.T5.2.2.2.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">52.56</td>
<td id="A3.T5.2.2.2.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">24.83</td>
<td id="A3.T5.2.2.2.9" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">101.09</td>
<td id="A3.T5.2.2.2.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">23.33</td>
<td id="A3.T5.2.2.2.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">88.21</td>
</tr>
<tr id="A3.T5.3.3.3" class="ltx_tr">
<td id="A3.T5.3.3.3.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="A3.T5.3.3.3.1.1" class="ltx_text ltx_font_smallcaps">typical</span> (<math id="A3.T5.3.3.3.1.m1.1" class="ltx_Math" alttext="p=0.6" display="inline"><semantics id="A3.T5.3.3.3.1.m1.1a"><mrow id="A3.T5.3.3.3.1.m1.1.1" xref="A3.T5.3.3.3.1.m1.1.1.cmml"><mi id="A3.T5.3.3.3.1.m1.1.1.2" xref="A3.T5.3.3.3.1.m1.1.1.2.cmml">p</mi><mo id="A3.T5.3.3.3.1.m1.1.1.1" xref="A3.T5.3.3.3.1.m1.1.1.1.cmml">=</mo><mn id="A3.T5.3.3.3.1.m1.1.1.3" xref="A3.T5.3.3.3.1.m1.1.1.3.cmml">0.6</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.T5.3.3.3.1.m1.1b"><apply id="A3.T5.3.3.3.1.m1.1.1.cmml" xref="A3.T5.3.3.3.1.m1.1.1"><eq id="A3.T5.3.3.3.1.m1.1.1.1.cmml" xref="A3.T5.3.3.3.1.m1.1.1.1"></eq><ci id="A3.T5.3.3.3.1.m1.1.1.2.cmml" xref="A3.T5.3.3.3.1.m1.1.1.2">𝑝</ci><cn type="float" id="A3.T5.3.3.3.1.m1.1.1.3.cmml" xref="A3.T5.3.3.3.1.m1.1.1.3">0.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T5.3.3.3.1.m1.1c">p=0.6</annotation></semantics></math>)</td>
<td id="A3.T5.3.3.3.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">47.27</td>
<td id="A3.T5.3.3.3.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">54.18</td>
<td id="A3.T5.3.3.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">39.39</td>
<td id="A3.T5.3.3.3.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">29.82</td>
<td id="A3.T5.3.3.3.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">22.18</td>
<td id="A3.T5.3.3.3.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">47.78</td>
<td id="A3.T5.3.3.3.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">22.79</td>
<td id="A3.T5.3.3.3.9" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">84.43</td>
<td id="A3.T5.3.3.3.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">21.47</td>
<td id="A3.T5.3.3.3.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">86.95</td>
</tr>
<tr id="A3.T5.9.9.13.2" class="ltx_tr">
<td id="A3.T5.9.9.13.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="4"><span id="A3.T5.9.9.13.2.1.1" class="ltx_text ltx_font_smallcaps">Vcr</span></td>
<td id="A3.T5.9.9.13.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.13.2.2.1" class="ltx_text ltx_font_smallcaps">beamsearch</span></td>
<td id="A3.T5.9.9.13.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">40.23</td>
<td id="A3.T5.9.9.13.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">26.41</td>
<td id="A3.T5.9.9.13.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">20.15</td>
<td id="A3.T5.9.9.13.2.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">15.95</td>
<td id="A3.T5.9.9.13.2.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">12.47</td>
<td id="A3.T5.9.9.13.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">29.13</td>
<td id="A3.T5.9.9.13.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">16.82</td>
<td id="A3.T5.9.9.13.2.10" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">49.72</td>
<td id="A3.T5.9.9.13.2.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">27.70</td>
<td id="A3.T5.9.9.13.2.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">81.84</td>
</tr>
<tr id="A3.T5.4.4.4" class="ltx_tr">
<td id="A3.T5.4.4.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="A3.T5.4.4.4.1.1" class="ltx_text ltx_font_smallcaps">top-k</span> (<math id="A3.T5.4.4.4.1.m1.1" class="ltx_Math" alttext="k=50" display="inline"><semantics id="A3.T5.4.4.4.1.m1.1a"><mrow id="A3.T5.4.4.4.1.m1.1.1" xref="A3.T5.4.4.4.1.m1.1.1.cmml"><mi id="A3.T5.4.4.4.1.m1.1.1.2" xref="A3.T5.4.4.4.1.m1.1.1.2.cmml">k</mi><mo id="A3.T5.4.4.4.1.m1.1.1.1" xref="A3.T5.4.4.4.1.m1.1.1.1.cmml">=</mo><mn id="A3.T5.4.4.4.1.m1.1.1.3" xref="A3.T5.4.4.4.1.m1.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.T5.4.4.4.1.m1.1b"><apply id="A3.T5.4.4.4.1.m1.1.1.cmml" xref="A3.T5.4.4.4.1.m1.1.1"><eq id="A3.T5.4.4.4.1.m1.1.1.1.cmml" xref="A3.T5.4.4.4.1.m1.1.1.1"></eq><ci id="A3.T5.4.4.4.1.m1.1.1.2.cmml" xref="A3.T5.4.4.4.1.m1.1.1.2">𝑘</ci><cn type="integer" id="A3.T5.4.4.4.1.m1.1.1.3.cmml" xref="A3.T5.4.4.4.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T5.4.4.4.1.m1.1c">k=50</annotation></semantics></math>)</td>
<td id="A3.T5.4.4.4.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">33.19</td>
<td id="A3.T5.4.4.4.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">20.98</td>
<td id="A3.T5.4.4.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">14.89</td>
<td id="A3.T5.4.4.4.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">11.18</td>
<td id="A3.T5.4.4.4.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">8.33</td>
<td id="A3.T5.4.4.4.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">23.65</td>
<td id="A3.T5.4.4.4.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">13.72</td>
<td id="A3.T5.4.4.4.9" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">32.73</td>
<td id="A3.T5.4.4.4.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">21.99</td>
<td id="A3.T5.4.4.4.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">80.31</td>
</tr>
<tr id="A3.T5.5.5.5" class="ltx_tr">
<td id="A3.T5.5.5.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="A3.T5.5.5.5.1.1" class="ltx_text ltx_font_smallcaps">nucleus</span> (<math id="A3.T5.5.5.5.1.m1.1" class="ltx_Math" alttext="p=0.1" display="inline"><semantics id="A3.T5.5.5.5.1.m1.1a"><mrow id="A3.T5.5.5.5.1.m1.1.1" xref="A3.T5.5.5.5.1.m1.1.1.cmml"><mi id="A3.T5.5.5.5.1.m1.1.1.2" xref="A3.T5.5.5.5.1.m1.1.1.2.cmml">p</mi><mo id="A3.T5.5.5.5.1.m1.1.1.1" xref="A3.T5.5.5.5.1.m1.1.1.1.cmml">=</mo><mn id="A3.T5.5.5.5.1.m1.1.1.3" xref="A3.T5.5.5.5.1.m1.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.T5.5.5.5.1.m1.1b"><apply id="A3.T5.5.5.5.1.m1.1.1.cmml" xref="A3.T5.5.5.5.1.m1.1.1"><eq id="A3.T5.5.5.5.1.m1.1.1.1.cmml" xref="A3.T5.5.5.5.1.m1.1.1.1"></eq><ci id="A3.T5.5.5.5.1.m1.1.1.2.cmml" xref="A3.T5.5.5.5.1.m1.1.1.2">𝑝</ci><cn type="float" id="A3.T5.5.5.5.1.m1.1.1.3.cmml" xref="A3.T5.5.5.5.1.m1.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T5.5.5.5.1.m1.1c">p=0.1</annotation></semantics></math>)</td>
<td id="A3.T5.5.5.5.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.5.5.5.2.1" class="ltx_text ltx_font_bold">40.27</span></td>
<td id="A3.T5.5.5.5.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">31.42</td>
<td id="A3.T5.5.5.5.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">22.95</td>
<td id="A3.T5.5.5.5.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">17.62</td>
<td id="A3.T5.5.5.5.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">13.44</td>
<td id="A3.T5.5.5.5.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">29.53</td>
<td id="A3.T5.5.5.5.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">17.54</td>
<td id="A3.T5.5.5.5.9" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">47.33</td>
<td id="A3.T5.5.5.5.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">26.45</td>
<td id="A3.T5.5.5.5.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">81.91</td>
</tr>
<tr id="A3.T5.6.6.6" class="ltx_tr">
<td id="A3.T5.6.6.6.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="A3.T5.6.6.6.1.1" class="ltx_text ltx_font_smallcaps">typical</span> (<math id="A3.T5.6.6.6.1.m1.1" class="ltx_Math" alttext="p=0.4" display="inline"><semantics id="A3.T5.6.6.6.1.m1.1a"><mrow id="A3.T5.6.6.6.1.m1.1.1" xref="A3.T5.6.6.6.1.m1.1.1.cmml"><mi id="A3.T5.6.6.6.1.m1.1.1.2" xref="A3.T5.6.6.6.1.m1.1.1.2.cmml">p</mi><mo id="A3.T5.6.6.6.1.m1.1.1.1" xref="A3.T5.6.6.6.1.m1.1.1.1.cmml">=</mo><mn id="A3.T5.6.6.6.1.m1.1.1.3" xref="A3.T5.6.6.6.1.m1.1.1.3.cmml">0.4</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.T5.6.6.6.1.m1.1b"><apply id="A3.T5.6.6.6.1.m1.1.1.cmml" xref="A3.T5.6.6.6.1.m1.1.1"><eq id="A3.T5.6.6.6.1.m1.1.1.1.cmml" xref="A3.T5.6.6.6.1.m1.1.1.1"></eq><ci id="A3.T5.6.6.6.1.m1.1.1.2.cmml" xref="A3.T5.6.6.6.1.m1.1.1.2">𝑝</ci><cn type="float" id="A3.T5.6.6.6.1.m1.1.1.3.cmml" xref="A3.T5.6.6.6.1.m1.1.1.3">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T5.6.6.6.1.m1.1c">p=0.4</annotation></semantics></math>)</td>
<td id="A3.T5.6.6.6.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">35.12</td>
<td id="A3.T5.6.6.6.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">23.42</td>
<td id="A3.T5.6.6.6.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">16.88</td>
<td id="A3.T5.6.6.6.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">12.83</td>
<td id="A3.T5.6.6.6.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">9.64</td>
<td id="A3.T5.6.6.6.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">25.36</td>
<td id="A3.T5.6.6.6.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">14.70</td>
<td id="A3.T5.6.6.6.9" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">35.85</td>
<td id="A3.T5.6.6.6.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">23.32</td>
<td id="A3.T5.6.6.6.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">80.70</td>
</tr>
<tr id="A3.T5.9.9.14.3" class="ltx_tr">
<td id="A3.T5.9.9.14.3.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="4"><span id="A3.T5.9.9.14.3.1.1" class="ltx_text ltx_font_smallcaps">Vqa-x</span></td>
<td id="A3.T5.9.9.14.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.14.3.2.1" class="ltx_text ltx_font_smallcaps">beamsearch</span></td>
<td id="A3.T5.9.9.14.3.3" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.14.3.3.1" class="ltx_text" style="background-color:#DDEBF7;">35.88</span></td>
<td id="A3.T5.9.9.14.3.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.14.3.4.1" class="ltx_text" style="background-color:#DDEBF7;">37.84</span></td>
<td id="A3.T5.9.9.14.3.5" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.14.3.5.1" class="ltx_text" style="background-color:#DDEBF7;">24.91</span></td>
<td id="A3.T5.9.9.14.3.6" class="ltx_td ltx_align_right ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.14.3.6.1" class="ltx_text" style="background-color:#DDEBF7;">16.67</span></td>
<td id="A3.T5.9.9.14.3.7" class="ltx_td ltx_align_right ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.14.3.7.1" class="ltx_text" style="background-color:#DDEBF7;">10.97</span></td>
<td id="A3.T5.9.9.14.3.8" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.14.3.8.1" class="ltx_text" style="background-color:#DDEBF7;">31.32</span></td>
<td id="A3.T5.9.9.14.3.9" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.14.3.9.1" class="ltx_text" style="background-color:#DDEBF7;">17.90</span></td>
<td id="A3.T5.9.9.14.3.10" class="ltx_td ltx_align_right ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.14.3.10.1" class="ltx_text" style="background-color:#DDEBF7;">38.23</span></td>
<td id="A3.T5.9.9.14.3.11" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.14.3.11.1" class="ltx_text" style="background-color:#DDEBF7;">16.23</span></td>
<td id="A3.T5.9.9.14.3.12" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.14.3.12.1" class="ltx_text" style="background-color:#DDEBF7;">84.39</span></td>
</tr>
<tr id="A3.T5.7.7.7" class="ltx_tr">
<td id="A3.T5.7.7.7.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="A3.T5.7.7.7.1.1" class="ltx_text ltx_font_smallcaps">top-k</span> (<math id="A3.T5.7.7.7.1.m1.1" class="ltx_Math" alttext="k=50" display="inline"><semantics id="A3.T5.7.7.7.1.m1.1a"><mrow id="A3.T5.7.7.7.1.m1.1.1" xref="A3.T5.7.7.7.1.m1.1.1.cmml"><mi id="A3.T5.7.7.7.1.m1.1.1.2" xref="A3.T5.7.7.7.1.m1.1.1.2.cmml">k</mi><mo id="A3.T5.7.7.7.1.m1.1.1.1" xref="A3.T5.7.7.7.1.m1.1.1.1.cmml">=</mo><mn id="A3.T5.7.7.7.1.m1.1.1.3" xref="A3.T5.7.7.7.1.m1.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.T5.7.7.7.1.m1.1b"><apply id="A3.T5.7.7.7.1.m1.1.1.cmml" xref="A3.T5.7.7.7.1.m1.1.1"><eq id="A3.T5.7.7.7.1.m1.1.1.1.cmml" xref="A3.T5.7.7.7.1.m1.1.1.1"></eq><ci id="A3.T5.7.7.7.1.m1.1.1.2.cmml" xref="A3.T5.7.7.7.1.m1.1.1.2">𝑘</ci><cn type="integer" id="A3.T5.7.7.7.1.m1.1.1.3.cmml" xref="A3.T5.7.7.7.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T5.7.7.7.1.m1.1c">k=50</annotation></semantics></math>)</td>
<td id="A3.T5.7.7.7.2" class="ltx_td ltx_align_center" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.7.7.7.2.1" class="ltx_text" style="background-color:#DDEBF7;">33.28</span></td>
<td id="A3.T5.7.7.7.3" class="ltx_td ltx_align_center" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.7.7.7.3.1" class="ltx_text" style="background-color:#DDEBF7;">38.35</span></td>
<td id="A3.T5.7.7.7.4" class="ltx_td ltx_align_center" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.7.7.7.4.1" class="ltx_text" style="background-color:#DDEBF7;">23.11</span></td>
<td id="A3.T5.7.7.7.5" class="ltx_td ltx_align_right" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.7.7.7.5.1" class="ltx_text" style="background-color:#DDEBF7;">14.21</span></td>
<td id="A3.T5.7.7.7.6" class="ltx_td ltx_align_right" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.7.7.7.6.1" class="ltx_text" style="background-color:#DDEBF7;">8.45</span></td>
<td id="A3.T5.7.7.7.7" class="ltx_td ltx_align_center" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.7.7.7.7.1" class="ltx_text" style="background-color:#DDEBF7;">29.15</span></td>
<td id="A3.T5.7.7.7.8" class="ltx_td ltx_align_center" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.7.7.7.8.1" class="ltx_text" style="background-color:#DDEBF7;">17.05</span></td>
<td id="A3.T5.7.7.7.9" class="ltx_td ltx_align_right" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.7.7.7.9.1" class="ltx_text" style="background-color:#DDEBF7;">32.89</span></td>
<td id="A3.T5.7.7.7.10" class="ltx_td ltx_align_center" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.7.7.7.10.1" class="ltx_text" style="background-color:#DDEBF7;">15.26</span></td>
<td id="A3.T5.7.7.7.11" class="ltx_td ltx_align_center" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.7.7.7.11.1" class="ltx_text" style="background-color:#DDEBF7;">83.41</span></td>
</tr>
<tr id="A3.T5.8.8.8" class="ltx_tr">
<td id="A3.T5.8.8.8.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="A3.T5.8.8.8.1.1" class="ltx_text ltx_font_smallcaps">nucleus</span> (<math id="A3.T5.8.8.8.1.m1.1" class="ltx_Math" alttext="p=0.1" display="inline"><semantics id="A3.T5.8.8.8.1.m1.1a"><mrow id="A3.T5.8.8.8.1.m1.1.1" xref="A3.T5.8.8.8.1.m1.1.1.cmml"><mi id="A3.T5.8.8.8.1.m1.1.1.2" xref="A3.T5.8.8.8.1.m1.1.1.2.cmml">p</mi><mo id="A3.T5.8.8.8.1.m1.1.1.1" xref="A3.T5.8.8.8.1.m1.1.1.1.cmml">=</mo><mn id="A3.T5.8.8.8.1.m1.1.1.3" xref="A3.T5.8.8.8.1.m1.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.T5.8.8.8.1.m1.1b"><apply id="A3.T5.8.8.8.1.m1.1.1.cmml" xref="A3.T5.8.8.8.1.m1.1.1"><eq id="A3.T5.8.8.8.1.m1.1.1.1.cmml" xref="A3.T5.8.8.8.1.m1.1.1.1"></eq><ci id="A3.T5.8.8.8.1.m1.1.1.2.cmml" xref="A3.T5.8.8.8.1.m1.1.1.2">𝑝</ci><cn type="float" id="A3.T5.8.8.8.1.m1.1.1.3.cmml" xref="A3.T5.8.8.8.1.m1.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T5.8.8.8.1.m1.1c">p=0.1</annotation></semantics></math>)</td>
<td id="A3.T5.8.8.8.2" class="ltx_td ltx_align_center" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.8.8.8.2.1" class="ltx_text ltx_font_bold" style="background-color:#DDEBF7;">40.67</span></td>
<td id="A3.T5.8.8.8.3" class="ltx_td ltx_align_center" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.8.8.8.3.1" class="ltx_text" style="background-color:#DDEBF7;">47.56</span></td>
<td id="A3.T5.8.8.8.4" class="ltx_td ltx_align_center" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.8.8.8.4.1" class="ltx_text" style="background-color:#DDEBF7;">31.44</span></td>
<td id="A3.T5.8.8.8.5" class="ltx_td ltx_align_right" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.8.8.8.5.1" class="ltx_text" style="background-color:#DDEBF7;">21.47</span></td>
<td id="A3.T5.8.8.8.6" class="ltx_td ltx_align_right" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.8.8.8.6.1" class="ltx_text" style="background-color:#DDEBF7;">14.63</span></td>
<td id="A3.T5.8.8.8.7" class="ltx_td ltx_align_center" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.8.8.8.7.1" class="ltx_text" style="background-color:#DDEBF7;">35.12</span></td>
<td id="A3.T5.8.8.8.8" class="ltx_td ltx_align_center" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.8.8.8.8.1" class="ltx_text" style="background-color:#DDEBF7;">20.29</span></td>
<td id="A3.T5.8.8.8.9" class="ltx_td ltx_align_right" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.8.8.8.9.1" class="ltx_text" style="background-color:#DDEBF7;">50.35</span></td>
<td id="A3.T5.8.8.8.10" class="ltx_td ltx_align_center" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.8.8.8.10.1" class="ltx_text" style="background-color:#DDEBF7;">19.13</span></td>
<td id="A3.T5.8.8.8.11" class="ltx_td ltx_align_center" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.8.8.8.11.1" class="ltx_text" style="background-color:#DDEBF7;">85.40</span></td>
</tr>
<tr id="A3.T5.9.9.9" class="ltx_tr">
<td id="A3.T5.9.9.9.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="A3.T5.9.9.9.1.1" class="ltx_text ltx_font_smallcaps">typical</span> (<math id="A3.T5.9.9.9.1.m1.1" class="ltx_Math" alttext="p=0.5" display="inline"><semantics id="A3.T5.9.9.9.1.m1.1a"><mrow id="A3.T5.9.9.9.1.m1.1.1" xref="A3.T5.9.9.9.1.m1.1.1.cmml"><mi id="A3.T5.9.9.9.1.m1.1.1.2" xref="A3.T5.9.9.9.1.m1.1.1.2.cmml">p</mi><mo id="A3.T5.9.9.9.1.m1.1.1.1" xref="A3.T5.9.9.9.1.m1.1.1.1.cmml">=</mo><mn id="A3.T5.9.9.9.1.m1.1.1.3" xref="A3.T5.9.9.9.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.T5.9.9.9.1.m1.1b"><apply id="A3.T5.9.9.9.1.m1.1.1.cmml" xref="A3.T5.9.9.9.1.m1.1.1"><eq id="A3.T5.9.9.9.1.m1.1.1.1.cmml" xref="A3.T5.9.9.9.1.m1.1.1.1"></eq><ci id="A3.T5.9.9.9.1.m1.1.1.2.cmml" xref="A3.T5.9.9.9.1.m1.1.1.2">𝑝</ci><cn type="float" id="A3.T5.9.9.9.1.m1.1.1.3.cmml" xref="A3.T5.9.9.9.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T5.9.9.9.1.m1.1c">p=0.5</annotation></semantics></math>)</td>
<td id="A3.T5.9.9.9.2" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.9.2.1" class="ltx_text" style="background-color:#DDEBF7;">36.31</span></td>
<td id="A3.T5.9.9.9.3" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.9.3.1" class="ltx_text" style="background-color:#DDEBF7;">40.85</span></td>
<td id="A3.T5.9.9.9.4" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.9.4.1" class="ltx_text" style="background-color:#DDEBF7;">25.57</span></td>
<td id="A3.T5.9.9.9.5" class="ltx_td ltx_align_right ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.9.5.1" class="ltx_text" style="background-color:#DDEBF7;">16.82</span></td>
<td id="A3.T5.9.9.9.6" class="ltx_td ltx_align_right ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.9.6.1" class="ltx_text" style="background-color:#DDEBF7;">11.14</span></td>
<td id="A3.T5.9.9.9.7" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.9.7.1" class="ltx_text" style="background-color:#DDEBF7;">31.08</span></td>
<td id="A3.T5.9.9.9.8" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.9.8.1" class="ltx_text" style="background-color:#DDEBF7;">18.15</span></td>
<td id="A3.T5.9.9.9.9" class="ltx_td ltx_align_right ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.9.9.1" class="ltx_text" style="background-color:#DDEBF7;">39.71</span></td>
<td id="A3.T5.9.9.9.10" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.9.10.1" class="ltx_text" style="background-color:#DDEBF7;">16.62</span></td>
<td id="A3.T5.9.9.9.11" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T5.9.9.9.11.1" class="ltx_text" style="background-color:#DDEBF7;">83.93</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Explanation scores with automatic NLG for generated explanations (<span id="A3.T5.11.1" class="ltx_text ltx_font_typewriter">QA<math id="A3.T5.11.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="A3.T5.11.1.m1.1b"><mo stretchy="false" id="A3.T5.11.1.m1.1.1" xref="A3.T5.11.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A3.T5.11.1.m1.1c"><ci id="A3.T5.11.1.m1.1.1.cmml" xref="A3.T5.11.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T5.11.1.m1.1d">\rightarrow</annotation></semantics></math>E</span>) from <span id="A3.T5.13.2" class="ltx_text ltx_font_smallcaps">umae<sub id="A3.T5.13.2.1" class="ltx_sub">all</sub></span> model with different decoding strategies.
The last two rows (with blue shadow) indicate out-of-domain performance.
</figcaption>
</figure>
<figure id="A3.T6" class="ltx_table">
<div id="A3.T6.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:384.4pt;height:108.8pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-64.1pt,18.0pt) scale(0.75,0.75) ;">
<table id="A3.T6.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A3.T6.3.3.4.1" class="ltx_tr">
<th id="A3.T6.3.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="A3.T6.3.3.4.1.1.1" class="ltx_text ltx_font_smallcaps">Dataset</span></th>
<th id="A3.T6.3.3.4.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="A3.T6.3.3.4.1.2.1" class="ltx_text ltx_font_smallcaps">Decoding</span></th>
<th id="A3.T6.3.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">e<span id="A3.T6.3.3.4.1.3.1" class="ltx_text ltx_font_smallcaps">-V</span>i<span id="A3.T6.3.3.4.1.3.2" class="ltx_text ltx_font_smallcaps">l</span>
</th>
<th id="A3.T6.3.3.4.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="8"><span id="A3.T6.3.3.4.1.4.1" class="ltx_text ltx_font_smallcaps">N-gram Scores</span></th>
<th id="A3.T6.3.3.4.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.4.1.5.1" class="ltx_text ltx_font_smallcaps">Learnt Sc.</span></th>
</tr>
<tr id="A3.T6.3.3.5.2" class="ltx_tr">
<td id="A3.T6.3.3.5.2.1" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.5.2.1.1" class="ltx_text ltx_font_italic">S<sub id="A3.T6.3.3.5.2.1.1.1" class="ltx_sub"><span id="A3.T6.3.3.5.2.1.1.1.1" class="ltx_text ltx_font_smallcaps">E</span></sub></span></td>
<td id="A3.T6.3.3.5.2.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.5.2.2.1" class="ltx_text ltx_font_smallcaps">bleu1</span></td>
<td id="A3.T6.3.3.5.2.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.5.2.3.1" class="ltx_text ltx_font_smallcaps">bleu2</span></td>
<td id="A3.T6.3.3.5.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.5.2.4.1" class="ltx_text ltx_font_smallcaps">bleu3</span></td>
<td id="A3.T6.3.3.5.2.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.5.2.5.1" class="ltx_text ltx_font_smallcaps">bleu4</span></td>
<td id="A3.T6.3.3.5.2.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.5.2.6.1" class="ltx_text ltx_font_smallcaps">rouge-l</span></td>
<td id="A3.T6.3.3.5.2.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.5.2.7.1" class="ltx_text ltx_font_smallcaps">meteor</span></td>
<td id="A3.T6.3.3.5.2.8" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="A3.T6.3.3.5.2.8.1" class="ltx_text ltx_font_smallcaps">cide</span>r</td>
<td id="A3.T6.3.3.5.2.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.5.2.9.1" class="ltx_text ltx_font_smallcaps">spice</span></td>
<td id="A3.T6.3.3.5.2.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.5.2.10.1" class="ltx_text ltx_font_smallcaps">bertscore</span></td>
</tr>
<tr id="A3.T6.3.3.6.3" class="ltx_tr">
<th id="A3.T6.3.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="A3.T6.3.3.6.3.1.1" class="ltx_text ltx_font_smallcaps">A-okvqa</span></th>
<th id="A3.T6.3.3.6.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.6.3.2.1" class="ltx_text ltx_font_smallcaps">beamsearch</span></th>
<th id="A3.T6.3.3.6.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.6.3.3.1" class="ltx_text ltx_font_bold">47.01</span></th>
<th id="A3.T6.3.3.6.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">54.75</th>
<th id="A3.T6.3.3.6.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">41.39</th>
<th id="A3.T6.3.3.6.3.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.6.3.6.1" class="ltx_text ltx_font_bold">32.08</span></th>
<th id="A3.T6.3.3.6.3.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.6.3.7.1" class="ltx_text ltx_font_bold">24.25</span></th>
<th id="A3.T6.3.3.6.3.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.6.3.8.1" class="ltx_text ltx_font_bold">49.75</span></th>
<th id="A3.T6.3.3.6.3.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.6.3.9.1" class="ltx_text ltx_font_bold">22.54</span></th>
<th id="A3.T6.3.3.6.3.10" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.6.3.10.1" class="ltx_text ltx_font_bold">86.28</span></th>
<th id="A3.T6.3.3.6.3.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.6.3.11.1" class="ltx_text ltx_font_bold">20.68</span></th>
<th id="A3.T6.3.3.6.3.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.6.3.12.1" class="ltx_text ltx_font_bold">87.39</span></th>
</tr>
<tr id="A3.T6.1.1.1" class="ltx_tr">
<td id="A3.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="A3.T6.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">nucleus</span> (<math id="A3.T6.1.1.1.1.m1.1" class="ltx_Math" alttext="p=0.5" display="inline"><semantics id="A3.T6.1.1.1.1.m1.1a"><mrow id="A3.T6.1.1.1.1.m1.1.1" xref="A3.T6.1.1.1.1.m1.1.1.cmml"><mi id="A3.T6.1.1.1.1.m1.1.1.2" xref="A3.T6.1.1.1.1.m1.1.1.2.cmml">p</mi><mo id="A3.T6.1.1.1.1.m1.1.1.1" xref="A3.T6.1.1.1.1.m1.1.1.1.cmml">=</mo><mn id="A3.T6.1.1.1.1.m1.1.1.3" xref="A3.T6.1.1.1.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.T6.1.1.1.1.m1.1b"><apply id="A3.T6.1.1.1.1.m1.1.1.cmml" xref="A3.T6.1.1.1.1.m1.1.1"><eq id="A3.T6.1.1.1.1.m1.1.1.1.cmml" xref="A3.T6.1.1.1.1.m1.1.1.1"></eq><ci id="A3.T6.1.1.1.1.m1.1.1.2.cmml" xref="A3.T6.1.1.1.1.m1.1.1.2">𝑝</ci><cn type="float" id="A3.T6.1.1.1.1.m1.1.1.3.cmml" xref="A3.T6.1.1.1.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.1.1.1.1.m1.1c">p=0.5</annotation></semantics></math>)</td>
<td id="A3.T6.1.1.1.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">46.72</td>
<td id="A3.T6.1.1.1.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.1.1.1.3.1" class="ltx_text ltx_font_bold">55.53</span></td>
<td id="A3.T6.1.1.1.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.1.1.1.4.1" class="ltx_text ltx_font_bold">41.63</span></td>
<td id="A3.T6.1.1.1.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">31.91</td>
<td id="A3.T6.1.1.1.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">23.67</td>
<td id="A3.T6.1.1.1.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">49.16</td>
<td id="A3.T6.1.1.1.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">22.48</td>
<td id="A3.T6.1.1.1.9" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">82.37</td>
<td id="A3.T6.1.1.1.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">20.67</td>
<td id="A3.T6.1.1.1.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">87.18</td>
</tr>
<tr id="A3.T6.3.3.7.4" class="ltx_tr">
<th id="A3.T6.3.3.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="A3.T6.3.3.7.4.1.1" class="ltx_text ltx_font_smallcaps">Vcr</span></th>
<th id="A3.T6.3.3.7.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.7.4.2.1" class="ltx_text ltx_font_smallcaps">beamsearch</span></th>
<th id="A3.T6.3.3.7.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.7.4.3.1" class="ltx_text ltx_font_bold">37.02</span></th>
<th id="A3.T6.3.3.7.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">25.00</th>
<th id="A3.T6.3.3.7.4.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">18.90</th>
<th id="A3.T6.3.3.7.4.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.7.4.6.1" class="ltx_text ltx_font_bold">14.87</span></th>
<th id="A3.T6.3.3.7.4.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.7.4.7.1" class="ltx_text ltx_font_bold">11.54</span></th>
<th id="A3.T6.3.3.7.4.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.7.4.8.1" class="ltx_text ltx_font_bold">27.07</span></th>
<th id="A3.T6.3.3.7.4.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.7.4.9.1" class="ltx_text ltx_font_bold">15.66</span></th>
<th id="A3.T6.3.3.7.4.10" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.7.4.10.1" class="ltx_text ltx_font_bold">38.77</span></th>
<th id="A3.T6.3.3.7.4.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.7.4.11.1" class="ltx_text ltx_font_bold">25.03</span></th>
<th id="A3.T6.3.3.7.4.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.7.4.12.1" class="ltx_text ltx_font_bold">80.68</span></th>
</tr>
<tr id="A3.T6.2.2.2" class="ltx_tr">
<td id="A3.T6.2.2.2.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="A3.T6.2.2.2.1.1" class="ltx_text ltx_font_smallcaps">nucleus</span> (<math id="A3.T6.2.2.2.1.m1.1" class="ltx_Math" alttext="p=0.1" display="inline"><semantics id="A3.T6.2.2.2.1.m1.1a"><mrow id="A3.T6.2.2.2.1.m1.1.1" xref="A3.T6.2.2.2.1.m1.1.1.cmml"><mi id="A3.T6.2.2.2.1.m1.1.1.2" xref="A3.T6.2.2.2.1.m1.1.1.2.cmml">p</mi><mo id="A3.T6.2.2.2.1.m1.1.1.1" xref="A3.T6.2.2.2.1.m1.1.1.1.cmml">=</mo><mn id="A3.T6.2.2.2.1.m1.1.1.3" xref="A3.T6.2.2.2.1.m1.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.T6.2.2.2.1.m1.1b"><apply id="A3.T6.2.2.2.1.m1.1.1.cmml" xref="A3.T6.2.2.2.1.m1.1.1"><eq id="A3.T6.2.2.2.1.m1.1.1.1.cmml" xref="A3.T6.2.2.2.1.m1.1.1.1"></eq><ci id="A3.T6.2.2.2.1.m1.1.1.2.cmml" xref="A3.T6.2.2.2.1.m1.1.1.2">𝑝</ci><cn type="float" id="A3.T6.2.2.2.1.m1.1.1.3.cmml" xref="A3.T6.2.2.2.1.m1.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.2.2.2.1.m1.1c">p=0.1</annotation></semantics></math>)</td>
<td id="A3.T6.2.2.2.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">35.10</td>
<td id="A3.T6.2.2.2.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.2.2.2.3.1" class="ltx_text ltx_font_bold">27.41</span></td>
<td id="A3.T6.2.2.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.2.2.2.4.1" class="ltx_text ltx_font_bold">19.36</span></td>
<td id="A3.T6.2.2.2.5" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">14.50</td>
<td id="A3.T6.2.2.2.6" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">10.73</td>
<td id="A3.T6.2.2.2.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">26.18</td>
<td id="A3.T6.2.2.2.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">15.21</td>
<td id="A3.T6.2.2.2.9" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">34.99</td>
<td id="A3.T6.2.2.2.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">21.88</td>
<td id="A3.T6.2.2.2.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">80.52</td>
</tr>
<tr id="A3.T6.3.3.8.5" class="ltx_tr">
<th id="A3.T6.3.3.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_bb ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="A3.T6.3.3.8.5.1.1" class="ltx_text ltx_font_smallcaps">Vqa-x</span></th>
<th id="A3.T6.3.3.8.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.8.5.2.1" class="ltx_text ltx_font_smallcaps">beamsearch</span></th>
<th id="A3.T6.3.3.8.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.8.5.3.1" class="ltx_text" style="background-color:#DDEBF7;">38.13</span></th>
<th id="A3.T6.3.3.8.5.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.8.5.4.1" class="ltx_text" style="background-color:#DDEBF7;">39.91</span></th>
<th id="A3.T6.3.3.8.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.8.5.5.1" class="ltx_text" style="background-color:#DDEBF7;">26.30</span></th>
<th id="A3.T6.3.3.8.5.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.8.5.6.1" class="ltx_text" style="background-color:#DDEBF7;">17.99</span></th>
<th id="A3.T6.3.3.8.5.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.8.5.7.1" class="ltx_text" style="background-color:#DDEBF7;">12.46</span></th>
<th id="A3.T6.3.3.8.5.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.8.5.8.1" class="ltx_text" style="background-color:#DDEBF7;">31.69</span></th>
<th id="A3.T6.3.3.8.5.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.8.5.9.1" class="ltx_text" style="background-color:#DDEBF7;">19.11</span></th>
<th id="A3.T6.3.3.8.5.10" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.8.5.10.1" class="ltx_text" style="background-color:#DDEBF7;">42.10</span></th>
<th id="A3.T6.3.3.8.5.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.8.5.11.1" class="ltx_text" style="background-color:#DDEBF7;">18.15</span></th>
<th id="A3.T6.3.3.8.5.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.8.5.12.1" class="ltx_text" style="background-color:#DDEBF7;">84.95</span></th>
</tr>
<tr id="A3.T6.3.3.3" class="ltx_tr">
<td id="A3.T6.3.3.3.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="A3.T6.3.3.3.1.1" class="ltx_text ltx_font_smallcaps">nucleus</span> (<math id="A3.T6.3.3.3.1.m1.1" class="ltx_Math" alttext="p=0.1" display="inline"><semantics id="A3.T6.3.3.3.1.m1.1a"><mrow id="A3.T6.3.3.3.1.m1.1.1" xref="A3.T6.3.3.3.1.m1.1.1.cmml"><mi id="A3.T6.3.3.3.1.m1.1.1.2" xref="A3.T6.3.3.3.1.m1.1.1.2.cmml">p</mi><mo id="A3.T6.3.3.3.1.m1.1.1.1" xref="A3.T6.3.3.3.1.m1.1.1.1.cmml">=</mo><mn id="A3.T6.3.3.3.1.m1.1.1.3" xref="A3.T6.3.3.3.1.m1.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.T6.3.3.3.1.m1.1b"><apply id="A3.T6.3.3.3.1.m1.1.1.cmml" xref="A3.T6.3.3.3.1.m1.1.1"><eq id="A3.T6.3.3.3.1.m1.1.1.1.cmml" xref="A3.T6.3.3.3.1.m1.1.1.1"></eq><ci id="A3.T6.3.3.3.1.m1.1.1.2.cmml" xref="A3.T6.3.3.3.1.m1.1.1.2">𝑝</ci><cn type="float" id="A3.T6.3.3.3.1.m1.1.1.3.cmml" xref="A3.T6.3.3.3.1.m1.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.3.3.3.1.m1.1c">p=0.1</annotation></semantics></math>)</td>
<td id="A3.T6.3.3.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.3.2.1" class="ltx_text ltx_font_bold" style="background-color:#DDEBF7;">39.67</span></td>
<td id="A3.T6.3.3.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.3.3.1" class="ltx_text ltx_font_bold" style="background-color:#DDEBF7;">44.92</span></td>
<td id="A3.T6.3.3.3.4" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.3.4.1" class="ltx_text ltx_font_bold" style="background-color:#DDEBF7;">28.88</span></td>
<td id="A3.T6.3.3.3.5" class="ltx_td ltx_align_right ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.3.5.1" class="ltx_text ltx_font_bold" style="background-color:#DDEBF7;">19.04</span></td>
<td id="A3.T6.3.3.3.6" class="ltx_td ltx_align_right ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.3.6.1" class="ltx_text ltx_font_bold" style="background-color:#DDEBF7;">12.55</span></td>
<td id="A3.T6.3.3.3.7" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.3.7.1" class="ltx_text ltx_font_bold" style="background-color:#DDEBF7;">33.08</span></td>
<td id="A3.T6.3.3.3.8" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.3.8.1" class="ltx_text ltx_font_bold" style="background-color:#DDEBF7;">20.07</span></td>
<td id="A3.T6.3.3.3.9" class="ltx_td ltx_align_right ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.3.9.1" class="ltx_text ltx_font_bold" style="background-color:#DDEBF7;">44.28</span></td>
<td id="A3.T6.3.3.3.10" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.3.10.1" class="ltx_text ltx_font_bold" style="background-color:#DDEBF7;">19.19</span></td>
<td id="A3.T6.3.3.3.11" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A3.T6.3.3.3.11.1" class="ltx_text ltx_font_bold" style="background-color:#DDEBF7;">85.21</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Explanation scores with automatic NLG for generated explanations from <span id="A3.T6.5.1" class="ltx_text ltx_font_typewriter">Q<math id="A3.T6.5.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="A3.T6.5.1.m1.1b"><mo stretchy="false" id="A3.T6.5.1.m1.1.1" xref="A3.T6.5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A3.T6.5.1.m1.1c"><ci id="A3.T6.5.1.m1.1.1.cmml" xref="A3.T6.5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.5.1.m1.1d">\rightarrow</annotation></semantics></math>AE</span> with <span id="A3.T6.7.2" class="ltx_text ltx_font_smallcaps">umae<sub id="A3.T6.7.2.1" class="ltx_sub">all</sub></span> model.
The last two rows (with blue shadow) indicate out-of-domain performance.
</figcaption>
</figure>
<figure id="A3.F4" class="ltx_figure"><img src="/html/2301.10799/assets/x4.png" id="A3.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="506" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Examples of generated answers and explanations for A-OKVQA.</figcaption>
</figure>
<figure id="A3.F5" class="ltx_figure"><img src="/html/2301.10799/assets/x5.png" id="A3.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="523" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Examples of generated answers and explanations generation for VCR.</figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>More Explanation Scores</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.6" class="ltx_p">For decoding, we evaluate the performance of beam search with the size of 5, top-k sampling with <math id="A4.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A4.p1.1.m1.1a"><mi id="A4.p1.1.m1.1.1" xref="A4.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A4.p1.1.m1.1b"><ci id="A4.p1.1.m1.1.1.cmml" xref="A4.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.1.m1.1c">k</annotation></semantics></math> from <math id="A4.p1.2.m2.5" class="ltx_Math" alttext="\{50,100,200,...,1000\}" display="inline"><semantics id="A4.p1.2.m2.5a"><mrow id="A4.p1.2.m2.5.6.2" xref="A4.p1.2.m2.5.6.1.cmml"><mo stretchy="false" id="A4.p1.2.m2.5.6.2.1" xref="A4.p1.2.m2.5.6.1.cmml">{</mo><mn id="A4.p1.2.m2.1.1" xref="A4.p1.2.m2.1.1.cmml">50</mn><mo id="A4.p1.2.m2.5.6.2.2" xref="A4.p1.2.m2.5.6.1.cmml">,</mo><mn id="A4.p1.2.m2.2.2" xref="A4.p1.2.m2.2.2.cmml">100</mn><mo id="A4.p1.2.m2.5.6.2.3" xref="A4.p1.2.m2.5.6.1.cmml">,</mo><mn id="A4.p1.2.m2.3.3" xref="A4.p1.2.m2.3.3.cmml">200</mn><mo id="A4.p1.2.m2.5.6.2.4" xref="A4.p1.2.m2.5.6.1.cmml">,</mo><mi mathvariant="normal" id="A4.p1.2.m2.4.4" xref="A4.p1.2.m2.4.4.cmml">…</mi><mo id="A4.p1.2.m2.5.6.2.5" xref="A4.p1.2.m2.5.6.1.cmml">,</mo><mn id="A4.p1.2.m2.5.5" xref="A4.p1.2.m2.5.5.cmml">1000</mn><mo stretchy="false" id="A4.p1.2.m2.5.6.2.6" xref="A4.p1.2.m2.5.6.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="A4.p1.2.m2.5b"><set id="A4.p1.2.m2.5.6.1.cmml" xref="A4.p1.2.m2.5.6.2"><cn type="integer" id="A4.p1.2.m2.1.1.cmml" xref="A4.p1.2.m2.1.1">50</cn><cn type="integer" id="A4.p1.2.m2.2.2.cmml" xref="A4.p1.2.m2.2.2">100</cn><cn type="integer" id="A4.p1.2.m2.3.3.cmml" xref="A4.p1.2.m2.3.3">200</cn><ci id="A4.p1.2.m2.4.4.cmml" xref="A4.p1.2.m2.4.4">…</ci><cn type="integer" id="A4.p1.2.m2.5.5.cmml" xref="A4.p1.2.m2.5.5">1000</cn></set></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.2.m2.5c">\{50,100,200,...,1000\}</annotation></semantics></math>, and Nucleus and Typical <cite class="ltx_cite ltx_citemacro_cite">Meister et al. (<a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite> sampling, both with <math id="A4.p1.3.m3.1" class="ltx_Math" alttext="p" display="inline"><semantics id="A4.p1.3.m3.1a"><mi id="A4.p1.3.m3.1.1" xref="A4.p1.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A4.p1.3.m3.1b"><ci id="A4.p1.3.m3.1.1.cmml" xref="A4.p1.3.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.3.m3.1c">p</annotation></semantics></math> from <math id="A4.p1.4.m4.4" class="ltx_Math" alttext="\{0.1,0.2,...,0.9\}" display="inline"><semantics id="A4.p1.4.m4.4a"><mrow id="A4.p1.4.m4.4.5.2" xref="A4.p1.4.m4.4.5.1.cmml"><mo stretchy="false" id="A4.p1.4.m4.4.5.2.1" xref="A4.p1.4.m4.4.5.1.cmml">{</mo><mn id="A4.p1.4.m4.1.1" xref="A4.p1.4.m4.1.1.cmml">0.1</mn><mo id="A4.p1.4.m4.4.5.2.2" xref="A4.p1.4.m4.4.5.1.cmml">,</mo><mn id="A4.p1.4.m4.2.2" xref="A4.p1.4.m4.2.2.cmml">0.2</mn><mo id="A4.p1.4.m4.4.5.2.3" xref="A4.p1.4.m4.4.5.1.cmml">,</mo><mi mathvariant="normal" id="A4.p1.4.m4.3.3" xref="A4.p1.4.m4.3.3.cmml">…</mi><mo id="A4.p1.4.m4.4.5.2.4" xref="A4.p1.4.m4.4.5.1.cmml">,</mo><mn id="A4.p1.4.m4.4.4" xref="A4.p1.4.m4.4.4.cmml">0.9</mn><mo stretchy="false" id="A4.p1.4.m4.4.5.2.5" xref="A4.p1.4.m4.4.5.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="A4.p1.4.m4.4b"><set id="A4.p1.4.m4.4.5.1.cmml" xref="A4.p1.4.m4.4.5.2"><cn type="float" id="A4.p1.4.m4.1.1.cmml" xref="A4.p1.4.m4.1.1">0.1</cn><cn type="float" id="A4.p1.4.m4.2.2.cmml" xref="A4.p1.4.m4.2.2">0.2</cn><ci id="A4.p1.4.m4.3.3.cmml" xref="A4.p1.4.m4.3.3">…</ci><cn type="float" id="A4.p1.4.m4.4.4.cmml" xref="A4.p1.4.m4.4.4">0.9</cn></set></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.4.m4.4c">\{0.1,0.2,...,0.9\}</annotation></semantics></math>.
We show the details of the NLG scores using different decoding strategies for explanations generated from <span id="A4.p1.5.1" class="ltx_text ltx_font_typewriter">QA<math id="A4.p1.5.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="A4.p1.5.1.m1.1a"><mo stretchy="false" id="A4.p1.5.1.m1.1.1" xref="A4.p1.5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A4.p1.5.1.m1.1b"><ci id="A4.p1.5.1.m1.1.1.cmml" xref="A4.p1.5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.5.1.m1.1c">\rightarrow</annotation></semantics></math>E</span> in <a href="#A3.T5" title="Table 5 ‣ Appendix C Ablations on Modality Dependency ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 5</span></a>, and <span id="A4.p1.6.2" class="ltx_text ltx_font_typewriter">Q<math id="A4.p1.6.2.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="A4.p1.6.2.m1.1a"><mo stretchy="false" id="A4.p1.6.2.m1.1.1" xref="A4.p1.6.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A4.p1.6.2.m1.1b"><ci id="A4.p1.6.2.m1.1.1.cmml" xref="A4.p1.6.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.6.2.m1.1c">\rightarrow</annotation></semantics></math>AE</span> in <a href="#A3.T6" title="Table 6 ‣ Appendix C Ablations on Modality Dependency ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 6</span></a>.</p>
</div>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Examples of Generated Explanations</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">Examples of the explanations generated with beam search and Nucleus sampling for A-OKVQA are shown in <a href="#A3.F4" title="Figure 4 ‣ Appendix C Ablations on Modality Dependency ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>, and VCR in <a href="#A3.F5" title="Figure 5 ‣ Appendix C Ablations on Modality Dependency ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>.</p>
</div>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Joint Generation Performance</h2>

<div id="A6.p1" class="ltx_para">
<p id="A6.p1.1" class="ltx_p">We present the results of the proposed <span id="A6.p1.1.1" class="ltx_text ltx_font_typewriter">Q<math id="A6.p1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="A6.p1.1.1.m1.1a"><mo stretchy="false" id="A6.p1.1.1.m1.1.1" xref="A6.p1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A6.p1.1.1.m1.1b"><ci id="A6.p1.1.1.m1.1.1.cmml" xref="A6.p1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.1.1.m1.1c">\rightarrow</annotation></semantics></math>AE</span> task where answers and explanations are jointly generated.
We parse the generated sequence to the answer and the explanation and use the same sets of metrics as the separate generation for evaluation.
Results for answers in <a href="#A6.T7" title="Table 7 ‣ Appendix F Joint Generation Performance ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 7</span></a> and explanations in <a href="#A6.T8" title="Table 8 ‣ Appendix F Joint Generation Performance ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 8</span></a>.
For answers, since the perplexity metric does not directly compare the generation, we show the Multiple-Choice accuracy using the Glove metric for A-OKVQA and <span id="A6.p1.1.2" class="ltx_text ltx_font_typewriter">BERTScore</span> for VCR answer sentences.</p>
</div>
<figure id="A6.T7" class="ltx_table">
<div id="A6.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:150.5pt;height:57.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-18.8pt,7.2pt) scale(0.8,0.8) ;">
<table id="A6.T7.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A6.T7.1.1.1.1" class="ltx_tr">
<th id="A6.T7.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="A6.T7.1.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">Task</span></th>
<th id="A6.T7.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T7.1.1.1.1.2.1" class="ltx_text ltx_font_smallcaps">A-okvqa</span></th>
<th id="A6.T7.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T7.1.1.1.1.3.1" class="ltx_text ltx_font_smallcaps">Vcr</span></th>
<th id="A6.T7.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T7.1.1.1.1.4.1" class="ltx_text ltx_font_smallcaps">Vqa-x</span></th>
</tr>
<tr id="A6.T7.1.1.2.2" class="ltx_tr">
<th id="A6.T7.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T7.1.1.2.2.1.1" class="ltx_text ltx_font_smallcaps">mc (golve)</span></th>
<th id="A6.T7.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T7.1.1.2.2.2.1" class="ltx_text ltx_font_smallcaps">bertscore</span></th>
<th id="A6.T7.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T7.1.1.2.2.3.1" class="ltx_text ltx_font_smallcaps">da</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A6.T7.1.1.3.1" class="ltx_tr">
<th id="A6.T7.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T7.1.1.3.1.1.1" class="ltx_text ltx_font_smallcaps">q-&gt;a</span></th>
<td id="A6.T7.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">65.67</td>
<td id="A6.T7.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">81.91</td>
<td id="A6.T7.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T7.1.1.3.1.4.1" class="ltx_text" style="background-color:#DDEBF7;">77.65</span></td>
</tr>
<tr id="A6.T7.1.1.4.2" class="ltx_tr">
<th id="A6.T7.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T7.1.1.4.2.1.1" class="ltx_text ltx_font_smallcaps">q-&gt;ae</span></th>
<td id="A6.T7.1.1.4.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">65.67</td>
<td id="A6.T7.1.1.4.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">82.30</td>
<td id="A6.T7.1.1.4.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T7.1.1.4.2.4.1" class="ltx_text" style="background-color:#DDEBF7;">69.60</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Evaluation of answers generated given questions (<span id="A6.T7.4.1" class="ltx_text ltx_font_typewriter">Q-&gt;A</span>) and jointly generated with explanations (<span id="A6.T7.5.2" class="ltx_text ltx_font_typewriter">Q-&gt;AE</span>).
MC stands for Multiple Choice, DA for Direct Answer.
The last column with a blue shadow indicates out-of-domain performance.</figcaption>
</figure>
<figure id="A6.T8" class="ltx_table">
<div id="A6.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:203.8pt;height:67.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-34.0pt,11.3pt) scale(0.75,0.75) ;">
<table id="A6.T8.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A6.T8.1.1.1.1" class="ltx_tr">
<th id="A6.T8.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="A6.T8.1.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">Dataset</span></th>
<th id="A6.T8.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="2"><span id="A6.T8.1.1.1.1.2.1" class="ltx_text ltx_font_italic">S<sub id="A6.T8.1.1.1.1.2.1.1" class="ltx_sub"><span id="A6.T8.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_smallcaps">E</span></sub></span></th>
<th id="A6.T8.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="2"><span id="A6.T8.1.1.1.1.3.1" class="ltx_text ltx_font_smallcaps">Ngramscore</span></th>
<th id="A6.T8.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="2"><span id="A6.T8.1.1.1.1.4.1" class="ltx_text ltx_font_smallcaps">Bertscore</span></th>
</tr>
<tr id="A6.T8.1.1.2.2" class="ltx_tr">
<th id="A6.T8.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T8.1.1.2.2.1.1" class="ltx_text ltx_font_smallcaps">qa-&gt;e</span></th>
<th id="A6.T8.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T8.1.1.2.2.2.1" class="ltx_text ltx_font_smallcaps">q-&gt;ae</span></th>
<th id="A6.T8.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T8.1.1.2.2.3.1" class="ltx_text ltx_font_smallcaps">qa-&gt;e</span></th>
<th id="A6.T8.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T8.1.1.2.2.4.1" class="ltx_text ltx_font_smallcaps">q-&gt;ae</span></th>
<th id="A6.T8.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T8.1.1.2.2.5.1" class="ltx_text ltx_font_smallcaps">qa-&gt;e</span></th>
<th id="A6.T8.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T8.1.1.2.2.6.1" class="ltx_text ltx_font_smallcaps">q-&gt;ae</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A6.T8.1.1.3.1" class="ltx_tr">
<th id="A6.T8.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T8.1.1.3.1.1.1" class="ltx_text ltx_font_smallcaps">A-okvqa</span></th>
<td id="A6.T8.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">50.82</td>
<td id="A6.T8.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">47.01</td>
<td id="A6.T8.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">35.69</td>
<td id="A6.T8.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">32.15</td>
<td id="A6.T8.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">88.21</td>
<td id="A6.T8.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">87.39</td>
</tr>
<tr id="A6.T8.1.1.4.2" class="ltx_tr">
<th id="A6.T8.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T8.1.1.4.2.1.1" class="ltx_text ltx_font_smallcaps">Vcr</span></th>
<td id="A6.T8.1.1.4.2.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">40.27</td>
<td id="A6.T8.1.1.4.2.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">37.02</td>
<td id="A6.T8.1.1.4.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">26.70</td>
<td id="A6.T8.1.1.4.2.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">24.02</td>
<td id="A6.T8.1.1.4.2.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">81.91</td>
<td id="A6.T8.1.1.4.2.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">80.68</td>
</tr>
<tr id="A6.T8.1.1.5.3" class="ltx_tr">
<th id="A6.T8.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T8.1.1.5.3.1.1" class="ltx_text ltx_font_smallcaps">Vqa-x</span></th>
<td id="A6.T8.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T8.1.1.5.3.2.1" class="ltx_text" style="background-color:#DDEBF7;">40.67</span></td>
<td id="A6.T8.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T8.1.1.5.3.3.1" class="ltx_text" style="background-color:#DDEBF7;">39.67</span></td>
<td id="A6.T8.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T8.1.1.5.3.4.1" class="ltx_text" style="background-color:#DDEBF7;">26.69</span></td>
<td id="A6.T8.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T8.1.1.5.3.5.1" class="ltx_text" style="background-color:#DDEBF7;">25.85</span></td>
<td id="A6.T8.1.1.5.3.6" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T8.1.1.5.3.6.1" class="ltx_text" style="background-color:#DDEBF7;">85.40</span></td>
<td id="A6.T8.1.1.5.3.7" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DDEBF7;padding-left:5.0pt;padding-right:5.0pt;"><span id="A6.T8.1.1.5.3.7.1" class="ltx_text" style="background-color:#DDEBF7;">85.21</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Scores of explanations generated given answers (<span id="A6.T8.4.1" class="ltx_text ltx_font_typewriter">QA-&gt;E</span>) and jointly generated with answers (<span id="A6.T8.5.2" class="ltx_text ltx_font_typewriter">Q-&gt;AE</span>).
The last row with a blue shadow indicates out-of-domain performance.</figcaption>
</figure>
<figure id="A6.T9" class="ltx_table">
<div id="A6.T9.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:152.4pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-19.1pt,9.0pt) scale(0.8,0.8) ;">
<table id="A6.T9.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A6.T9.1.1.1.1" class="ltx_tr">
<th id="A6.T9.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="A6.T9.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A6.T9.1.1.1.1.2.1" class="ltx_text ltx_font_smallcaps">Ok-vqa</span></th>
<th id="A6.T9.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="A6.T9.1.1.1.1.3.1" class="ltx_text ltx_font_smallcaps">A-okvqa</span></th>
</tr>
<tr id="A6.T9.1.1.2.2" class="ltx_tr">
<th id="A6.T9.1.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="A6.T9.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A6.T9.1.1.2.2.2.1" class="ltx_text ltx_font_smallcaps">da</span></th>
<th id="A6.T9.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A6.T9.1.1.2.2.3.1" class="ltx_text ltx_font_smallcaps">mc (glove)</span></th>
<th id="A6.T9.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A6.T9.1.1.2.2.4.1" class="ltx_text ltx_font_smallcaps">da</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A6.T9.1.1.3.1" class="ltx_tr">
<th id="A6.T9.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="A6.T9.1.1.3.1.1.1" class="ltx_text ltx_font_smallcaps">Best</span></th>
<td id="A6.T9.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">80.94</td>
<td id="A6.T9.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">80.74</td>
<td id="A6.T9.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">66.20</td>
</tr>
<tr id="A6.T9.1.1.4.2" class="ltx_tr">
<th id="A6.T9.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="A6.T9.1.1.4.2.1.1" class="ltx_text ltx_font_smallcaps">Average</span></th>
<td id="A6.T9.1.1.4.2.2" class="ltx_td ltx_align_center">54.98</td>
<td id="A6.T9.1.1.4.2.3" class="ltx_td ltx_align_center">71.53</td>
<td id="A6.T9.1.1.4.2.4" class="ltx_td ltx_align_center">57.29</td>
</tr>
<tr id="A6.T9.1.1.5.3" class="ltx_tr">
<th id="A6.T9.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="A6.T9.1.1.5.3.1.1" class="ltx_text ltx_font_smallcaps">Worst</span></th>
<td id="A6.T9.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_bb">16.37</td>
<td id="A6.T9.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_bb">59.35</td>
<td id="A6.T9.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_bb">41.46</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Human performance on OK-VQA and A-OKVQA measured from the ground truth answers.
</figcaption>
</figure>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Datasets Quality and Issues</h2>

<div id="A7.p1" class="ltx_para">
<p id="A7.p1.1" class="ltx_p">As mentioned in <a href="#S5.SS3" title="5.3 Error Analysis ‣ 5 Results and Discussion ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection 5.3</span></a>, during error analysis we found that many errors are due to the issue in the dataset itself.
Concretely, we observe the following issues in the existing datasets:
(1) wrong answers
(2) subjective or unanswerable questions
(3) typos or unclear expressions
(4) not requiring images or knowledge to answer the question as designed.</p>
</div>
<div id="A7.p2" class="ltx_para">
<p id="A7.p2.1" class="ltx_p">Furthermore, since the answer and explanation for a question in VCR are obtained from the same person who authored the question, this may result in severe subjectivity in the answers or explanations.
For example, we find that many questions in VCR require knowledge of the <span id="A7.p2.1.1" class="ltx_text ltx_font_italic">movie plot</span> from which the image is extracted, rather than <span id="A7.p2.1.2" class="ltx_text ltx_font_italic">commonsense reasoning</span> to answer the questions.
While human annotators have an implicit understanding of the movies, the dataset itself does not contain relevant contextual information.</p>
</div>
<div id="A7.p3" class="ltx_para">
<p id="A7.p3.1" class="ltx_p">We show some of the issues in the datasets below.
<a href="#A7.F6" title="Figure 6 ‣ Appendix G Datasets Quality and Issues ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 6</span></a> shows examples from VCR that require an understanding of the movie plot to generate answers.
<a href="#A7.F7" title="Figure 7 ‣ Appendix G Datasets Quality and Issues ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 7</span></a> shows examples from OK-VQA where questions and answers are subjective or ambiguous.
<a href="#A7.F8" title="Figure 8 ‣ Appendix G Datasets Quality and Issues ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 8</span></a> shows examples from A-OKVQA and VQA-X that either contain wrong answers, questions that do not need visual input or typos which severely impact the model generation (“house” should be “horse”).</p>
</div>
<div id="A7.p4" class="ltx_para">
<p id="A7.p4.1" class="ltx_p">To understand the inter-annotator agreement for the datasets, we further measure the best, average and worst human performance on OK-VQA and A-OKVQA by selecting the most common answer, a random answer, and the least common answer, respectively, from the 10 ground truth answers for each question.
We calculate the performance using the VQA metric for direct answers, and the GloVe metric for Multiple Choice for simplicity.
Note that we also remove the answer selected from the ground truth answers when measuring human performance.
From the results in <a href="#A6.T9" title="Table 9 ‣ Appendix F Joint Generation Performance ‣ Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 9</span></a> we can see that the average performance on both datasets is relatively poor, which indicates the noise in the datasets.
The quality of the datasets needs to be more carefully inspected so that the model performance evaluated on these datasets can be more meaningful.</p>
</div>
<figure id="A7.F6" class="ltx_figure"><img src="/html/2301.10799/assets/x6.png" id="A7.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="392" height="164" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Questions that require knowledge of the movie plots to generate the answers from VCR.</figcaption>
</figure>
<figure id="A7.F7" class="ltx_figure"><img src="/html/2301.10799/assets/x7.png" id="A7.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="392" height="110" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Examples of subjective questions from OK-VQA.</figcaption>
</figure>
<figure id="A7.F8" class="ltx_figure"><img src="/html/2301.10799/assets/x8.png" id="A7.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="392" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Issues in the datasets that severely impact the model generation: wrong answers (left, from A-OKVQA), questions do not need visual input to answer (middle, from A-OKVQA), and typo (right, from VQA-X).</figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2301.10798" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2301.10799" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2301.10799">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2301.10799" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2301.10800" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 05:39:32 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
