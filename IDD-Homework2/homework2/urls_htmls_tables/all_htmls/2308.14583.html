<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2308.14583] Learning to Read Analog Gauges from Synthetic Data</title><meta property="og:description" content="Manually reading and logging gauge data is time-inefficient, and the effort increases according to the number of gauges available. We present a computer vision pipeline that automates the reading of analog gauges. We p…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Learning to Read Analog Gauges from Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Learning to Read Analog Gauges from Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2308.14583">

<!--Generated on Wed Feb 28 10:29:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Learning to Read Analog Gauges from Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Juan Leon-Alcazar
<br class="ltx_break">KAUST
<br class="ltx_break">Thuwal, Saudi Arabia
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">juancarlo.alcazar@kaust.edu.sa</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yazeed Alnumay
<br class="ltx_break">Aramco
<br class="ltx_break">Thuwal, Saudi Arabia
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">yazeed.alnumay@aramco.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Cheng Zheng
<br class="ltx_break">KAUST
<br class="ltx_break">Thuwal, Saudi Arabia
<br class="ltx_break"><span id="id4.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">cheng.zheng@kaust.edu.sa</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hassane Trigui
<br class="ltx_break">Aramco
<br class="ltx_break">Thuwal, Saudi Arabia
<br class="ltx_break"><span id="id5.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">hassane.trigui@aramco.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sahejad Patel
<br class="ltx_break">Aramco
<br class="ltx_break">Thuwal, Saudi Arabia
<br class="ltx_break"><span id="id6.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">sahejad.patel@aramco.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bernard Ghanem
<br class="ltx_break">KAUST
<br class="ltx_break">Thuwal, Saudi Arabia
<br class="ltx_break"><span id="id7.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">bernard.ghanem@kaust.edu.sa</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.1" class="ltx_p">Manually reading and logging gauge data is time-inefficient, and the effort increases according to the number of gauges available. We present a computer vision pipeline that automates the reading of analog gauges. We propose a two-stage CNN pipeline that identifies the key structural components of an analog gauge and outputs an angular reading. To facilitate the training of our approach, a synthetic dataset is generated thus obtaining a set of realistic analog gauges with their corresponding annotation. To validate our proposal, an additional real-world dataset was collected with 4.813 manually curated images. When compared against state-of-the-art methodologies, our method shows a significant improvement of 4.55<sup id="id1.1.1" class="ltx_sup"><span id="id1.1.1.1" class="ltx_text ltx_font_italic">∘</span></sup> in the average error, which is a 52% relative improvement. The resources for this project will be made available at: <a target="_blank" href="https://github.com/fuankarion/automatic-gauge-reading" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/fuankarion/automatic-gauge-reading</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Analog gauges are widespread across modern industrial facilities. Typically, these gauges monitor critical operational parameters such as temperature, pressure, and level indicators for active industrial processes. Therefore, keeping an accurate record of gauge reading data is essential to track an asset’s trends and its conditions over time, which would aid in failure investigation if one occurs.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Manually reading and logging gauge data is time-inefficient, and increases according to the number of gauges available. To approach this problem, digital transmitters allow the remote monitoring of gauges. However, in many cases, the facilities are aged, or it is prohibitively expensive to upgrade numerous analog gauges to digital ones. Additionally, on-site readings of gauges serve as a direct verification means for validating the accuracy of digital transmitters, thus ensuring redundancy and process safety.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2308.14583/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_img_landscape" width="438" height="217" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.6.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.7.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Sample Results.<span id="S1.F1.7.2.1" class="ltx_text ltx_font_medium"> The top row shows samples of gauges captured on the field. The bottom row displays our model’s predicted segmentation and regression of landmarks in these images. The segmentation result has three classes: background (not highlighted for easier visualization), gauge (cyan), and needle pointer (magenta). The gauge overlay also contains circles which visualize the estimated locations of the <span id="S1.F1.7.2.1.1" class="ltx_text" style="color:#70AD47;">• start marker</span>, <span id="S1.F1.7.2.1.2" class="ltx_text" style="color:#FFBF00;">• needle pointer</span>, and <span id="S1.F1.7.2.1.3" class="ltx_text" style="color:#BF0000;">• end marker</span>.</span></span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Since on-site readings can not be completely removed from the verification pipeline, some research efforts have explored the automatic reading of gauges from image data or video data captured directly in the facility <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. These studies are largely limited by the availability of datasets with real-world data and curated ground-truth annotations. Unlike most modern machine learning datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, analog gauge reading data is still small and insufficient to train deep convolutional models. This data deficiency can be explained as it is expensive to collect and manually label a large number of real-world gauges. Moreover, privacy and security issues arise if these gauges are actively in use in an industrial facility.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we approach the automatic analog gauge reading task and formulate a computer vision pipeline that enables the direct reading of analog gauges by processing a single picture of the corresponding device. Thereby providing automated readings which facilitates the digitization process, and simplifies the operator intervention. Our approach focuses on generating synthetic gauge images as a surrogate training set, and formulating a training time strategy that enables generalization from synthetic training data into real-world gauges. <a href="#S1.F1" title="In 1 Introduction ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a> contains some sample results from our method applied to real-world gauges.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We construct a synthetic gauge dataset by leveraging 3D modeling software. A single 3D model allows us to render images of various types of gauges from different view angles and simulates common noise sources. While synthetic gauge modeling mitigates the scarcity of training data, it poses a new challenge as it creates a domain gap between the rendered data (training) and the real-world examples (validation). With careful image augmentation, this domain gap can be effectively reduced, resulting in a model that can successfully operate in real-world scenarios. We validate the performance of our proposal on two real-world datasets containing over 7.500 labeled images.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our paper brings 3 contributions. i) We develop a Blender model that simulates analog gauges and can render gauge images that contain the spatial location of the gauge’s main components, and its reading. ii) We propose a two-stage (attention and classification) pipeline that identifies the key structural elements of the gauge and outputs a final reading. Paired with our training data this two-step approach produces state-of-the-art results. iii) We create a real-world dataset with 4,813 gauge readings which are manually curated and include multiple capture conditions and noise sources.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Automatic analog gauge reading is a machine vision task that involves object detection and landmark regression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Despite having a well-defined structure and goal, its solution is non-trivial. Therefore, multiple research efforts have addressed this problem. Overall, these approaches can be split into two main categories: classical computer vision and Deep Learning methods.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Most of the traditional computer vision methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> rely on the shape of the gauge (typically circular), and apply the Hough Transform <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> to estimate an initial gauge location. Upon this initial step, most of the approaches follow a secondary detection step which aims at establishing the location of key landmarks inside the gauge, commonly the pointer needle or the start and end markers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. The work of Bao <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> uses adaptive thresholding to enable a second Hough transform step that approximates the location of the pointer needle. The approach of Yi <span id="S2.p2.1.2" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.2.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib34" title="" class="ltx_ref">34</a><span id="S2.p2.1.2.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> proposed to use K-means clustering to identify the gauge elements, a similar method is outlined in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> where PCA and clustering are used to estimate the needle location on a binary image. The needle location step has also been approached by template matching and hand-crafted feature extraction (SIFT and RANSAC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Finally, some works have analyzed this task in video streams <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and rely on the Optical Flow for temporal stability on the predictions.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Following the success of Deep Learning methods for natural image processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, recent approaches have started training deep models that can directly regress the readings on a gauge. Some implementations rely on Deep Networks only for the detection procedure, commonly a CNN tuned for gauge detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, and apply a Neural Network trained for Optical Character Recognition in order to parse the scale values <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Currently, most of these Deep Learning approaches suffer from a lack of training data and can easily overfit the visual appearance of the gauges in the training set<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. This lack of generalization also holds for deep models that were trained on hybrid data, where the pointer needles of real gauges are artificially rotated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. Recently, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> devised a four-keypoint detection model to regress the minimum and maximum markers and the needle center and tip locations. Their model was trained on a large fully synthetic dataset and showed improved generalization to real images.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">In contrast to the current approaches, we devise a simple, yet effective deep-learning pipeline with only two-trainable stages. We show that this strategy can be trained exclusively on synthetic data and achieve state-of-the-art results on real-world datasets.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Automatic Gauge Reading</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2308.14583/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.4.2" class="ltx_text" style="font-size:90%;"> <span id="S3.F2.4.2.1" class="ltx_text ltx_font_bold">Synthetic Gauge Dataset.</span> We use the 3D creation suite Blender to generate diverse synthetic training samples. We design the rendered samples to include common artifacts and sources of noise, which makes the automatic gauge reading task more challenging. For example, we simulate light reflections on the gauge crystal and case (first row). We can include different patterns of dust/sand coverage over the crystal (second row). We also control for the presence of the needle’s shadow, which can generate alignment errors in the landmark detection task (third row).</span></figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section we outline the 3 stages of our approach: i) synthetic training data, ii) gauge component location, and iii) gauge landmark reading. The first component generates surrogate training data, by rendering synthetic gauges from a 3D model. The last two components are instantiated as convolutional neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. These networks aim at estimating a fine-grained label of all the gauge components, and then approximate the location of 3 key landmarks in the gauge: start marker, end marker, and the pointer needle’s tip.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Synthetic Training Data</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We decide against collecting training samples from gauges and manually labeling their readings. Such a process is expensive and slow and can be difficult to implement given the security constraints in some industrial facilities. Additionally, after gathering and labeling a large amount of data, we would still have to assure that it contains visually diverse gauges, and includes multiple capture conditions. Such diversity would mitigate the risk of overfitting to the global visual appearance of the collected gauges, instead of approximating the key landmarks on it.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">For the training phase, we rely on synthetic data that approximates the visual appearance of an analog gauge. We name this data split the <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">synthetic dataset</span>. We approach this task with the open-source 3D creation suite Blender <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. In Blender, we design a 3D model of a circular gauge which allows us to manipulate key elements of its appearance like needle type, case style, sticker min and max values, size, tint on the front crystal, and simulated dust particles amongst many others. <a href="#S3.F2" title="In 3 Automatic Gauge Reading ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> contains some samples of the synthetic dataset.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The flexibility of Blender allows us to freely mix and manipulate all the model elements in order to create multiple unique synthetic gauges. Moreover, we can adjust the camera parameters and lighting patterns while rendering an arbitrary view of the synthetic gauge. Finally, we can also control the needle orientation, which allows us to simulate multiple ground-truth values. This flexibility enables us to train convolutional networks in a fully supervised manner, since the ground-truth location of all the gauge components and the landmark angular locations are generated along with the rendered training data.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">We build a Blender script to randomize all gauge attributes and render the training dataset. The resulting synthetic dataset contains 12,000 images. Along with the rendered image, the script generates the corresponding ground-truth segmentation mask with 3 classes (background, gauge, needle), the angular location of the needle’s tip, and the maximum and minimum markers of the gauge sticker.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Gauge Detection and Reading</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Our approach for automatic gauge reading is a two-step classification pipeline. We first establish an approximate location of the most relevant gauge physical components, this estimation is then used as a spatial guidance to a second stage that effectively estimates the location of the key landmarks in the gauge, thus enabling a final angular reading.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2308.14583/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="426" height="185" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Method Overview.<span id="S3.F3.4.2.1" class="ltx_text ltx_font_medium"> We generate synthetic training data with diverse appearances and levels of noise from the Blender component (a). Additionally, we render the ground-truth semantic segmentation and output the location of the gauge key-points For all the synthetic data (b). At training time, we first optimize a segmentation network (c) that provides a localization prior for the gauge components. Then we use a concatenated tensor of the synthetic image and its estimated segmentation mask (f) to train a reading network that approximates the angular location of the pointer needle and the sticker start and end points (d). The angular locations of these points along with the known gauge unit and range are used to determine the gauge’s reading (g, h).</span></span></figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">In practice, the first step is approached by performing semantic segmentation on the gauge images, we estimate a pixel-wise mask of the gauge, thus partitioning it into 3 classes: Gauge Case, Needle, and Background. The second step is approached as a classification problem, where we establish the angular location of three key-points in the gauge: start marker, end marker, and pointer needle location. The combination of these 3 key-points provides an estimation of the position of the needle relative to the start and end of the gauge scale.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Segmentation Network</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">In the location step, we label every individual pixel in the input image. As a result, we obtain an approximate spatial location of the gauge in the picture (given by the gauge case class), and an initial location of the most relevant landmark in the process, namely the pixels assigned to the needle class. We approach the segmentation step using DeepLabv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, with a ResNet-18 backbone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. At training time, we rely exclusively on the synthetic data and the associated ground-truth.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Reading Network</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">In the reading step our main goal is to localize 3 landmarks: the start marker, the end marker, and the needle tip (see the blue dots in <a href="#S3.F3" title="In 3.2 Gauge Detection and Reading ‣ 3 Automatic Gauge Reading ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a> (g)). The ground-truth for these targets is provided in the synthetic dataset as angles between 0 to 359. We parameterize all three labels as the angle created between the respective gauge landmark and a horizontal line in the middle of the image, which we define as a 0<math id="S3.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\degree" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.1a"><mi mathvariant="normal" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">°</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1">°</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.1c">\degree</annotation></semantics></math> point (see the dashed pink lines <a href="#S3.F3" title="In 3.2 Gauge Detection and Reading ‣ 3 Automatic Gauge Reading ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a> (g)).</p>
</div>
<div id="S3.SS2.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p2.1" class="ltx_p">We jointly model all these prediction targets with a single convolutional backbone augmented with three independent prediction heads (one head per prediction target). Although this task could be approached as a regression problem (<span id="S3.SS2.SSS0.Px2.p2.1.1" class="ltx_text ltx_font_italic">i.e</span> predict a number between 0 to 359), we favor the empirical stability of the gradients created by the cross-entropy loss and approach this problem as a classification task (<span id="S3.SS2.SSS0.Px2.p2.1.2" class="ltx_text ltx_font_italic">i.e</span> select a class between 0 to 359). Therefore, we quantize the ground-truth values into 360 bins (one bin representing one degree) and optimize the task as a classification problem. To this end, we chose the MobilnetV2<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> or EfficientNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> as the backbones and complement each encoder with three linear heads, each head has an output size of <math id="S3.SS2.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="1\times 360" display="inline"><semantics id="S3.SS2.SSS0.Px2.p2.1.m1.1a"><mrow id="S3.SS2.SSS0.Px2.p2.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.cmml"><mn id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.1" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml">360</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.1.m1.1b"><apply id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1"><times id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2">1</cn><cn type="integer" id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3">360</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.1.m1.1c">1\times 360</annotation></semantics></math>.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2308.14583/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Landmark Detection Dataset.<span id="S3.F4.4.2.1" class="ltx_text ltx_font_medium"> We show some samples of the Landmark detection dataset, each image has an associated ground truth that contains the needle angle, and the location of the start and end of the sticker. We create this dataset using a mechanical arm where we control the relative position of the gauge and the camera, along with the location of the pointer needle. By synchronizing the motion of the arm and the servomotors with the capture device, we can obtain an exact ground truth reading for every frame.</span></span></figcaption>
</figure>
<div id="S3.SS2.SSS0.Px2.p3" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p3.1" class="ltx_p">At training time, we leverage the location prior provided by the segmentation step by appending the segmentation result to the input image in the channel dimension. This segmentation provides a form of visual guidance that allows us to direct the network’s attention toward the gauge instead of the background. We extend the filters on the first layer of each backbone to accept 4 channel inputs, the new filter data is initially set to the average of the original 3 channel filter. Additional training details are provided in the <span id="S3.SS2.SSS0.Px2.p3.1.1" class="ltx_text ltx_font_bold">supplementary material</span>.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Gauge Reading Datasets</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we outline our second dataset contribution, namely two real-world datasets that contain manually curated ground-truth labels. The first one is the <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">land-mark dataset</span> which contains labels for the location of the start and end markers, along with the pointer needle orientation on 4.813 real-world images. The landmark dataset serves as a held-out test-set to validate our proposed approach. The second dataset provides the semantic segmentation ground-truth for 59 real-world images, it serves as a validation set for the semantic segmentation step.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Landmark Regression Dataset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The landmark dataset consists of 5 video sequences captured using a GoPro camera and an analog gauge attached to the end of a Novint Falcon haptic controller, which was adapted to serve as a robotic arm. This mechanism enables us to control the location of the gauge in the Y-axis (up and down the camera plane) and Z-axis (away or close to the camera plane). This allows us to simulate the capture of gauge images from diverse points of view and varying distances. All the image data is captured and encoded as a video sequence at 720p (<math id="S4.SS1.p1.1.m1.1" class="ltx_math_unparsed" alttext="1280\times 720)" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1b"><mn id="S4.SS1.p1.1.m1.1.1">1280</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.1.m1.1.2">×</mo><mn id="S4.SS1.p1.1.m1.1.3">720</mn><mo stretchy="false" id="S4.SS1.p1.1.m1.1.4">)</mo></mrow><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">1280\times 720)</annotation></semantics></math> resolution.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">In addition to the capture viewpoint, the gauge is manipulated with two additional servomotors controlled by an Arduino Uno microcontroller. The first servomotor controls the orientation of the pointer needle allowing us to control the actual gauge reading. As the servomotor is not directly connected to the needle, but rather is attached to the internal mechanical structure of the gauge. The mapping of the servo motor rotation and the needle’s effective rotation is non-linear. To approximate this mapping, we sampled and manually recorded the amount of motor rotation input required for the needle to match a specific reading (tick marker) on the gauge’s face. Then we linearly interpolate the motor inputs to approximate intermediate angles. The second servomotor can rotate the gauge’s face relative to the camera plane (the gauge face can be oriented towards the left or right of the Field of View, which represents the yaw rotation).</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">To account for some real-world capture conditions and noise, 5 different environments were simulated. These scenarios include varying light sources and camera locations. In total, each scenario contains between 960 to 966 images. A few samples from the landmark dataset in each of the 5 proposed scenarios are presented in <a href="#S3.F4" title="In Reading Network ‣ 3.2 Gauge Detection and Reading ‣ 3 Automatic Gauge Reading ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Natural Lighting.</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">Serves as the baseline scenario without any complex conditions. We arrange some artificial lighting that approximates an outdoor setup by daylight and locate the camera close to the gauge (30 cm away). As a result, the case of the gauge occupies on average 30% of the image’s area. Clearly, the gauge is the main object in the image, but there is some background clutter on the capture.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Far Camera.</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">Simulates the model’s performance when reading slightly more distant gauges. These images were captured 50 cm away from the gauge. On average, the gauge’s case occupies 10% of the image area, which means the global image features are dominated by the background.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Dim Light.</h4>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p">In this scenario we turn off the artificial lighting, such that the capture condition simulates the use of the gauge reading tool indoors, or with diminished sunlight. The reduced lighting paired with the relatively high (25 fps) video capture introduces some noticeable motion blur artifacts in this scenario, making the location of the landmarks more challenging.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Glass Reflection.</h4>

<div id="S4.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px4.p1.1" class="ltx_p">This scenario was captured with an open window allowing for direct sunlight towards the front of the gauge, additional objects were added to the scene to create reflections that could alter the appearance of the gauge.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Needle Shadow.</h4>

<div id="S4.SS1.SSS0.Px5.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px5.p1.1" class="ltx_p">For this scenario, we set the artificial light source much closer to the gauge and arrange the lightning direction at an angle (never fully frontal). This setup creates sharp shadows in the gauge sticker, mainly from the gauge case and the needle. These conditions often lead to erroneous estimations, as automatic reading models tend to miss-classify the needle’s shadow as the actual needle.</p>
</div>
<div id="S4.SS1.SSS0.Px5.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px5.p2.1" class="ltx_p">All the gauge readings in the landmark dataset are controlled by the servomotors and carefully synchronized with the video capture. Therefore, our landmark dataset offers a standard benchmark with accurate ground truth labels that enables the validation (and potentially training) of automatic gauge reading tools. Overall, the landmark dataset presents a diverse set of possible failure cases to test against, although it has a bias regarding the gauge type as the same gauge was used for all 5 capture scenarios.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2308.14583/assets/figures/RealSegmentedVisualization.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="86" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Segmented Gauge Dataset.<span id="S4.F5.4.2.1" class="ltx_text ltx_font_medium"> We provide a dataset with 59 manually segmented real-world gauges. This dataset includes multiple indoor and outdoor gauges and supports the validation of the attention step in our pipeline. In this figure the Gauge pixels are highlighted in Bright cyan, while the pointer needle pixels are shown in magenta, we do not highlight the background pixels for easier visualization.</span></span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Segmented Gauge Dataset</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Our proposed approach relies on image segmentation of the background, gauge, and pointer needle. To validate the performance of our methods on this initial task, we manually segmented 59 real images. Of these 59 images, 43 are part of the Landmark Dataset, the remaining 16 were obtained from other gauges inside an industrial facility. The label distribution in this dataset is highly imbalanced, with the background, needle, and gauge classes representing 66.26%, 1.03%, and 32.71% of the pixels, respectively. We provide this dataset as the segmentation of gauges is a key step in our approach. In addition, it will also support future automatic reading methods that build upon a location step. We visualize some samples of the segmentations contained in this dataset in <a href="#S4.F5" title="In Needle Shadow. ‣ 4.1 Landmark Regression Dataset ‣ 4 Gauge Reading Datasets ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Analysis</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We now perform the empirical evaluation of our method, we begin with a direct comparison against the state-of-the-art in the test-set proposed by Howells <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Then, we provide the evaluation of our approach in the proposed Landmark dataset. We conclude by assessing the effectiveness of the attention component using our segmentation dataset.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Comparison against the State-of-the-art</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The work of Howells <em id="S5.SS1.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S5.SS1.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> provides its own real-world angle reading dataset. We run our proposed approach in their test-set and perform a direct comparison. Their test-set consists of 6 visually distinct gauges, for each individual gauge there are 3 video clips containing a total of 450 frames (150 frames per video clip, 2.700 images total). Each clip shows a static capture (i.e. no camera or gauge motion, no illumination changes) of the gauge where only the pointer needle changes its orientation. The associated angle ground-truth is available for every clip.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The gauge reading dataset in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> was deliberately collected with small gauges, occupying about 10-15% of the image, such that reading pipelines first crop out the target gauge and then regress the needle’s location. Since the semantic segmentation step in our pipeline works as an attention mechanism rather than a detection step, we use OpenCV’s implementation of the Hough Transform circle detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> to extract an initial crop of the image. We keep the default settings for the algorithm and directly apply our method to the cropped detections. <a href="#S5.T1" title="In 5.1 Comparison against the State-of-the-art ‣ 5 Results and Analysis ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a> summarizes the results of our approach in the test-set of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T1.2.1" class="ltx_tr">
<td id="S5.T1.2.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="S5.T1.2.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S5.T1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S5.T1.2.1.2.1" class="ltx_text ltx_font_bold">Ours</span></td>
<td id="S5.T1.2.1.3" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="S5.T1.2.2" class="ltx_tr">
<td id="S5.T1.2.2.1" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T1.2.2.1.1" class="ltx_text ltx_font_bold">Small</span></td>
<td id="S5.T1.2.2.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T1.2.2.2.1" class="ltx_text ltx_font_bold">Large</span></td>
<td id="S5.T1.2.2.3" class="ltx_td ltx_align_right"><span id="S5.T1.2.2.3.1" class="ltx_text ltx_font_bold">Howells <em id="S5.T1.2.2.3.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S5.T1.2.2.3.1.2" class="ltx_text"></span></span></td>
</tr>
<tr id="S5.T1.2.3" class="ltx_tr">
<td id="S5.T1.2.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T1.2.3.1.1" class="ltx_text" style="color:#BFBFBF;">meter_a</span></td>
<td id="S5.T1.2.3.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T1.2.3.2.1" class="ltx_text" style="color:#BFBFBF;">19.33</span></td>
<td id="S5.T1.2.3.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T1.2.3.3.1" class="ltx_text" style="color:#BFBFBF;">16.62</span></td>
<td id="S5.T1.2.3.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T1.2.3.4.1" class="ltx_text" style="color:#BFBFBF;"><span id="S5.T1.2.3.4.1.1" class="ltx_text ltx_font_bold">9.24</span></span></td>
</tr>
<tr id="S5.T1.2.4" class="ltx_tr">
<td id="S5.T1.2.4.1" class="ltx_td ltx_align_left">meter_a (rev.)</td>
<td id="S5.T1.2.4.2" class="ltx_td ltx_align_right">5.69</td>
<td id="S5.T1.2.4.3" class="ltx_td ltx_align_right"><span id="S5.T1.2.4.3.1" class="ltx_text ltx_font_bold">2.50</span></td>
<td id="S5.T1.2.4.4" class="ltx_td ltx_align_right">—</td>
</tr>
<tr id="S5.T1.2.5" class="ltx_tr">
<td id="S5.T1.2.5.1" class="ltx_td ltx_align_left">meter_b</td>
<td id="S5.T1.2.5.2" class="ltx_td ltx_align_right"><span id="S5.T1.2.5.2.1" class="ltx_text ltx_font_bold">8.23</span></td>
<td id="S5.T1.2.5.3" class="ltx_td ltx_align_right">8.29</td>
<td id="S5.T1.2.5.4" class="ltx_td ltx_align_right">26.06</td>
</tr>
<tr id="S5.T1.2.6" class="ltx_tr">
<td id="S5.T1.2.6.1" class="ltx_td ltx_align_left">meter_c</td>
<td id="S5.T1.2.6.2" class="ltx_td ltx_align_right">2.74</td>
<td id="S5.T1.2.6.3" class="ltx_td ltx_align_right">1.97</td>
<td id="S5.T1.2.6.4" class="ltx_td ltx_align_right"><span id="S5.T1.2.6.4.1" class="ltx_text ltx_font_bold">1.94</span></td>
</tr>
<tr id="S5.T1.2.7" class="ltx_tr">
<td id="S5.T1.2.7.1" class="ltx_td ltx_align_left">meter_d</td>
<td id="S5.T1.2.7.2" class="ltx_td ltx_align_right">7.68</td>
<td id="S5.T1.2.7.3" class="ltx_td ltx_align_right"><span id="S5.T1.2.7.3.1" class="ltx_text ltx_font_bold">7.36</span></td>
<td id="S5.T1.2.7.4" class="ltx_td ltx_align_right">11.70</td>
</tr>
<tr id="S5.T1.2.8" class="ltx_tr">
<td id="S5.T1.2.8.1" class="ltx_td ltx_align_left">meter_e</td>
<td id="S5.T1.2.8.2" class="ltx_td ltx_align_right">4.32</td>
<td id="S5.T1.2.8.3" class="ltx_td ltx_align_right">4.83</td>
<td id="S5.T1.2.8.4" class="ltx_td ltx_align_right"><span id="S5.T1.2.8.4.1" class="ltx_text ltx_font_bold">3.70</span></td>
</tr>
<tr id="S5.T1.2.9" class="ltx_tr">
<td id="S5.T1.2.9.1" class="ltx_td ltx_align_left">meter_f</td>
<td id="S5.T1.2.9.2" class="ltx_td ltx_align_right">4.64</td>
<td id="S5.T1.2.9.3" class="ltx_td ltx_align_right">4.69</td>
<td id="S5.T1.2.9.4" class="ltx_td ltx_align_right"><span id="S5.T1.2.9.4.1" class="ltx_text ltx_font_bold">4.31</span></td>
</tr>
<tr id="S5.T1.2.10" class="ltx_tr">
<td id="S5.T1.2.10.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T1.2.10.1.1" class="ltx_text ltx_font_bold">Average</span></td>
<td id="S5.T1.2.10.2" class="ltx_td ltx_align_right ltx_border_t">7.82</td>
<td id="S5.T1.2.10.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T1.2.10.3.1" class="ltx_text ltx_font_bold">7.30</span></td>
<td id="S5.T1.2.10.4" class="ltx_td ltx_align_right ltx_border_t">9.49</td>
</tr>
<tr id="S5.T1.2.11" class="ltx_tr">
<td id="S5.T1.2.11.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T1.2.11.1.1" class="ltx_text ltx_font_bold">Average (rev.)</span></td>
<td id="S5.T1.2.11.2" class="ltx_td ltx_align_right ltx_border_bb">5.55</td>
<td id="S5.T1.2.11.3" class="ltx_td ltx_align_right ltx_border_bb"><span id="S5.T1.2.11.3.1" class="ltx_text ltx_font_bold">4.94</span></td>
<td id="S5.T1.2.11.4" class="ltx_td ltx_align_right ltx_border_bb">—</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.4.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S5.T1.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">State-of-the-art Comparison.<span id="S5.T1.5.2.1" class="ltx_text ltx_font_medium"> We test our pipeline in the test-set of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. We report the mean absolute error of the predicted gauge angle readings in degrees. Our small model uses MobileNetV2, which is the same model used by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. The large model uses the EfficientNet-B2 backbone.</span></span></figcaption>
</figure>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.6" class="ltx_p">Overall, we observe that our proposal outperforms the state-of-the-art by at least 1.67<sup id="S5.SS1.p3.6.1" class="ltx_sup"><span id="S5.SS1.p3.6.1.1" class="ltx_text ltx_font_italic">∘</span></sup>, which represents a relative improvement of 17%. Although <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> outperforms our method in 3 sequences (c, e, f), we observe that the results are nearly identical in those scenarios as their absolute differences are 0.03<sup id="S5.SS1.p3.6.2" class="ltx_sup"><span id="S5.SS1.p3.6.2.1" class="ltx_text ltx_font_italic">∘</span></sup>, 0.62<sup id="S5.SS1.p3.6.3" class="ltx_sup"><span id="S5.SS1.p3.6.3.1" class="ltx_text ltx_font_italic">∘</span></sup>, and 0.33<sup id="S5.SS1.p3.6.4" class="ltx_sup"><span id="S5.SS1.p3.6.4.1" class="ltx_text ltx_font_italic">∘</span></sup> respectively. Meanwhile, our method largely outperforms their baseline in the more challenging sequences (b and d), reducing the error by 17.83<sup id="S5.SS1.p3.6.5" class="ltx_sup"><span id="S5.SS1.p3.6.5.1" class="ltx_text ltx_font_italic">∘</span></sup> (68.4% relative improvement) in sequence b, and 4.34<sup id="S5.SS1.p3.6.6" class="ltx_sup"><span id="S5.SS1.p3.6.6.1" class="ltx_text ltx_font_italic">∘</span></sup> (37.1% relative error reduction).</p>
</div>
<section id="S5.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Ground-truth and Predictions Design</h4>

<div id="S5.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px1.p1.4" class="ltx_p">We note that the test-set of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> follows a different approach to our training set. While we use every angle from 0<sup id="S5.SS1.SSS0.Px1.p1.4.1" class="ltx_sup"><span id="S5.SS1.SSS0.Px1.p1.4.1.1" class="ltx_text ltx_font_italic">∘</span></sup> to 360<sup id="S5.SS1.SSS0.Px1.p1.4.2" class="ltx_sup"><span id="S5.SS1.SSS0.Px1.p1.4.2.1" class="ltx_text ltx_font_italic">∘</span></sup> in our labels and prediction heads, Howells <em id="S5.SS1.SSS0.Px1.p1.4.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S5.SS1.SSS0.Px1.p1.4.4" class="ltx_text"></span> use labels and predictions between 0<sup id="S5.SS1.SSS0.Px1.p1.4.5" class="ltx_sup"><span id="S5.SS1.SSS0.Px1.p1.4.5.1" class="ltx_text ltx_font_italic">∘</span></sup> to 180<sup id="S5.SS1.SSS0.Px1.p1.4.6" class="ltx_sup"><span id="S5.SS1.SSS0.Px1.p1.4.6.1" class="ltx_text ltx_font_italic">∘</span></sup>. Their angle prediction formula from four key-points always takes the smallest of the two possible angles between the start marker and needle tip. A visualization of this discrepancy is presented in <a href="#S5.F6" title="In Ground-truth and Predictions Design ‣ 5.1 Comparison against the State-of-the-art ‣ 5 Results and Analysis ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2308.14583/assets/x5.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="460" height="227" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.6.2.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Benchmark Ground-Truth Discrepancies.<span id="S5.F6.2.1.1" class="ltx_text ltx_font_medium"> The angle extraction method used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> extracts the explement angle between the start marker and needle tip when the angle exceeds 180<sup id="S5.F6.2.1.1.1" class="ltx_sup"><span id="S5.F6.2.1.1.1.1" class="ltx_text ltx_font_italic">∘</span></sup>. The left figure shows angles for which both methods succeed. The right figure shows an apparent failure case of our proposed model, given the design of the ground-truth labels.</span></span></figcaption>
</figure>
<div id="S5.SS1.SSS0.Px1.p2" class="ltx_para">
<p id="S5.SS1.SSS0.Px1.p2.6" class="ltx_p">Our model’s classification head with angles in the range <math id="S5.SS1.SSS0.Px1.p2.1.m1.1" class="ltx_math_unparsed" alttext="[0^{\circ}" display="inline"><semantics id="S5.SS1.SSS0.Px1.p2.1.m1.1a"><mrow id="S5.SS1.SSS0.Px1.p2.1.m1.1b"><mo stretchy="false" id="S5.SS1.SSS0.Px1.p2.1.m1.1.1">[</mo><msup id="S5.SS1.SSS0.Px1.p2.1.m1.1.2"><mn id="S5.SS1.SSS0.Px1.p2.1.m1.1.2.2">0</mn><mo id="S5.SS1.SSS0.Px1.p2.1.m1.1.2.3">∘</mo></msup></mrow><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p2.1.m1.1c">[0^{\circ}</annotation></semantics></math>, 360<sup id="S5.SS1.SSS0.Px1.p2.6.1" class="ltx_sup"><span id="S5.SS1.SSS0.Px1.p2.6.1.1" class="ltx_text ltx_font_italic">∘</span></sup> ) overcomes the aforementioned limitation. Moreover, the same four key-point method proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> can be modified to incorporate the sign of the 2D cross-product of the central edges <math id="S5.SS1.SSS0.Px1.p2.3.m3.1" class="ltx_Math" alttext="\overrightarrow{CS}\times\overrightarrow{CT}" display="inline"><semantics id="S5.SS1.SSS0.Px1.p2.3.m3.1a"><mrow id="S5.SS1.SSS0.Px1.p2.3.m3.1.1" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.cmml"><mover accent="true" id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.cmml"><mrow id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.2" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.2.cmml"><mi id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.2.2" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.2.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.2.1" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.2.1.cmml">​</mo><mi id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.2.3" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.2.3.cmml">S</mi></mrow><mo stretchy="false" id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.1" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.1.cmml">→</mo></mover><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.1" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.1.cmml">×</mo><mover accent="true" id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.cmml"><mrow id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.2" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.2.cmml"><mi id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.2.2" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.2.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.2.1" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.2.1.cmml">​</mo><mi id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.2.3" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.2.3.cmml">T</mi></mrow><mo stretchy="false" id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.1" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.1.cmml">→</mo></mover></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p2.3.m3.1b"><apply id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.cmml" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1"><times id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.1"></times><apply id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.cmml" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2"><ci id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.1.cmml" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.1">→</ci><apply id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.2.cmml" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.2"><times id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.2.1.cmml" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.2.1"></times><ci id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.2.2.cmml" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.2.2">𝐶</ci><ci id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.2.3.cmml" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.2.2.3">𝑆</ci></apply></apply><apply id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.cmml" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3"><ci id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.1.cmml" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.1">→</ci><apply id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.2.cmml" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.2"><times id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.2.1.cmml" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.2.1"></times><ci id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.2.2.cmml" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.2.2">𝐶</ci><ci id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.2.3.cmml" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.3.2.3">𝑇</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p2.3.m3.1c">\overrightarrow{CS}\times\overrightarrow{CT}</annotation></semantics></math> to determine if the angle <math id="S5.SS1.SSS0.Px1.p2.4.m4.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S5.SS1.SSS0.Px1.p2.4.m4.1a"><mi id="S5.SS1.SSS0.Px1.p2.4.m4.1.1" xref="S5.SS1.SSS0.Px1.p2.4.m4.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p2.4.m4.1b"><ci id="S5.SS1.SSS0.Px1.p2.4.m4.1.1.cmml" xref="S5.SS1.SSS0.Px1.p2.4.m4.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p2.4.m4.1c">\theta</annotation></semantics></math> should be used or its explement (<math id="S5.SS1.SSS0.Px1.p2.5.m5.1" class="ltx_Math" alttext="360^{\circ}-\theta" display="inline"><semantics id="S5.SS1.SSS0.Px1.p2.5.m5.1a"><mrow id="S5.SS1.SSS0.Px1.p2.5.m5.1.1" xref="S5.SS1.SSS0.Px1.p2.5.m5.1.1.cmml"><msup id="S5.SS1.SSS0.Px1.p2.5.m5.1.1.2" xref="S5.SS1.SSS0.Px1.p2.5.m5.1.1.2.cmml"><mn id="S5.SS1.SSS0.Px1.p2.5.m5.1.1.2.2" xref="S5.SS1.SSS0.Px1.p2.5.m5.1.1.2.2.cmml">360</mn><mo id="S5.SS1.SSS0.Px1.p2.5.m5.1.1.2.3" xref="S5.SS1.SSS0.Px1.p2.5.m5.1.1.2.3.cmml">∘</mo></msup><mo id="S5.SS1.SSS0.Px1.p2.5.m5.1.1.1" xref="S5.SS1.SSS0.Px1.p2.5.m5.1.1.1.cmml">−</mo><mi id="S5.SS1.SSS0.Px1.p2.5.m5.1.1.3" xref="S5.SS1.SSS0.Px1.p2.5.m5.1.1.3.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p2.5.m5.1b"><apply id="S5.SS1.SSS0.Px1.p2.5.m5.1.1.cmml" xref="S5.SS1.SSS0.Px1.p2.5.m5.1.1"><minus id="S5.SS1.SSS0.Px1.p2.5.m5.1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p2.5.m5.1.1.1"></minus><apply id="S5.SS1.SSS0.Px1.p2.5.m5.1.1.2.cmml" xref="S5.SS1.SSS0.Px1.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.SSS0.Px1.p2.5.m5.1.1.2.1.cmml" xref="S5.SS1.SSS0.Px1.p2.5.m5.1.1.2">superscript</csymbol><cn type="integer" id="S5.SS1.SSS0.Px1.p2.5.m5.1.1.2.2.cmml" xref="S5.SS1.SSS0.Px1.p2.5.m5.1.1.2.2">360</cn><compose id="S5.SS1.SSS0.Px1.p2.5.m5.1.1.2.3.cmml" xref="S5.SS1.SSS0.Px1.p2.5.m5.1.1.2.3"></compose></apply><ci id="S5.SS1.SSS0.Px1.p2.5.m5.1.1.3.cmml" xref="S5.SS1.SSS0.Px1.p2.5.m5.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p2.5.m5.1c">360^{\circ}-\theta</annotation></semantics></math>). We correct for this discrepancy on every ground-truth angle that is above 180<sup id="S5.SS1.SSS0.Px1.p2.6.2" class="ltx_sup"><span id="S5.SS1.SSS0.Px1.p2.6.2.1" class="ltx_text ltx_font_italic">∘</span></sup>. Additional benchmark ground-truth discussion is provided in the <span id="S5.SS1.SSS0.Px1.p2.6.3" class="ltx_text ltx_font_bold">supplementary material</span>.</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Ground-truth Correction</h4>

<div id="S5.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px2.p1.1" class="ltx_p">In <a href="#S5.T1" title="In 5.1 Comparison against the State-of-the-art ‣ 5 Results and Analysis ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a> we include two different evaluations for meter_a, we identify that the sequence meter_a, contains flawed ground-truth data that does not match the actual reading of the gauge. We manually inspect the entire sequence and identify that this annotation error appears on 40 contiguous frames of the first video capture and on the entire 150 frames of the second. We detail the corrections made in the <span id="S5.SS1.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_bold">supplementary material</span>.</p>
</div>
<div id="S5.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S5.SS1.SSS0.Px2.p2.1" class="ltx_p">We create a revisited (rev) version of meter_a, with ground-truth values that match the needle location. For completeness, we report the performance of our method on both the full sequence and the revisited subset. We observe a significant gap when the revisited data (rev) is used. Furthermore, we also break down the average performance according to the selected sequence_a data (original or revisited). With the revisited data, our approach increases the performance gap against the state-of-the-art by up to 4.55<sup id="S5.SS1.SSS0.Px2.p2.1.1" class="ltx_sup"><span id="S5.SS1.SSS0.Px2.p2.1.1.1" class="ltx_text ltx_font_italic">∘</span></sup> in the average error (52% relative improvement). <a href="#S5.F7" title="In Ground-truth Correction ‣ 5.1 Comparison against the State-of-the-art ‣ 5 Results and Analysis ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a> shows our model’s performance on two videos of the meter_a, video 3 uses the original ground truth, while video2 uses our revisited version. In the <span id="S5.SS1.SSS0.Px2.p2.1.2" class="ltx_text ltx_font_bold">supplementary material</span>, we include a similar analysis for every video in the dataset.</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2308.14583/assets/x6.png" id="S5.F7.g1" class="ltx_graphics ltx_img_landscape" width="437" height="347" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S5.F7.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Benchmark Time Series Prediction Samples.<span id="S5.F7.4.2.1" class="ltx_text ltx_font_medium"> Our small (MobileNetV2) model’s performance on two sample videos from the benchmark dataset. Our model’s needle angle prediction closely matches the ground-truth. All the time series results are presented in the supplementary material.</span></span></figcaption>
</figure>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Landmark Dataset Evaluation</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We now assess the effectiveness of our pipeline in the landmark dataset. Unlike the test-set of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, we directly apply our method to the test data, <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">i.e.</span> we do not perform any additional detection step or image cropping. For this evaluation, we also test how our synthetic dataset enables the fine-tuning of models deeper than MobileNetV2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. To this end, we choose to fine-tune the EfficienNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, <a href="#S5.T2" title="In 5.2 Landmark Dataset Evaluation ‣ 5 Results and Analysis ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a> summarizes the results.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T2.2.1" class="ltx_tr">
<td id="S5.T2.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2"><span id="S5.T2.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Scenario</span></td>
<td id="S5.T2.2.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Mobile-</span></td>
<td id="S5.T2.2.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S5.T2.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">EfficientNet</span></td>
</tr>
<tr id="S5.T2.2.2" class="ltx_tr">
<td id="S5.T2.2.2.1" class="ltx_td ltx_align_center"><span id="S5.T2.2.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">NetV2</span></td>
<td id="S5.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">B0</span></td>
<td id="S5.T2.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">B1</span></td>
<td id="S5.T2.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">B2</span></td>
<td id="S5.T2.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.2.2.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">B3</span></td>
</tr>
<tr id="S5.T2.2.3" class="ltx_tr">
<td id="S5.T2.2.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.2.3.1.1" class="ltx_text" style="font-size:90%;">Natural Lighting</span></td>
<td id="S5.T2.2.3.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T2.2.3.2.1" class="ltx_text" style="font-size:90%;">2.83</span></td>
<td id="S5.T2.2.3.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T2.2.3.3.1" class="ltx_text" style="font-size:90%;">2.62</span></td>
<td id="S5.T2.2.3.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T2.2.3.4.1" class="ltx_text" style="font-size:90%;">2.59</span></td>
<td id="S5.T2.2.3.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T2.2.3.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">2.29</span></td>
<td id="S5.T2.2.3.6" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T2.2.3.6.1" class="ltx_text" style="font-size:90%;">2.31</span></td>
</tr>
<tr id="S5.T2.2.4" class="ltx_tr">
<td id="S5.T2.2.4.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S5.T2.2.4.1.1" class="ltx_text" style="font-size:90%;">Far Camera</span></td>
<td id="S5.T2.2.4.2" class="ltx_td ltx_align_right"><span id="S5.T2.2.4.2.1" class="ltx_text" style="font-size:90%;">5.15</span></td>
<td id="S5.T2.2.4.3" class="ltx_td ltx_align_right"><span id="S5.T2.2.4.3.1" class="ltx_text" style="font-size:90%;">2.81</span></td>
<td id="S5.T2.2.4.4" class="ltx_td ltx_align_right"><span id="S5.T2.2.4.4.1" class="ltx_text" style="font-size:90%;">2.93</span></td>
<td id="S5.T2.2.4.5" class="ltx_td ltx_align_right"><span id="S5.T2.2.4.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">2.48</span></td>
<td id="S5.T2.2.4.6" class="ltx_td ltx_align_right"><span id="S5.T2.2.4.6.1" class="ltx_text" style="font-size:90%;">2.52</span></td>
</tr>
<tr id="S5.T2.2.5" class="ltx_tr">
<td id="S5.T2.2.5.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S5.T2.2.5.1.1" class="ltx_text" style="font-size:90%;">Dim Light</span></td>
<td id="S5.T2.2.5.2" class="ltx_td ltx_align_right"><span id="S5.T2.2.5.2.1" class="ltx_text" style="font-size:90%;">3.62</span></td>
<td id="S5.T2.2.5.3" class="ltx_td ltx_align_right"><span id="S5.T2.2.5.3.1" class="ltx_text" style="font-size:90%;">5.50</span></td>
<td id="S5.T2.2.5.4" class="ltx_td ltx_align_right"><span id="S5.T2.2.5.4.1" class="ltx_text" style="font-size:90%;">4.56</span></td>
<td id="S5.T2.2.5.5" class="ltx_td ltx_align_right"><span id="S5.T2.2.5.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">3.20</span></td>
<td id="S5.T2.2.5.6" class="ltx_td ltx_align_right"><span id="S5.T2.2.5.6.1" class="ltx_text" style="font-size:90%;">4.10</span></td>
</tr>
<tr id="S5.T2.2.6" class="ltx_tr">
<td id="S5.T2.2.6.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S5.T2.2.6.1.1" class="ltx_text" style="font-size:90%;">Glass Reflection</span></td>
<td id="S5.T2.2.6.2" class="ltx_td ltx_align_right"><span id="S5.T2.2.6.2.1" class="ltx_text" style="font-size:90%;">2.88</span></td>
<td id="S5.T2.2.6.3" class="ltx_td ltx_align_right"><span id="S5.T2.2.6.3.1" class="ltx_text" style="font-size:90%;">2.60</span></td>
<td id="S5.T2.2.6.4" class="ltx_td ltx_align_right"><span id="S5.T2.2.6.4.1" class="ltx_text" style="font-size:90%;">2.64</span></td>
<td id="S5.T2.2.6.5" class="ltx_td ltx_align_right"><span id="S5.T2.2.6.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">2.37</span></td>
<td id="S5.T2.2.6.6" class="ltx_td ltx_align_right"><span id="S5.T2.2.6.6.1" class="ltx_text" style="font-size:90%;">2.44</span></td>
</tr>
<tr id="S5.T2.2.7" class="ltx_tr">
<td id="S5.T2.2.7.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S5.T2.2.7.1.1" class="ltx_text" style="font-size:90%;">Needle Shadow</span></td>
<td id="S5.T2.2.7.2" class="ltx_td ltx_align_right"><span id="S5.T2.2.7.2.1" class="ltx_text" style="font-size:90%;">3.15</span></td>
<td id="S5.T2.2.7.3" class="ltx_td ltx_align_right"><span id="S5.T2.2.7.3.1" class="ltx_text" style="font-size:90%;">3.36</span></td>
<td id="S5.T2.2.7.4" class="ltx_td ltx_align_right"><span id="S5.T2.2.7.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">2.85</span></td>
<td id="S5.T2.2.7.5" class="ltx_td ltx_align_right"><span id="S5.T2.2.7.5.1" class="ltx_text" style="font-size:90%;">3.11</span></td>
<td id="S5.T2.2.7.6" class="ltx_td ltx_align_right"><span id="S5.T2.2.7.6.1" class="ltx_text" style="font-size:90%;">3.14</span></td>
</tr>
<tr id="S5.T2.2.8" class="ltx_tr">
<td id="S5.T2.2.8.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t"><span id="S5.T2.2.8.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Average</span></td>
<td id="S5.T2.2.8.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S5.T2.2.8.2.1" class="ltx_text" style="font-size:90%;">3.53</span></td>
<td id="S5.T2.2.8.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S5.T2.2.8.3.1" class="ltx_text" style="font-size:90%;">3.38</span></td>
<td id="S5.T2.2.8.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S5.T2.2.8.4.1" class="ltx_text" style="font-size:90%;">3.11</span></td>
<td id="S5.T2.2.8.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S5.T2.2.8.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">2.69</span></td>
<td id="S5.T2.2.8.6" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S5.T2.2.8.6.1" class="ltx_text" style="font-size:90%;">2.90</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S5.T2.6.1" class="ltx_text ltx_font_bold">Performance on the Landmark Dataset.</span> We evaluate our proposed pipeline on the Landmark Dataset using MobilnetV2 and various sizes of the EfficientNet. The cell values represent the mean absolute angular errors in degrees. The best results are obtained with the EfficientNet B2, nevertheless, the other models remain close in terms of absolute distance.</figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">The best results are obtained with the EfficientNet B2 Network, which corresponds to our Large model in <a href="#S5.T1" title="In 5.1 Comparison against the State-of-the-art ‣ 5 Results and Analysis ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>. Although the MobilnetV2 (small model) underperforms the large model in every scenario, the small model shows competitive results in the presence of shadows or low light. Our benchmark shows that the baseline scenario (Natural Lighting) reports the best results, whereas our pipeline reduces its performance in the presence of low light or strong shadows.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">We observe that the EfficientNet family can be fine-tuned with our synthetic data up to the B2 size without showing overfit. The larger B3 model does not report any empirical improvement in any scenario. Our best model reports an average angular distance across all scenarios of 2.69<sup id="S5.SS2.p3.1.1" class="ltx_sup"><span id="S5.SS2.p3.1.1.1" class="ltx_text ltx_font_italic">∘</span></sup>.</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2308.14583/assets/x7.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="103" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S5.F8.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Qualitative Field Results.<span id="S5.F8.4.2.1" class="ltx_text ltx_font_medium"> Field samples were used for field sample analysis in <a href="#S5.SS3" title="5.3 Qualitative Analysis ‣ 5 Results and Analysis ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.3</span></a> showing our models’ performance on difficult gauges with odd view angles, harsh lighting conditions, and various obstructions. Using the EfficientNet-B2 model, the field gauge images were correctly predicted within 2% of the gauge’s range or two marker ticks of the ground-truth.</span></span></figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Qualitative Analysis</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We tested our large (EfficientNet-B2) and small (MobileNetV2) models on real-world gauges located in an industrial facility. These gauges are mostly outdoors and exhibit various signs of environmental degradation. <a href="#S5.F8" title="In 5.2 Landmark Dataset Evaluation ‣ 5 Results and Analysis ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a> shows a set of 18 field gauges that were captured under various visual conditions such as extreme brightness, low lighting, reflections, glare, etc. Many gauge glasses were dusty, tinted, cracked, or opaque due to sunlight exposure. Some of the gauges were not easily accessible and were captured from a distance or from non-frontal view angles.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.2" class="ltx_p">For this assessment, we include the gauge min and max values, and manually estimate the gauge actual reading (not only the angular locations). Our EfficientNet-B2 model was able to successfully predict all the field samples in <a href="#S5.F8" title="In 5.2 Landmark Dataset Evaluation ‣ 5 Results and Analysis ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a> with an error of less than 2% of the gauge’s range or two marker ticks of the ground-truth, while the MobileNetV2 model predicted 14 out of the 18 sample images correctly under the same tolerance criterion. The range percentage and marker ticks criterion is commonly used in the field instead of the angular error, it is faster and easier to compute and compare. The EfficientNet-B2 and MobileNetV2 models have achieved <math id="S5.SS3.p2.1.m1.1" class="ltx_Math" alttext="2.3^{\circ}" display="inline"><semantics id="S5.SS3.p2.1.m1.1a"><msup id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml"><mn id="S5.SS3.p2.1.m1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.2.cmml">2.3</mn><mo id="S5.SS3.p2.1.m1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><apply id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.1.m1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1">superscript</csymbol><cn type="float" id="S5.SS3.p2.1.m1.1.1.2.cmml" xref="S5.SS3.p2.1.m1.1.1.2">2.3</cn><compose id="S5.SS3.p2.1.m1.1.1.3.cmml" xref="S5.SS3.p2.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">2.3^{\circ}</annotation></semantics></math> and <math id="S5.SS3.p2.2.m2.1" class="ltx_Math" alttext="4.46^{\circ}" display="inline"><semantics id="S5.SS3.p2.2.m2.1a"><msup id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml"><mn id="S5.SS3.p2.2.m2.1.1.2" xref="S5.SS3.p2.2.m2.1.1.2.cmml">4.46</mn><mo id="S5.SS3.p2.2.m2.1.1.3" xref="S5.SS3.p2.2.m2.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><apply id="S5.SS3.p2.2.m2.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.2.m2.1.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1">superscript</csymbol><cn type="float" id="S5.SS3.p2.2.m2.1.1.2.cmml" xref="S5.SS3.p2.2.m2.1.1.2">4.46</cn><compose id="S5.SS3.p2.2.m2.1.1.3.cmml" xref="S5.SS3.p2.2.m2.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">4.46^{\circ}</annotation></semantics></math> MAE respectively on this sample of field images.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Segmentation Results</h3>

<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T3.2.1" class="ltx_tr">
<td id="S5.T3.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2"><span id="S5.T3.2.1.1.1" class="ltx_text ltx_font_bold">Class</span></td>
<td id="S5.T3.2.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.2.1.2.1" class="ltx_text ltx_font_bold">Percentage</span></td>
<td id="S5.T3.2.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.2.1.3.1" class="ltx_text ltx_font_bold">Mean</span></td>
</tr>
<tr id="S5.T3.2.2" class="ltx_tr">
<td id="S5.T3.2.2.1" class="ltx_td ltx_align_center"><span id="S5.T3.2.2.1.1" class="ltx_text ltx_font_bold">of Dataset</span></td>
<td id="S5.T3.2.2.2" class="ltx_td ltx_align_center"><span id="S5.T3.2.2.2.1" class="ltx_text ltx_font_bold">IoU</span></td>
</tr>
<tr id="S5.T3.2.3" class="ltx_tr">
<td id="S5.T3.2.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Background</td>
<td id="S5.T3.2.3.2" class="ltx_td ltx_align_right ltx_border_t">66.26 %</td>
<td id="S5.T3.2.3.3" class="ltx_td ltx_align_right ltx_border_t">94.60 %</td>
</tr>
<tr id="S5.T3.2.4" class="ltx_tr">
<td id="S5.T3.2.4.1" class="ltx_td ltx_align_left ltx_border_r">Gauge Case</td>
<td id="S5.T3.2.4.2" class="ltx_td ltx_align_right">32.71 %</td>
<td id="S5.T3.2.4.3" class="ltx_td ltx_align_right">88.13 %</td>
</tr>
<tr id="S5.T3.2.5" class="ltx_tr">
<td id="S5.T3.2.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Needle Pointer</td>
<td id="S5.T3.2.5.2" class="ltx_td ltx_align_right ltx_border_bb">1.03 %</td>
<td id="S5.T3.2.5.3" class="ltx_td ltx_align_right ltx_border_bb">76.28 %</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.4.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Segmentation Performance.<span id="S5.T3.5.2.1" class="ltx_text ltx_font_medium"> Intersection over the union of our method on the segmented dataset from <a href="#S4.SS2" title="4.2 Segmented Gauge Dataset ‣ 4 Gauge Reading Datasets ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>. The segmentation classes are highly imbalanced, with the needle class accounting only for just 1% of the labeled pixels. Consequently, this is the class that is harder to segment.</span></span></figcaption>
</figure>
<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Finally, we validate the segmentation step using our real-world segmented dataset from <a href="#S4.SS2" title="4.2 Segmented Gauge Dataset ‣ 4 Gauge Reading Datasets ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>. The evaluation of the segmentation performance is presented in <a href="#S5.T3" title="In 5.4 Segmentation Results ‣ 5 Results and Analysis ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>. Overall, the segmentation network achieves a very good result at locating the gauge case, with an IoU over 94% IoU for the background class, and the case IoU just below 89%. Although the needle pointer also shows satisfactory performance, it is the lowest among the 3 classes at 76.3%.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">We complement this result with the analysis of the error distribution for each individual class in <a href="#S5.F9" title="In 5.4 Segmentation Results ‣ 5 Results and Analysis ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9</span></a>. The gauge case (cyan line) and the background show that most of the images report an IoU over 85% for these two classes. However, the needle has a long tail that extends to just above 55%. Although the needle performance is lower in comparison to the other 2 classes, most of the images obtain an approximate segmentation of the needle pointer.</p>
</div>
<figure id="S5.F9" class="ltx_figure"><img src="/html/2308.14583/assets/x8.png" id="S5.F9.g1" class="ltx_graphics ltx_img_landscape" width="437" height="237" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.3.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S5.F9.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Segmentation Model Results.<span id="S5.F9.4.2.1" class="ltx_text ltx_font_medium"> Distribution of segmentation model IoU results on real images from the segmented dataset in <a href="#S4.SS2" title="4.2 Segmented Gauge Dataset ‣ 4 Gauge Reading Datasets ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>. IoU values are binned into 5% ranges, i.e. 90-95%. Naturally, the sparse nature of the needle class resulted in the poorest results.</span></span></figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We have introduced a simple yet effective pipeline for the automatic gauge reading task. By means of a two-stage CNN network that is trained on synthetic data. Our proposal is validated on two real-world datasets showing that synthetic data and augmentations can effectively bridge the domain gap. Our method provides a robust technique to read and directly digitize analog gauges, showing strong performance even in challenging and noisy real-world scenarios.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
E Corra Alegria and A Cruz Serra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Automatic calibration of analog and digital measuring instruments
using computer vision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on instrumentation and measurement</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">,
49(1):94–99, 2000.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Haojing Bao, Qingchang Tan, Siyuan Liu, and Jianwei Miao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Computer vision measurement of pointer meter readings based on
inverse perspective mapping.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Applied Sciences</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 9(18):3729, 2019.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
G. Bradski.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">The OpenCV Library.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Dr. Dobb’s Journal of Software Tools</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 2000.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Weidong Cai, Bo Ma, Liu Zhang, and Yongming Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">A pointer meter recognition method based on virtual sample generation
technology.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Measurement</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 163:107962, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Shruti Chavan, Xinrui Yu, and Jafar Saniie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">High precision analog gauge reader using optical flow and computer
vision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2022 IEEE International Conference on Electro Information
Technology (eIT)</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 171–175, 2022.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig
Adam.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Encoder-decoder with atrous separable convolution for semantic image
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European conference on computer vision
(ECCV)</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 801–818, 2018.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Blender Online Community.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Blender - a 3D modelling and rendering package</span><span id="bib.bib7.3.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.4.1" class="ltx_text" style="font-size:90%;">Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
MMSegmentation Contributors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">MMSegmentation: Openmmlab semantic segmentation toolbox and
benchmark.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/open-mmlab/mmsegmentation" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/open-mmlab/mmsegmentation</a><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,
Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">The cityscapes dataset for semantic urban scene understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 3213–3223, 2016.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2009 IEEE conference on computer vision and pattern
recognition</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 248–255. Ieee, 2009.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Stefan Dumberger, Raimund Edlinger, and Roman Froschauer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Autonomous real-time gauge reading in an industrial environment.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2020 25th IEEE International Conference on Emerging
Technologies and Factory Automation (ETFA)</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, volume 1, pages 1281–1284,
2020.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, abs/1512.03385, 2015.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Ben Howells, James Charles, and Roberto Cipolla.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Real-time analogue gauge transcription on mobile phone.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) Workshops</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 2369–2377, June 2021.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
J. Illingworth and J. Kittler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">The adaptive hough transform.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">,
PAMI-9(5):690–698, 1987.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
P Jinuntuya, W Sudatham, and N Jinuntuya.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Coordinates transformation method for pointer gauge reading by
machine vision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Journal of Physics: Conference Series</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, volume 2431, pages
012–019. IOP Publishing, 2023.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra
Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">The kinetics human action video dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1705.06950</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Imagenet classification with deep convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Communications of the ACM</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 60(6):84–90, 2017.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Jakob S Lauridsen, Julius AG Graasmé, Malte Pedersen, David Getreuer
Jensen, Søren Holm Andersen, and Thomas B Moeslund.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Reading circular analogue gauges using digital image processing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">14th International Joint Conference on Computer Vision,
Imaging and Computer Graphics Theory and Applications (Visigrapp 2019)</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">,
pages 373–382. SCITEPRESS Digital Library, 2019.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard,
Wayne Hubbard, and Lawrence D Jackel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Backpropagation applied to handwritten zip code recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural computation</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 1(4):541–551, 1989.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Gradient-based learning applied to document recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 86(11):2278–2324, 1998.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Zhu Li, Yisha Zhou, Qinghua Sheng, Kunjian Chen, and Jian Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">A high-robust automatic reading algorithm of pointer meters based on
text detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 20(20):5946, 2020.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2014: 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings, Part V 13</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 740–755.
Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Yue Lin, Qinghua Zhong, and Hailing Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">A pointer type instrument intelligent reading system design based on
convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Frontiers in Physics</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 8:618917, 2020.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Yang Liu, Jun Liu, and Yichen Ke.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">A detection and recognition system of pointer meters in substations
based on computer vision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Measurement</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">, 152:107333, 2020.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Yifan Ma and Qi Jiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">A robust and high-precision automatic reading algorithm of pointer
meters based on machine vision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Measurement Science and Technology</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 30(1):015401, 2018.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Xiaoming Mai, Wensheng Li, Yan Huang, and Yingyi Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">An automatic meter reading method based on one-dimensional measuring
curve mapping.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 IEEE International Conference of Intelligent Robotic and
Control Engineering (IRCE)</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 69–73, 2018.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Automatic differentiation in pytorch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS-W</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Mobilenetv2: Inverted residuals and linear bottlenecks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 4510–4520, 2018.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Mingxing Tan and Quoc Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Efficientnet: Rethinking model scaling for convolutional neural
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages
6105–6114. PMLR, 2019.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Erlin Tian, Huanlong Zhang, and Marlia Mohd Hanafiah.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">A pointer location algorithm for computer visionbased automatic
reading recognition of pointer gauges.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Open Physics</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 17(1):86–92, 2019.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Visarut Trairattanapa, Sasin Phimsiri, Chaitat Utintu, Riu Cherdchusakulcha,
Teepakorn Tosawadi, Ek Thamwiwatthana, Suchat Tungjitnob, Peemapol
Tangamonsiri, Aphisit Takutruea, Apirat Keomeesuan, Tanapoom Jitnaknan, and
Vasin Suttichaya.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Real-time multiple analog gauges reader for an autonomous robot
application.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2022 17th International Joint Symposium on Artificial
Intelligence and Natural Language Processing (iSAI-NLP)</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, pages 1–6, 2022.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Junzhe Wang, Jian Huang, and Rong Cheng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Automatic reading system for analog instruments based on computer
vision and inspection robot for power plant.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 10th International Conference on Modelling,
Identification and Control (ICMIC)</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages 1–6. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Ming Yi, Zhenhua Yang, Fengyu Guo, and Jialin Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">A clustering-based algorithm for automatic detection of automobile
dashboard.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IECON 2017-43rd Annual Conference of the IEEE Industrial
Electronics Society</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 3259–3264. IEEE, 2017.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Implementation Details</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.2" class="ltx_p">We implement the segmentation network using the MMsegmentation toolbox <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> running over the PyTorch framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, we trained the segmentation network for 80,000 iterations using a learning rate of <math id="S7.p1.1.m1.1" class="ltx_Math" alttext="1e^{-7}" display="inline"><semantics id="S7.p1.1.m1.1a"><mrow id="S7.p1.1.m1.1.1" xref="S7.p1.1.m1.1.1.cmml"><mn id="S7.p1.1.m1.1.1.2" xref="S7.p1.1.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S7.p1.1.m1.1.1.1" xref="S7.p1.1.m1.1.1.1.cmml">​</mo><msup id="S7.p1.1.m1.1.1.3" xref="S7.p1.1.m1.1.1.3.cmml"><mi id="S7.p1.1.m1.1.1.3.2" xref="S7.p1.1.m1.1.1.3.2.cmml">e</mi><mrow id="S7.p1.1.m1.1.1.3.3" xref="S7.p1.1.m1.1.1.3.3.cmml"><mo id="S7.p1.1.m1.1.1.3.3a" xref="S7.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S7.p1.1.m1.1.1.3.3.2" xref="S7.p1.1.m1.1.1.3.3.2.cmml">7</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.p1.1.m1.1b"><apply id="S7.p1.1.m1.1.1.cmml" xref="S7.p1.1.m1.1.1"><times id="S7.p1.1.m1.1.1.1.cmml" xref="S7.p1.1.m1.1.1.1"></times><cn type="integer" id="S7.p1.1.m1.1.1.2.cmml" xref="S7.p1.1.m1.1.1.2">1</cn><apply id="S7.p1.1.m1.1.1.3.cmml" xref="S7.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.p1.1.m1.1.1.3.1.cmml" xref="S7.p1.1.m1.1.1.3">superscript</csymbol><ci id="S7.p1.1.m1.1.1.3.2.cmml" xref="S7.p1.1.m1.1.1.3.2">𝑒</ci><apply id="S7.p1.1.m1.1.1.3.3.cmml" xref="S7.p1.1.m1.1.1.3.3"><minus id="S7.p1.1.m1.1.1.3.3.1.cmml" xref="S7.p1.1.m1.1.1.3.3"></minus><cn type="integer" id="S7.p1.1.m1.1.1.3.3.2.cmml" xref="S7.p1.1.m1.1.1.3.3.2">7</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.1.m1.1c">1e^{-7}</annotation></semantics></math> and a batch size of 18. We initialize the segmentation network with the pre-trained weights of the Cityscapes dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and train until convergence on a held-out set of 500 synthetic images. We empirically find that augmentation plays a key role in closing the domain gap between synthetic (training set) and real-world images (validation sets), therefore we alter the image contrast, brightness, and saturation, we also apply random rotations up to 20°, introduce Gaussian blur and randomly crop out small squares in the input image. We train with an input image size of 512<math id="S7.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S7.p1.2.m2.1a"><mo id="S7.p1.2.m2.1.1" xref="S7.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.p1.2.m2.1b"><times id="S7.p1.2.m2.1.1.cmml" xref="S7.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.2.m2.1c">\times</annotation></semantics></math>512.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.2" class="ltx_p">For the reading network, we train the MobilNetV2 backbone with for 120 epochs using a learning rate of <math id="S7.p2.1.m1.1" class="ltx_Math" alttext="1e^{-7}" display="inline"><semantics id="S7.p2.1.m1.1a"><mrow id="S7.p2.1.m1.1.1" xref="S7.p2.1.m1.1.1.cmml"><mn id="S7.p2.1.m1.1.1.2" xref="S7.p2.1.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S7.p2.1.m1.1.1.1" xref="S7.p2.1.m1.1.1.1.cmml">​</mo><msup id="S7.p2.1.m1.1.1.3" xref="S7.p2.1.m1.1.1.3.cmml"><mi id="S7.p2.1.m1.1.1.3.2" xref="S7.p2.1.m1.1.1.3.2.cmml">e</mi><mrow id="S7.p2.1.m1.1.1.3.3" xref="S7.p2.1.m1.1.1.3.3.cmml"><mo id="S7.p2.1.m1.1.1.3.3a" xref="S7.p2.1.m1.1.1.3.3.cmml">−</mo><mn id="S7.p2.1.m1.1.1.3.3.2" xref="S7.p2.1.m1.1.1.3.3.2.cmml">7</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.p2.1.m1.1b"><apply id="S7.p2.1.m1.1.1.cmml" xref="S7.p2.1.m1.1.1"><times id="S7.p2.1.m1.1.1.1.cmml" xref="S7.p2.1.m1.1.1.1"></times><cn type="integer" id="S7.p2.1.m1.1.1.2.cmml" xref="S7.p2.1.m1.1.1.2">1</cn><apply id="S7.p2.1.m1.1.1.3.cmml" xref="S7.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.p2.1.m1.1.1.3.1.cmml" xref="S7.p2.1.m1.1.1.3">superscript</csymbol><ci id="S7.p2.1.m1.1.1.3.2.cmml" xref="S7.p2.1.m1.1.1.3.2">𝑒</ci><apply id="S7.p2.1.m1.1.1.3.3.cmml" xref="S7.p2.1.m1.1.1.3.3"><minus id="S7.p2.1.m1.1.1.3.3.1.cmml" xref="S7.p2.1.m1.1.1.3.3"></minus><cn type="integer" id="S7.p2.1.m1.1.1.3.3.2.cmml" xref="S7.p2.1.m1.1.1.3.3.2">7</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.1.m1.1c">1e^{-7}</annotation></semantics></math> and perform a learning rate step every 50 epochs, we also include a dropout layer just before the classifier and set the dropout probability to 0.4. For the EfficentNet- B2 model we use a learning rate of <math id="S7.p2.2.m2.1" class="ltx_Math" alttext="1e^{-7}" display="inline"><semantics id="S7.p2.2.m2.1a"><mrow id="S7.p2.2.m2.1.1" xref="S7.p2.2.m2.1.1.cmml"><mn id="S7.p2.2.m2.1.1.2" xref="S7.p2.2.m2.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S7.p2.2.m2.1.1.1" xref="S7.p2.2.m2.1.1.1.cmml">​</mo><msup id="S7.p2.2.m2.1.1.3" xref="S7.p2.2.m2.1.1.3.cmml"><mi id="S7.p2.2.m2.1.1.3.2" xref="S7.p2.2.m2.1.1.3.2.cmml">e</mi><mrow id="S7.p2.2.m2.1.1.3.3" xref="S7.p2.2.m2.1.1.3.3.cmml"><mo id="S7.p2.2.m2.1.1.3.3a" xref="S7.p2.2.m2.1.1.3.3.cmml">−</mo><mn id="S7.p2.2.m2.1.1.3.3.2" xref="S7.p2.2.m2.1.1.3.3.2.cmml">7</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.p2.2.m2.1b"><apply id="S7.p2.2.m2.1.1.cmml" xref="S7.p2.2.m2.1.1"><times id="S7.p2.2.m2.1.1.1.cmml" xref="S7.p2.2.m2.1.1.1"></times><cn type="integer" id="S7.p2.2.m2.1.1.2.cmml" xref="S7.p2.2.m2.1.1.2">1</cn><apply id="S7.p2.2.m2.1.1.3.cmml" xref="S7.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S7.p2.2.m2.1.1.3.1.cmml" xref="S7.p2.2.m2.1.1.3">superscript</csymbol><ci id="S7.p2.2.m2.1.1.3.2.cmml" xref="S7.p2.2.m2.1.1.3.2">𝑒</ci><apply id="S7.p2.2.m2.1.1.3.3.cmml" xref="S7.p2.2.m2.1.1.3.3"><minus id="S7.p2.2.m2.1.1.3.3.1.cmml" xref="S7.p2.2.m2.1.1.3.3"></minus><cn type="integer" id="S7.p2.2.m2.1.1.3.3.2.cmml" xref="S7.p2.2.m2.1.1.3.3.2">7</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.2.m2.1c">1e^{-7}</annotation></semantics></math> and a dropout of 0.5. For both networks we use the ImageNet pre-trained weights, and include a similar augmentation strategy to the one proposed on the segmentation, but remove the rotations as we observe no empirical improvement. We train the EfficientNet B2 model to convergence in about 6 hours using a single NVIDIA-V100 GPU.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Ablation Analysis</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">We present an ablation analysis on two elements in our proposed pipeline, we evaluate the effectiveness of the network if the attention provided by the segmentation is removed and if we drop all the augmentation from the training process. We perform this ablation analysis in the landmark dataset, <a href="#S8.T4" title="In 8 Ablation Analysis ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a> summarizes these results. Overall, we observe both scenarios generate a drop in performance to about 6.1°MAE, with the dim light scenario showing the largest performance degradation, but also the far camera setting shows a relative loss in performance of 40% and 38%.</p>
</div>
<figure id="S8.T4" class="ltx_table">
<table id="S8.T4.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S8.T4.2.1" class="ltx_tr">
<td id="S8.T4.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span id="S8.T4.2.1.1.1" class="ltx_text" style="font-size:90%;">Scenario</span></td>
<td id="S8.T4.2.1.2" class="ltx_td ltx_align_right ltx_border_tt"><span id="S8.T4.2.1.2.1" class="ltx_text" style="font-size:90%;">Large</span></td>
<td id="S8.T4.2.1.3" class="ltx_td ltx_align_right ltx_border_tt"><span id="S8.T4.2.1.3.1" class="ltx_text" style="font-size:90%;">No Seg.</span></td>
<td id="S8.T4.2.1.4" class="ltx_td ltx_align_right ltx_border_tt"><span id="S8.T4.2.1.4.1" class="ltx_text" style="font-size:90%;">No Aug.</span></td>
</tr>
<tr id="S8.T4.2.2" class="ltx_tr">
<td id="S8.T4.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S8.T4.2.2.1.1" class="ltx_text" style="font-size:90%;">Natural Lighting</span></td>
<td id="S8.T4.2.2.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S8.T4.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">2.29</span></td>
<td id="S8.T4.2.2.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S8.T4.2.2.3.1" class="ltx_text" style="font-size:90%;">3.49</span></td>
<td id="S8.T4.2.2.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S8.T4.2.2.4.1" class="ltx_text" style="font-size:90%;">3.29</span></td>
</tr>
<tr id="S8.T4.2.3" class="ltx_tr">
<td id="S8.T4.2.3.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S8.T4.2.3.1.1" class="ltx_text" style="font-size:90%;">Far Camera</span></td>
<td id="S8.T4.2.3.2" class="ltx_td ltx_align_right"><span id="S8.T4.2.3.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">2.48</span></td>
<td id="S8.T4.2.3.3" class="ltx_td ltx_align_right"><span id="S8.T4.2.3.3.1" class="ltx_text" style="font-size:90%;">4.90</span></td>
<td id="S8.T4.2.3.4" class="ltx_td ltx_align_right"><span id="S8.T4.2.3.4.1" class="ltx_text" style="font-size:90%;">4.77</span></td>
</tr>
<tr id="S8.T4.2.4" class="ltx_tr">
<td id="S8.T4.2.4.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S8.T4.2.4.1.1" class="ltx_text" style="font-size:90%;">Dim Light</span></td>
<td id="S8.T4.2.4.2" class="ltx_td ltx_align_right"><span id="S8.T4.2.4.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">3.20</span></td>
<td id="S8.T4.2.4.3" class="ltx_td ltx_align_right"><span id="S8.T4.2.4.3.1" class="ltx_text" style="font-size:90%;">14.48</span></td>
<td id="S8.T4.2.4.4" class="ltx_td ltx_align_right"><span id="S8.T4.2.4.4.1" class="ltx_text" style="font-size:90%;">14.45</span></td>
</tr>
<tr id="S8.T4.2.5" class="ltx_tr">
<td id="S8.T4.2.5.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S8.T4.2.5.1.1" class="ltx_text" style="font-size:90%;">Glass Reflection</span></td>
<td id="S8.T4.2.5.2" class="ltx_td ltx_align_right"><span id="S8.T4.2.5.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">2.37</span></td>
<td id="S8.T4.2.5.3" class="ltx_td ltx_align_right"><span id="S8.T4.2.5.3.1" class="ltx_text" style="font-size:90%;">3.71</span></td>
<td id="S8.T4.2.5.4" class="ltx_td ltx_align_right"><span id="S8.T4.2.5.4.1" class="ltx_text" style="font-size:90%;">4.26</span></td>
</tr>
<tr id="S8.T4.2.6" class="ltx_tr">
<td id="S8.T4.2.6.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S8.T4.2.6.1.1" class="ltx_text" style="font-size:90%;">Needle Shadow</span></td>
<td id="S8.T4.2.6.2" class="ltx_td ltx_align_right"><span id="S8.T4.2.6.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">3.11</span></td>
<td id="S8.T4.2.6.3" class="ltx_td ltx_align_right"><span id="S8.T4.2.6.3.1" class="ltx_text" style="font-size:90%;">3.96</span></td>
<td id="S8.T4.2.6.4" class="ltx_td ltx_align_right"><span id="S8.T4.2.6.4.1" class="ltx_text" style="font-size:90%;">3.99</span></td>
</tr>
<tr id="S8.T4.2.7" class="ltx_tr">
<td id="S8.T4.2.7.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t"><span id="S8.T4.2.7.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Average</span></td>
<td id="S8.T4.2.7.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S8.T4.2.7.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">2.69</span></td>
<td id="S8.T4.2.7.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S8.T4.2.7.3.1" class="ltx_text" style="font-size:90%;">6.11</span></td>
<td id="S8.T4.2.7.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S8.T4.2.7.4.1" class="ltx_text" style="font-size:90%;">6.15</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span><span id="S8.T4.6.1" class="ltx_text ltx_font_bold">Ablation Analysis</span>. We show the relevance of two components in our poplin, the segmentation step (No Seg.) and the data augmentation (No Aug.) overall the largest degradation is shown in low light conditions.</figcaption>
</figure>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Benchmark Ground-truth Corrections</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">As stated on the main paper, we now outline in detail the corrections made to the ground-truth labels of the benchmark dataset provided by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>:</p>
</div>
<section id="S9.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Angles above 180°</h4>

<div id="S9.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S9.SS0.SSS0.Px1.p1.1" class="ltx_p">The test sequences <span id="S9.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_typewriter">mater_b_vid2</span>, <span id="S9.SS0.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_typewriter">meter_e_vid1</span>, and <span id="S9.SS0.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_typewriter">meter_3_vid2</span> contain angles that exceed 180 degrees. As mentioned in the main paper the ground-truth did not use the clockwise angular distance from start to needle angle. Instead, it calculated the shortest angular distance in any direction (clockwise or counterclockwise).</p>
</div>
<div id="S9.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S9.SS0.SSS0.Px1.p2.1" class="ltx_p">We noticed that <span id="S9.SS0.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_typewriter">vid_a_1</span>’s first few frames are incorrectly labeled, where the needle is exactly on the start, but the ground-truth label is listed around 20°. We opted to modify those values to reflect the actual ground-truth and report the revisited (rev) performance using those new values. For completeness, we also report the performance with the original ground-truth values. All the frames from <span id="S9.SS0.SSS0.Px1.p2.1.2" class="ltx_text ltx_font_typewriter">vid_a_2</span> were off by roughly 42°from the actual reading. The metadata of that video’s ground-truth asserted that the readings were measured from the marker for 2 bar, even though the minimum marker is at roughly 0.5 bar. This caused an almost constant offset between the actual reading and the provided ground-truth.</p>
</div>
<div id="S9.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S9.SS0.SSS0.Px1.p3.1" class="ltx_p">We also note that in Howells <em id="S9.SS0.SSS0.Px1.p3.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S9.SS0.SSS0.Px1.p3.1.2" class="ltx_text"></span> the MAE for all of <span id="S9.SS0.SSS0.Px1.p3.1.3" class="ltx_text ltx_font_typewriter">meter_a</span> was less than 10°, however the (nearly constant) 42°offset error for all frames of video 2 (a third of total frames) should result in about 14°total error for a model with perfect angle readings in the first and third video. However, the results of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> are well below this estimation (9.24°). We believe that the published ground-truth could be different (or perhaps an older version) from the one used in their experimental setup.</p>
</div>
<div id="S9.SS0.SSS0.Px1.p4" class="ltx_para">
<p id="S9.SS0.SSS0.Px1.p4.1" class="ltx_p">The <span id="S9.SS0.SSS0.Px1.p4.1.1" class="ltx_text ltx_font_typewriter">delta_a</span> value for <span id="S9.SS0.SSS0.Px1.p4.1.2" class="ltx_text ltx_font_typewriter">meter_e</span>, which specifies the angle between the start and end of the markers, was corrected from roughly 150°to 300°. This does not affect Howells <em id="S9.SS0.SSS0.Px1.p4.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S9.SS0.SSS0.Px1.p4.1.4" class="ltx_text"></span> or our model, but perhaps future works which rely strongly on this angular distance could be affected.</p>
</div>
</section>
</section>
<section id="S10" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Detailed Benchmark Results</h2>

<div id="S10.p1" class="ltx_para">
<p id="S10.p1.1" class="ltx_p">Full benchmark results for the large EfficientNet-B2 model are shown in <a href="#S10.F10" title="In 10 Detailed Benchmark Results ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">10</span></a> and the results for the small MobileNetV2 model are illustrated in <a href="#S10.F11" title="In 10 Detailed Benchmark Results ‣ Learning to Read Analog Gauges from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">11</span></a></p>
</div>
<figure id="S10.F10" class="ltx_figure"><img src="/html/2308.14583/assets/x9.png" id="S10.F10.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="498" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S10.F10.4.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S10.F10.5.2" class="ltx_text" style="font-size:90%;">Full benchmark results for the EfficientNet-B2 model. The modified ground-truth values for <span id="S10.F10.5.2.1" class="ltx_text ltx_font_typewriter">meter_a</span> are also shown. The MAE values for <span id="S10.F10.5.2.2" class="ltx_text ltx_font_typewriter">meter_a</span> are with respect to the unmodified ground-truth.</span></figcaption>
</figure>
<figure id="S10.F11" class="ltx_figure"><img src="/html/2308.14583/assets/x10.png" id="S10.F11.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="498" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S10.F11.4.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S10.F11.5.2" class="ltx_text" style="font-size:90%;">Full benchmark results for the MobileNetV2 model. The modified ground-truth values for <span id="S10.F11.5.2.1" class="ltx_text ltx_font_typewriter">meter_a</span> are also shown. The MAE values for <span id="S10.F11.5.2.2" class="ltx_text ltx_font_typewriter">meter_a</span> are with respect to the unmodified ground-truth.</span></figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2308.14582" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2308.14583" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2308.14583">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2308.14583" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2308.14584" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 10:29:43 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
