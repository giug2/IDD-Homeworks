<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>AutoPET Challenge: Tumour Synthesis for Data Augmentation</title>
<!--Generated on Thu Sep 12 05:28:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Tumour Generation Data Augmentation PET-CT" lang="en" name="keywords"/>
<base href="/html/2409.08068v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S1" title="In AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S1.SS1" title="In 1 Introduction ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Exploratory Data Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S1.SS1.SSS1" title="In 1.1 Exploratory Data Analysis ‣ 1 Introduction ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1.1 </span>Insufficient dataset size</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S1.SS1.SSS2" title="In 1.1 Exploratory Data Analysis ‣ 1 Introduction ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1.2 </span>Countermeasures</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S2" title="In AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S2.SS1" title="In 2 Related Work ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Towards Generalizable Tumour Synthesis (DiffTumor) <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref">13</span>]</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S3" title="In AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S3.SS1" title="In 3 Methodology ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>DynUnet baseline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S3.SS2" title="In 3 Methodology ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>DiffTumor adaptation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S4" title="In AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S4.SS1" title="In 4 Results ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S4.SS2" title="In 4 Results ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Quantitative Result</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S4.SS3" title="In 4 Results ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Qualitative Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S4.SS3.SSS1" title="In 4.3 Qualitative Results ‣ 4 Results ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Predicted Mask</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S4.SS3.SSS2" title="In 4.3 Qualitative Results ‣ 4 Results ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Synthesized Tumour</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S5" title="In AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S5.SS0.SSS1" title="In 5 Conclusion ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.0.1 </span>Acknowledgements</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S5.SS0.SSS2" title="In 5 Conclusion ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.0.2 </span><span class="ltx_ERROR undefined">\discintname</span></span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong SAR </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Department of Electronic Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong SAR</span></span></span>
<h1 class="ltx_title ltx_title_document">AutoPET Challenge: Tumour Synthesis for Data Augmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lap Yan Lennon Chan 
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chenxin Li
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yixuan Yuan
</span><span class="ltx_author_notes">22</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Accurate lesion segmentation in whole-body PET/CT scans is crucial for cancer diagnosis and treatment planning, but limited datasets often hinder the performance of automated segmentation models. In this paper, we explore the potential of leveraging the deep prior from a generative model to serve as a data augmenter for automated lesion segmentation in PET/CT scans. We adapt the DiffTumor method, originally designed for CT images, to generate synthetic PET-CT images with lesions. Our approach trains the generative model on the AutoPET dataset and uses it to expand the training data. We then compare the performance of segmentation models trained on the original and augmented datasets. Our findings show that the model trained on the augmented dataset achieves a higher Dice score, demonstrating the potential of our data augmentation approach. In a nutshell, this work presents a promising direction for improving lesion segmentation in whole-body PET/CT scans with limited datasets, potentially enhancing the accuracy and reliability of cancer diagnostics.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Tumour Generation Data Augmentation PET-CT
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Over the past decades, PET/CT has emerged as a pivotal tool in oncological diagnostics, management and treatment planning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib1" title="">1</a>]</cite>. Lesion segmentation, which is a necessary step for quantitative image analysis, is performed manually and is thus tedious, time-consuming and costly <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib3" title="">3</a>]</cite>. Machine Learning, however, offers the potential for fast and fully automated quantitative analysis of PET/CT images. With this in mind, we participated in the AutoPET-III challenge held in MICCAI 2024, the task of which was to automate the Lesion Segmentation of Whole-Body PET/CT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib4" title="">4</a>]</cite>. We adopted a data-centric approach because of the inherent imperfections in the given dataset by the organiser. In this approach, we intended to perform data augmentation to the AutoPET dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib6" title="">6</a>]</cite> without changing the baseline segmentation model (which in this challenge is DynUnet, a MONAI implementation of nnUnet) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib8" title="">8</a>]</cite>. Our intended approach can best be summarised as utilizing deep generative models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib11" title="">11</a>]</cite> to augment the imperfect AutoPET dataset such that the baseline segmentation model achieves better generalization ability.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Exploratory Data Analysis </h3>
<section class="ltx_subsubsection" id="S1.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.1.1 </span>Insufficient dataset size</h4>
<div class="ltx_para" id="S1.SS1.SSS1.p1">
<p class="ltx_p" id="S1.SS1.SSS1.p1.1">The dataset contains 1614 paired CT-PET scans and lesion segmentation masks. However, previous research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib12" title="">12</a>]</cite> shows how the size of the training dataset corresponds positively with the performance of the tumour segmentation model (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S1.F1" title="Figure 1 ‣ 1.1.1 Insufficient dataset size ‣ 1.1 Exploratory Data Analysis ‣ 1 Introduction ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_tag">1</span></a>). And the training dataset’s size of 1614 could be a limiting factor in segmentation model performance.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="199" id="S1.F1.g1" src="extracted/5845927/scaling_law_freetumor.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Dice score for tumour segmentation models trained with different numbers of cases (real or synthesized), the trend shows that in general, more training cases lead to better performance. The result of DiffTumor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib13" title="">13</a>]</cite> on 11k is re-implemented.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S1.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.1.2 </span>Countermeasures</h4>
<div class="ltx_para" id="S1.SS1.SSS2.p1">
<p class="ltx_p" id="S1.SS1.SSS2.p1.1">To address the problem of a small dataset, our intuition is to generate synthesized PET-CT images to augment the dataset, such that we could enlarge the dataset while introducing more diverse, but still in-distribution data into the training set for more robust learning.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Towards Generalizable Tumour Synthesis (DiffTumor) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib13" title="">13</a>]</cite>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The researchers propose the synthesis of tumoured CT images from healthy CT images using a lesion segmentation mask as a condition for augmenting a dataset of tumored CT images to train a more robust tumour segmentation model.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">The whole pipeline consists of 3 steps. In step 1, healthy CT images are used to train an autoencoder that represents CT images in a compressed latent space. In step 2, a latent diffusion model is trained to generate the latent representation of tumoured CT images with the condition of healthy CT images and the tumour segmentation mask (along with the organ segmentation mask) by learning the reverse process of adding noises to the latent representation of the tumoured CT images. In Step 3, tumoured CT images are synthesized with out-of-dataset healthy CT images and a tumour segmentation mask (again, along with the organ segmentation mask) to augment the original dataset, which is then used to train a tumour segmentation model that performs much better than if it is trained by a dataset that is not augmented.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">A better performance with synthetic tumours than with real tumours is achieved under a nnU-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib8" title="">8</a>]</cite> backbone (Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S2.T1" title="Table 1 ‣ 2.1 Towards Generalizable Tumour Synthesis (DiffTumor) [13] ‣ 2 Related Work ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S2.T1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1.1.1.1">nnU-Net</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.1.1.2">metrics</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.1.1.3">fold0</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.1.1.4">fold1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.1.1.5">fold2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.1.1.6">fold3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.1.1.7">fold4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.1.1.8">average</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.2.2.1.1" rowspan="2"><span class="ltx_text" id="S2.T1.2.2.1.1.1">real tumours</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.1.2">DSC (%)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.1.3">73.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.1.4">76.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.1.5">80.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.1.6">80.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.1.7">73.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.2.1.8">76.9</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.3.2">
<td class="ltx_td ltx_align_center" id="S2.T1.2.3.2.1">NSD (%)</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.3.2.2">62.7</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.3.2.3">70.2</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.3.2.4">71.2</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.3.2.5">70.8</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.3.2.6">67.5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.3.2.7">68.5</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S2.T1.2.4.3.1" rowspan="2"><span class="ltx_text" id="S2.T1.2.4.3.1.1">DiffTumor</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.4.3.2">DSC (%)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.4.3.3">84.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.4.3.4">83.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.4.3.5">81.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.4.3.6">83.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.4.3.7">77.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.4.3.8">82.1</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.5.4">
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T1.2.5.4.1">NSD (%)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T1.2.5.4.2">78.3</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T1.2.5.4.3">74.4</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T1.2.5.4.4">74.1</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T1.2.5.4.5">76.9</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T1.2.5.4.6">72.3</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T1.2.5.4.7">75.2</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S2.T1.4.2" style="font-size:90%;">Performance of nnU-Net on different metrics</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We adapted DiffTumor to our task to conduct data augmentation, then compared the performance of DynUnet trained on the augmented dataset and that of the baseline DynUnet.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>DynUnet baseline</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We use the DynUnet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib8" title="">8</a>]</cite>, which is based on nnU-Net. The nnU-Net fingerprint extractor and planner on the autoPET3 dataset are utilized to configure the model.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>DiffTumor adaptation</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The pipeline for data augmentation using a deep generative model borrows largely from DiffTumor difftumour but we adapt it to our use. Most notably, instead of taking in and outputting CT images, PET-CT images are taken in and outputted. In Step 1, whereas DiffTumor trains its autoencoder on a large healthy CT dataset, we train ours only with the AutoPET dataset following the principle of data augmentation, where no new external data should be introduced. In Step 2, unlike DiffTumor which trains only on a certain organ as it wants to demonstrate the generalizability to other organs, we do not need to or intend to demonstrate that. Therefore, Step 2 is trained on the whole Autopet dataset. For Step 2, since we do not have the ground truth organ segmentation mask to accompany the tumour segmentation mask, we utilized SegResNet from “Wholebody ct segmentation” model provided in MONAI model zoo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib16" title="">16</a>]</cite> to produce a pseudo-organ mask instead. We do not follow Step 3 of DiffTumor since we will use the training settings of DynUnet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#bib.bib17" title="">17</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setting</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The AutoPET dataset contains 1614 full-body PET-CT scans, 1291 of which belong to the training set and 323 belong to the validation set. In our experiment, we use the 1291 training data to train the deep generative model for data augmentation. During data augmentation, we generate 3 different synthesized data per 1 training data, thus the augmented training set has a size of 5152 (with some synthesized data from 4 images rejected due to failure of generating pseudo-organ mask by SegResNet).</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">The input patch size is configured to be (128, 160, 112) with a batch size of 2. Training runs for a total of 581 epochs.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Quantitative Result</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S4.T2" title="Table 2 ‣ 4.2 Quantitative Result ‣ 4 Results ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_tag">2</span></a> shows the dice scores for our trained segmentation model (DynUnet) on the baseline dataset and the augmented dataset under different data transformation techniques (and thus different dataset size). We have completed experiments both under the condition of taking 1 random transform per image and 15 random transforms per image.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.2.1.1.1">Transforms per image</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.2.1.1.2">Baseline/Augmented</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.2.1.1.3">Training Dataset size</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.2.1.1.4">DSC</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.1.1" rowspan="2"><span class="ltx_text" id="S4.T2.2.2.1.1.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.1.2">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.1.3">1291</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.1.4">0.3650</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.3.2">
<td class="ltx_td ltx_align_center" id="S4.T2.2.3.2.1">Augmented</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.3.2.2">5152</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.3.2.3">0.4542</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.4.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.4.3.1" rowspan="2"><span class="ltx_text" id="S4.T2.2.4.3.1.1">15</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.4.3.2">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.4.3.3">19365</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.4.3.4">0.5398</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.5.4">
<td class="ltx_td ltx_align_center" id="S4.T2.2.5.4.1">Augmented</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.5.4.2">77280</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.5.4.3">0.6143</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.6.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.2.6.5.1" rowspan="2"><span class="ltx_text" id="S4.T2.2.6.5.1.1">30</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.6.5.2">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.6.5.3">38730</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.6.5.4">0.5859</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.7.6">
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.2.7.6.1">Augmented</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.2.7.6.2">154560</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.2.7.6.3">0.6179</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.4.2" style="font-size:90%;">Quantitative Comparisons between the Baseline Model and Model trained on the Augmented Dataset</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Qualitative Results</h3>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Predicted Mask</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">The following (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S4.F2" title="Figure 2 ‣ 4.3.1 Predicted Mask ‣ 4.3 Qualitative Results ‣ 4 Results ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_tag">2</span></a>) are two examples taken from the validation set. The first one is a success in that it improves from the baseline model by successfully segmenting a tumour which the baseline fails to do. The second one, however, presents a false positive displayed by the model trained on the augmented dataset not displayed by the baseline. All in all, while in general, the model trained on the augmented dataset improves from the baseline, there are cases where its performance worsens just as there are cases where it improves.</p>
</div>
<figure class="ltx_figure" id="S4.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="299" id="S4.F2.g1" src="extracted/5845927/pos-neg_1.png" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="299" id="S4.F2.g2" src="extracted/5845927/pos-neg_2.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S4.F2.3.2" style="font-size:90%;">The successful and unsuccessful case of lesion segmentation</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS1.p2">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1">Below are some more examples (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S4.F3" title="Figure 3 ‣ 4.3.1 Predicted Mask ‣ 4.3 Qualitative Results ‣ 4 Results ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_tag">3</span></a> and  <a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S4.F4" title="Figure 4 ‣ 4.3.1 Predicted Mask ‣ 4.3 Qualitative Results ‣ 4 Results ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_tag">4</span></a>).:</p>
</div>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="299" id="S4.F3.g1" src="extracted/5845927/more_examples_1.png" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="299" id="S4.F3.g2" src="extracted/5845927/more_examples_2.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.3.2" style="font-size:90%;">More Lesion Segmentation Results</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="299" id="S4.F4.g1" src="extracted/5845927/more_examples_3.png" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="299" id="S4.F4.g2" src="extracted/5845927/more_examples_4.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.3.2" style="font-size:90%;">More Lesion Segmentation Results</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Synthesized Tumour</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">Here are some tumours synthesized on the validation set (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08068v1#S4.F5" title="Figure 5 ‣ 4.3.2 Synthesized Tumour ‣ 4.3 Qualitative Results ‣ 4 Results ‣ AutoPET Challenge: Tumour Synthesis for Data Augmentation"><span class="ltx_text ltx_ref_tag">5</span></a>):</p>
</div>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="150" id="S4.F5.g1" src="extracted/5845927/synthesized_1.png" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="150" id="S4.F5.g2" src="extracted/5845927/synthesized_2.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.3.2" style="font-size:90%;">Tumour Generation in unseen data. The generation network learns well by not automatically translating every tumour to be shown as a PET tracer as that is not always the case.</span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In conclusion, we have successfully demonstrated the viability of a novel data augmentation paradigm for multitracer, multicentre full-body PET-CT lesion segmentation. This approach leverages a Deep Generative Model to learn the distribution of existing data and synthesize new samples, thereby augmenting the original dataset for downstream applications. Our results show promise in advancing the state-of-the-art in this challenging task. However, we acknowledge that further comprehensive analysis is necessary to fully elucidate the benefits and potential limitations of this method.</p>
</div>
<div class="ltx_para" id="S5.p2">
<span class="ltx_ERROR undefined" id="S5.p2.1">{credits}</span>
</div>
<section class="ltx_subsubsection" id="S5.SS0.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.0.1 </span>Acknowledgements</h4>
<div class="ltx_para" id="S5.SS0.SSS1.p1">
<p class="ltx_p" id="S5.SS0.SSS1.p1.1">This study was funded by the Faculty of Engineering of the Chinese University of Hong Kong. The authors acknowledge the AutoPET challenge for the free publicly available PET/CT images used in this study.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS0.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.0.2 </span><span class="ltx_ERROR undefined" id="S5.SS0.SSS2.1.1">\discintname</span>
</h4>
<div class="ltx_para" id="S5.SS0.SSS2.p1">
<p class="ltx_p" id="S5.SS0.SSS2.p1.1">The authors have no competing interests to declare that are relevant to the content of this article.

</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Heron, D.E., Andrade, R.S., Beriwal, S., Smith, R.P.: PET-CT in Radiation Oncology: The Impact on Diagnosis, Treatment Planning, and Assessment of Treatment Response. American Journal of Clinical Oncology, vol. 31, no. 4, pp. 352–362 (2008)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Sun, L., Li, C., Ding, X., Huang, Y., Chen, Z., Wang, G., Yu, Y., Paisley, J.: Few-shot Medical Image Segmentation Using a Global Correlation Network with Discriminative Embedding, Computers in Biology and Medicine, vol. 140, p. 105067 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Sun, L., Li, C., Ding, X., Huang, Y., Chen, Z., Wang, G., Yu, Y., Paisley, J.: Unsupervised Anomaly Segmentation for Brain Lesions Using Dual Semantic-Manifold Reconstruction, in Proceedings of the International Conference on Neural Information Processing, Springer, Cham, 2022, pp. 133–144 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Ingrisch, M., Dexl, J., Jeblick, K., Cyran, C., Gatidis, S., &amp; Kuestner, T. (2024). Automated Lesion Segmentation in Whole-Body PET/CT - Multitracer Multicenter generalization. 27th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2024). Zenodo. https://doi.org/10.5281/zenodo.10990932

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Gatidis S, Kuestner T. A whole-body FDG-PET/CT dataset with manually annotated tumor lesions
(FDG-PET-CT-Lesions) [Dataset]. The Cancer Imaging Archive, 2022. DOI: 10.7937/gkr0-xv29

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Jeblick, K., et al. A whole-body PSMA-PET/CT dataset with manually annotated tumor lesions
(PSMA-PET-CT-Lesions) (Version 1) [Dataset]. The Cancer Imaging Archive, 2024. DOI: 10.7937/r7ep-3x37

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Isensee, F., Jäger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: Automated design of deep learning methods for biomedical image segmentation. arXiv preprint arXiv:1904.08128 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Isensee, F., Petersen, J., Klein, A., Zimmerer, D., Jaeger, P.F., Kohl, S., Wasserthal, J., Koehler, G., Norajitra, T., Wirkert, S.: nnu-net: Self-adapting framework for u-net-based medical image segmentation. arXiv preprint arXiv:1809.10486 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Croitoru, F.-A., Hondru, V., Ionescu, R.T., Shah, M.: Diffusion Models in Vision: A Survey, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 9, pp. 10850–10869 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Li, C., Liu, H., Liu, Y., Feng, B.Y., Li, W., Liu, X., Chen, Z., Shao, J., Yuan, Y.: Endora: Video Generation Models as Endoscopy Simulators, arXiv preprint arXiv:2403.11050 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Li, C., Liu, X., Li, W., Wang, C., Liu, H., Yuan, Y.: U-KAN Makes Strong Backbone for Medical Image Segmentation and Generation, arXiv preprint arXiv:2406.02918 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Wu, L., Zhuang, J., Ni, X., Chen, H.: FreeTumor: Advance Tumor Segmentation via Large-Scale Tumor Synthesis. arXiv preprint arXiv:2406.01264 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Chen, Q., Chen, X., Song, H., Xiong, Z., Yuille, A., Wei, C., Zhou, Z.: Towards generalizable tumor synthesis. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11147-11158 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Wasserthal, J., Breit, H.-C., Meyer, M.T., Pradella, M., Hinck, D., Sauter, A.W., Heye, T., Boll, D.T., Cyriac, J., Yang, S.: TotalSegmentator: robust segmentation of 104 anatomic structures in CT images. Radiology: Artificial Intelligence 5 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Myronenko, A., Siddiquee, M.M.R., Yang, D., He, Y., Xu, D.: Automated head and neck tumor segmentation from 3D PET/CT HECKTOR 2022 challenge report. 3D Head and Neck Tumor Segmentation in PET/CT Challenge, pp. 31-37. Springer (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Myronenko, A.: 3D MRI brain tumor segmentation using autoencoder regularization. In: Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 4th International Workshop, BrainLes 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Revised Selected Papers, Part II 4, pp. 311-320. Springer (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Tang, Y., Gao, R., Lee, H.H., Han, S., Chen, Y., Gao, D., Nath, V., Bermudez, C., Savona, M.R., Abramson, R.G.: High-resolution 3D abdominal segmentation with random patch network fusion. Medical image analysis 69, 101894 (2021)

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 12 05:28:57 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
