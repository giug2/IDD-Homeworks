<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Effects of Using Synthetic Data on Deep Recommender Models’ Performance</title>
<!--Generated on Wed Jun 26 12:06:29 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.18286v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#S1" title="In Effects of Using Synthetic Data on Deep Recommender Models’ Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#S2" title="In Effects of Using Synthetic Data on Deep Recommender Models’ Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#S3" title="In Effects of Using Synthetic Data on Deep Recommender Models’ Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Synthetic Data Generation Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#S4" title="In Effects of Using Synthetic Data on Deep Recommender Models’ Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#S5" title="In Effects of Using Synthetic Data on Deep Recommender Models’ Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#S6" title="In Effects of Using Synthetic Data on Deep Recommender Models’ Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Effects of Using Synthetic Data on Deep Recommender Models’ Performance
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Fatih Cihan Taskin 
<br class="ltx_break"/>AI Enablement 
<br class="ltx_break"/>Huawei Türkiye R&amp;D Center 
<br class="ltx_break"/>Istanbul, Turkey 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">fatih.cihan.taskin@huawei.com</span>
<br class="ltx_break"/>&amp;Ilknur Akcay 
<br class="ltx_break"/>AI Enablement 
<br class="ltx_break"/>Huawei Türkiye R&amp;D Center 
<br class="ltx_break"/>Istanbul, Turkey
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">ilknur.akcay@huawei.com</span>
<br class="ltx_break"/>&amp;Muhammed Pesen 
<br class="ltx_break"/>AI Enablement 
<br class="ltx_break"/>Huawei Türkiye R&amp;D Center 
<br class="ltx_break"/>Istanbul, Turkey 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.3.id3">muhammedpesenn@gmail.com</span>
<br class="ltx_break"/>&amp;      Said Aldemir 
<br class="ltx_break"/>       AI Enablement 
<br class="ltx_break"/>         Huawei Türkiye R&amp;D Center 
<br class="ltx_break"/>       Istanbul, Turkey 
<br class="ltx_break"/>       <span class="ltx_text ltx_font_typewriter" id="id4.4.id4">said.aldemir1@huawei.com</span>
<br class="ltx_break"/>&amp;      Ipek Iraz Esin 
<br class="ltx_break"/>      AI Enablement 
<br class="ltx_break"/>      Huawei Türkiye R&amp;D Center 
<br class="ltx_break"/>      Istanbul, Turkey 
<br class="ltx_break"/>      <span class="ltx_text ltx_font_typewriter" id="id5.5.id5">ipek.iraz.esin@huawei.com</span>
<br class="ltx_break"/>&amp;Furkan Durmus 
<br class="ltx_break"/>AI Enablement 
<br class="ltx_break"/>Huawei Türkiye R&amp;D Center 
<br class="ltx_break"/>Istanbul, Turkey 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id6.6.id6">furkan.durmus2@huawei.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1">Recommender systems are essential for enhancing user experiences by suggesting items based on individual preferences. However, these systems frequently face the challenge of data imbalance, characterized by a predominance of negative interactions over positive ones. This imbalance can result in biased recommendations favoring popular items. This study investigates the effectiveness of synthetic data generation in addressing data imbalances within recommender systems. Six different methods were used to generate synthetic data. Our experimental approach involved generating synthetic data using these methods and integrating the generated samples into the original dataset. Our results show that the inclusion of generated negative samples consistently improves the Area Under the Curve (AUC) scores. The significant impact of synthetic negative samples highlights the potential of data augmentation strategies to address issues of data sparsity and imbalance, ultimately leading to improved performance of recommender systems.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p" id="p1.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="p1.1.1">K</em><span class="ltx_text ltx_font_bold" id="p1.1.2">eywords</span> Recommender Systems, Click-Through Rate Prediction, Synthetic Data Generation</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recommender systems have become significant components of digital advertising platforms that recommend the most suitable ads to users. Especially with the development of deep learning algorithms, modeling complex user behaviors has been attempted to increase both the revenue of companies and improve user satisfaction by enhancing the success of recommender models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib1" title="">1</a>]</cite>. The main goal of a recommender model is the accurate prediction of Click-Through Rates (CTR), which measure the likelihood of a user interacting with a given item. Since the click rate of users on the displayed advertisements is generally low <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib2" title="">2</a>]</cite>, the dataset used in training the models will be quite imbalanced. This imbalance poses substantial challenges like popularity bias, user activity bias, and poor generalization, hence lower recommendation quality <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib4" title="">4</a>]</cite>. Disparities in user activity and item popularity can negatively affect a model’s learning process, creating a self-reinforcing cycle where already popular items gain even more prominence. This phenomenon, known as a feedback loop, prioritizes the preferences of highly active users while overlooking those of less engaged individuals <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib5" title="">5</a>]</cite>. As a result, the user experience suffers due to increased homogeneity in recommendations, which limits exposure to diverse content. Moreover, this loop hinders the visibility of niche items, ultimately undermining the long-term objectives of promoting diversity and ensuring fairness within the recommendation system.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To address the data imbalance problem, two main approaches are typically employed: resampling strategies and weighting mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib6" title="">6</a>]</cite>. Resampling techniques, such as oversampling the minority class or undersampling the majority class, aim to equalize the class distribution by manipulating the training data. On the other hand, weighting mechanisms assign different weights to the samples depending on the class so that the minority class becomes more important during the training process. Although traditional methods have been effective in mitigating the impact of unbalanced datasets, the use of synthetic data generation has gained popularity in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib7" title="">7</a>]</cite> due to its ability to create large, diverse, and balanced datasets. This approach not only helps improve the performance of machine learning models but also addresses limitations such as data scarcity and privacy concerns. Simple oversampling methods and generative models are mostly used to create artificial samples that resemble the characteristics of the minority class. The imbalance between the classes can be mitigated by augmenting the training data with these synthetically generated samples, leading to improved model performance and generalization.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">The use of synthetic data generation techniques is primarily associated with data privacy concerns in deep learning context <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib8" title="">8</a>]</cite>. By creating artificial data that mimics the characteristics of the original dataset, companies can ensure the anonymity of sensitive information while still conducting meaningful analysis. This approach has proven useful in maintaining confidentiality and mitigating potential risks associated with data privacy concerns. However, in this study, we pursue the research question of how the success of CTR prediction tasks is affected by synthetic data generation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In our study, we employed a variety of techniques to generate synthetic datasets to augment our original data. The methods used include random oversampling, several variants of the Synthetic Minority Over-sampling Technique (SMOTE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib9" title="">9</a>]</cite>, Conditional Tabular Generative Adversarial Network (CTGAN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib10" title="">10</a>]</cite>, Gaussian Copula <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib11" title="">11</a>]</cite>, Copula Generative Adversarial Network (Copula GAN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib11" title="">11</a>]</cite>, Tabular Variational AutoEncoder (TVAE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib10" title="">10</a>]</cite>, and the Tabular Diffusion Probabilistic Model (TabDDPM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib12" title="">12</a>]</cite>. It aims to observe the impact of synthetic data by exploring several scenarios and a variety of techniques. In different synthetic data generation scenarios, only positive samples were produced and added to the original data at the rate of 25% and 50%, only negative samples were produced and added to the original data at the rate of 25% and 50%, and also hybrid only datasets with the same amount of original data belonging to both labels were produced and used instead of the original dataset. As a result, 5 different datasets were created and evaluated by training CTR prediction models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Subsequently, we conducted extensive experiments to assess how the inclusion of these synthetic datasets influenced the offline performance of our deep recommender models. The aim was to determine which synthetic data generation technique and scenario enhanced the model’s performance most effectively, providing insights into the optimal strategies for data augmentation in recommender systems.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="234" id="S1.F1.g1" src="extracted/5692468/MainFigure.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A real dataset is augmented with synthetic data generated from the original dataset to enhance the training process. This combined dataset undergoes an embedding operation to transform sparse features to embedding vectors. Following this, the input embedding matrix is fed into the CTR prediction model, which uses the embedded information to predict the likelihood of a user clicking on an advertisement by capturing complex relationships among the features.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In deep learning, the quality and quantity of data have a direct impact on the accuracy and generalization capability of the model. Generating synthetic data is a solution to critical problems such as data scarcity, high collection costs, and privacy concerns. By creating realistic and diverse datasets, synthetic data facilitates the development of deep learning models in various domains like computer vision and natural language processing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib17" title="">17</a>]</cite>. This approach not only improves the performance of the models but also enables the inclusion of rare and unusual scenarios, ensuring better adaptability and reliability in real-world applications. While GANs and VAEs are generally used previously, diffusion models have become widespread as an alternative to GANs in recent years and have shown success in many different fields <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib18" title="">18</a>]</cite>. It is claimed that diffusion models are more successful compared to GANs, but problems such as diffusion models being more data greedy and more costly are also mentioned <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib20" title="">20</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Vallelado et al. proposed the use of diffusion models with transformer conditioning for data imputation and generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib21" title="">21</a>]</cite>. Diffusion models, known for capturing complex data distributions, are enhanced with transformers to model dependencies and long-range interactions within tabular data. Their approach was evaluated against state-of-the-art techniques such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) on benchmark datasets. For data imputation, the models’ accuracy in estimating missing values while preserving data distribution was assessed. For data generation, the quality and diversity of synthetic data samples were evaluated.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Endres et al. conducted a comparative study on 3 tabular datasets by using GAN-based, SMOTE, and VAE-based models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib22" title="">22</a>]</cite>. The produced datasets were evaluated by statistical metrics and processing time. However, the generated datasets were not used in deep learning model training. Additionally, they did not consider the numeric and categorical features, especially for the SMOTE method.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Slokom et al. proposed SynRec, which is a data protection framework that uses data synthesis to protect sensitive information in recommender systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib23" title="">23</a>]</cite>. It replaces original values with synthetic ones or generates new users, ensuring that sensitive information is concealed while maintaining data usability for comparing recommender systems. This enables companies to safely share data with researchers for algorithm development and collaborative research. The paper demonstrates feasibility of the concept through preliminary experiments and outlines future challenges for practical implementation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">Noble et al. proposed two metrics, Identifiability (measuring privacy risk) and Realism (comparing recommendation performance between real and synthetic data), because there’s a trade-off between the detailed user data needed for high-quality recommendations and privacy concerns <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib24" title="">24</a>]</cite>. Analyzing seven data generation algorithms for movies and songs across 28 settings, they constructed Pareto frontiers of Realism vs. Identifiability, offering insights to guide future synthetic data research.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Synthetic Data Generation Methods</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.p1.1.1">SMOTEN</span>:
SMOTE (Synthetic Minority Over-sampling Technique) is a synthetic data generation method that aims to balance of imbalanced datasets. To generate data, k-nearest neighbors are found for each sample in the minority class. While these neighbors can typically be found using the Euclidean distance, they can also be found using different distance metrics. The action is taken according to the number of neighbors specified in the parameter. Synthetic data is produced according to interpolation between the sample data and the selected neighbor. Different SMOTE algorithms can be used depending on the type of dataset used and the selection of neighborhoods. SMOTE algorithm works with numeric datasets. There are also SMOTE variants that work with different data types. In this study, the SMOTEN method (Synthetic Minority Over-sampling Technique for Nominal Features) is specifically tailored to handle categorical data by creating synthetic samples through a process that respects the nominal nature of the features. To address memory constraints, we selected a small value for the k-neighbors parameter, setting it to 2. This choice helped to manage computational resources effectively. In addition, the dataset was divided into 4 chunks containing around 50000 samples each due to memory constraints. The SMOTEN algorithm is run for each chunk, and finally, all chunks are merged.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.p2.1.1">CT-GAN</span>:
CTGAN is a generative adversarial network (GAN) architecture designed to generate synthetic tabular data. CTGAN focuses on learning the joint distribution of the data through adversarial training, aiming to generate structured data that closely resembles the statistical properties of the original dataset in terms of feature correlations and distributions. It consists of two main components: a generator and a discriminator. The generator takes random noise as input and transforms it into synthetic data points. It generates synthetic data that resembles the statistical properties in the original dataset. The discriminator aims to distinguish between samples from the original dataset and synthetic samples produced by the generator and provides feedback to the generator on how realistic its generated samples are. Since these two components are engaged in an adversarial training process, while the discriminator improves at identifying synthetic data, the generator generates gradually higher-quality synthetic data. CTGAN is beneficial for applications that need synthetic data generation, such as data augmentation, privacy-preserving data sharing, and machine learning model testing. For the model structure, the embedding dimension was 128, with both the generator and discriminator dimensions set to (256, 256). The learning rates for both the generator and discriminator were 0.0002, with a decay of 1e-06 for each. The batch size was 500, and the discriminator steps were set to 1. Logging frequency was enabled, while verbose mode was disabled. The number of epochs was set to 300, with a pack size (PAC) of 10. CUDA was enabled for computation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.p3.1.1">CopulaGAN</span>:
CopulaGAN is a generative adversarial network (GAN) architecture designed to generate synthetic tabular data that preserves complex relationships between variables. CopulaGAN uses copulas to describe the dependence structure between random variables while separately modeling their marginal distributions. With this concept, synthetic data that preserves the marginal distributions and dependencies observed in the original dataset is generated. Unlike CTGAN, CopulaGAN focuses on capturing the joint distribution of variables more explicitly through the use of copulas. Since CopulaGAN separates the modeling of marginal distributions and dependencies, it effectively captures complex relationships in tabular data, leading to more realistic and convenient synthetic datasets compared to traditional GAN approaches. CopulaGAN, like CTGAN, is beneficial for applications that need synthetic data generation, such as data augmentation, privacy-preserving data sharing, and machine learning model testing. For the model structure, the embedding dimension was 128, with both the generator and discriminator dimensions set to (256, 256). The learning rates for both the generator and discriminator were 0.0002, with a decay of 1e-06 for each. The batch size was 500, and the discriminator steps were set to 1. Logging frequency was enabled, while verbose mode was disabled. The number of epochs was set to 300, with a pack size (PAC) of 10. CUDA was enabled for computation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p4">
<p class="ltx_p" id="S3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.p4.1.1">TVAE</span>:
The Tabular Variational Autoencoder (TVAE) model was developed to produce artificial tabular data. By utilizing the concepts of variational autoencoders (VAEs), TVAE is able to provide synthetic data that closely resembles the statistical characteristics and patterns found in the original dataset by capturing both the joint and conditional distributions of data in a tabular format. The architecture of the model is comprised of two parts: an encoder that converts actual tabular data into a latent space representation and a decoder that uses this latent representation to reconstruct the tabular data. TVAE generates accurate and diverse synthetic data by using a specialized loss function that combines Kullback-Leibler (KL) divergence and reconstruction loss. For the model structure, the embedding dimension was 128, with both the compress and decompress dimensions set to (128, 128). The L2 regularization parameters were 1e-06. The batch size was 500. The number of epochs was set to 300. CUDA was enabled for computation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p5">
<p class="ltx_p" id="S3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.p5.1.1">TabDDPM</span>:
Diffusion models are one of the relatively new classes of generative models that have shown promise in addressing class imbalances in datasets. By employing a technique similar to reverse Markov chains, they gradually convert random noise into samples that resemble the targeted collection of data. These models produce complex and varied synthetic samples that are similar to real data through iterative adjustments.
Diffusion models are particularly useful because they can generate high-quality synthetic examples. They can significantly enhance the representation of insufficiently represented classes in the training data through the generation of synthetic samples. These models can improve the performance of machine learning tasks and effectively address class imbalances when included in the data preprocessing pipeline. The model structure used in this study includes a multi-layer perceptron (MLP) with layers [512, 1024, 1024, 1024, 1024, 1024, 512] and no dropout. The diffusion process is conducted over 100 timesteps with a Gaussian loss type set to "mse" and a scheduler set to "cosine." For training, normalization is performed using the quantile method.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p6">
<p class="ltx_p" id="S3.p6.1"><span class="ltx_text ltx_font_bold" id="S3.p6.1.1">Gaussian Copula</span>:
Gaussian Copula treats each column in data as a variable and uses multivariate probability distribution models to determine the relationship between variables. Firstly, the marginal distribution of each variable is determined. Then, multivariate normal distribution is used to see the relationship of the variables with each other. Gaussian Copula combines these two distributions to establish a relationship between the data. This allows us to capture linear dependencies between variables. For the model structure, numerical distributions was None and default distributions was ’beta’.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To evaluate the impact of synthetic data on recommendation models, we utilized the Frappe<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.baltrunas.info/context-aware</span></span></span> dataset. Additionally, we used FuxiCTR<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://fuxictr.github.io/tutorials/v2.0/</span></span></span> repository <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib26" title="">26</a>]</cite> due to its ability to ensure the repeatability and reliability of our experiments, which is crucial for validating our findings. The experimental environment was configured with the following specifications: Operating System: Ubuntu 22.04.3 LTS, Python Version: 3.9.7, Python Distribution: Anaconda3, RAM: 252 Gb, CPU: Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz, and GPU: NVIDIA TITAN RTX.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Five distinct scenarios were designed to systematically observe the effects of synthetic data on the recommendation models. For each scenario, it is ensured that only the training data is changed according to the synthetic data generation scenario, and the validation and test sets are kept unchanged to maintain unbiased results. By isolating the changes to just the training data, we can accurately assess how those specific modifications impact the model’s performance on unseen data. Keeping the validation and test sets consistent across all scenarios allows for a fair comparison of the different synthetic data generation techniques. These scenarios are described as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.p3.1.1">Scenario 1 (S1)</span>: In this scenario, synthetic positive samples amounting to 25% of the original number of positive samples in the dataset were generated. These synthetic samples were then added to the original dataset. The purpose was to observe the effect of a modest increase in positive interactions on the model’s performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p4">
<p class="ltx_p" id="S4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.p4.1.1">Scenario 2 (S2)</span>: Here, synthetic positive samples equal to 50% of the original positive samples were created and incorporated into the original dataset. This scenario aimed to investigate the impact of a more substantial addition of positive data on the recommendation model.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p5">
<p class="ltx_p" id="S4.p5.1"><span class="ltx_text ltx_font_bold" id="S4.p5.1.1">Scenario 3 (S3)</span>: For this scenario, we introduced synthetic negative samples equivalent to 25% of the original number of negative samples. These were added to the dataset to understand how a slight increase in negative interactions influences the model’s recommendations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p6">
<p class="ltx_p" id="S4.p6.1"><span class="ltx_text ltx_font_bold" id="S4.p6.1.1">Scenario 4 (S4)</span>: In this case, synthetic negative samples amounting to 50% of the original negative samples were generated and added to the dataset. This scenario was designed to evaluate the effects of a significant increase in negative data on the model’s accuracy and reliability.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p7">
<p class="ltx_p" id="S4.p7.1"><span class="ltx_text ltx_font_bold" id="S4.p7.1.1">Scenario 5 (S5)</span>: This scenario involved generating a completely synthetic dataset that matched the original dataset in size. The aim was to assess the performance of the recommendation models when trained on entirely synthetic data, comparing it directly with models trained on the original dataset in terms of the CTR prediction performance. When the SMOTE algorithm generates data, it creates a new dataset by adding synthetic data over the original data. Therefore, when running the S5 scenario with the SMOTE algorithm, firstly generated a dataset that is approximately 2 times the size of the existing dataset. The generated dataset contained both the original data and the synthetic data. The original data was removed from this dataset, and only the synthetic data was used.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p8">
<p class="ltx_p" id="S4.p8.1">In scenarios S1, S2, S3, and S4, we could not create synthetic data that had only positive or negative samples. Instead, we generated a hybrid dataset in S5 that consisted of only synthetic data and then extracted the positive and negative samples from this hybrid dataset to incorporate the desired proportions into the original dataset. By comparing these scenarios, we aimed to thoroughly understand how different proportions and types of synthetic data affect the performance of recommendation models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p9">
<p class="ltx_p" id="S4.p9.1">Since the presence of some feature combinations during synthetic data production disrupts the data distribution, a constraint has been introduced for these combinations. For example, in the country and city features in one sample, the country is Brazil, but the city is Istanbul, which disrupts the distribution of the data and increases noise. To prevent this, a feature-by-feature analysis was performed to analyze which features had unique values and which did not, and the constraint was used in CTGAN, TVAE, CopulaGAN, and GaussianCopula methods to prevent combinations that were not in the original data from occurring in synthetic data.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>This table shows the number of samples that identically match between the synthetic dataset and original dataset for each method.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1">
<tr class="ltx_tr" id="S4.T1.1.1">
<td class="ltx_td" id="S4.T1.1.1.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.2">Gaussian Copula</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.3">TVAE</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4">CTGan</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5">CopulaGan</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6">Diffusion</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7">Smote</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.2.1">Identical Samples</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.2.2">30</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.2.3">1226</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.2.4">489</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.2.5">292</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.2.6">755</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.2.7">41272</td>
</tr>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S4.p10">
<p class="ltx_p" id="S4.p10.1">Figure 1 shows the main pipeline of synthetic data generation and training of CTR prediction models. A new dataset is created by combining synthetic data derived from the original dataset to improve the model training process. This combined dataset is then subjected to an embedding operation, transforming sparse features into embedding vectors. Subsequently, the resultant input embedding matrix is fed into the CTR prediction model, which leverages the embedded information to predict the likelihood of a user clicking on an advertisement by capturing complex interactions among the features. 3 different recommender models are chosen: DNN, DeepFM, and Masknet which are described briefly below.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p11">
<p class="ltx_p" id="S4.p11.1"><span class="ltx_text ltx_font_bold" id="S4.p11.1.1">DNN</span>: Deep Neural Network (DNN) is the baseline CTR prediction model that consists of multiple hidden layers between the input and output layers. It is used to learn complex patterns and interactions from sparse feature inputs in CTR prediction problems.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p12">
<p class="ltx_p" id="S4.p12.1"><span class="ltx_text ltx_font_bold" id="S4.p12.1.1">DeepFM</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib27" title="">27</a>]</cite>: DeepFM is a hybrid model combining the strengths of Factorization Machines (FM) and DNNs in the CTR prediction task. The FM component is responsible for modeling low-order feature interactions, whereas the DNN component learns high-order feature interactions through multiple hidden layers. The outputs of the FM component and DNN component are combined through concatenation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p13">
<p class="ltx_p" id="S4.p13.1"><span class="ltx_text ltx_font_bold" id="S4.p13.1.1">MaskNet</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#bib.bib28" title="">28</a>]</cite>: MaskNet model proposed a novel feature masking mechanism to perform an adaptive selection of the most relevant features for each input.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance comparison of CTR prediction models trained on datasets from 5 different scenarios, produced using 6 different synthetic methods in terms of AUC score. For each synthetic data generation scenario, 3 different CTR prediction models were trained. Bold values indicate the highest results among different scenarios for each model.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.1" style="width:498.0pt;height:295.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.0pt,5.4pt) scale(0.965,0.965) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1">Scenario</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S4.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1">GaussianCopula</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S4.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.3.1">TVAE</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S4.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.4.1">CTGAN</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.1.1">DeepFM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.2.1">DNN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.3.1">MaskNet</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.4.1">DeepFM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.5.1">DNN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.6.1">MaskNet</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.7"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.7.1">DeepFM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.8"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.8.1">DNN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.9"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.9.1">MaskNet</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.3.1">Original</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.2">0.98409</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.3">0.98396</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.4">0.98329</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.5">0.98409</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.6">0.98396</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.7">0.98329</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.8">0.98409</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.9">0.98396</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.10">0.98329</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.4.1">S1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.2">0.98233</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.3">0.98245</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.4">0.98131</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.5">0.98307</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.6">0.98216</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.7">0.98144</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.8">0.98244</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.9">0.98163</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.10">0.98098</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.5.1">S2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.2">0.98231</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.3">0.98192</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.4">0.98143</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.5">0.98191</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.6">0.98247</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.7">0.98061</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.8">0.98213</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.9">0.98172</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.10">0.98157</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.6.1">S3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.6.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.6.2.1">0.98480</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.6.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.6.3.1">0.98510</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.6.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.6.4.1">0.98447</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.6.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.6.5.1">0.98417</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.6.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.6.6.1">0.98456</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.6.7"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.6.7.1">0.98403</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.6.8">0.98454</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.6.9">0.98420</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.6.10"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.6.10.1">0.98395</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.7.1">S4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.2">0.98461</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.3">0.98419</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.4">0.98438</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.5">0.98389</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.6">0.98332</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.7">0.98284</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.8"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.7.8.1">0.98461</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.9"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.7.9.1">0.98485</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.10">0.98384</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.8.1">S5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.8.2">0.78847</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.8.3">0.78935</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.8.4">0.79928</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.8.5">0.73345</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.8.6">0.73176</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.8.7">0.73352</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.8.8">0.78643</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.8.9">0.78876</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.8.10">0.79026</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.9">
<td class="ltx_td ltx_border_t" id="S4.T2.1.1.9.1"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.1.1.9.2"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.1.1.9.3"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.1.1.9.4"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.1.1.9.5"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.1.1.9.6"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.1.1.9.7"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.1.1.9.8"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.1.1.9.9"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.1.1.9.10"></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.10.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.10.1.1">Scenario</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S4.T2.1.1.10.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.10.2.1">CopulaGAN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S4.T2.1.1.10.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.10.3.1">Diffusion</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S4.T2.1.1.10.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.10.4.1">SMOTEN</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.11">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.11.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.11.1.1">DeepFM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.11.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.11.2.1">DNN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.11.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.11.3.1">MaskNet</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.11.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.11.4.1">DeepFM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.11.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.11.5.1">DNN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.11.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.11.6.1">MaskNet</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.11.7"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.11.7.1">DeepFM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.11.8"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.11.8.1">DNN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.11.9"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.11.9.1">MaskNet</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.12">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.12.1">Original</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.12.2">0.98409</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.12.3">0.98396</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.12.4">0.98329</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.12.5">0.98409</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.12.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.12.6.1">0.98396</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.12.7">0.98329</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.12.8">0.98430</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.12.9">0.98396</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.12.10">0.98329</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.13">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.13.1">S1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.13.2">0.98118</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.13.3">0.98167</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.13.4">0.98069</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.13.5">0.97920</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.13.6">0.98178</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.13.7">0.97759</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.13.8">0.98386</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.13.9">0.98398</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.13.10">0.98316</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.14">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.14.1">S2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.14.2">0.98218</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.14.3">0.98207</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.14.4">0.98078</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.14.5">0.98061</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.14.6">0.98096</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.14.7">0.97501</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.14.8">0.98381</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.14.9">0.98357</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.14.10">0.98348</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.15">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.15.1">S3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.15.2">0.98390</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.15.3">0.98445</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.15.4">0.98320</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.15.5">0.98438</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.15.6">0.98199</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.15.7">0.98317</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.15.8">0.98387</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.15.9"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.15.9.1">0.98433</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.15.10">0.98333</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.16">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.16.1">S4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.16.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.16.2.1">0.98500</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.16.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.16.3.1">0.98498</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.16.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.16.4.1">0.98369</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.16.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.16.5.1">0.98455</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.16.6">0.98450</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.16.7"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.16.7.1">0.98402</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.16.8"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.16.8.1">0.98476</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.16.9">0.98413</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.16.10"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.16.10.1">0.98399</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.17">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S4.T2.1.1.17.1">S5</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.17.2">0.74251</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.17.3">0.73928</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.17.4">0.74523</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.17.5">0.74668</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.17.6">0.74287</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.17.7">0.75931</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.17.8">0.96481</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.17.9">0.96399</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.17.10">0.96287</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this study, we explored the impact of synthetic data generation techniques on the performance of deep recommender systems. For this purpose, we employed 6 methods: Synthetic Minority Over-sampling Technique for Nominal features (SMOTEN), Conditional Generative Adversarial Networks (CTGAN), Copula Generative Adversarial Network (CopulaGAN), Tabular Variational Autoencoders (TVAE), Gaussian Copula and Tabular Denoising Diffusion Probabilistic Model (TabDDPM) to generate synthetic data and evaluated their influence on CTR prediction performance by comparing the Area Under Curve (AUC) metric by using synthetic and original datasets in 5 different scenarios with 3 different deep recommender models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">According to the experimental results, the AUC increased in scenarios S3 and S4, except for the DNN model when using the TabDDPM method. The dataset has a CTR rate of 0.33, meaning it is highly imbalanced with few positive samples. Despite this, generating more positive samples did not improve the AUC for any method and model. This shows that trying to balance the dataset by adding positive samples is ineffective. Interestingly, even though there were more negative samples, adding synthetic negative samples did increase the AUC score.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Scenario S5 shows the results of models trained on only synthetic data without using any original data. This scenario aims to assess how well the generated datasets match the real dataset. The key point here is that SMOTEN produced the most similar results to the original data and the highest AUC scores in S5. However, it is important to note that SMOTEN essentially copies the original dataset and produces nearly identical samples, which is not desired. If the goal of using synthetic data were to address privacy concerns, SMOTEN would fail. Higher AUC scores do not mean that SMOTEN is successful in this context. Table <a class="ltx_ref" href="https://arxiv.org/html/2406.18286v1#S4.T1" title="Table 1 ‣ 4 Experimental Setup ‣ Effects of Using Synthetic Data on Deep Recommender Models’ Performance"><span class="ltx_text ltx_ref_tag">1</span></a> shows the number of identical samples within the real dataset. Clearly, it can be realized that SMOTEN copies most of the samples identically by comparing the other methods. This can be the reason why it gets higher AUC results for S5.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Overall, the Gaussian Copula method works better in generating synthetic datasets. In the S5 scenario, AUC values get higher among all methods if we exclude SMOTE due to the given reason above. Regarding the GaussianCopula model, which is the best-performing one, the original AUC values for DeepFM, DNN, and MaskNet are 98.409%, 98.396%, and 98.329%, respectively. In the best-performing scenario (S3), the AUC values increase to 98.480% for DeepFM, 98.510% for DNN, and 98.447% for MaskNet. This represents an AUC increase of 0.071% for DeepFM, 0.114% for DNN, and 0.118% for MaskNet.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">This study has limitations that open doors for further research. The experiments were likely conducted on a specific dataset and recommender system architecture. Exploring the applicability and generalizability of these findings across different datasets and model architectures would be valuable. Additionally, investigating the impact of synthetic data generation on other recommendation metrics beyond AUC, such as novelty or diversity, would provide a more comprehensive understanding of its effects.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">While our study demonstrates the potential benefits of synthetic data generation in improving deep recommender system performance, several limitations need to be addressed in future research. First, the quality of the synthetic data heavily depends on the generation method and the complexity of the original dataset. More research is needed to determine the optimal generation method for different types of datasets and recommendation tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p7">
<p class="ltx_p" id="S5.p7.1">Lastly, the generalizability of our findings across different domains and datasets should be explored. Further experimentation on a broader range of datasets and recommendation scenarios will provide a more comprehensive understanding of the effectiveness and limitations of synthetic data generation in deep recommender systems.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Our findings indicate that the most significant performance improvement in terms of the AUC scores was observed when only generated negative samples were added to the original dataset. Adding only positive samples or both types of samples, whether combined with the original data or used separately, did not yield the same level of improvement.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">However, it is important to note that generating synthetic datasets is a high-cost process that requires substantial computational resources and careful design. Therefore, it is crucial to consider the potential benefits and costs very carefully before implementing synthetic data generation in practical applications. Furthermore, our experiments were conducted in an offline setting, which may not fully capture the complexities and dynamics of real-world usage. To validate these findings, it is essential to conduct online experiments and A/B testing to observe the effects in a live environment. This will help to ensure that the improvements seen offline translate to actual user interactions and enhance the overall effectiveness of recommender systems.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Future work should investigate the underlying reasons why negative samples have a more substantial impact on CTR model performance and should explore different data generatio methods to further optimize synthetic data generation for recommender systems. Additionally, it would be beneficial to test these findings across different domains and datasets to ensure their generalizability.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay.

</span>
<span class="ltx_bibblock">Deep learning based recommender system: A survey and new perspectives.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">ACM Computing Surveys</span>, 52(1):1–38, February 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Matthew Richardson, Ewa Dominowska, and Robert Ragno.

</span>
<span class="ltx_bibblock">Predicting clicks: estimating the click-through rate for new ads.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 16th international conference on World Wide Web</span>, pages 521–530, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Tianxin Wei, Fuli Feng, Jiawei Chen, Ziwei Wu, Jinfeng Yi, and Xiangnan He.

</span>
<span class="ltx_bibblock">Model-agnostic counterfactual reasoning for eliminating popularity bias in recommender system.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</span>, KDD ’21, page 1791–1800, New York, NY, USA, 2021. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He.

</span>
<span class="ltx_bibblock">Bias and debias in recommender system: A survey and future directions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">ACM Trans. Inf. Syst.</span>, 41(3), feb 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Masoud Mansoury, Himan Abdollahpouri, Mykola Pechenizkiy, Bamshad Mobasher, and Robin Burke.

</span>
<span class="ltx_bibblock">Feedback loop and bias amplification in recommender systems, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Haibo He and Edwardo A. Garcia.

</span>
<span class="ltx_bibblock">Learning from imbalanced data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">IEEE Transactions on Knowledge and Data Engineering</span>, 21(9):1263–1284, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Joao Fonseca and Fernando Bacao.

</span>
<span class="ltx_bibblock">Tabular and latent space synthetic data generation: a literature review.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Journal of Big Data</span>, 10(1):115, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Yingzhou Lu, Minjie Shen, Huazheng Wang, Xiao Wang, Capucine van Rechem, Tianfan Fu, and Wenqi Wei.

</span>
<span class="ltx_bibblock">Machine learning for synthetic data generation: A review, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Kevin W. Bowyer, Nitesh V. Chawla, Lawrence O. Hall, and W. Philip Kegelmeyer.

</span>
<span class="ltx_bibblock">SMOTE: synthetic minority over-sampling technique.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">CoRR</span>, abs/1106.1813, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni.

</span>
<span class="ltx_bibblock">Modeling tabular data using conditional gan.

</span>
<span class="ltx_bibblock">In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Advances in Neural Information Processing Systems</span>, volume 32. Curran Associates, Inc., 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Neha Patki, Roy Wedge, and Kalyan Veeramachaneni.

</span>
<span class="ltx_bibblock">The synthetic data vault.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">IEEE International Conference on Data Science and Advanced Analytics (DSAA)</span>, pages 399–410, Oct 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko.

</span>
<span class="ltx_bibblock">Tabddpm: Modelling tabular data with diffusion models, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Firas Khader, Gustav Müller-Franzes, Soroosh Tayebi Arasteh, Tianyu Han, Christoph Haarburger, Maximilian Schulze-Hagen, Philipp Schad, Sandy Engelhardt, Bettina Baeßler, Sebastian Foersch, Johannes Stegmaier, Christiane Kuhl, Sven Nebelung, Jakob Nikolas Kather, and Daniel Truhn.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models for 3d medical image generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Scientific Reports</span>, 13(1), May 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
John T. Guibas, Tejpal S. Virdi, and Peter S. Li.

</span>
<span class="ltx_bibblock">Synthetic medical images from dual generative adversarial networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">CoRR</span>, abs/1709.01872, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Hoo-Chang Shin, Neil A. Tenenholtz, Jameson K. Rogers, Christopher G. Schwarz, Matthew L. Senjem, Jeffrey L. Gunter, Katherine P. Andriole, and Mark Michalski.

</span>
<span class="ltx_bibblock">Medical image synthesis for data augmentation and anonymization using generative adversarial networks.

</span>
<span class="ltx_bibblock">In Ali Gooya, Orcun Goksel, Ipek Oguz, and Ninon Burgos, editors, <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Simulation and Synthesis in Medical Imaging</span>, pages 1–11, Cham, 2018. Springer International Publishing.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Michael Loecher, Luigi E. Perotti, and Daniel B. Ennis.

</span>
<span class="ltx_bibblock">Using synthetic data generation to train a cardiac motion tag tracking neural network.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Medical Image Analysis</span>, 74:102223, December 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Youssef Skandarani, Pierre-Marc Jodoin, and Alain Lalande.

</span>
<span class="ltx_bibblock">Gans for medical image synthesis: An empirical study.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Journal of Imaging</span>, 9(3):69, March 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Jonathan Ho, Ajay Jain, and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">CoRR</span>, abs/2006.11239, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Prafulla Dhariwal and Alexander Nichol.

</span>
<span class="ltx_bibblock">Diffusion models beat gans on image synthesis.

</span>
<span class="ltx_bibblock">In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Advances in Neural Information Processing Systems</span>, volume 34, pages 8780–8794. Curran Associates, Inc., 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Muhammad Usman Akbar, Måns Larsson, Ida Blystad, and Anders Eklund.

</span>
<span class="ltx_bibblock">Brain tumor segmentation using synthetic mr images - a comparison of gans and diffusion models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Scientific Data</span>, 11(1), February 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Mario Villaizán-Vallelado, Matteo Salvatori, Carlos Segura, and Ioannis Arapakis.

</span>
<span class="ltx_bibblock">Diffusion models for tabular data imputation and synthetic data generation, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Markus Endres, Asha Mannarapotta Venugopal, and Tung Son Tran.

</span>
<span class="ltx_bibblock">Synthetic data generation: A comparative study.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">International Database Engineered Applications Symposium</span>, IDEAS’22. ACM, August 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Manel Slokom.

</span>
<span class="ltx_bibblock">Comparing recommender systems using synthetic data.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 12th ACM Conference on Recommender Systems</span>, RecSys ’18, page 548–552, New York, NY, USA, 2018. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Isaac Noble, Ivan Vendrov, Xinyuan Xu, and Deepak Ramachandran.

</span>
<span class="ltx_bibblock">Realistic but non-identifiable synthetic user data generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">EvalRS@KDD</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, and Xiuqiang He.

</span>
<span class="ltx_bibblock">Open benchmarking for click-through rate prediction.

</span>
<span class="ltx_bibblock">In Gianluca Demartini, Guido Zuccon, J. Shane Culpepper, Zi Huang, and Hanghang Tong, editors, <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">CIKM ’21: The 30th ACM International Conference on Information and Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021</span>, pages 2759–2769. ACM, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Jieming Zhu, Quanyu Dai, Liangcai Su, Rong Ma, Jinyang Liu, Guohao Cai, Xi Xiao, and Rui Zhang.

</span>
<span class="ltx_bibblock">BARS: towards open benchmarking for recommender systems.

</span>
<span class="ltx_bibblock">In Enrique Amigó, Pablo Castells, Julio Gonzalo, Ben Carterette, J. Shane Culpepper, and Gabriella Kazai, editors, <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">SIGIR ’22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022</span>, pages 2912–2923. ACM, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He.

</span>
<span class="ltx_bibblock">Deepfm: A factorization-machine based neural network for ctr prediction, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Zhiqiang Wang, Qingyun She, and Junlin Zhang.

</span>
<span class="ltx_bibblock">Masknet: Introducing feature-wise multiplication to ctr ranking models by instance-guided mask, 2021.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jun 26 12:06:29 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
