<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.11470] A Federated Learning Approach to Privacy Preserving Offensive Language Identification</title><meta property="og:description" content="The spread of various forms of offensive speech online is an important concern in social media. While platforms have been investing heavily in ways of coping with this problem, the question of privacy remains largely u…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Federated Learning Approach to Privacy Preserving Offensive Language Identification">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Federated Learning Approach to Privacy Preserving Offensive Language Identification">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.11470">

<!--Generated on Sun May  5 22:38:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">A Federated Learning Approach to Privacy Preserving 
<br class="ltx_break">Offensive Language Identification</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">The spread of various forms of offensive speech online is an important concern in social media. While platforms have been investing heavily in ways of coping with this problem, the question of privacy remains largely unaddressed. Models trained to detect offensive language on social media are trained and/or fine-tuned using large amounts of data often stored in centralized servers. Since most social media data originates from end users, we propose a privacy preserving decentralized architecture for identifying offensive language online by introducing Federated Learning (FL) in the context of offensive language identification. FL is a decentralized architecture that allows multiple models to be trained locally without the need for data sharing hence preserving users’ privacy. We propose a model fusion approach to perform FL. We trained multiple deep learning models on four publicly available English benchmark datasets (AHSD, HASOC, HateXplain, OLID) and evaluated their performance in detail. We also present initial cross-lingual experiments in English and Spanish. We show that the proposed model fusion approach outperforms baselines in all the datasets while preserving privacy.

<br class="ltx_break">
<br class="ltx_break">
<span id="id2.id1.1" class="ltx_text ltx_font_bold">Keywords: </span>federated learning, offensive language identification, privacy</p>
</div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\NAT@set@cites</span>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text"></span></p>
</div>
<div id="id1" class="ltx_logical-block">
<div id="id1.p1" class="ltx_para">
<p id="id1.p1.1" class="ltx_p ltx_align_center"><span id="id1.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">A Federated Learning Approach to Privacy Preserving 
<br class="ltx_break">Offensive Language Identification</span></p>
<br class="ltx_break ltx_centering">
<table id="id1.p1.2" class="ltx_tabular ltx_centering ltx_align_top">
<tr id="id1.p1.2.1" class="ltx_tr">
<td id="id1.p1.2.1.1" class="ltx_td ltx_align_center"><span id="id1.p1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:120%;">Marcos Zampieri<sup id="id1.p1.2.1.1.1.1" class="ltx_sup">1</sup>, Damith Premasiri<sup id="id1.p1.2.1.1.1.2" class="ltx_sup">2</sup>, Tharindu Ranasinghe<sup id="id1.p1.2.1.1.1.3" class="ltx_sup">3</sup>,</span></td>
</tr>
<tr id="id1.p1.2.2" class="ltx_tr">
<td id="id1.p1.2.2.1" class="ltx_td ltx_align_center">
<sup id="id1.p1.2.2.1.1" class="ltx_sup">1</sup>George Mason University, USA,
<sup id="id1.p1.2.2.1.2" class="ltx_sup">2</sup>Lancaster University, UK,
<sup id="id1.p1.2.2.1.3" class="ltx_sup">3</sup>Aston University, UK</td>
</tr>
<tr id="id1.p1.2.3" class="ltx_tr">
<td id="id1.p1.2.3.1" class="ltx_td ltx_align_center"><span id="id1.p1.2.3.1.1" class="ltx_text ltx_font_typewriter">mzampier@gmu.edu</span></td>
</tr>
</table>
<p id="id1.p1.3" class="ltx_p ltx_align_center"><span id="id1.p1.3.1" class="ltx_text ltx_font_italic">Abstract content</span></p>
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.   Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">NLP systems relying on modern deep learning paradigms are trained on very large amounts of data. In several applications and domains (e.g., social media), most data used to train machine learning models comes from end users. Such confidential data often cannot be shared without compromising users’ privacy. This is an important concern for organizations that handle large amounts of confidential data, such as financial institutions, healthcare facilities, law firms, and many others. With the widespread use of personal computing devices (e.g., PCs, smartphones, and virtual assistants), data privacy also became a great concern to individuals, which motivated several countries to pass legislation aiming to protect users’ privacy such as the European Union General Data Protection Regulation (GDPR)<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://gdpr.eu/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://gdpr.eu/</a></span></span></span> and the Swiss Datenschutzgesetz (DSG).<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://www.edoeb.admin.ch/edoeb/de/home/datenschutz/ueberblick/datenschutz.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.edoeb.admin.ch/edoeb/de/home/datenschutz/ueberblick/datenschutz.html</a></span></span></span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The need for privacy-preserving machine learning models that can handle confidential data while protecting organizations’ and users’ privacy emerges from this situation. To address this important challenge, Federated Learning (FL) has become an increasingly popular machine learning paradigm <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib28" title="" class="ltx_ref">2017</a>)</cite> as it allows us to train robust machine learning models across multiple devices or servers without exchanging data. In FL, multiple clients work together under the coordination of a central server. Each client’s data is stored locally and not exchanged among clients or with the central server. FL, therefore, offers the possibility of training robust machine learning models on large numbers of decentralized local data repositories without compromising privacy. FL models have been successfully applied in a wide range of applications in computer networks <cite class="ltx_cite ltx_citemacro_cite">Lim et al. (<a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite>, computer vision <cite class="ltx_cite ltx_citemacro_cite">Yan et al. (<a href="#bib.bib45" title="" class="ltx_ref">2021</a>)</cite>, information retrieval <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib43" title="" class="ltx_ref">2021</a>)</cite>, NLP <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>, and many others.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<div id="S1.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:507.9pt;height:90pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.2pt,4.9pt) scale(0.9,0.9) ;">
<table id="S1.T1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1.1" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S1.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="2"><span id="S1.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Training</span></td>
<td id="S1.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="2"><span id="S1.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Testing</span></td>
<td id="S1.T1.1.1.1.4" class="ltx_td ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
</tr>
<tr id="S1.T1.1.1.2" class="ltx_tr">
<td id="S1.T1.1.1.2.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S1.T1.1.1.2.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S1.T1.1.1.2.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S1.T1.1.1.2.2.1" class="ltx_text ltx_font_bold">Inst.</span></td>
<td id="S1.T1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S1.T1.1.1.2.3.1" class="ltx_text ltx_font_bold">OFF %</span></td>
<td id="S1.T1.1.1.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S1.T1.1.1.2.4.1" class="ltx_text ltx_font_bold">Inst.</span></td>
<td id="S1.T1.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S1.T1.1.1.2.5.1" class="ltx_text ltx_font_bold">OFF %</span></td>
<td id="S1.T1.1.1.2.6" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S1.T1.1.1.2.6.1" class="ltx_text ltx_font_bold">Data Sources</span></td>
</tr>
<tr id="S1.T1.1.1.3" class="ltx_tr">
<td id="S1.T1.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">AHSD <cite class="ltx_cite ltx_citemacro_cite">Davidson et al. (<a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite>
</td>
<td id="S1.T1.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">19,822</td>
<td id="S1.T1.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.83</td>
<td id="S1.T1.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">4,956</td>
<td id="S1.T1.1.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.82</td>
<td id="S1.T1.1.1.3.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Twitter</td>
</tr>
<tr id="S1.T1.1.1.4" class="ltx_tr">
<td id="S1.T1.1.1.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">HASOC <cite class="ltx_cite ltx_citemacro_cite">Mandl et al. (<a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="S1.T1.1.1.4.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5,604</td>
<td id="S1.T1.1.1.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">0.36</td>
<td id="S1.T1.1.1.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">1,401</td>
<td id="S1.T1.1.1.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">0.35</td>
<td id="S1.T1.1.1.4.6" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Twitter, Facebook</td>
</tr>
<tr id="S1.T1.1.1.5" class="ltx_tr">
<td id="S1.T1.1.1.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">HateXplain <cite class="ltx_cite ltx_citemacro_cite">Mathew et al. (<a href="#bib.bib27" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S1.T1.1.1.5.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">11,535</td>
<td id="S1.T1.1.1.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">0.59</td>
<td id="S1.T1.1.1.5.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3,844</td>
<td id="S1.T1.1.1.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">0.58</td>
<td id="S1.T1.1.1.5.6" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Twitter, Gab</td>
</tr>
<tr id="S1.T1.1.1.6" class="ltx_tr">
<td id="S1.T1.1.1.6.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">OLID <cite class="ltx_cite ltx_citemacro_cite">Zampieri et al. (<a href="#bib.bib46" title="" class="ltx_ref">2019a</a>)</cite>
</td>
<td id="S1.T1.1.1.6.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;">13,240</td>
<td id="S1.T1.1.1.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">0.33</td>
<td id="S1.T1.1.1.6.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;">860</td>
<td id="S1.T1.1.1.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">0.27</td>
<td id="S1.T1.1.1.6.6" class="ltx_td ltx_align_left ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;">Twitter</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The four datasets, including the number of instances (Inst.) in the training and testing sets, the OFF % in each set and the data source.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we explore the use of FL in offensive language identification online through a model fusion technique <cite class="ltx_cite ltx_citemacro_cite">Choshen et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>. Datasets containing the various forms of offensive speech (e.g., hate speech, cyberbullying, etc.) are sensitive in nature, which creates an interesting use case for FL. The use of FL and other privacy-preserving paradigms allows social media platforms to work together to solve this important issue without the need to exchange confidential information, thus preserving users’ privacy. While FL has recently started to be explored in NLP <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>); Lin et al. (<a href="#bib.bib22" title="" class="ltx_ref">2022b</a>)</cite>, including the workshop on Federated Learning for NLP (FL4NLP) at ACL-2022 <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022a</a>)</cite>, to the best of our knowledge, no studies have yet explored the use of FL in the context of offensive language identification. Our work fills this gap by introducing FL in the context of offensive language identification online and by providing the community with an evaluation of FL methods using four publicly available English offensive language benchmark datasets presented in Section <a href="#S3" title="3. Data ‣ A Federated Learning Approach to Privacy Preserving Offensive Language Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">One recent study <cite class="ltx_cite ltx_citemacro_cite">Gala et al. (<a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite> proposed FL in offensive language identification, but it lacks the consideration of combining different data. Their architecture solely focuses on distributed training on the same dataset with multiple clients and evaluating <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">fedopt <cite class="ltx_cite ltx_citemacro_cite">Reddi et al. (<a href="#bib.bib37" title="" class="ltx_ref">2021</a>)</cite>, fedprox <cite class="ltx_cite ltx_citemacro_cite">Sahu et al. (<a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite></span> algorithms to optimise the global model. Our main focus in this study is on combining multiple models using FL, which could identify offensive content in different data.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.   Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Offensive Language Identification</span> The task of automatically identifying offensive language online has been substantially explored in the literature <cite class="ltx_cite ltx_citemacro_cite">MacAvaney et al. (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>); Melton et al. (<a href="#bib.bib29" title="" class="ltx_ref">2020</a>); Zia et al. (<a href="#bib.bib50" title="" class="ltx_ref">2022</a>); Weerasooriya et al. (<a href="#bib.bib44" title="" class="ltx_ref">2023</a>)</cite>. Multiple types of offensive content have been addressed, such as <span id="S2.p1.1.2" class="ltx_text ltx_font_italic">aggression</span>, <span id="S2.p1.1.3" class="ltx_text ltx_font_italic">cyberbulling</span>, and <span id="S2.p1.1.4" class="ltx_text ltx_font_italic">hate speech</span> using classical machine learning classifiers (e.g., Support Vector Machines) <cite class="ltx_cite ltx_citemacro_cite">Malmasi and Zampieri (<a href="#bib.bib24" title="" class="ltx_ref">2017</a>, <a href="#bib.bib25" title="" class="ltx_ref">2018</a>)</cite>, neural networks <cite class="ltx_cite ltx_citemacro_cite">Gambäck and Sikdar (<a href="#bib.bib16" title="" class="ltx_ref">2017</a>); Djuric et al. (<a href="#bib.bib10" title="" class="ltx_ref">2015</a>); Hettiarachchi and
Ranasinghe (<a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite>, pre-trained general-purpose transformer-based language models <cite class="ltx_cite ltx_citemacro_cite">Ranasinghe and Zampieri (<a href="#bib.bib34" title="" class="ltx_ref">2020</a>, <a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite>, and fine-tuned language models on offensive language datasets <cite class="ltx_cite ltx_citemacro_cite">Caselli et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>); Sarkar et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite>. The vast majority of studies addressed offensive content in English and other widely-spoken resource-rich languages such as Arabic <cite class="ltx_cite ltx_citemacro_cite">Mubarak et al. (<a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>, Portuguese <cite class="ltx_cite ltx_citemacro_cite">Fortuna et al. (<a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite> and Turkish <cite class="ltx_cite ltx_citemacro_cite">Çöltekin (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite> while a few studies dealt with low-resource languages <cite class="ltx_cite ltx_citemacro_cite">Fišer et al. (<a href="#bib.bib12" title="" class="ltx_ref">2017</a>); Gaikwad et al. (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>); Raihan et al. (<a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite>. Multiple competitions on this topic have been organized creating important benchmark datasets such as OffensEval <cite class="ltx_cite ltx_citemacro_cite">Zampieri et al. (<a href="#bib.bib47" title="" class="ltx_ref">2019b</a>, <a href="#bib.bib48" title="" class="ltx_ref">2020</a>)</cite>, HASOC <cite class="ltx_cite ltx_citemacro_cite">Mandl et al. (<a href="#bib.bib26" title="" class="ltx_ref">2020</a>); Modha et al. (<a href="#bib.bib30" title="" class="ltx_ref">2021</a>); Satapara et al. (<a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite>, TRAC <cite class="ltx_cite ltx_citemacro_cite">Kumar et al. (<a href="#bib.bib18" title="" class="ltx_ref">2018</a>, <a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite>, and HatEval <cite class="ltx_cite ltx_citemacro_cite">Basile et al. (<a href="#bib.bib1" title="" class="ltx_ref">2019</a>)</cite>. While substantial progress has been made in the past few years, to the best of our knowledge, none of the aforementioned studies or competitions has addressed the question of data privacy.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Federated Learning in NLP</span> With the goal of preserving users’ data privacy, FL architectures have been extensively studied in a variety of domains <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib43" title="" class="ltx_ref">2021</a>)</cite> in the past several years. Only more recently, however, FL has been explored for text and speech processing <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib22" title="" class="ltx_ref">2022b</a>); Silva et al. (<a href="#bib.bib42" title="" class="ltx_ref">2023</a>); Zhang et al. (<a href="#bib.bib49" title="" class="ltx_ref">2023</a>); Che et al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>. Recent workshops co-located with top-tier conferences confirm this growing interest in FL and privacy in general. The workshop on Privacy in Natural Language Processing (PrivateNLP) <cite class="ltx_cite ltx_citemacro_cite">Feyisetan et al. (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>, which is currently in its fourth edition, addressed the interplay between NLP and data privacy while the aforementioned FL4NLP workshop <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022a</a>)</cite> co-located with ACL-2022 was the first workshop organized focusing exclusively on FL for NLP. Most papers presented in the workshop, however, dealt with language modelling and learning representation rather than with downstream tasks and applications such as offensive language identification. As we mentioned before, a recent study applied different FL strategies in offensive language identification <cite class="ltx_cite ltx_citemacro_cite">Gala et al. (<a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite>. However, their study focuses on distributed training on the same dataset <cite class="ltx_cite ltx_citemacro_cite">Sahu et al. (<a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.   Data</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We use four popular publicly available datasets containing English data summarized in Table <a href="#S1.T1" title="Table 1 ‣ 1. Introduction ‣ A Federated Learning Approach to Privacy Preserving Offensive Language Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. As the datasets were annotated using different guidelines and labels, following the methodology described in previous work <cite class="ltx_cite ltx_citemacro_cite">Ranasinghe and Zampieri (<a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite>, we map all labels to OLID level A <cite class="ltx_cite ltx_citemacro_cite">Zampieri et al. (<a href="#bib.bib46" title="" class="ltx_ref">2019a</a>)</cite>, which contains the labels offensive (OFF) vs. not offensive (NOT). We choose OLID due to the flexibility provided by its general three-level hierarchical taxonomy below, where the OFF class contains all types of offensive content, from general profanity to hate speech, while the NOT class contains non-offensive examples. The OLID taxonomy is presented next:</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2404.11470/assets/federated_learning.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="339" height="164" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The three stages of the FL pipeline in the proposed fused model.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Level A:</span> Offensive (OFF) vs. Non-offensive (NOT).</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Level B:</span> Classification of the type of offensive (OFF) tweet - Targeted (TIN) vs. Untargeted (UNT).</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Level C:</span> Classification of the target of a targeted (TIN) tweet - Individual (IND) vs. Group (GRP) vs. Other (OTH).</p>
</div>
</li>
</ul>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p">In the OLID taxonomy, offensive (OFF) posts targeted (TIN) at an individual are often cyberbulling whereas offensive (OFF) posts targeted (TIN) at a group is often hate speech.</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">AHSD</span> <cite class="ltx_cite ltx_citemacro_cite">Davidson et al. (<a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite> is one of the most popular hate speech datasets available. The dataset contains data retrieved from Twitter and it was annotated using crowdsourcing. The annotation taxonomy contains three classes; Offensive, Hate, and Neither. We conflate Offensive and Hate under a class OFF while neither class corresponds to OLID’s NOT class.</p>
</div>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p id="S3.p5.1" class="ltx_p"><span id="S3.p5.1.1" class="ltx_text ltx_font_bold">OLID</span> <cite class="ltx_cite ltx_citemacro_cite">Zampieri et al. (<a href="#bib.bib46" title="" class="ltx_ref">2019a</a>)</cite> is the official dataset of the SemEval-2019 Task 6 (OffensEval) <cite class="ltx_cite ltx_citemacro_cite">Zampieri et al. (<a href="#bib.bib47" title="" class="ltx_ref">2019b</a>)</cite>. It contains data from Twitter annotated with a three-level hierarchical annotation in which level A classifies posts into offensive and not offensive; level B differentiates between targeted pots (insults and threats) and untargeted posts (general profanity); and level C classifies them into three targets: individual, group, or other. We adopt the labels in OLID level A as our classification labels.</p>
</div>
<div id="S3.p6" class="ltx_para ltx_noindent">
<p id="S3.p6.1" class="ltx_p"><span id="S3.p6.1.1" class="ltx_text ltx_font_bold">HASOC</span> <cite class="ltx_cite ltx_citemacro_cite">Mandl et al. (<a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite> is the dataset used in the HASOC shared task 2020. It contains posts retrieved from Twitter and Facebook. The upper level of the annotation taxonomy used in HASOC is the same as OLID’s level A, which allows us to directly use the same labels in our models.</p>
</div>
<div id="S3.p7" class="ltx_para ltx_noindent">
<p id="S3.p7.1" class="ltx_p"><span id="S3.p7.1.1" class="ltx_text ltx_font_bold">HateXplain</span> <cite class="ltx_cite ltx_citemacro_cite">Mathew et al. (<a href="#bib.bib27" title="" class="ltx_ref">2021</a>)</cite> is a recent dataset collected for the explainability of hate speech. It contains both token- and post-level annotation of Twitter and Gab posts. The annotation taxonomy contains three classes; hate speech, offensive speech, and normal. Following the annotation guidelines of OLID <cite class="ltx_cite ltx_citemacro_cite">Zampieri et al. (<a href="#bib.bib46" title="" class="ltx_ref">2019a</a>)</cite>, we mapped the hate speech and offensive speech classes to offensive (OFF) and normal class to not offensive (NOT).</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.   Methodology</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The proposed FL pipeline contains three steps depicted in Figure <a href="#S3.F1" title="Figure 1 ‣ 3. Data ‣ A Federated Learning Approach to Privacy Preserving Offensive Language Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We describe these steps below.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.3" class="ltx_p"><span id="S4.p2.3.1" class="ltx_text ltx_font_bold ltx_font_italic">Initial Model Training</span>
Transformer models have achieved state-of-the-art performance in many NLP tasks <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>, including offensive language identification <cite class="ltx_cite ltx_citemacro_cite">Ranasinghe et al. (<a href="#bib.bib36" title="" class="ltx_ref">2019</a>); Sarkar et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite>. Therefore, our methodology in this paper builds around pre-trained transformers. For the text classification tasks such as offensive language identification, we use the pre-trained transformer models by utilizing the hidden representation of the classification token (<span id="S4.p2.3.2" class="ltx_text ltx_font_smallcaps">CLS</span>) as shown in Figure <a href="#S4.F2" title="Figure 2 ‣ 4. Methodology ‣ A Federated Learning Approach to Privacy Preserving Offensive Language Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. For this task, we implemented a softmax layer on top of the <span id="S4.p2.3.3" class="ltx_text ltx_font_smallcaps">CLS</span> token, i.e., the predicted probabilities are <math id="S4.p2.1.m1.3" class="ltx_Math" alttext="\bm{y}^{(B)}=\operatorname*{softmax}(W\bm{h}+b)" display="inline"><semantics id="S4.p2.1.m1.3a"><mrow id="S4.p2.1.m1.3.3" xref="S4.p2.1.m1.3.3.cmml"><msup id="S4.p2.1.m1.3.3.3" xref="S4.p2.1.m1.3.3.3.cmml"><mi id="S4.p2.1.m1.3.3.3.2" xref="S4.p2.1.m1.3.3.3.2.cmml">𝒚</mi><mrow id="S4.p2.1.m1.1.1.1.3" xref="S4.p2.1.m1.3.3.3.cmml"><mo stretchy="false" id="S4.p2.1.m1.1.1.1.3.1" xref="S4.p2.1.m1.3.3.3.cmml">(</mo><mi id="S4.p2.1.m1.1.1.1.1" xref="S4.p2.1.m1.1.1.1.1.cmml">B</mi><mo stretchy="false" id="S4.p2.1.m1.1.1.1.3.2" xref="S4.p2.1.m1.3.3.3.cmml">)</mo></mrow></msup><mo rspace="0.1389em" id="S4.p2.1.m1.3.3.2" xref="S4.p2.1.m1.3.3.2.cmml">=</mo><mrow id="S4.p2.1.m1.3.3.1.1" xref="S4.p2.1.m1.3.3.1.2.cmml"><mo lspace="0.1389em" rspace="0em" id="S4.p2.1.m1.2.2" xref="S4.p2.1.m1.2.2.cmml">softmax</mo><mrow id="S4.p2.1.m1.3.3.1.1.1" xref="S4.p2.1.m1.3.3.1.2.cmml"><mo stretchy="false" id="S4.p2.1.m1.3.3.1.1.1.2" xref="S4.p2.1.m1.3.3.1.2.cmml">(</mo><mrow id="S4.p2.1.m1.3.3.1.1.1.1" xref="S4.p2.1.m1.3.3.1.1.1.1.cmml"><mrow id="S4.p2.1.m1.3.3.1.1.1.1.2" xref="S4.p2.1.m1.3.3.1.1.1.1.2.cmml"><mi id="S4.p2.1.m1.3.3.1.1.1.1.2.2" xref="S4.p2.1.m1.3.3.1.1.1.1.2.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.p2.1.m1.3.3.1.1.1.1.2.1" xref="S4.p2.1.m1.3.3.1.1.1.1.2.1.cmml">​</mo><mi id="S4.p2.1.m1.3.3.1.1.1.1.2.3" xref="S4.p2.1.m1.3.3.1.1.1.1.2.3.cmml">𝒉</mi></mrow><mo id="S4.p2.1.m1.3.3.1.1.1.1.1" xref="S4.p2.1.m1.3.3.1.1.1.1.1.cmml">+</mo><mi id="S4.p2.1.m1.3.3.1.1.1.1.3" xref="S4.p2.1.m1.3.3.1.1.1.1.3.cmml">b</mi></mrow><mo stretchy="false" id="S4.p2.1.m1.3.3.1.1.1.3" xref="S4.p2.1.m1.3.3.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.3b"><apply id="S4.p2.1.m1.3.3.cmml" xref="S4.p2.1.m1.3.3"><eq id="S4.p2.1.m1.3.3.2.cmml" xref="S4.p2.1.m1.3.3.2"></eq><apply id="S4.p2.1.m1.3.3.3.cmml" xref="S4.p2.1.m1.3.3.3"><csymbol cd="ambiguous" id="S4.p2.1.m1.3.3.3.1.cmml" xref="S4.p2.1.m1.3.3.3">superscript</csymbol><ci id="S4.p2.1.m1.3.3.3.2.cmml" xref="S4.p2.1.m1.3.3.3.2">𝒚</ci><ci id="S4.p2.1.m1.1.1.1.1.cmml" xref="S4.p2.1.m1.1.1.1.1">𝐵</ci></apply><apply id="S4.p2.1.m1.3.3.1.2.cmml" xref="S4.p2.1.m1.3.3.1.1"><ci id="S4.p2.1.m1.2.2.cmml" xref="S4.p2.1.m1.2.2">softmax</ci><apply id="S4.p2.1.m1.3.3.1.1.1.1.cmml" xref="S4.p2.1.m1.3.3.1.1.1.1"><plus id="S4.p2.1.m1.3.3.1.1.1.1.1.cmml" xref="S4.p2.1.m1.3.3.1.1.1.1.1"></plus><apply id="S4.p2.1.m1.3.3.1.1.1.1.2.cmml" xref="S4.p2.1.m1.3.3.1.1.1.1.2"><times id="S4.p2.1.m1.3.3.1.1.1.1.2.1.cmml" xref="S4.p2.1.m1.3.3.1.1.1.1.2.1"></times><ci id="S4.p2.1.m1.3.3.1.1.1.1.2.2.cmml" xref="S4.p2.1.m1.3.3.1.1.1.1.2.2">𝑊</ci><ci id="S4.p2.1.m1.3.3.1.1.1.1.2.3.cmml" xref="S4.p2.1.m1.3.3.1.1.1.1.2.3">𝒉</ci></apply><ci id="S4.p2.1.m1.3.3.1.1.1.1.3.cmml" xref="S4.p2.1.m1.3.3.1.1.1.1.3">𝑏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.3c">\bm{y}^{(B)}=\operatorname*{softmax}(W\bm{h}+b)</annotation></semantics></math>, where <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="W\in\mathbb{R}^{k\times d}" display="inline"><semantics id="S4.p2.2.m2.1a"><mrow id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mi id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">W</mi><mo id="S4.p2.2.m2.1.1.1" xref="S4.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml"><mi id="S4.p2.2.m2.1.1.3.2" xref="S4.p2.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S4.p2.2.m2.1.1.3.3" xref="S4.p2.2.m2.1.1.3.3.cmml"><mi id="S4.p2.2.m2.1.1.3.3.2" xref="S4.p2.2.m2.1.1.3.3.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S4.p2.2.m2.1.1.3.3.1" xref="S4.p2.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S4.p2.2.m2.1.1.3.3.3" xref="S4.p2.2.m2.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><in id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1.1"></in><ci id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">𝑊</ci><apply id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.p2.2.m2.1.1.3.1.cmml" xref="S4.p2.2.m2.1.1.3">superscript</csymbol><ci id="S4.p2.2.m2.1.1.3.2.cmml" xref="S4.p2.2.m2.1.1.3.2">ℝ</ci><apply id="S4.p2.2.m2.1.1.3.3.cmml" xref="S4.p2.2.m2.1.1.3.3"><times id="S4.p2.2.m2.1.1.3.3.1.cmml" xref="S4.p2.2.m2.1.1.3.3.1"></times><ci id="S4.p2.2.m2.1.1.3.3.2.cmml" xref="S4.p2.2.m2.1.1.3.3.2">𝑘</ci><ci id="S4.p2.2.m2.1.1.3.3.3.cmml" xref="S4.p2.2.m2.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">W\in\mathbb{R}^{k\times d}</annotation></semantics></math> is the softmax weight matrix, and <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.p2.3.m3.1a"><mi id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><ci id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">k</annotation></semantics></math> is the number of labels. which in our case is always equal to two.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2404.11470/assets/sentence_level.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="202" height="186" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A sample transformer model for offensive language identification <cite class="ltx_cite ltx_citemacro_cite">Ranasinghe and Zampieri (<a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite> predicting offensive and not offensive labels.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">We used this text classification architecture to build separate models for each dataset that we introduced in the previous section. We trained the model using the training sets of each dataset. We employed a batch-size of 16, Adam optimiser with learning rate <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="4\mathrm{e}{-5}" display="inline"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mrow id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml"><mn id="S4.p3.1.m1.1.1.2.2" xref="S4.p3.1.m1.1.1.2.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S4.p3.1.m1.1.1.2.1" xref="S4.p3.1.m1.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S4.p3.1.m1.1.1.2.3" xref="S4.p3.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S4.p3.1.m1.1.1.1" xref="S4.p3.1.m1.1.1.1.cmml">−</mo><mn id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><minus id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1"></minus><apply id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2"><times id="S4.p3.1.m1.1.1.2.1.cmml" xref="S4.p3.1.m1.1.1.2.1"></times><cn type="integer" id="S4.p3.1.m1.1.1.2.2.cmml" xref="S4.p3.1.m1.1.1.2.2">4</cn><ci id="S4.p3.1.m1.1.1.2.3.cmml" xref="S4.p3.1.m1.1.1.2.3">e</ci></apply><cn type="integer" id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">4\mathrm{e}{-5}</annotation></semantics></math>, and a linear learning rate warm-up over 10% of the training data. During the training process, the parameters of the transformer model and the parameters of the subsequent layers were updated. The models were evaluated while training using an evaluation set that had one-fifth of the rows in training data. We performed early stopping if the evaluation loss did not improve over three evaluation steps. All the models were trained for three epochs.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:345.4pt;height:258.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-15.0pt,11.2pt) scale(0.92,0.92) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Approach</span></td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">Models</span></td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.1.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.1.1.1.6.1" class="ltx_text ltx_font_bold">Macro F1</span></td>
</tr>
<tr id="S4.T2.1.1.2" class="ltx_tr">
<td id="S4.T2.1.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="4"><span id="S4.T2.1.1.2.1.1" class="ltx_text">AHSD</span></td>
<td id="S4.T2.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">non-fused</td>
<td id="S4.T2.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">AHSD</td>
<td id="S4.T2.1.1.2.4" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.1.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.1.1.2.7" class="ltx_td ltx_align_left ltx_border_t">0.931 ±0.01</td>
</tr>
<tr id="S4.T2.1.1.3" class="ltx_tr">
<td id="S4.T2.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r">fusion with FT</td>
<td id="S4.T2.1.1.3.2" class="ltx_td ltx_align_center">AHSD</td>
<td id="S4.T2.1.1.3.3" class="ltx_td ltx_align_center">OLID</td>
<td id="S4.T2.1.1.3.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.1.3.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.1.3.6" class="ltx_td ltx_align_left">0.921 ±0.00</td>
</tr>
<tr id="S4.T2.1.1.4" class="ltx_tr">
<td id="S4.T2.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r">fusion without FT</td>
<td id="S4.T2.1.1.4.2" class="ltx_td ltx_align_center">AHSD</td>
<td id="S4.T2.1.1.4.3" class="ltx_td ltx_align_center">OLID</td>
<td id="S4.T2.1.1.4.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.1.4.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.1.4.6" class="ltx_td ltx_align_left">0.866 ±0.00</td>
</tr>
<tr id="S4.T2.1.1.5" class="ltx_tr">
<td id="S4.T2.1.1.5.1" class="ltx_td ltx_align_center ltx_border_r">ensemble</td>
<td id="S4.T2.1.1.5.2" class="ltx_td ltx_align_center">AHSD</td>
<td id="S4.T2.1.1.5.3" class="ltx_td ltx_align_center">OLID</td>
<td id="S4.T2.1.1.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.1.5.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.1.5.6" class="ltx_td ltx_align_left">0.845 ±0.01</td>
</tr>
<tr id="S4.T2.1.1.6" class="ltx_tr">
<td id="S4.T2.1.1.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="4"><span id="S4.T2.1.1.6.1.1" class="ltx_text">OLID</span></td>
<td id="S4.T2.1.1.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">non-fused</td>
<td id="S4.T2.1.1.6.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.1.1.6.4" class="ltx_td ltx_align_center ltx_border_t">OLID</td>
<td id="S4.T2.1.1.6.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.1.1.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.1.1.6.7" class="ltx_td ltx_align_left ltx_border_t">0.854 ±0.00</td>
</tr>
<tr id="S4.T2.1.1.7" class="ltx_tr">
<td id="S4.T2.1.1.7.1" class="ltx_td ltx_align_center ltx_border_r">fusion with FT</td>
<td id="S4.T2.1.1.7.2" class="ltx_td ltx_align_center">AHSD</td>
<td id="S4.T2.1.1.7.3" class="ltx_td ltx_align_center">OLID</td>
<td id="S4.T2.1.1.7.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.1.7.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.1.7.6" class="ltx_td ltx_align_left">0.837 ±0.03</td>
</tr>
<tr id="S4.T2.1.1.8" class="ltx_tr">
<td id="S4.T2.1.1.8.1" class="ltx_td ltx_align_center ltx_border_r">fusion without FT</td>
<td id="S4.T2.1.1.8.2" class="ltx_td ltx_align_center">AHSD</td>
<td id="S4.T2.1.1.8.3" class="ltx_td ltx_align_center">OLID</td>
<td id="S4.T2.1.1.8.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.1.8.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.1.8.6" class="ltx_td ltx_align_left">0.836 ±0.00</td>
</tr>
<tr id="S4.T2.1.1.9" class="ltx_tr">
<td id="S4.T2.1.1.9.1" class="ltx_td ltx_align_center ltx_border_r">ensemble</td>
<td id="S4.T2.1.1.9.2" class="ltx_td"></td>
<td id="S4.T2.1.1.9.3" class="ltx_td ltx_align_center">OLID</td>
<td id="S4.T2.1.1.9.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.1.9.5" class="ltx_td ltx_align_center ltx_border_r">HateX</td>
<td id="S4.T2.1.1.9.6" class="ltx_td ltx_align_left">0.785 ±0.04</td>
</tr>
<tr id="S4.T2.1.1.10" class="ltx_tr">
<td id="S4.T2.1.1.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="4"><span id="S4.T2.1.1.10.1.1" class="ltx_text">HASOC</span></td>
<td id="S4.T2.1.1.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">non-fused</td>
<td id="S4.T2.1.1.10.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.1.1.10.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.1.1.10.5" class="ltx_td ltx_align_center ltx_border_t">HASOC</td>
<td id="S4.T2.1.1.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.1.1.10.7" class="ltx_td ltx_align_left ltx_border_t">0.798 ±0.01</td>
</tr>
<tr id="S4.T2.1.1.11" class="ltx_tr">
<td id="S4.T2.1.1.11.1" class="ltx_td ltx_align_center ltx_border_r">fusion without FT</td>
<td id="S4.T2.1.1.11.2" class="ltx_td ltx_align_center">AHSD</td>
<td id="S4.T2.1.1.11.3" class="ltx_td ltx_align_center">OLID</td>
<td id="S4.T2.1.1.11.4" class="ltx_td ltx_align_center">HASOC</td>
<td id="S4.T2.1.1.11.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.1.11.6" class="ltx_td ltx_align_left">0.770 ±0.01</td>
</tr>
<tr id="S4.T2.1.1.12" class="ltx_tr">
<td id="S4.T2.1.1.12.1" class="ltx_td ltx_align_center ltx_border_r">fusion with FT</td>
<td id="S4.T2.1.1.12.2" class="ltx_td ltx_align_center">AHSD</td>
<td id="S4.T2.1.1.12.3" class="ltx_td ltx_align_center">OLID</td>
<td id="S4.T2.1.1.12.4" class="ltx_td ltx_align_center">HASOC</td>
<td id="S4.T2.1.1.12.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.1.12.6" class="ltx_td ltx_align_left">0.754 ±0.07</td>
</tr>
<tr id="S4.T2.1.1.13" class="ltx_tr">
<td id="S4.T2.1.1.13.1" class="ltx_td ltx_align_center ltx_border_r">ensemble</td>
<td id="S4.T2.1.1.13.2" class="ltx_td ltx_align_center">AHSD</td>
<td id="S4.T2.1.1.13.3" class="ltx_td"></td>
<td id="S4.T2.1.1.13.4" class="ltx_td ltx_align_center">HASOC</td>
<td id="S4.T2.1.1.13.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.1.13.6" class="ltx_td ltx_align_left">0.647±0.02</td>
</tr>
<tr id="S4.T2.1.1.14" class="ltx_tr">
<td id="S4.T2.1.1.14.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="4"><span id="S4.T2.1.1.14.1.1" class="ltx_text">HateX</span></td>
<td id="S4.T2.1.1.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">non-fused</td>
<td id="S4.T2.1.1.14.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.1.1.14.4" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.1.1.14.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.1.1.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">HateX</td>
<td id="S4.T2.1.1.14.7" class="ltx_td ltx_align_left ltx_border_t">0.795 ±0.01</td>
</tr>
<tr id="S4.T2.1.1.15" class="ltx_tr">
<td id="S4.T2.1.1.15.1" class="ltx_td ltx_align_center ltx_border_r">fusion with FT</td>
<td id="S4.T2.1.1.15.2" class="ltx_td ltx_align_center">AHSD</td>
<td id="S4.T2.1.1.15.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.1.15.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.1.15.5" class="ltx_td ltx_align_center ltx_border_r">HateX</td>
<td id="S4.T2.1.1.15.6" class="ltx_td ltx_align_left">0.777 ±0.00</td>
</tr>
<tr id="S4.T2.1.1.16" class="ltx_tr">
<td id="S4.T2.1.1.16.1" class="ltx_td ltx_align_center ltx_border_r">fusion without FT</td>
<td id="S4.T2.1.1.16.2" class="ltx_td ltx_align_center">AHSD</td>
<td id="S4.T2.1.1.16.3" class="ltx_td ltx_align_center">OLID</td>
<td id="S4.T2.1.1.16.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.1.16.5" class="ltx_td ltx_align_center ltx_border_r">HateX</td>
<td id="S4.T2.1.1.16.6" class="ltx_td ltx_align_left">0.772 ±0.01</td>
</tr>
<tr id="S4.T2.1.1.17" class="ltx_tr">
<td id="S4.T2.1.1.17.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">ensemble</td>
<td id="S4.T2.1.1.17.2" class="ltx_td ltx_align_center ltx_border_b">-</td>
<td id="S4.T2.1.1.17.3" class="ltx_td ltx_align_center ltx_border_b">-</td>
<td id="S4.T2.1.1.17.4" class="ltx_td ltx_align_center ltx_border_b">HASOC</td>
<td id="S4.T2.1.1.17.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">HateX</td>
<td id="S4.T2.1.1.17.6" class="ltx_td ltx_align_left ltx_border_b">0.654 ±0.01</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The best result for each dataset for each approach; non-fused models, fused models with fine-tuning (FT), fused models without finetuning and ensemble. We only report the results with fBERT. The results are ordered from Macro F1.</figcaption>
</figure>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">We repeated this process with two popular pre-trained transformer models; <span id="S4.p4.1.1" class="ltx_text ltx_font_italic">bert-large-cased</span> <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite> and <span id="S4.p4.1.2" class="ltx_text ltx_font_italic">fBERT</span> <cite class="ltx_cite ltx_citemacro_cite">Sarkar et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite>. The <span id="S4.p4.1.3" class="ltx_text ltx_font_italic">bert-large-cased</span> is a general purpose pre-trained transformer model while <span id="S4.p4.1.4" class="ltx_text ltx_font_italic">fBERT</span> is a domain-specific pre-trained transformer model for offensive language identification that has been trained on over <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="1.4" display="inline"><semantics id="S4.p4.1.m1.1a"><mn id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml">1.4</mn><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><cn type="float" id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1">1.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">1.4</annotation></semantics></math> million offensive tweets in SOLID dataset <cite class="ltx_cite ltx_citemacro_cite">Rosenthal et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite> and has shown state-of-the-art results in several offensive language identification benchmarks <cite class="ltx_cite ltx_citemacro_cite">Sarkar et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S4.p5" class="ltx_para ltx_noindent">
<p id="S4.p5.4" class="ltx_p"><span id="S4.p5.4.1" class="ltx_text ltx_font_bold ltx_font_italic">Model Fusion</span>
In order to combine the different models created using different datasets, we followed a recent approach named model fusion <cite class="ltx_cite ltx_citemacro_cite">Choshen et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>. Model Fusion is the process of taking several fine-tuned models and creating a new base model. Formally, given an initialization base model <math id="S4.p5.1.m1.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S4.p5.1.m1.1a"><mi id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><ci id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">P</annotation></semantics></math> and <math id="S4.p5.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.p5.2.m2.1a"><mi id="S4.p5.2.m2.1.1" xref="S4.p5.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.p5.2.m2.1b"><ci id="S4.p5.2.m2.1.1.cmml" xref="S4.p5.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.2.m2.1c">n</annotation></semantics></math> models fine-tuned on it, let <math id="S4.p5.3.m3.2" class="ltx_Math" alttext="W_{1},W_{2}\ldots W_{n}\in\mathbb{R}^{d}" display="inline"><semantics id="S4.p5.3.m3.2a"><mrow id="S4.p5.3.m3.2.2" xref="S4.p5.3.m3.2.2.cmml"><mrow id="S4.p5.3.m3.2.2.2.2" xref="S4.p5.3.m3.2.2.2.3.cmml"><msub id="S4.p5.3.m3.1.1.1.1.1" xref="S4.p5.3.m3.1.1.1.1.1.cmml"><mi id="S4.p5.3.m3.1.1.1.1.1.2" xref="S4.p5.3.m3.1.1.1.1.1.2.cmml">W</mi><mn id="S4.p5.3.m3.1.1.1.1.1.3" xref="S4.p5.3.m3.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S4.p5.3.m3.2.2.2.2.3" xref="S4.p5.3.m3.2.2.2.3.cmml">,</mo><mrow id="S4.p5.3.m3.2.2.2.2.2" xref="S4.p5.3.m3.2.2.2.2.2.cmml"><msub id="S4.p5.3.m3.2.2.2.2.2.2" xref="S4.p5.3.m3.2.2.2.2.2.2.cmml"><mi id="S4.p5.3.m3.2.2.2.2.2.2.2" xref="S4.p5.3.m3.2.2.2.2.2.2.2.cmml">W</mi><mn id="S4.p5.3.m3.2.2.2.2.2.2.3" xref="S4.p5.3.m3.2.2.2.2.2.2.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S4.p5.3.m3.2.2.2.2.2.1" xref="S4.p5.3.m3.2.2.2.2.2.1.cmml">​</mo><mi mathvariant="normal" id="S4.p5.3.m3.2.2.2.2.2.3" xref="S4.p5.3.m3.2.2.2.2.2.3.cmml">…</mi><mo lspace="0em" rspace="0em" id="S4.p5.3.m3.2.2.2.2.2.1a" xref="S4.p5.3.m3.2.2.2.2.2.1.cmml">​</mo><msub id="S4.p5.3.m3.2.2.2.2.2.4" xref="S4.p5.3.m3.2.2.2.2.2.4.cmml"><mi id="S4.p5.3.m3.2.2.2.2.2.4.2" xref="S4.p5.3.m3.2.2.2.2.2.4.2.cmml">W</mi><mi id="S4.p5.3.m3.2.2.2.2.2.4.3" xref="S4.p5.3.m3.2.2.2.2.2.4.3.cmml">n</mi></msub></mrow></mrow><mo id="S4.p5.3.m3.2.2.3" xref="S4.p5.3.m3.2.2.3.cmml">∈</mo><msup id="S4.p5.3.m3.2.2.4" xref="S4.p5.3.m3.2.2.4.cmml"><mi id="S4.p5.3.m3.2.2.4.2" xref="S4.p5.3.m3.2.2.4.2.cmml">ℝ</mi><mi id="S4.p5.3.m3.2.2.4.3" xref="S4.p5.3.m3.2.2.4.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.3.m3.2b"><apply id="S4.p5.3.m3.2.2.cmml" xref="S4.p5.3.m3.2.2"><in id="S4.p5.3.m3.2.2.3.cmml" xref="S4.p5.3.m3.2.2.3"></in><list id="S4.p5.3.m3.2.2.2.3.cmml" xref="S4.p5.3.m3.2.2.2.2"><apply id="S4.p5.3.m3.1.1.1.1.1.cmml" xref="S4.p5.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p5.3.m3.1.1.1.1.1.1.cmml" xref="S4.p5.3.m3.1.1.1.1.1">subscript</csymbol><ci id="S4.p5.3.m3.1.1.1.1.1.2.cmml" xref="S4.p5.3.m3.1.1.1.1.1.2">𝑊</ci><cn type="integer" id="S4.p5.3.m3.1.1.1.1.1.3.cmml" xref="S4.p5.3.m3.1.1.1.1.1.3">1</cn></apply><apply id="S4.p5.3.m3.2.2.2.2.2.cmml" xref="S4.p5.3.m3.2.2.2.2.2"><times id="S4.p5.3.m3.2.2.2.2.2.1.cmml" xref="S4.p5.3.m3.2.2.2.2.2.1"></times><apply id="S4.p5.3.m3.2.2.2.2.2.2.cmml" xref="S4.p5.3.m3.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.p5.3.m3.2.2.2.2.2.2.1.cmml" xref="S4.p5.3.m3.2.2.2.2.2.2">subscript</csymbol><ci id="S4.p5.3.m3.2.2.2.2.2.2.2.cmml" xref="S4.p5.3.m3.2.2.2.2.2.2.2">𝑊</ci><cn type="integer" id="S4.p5.3.m3.2.2.2.2.2.2.3.cmml" xref="S4.p5.3.m3.2.2.2.2.2.2.3">2</cn></apply><ci id="S4.p5.3.m3.2.2.2.2.2.3.cmml" xref="S4.p5.3.m3.2.2.2.2.2.3">…</ci><apply id="S4.p5.3.m3.2.2.2.2.2.4.cmml" xref="S4.p5.3.m3.2.2.2.2.2.4"><csymbol cd="ambiguous" id="S4.p5.3.m3.2.2.2.2.2.4.1.cmml" xref="S4.p5.3.m3.2.2.2.2.2.4">subscript</csymbol><ci id="S4.p5.3.m3.2.2.2.2.2.4.2.cmml" xref="S4.p5.3.m3.2.2.2.2.2.4.2">𝑊</ci><ci id="S4.p5.3.m3.2.2.2.2.2.4.3.cmml" xref="S4.p5.3.m3.2.2.2.2.2.4.3">𝑛</ci></apply></apply></list><apply id="S4.p5.3.m3.2.2.4.cmml" xref="S4.p5.3.m3.2.2.4"><csymbol cd="ambiguous" id="S4.p5.3.m3.2.2.4.1.cmml" xref="S4.p5.3.m3.2.2.4">superscript</csymbol><ci id="S4.p5.3.m3.2.2.4.2.cmml" xref="S4.p5.3.m3.2.2.4.2">ℝ</ci><ci id="S4.p5.3.m3.2.2.4.3.cmml" xref="S4.p5.3.m3.2.2.4.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.3.m3.2c">W_{1},W_{2}\ldots W_{n}\in\mathbb{R}^{d}</annotation></semantics></math> be the weights fine-tuned by the models over <math id="S4.p5.4.m4.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S4.p5.4.m4.1a"><mi id="S4.p5.4.m4.1.1" xref="S4.p5.4.m4.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.p5.4.m4.1b"><ci id="S4.p5.4.m4.1.1.cmml" xref="S4.p5.4.m4.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.4.m4.1c">P</annotation></semantics></math>. Fusing is a function</p>
</div>
<div id="S4.p6" class="ltx_para">
<div id="S4.p6.1" class="ltx_inline-block ltx_transformed_outer" style="width:368.6pt;height:9.9pt;vertical-align:-2.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-35.9pt,0.8pt) scale(0.837135612154529,0.837135612154529) ;">
<div id="S4.p6.1.1" class="ltx_block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<table id="Sx1.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E1.m1.1" class="ltx_Math" alttext="\displaystyle W_{fuse}" display="inline"><semantics id="S4.E1.m1.1a"><msub id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml"><mi id="S4.E1.m1.1.1.2" xref="S4.E1.m1.1.1.2.cmml">W</mi><mrow id="S4.E1.m1.1.1.3" xref="S4.E1.m1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.3.2" xref="S4.E1.m1.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.1" xref="S4.E1.m1.1.1.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.3.3" xref="S4.E1.m1.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.1a" xref="S4.E1.m1.1.1.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.3.4" xref="S4.E1.m1.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.1b" xref="S4.E1.m1.1.1.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.3.5" xref="S4.E1.m1.1.1.3.5.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.cmml" xref="S4.E1.m1.1.1">subscript</csymbol><ci id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1.2">𝑊</ci><apply id="S4.E1.m1.1.1.3.cmml" xref="S4.E1.m1.1.1.3"><times id="S4.E1.m1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.3.1"></times><ci id="S4.E1.m1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.3.2">𝑓</ci><ci id="S4.E1.m1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.3.3">𝑢</ci><ci id="S4.E1.m1.1.1.3.4.cmml" xref="S4.E1.m1.1.1.3.4">𝑠</ci><ci id="S4.E1.m1.1.1.3.5.cmml" xref="S4.E1.m1.1.1.3.5">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">\displaystyle W_{fuse}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E1.m2.4" class="ltx_Math" alttext="\displaystyle=f(W_{1},W_{2},\ldots,W_{n})" display="inline"><semantics id="S4.E1.m2.4a"><mrow id="S4.E1.m2.4.4" xref="S4.E1.m2.4.4.cmml"><mi id="S4.E1.m2.4.4.5" xref="S4.E1.m2.4.4.5.cmml"></mi><mo id="S4.E1.m2.4.4.4" xref="S4.E1.m2.4.4.4.cmml">=</mo><mrow id="S4.E1.m2.4.4.3" xref="S4.E1.m2.4.4.3.cmml"><mi id="S4.E1.m2.4.4.3.5" xref="S4.E1.m2.4.4.3.5.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.E1.m2.4.4.3.4" xref="S4.E1.m2.4.4.3.4.cmml">​</mo><mrow id="S4.E1.m2.4.4.3.3.3" xref="S4.E1.m2.4.4.3.3.4.cmml"><mo stretchy="false" id="S4.E1.m2.4.4.3.3.3.4" xref="S4.E1.m2.4.4.3.3.4.cmml">(</mo><msub id="S4.E1.m2.2.2.1.1.1.1" xref="S4.E1.m2.2.2.1.1.1.1.cmml"><mi id="S4.E1.m2.2.2.1.1.1.1.2" xref="S4.E1.m2.2.2.1.1.1.1.2.cmml">W</mi><mn id="S4.E1.m2.2.2.1.1.1.1.3" xref="S4.E1.m2.2.2.1.1.1.1.3.cmml">1</mn></msub><mo id="S4.E1.m2.4.4.3.3.3.5" xref="S4.E1.m2.4.4.3.3.4.cmml">,</mo><msub id="S4.E1.m2.3.3.2.2.2.2" xref="S4.E1.m2.3.3.2.2.2.2.cmml"><mi id="S4.E1.m2.3.3.2.2.2.2.2" xref="S4.E1.m2.3.3.2.2.2.2.2.cmml">W</mi><mn id="S4.E1.m2.3.3.2.2.2.2.3" xref="S4.E1.m2.3.3.2.2.2.2.3.cmml">2</mn></msub><mo id="S4.E1.m2.4.4.3.3.3.6" xref="S4.E1.m2.4.4.3.3.4.cmml">,</mo><mi mathvariant="normal" id="S4.E1.m2.1.1" xref="S4.E1.m2.1.1.cmml">…</mi><mo id="S4.E1.m2.4.4.3.3.3.7" xref="S4.E1.m2.4.4.3.3.4.cmml">,</mo><msub id="S4.E1.m2.4.4.3.3.3.3" xref="S4.E1.m2.4.4.3.3.3.3.cmml"><mi id="S4.E1.m2.4.4.3.3.3.3.2" xref="S4.E1.m2.4.4.3.3.3.3.2.cmml">W</mi><mi id="S4.E1.m2.4.4.3.3.3.3.3" xref="S4.E1.m2.4.4.3.3.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="S4.E1.m2.4.4.3.3.3.8" xref="S4.E1.m2.4.4.3.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m2.4b"><apply id="S4.E1.m2.4.4.cmml" xref="S4.E1.m2.4.4"><eq id="S4.E1.m2.4.4.4.cmml" xref="S4.E1.m2.4.4.4"></eq><csymbol cd="latexml" id="S4.E1.m2.4.4.5.cmml" xref="S4.E1.m2.4.4.5">absent</csymbol><apply id="S4.E1.m2.4.4.3.cmml" xref="S4.E1.m2.4.4.3"><times id="S4.E1.m2.4.4.3.4.cmml" xref="S4.E1.m2.4.4.3.4"></times><ci id="S4.E1.m2.4.4.3.5.cmml" xref="S4.E1.m2.4.4.3.5">𝑓</ci><vector id="S4.E1.m2.4.4.3.3.4.cmml" xref="S4.E1.m2.4.4.3.3.3"><apply id="S4.E1.m2.2.2.1.1.1.1.cmml" xref="S4.E1.m2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m2.2.2.1.1.1.1.1.cmml" xref="S4.E1.m2.2.2.1.1.1.1">subscript</csymbol><ci id="S4.E1.m2.2.2.1.1.1.1.2.cmml" xref="S4.E1.m2.2.2.1.1.1.1.2">𝑊</ci><cn type="integer" id="S4.E1.m2.2.2.1.1.1.1.3.cmml" xref="S4.E1.m2.2.2.1.1.1.1.3">1</cn></apply><apply id="S4.E1.m2.3.3.2.2.2.2.cmml" xref="S4.E1.m2.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S4.E1.m2.3.3.2.2.2.2.1.cmml" xref="S4.E1.m2.3.3.2.2.2.2">subscript</csymbol><ci id="S4.E1.m2.3.3.2.2.2.2.2.cmml" xref="S4.E1.m2.3.3.2.2.2.2.2">𝑊</ci><cn type="integer" id="S4.E1.m2.3.3.2.2.2.2.3.cmml" xref="S4.E1.m2.3.3.2.2.2.2.3">2</cn></apply><ci id="S4.E1.m2.1.1.cmml" xref="S4.E1.m2.1.1">…</ci><apply id="S4.E1.m2.4.4.3.3.3.3.cmml" xref="S4.E1.m2.4.4.3.3.3.3"><csymbol cd="ambiguous" id="S4.E1.m2.4.4.3.3.3.3.1.cmml" xref="S4.E1.m2.4.4.3.3.3.3">subscript</csymbol><ci id="S4.E1.m2.4.4.3.3.3.3.2.cmml" xref="S4.E1.m2.4.4.3.3.3.3.2">𝑊</ci><ci id="S4.E1.m2.4.4.3.3.3.3.3.cmml" xref="S4.E1.m2.4.4.3.3.3.3.3">𝑛</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m2.4c">\displaystyle=f(W_{1},W_{2},\ldots,W_{n})</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E1.m3.1" class="ltx_Math" alttext="\displaystyle\mathbb{R}^{d}\times\mathbb{R}^{d}\times\ldots\times\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}" display="inline"><semantics id="S4.E1.m3.1a"><mrow id="S4.E1.m3.1.1" xref="S4.E1.m3.1.1.cmml"><mrow id="S4.E1.m3.1.1.2" xref="S4.E1.m3.1.1.2.cmml"><msup id="S4.E1.m3.1.1.2.2" xref="S4.E1.m3.1.1.2.2.cmml"><mi id="S4.E1.m3.1.1.2.2.2" xref="S4.E1.m3.1.1.2.2.2.cmml">ℝ</mi><mi id="S4.E1.m3.1.1.2.2.3" xref="S4.E1.m3.1.1.2.2.3.cmml">d</mi></msup><mo lspace="0.222em" rspace="0.222em" id="S4.E1.m3.1.1.2.1" xref="S4.E1.m3.1.1.2.1.cmml">×</mo><msup id="S4.E1.m3.1.1.2.3" xref="S4.E1.m3.1.1.2.3.cmml"><mi id="S4.E1.m3.1.1.2.3.2" xref="S4.E1.m3.1.1.2.3.2.cmml">ℝ</mi><mi id="S4.E1.m3.1.1.2.3.3" xref="S4.E1.m3.1.1.2.3.3.cmml">d</mi></msup><mo lspace="0.222em" rspace="0.222em" id="S4.E1.m3.1.1.2.1a" xref="S4.E1.m3.1.1.2.1.cmml">×</mo><mi mathvariant="normal" id="S4.E1.m3.1.1.2.4" xref="S4.E1.m3.1.1.2.4.cmml">…</mi><mo lspace="0.222em" rspace="0.222em" id="S4.E1.m3.1.1.2.1b" xref="S4.E1.m3.1.1.2.1.cmml">×</mo><msup id="S4.E1.m3.1.1.2.5" xref="S4.E1.m3.1.1.2.5.cmml"><mi id="S4.E1.m3.1.1.2.5.2" xref="S4.E1.m3.1.1.2.5.2.cmml">ℝ</mi><mi id="S4.E1.m3.1.1.2.5.3" xref="S4.E1.m3.1.1.2.5.3.cmml">d</mi></msup></mrow><mo stretchy="false" id="S4.E1.m3.1.1.1" xref="S4.E1.m3.1.1.1.cmml">→</mo><msup id="S4.E1.m3.1.1.3" xref="S4.E1.m3.1.1.3.cmml"><mi id="S4.E1.m3.1.1.3.2" xref="S4.E1.m3.1.1.3.2.cmml">ℝ</mi><mi id="S4.E1.m3.1.1.3.3" xref="S4.E1.m3.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m3.1b"><apply id="S4.E1.m3.1.1.cmml" xref="S4.E1.m3.1.1"><ci id="S4.E1.m3.1.1.1.cmml" xref="S4.E1.m3.1.1.1">→</ci><apply id="S4.E1.m3.1.1.2.cmml" xref="S4.E1.m3.1.1.2"><times id="S4.E1.m3.1.1.2.1.cmml" xref="S4.E1.m3.1.1.2.1"></times><apply id="S4.E1.m3.1.1.2.2.cmml" xref="S4.E1.m3.1.1.2.2"><csymbol cd="ambiguous" id="S4.E1.m3.1.1.2.2.1.cmml" xref="S4.E1.m3.1.1.2.2">superscript</csymbol><ci id="S4.E1.m3.1.1.2.2.2.cmml" xref="S4.E1.m3.1.1.2.2.2">ℝ</ci><ci id="S4.E1.m3.1.1.2.2.3.cmml" xref="S4.E1.m3.1.1.2.2.3">𝑑</ci></apply><apply id="S4.E1.m3.1.1.2.3.cmml" xref="S4.E1.m3.1.1.2.3"><csymbol cd="ambiguous" id="S4.E1.m3.1.1.2.3.1.cmml" xref="S4.E1.m3.1.1.2.3">superscript</csymbol><ci id="S4.E1.m3.1.1.2.3.2.cmml" xref="S4.E1.m3.1.1.2.3.2">ℝ</ci><ci id="S4.E1.m3.1.1.2.3.3.cmml" xref="S4.E1.m3.1.1.2.3.3">𝑑</ci></apply><ci id="S4.E1.m3.1.1.2.4.cmml" xref="S4.E1.m3.1.1.2.4">…</ci><apply id="S4.E1.m3.1.1.2.5.cmml" xref="S4.E1.m3.1.1.2.5"><csymbol cd="ambiguous" id="S4.E1.m3.1.1.2.5.1.cmml" xref="S4.E1.m3.1.1.2.5">superscript</csymbol><ci id="S4.E1.m3.1.1.2.5.2.cmml" xref="S4.E1.m3.1.1.2.5.2">ℝ</ci><ci id="S4.E1.m3.1.1.2.5.3.cmml" xref="S4.E1.m3.1.1.2.5.3">𝑑</ci></apply></apply><apply id="S4.E1.m3.1.1.3.cmml" xref="S4.E1.m3.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m3.1.1.3.1.cmml" xref="S4.E1.m3.1.1.3">superscript</csymbol><ci id="S4.E1.m3.1.1.3.2.cmml" xref="S4.E1.m3.1.1.3.2">ℝ</ci><ci id="S4.E1.m3.1.1.3.3.cmml" xref="S4.E1.m3.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m3.1c">\displaystyle\mathbb{R}^{d}\times\mathbb{R}^{d}\times\ldots\times\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</span></div>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p">In this work, we propose the simplest form of fusion. For each weight shared by all models, assign the average weight to the model.</p>
</div>
<div id="S4.p8" class="ltx_para">
<div id="S4.p8.1" class="ltx_inline-block ltx_transformed_outer" style="width:368.6pt;height:11.9pt;vertical-align:-3.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-35.9pt,0.9pt) scale(0.837135612154529,0.837135612154529) ;">
<div id="S4.p8.1.1" class="ltx_block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<table id="Sx1.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E2.m1.1" class="ltx_Math" alttext="\displaystyle W_{fuse}" display="inline"><semantics id="S4.E2.m1.1a"><msub id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><mi id="S4.E2.m1.1.1.2" xref="S4.E2.m1.1.1.2.cmml">W</mi><mrow id="S4.E2.m1.1.1.3" xref="S4.E2.m1.1.1.3.cmml"><mi id="S4.E2.m1.1.1.3.2" xref="S4.E2.m1.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.1" xref="S4.E2.m1.1.1.3.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3" xref="S4.E2.m1.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.1a" xref="S4.E2.m1.1.1.3.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.4" xref="S4.E2.m1.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.1b" xref="S4.E2.m1.1.1.3.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.5" xref="S4.E2.m1.1.1.3.5.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1">subscript</csymbol><ci id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1.2">𝑊</ci><apply id="S4.E2.m1.1.1.3.cmml" xref="S4.E2.m1.1.1.3"><times id="S4.E2.m1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.3.1"></times><ci id="S4.E2.m1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.3.2">𝑓</ci><ci id="S4.E2.m1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.3.3">𝑢</ci><ci id="S4.E2.m1.1.1.3.4.cmml" xref="S4.E2.m1.1.1.3.4">𝑠</ci><ci id="S4.E2.m1.1.1.3.5.cmml" xref="S4.E2.m1.1.1.3.5">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">\displaystyle W_{fuse}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E2.m2.4" class="ltx_Math" alttext="\displaystyle=f\left(W_{1},W_{2},\ldots,W_{n}\right)" display="inline"><semantics id="S4.E2.m2.4a"><mrow id="S4.E2.m2.4.4" xref="S4.E2.m2.4.4.cmml"><mi id="S4.E2.m2.4.4.5" xref="S4.E2.m2.4.4.5.cmml"></mi><mo id="S4.E2.m2.4.4.4" xref="S4.E2.m2.4.4.4.cmml">=</mo><mrow id="S4.E2.m2.4.4.3" xref="S4.E2.m2.4.4.3.cmml"><mi id="S4.E2.m2.4.4.3.5" xref="S4.E2.m2.4.4.3.5.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.E2.m2.4.4.3.4" xref="S4.E2.m2.4.4.3.4.cmml">​</mo><mrow id="S4.E2.m2.4.4.3.3.3" xref="S4.E2.m2.4.4.3.3.4.cmml"><mo id="S4.E2.m2.4.4.3.3.3.4" xref="S4.E2.m2.4.4.3.3.4.cmml">(</mo><msub id="S4.E2.m2.2.2.1.1.1.1" xref="S4.E2.m2.2.2.1.1.1.1.cmml"><mi id="S4.E2.m2.2.2.1.1.1.1.2" xref="S4.E2.m2.2.2.1.1.1.1.2.cmml">W</mi><mn id="S4.E2.m2.2.2.1.1.1.1.3" xref="S4.E2.m2.2.2.1.1.1.1.3.cmml">1</mn></msub><mo id="S4.E2.m2.4.4.3.3.3.5" xref="S4.E2.m2.4.4.3.3.4.cmml">,</mo><msub id="S4.E2.m2.3.3.2.2.2.2" xref="S4.E2.m2.3.3.2.2.2.2.cmml"><mi id="S4.E2.m2.3.3.2.2.2.2.2" xref="S4.E2.m2.3.3.2.2.2.2.2.cmml">W</mi><mn id="S4.E2.m2.3.3.2.2.2.2.3" xref="S4.E2.m2.3.3.2.2.2.2.3.cmml">2</mn></msub><mo id="S4.E2.m2.4.4.3.3.3.6" xref="S4.E2.m2.4.4.3.3.4.cmml">,</mo><mi mathvariant="normal" id="S4.E2.m2.1.1" xref="S4.E2.m2.1.1.cmml">…</mi><mo id="S4.E2.m2.4.4.3.3.3.7" xref="S4.E2.m2.4.4.3.3.4.cmml">,</mo><msub id="S4.E2.m2.4.4.3.3.3.3" xref="S4.E2.m2.4.4.3.3.3.3.cmml"><mi id="S4.E2.m2.4.4.3.3.3.3.2" xref="S4.E2.m2.4.4.3.3.3.3.2.cmml">W</mi><mi id="S4.E2.m2.4.4.3.3.3.3.3" xref="S4.E2.m2.4.4.3.3.3.3.3.cmml">n</mi></msub><mo id="S4.E2.m2.4.4.3.3.3.8" xref="S4.E2.m2.4.4.3.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m2.4b"><apply id="S4.E2.m2.4.4.cmml" xref="S4.E2.m2.4.4"><eq id="S4.E2.m2.4.4.4.cmml" xref="S4.E2.m2.4.4.4"></eq><csymbol cd="latexml" id="S4.E2.m2.4.4.5.cmml" xref="S4.E2.m2.4.4.5">absent</csymbol><apply id="S4.E2.m2.4.4.3.cmml" xref="S4.E2.m2.4.4.3"><times id="S4.E2.m2.4.4.3.4.cmml" xref="S4.E2.m2.4.4.3.4"></times><ci id="S4.E2.m2.4.4.3.5.cmml" xref="S4.E2.m2.4.4.3.5">𝑓</ci><vector id="S4.E2.m2.4.4.3.3.4.cmml" xref="S4.E2.m2.4.4.3.3.3"><apply id="S4.E2.m2.2.2.1.1.1.1.cmml" xref="S4.E2.m2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m2.2.2.1.1.1.1.1.cmml" xref="S4.E2.m2.2.2.1.1.1.1">subscript</csymbol><ci id="S4.E2.m2.2.2.1.1.1.1.2.cmml" xref="S4.E2.m2.2.2.1.1.1.1.2">𝑊</ci><cn type="integer" id="S4.E2.m2.2.2.1.1.1.1.3.cmml" xref="S4.E2.m2.2.2.1.1.1.1.3">1</cn></apply><apply id="S4.E2.m2.3.3.2.2.2.2.cmml" xref="S4.E2.m2.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S4.E2.m2.3.3.2.2.2.2.1.cmml" xref="S4.E2.m2.3.3.2.2.2.2">subscript</csymbol><ci id="S4.E2.m2.3.3.2.2.2.2.2.cmml" xref="S4.E2.m2.3.3.2.2.2.2.2">𝑊</ci><cn type="integer" id="S4.E2.m2.3.3.2.2.2.2.3.cmml" xref="S4.E2.m2.3.3.2.2.2.2.3">2</cn></apply><ci id="S4.E2.m2.1.1.cmml" xref="S4.E2.m2.1.1">…</ci><apply id="S4.E2.m2.4.4.3.3.3.3.cmml" xref="S4.E2.m2.4.4.3.3.3.3"><csymbol cd="ambiguous" id="S4.E2.m2.4.4.3.3.3.3.1.cmml" xref="S4.E2.m2.4.4.3.3.3.3">subscript</csymbol><ci id="S4.E2.m2.4.4.3.3.3.3.2.cmml" xref="S4.E2.m2.4.4.3.3.3.3.2">𝑊</ci><ci id="S4.E2.m2.4.4.3.3.3.3.3.cmml" xref="S4.E2.m2.4.4.3.3.3.3.3">𝑛</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m2.4c">\displaystyle=f\left(W_{1},W_{2},\ldots,W_{n}\right)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E2.m3.1" class="ltx_Math" alttext="\displaystyle=\frac{W_{1}+W_{2}+\ldots+W_{n}}{n}" display="inline"><semantics id="S4.E2.m3.1a"><mrow id="S4.E2.m3.1.1" xref="S4.E2.m3.1.1.cmml"><mi id="S4.E2.m3.1.1.2" xref="S4.E2.m3.1.1.2.cmml"></mi><mo id="S4.E2.m3.1.1.1" xref="S4.E2.m3.1.1.1.cmml">=</mo><mstyle displaystyle="true" id="S4.E2.m3.1.1.3" xref="S4.E2.m3.1.1.3.cmml"><mfrac id="S4.E2.m3.1.1.3a" xref="S4.E2.m3.1.1.3.cmml"><mrow id="S4.E2.m3.1.1.3.2" xref="S4.E2.m3.1.1.3.2.cmml"><msub id="S4.E2.m3.1.1.3.2.2" xref="S4.E2.m3.1.1.3.2.2.cmml"><mi id="S4.E2.m3.1.1.3.2.2.2" xref="S4.E2.m3.1.1.3.2.2.2.cmml">W</mi><mn id="S4.E2.m3.1.1.3.2.2.3" xref="S4.E2.m3.1.1.3.2.2.3.cmml">1</mn></msub><mo id="S4.E2.m3.1.1.3.2.1" xref="S4.E2.m3.1.1.3.2.1.cmml">+</mo><msub id="S4.E2.m3.1.1.3.2.3" xref="S4.E2.m3.1.1.3.2.3.cmml"><mi id="S4.E2.m3.1.1.3.2.3.2" xref="S4.E2.m3.1.1.3.2.3.2.cmml">W</mi><mn id="S4.E2.m3.1.1.3.2.3.3" xref="S4.E2.m3.1.1.3.2.3.3.cmml">2</mn></msub><mo id="S4.E2.m3.1.1.3.2.1a" xref="S4.E2.m3.1.1.3.2.1.cmml">+</mo><mi mathvariant="normal" id="S4.E2.m3.1.1.3.2.4" xref="S4.E2.m3.1.1.3.2.4.cmml">…</mi><mo id="S4.E2.m3.1.1.3.2.1b" xref="S4.E2.m3.1.1.3.2.1.cmml">+</mo><msub id="S4.E2.m3.1.1.3.2.5" xref="S4.E2.m3.1.1.3.2.5.cmml"><mi id="S4.E2.m3.1.1.3.2.5.2" xref="S4.E2.m3.1.1.3.2.5.2.cmml">W</mi><mi id="S4.E2.m3.1.1.3.2.5.3" xref="S4.E2.m3.1.1.3.2.5.3.cmml">n</mi></msub></mrow><mi id="S4.E2.m3.1.1.3.3" xref="S4.E2.m3.1.1.3.3.cmml">n</mi></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m3.1b"><apply id="S4.E2.m3.1.1.cmml" xref="S4.E2.m3.1.1"><eq id="S4.E2.m3.1.1.1.cmml" xref="S4.E2.m3.1.1.1"></eq><csymbol cd="latexml" id="S4.E2.m3.1.1.2.cmml" xref="S4.E2.m3.1.1.2">absent</csymbol><apply id="S4.E2.m3.1.1.3.cmml" xref="S4.E2.m3.1.1.3"><divide id="S4.E2.m3.1.1.3.1.cmml" xref="S4.E2.m3.1.1.3"></divide><apply id="S4.E2.m3.1.1.3.2.cmml" xref="S4.E2.m3.1.1.3.2"><plus id="S4.E2.m3.1.1.3.2.1.cmml" xref="S4.E2.m3.1.1.3.2.1"></plus><apply id="S4.E2.m3.1.1.3.2.2.cmml" xref="S4.E2.m3.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.E2.m3.1.1.3.2.2.1.cmml" xref="S4.E2.m3.1.1.3.2.2">subscript</csymbol><ci id="S4.E2.m3.1.1.3.2.2.2.cmml" xref="S4.E2.m3.1.1.3.2.2.2">𝑊</ci><cn type="integer" id="S4.E2.m3.1.1.3.2.2.3.cmml" xref="S4.E2.m3.1.1.3.2.2.3">1</cn></apply><apply id="S4.E2.m3.1.1.3.2.3.cmml" xref="S4.E2.m3.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.E2.m3.1.1.3.2.3.1.cmml" xref="S4.E2.m3.1.1.3.2.3">subscript</csymbol><ci id="S4.E2.m3.1.1.3.2.3.2.cmml" xref="S4.E2.m3.1.1.3.2.3.2">𝑊</ci><cn type="integer" id="S4.E2.m3.1.1.3.2.3.3.cmml" xref="S4.E2.m3.1.1.3.2.3.3">2</cn></apply><ci id="S4.E2.m3.1.1.3.2.4.cmml" xref="S4.E2.m3.1.1.3.2.4">…</ci><apply id="S4.E2.m3.1.1.3.2.5.cmml" xref="S4.E2.m3.1.1.3.2.5"><csymbol cd="ambiguous" id="S4.E2.m3.1.1.3.2.5.1.cmml" xref="S4.E2.m3.1.1.3.2.5">subscript</csymbol><ci id="S4.E2.m3.1.1.3.2.5.2.cmml" xref="S4.E2.m3.1.1.3.2.5.2">𝑊</ci><ci id="S4.E2.m3.1.1.3.2.5.3.cmml" xref="S4.E2.m3.1.1.3.2.5.3">𝑛</ci></apply></apply><ci id="S4.E2.m3.1.1.3.3.cmml" xref="S4.E2.m3.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m3.1c">\displaystyle=\frac{W_{1}+W_{2}+\ldots+W_{n}}{n}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
</span></div>
</div>
<div id="S4.p9" class="ltx_para">
<p id="S4.p9.4" class="ltx_p">In order to empirically evaluate model fusion in offensive language identification, we consider all possible seven combinations. These include different combinations of two models, such as <math id="S4.p9.1.m1.1" class="ltx_Math" alttext="AHSD+OLID" display="inline"><semantics id="S4.p9.1.m1.1a"><mrow id="S4.p9.1.m1.1.1" xref="S4.p9.1.m1.1.1.cmml"><mrow id="S4.p9.1.m1.1.1.2" xref="S4.p9.1.m1.1.1.2.cmml"><mi id="S4.p9.1.m1.1.1.2.2" xref="S4.p9.1.m1.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.p9.1.m1.1.1.2.1" xref="S4.p9.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.p9.1.m1.1.1.2.3" xref="S4.p9.1.m1.1.1.2.3.cmml">H</mi><mo lspace="0em" rspace="0em" id="S4.p9.1.m1.1.1.2.1a" xref="S4.p9.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.p9.1.m1.1.1.2.4" xref="S4.p9.1.m1.1.1.2.4.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.p9.1.m1.1.1.2.1b" xref="S4.p9.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.p9.1.m1.1.1.2.5" xref="S4.p9.1.m1.1.1.2.5.cmml">D</mi></mrow><mo id="S4.p9.1.m1.1.1.1" xref="S4.p9.1.m1.1.1.1.cmml">+</mo><mrow id="S4.p9.1.m1.1.1.3" xref="S4.p9.1.m1.1.1.3.cmml"><mi id="S4.p9.1.m1.1.1.3.2" xref="S4.p9.1.m1.1.1.3.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S4.p9.1.m1.1.1.3.1" xref="S4.p9.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.p9.1.m1.1.1.3.3" xref="S4.p9.1.m1.1.1.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.p9.1.m1.1.1.3.1a" xref="S4.p9.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.p9.1.m1.1.1.3.4" xref="S4.p9.1.m1.1.1.3.4.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.p9.1.m1.1.1.3.1b" xref="S4.p9.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.p9.1.m1.1.1.3.5" xref="S4.p9.1.m1.1.1.3.5.cmml">D</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p9.1.m1.1b"><apply id="S4.p9.1.m1.1.1.cmml" xref="S4.p9.1.m1.1.1"><plus id="S4.p9.1.m1.1.1.1.cmml" xref="S4.p9.1.m1.1.1.1"></plus><apply id="S4.p9.1.m1.1.1.2.cmml" xref="S4.p9.1.m1.1.1.2"><times id="S4.p9.1.m1.1.1.2.1.cmml" xref="S4.p9.1.m1.1.1.2.1"></times><ci id="S4.p9.1.m1.1.1.2.2.cmml" xref="S4.p9.1.m1.1.1.2.2">𝐴</ci><ci id="S4.p9.1.m1.1.1.2.3.cmml" xref="S4.p9.1.m1.1.1.2.3">𝐻</ci><ci id="S4.p9.1.m1.1.1.2.4.cmml" xref="S4.p9.1.m1.1.1.2.4">𝑆</ci><ci id="S4.p9.1.m1.1.1.2.5.cmml" xref="S4.p9.1.m1.1.1.2.5">𝐷</ci></apply><apply id="S4.p9.1.m1.1.1.3.cmml" xref="S4.p9.1.m1.1.1.3"><times id="S4.p9.1.m1.1.1.3.1.cmml" xref="S4.p9.1.m1.1.1.3.1"></times><ci id="S4.p9.1.m1.1.1.3.2.cmml" xref="S4.p9.1.m1.1.1.3.2">𝑂</ci><ci id="S4.p9.1.m1.1.1.3.3.cmml" xref="S4.p9.1.m1.1.1.3.3">𝐿</ci><ci id="S4.p9.1.m1.1.1.3.4.cmml" xref="S4.p9.1.m1.1.1.3.4">𝐼</ci><ci id="S4.p9.1.m1.1.1.3.5.cmml" xref="S4.p9.1.m1.1.1.3.5">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.1.m1.1c">AHSD+OLID</annotation></semantics></math> and <math id="S4.p9.2.m2.1" class="ltx_Math" alttext="HASOC+HateX" display="inline"><semantics id="S4.p9.2.m2.1a"><mrow id="S4.p9.2.m2.1.1" xref="S4.p9.2.m2.1.1.cmml"><mrow id="S4.p9.2.m2.1.1.2" xref="S4.p9.2.m2.1.1.2.cmml"><mi id="S4.p9.2.m2.1.1.2.2" xref="S4.p9.2.m2.1.1.2.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S4.p9.2.m2.1.1.2.1" xref="S4.p9.2.m2.1.1.2.1.cmml">​</mo><mi id="S4.p9.2.m2.1.1.2.3" xref="S4.p9.2.m2.1.1.2.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.p9.2.m2.1.1.2.1a" xref="S4.p9.2.m2.1.1.2.1.cmml">​</mo><mi id="S4.p9.2.m2.1.1.2.4" xref="S4.p9.2.m2.1.1.2.4.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.p9.2.m2.1.1.2.1b" xref="S4.p9.2.m2.1.1.2.1.cmml">​</mo><mi id="S4.p9.2.m2.1.1.2.5" xref="S4.p9.2.m2.1.1.2.5.cmml">O</mi><mo lspace="0em" rspace="0em" id="S4.p9.2.m2.1.1.2.1c" xref="S4.p9.2.m2.1.1.2.1.cmml">​</mo><mi id="S4.p9.2.m2.1.1.2.6" xref="S4.p9.2.m2.1.1.2.6.cmml">C</mi></mrow><mo id="S4.p9.2.m2.1.1.1" xref="S4.p9.2.m2.1.1.1.cmml">+</mo><mrow id="S4.p9.2.m2.1.1.3" xref="S4.p9.2.m2.1.1.3.cmml"><mi id="S4.p9.2.m2.1.1.3.2" xref="S4.p9.2.m2.1.1.3.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S4.p9.2.m2.1.1.3.1" xref="S4.p9.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.p9.2.m2.1.1.3.3" xref="S4.p9.2.m2.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p9.2.m2.1.1.3.1a" xref="S4.p9.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.p9.2.m2.1.1.3.4" xref="S4.p9.2.m2.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p9.2.m2.1.1.3.1b" xref="S4.p9.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.p9.2.m2.1.1.3.5" xref="S4.p9.2.m2.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p9.2.m2.1.1.3.1c" xref="S4.p9.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.p9.2.m2.1.1.3.6" xref="S4.p9.2.m2.1.1.3.6.cmml">X</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p9.2.m2.1b"><apply id="S4.p9.2.m2.1.1.cmml" xref="S4.p9.2.m2.1.1"><plus id="S4.p9.2.m2.1.1.1.cmml" xref="S4.p9.2.m2.1.1.1"></plus><apply id="S4.p9.2.m2.1.1.2.cmml" xref="S4.p9.2.m2.1.1.2"><times id="S4.p9.2.m2.1.1.2.1.cmml" xref="S4.p9.2.m2.1.1.2.1"></times><ci id="S4.p9.2.m2.1.1.2.2.cmml" xref="S4.p9.2.m2.1.1.2.2">𝐻</ci><ci id="S4.p9.2.m2.1.1.2.3.cmml" xref="S4.p9.2.m2.1.1.2.3">𝐴</ci><ci id="S4.p9.2.m2.1.1.2.4.cmml" xref="S4.p9.2.m2.1.1.2.4">𝑆</ci><ci id="S4.p9.2.m2.1.1.2.5.cmml" xref="S4.p9.2.m2.1.1.2.5">𝑂</ci><ci id="S4.p9.2.m2.1.1.2.6.cmml" xref="S4.p9.2.m2.1.1.2.6">𝐶</ci></apply><apply id="S4.p9.2.m2.1.1.3.cmml" xref="S4.p9.2.m2.1.1.3"><times id="S4.p9.2.m2.1.1.3.1.cmml" xref="S4.p9.2.m2.1.1.3.1"></times><ci id="S4.p9.2.m2.1.1.3.2.cmml" xref="S4.p9.2.m2.1.1.3.2">𝐻</ci><ci id="S4.p9.2.m2.1.1.3.3.cmml" xref="S4.p9.2.m2.1.1.3.3">𝑎</ci><ci id="S4.p9.2.m2.1.1.3.4.cmml" xref="S4.p9.2.m2.1.1.3.4">𝑡</ci><ci id="S4.p9.2.m2.1.1.3.5.cmml" xref="S4.p9.2.m2.1.1.3.5">𝑒</ci><ci id="S4.p9.2.m2.1.1.3.6.cmml" xref="S4.p9.2.m2.1.1.3.6">𝑋</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.2.m2.1c">HASOC+HateX</annotation></semantics></math>, different combinations of three models, such as <math id="S4.p9.3.m3.1" class="ltx_Math" alttext="AHSD+OLID+HASOC" display="inline"><semantics id="S4.p9.3.m3.1a"><mrow id="S4.p9.3.m3.1.1" xref="S4.p9.3.m3.1.1.cmml"><mrow id="S4.p9.3.m3.1.1.2" xref="S4.p9.3.m3.1.1.2.cmml"><mi id="S4.p9.3.m3.1.1.2.2" xref="S4.p9.3.m3.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.p9.3.m3.1.1.2.1" xref="S4.p9.3.m3.1.1.2.1.cmml">​</mo><mi id="S4.p9.3.m3.1.1.2.3" xref="S4.p9.3.m3.1.1.2.3.cmml">H</mi><mo lspace="0em" rspace="0em" id="S4.p9.3.m3.1.1.2.1a" xref="S4.p9.3.m3.1.1.2.1.cmml">​</mo><mi id="S4.p9.3.m3.1.1.2.4" xref="S4.p9.3.m3.1.1.2.4.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.p9.3.m3.1.1.2.1b" xref="S4.p9.3.m3.1.1.2.1.cmml">​</mo><mi id="S4.p9.3.m3.1.1.2.5" xref="S4.p9.3.m3.1.1.2.5.cmml">D</mi></mrow><mo id="S4.p9.3.m3.1.1.1" xref="S4.p9.3.m3.1.1.1.cmml">+</mo><mrow id="S4.p9.3.m3.1.1.3" xref="S4.p9.3.m3.1.1.3.cmml"><mi id="S4.p9.3.m3.1.1.3.2" xref="S4.p9.3.m3.1.1.3.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S4.p9.3.m3.1.1.3.1" xref="S4.p9.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.p9.3.m3.1.1.3.3" xref="S4.p9.3.m3.1.1.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.p9.3.m3.1.1.3.1a" xref="S4.p9.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.p9.3.m3.1.1.3.4" xref="S4.p9.3.m3.1.1.3.4.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.p9.3.m3.1.1.3.1b" xref="S4.p9.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.p9.3.m3.1.1.3.5" xref="S4.p9.3.m3.1.1.3.5.cmml">D</mi></mrow><mo id="S4.p9.3.m3.1.1.1a" xref="S4.p9.3.m3.1.1.1.cmml">+</mo><mrow id="S4.p9.3.m3.1.1.4" xref="S4.p9.3.m3.1.1.4.cmml"><mi id="S4.p9.3.m3.1.1.4.2" xref="S4.p9.3.m3.1.1.4.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S4.p9.3.m3.1.1.4.1" xref="S4.p9.3.m3.1.1.4.1.cmml">​</mo><mi id="S4.p9.3.m3.1.1.4.3" xref="S4.p9.3.m3.1.1.4.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.p9.3.m3.1.1.4.1a" xref="S4.p9.3.m3.1.1.4.1.cmml">​</mo><mi id="S4.p9.3.m3.1.1.4.4" xref="S4.p9.3.m3.1.1.4.4.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.p9.3.m3.1.1.4.1b" xref="S4.p9.3.m3.1.1.4.1.cmml">​</mo><mi id="S4.p9.3.m3.1.1.4.5" xref="S4.p9.3.m3.1.1.4.5.cmml">O</mi><mo lspace="0em" rspace="0em" id="S4.p9.3.m3.1.1.4.1c" xref="S4.p9.3.m3.1.1.4.1.cmml">​</mo><mi id="S4.p9.3.m3.1.1.4.6" xref="S4.p9.3.m3.1.1.4.6.cmml">C</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p9.3.m3.1b"><apply id="S4.p9.3.m3.1.1.cmml" xref="S4.p9.3.m3.1.1"><plus id="S4.p9.3.m3.1.1.1.cmml" xref="S4.p9.3.m3.1.1.1"></plus><apply id="S4.p9.3.m3.1.1.2.cmml" xref="S4.p9.3.m3.1.1.2"><times id="S4.p9.3.m3.1.1.2.1.cmml" xref="S4.p9.3.m3.1.1.2.1"></times><ci id="S4.p9.3.m3.1.1.2.2.cmml" xref="S4.p9.3.m3.1.1.2.2">𝐴</ci><ci id="S4.p9.3.m3.1.1.2.3.cmml" xref="S4.p9.3.m3.1.1.2.3">𝐻</ci><ci id="S4.p9.3.m3.1.1.2.4.cmml" xref="S4.p9.3.m3.1.1.2.4">𝑆</ci><ci id="S4.p9.3.m3.1.1.2.5.cmml" xref="S4.p9.3.m3.1.1.2.5">𝐷</ci></apply><apply id="S4.p9.3.m3.1.1.3.cmml" xref="S4.p9.3.m3.1.1.3"><times id="S4.p9.3.m3.1.1.3.1.cmml" xref="S4.p9.3.m3.1.1.3.1"></times><ci id="S4.p9.3.m3.1.1.3.2.cmml" xref="S4.p9.3.m3.1.1.3.2">𝑂</ci><ci id="S4.p9.3.m3.1.1.3.3.cmml" xref="S4.p9.3.m3.1.1.3.3">𝐿</ci><ci id="S4.p9.3.m3.1.1.3.4.cmml" xref="S4.p9.3.m3.1.1.3.4">𝐼</ci><ci id="S4.p9.3.m3.1.1.3.5.cmml" xref="S4.p9.3.m3.1.1.3.5">𝐷</ci></apply><apply id="S4.p9.3.m3.1.1.4.cmml" xref="S4.p9.3.m3.1.1.4"><times id="S4.p9.3.m3.1.1.4.1.cmml" xref="S4.p9.3.m3.1.1.4.1"></times><ci id="S4.p9.3.m3.1.1.4.2.cmml" xref="S4.p9.3.m3.1.1.4.2">𝐻</ci><ci id="S4.p9.3.m3.1.1.4.3.cmml" xref="S4.p9.3.m3.1.1.4.3">𝐴</ci><ci id="S4.p9.3.m3.1.1.4.4.cmml" xref="S4.p9.3.m3.1.1.4.4">𝑆</ci><ci id="S4.p9.3.m3.1.1.4.5.cmml" xref="S4.p9.3.m3.1.1.4.5">𝑂</ci><ci id="S4.p9.3.m3.1.1.4.6.cmml" xref="S4.p9.3.m3.1.1.4.6">𝐶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.3.m3.1c">AHSD+OLID+HASOC</annotation></semantics></math> and <math id="S4.p9.4.m4.1" class="ltx_Math" alttext="AHSD+OLID+HASOC" display="inline"><semantics id="S4.p9.4.m4.1a"><mrow id="S4.p9.4.m4.1.1" xref="S4.p9.4.m4.1.1.cmml"><mrow id="S4.p9.4.m4.1.1.2" xref="S4.p9.4.m4.1.1.2.cmml"><mi id="S4.p9.4.m4.1.1.2.2" xref="S4.p9.4.m4.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.p9.4.m4.1.1.2.1" xref="S4.p9.4.m4.1.1.2.1.cmml">​</mo><mi id="S4.p9.4.m4.1.1.2.3" xref="S4.p9.4.m4.1.1.2.3.cmml">H</mi><mo lspace="0em" rspace="0em" id="S4.p9.4.m4.1.1.2.1a" xref="S4.p9.4.m4.1.1.2.1.cmml">​</mo><mi id="S4.p9.4.m4.1.1.2.4" xref="S4.p9.4.m4.1.1.2.4.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.p9.4.m4.1.1.2.1b" xref="S4.p9.4.m4.1.1.2.1.cmml">​</mo><mi id="S4.p9.4.m4.1.1.2.5" xref="S4.p9.4.m4.1.1.2.5.cmml">D</mi></mrow><mo id="S4.p9.4.m4.1.1.1" xref="S4.p9.4.m4.1.1.1.cmml">+</mo><mrow id="S4.p9.4.m4.1.1.3" xref="S4.p9.4.m4.1.1.3.cmml"><mi id="S4.p9.4.m4.1.1.3.2" xref="S4.p9.4.m4.1.1.3.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S4.p9.4.m4.1.1.3.1" xref="S4.p9.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.p9.4.m4.1.1.3.3" xref="S4.p9.4.m4.1.1.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.p9.4.m4.1.1.3.1a" xref="S4.p9.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.p9.4.m4.1.1.3.4" xref="S4.p9.4.m4.1.1.3.4.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.p9.4.m4.1.1.3.1b" xref="S4.p9.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.p9.4.m4.1.1.3.5" xref="S4.p9.4.m4.1.1.3.5.cmml">D</mi></mrow><mo id="S4.p9.4.m4.1.1.1a" xref="S4.p9.4.m4.1.1.1.cmml">+</mo><mrow id="S4.p9.4.m4.1.1.4" xref="S4.p9.4.m4.1.1.4.cmml"><mi id="S4.p9.4.m4.1.1.4.2" xref="S4.p9.4.m4.1.1.4.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S4.p9.4.m4.1.1.4.1" xref="S4.p9.4.m4.1.1.4.1.cmml">​</mo><mi id="S4.p9.4.m4.1.1.4.3" xref="S4.p9.4.m4.1.1.4.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.p9.4.m4.1.1.4.1a" xref="S4.p9.4.m4.1.1.4.1.cmml">​</mo><mi id="S4.p9.4.m4.1.1.4.4" xref="S4.p9.4.m4.1.1.4.4.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.p9.4.m4.1.1.4.1b" xref="S4.p9.4.m4.1.1.4.1.cmml">​</mo><mi id="S4.p9.4.m4.1.1.4.5" xref="S4.p9.4.m4.1.1.4.5.cmml">O</mi><mo lspace="0em" rspace="0em" id="S4.p9.4.m4.1.1.4.1c" xref="S4.p9.4.m4.1.1.4.1.cmml">​</mo><mi id="S4.p9.4.m4.1.1.4.6" xref="S4.p9.4.m4.1.1.4.6.cmml">C</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p9.4.m4.1b"><apply id="S4.p9.4.m4.1.1.cmml" xref="S4.p9.4.m4.1.1"><plus id="S4.p9.4.m4.1.1.1.cmml" xref="S4.p9.4.m4.1.1.1"></plus><apply id="S4.p9.4.m4.1.1.2.cmml" xref="S4.p9.4.m4.1.1.2"><times id="S4.p9.4.m4.1.1.2.1.cmml" xref="S4.p9.4.m4.1.1.2.1"></times><ci id="S4.p9.4.m4.1.1.2.2.cmml" xref="S4.p9.4.m4.1.1.2.2">𝐴</ci><ci id="S4.p9.4.m4.1.1.2.3.cmml" xref="S4.p9.4.m4.1.1.2.3">𝐻</ci><ci id="S4.p9.4.m4.1.1.2.4.cmml" xref="S4.p9.4.m4.1.1.2.4">𝑆</ci><ci id="S4.p9.4.m4.1.1.2.5.cmml" xref="S4.p9.4.m4.1.1.2.5">𝐷</ci></apply><apply id="S4.p9.4.m4.1.1.3.cmml" xref="S4.p9.4.m4.1.1.3"><times id="S4.p9.4.m4.1.1.3.1.cmml" xref="S4.p9.4.m4.1.1.3.1"></times><ci id="S4.p9.4.m4.1.1.3.2.cmml" xref="S4.p9.4.m4.1.1.3.2">𝑂</ci><ci id="S4.p9.4.m4.1.1.3.3.cmml" xref="S4.p9.4.m4.1.1.3.3">𝐿</ci><ci id="S4.p9.4.m4.1.1.3.4.cmml" xref="S4.p9.4.m4.1.1.3.4">𝐼</ci><ci id="S4.p9.4.m4.1.1.3.5.cmml" xref="S4.p9.4.m4.1.1.3.5">𝐷</ci></apply><apply id="S4.p9.4.m4.1.1.4.cmml" xref="S4.p9.4.m4.1.1.4"><times id="S4.p9.4.m4.1.1.4.1.cmml" xref="S4.p9.4.m4.1.1.4.1"></times><ci id="S4.p9.4.m4.1.1.4.2.cmml" xref="S4.p9.4.m4.1.1.4.2">𝐻</ci><ci id="S4.p9.4.m4.1.1.4.3.cmml" xref="S4.p9.4.m4.1.1.4.3">𝐴</ci><ci id="S4.p9.4.m4.1.1.4.4.cmml" xref="S4.p9.4.m4.1.1.4.4">𝑆</ci><ci id="S4.p9.4.m4.1.1.4.5.cmml" xref="S4.p9.4.m4.1.1.4.5">𝑂</ci><ci id="S4.p9.4.m4.1.1.4.6.cmml" xref="S4.p9.4.m4.1.1.4.6">𝐶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.4.m4.1c">AHSD+OLID+HASOC</annotation></semantics></math> and finally, the combination of all four models.</p>
</div>
<div id="S4.p10" class="ltx_para ltx_noindent">
<p id="S4.p10.1" class="ltx_p"><span id="S4.p10.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Further Finetunning</span>
The weights of the fused model resulting from step 2 can be anomalous as we followed a naive averaging method. Therefore, we performed a further finetuning step on the fused model. In this step, we fine-tuned the fused model using only one available dataset in a particular environment. We followed the same classification objective described in step 1 and used the same configurations. However, to avoid the model being biased toward the finetuning dataset, we only used <math id="S4.p10.1.m1.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S4.p10.1.m1.1a"><mrow id="S4.p10.1.m1.1.1" xref="S4.p10.1.m1.1.1.cmml"><mn id="S4.p10.1.m1.1.1.2" xref="S4.p10.1.m1.1.1.2.cmml">20</mn><mo id="S4.p10.1.m1.1.1.1" xref="S4.p10.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p10.1.m1.1b"><apply id="S4.p10.1.m1.1.1.cmml" xref="S4.p10.1.m1.1.1"><csymbol cd="latexml" id="S4.p10.1.m1.1.1.1.cmml" xref="S4.p10.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.p10.1.m1.1.1.2.cmml" xref="S4.p10.1.m1.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p10.1.m1.1c">20\%</annotation></semantics></math> of the available training data in the finetuning step.</p>
</div>
<div id="S4.p11" class="ltx_para ltx_noindent">
<p id="S4.p11.1" class="ltx_p">The whole pipeline described above simulates a real-life scenario where the data can not be shared. The machine learning models are trained in separate environments using their own data, as in the first step. In the second step, with model fusion, we combined the models. In the final step. We further fine-tuned the fused model on a particular dataset where we repeated the process for all four datasets. Therefore with this pipeline, the datasets are not shared, and privacy is preserved among the different environments.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.1.   Baseline Models</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We compared our fusion-based approach to two baseline models.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Non-fused Baseline</span> We train a transformer-based baseline using the training set of one of the datasets and evaluate it on the test set of that particular dataset as well as on the test sets of other datasets. We repeated the process for all four datasets with two transformer models; <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_italic">bert-large-cased</span> <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite> and <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_italic">fBERT</span> <cite class="ltx_cite ltx_citemacro_cite">Sarkar et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite>. This baseline reflects the most common approach in offensive language detection, where a model is trained on a dataset available for a particular environment, but evaluated on other datasets in different environments as well.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Ensemble Baseline</span> We also used an ensemble baseline; where we trained four separate transformer models on each dataset. For each test instance, we predicted values from all four models, and the final label is the label predicted with the highest probability from all four models. Similar to our previous experiments we repeated the process for <span id="S4.SS1.p3.1.2" class="ltx_text ltx_font_italic">bert-large-cased</span> <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite> and <span id="S4.SS1.p3.1.3" class="ltx_text ltx_font_italic">fBERT</span> <cite class="ltx_cite ltx_citemacro_cite">Sarkar et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.   Results and Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.3" class="ltx_p">In Table <a href="#S4.T2" title="Table 2 ‣ 4. Methodology ‣ A Federated Learning Approach to Privacy Preserving Offensive Language Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we present the best results from each approach for each dataset. We show the results for fBERT as it provided better overall results. For the AHSD test set, the best result, <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="0.921" display="inline"><semantics id="S5.p1.1.m1.1a"><mn id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">0.921</mn><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><cn type="float" id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">0.921</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">0.921</annotation></semantics></math> Macro F1 score, is obtained when fBERT models are trained on AHSD and OLID and fused, then further fine-tuned on AHSD. For OLID the best result, <math id="S5.p1.2.m2.1" class="ltx_Math" alttext="0.839" display="inline"><semantics id="S5.p1.2.m2.1a"><mn id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml">0.839</mn><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><cn type="float" id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1">0.839</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">0.839</annotation></semantics></math> Macro F1 score was provided when BERT-large-cased models trained on AHSD and OLID were fused and further fine-tuned on AHSD. Similarly, for HateX the best result, <math id="S5.p1.3.m3.1" class="ltx_Math" alttext="0.777" display="inline"><semantics id="S5.p1.3.m3.1a"><mn id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml">0.777</mn><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><cn type="float" id="S5.p1.3.m3.1.1.cmml" xref="S5.p1.3.m3.1.1">0.777</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">0.777</annotation></semantics></math> was provided when the fBERT models trained on AHSD and HateX were fused and further fine-tuned on HateX. However, HASOC follows a different pattern, and the best result was produced when fBERT models trained on AHSD, OLID and HASOC were fused, and further fine-tuned on AHSD. Overall, fBERT models provided slightly better results than BERT-large-cased models in most experiments. This is mainly because the fBERT model was trained on domain-specific data on offensive language identification. Finally, we present all results of the fused models and the non-fused model baseline in Table <a href="#S5.T3" title="Table 3 ‣ 5.1. Discussion ‣ 5. Results and Discussion ‣ A Federated Learning Approach to Privacy Preserving Offensive Language Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> in terms of Macro F1 score.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">5.1.   Discussion</h3>

<figure id="S5.T3" class="ltx_table">
<div id="S5.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:447.5pt;height:348.4pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-137.1pt,106.6pt) scale(0.62,0.62) ;">
<table id="S5.T3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<td id="S5.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T3.1.1.1.1.1" class="ltx_text">
<span id="S5.T3.1.1.1.1.1.1" class="ltx_inline-block">
<span id="S5.T3.1.1.1.1.1.1.1" class="ltx_p"><span id="S5.T3.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Fine-tuned</span></span>
<span id="S5.T3.1.1.1.1.1.1.2" class="ltx_p"><span id="S5.T3.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Dataset</span></span>
</span></span></td>
<td id="S5.T3.1.1.1.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S5.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">Fused Models</span></td>
<td id="S5.T3.1.1.1.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" colspan="4"><span id="S5.T3.1.1.1.5.1" class="ltx_text ltx_font_bold">BERT-large-cased</span></td>
<td id="S5.T3.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" colspan="4"><span id="S5.T3.1.1.1.6.1" class="ltx_text ltx_font_bold">fBERT</span></td>
</tr>
<tr id="S5.T3.1.1.2" class="ltx_tr">
<td id="S5.T3.1.1.2.1" class="ltx_td"></td>
<td id="S5.T3.1.1.2.2" class="ltx_td"></td>
<td id="S5.T3.1.1.2.3" class="ltx_td"></td>
<td id="S5.T3.1.1.2.4" class="ltx_td ltx_border_r"></td>
<td id="S5.T3.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.2.5.1" class="ltx_text ltx_font_bold">AHSD</span></td>
<td id="S5.T3.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.2.6.1" class="ltx_text ltx_font_bold">OLID</span></td>
<td id="S5.T3.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.2.7.1" class="ltx_text ltx_font_bold">HASOC</span></td>
<td id="S5.T3.1.1.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.1.1.2.8.1" class="ltx_text ltx_font_bold">HATEX</span></td>
<td id="S5.T3.1.1.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.2.9.1" class="ltx_text ltx_font_bold">AHSD</span></td>
<td id="S5.T3.1.1.2.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.2.10.1" class="ltx_text ltx_font_bold">OLID</span></td>
<td id="S5.T3.1.1.2.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.2.11.1" class="ltx_text ltx_font_bold">HASOC</span></td>
<td id="S5.T3.1.1.2.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.2.12.1" class="ltx_text ltx_font_bold">HATEX</span></td>
</tr>
<tr id="S5.T3.1.1.3" class="ltx_tr">
<td id="S5.T3.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="8"><span id="S5.T3.1.1.3.1.1" class="ltx_text">AHSD</span></td>
<td id="S5.T3.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">AHSD</td>
<td id="S5.T3.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">OLID</td>
<td id="S5.T3.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T3.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">0.900±0.00</td>
<td id="S5.T3.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t">0.830±0.07</td>
<td id="S5.T3.1.1.3.8" class="ltx_td ltx_align_center ltx_border_t">0.610±0.00</td>
<td id="S5.T3.1.1.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.554±0.06</td>
<td id="S5.T3.1.1.3.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.3.10.1" class="ltx_text ltx_font_bold">0.921±0.00</span></td>
<td id="S5.T3.1.1.3.11" class="ltx_td ltx_align_center ltx_border_t">0.836±0.09</td>
<td id="S5.T3.1.1.3.12" class="ltx_td ltx_align_center ltx_border_t">0.627±0.00</td>
<td id="S5.T3.1.1.3.13" class="ltx_td ltx_align_center ltx_border_t">0.628±0.00</td>
</tr>
<tr id="S5.T3.1.1.4" class="ltx_tr">
<td id="S5.T3.1.1.4.1" class="ltx_td ltx_align_center">AHSD</td>
<td id="S5.T3.1.1.4.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.4.3" class="ltx_td ltx_align_center">HASOC</td>
<td id="S5.T3.1.1.4.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T3.1.1.4.5" class="ltx_td ltx_align_center">0.778±0.14</td>
<td id="S5.T3.1.1.4.6" class="ltx_td ltx_align_center">0.627±0.00</td>
<td id="S5.T3.1.1.4.7" class="ltx_td ltx_align_center">0.637±0.00</td>
<td id="S5.T3.1.1.4.8" class="ltx_td ltx_align_center ltx_border_r">0.607±0.02</td>
<td id="S5.T3.1.1.4.9" class="ltx_td ltx_align_center">0.776±0.04</td>
<td id="S5.T3.1.1.4.10" class="ltx_td ltx_align_center">0.722±0.00</td>
<td id="S5.T3.1.1.4.11" class="ltx_td ltx_align_center">0.632±0.00</td>
<td id="S5.T3.1.1.4.12" class="ltx_td ltx_align_center">0.677±0.05</td>
</tr>
<tr id="S5.T3.1.1.5" class="ltx_tr">
<td id="S5.T3.1.1.5.1" class="ltx_td ltx_align_center">AHSD</td>
<td id="S5.T3.1.1.5.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.5.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.5.4" class="ltx_td ltx_align_center ltx_border_r">HATEX</td>
<td id="S5.T3.1.1.5.5" class="ltx_td ltx_align_center">0.727±0.03</td>
<td id="S5.T3.1.1.5.6" class="ltx_td ltx_align_center">0.697±0.00</td>
<td id="S5.T3.1.1.5.7" class="ltx_td ltx_align_center">0.660±0.04</td>
<td id="S5.T3.1.1.5.8" class="ltx_td ltx_align_center ltx_border_r">0.594±0.00</td>
<td id="S5.T3.1.1.5.9" class="ltx_td ltx_align_center">0.781±0.03</td>
<td id="S5.T3.1.1.5.10" class="ltx_td ltx_align_center">0.707±0.00</td>
<td id="S5.T3.1.1.5.11" class="ltx_td ltx_align_center">0.673±0.03</td>
<td id="S5.T3.1.1.5.12" class="ltx_td ltx_align_center">0.648±0.00</td>
</tr>
<tr id="S5.T3.1.1.6" class="ltx_tr">
<td id="S5.T3.1.1.6.1" class="ltx_td ltx_align_center ltx_border_t">AHSD</td>
<td id="S5.T3.1.1.6.2" class="ltx_td ltx_align_center ltx_border_t">OLID</td>
<td id="S5.T3.1.1.6.3" class="ltx_td ltx_align_center ltx_border_t">HASOC</td>
<td id="S5.T3.1.1.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T3.1.1.6.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.6.5.1" class="ltx_text ltx_font_bold">0.919±0.00</span></td>
<td id="S5.T3.1.1.6.6" class="ltx_td ltx_align_center ltx_border_t">0.837±0.08</td>
<td id="S5.T3.1.1.6.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.6.7.1" class="ltx_text ltx_font_bold">0.766±0.02</span></td>
<td id="S5.T3.1.1.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.636±0.00</td>
<td id="S5.T3.1.1.6.9" class="ltx_td ltx_align_center ltx_border_t">0.915±0.00</td>
<td id="S5.T3.1.1.6.10" class="ltx_td ltx_align_center ltx_border_t">0.835±0.08</td>
<td id="S5.T3.1.1.6.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.6.11.1" class="ltx_text ltx_font_bold">0.770±0.01</span></td>
<td id="S5.T3.1.1.6.12" class="ltx_td ltx_align_center ltx_border_t">0.623±0.00</td>
</tr>
<tr id="S5.T3.1.1.7" class="ltx_tr">
<td id="S5.T3.1.1.7.1" class="ltx_td ltx_align_center">AHSD</td>
<td id="S5.T3.1.1.7.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.7.3" class="ltx_td ltx_align_center">HASOC</td>
<td id="S5.T3.1.1.7.4" class="ltx_td ltx_align_center ltx_border_r">HATEX</td>
<td id="S5.T3.1.1.7.5" class="ltx_td ltx_align_center">0.705±0.06</td>
<td id="S5.T3.1.1.7.6" class="ltx_td ltx_align_center">0.674±0.00</td>
<td id="S5.T3.1.1.7.7" class="ltx_td ltx_align_center">0.595±0.03</td>
<td id="S5.T3.1.1.7.8" class="ltx_td ltx_align_center ltx_border_r">0.565±0.00</td>
<td id="S5.T3.1.1.7.9" class="ltx_td ltx_align_center">0.734±0.03</td>
<td id="S5.T3.1.1.7.10" class="ltx_td ltx_align_center">0.704±0.00</td>
<td id="S5.T3.1.1.7.11" class="ltx_td ltx_align_center">0.643±0.00</td>
<td id="S5.T3.1.1.7.12" class="ltx_td ltx_align_center">0.643±0.00</td>
</tr>
<tr id="S5.T3.1.1.8" class="ltx_tr">
<td id="S5.T3.1.1.8.1" class="ltx_td ltx_align_center">AHSD</td>
<td id="S5.T3.1.1.8.2" class="ltx_td ltx_align_center">OLID</td>
<td id="S5.T3.1.1.8.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.8.4" class="ltx_td ltx_align_center ltx_border_r">HATEX</td>
<td id="S5.T3.1.1.8.5" class="ltx_td ltx_align_center">0.905±0.00</td>
<td id="S5.T3.1.1.8.6" class="ltx_td ltx_align_center">0.813±0.09</td>
<td id="S5.T3.1.1.8.7" class="ltx_td ltx_align_center">0.628±0.00</td>
<td id="S5.T3.1.1.8.8" class="ltx_td ltx_align_center ltx_border_r">0.719±0.05</td>
<td id="S5.T3.1.1.8.9" class="ltx_td ltx_align_center">0.914±0.00</td>
<td id="S5.T3.1.1.8.10" class="ltx_td ltx_align_center">0.834±0.08</td>
<td id="S5.T3.1.1.8.11" class="ltx_td ltx_align_center">0.627±0.00</td>
<td id="S5.T3.1.1.8.12" class="ltx_td ltx_align_center">0.772±0.01</td>
</tr>
<tr id="S5.T3.1.1.9" class="ltx_tr">
<td id="S5.T3.1.1.9.1" class="ltx_td ltx_align_center ltx_border_t">AHSD</td>
<td id="S5.T3.1.1.9.2" class="ltx_td ltx_align_center ltx_border_t">OLID</td>
<td id="S5.T3.1.1.9.3" class="ltx_td ltx_align_center ltx_border_t">HASOC</td>
<td id="S5.T3.1.1.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">HATEX</td>
<td id="S5.T3.1.1.9.5" class="ltx_td ltx_align_center ltx_border_t">0.716±0.03</td>
<td id="S5.T3.1.1.9.6" class="ltx_td ltx_align_center ltx_border_t">0.708±0.00</td>
<td id="S5.T3.1.1.9.7" class="ltx_td ltx_align_center ltx_border_t">0.646±0.05</td>
<td id="S5.T3.1.1.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.652±0.06</td>
<td id="S5.T3.1.1.9.9" class="ltx_td ltx_align_center ltx_border_t">0.730±0.01</td>
<td id="S5.T3.1.1.9.10" class="ltx_td ltx_align_center ltx_border_t">0.724±0.00</td>
<td id="S5.T3.1.1.9.11" class="ltx_td ltx_align_center ltx_border_t">0.668±0.04</td>
<td id="S5.T3.1.1.9.12" class="ltx_td ltx_align_center ltx_border_t">0.684±0.04</td>
</tr>
<tr id="S5.T3.1.1.10" class="ltx_tr">
<td id="S5.T3.1.1.10.1" class="ltx_td ltx_align_center ltx_border_t" colspan="3"><span id="S5.T3.1.1.10.1.1" class="ltx_text ltx_font_bold">Non-fused Baseline</span></td>
<td id="S5.T3.1.1.10.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.10.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.10.3.1" class="ltx_text ltx_framed ltx_framed_underline">0.926±0.01</span></td>
<td id="S5.T3.1.1.10.4" class="ltx_td ltx_align_center ltx_border_t">0.699±0.03</td>
<td id="S5.T3.1.1.10.5" class="ltx_td ltx_align_center ltx_border_t">0.630±0.05</td>
<td id="S5.T3.1.1.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.586±0.06</td>
<td id="S5.T3.1.1.10.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.10.7.1" class="ltx_text ltx_framed ltx_framed_underline">0.931±0.01</span></td>
<td id="S5.T3.1.1.10.8" class="ltx_td ltx_align_center ltx_border_t">0.743±0.03</td>
<td id="S5.T3.1.1.10.9" class="ltx_td ltx_align_center ltx_border_t">0.682±0.04</td>
<td id="S5.T3.1.1.10.10" class="ltx_td ltx_align_center ltx_border_t">0.606±0.06</td>
</tr>
<tr id="S5.T3.1.1.11" class="ltx_tr">
<td id="S5.T3.1.1.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="8"><span id="S5.T3.1.1.11.1.1" class="ltx_text">OLID</span></td>
<td id="S5.T3.1.1.11.2" class="ltx_td ltx_align_center ltx_border_t">AHSD</td>
<td id="S5.T3.1.1.11.3" class="ltx_td ltx_align_center ltx_border_t">OLID</td>
<td id="S5.T3.1.1.11.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T3.1.1.11.6" class="ltx_td ltx_align_center ltx_border_t">0.893±0.00</td>
<td id="S5.T3.1.1.11.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.11.7.1" class="ltx_text ltx_font_bold">0.839±0.05</span></td>
<td id="S5.T3.1.1.11.8" class="ltx_td ltx_align_center ltx_border_t">0.647±0.00</td>
<td id="S5.T3.1.1.11.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.621±0.03</td>
<td id="S5.T3.1.1.11.10" class="ltx_td ltx_align_center ltx_border_t">0.866±0.00</td>
<td id="S5.T3.1.1.11.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.11.11.1" class="ltx_text ltx_font_bold">0.837±0.03</span></td>
<td id="S5.T3.1.1.11.12" class="ltx_td ltx_align_center ltx_border_t">0.601±0.00</td>
<td id="S5.T3.1.1.11.13" class="ltx_td ltx_align_center ltx_border_t">0.598±0.00</td>
</tr>
<tr id="S5.T3.1.1.12" class="ltx_tr">
<td id="S5.T3.1.1.12.1" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.12.2" class="ltx_td ltx_align_center">OLID</td>
<td id="S5.T3.1.1.12.3" class="ltx_td ltx_align_center">HASOC</td>
<td id="S5.T3.1.1.12.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T3.1.1.12.5" class="ltx_td ltx_align_center">0.715±0.00</td>
<td id="S5.T3.1.1.12.6" class="ltx_td ltx_align_center">0.405±0.01</td>
<td id="S5.T3.1.1.12.7" class="ltx_td ltx_align_center">0.392±0.00</td>
<td id="S5.T3.1.1.12.8" class="ltx_td ltx_align_center ltx_border_r">0.651±0.06</td>
<td id="S5.T3.1.1.12.9" class="ltx_td ltx_align_center">0.718±0.00</td>
<td id="S5.T3.1.1.12.10" class="ltx_td ltx_align_center">0.725±0.07</td>
<td id="S5.T3.1.1.12.11" class="ltx_td ltx_align_center">0.655±0.00</td>
<td id="S5.T3.1.1.12.12" class="ltx_td ltx_align_center">0.667±0.05</td>
</tr>
<tr id="S5.T3.1.1.13" class="ltx_tr">
<td id="S5.T3.1.1.13.1" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.13.2" class="ltx_td ltx_align_center">OLID</td>
<td id="S5.T3.1.1.13.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.13.4" class="ltx_td ltx_align_center ltx_border_r">HATEX</td>
<td id="S5.T3.1.1.13.5" class="ltx_td ltx_align_center">0.696±0.00</td>
<td id="S5.T3.1.1.13.6" class="ltx_td ltx_align_center">0.692±0.08</td>
<td id="S5.T3.1.1.13.7" class="ltx_td ltx_align_center">0.656±0.04</td>
<td id="S5.T3.1.1.13.8" class="ltx_td ltx_align_center ltx_border_r">0.616±0.00</td>
<td id="S5.T3.1.1.13.9" class="ltx_td ltx_align_center">0.679±0.07</td>
<td id="S5.T3.1.1.13.10" class="ltx_td ltx_align_center">0.723±0.07</td>
<td id="S5.T3.1.1.13.11" class="ltx_td ltx_align_center">0.611±0.00</td>
<td id="S5.T3.1.1.13.12" class="ltx_td ltx_align_center">0.650±0.00</td>
</tr>
<tr id="S5.T3.1.1.14" class="ltx_tr">
<td id="S5.T3.1.1.14.1" class="ltx_td ltx_align_center ltx_border_t">AHSD</td>
<td id="S5.T3.1.1.14.2" class="ltx_td ltx_align_center ltx_border_t">OLID</td>
<td id="S5.T3.1.1.14.3" class="ltx_td ltx_align_center ltx_border_t">HASOC</td>
<td id="S5.T3.1.1.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T3.1.1.14.5" class="ltx_td ltx_align_center ltx_border_t">0.868±0.00</td>
<td id="S5.T3.1.1.14.6" class="ltx_td ltx_align_center ltx_border_t">0.826±0.04</td>
<td id="S5.T3.1.1.14.7" class="ltx_td ltx_align_center ltx_border_t">0.756±0.00</td>
<td id="S5.T3.1.1.14.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.608±0.00</td>
<td id="S5.T3.1.1.14.9" class="ltx_td ltx_align_center ltx_border_t">0.840±0.00</td>
<td id="S5.T3.1.1.14.10" class="ltx_td ltx_align_center ltx_border_t">0.819±0.02</td>
<td id="S5.T3.1.1.14.11" class="ltx_td ltx_align_center ltx_border_t">0.759±0.09</td>
<td id="S5.T3.1.1.14.12" class="ltx_td ltx_align_center ltx_border_t">0.606±0.00</td>
</tr>
<tr id="S5.T3.1.1.15" class="ltx_tr">
<td id="S5.T3.1.1.15.1" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.15.2" class="ltx_td ltx_align_center">OLID</td>
<td id="S5.T3.1.1.15.3" class="ltx_td ltx_align_center">HASOC</td>
<td id="S5.T3.1.1.15.4" class="ltx_td ltx_align_center ltx_border_r">HATEX</td>
<td id="S5.T3.1.1.15.5" class="ltx_td ltx_align_center">0.687±0.00</td>
<td id="S5.T3.1.1.15.6" class="ltx_td ltx_align_center">0.649±0.09</td>
<td id="S5.T3.1.1.15.7" class="ltx_td ltx_align_center">0.586±0.01</td>
<td id="S5.T3.1.1.15.8" class="ltx_td ltx_align_center ltx_border_r">0.596±0.00</td>
<td id="S5.T3.1.1.15.9" class="ltx_td ltx_align_center">0.729±0.00</td>
<td id="S5.T3.1.1.15.10" class="ltx_td ltx_align_center">0.694±0.08</td>
<td id="S5.T3.1.1.15.11" class="ltx_td ltx_align_center">0.637±0.01</td>
<td id="S5.T3.1.1.15.12" class="ltx_td ltx_align_center">0.630±0.00</td>
</tr>
<tr id="S5.T3.1.1.16" class="ltx_tr">
<td id="S5.T3.1.1.16.1" class="ltx_td ltx_align_center">AHSD</td>
<td id="S5.T3.1.1.16.2" class="ltx_td ltx_align_center">OLID</td>
<td id="S5.T3.1.1.16.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.16.4" class="ltx_td ltx_align_center ltx_border_r">HATEX</td>
<td id="S5.T3.1.1.16.5" class="ltx_td ltx_align_center">0.847±0.00</td>
<td id="S5.T3.1.1.16.6" class="ltx_td ltx_align_center">0.812±0.04</td>
<td id="S5.T3.1.1.16.7" class="ltx_td ltx_align_center">0.642±0.00</td>
<td id="S5.T3.1.1.16.8" class="ltx_td ltx_align_center ltx_border_r">0.751±0.09</td>
<td id="S5.T3.1.1.16.9" class="ltx_td ltx_align_center">0.861±0.00</td>
<td id="S5.T3.1.1.16.10" class="ltx_td ltx_align_center">0.831±0.03</td>
<td id="S5.T3.1.1.16.11" class="ltx_td ltx_align_center">0.615±0.00</td>
<td id="S5.T3.1.1.16.12" class="ltx_td ltx_align_center">0.752±0.01</td>
</tr>
<tr id="S5.T3.1.1.17" class="ltx_tr">
<td id="S5.T3.1.1.17.1" class="ltx_td ltx_align_center ltx_border_t">AHSD</td>
<td id="S5.T3.1.1.17.2" class="ltx_td ltx_align_center ltx_border_t">OLID</td>
<td id="S5.T3.1.1.17.3" class="ltx_td ltx_align_center ltx_border_t">HASOC</td>
<td id="S5.T3.1.1.17.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">HATEX</td>
<td id="S5.T3.1.1.17.5" class="ltx_td ltx_align_center ltx_border_t">0.713±0.00</td>
<td id="S5.T3.1.1.17.6" class="ltx_td ltx_align_center ltx_border_t">0.777±0.00</td>
<td id="S5.T3.1.1.17.7" class="ltx_td ltx_align_center ltx_border_t">0.672±0.07</td>
<td id="S5.T3.1.1.17.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.699±0.08</td>
<td id="S5.T3.1.1.17.9" class="ltx_td ltx_align_center ltx_border_t">0.708±0.08</td>
<td id="S5.T3.1.1.17.10" class="ltx_td ltx_align_center ltx_border_t">0.793±0.00</td>
<td id="S5.T3.1.1.17.11" class="ltx_td ltx_align_center ltx_border_t">0.682±0.08</td>
<td id="S5.T3.1.1.17.12" class="ltx_td ltx_align_center ltx_border_t">0.707±0.09</td>
</tr>
<tr id="S5.T3.1.1.18" class="ltx_tr">
<td id="S5.T3.1.1.18.1" class="ltx_td ltx_align_center ltx_border_t" colspan="3"><span id="S5.T3.1.1.18.1.1" class="ltx_text ltx_font_bold">Non-fused Baseline</span></td>
<td id="S5.T3.1.1.18.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.18.3" class="ltx_td ltx_align_center ltx_border_t">0.685±0.02</td>
<td id="S5.T3.1.1.18.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.18.4.1" class="ltx_text ltx_framed ltx_framed_underline">0.845±0.00</span></td>
<td id="S5.T3.1.1.18.5" class="ltx_td ltx_align_center ltx_border_t">0.636±0.05</td>
<td id="S5.T3.1.1.18.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.620±0.06</td>
<td id="S5.T3.1.1.18.7" class="ltx_td ltx_align_center ltx_border_t">0.702±0.01</td>
<td id="S5.T3.1.1.18.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.18.8.1" class="ltx_text ltx_framed ltx_framed_underline">0.851±0.00</span></td>
<td id="S5.T3.1.1.18.9" class="ltx_td ltx_align_center ltx_border_t">0.653±0.05</td>
<td id="S5.T3.1.1.18.10" class="ltx_td ltx_align_center ltx_border_t">0.645±0.08</td>
</tr>
<tr id="S5.T3.1.1.19" class="ltx_tr">
<td id="S5.T3.1.1.19.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="8"><span id="S5.T3.1.1.19.1.1" class="ltx_text">HASOC</span></td>
<td id="S5.T3.1.1.19.2" class="ltx_td ltx_align_center ltx_border_t">AHSD</td>
<td id="S5.T3.1.1.19.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.19.4" class="ltx_td ltx_align_center ltx_border_t">HASOC</td>
<td id="S5.T3.1.1.19.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T3.1.1.19.6" class="ltx_td ltx_align_center ltx_border_t">0.777±0.13</td>
<td id="S5.T3.1.1.19.7" class="ltx_td ltx_align_center ltx_border_t">0.419±0.00</td>
<td id="S5.T3.1.1.19.8" class="ltx_td ltx_align_center ltx_border_t">0.652±0.00</td>
<td id="S5.T3.1.1.19.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.356±0.06</td>
<td id="S5.T3.1.1.19.10" class="ltx_td ltx_align_center ltx_border_t">0.792±0.11</td>
<td id="S5.T3.1.1.19.11" class="ltx_td ltx_align_center ltx_border_t">0.785±0.05</td>
<td id="S5.T3.1.1.19.12" class="ltx_td ltx_align_center ltx_border_t">0.680±0.00</td>
<td id="S5.T3.1.1.19.13" class="ltx_td ltx_align_center ltx_border_t">0.708±0.08</td>
</tr>
<tr id="S5.T3.1.1.20" class="ltx_tr">
<td id="S5.T3.1.1.20.1" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.20.2" class="ltx_td ltx_align_center">OLID</td>
<td id="S5.T3.1.1.20.3" class="ltx_td ltx_align_center">HASOC</td>
<td id="S5.T3.1.1.20.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T3.1.1.20.5" class="ltx_td ltx_align_center">0.147±0.00</td>
<td id="S5.T3.1.1.20.6" class="ltx_td ltx_align_center">0.707±0.05</td>
<td id="S5.T3.1.1.20.7" class="ltx_td ltx_align_center">0.656±0.00</td>
<td id="S5.T3.1.1.20.8" class="ltx_td ltx_align_center ltx_border_r">0.220±0.07</td>
<td id="S5.T3.1.1.20.9" class="ltx_td ltx_align_center">0.717±0.00</td>
<td id="S5.T3.1.1.20.10" class="ltx_td ltx_align_center">0.734±0.05</td>
<td id="S5.T3.1.1.20.11" class="ltx_td ltx_align_center">0.683±0.00</td>
<td id="S5.T3.1.1.20.12" class="ltx_td ltx_align_center">0.673±0.04</td>
</tr>
<tr id="S5.T3.1.1.21" class="ltx_tr">
<td id="S5.T3.1.1.21.1" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.21.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.21.3" class="ltx_td ltx_align_center">HASOC</td>
<td id="S5.T3.1.1.21.4" class="ltx_td ltx_align_center ltx_border_r">HATEX</td>
<td id="S5.T3.1.1.21.5" class="ltx_td ltx_align_center">0.530±0.05</td>
<td id="S5.T3.1.1.21.6" class="ltx_td ltx_align_center">0.480±0.00</td>
<td id="S5.T3.1.1.21.7" class="ltx_td ltx_align_center">0.695±0.04</td>
<td id="S5.T3.1.1.21.8" class="ltx_td ltx_align_center ltx_border_r">0.738±0.00</td>
<td id="S5.T3.1.1.21.9" class="ltx_td ltx_align_center">0.761±0.03</td>
<td id="S5.T3.1.1.21.10" class="ltx_td ltx_align_center">0.791±0.00</td>
<td id="S5.T3.1.1.21.11" class="ltx_td ltx_align_center">0.689±0.00</td>
<td id="S5.T3.1.1.21.12" class="ltx_td ltx_align_center">0.690±0.00</td>
</tr>
<tr id="S5.T3.1.1.22" class="ltx_tr">
<td id="S5.T3.1.1.22.1" class="ltx_td ltx_align_center ltx_border_t">AHSD</td>
<td id="S5.T3.1.1.22.2" class="ltx_td ltx_align_center ltx_border_t">OLID</td>
<td id="S5.T3.1.1.22.3" class="ltx_td ltx_align_center ltx_border_t">HASOC</td>
<td id="S5.T3.1.1.22.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T3.1.1.22.5" class="ltx_td ltx_align_center ltx_border_t">0.864±0.00</td>
<td id="S5.T3.1.1.22.6" class="ltx_td ltx_align_center ltx_border_t">0.812±0.05</td>
<td id="S5.T3.1.1.22.7" class="ltx_td ltx_align_center ltx_border_t">0.763±0.08</td>
<td id="S5.T3.1.1.22.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.624±0.00</td>
<td id="S5.T3.1.1.22.9" class="ltx_td ltx_align_center ltx_border_t">0.805±0.00</td>
<td id="S5.T3.1.1.22.10" class="ltx_td ltx_align_center ltx_border_t">0.801±0.00</td>
<td id="S5.T3.1.1.22.11" class="ltx_td ltx_align_center ltx_border_t">0.754±0.07</td>
<td id="S5.T3.1.1.22.12" class="ltx_td ltx_align_center ltx_border_t">0.635±0.00</td>
</tr>
<tr id="S5.T3.1.1.23" class="ltx_tr">
<td id="S5.T3.1.1.23.1" class="ltx_td ltx_align_center">AHSD</td>
<td id="S5.T3.1.1.23.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.23.3" class="ltx_td ltx_align_center">HASOC</td>
<td id="S5.T3.1.1.23.4" class="ltx_td ltx_align_center ltx_border_r">HATEX</td>
<td id="S5.T3.1.1.23.5" class="ltx_td ltx_align_center">0.754±0.01</td>
<td id="S5.T3.1.1.23.6" class="ltx_td ltx_align_center">0.419±0.00</td>
<td id="S5.T3.1.1.23.7" class="ltx_td ltx_align_center">0.686±0.01</td>
<td id="S5.T3.1.1.23.8" class="ltx_td ltx_align_center ltx_border_r">0.698±0.00</td>
<td id="S5.T3.1.1.23.9" class="ltx_td ltx_align_center">0.734±0.09</td>
<td id="S5.T3.1.1.23.10" class="ltx_td ltx_align_center">0.780±0.00</td>
<td id="S5.T3.1.1.23.11" class="ltx_td ltx_align_center">0.668±0.01</td>
<td id="S5.T3.1.1.23.12" class="ltx_td ltx_align_center">0.661±0.00</td>
</tr>
<tr id="S5.T3.1.1.24" class="ltx_tr">
<td id="S5.T3.1.1.24.1" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.24.2" class="ltx_td ltx_align_center">OLID</td>
<td id="S5.T3.1.1.24.3" class="ltx_td ltx_align_center">HASOC</td>
<td id="S5.T3.1.1.24.4" class="ltx_td ltx_align_center ltx_border_r">HATEX</td>
<td id="S5.T3.1.1.24.5" class="ltx_td ltx_align_center">0.732±0.00</td>
<td id="S5.T3.1.1.24.6" class="ltx_td ltx_align_center">0.700±0.04</td>
<td id="S5.T3.1.1.24.7" class="ltx_td ltx_align_center">0.675±0.01</td>
<td id="S5.T3.1.1.24.8" class="ltx_td ltx_align_center ltx_border_r">0.686±0.00</td>
<td id="S5.T3.1.1.24.9" class="ltx_td ltx_align_center">0.736±0.00</td>
<td id="S5.T3.1.1.24.10" class="ltx_td ltx_align_center">0.712±0.06</td>
<td id="S5.T3.1.1.24.11" class="ltx_td ltx_align_center">0.671±0.00</td>
<td id="S5.T3.1.1.24.12" class="ltx_td ltx_align_center">0.676±0.00</td>
</tr>
<tr id="S5.T3.1.1.25" class="ltx_tr">
<td id="S5.T3.1.1.25.1" class="ltx_td ltx_align_center ltx_border_t">AHSD</td>
<td id="S5.T3.1.1.25.2" class="ltx_td ltx_align_center ltx_border_t">OLID</td>
<td id="S5.T3.1.1.25.3" class="ltx_td ltx_align_center ltx_border_t">HASOC</td>
<td id="S5.T3.1.1.25.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">HATEX</td>
<td id="S5.T3.1.1.25.5" class="ltx_td ltx_align_center ltx_border_t">0.703±0.09</td>
<td id="S5.T3.1.1.25.6" class="ltx_td ltx_align_center ltx_border_t">0.647±0.00</td>
<td id="S5.T3.1.1.25.7" class="ltx_td ltx_align_center ltx_border_t">0.651±0.00</td>
<td id="S5.T3.1.1.25.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.651±0.00</td>
<td id="S5.T3.1.1.25.9" class="ltx_td ltx_align_center ltx_border_t">0.719±0.06</td>
<td id="S5.T3.1.1.25.10" class="ltx_td ltx_align_center ltx_border_t">0.781±0.00</td>
<td id="S5.T3.1.1.25.11" class="ltx_td ltx_align_center ltx_border_t">0.702±0.06</td>
<td id="S5.T3.1.1.25.12" class="ltx_td ltx_align_center ltx_border_t">0.718±0.06</td>
</tr>
<tr id="S5.T3.1.1.26" class="ltx_tr">
<td id="S5.T3.1.1.26.1" class="ltx_td ltx_align_center ltx_border_t" colspan="3"><span id="S5.T3.1.1.26.1.1" class="ltx_text ltx_font_bold">Non-fused Baseline</span></td>
<td id="S5.T3.1.1.26.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.26.3" class="ltx_td ltx_align_center ltx_border_t">0.620±0.03</td>
<td id="S5.T3.1.1.26.4" class="ltx_td ltx_align_center ltx_border_t">0.492±0.01</td>
<td id="S5.T3.1.1.26.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.26.5.1" class="ltx_text ltx_framed ltx_framed_underline">0.788±0.01</span></td>
<td id="S5.T3.1.1.26.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.555±0.06</td>
<td id="S5.T3.1.1.26.7" class="ltx_td ltx_align_center ltx_border_t">0.645±0.02</td>
<td id="S5.T3.1.1.26.8" class="ltx_td ltx_align_center ltx_border_t">0.532±0.01</td>
<td id="S5.T3.1.1.26.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.26.9.1" class="ltx_text ltx_framed ltx_framed_underline">0.798±0.01</span></td>
<td id="S5.T3.1.1.26.10" class="ltx_td ltx_align_center ltx_border_t">0.575±0.05</td>
</tr>
<tr id="S5.T3.1.1.27" class="ltx_tr">
<td id="S5.T3.1.1.27.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="8"><span id="S5.T3.1.1.27.1.1" class="ltx_text">HATEX</span></td>
<td id="S5.T3.1.1.27.2" class="ltx_td ltx_align_center ltx_border_t">AHSD</td>
<td id="S5.T3.1.1.27.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.27.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.27.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">HATEX</td>
<td id="S5.T3.1.1.27.6" class="ltx_td ltx_align_center ltx_border_t">0.758±0.01</td>
<td id="S5.T3.1.1.27.7" class="ltx_td ltx_align_center ltx_border_t">0.449±0.00</td>
<td id="S5.T3.1.1.27.8" class="ltx_td ltx_align_center ltx_border_t">0.531±0.08</td>
<td id="S5.T3.1.1.27.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.744±0.00</td>
<td id="S5.T3.1.1.27.10" class="ltx_td ltx_align_center ltx_border_t">0.671±0.01</td>
<td id="S5.T3.1.1.27.11" class="ltx_td ltx_align_center ltx_border_t">0.591±0.00</td>
<td id="S5.T3.1.1.27.12" class="ltx_td ltx_align_center ltx_border_t">0.587±0.00</td>
<td id="S5.T3.1.1.27.13" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.27.13.1" class="ltx_text ltx_font_bold">0.777±0.00</span></td>
</tr>
<tr id="S5.T3.1.1.28" class="ltx_tr">
<td id="S5.T3.1.1.28.1" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.28.2" class="ltx_td ltx_align_center">OLID</td>
<td id="S5.T3.1.1.28.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.28.4" class="ltx_td ltx_align_center ltx_border_r">HATEX</td>
<td id="S5.T3.1.1.28.5" class="ltx_td ltx_align_center">0.650±0.00</td>
<td id="S5.T3.1.1.28.6" class="ltx_td ltx_align_center">0.689±0.06</td>
<td id="S5.T3.1.1.28.7" class="ltx_td ltx_align_center">0.557±0.09</td>
<td id="S5.T3.1.1.28.8" class="ltx_td ltx_align_center ltx_border_r">0.749±0.00</td>
<td id="S5.T3.1.1.28.9" class="ltx_td ltx_align_center">0.584±0.02</td>
<td id="S5.T3.1.1.28.10" class="ltx_td ltx_align_center">0.668±0.01</td>
<td id="S5.T3.1.1.28.11" class="ltx_td ltx_align_center">0.599±0.00</td>
<td id="S5.T3.1.1.28.12" class="ltx_td ltx_align_center">0.775±0.00</td>
</tr>
<tr id="S5.T3.1.1.29" class="ltx_tr">
<td id="S5.T3.1.1.29.1" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.29.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.29.3" class="ltx_td ltx_align_center">HASOC</td>
<td id="S5.T3.1.1.29.4" class="ltx_td ltx_align_center ltx_border_r">HATEX</td>
<td id="S5.T3.1.1.29.5" class="ltx_td ltx_align_center">0.538±0.01</td>
<td id="S5.T3.1.1.29.6" class="ltx_td ltx_align_center">0.545±0.0</td>
<td id="S5.T3.1.1.29.7" class="ltx_td ltx_align_center">0.710±0.05</td>
<td id="S5.T3.1.1.29.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.1.1.29.8.1" class="ltx_text ltx_font_bold">0.756±0.00</span></td>
<td id="S5.T3.1.1.29.9" class="ltx_td ltx_align_center">0.527±0.05</td>
<td id="S5.T3.1.1.29.10" class="ltx_td ltx_align_center">0.573±0.00</td>
<td id="S5.T3.1.1.29.11" class="ltx_td ltx_align_center">0.707±0.07</td>
<td id="S5.T3.1.1.29.12" class="ltx_td ltx_align_center">0.772±0.00</td>
</tr>
<tr id="S5.T3.1.1.30" class="ltx_tr">
<td id="S5.T3.1.1.30.1" class="ltx_td ltx_align_center ltx_border_t">AHSD</td>
<td id="S5.T3.1.1.30.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.30.3" class="ltx_td ltx_align_center ltx_border_t">HASOC</td>
<td id="S5.T3.1.1.30.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">HATEX</td>
<td id="S5.T3.1.1.30.5" class="ltx_td ltx_align_center ltx_border_t">0.692±0.04</td>
<td id="S5.T3.1.1.30.6" class="ltx_td ltx_align_center ltx_border_t">0.529±0.00</td>
<td id="S5.T3.1.1.30.7" class="ltx_td ltx_align_center ltx_border_t">0.693±0.05</td>
<td id="S5.T3.1.1.30.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.741±0.00</td>
<td id="S5.T3.1.1.30.9" class="ltx_td ltx_align_center ltx_border_t">0.636±0.10</td>
<td id="S5.T3.1.1.30.10" class="ltx_td ltx_align_center ltx_border_t">0.588±0.00</td>
<td id="S5.T3.1.1.30.11" class="ltx_td ltx_align_center ltx_border_t">0.688±0.08</td>
<td id="S5.T3.1.1.30.12" class="ltx_td ltx_align_center ltx_border_t">0.767±0.00</td>
</tr>
<tr id="S5.T3.1.1.31" class="ltx_tr">
<td id="S5.T3.1.1.31.1" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.31.2" class="ltx_td ltx_align_center">OLID</td>
<td id="S5.T3.1.1.31.3" class="ltx_td ltx_align_center">HASOC</td>
<td id="S5.T3.1.1.31.4" class="ltx_td ltx_align_center ltx_border_r">HATEX</td>
<td id="S5.T3.1.1.31.5" class="ltx_td ltx_align_center">0.561±0.00</td>
<td id="S5.T3.1.1.31.6" class="ltx_td ltx_align_center">0.640±0.09</td>
<td id="S5.T3.1.1.31.7" class="ltx_td ltx_align_center">0.690±0.06</td>
<td id="S5.T3.1.1.31.8" class="ltx_td ltx_align_center ltx_border_r">0.755±0.00</td>
<td id="S5.T3.1.1.31.9" class="ltx_td ltx_align_center">0.526±0.00</td>
<td id="S5.T3.1.1.31.10" class="ltx_td ltx_align_center">0.664±0.08</td>
<td id="S5.T3.1.1.31.11" class="ltx_td ltx_align_center">0.689±0.08</td>
<td id="S5.T3.1.1.31.12" class="ltx_td ltx_align_center">0.772±0.00</td>
</tr>
<tr id="S5.T3.1.1.32" class="ltx_tr">
<td id="S5.T3.1.1.32.1" class="ltx_td ltx_align_center">AHSD</td>
<td id="S5.T3.1.1.32.2" class="ltx_td ltx_align_center">OLID</td>
<td id="S5.T3.1.1.32.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.32.4" class="ltx_td ltx_align_center ltx_border_r">HATEX</td>
<td id="S5.T3.1.1.32.5" class="ltx_td ltx_align_center">0.522±0.00</td>
<td id="S5.T3.1.1.32.6" class="ltx_td ltx_align_center">0.597±0.08</td>
<td id="S5.T3.1.1.32.7" class="ltx_td ltx_align_center">0.607±0.00</td>
<td id="S5.T3.1.1.32.8" class="ltx_td ltx_align_center ltx_border_r">0.645±0.09</td>
<td id="S5.T3.1.1.32.9" class="ltx_td ltx_align_center">0.532±0.00</td>
<td id="S5.T3.1.1.32.10" class="ltx_td ltx_align_center">0.563±0.03</td>
<td id="S5.T3.1.1.32.11" class="ltx_td ltx_align_center">0.613±0.00</td>
<td id="S5.T3.1.1.32.12" class="ltx_td ltx_align_center">0.633±0.10</td>
</tr>
<tr id="S5.T3.1.1.33" class="ltx_tr">
<td id="S5.T3.1.1.33.1" class="ltx_td ltx_align_center ltx_border_t">AHSD</td>
<td id="S5.T3.1.1.33.2" class="ltx_td ltx_align_center ltx_border_t">OLID</td>
<td id="S5.T3.1.1.33.3" class="ltx_td ltx_align_center ltx_border_t">HASOC</td>
<td id="S5.T3.1.1.33.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">HATEX</td>
<td id="S5.T3.1.1.33.5" class="ltx_td ltx_align_center ltx_border_t">0.627±0.08</td>
<td id="S5.T3.1.1.33.6" class="ltx_td ltx_align_center ltx_border_t">0.532±0.00</td>
<td id="S5.T3.1.1.33.7" class="ltx_td ltx_align_center ltx_border_t">0.635±0.09</td>
<td id="S5.T3.1.1.33.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.642±0.11</td>
<td id="S5.T3.1.1.33.9" class="ltx_td ltx_align_center ltx_border_t">0.631±0.09</td>
<td id="S5.T3.1.1.33.10" class="ltx_td ltx_align_center ltx_border_t">0.565±0.00</td>
<td id="S5.T3.1.1.33.11" class="ltx_td ltx_align_center ltx_border_t">0.652±0.09</td>
<td id="S5.T3.1.1.33.12" class="ltx_td ltx_align_center ltx_border_t">0.671±0.11</td>
</tr>
<tr id="S5.T3.1.1.34" class="ltx_tr">
<td id="S5.T3.1.1.34.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" colspan="3"><span id="S5.T3.1.1.34.1.1" class="ltx_text ltx_font_bold">Non-fused Baseline</span></td>
<td id="S5.T3.1.1.34.2" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.34.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.569±0.03</td>
<td id="S5.T3.1.1.34.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.504±0.01</td>
<td id="S5.T3.1.1.34.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.604±0.02</td>
<td id="S5.T3.1.1.34.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T3.1.1.34.6.1" class="ltx_text ltx_framed ltx_framed_underline">0.782±0.02</span></td>
<td id="S5.T3.1.1.34.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.581±0.01</td>
<td id="S5.T3.1.1.34.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.523±0.01</td>
<td id="S5.T3.1.1.34.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.612±0.01</td>
<td id="S5.T3.1.1.34.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T3.1.1.34.10.1" class="ltx_text ltx_framed ltx_framed_underline">0.795±0.01</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Macro F1 score results for the fuse models (BERT-large-cased and fBERT) compared to the baseline systems fine-tuned on the four datasets. Results are reported on 10 runs along with standard deviation. The best results from the fused approach for each model are in bold. Results for the non-fused baseline model evaluated on the same dataset are underlined.</figcaption>
</figure>
<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">We discuss the following four main findings from our results;</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">(1) The fused model performs better when evaluated on the same dataset used in further finetunning.</span> All the datasets except for HASOC, the best result was produced when the fused model was further fine-tuned on that particular dataset. For HASOC too, when the fBERT model trained on AHSD, OLID and HASOC were fused and further fine-tuned on HASOC provided 0.754 Macro F1 score, which is very close to the best result (0.770). With the results, we can conclude that the fused model performs better when evaluated on the same dataset used in further finetunning. This observation reflects an ideal scenario in real-world applications where we want an ML model to perform excellently in data specific to our environment/ platform. This objective can be achieved successfully with model fusion and finetunning as we see in the results.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">(2) The fused model generalizes well across datasets even when it is not used in finetunning.</span> One drawback of fused models is that the result slightly decreases compared to the non-fused models trained only using a particular dataset. In the results, this is clear as there is a decrease in the Macro F1 score between underlined values and bolded values. Furthermore as you can see in Table <a href="#S4.T2" title="Table 2 ‣ 4. Methodology ‣ A Federated Learning Approach to Privacy Preserving Offensive Language Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the best result in all the datasets were produced with the non-fused baseline. However, after further investigating this, it is clear that non-fused models do not often generalise well across other datasets. For example in Table <a href="#S5.T3" title="Table 3 ‣ 5.1. Discussion ‣ 5. Results and Discussion ‣ A Federated Learning Approach to Privacy Preserving Offensive Language Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the non-fused model trained on AHSD only provides 0.699 Macro F1 score for OLID. However, AHSD and OLID fused model further fine-tuned on AHSD provides 0.830 Macro F1 score. This is similar to the majority of the experiments, and fused models provide better results than non-fused models in other datasets. This observation again reflects an ideal scenario in real-world applications where we want an ML model to perform well across data not specific to our environment/ platform. As we see in the results, this objective can be achieved successfully with model fusion.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.p4.1" class="ltx_p"><span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold ltx_font_italic">(3) The Fused model outperforms the ensemble baseline in all the datasets.</span>
As shown in Table <a href="#S4.T2" title="Table 2 ‣ 4. Methodology ‣ A Federated Learning Approach to Privacy Preserving Offensive Language Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, model fusion approaches with and without finetunning on a particular dataset outperform the best ensemble model. For HASOC, there is a large gap between the ensemble model and fused models as the ensemble model produces only 0.670 Macro F1 score while the fused model provides 0.770 Macro F1 score. The other datasets also follow a similar pattern. This is a key observation because we have presented a fusion based approach for FL that can surpass an ensemble based model preserving privacy across different datasets. The platforms/ environments that are interested in developing a FL approach should focus on model fusion based strategies that outperform ensemble based models as we showed in the results.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para ltx_noindent">
<p id="S5.SS1.p5.1" class="ltx_p"><span id="S5.SS1.p5.1.1" class="ltx_text ltx_font_bold ltx_font_italic">(4) The Fused model performance heavily depends on the datasets it was trained on.</span>
Our final observation is that the fused model performance depends on the datasets that it was trained on. For example, when the model fusion was performed between AHSD and OLID, the final model provided excellent results on both datasets. This is due to the general nature of these two datasets covering multiple types of offensive content rather than focusing on a particular type of offensive content. On the other hand, results are not the same when the model fusion was performed between AHSD and HASOC where the final model did not provide good results for both datasets. This can be explained by the demography of the dataset as HASOC data is collected on Twitter users based in India. It is clear that model fusion would thrive in similar kinds of datasets, but would not perform well with different kinds of data.</p>
</div>
<div id="S5.SS1.p6" class="ltx_para ltx_noindent">
<p id="S5.SS1.p6.1" class="ltx_p">Overall, model fusion produces excellent results on the dataset that it was fine-tuned on, and it generalizes well across other datasets. Fused models outperform both of our baselines in all the datasets. Therefore, model fusion provides a successful approach to FL.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">5.2.   Multilingual Experiments</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We conducted initial multilingual experiments with the same FL setting. We used OffendES <cite class="ltx_cite ltx_citemacro_cite">Plaza-del Arco et al. (<a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite>, a Spanish offensive language identification dataset. For English we used the OLID dataset described before. Each instance in OffendES is labelled as belonging to one of the five classes; Offensive and targeted to a person (OFP), Offensive and targeted to a group (OFG), Offensive and not targeted to a person or a group (OFO), Non-offensive, but with expletive language (NOE), and Non-offensive (NO). We map the instances belonging to the OFP, OFG, OFO, and NOE to OLID OFF, and the NO class as NOT. Even though, the label NOE is considered non-offensive in OffendES, it contains profanity so we map it to OLID label OFF to conform with the OLID guidelines.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Instead of the monolingual BERT models we used in the previous experiments, we use cross-lingual models, specifically XLM-R <cite class="ltx_cite ltx_citemacro_cite">Conneau et al. (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>. We used the same FL settings and compared it with ensemble baseline. The results are shown in Table <a href="#S5.T4" title="Table 4 ‣ 5.2. Multilingual Experiments ‣ 5. Results and Discussion ‣ A Federated Learning Approach to Privacy Preserving Offensive Language Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<div id="S5.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:191.8pt;height:136.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.3pt,5.9pt) scale(0.92,0.92) ;">
<table id="S5.T4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<td id="S5.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S5.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Approach</span></td>
<td id="S5.T4.1.1.1.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">Macro F1</span></td>
</tr>
<tr id="S5.T4.1.1.2" class="ltx_tr">
<td id="S5.T4.1.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="4"><span id="S5.T4.1.1.2.1.1" class="ltx_text">English</span></td>
<td id="S5.T4.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">non-fused</td>
<td id="S5.T4.1.1.2.3" class="ltx_td ltx_align_left ltx_border_t">0.845 ±0.01</td>
</tr>
<tr id="S5.T4.1.1.3" class="ltx_tr">
<td id="S5.T4.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r">fusion with FT</td>
<td id="S5.T4.1.1.3.2" class="ltx_td ltx_align_left">0.829 ±0.03</td>
</tr>
<tr id="S5.T4.1.1.4" class="ltx_tr">
<td id="S5.T4.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r">fusion without FT</td>
<td id="S5.T4.1.1.4.2" class="ltx_td ltx_align_left">0.831 ±0.00</td>
</tr>
<tr id="S5.T4.1.1.5" class="ltx_tr">
<td id="S5.T4.1.1.5.1" class="ltx_td ltx_align_center ltx_border_r">ensemble</td>
<td id="S5.T4.1.1.5.2" class="ltx_td ltx_align_left">0.776 ±0.02</td>
</tr>
<tr id="S5.T4.1.1.6" class="ltx_tr">
<td id="S5.T4.1.1.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="4"><span id="S5.T4.1.1.6.1.1" class="ltx_text">Spanish</span></td>
<td id="S5.T4.1.1.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">non-fused</td>
<td id="S5.T4.1.1.6.3" class="ltx_td ltx_align_left ltx_border_t">0.812 ±0.04</td>
</tr>
<tr id="S5.T4.1.1.7" class="ltx_tr">
<td id="S5.T4.1.1.7.1" class="ltx_td ltx_align_center ltx_border_r">fusion with FT</td>
<td id="S5.T4.1.1.7.2" class="ltx_td ltx_align_left">0.809 ±0.02</td>
</tr>
<tr id="S5.T4.1.1.8" class="ltx_tr">
<td id="S5.T4.1.1.8.1" class="ltx_td ltx_align_center ltx_border_r">fusion without FT</td>
<td id="S5.T4.1.1.8.2" class="ltx_td ltx_align_left">0.792 ±0.01</td>
</tr>
<tr id="S5.T4.1.1.9" class="ltx_tr">
<td id="S5.T4.1.1.9.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">ensemble</td>
<td id="S5.T4.1.1.9.2" class="ltx_td ltx_align_left ltx_border_b">0.761 ±0.02</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>The results for multilingual experiments on English and Spanish; non-fused models, fused models with fine-tuning (FT), fused models without finetuning and ensemble. We report the results with xlm-roberta. The results are ordered from Macro F1.</figcaption>
</figure>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p">The results show that fusion based FL outperforms ensemble baseline in multilingual settings too. This opens new avenues for privacy preserving models for languages other than English and more specifically, low-resource languages.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.   Conclusion and Future Work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper introduced FL in the context of combining different offensive language identification models. While a recent study <cite class="ltx_cite ltx_citemacro_cite">Gala et al. (<a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite> uses FL learning in offensive language identification, their work is limited to distributed training on the same dataset with multiple clients. As far as we know, our research is the first study to use FL in combining multiple offensive language identification models. We evaluated a fusion-based FL architecture using a general BERT model and a fine-tuned fBERT model on four publicly available English benchmark datasets. We also presented initial cross-lingual experiments in English and Spanish. Our results show that the fusion model performances outperform the performance of an ensemble baseline model. We also show that the fused model generalizes well across all datasets tested. As the FL architecture does not require data sharing, we believe that FL is a promising research direction in offensive language identification due to its privacy preserving nature.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">In future work, we would like to explore other FL architectures and compare their performance to the fused model proposed in this paper. Finally. we would like to evaluate the performance of recently proposed large language models (LLMs) (e.g., GPT-4, LLama 2) for this task in FL settings.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">Bibliographical References</h2>

<div id="Sx1.p1" class="ltx_para">
<span id="Sx1.p1.1" class="ltx_ERROR undefined">\c@NAT@ctr</span>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography"></h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Basile et al. (2019)</span>
<span class="ltx_bibblock">
Valerio Basile, Cristina Bosco, Elisabetta Fersini, Debora Nozza, Viviana
Patti, Francisco Manuel Rangel Pardo, Paolo Rosso, and Manuela Sanguinetti.
2019.

</span>
<span class="ltx_bibblock">Semeval-2019 task 5: Multilingual detection of hate speech against
immigrants and women in twitter.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of SemEval</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caselli et al. (2020)</span>
<span class="ltx_bibblock">
Tommaso Caselli, Valerio Basile, Jelena Mitrović, and Michael Granitzer.
2020.

</span>
<span class="ltx_bibblock">Hatebert: Retraining bert for abusive language detection in english.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of WOAH</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Çöltekin (2020)</span>
<span class="ltx_bibblock">
Çağrı Çöltekin. 2020.

</span>
<span class="ltx_bibblock">A Corpus of Turkish Offensive Language on Social Media.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of LREC</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Che et al. (2023)</span>
<span class="ltx_bibblock">
Tianshi Che, Ji Liu, Yang Zhou, Jiaxiang Ren, Jiwen Zhou, Victor Sheng, Huaiyu
Dai, and Dejing Dou. 2023.

</span>
<span class="ltx_bibblock">Federated learning of large language models with parameter-efficient
prompt tuning and adaptive optimization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of EMNLP</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2019)</span>
<span class="ltx_bibblock">
Mingqing Chen, Ananda Theertha Suresh, Rajiv Mathews, Adeline Wong, Cyril
Allauzen, Françoise Beaufays, and Michael Riley. 2019.

</span>
<span class="ltx_bibblock">Federated learning of n-gram language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of CoNLL</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choshen et al. (2022)</span>
<span class="ltx_bibblock">
Leshem Choshen, Elad Venezian, Noam Slonim, and Yoav Katz. 2022.

</span>
<span class="ltx_bibblock">Fusing finetuned models for better pretraining.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.03044</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2019)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and
Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">Unsupervised cross-lingual representation learning at scale.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of ACL</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Davidson et al. (2017)</span>
<span class="ltx_bibblock">
Thomas Davidson, Dana Warmsley, Michael W. Macy, and Ingmar Weber. 2017.

</span>
<span class="ltx_bibblock">Automated hate speech detection and the problem of offensive
language.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of ICWSM</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional Transformers for Language
Understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of NAACL</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Djuric et al. (2015)</span>
<span class="ltx_bibblock">
Nemanja Djuric, Jing Zhou, Robin Morris, Mihajlo Grbovic, Vladan Radosavljevic,
and Narayan Bhamidipati. 2015.

</span>
<span class="ltx_bibblock">Hate speech detection with comment embeddings.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of WWW</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feyisetan et al. (2022)</span>
<span class="ltx_bibblock">
Oluwaseyi Feyisetan, Sepideh Ghanavati, Patricia Thaine, Ivan Habernal, and
Fatemehsadat Mireshghallah, editors. 2022.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fourth Workshop on Privacy in Natural
Language Processing</em>. ACL.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fišer et al. (2017)</span>
<span class="ltx_bibblock">
Darja Fišer, Tomaž Erjavec, and Nikola Ljubešić. 2017.

</span>
<span class="ltx_bibblock">Legal Framework, Dataset and Annotation Schema for Socially
Unacceptable On-line Discourse Practices in Slovene.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings ALW</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fortuna et al. (2019)</span>
<span class="ltx_bibblock">
Paula Fortuna, Joao Rocha da Silva, Leo Wanner, Sérgio Nunes, et al. 2019.

</span>
<span class="ltx_bibblock">A Hierarchically-labeled Portuguese Hate Speech Dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of ALW</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gaikwad et al. (2021)</span>
<span class="ltx_bibblock">
Saurabh Gaikwad, Tharindu Ranasinghe, Marcos Zampieri, and Christopher M Homan.
2021.

</span>
<span class="ltx_bibblock">Cross-lingual offensive language identification for low resource
languages: The case of marathi.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of RANLP</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gala et al. (2023)</span>
<span class="ltx_bibblock">
Jay Gala, Deep Gandhi, Jash Mehta, and Zeerak Talat. 2023.

</span>
<span class="ltx_bibblock">A federated approach for hate speech detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of EACL</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gambäck and Sikdar (2017)</span>
<span class="ltx_bibblock">
Björn Gambäck and Utpal Kumar Sikdar. 2017.

</span>
<span class="ltx_bibblock">Using Convolutional Neural Networks to Classify Hate-speech.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of ALW</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hettiarachchi and
Ranasinghe (2019)</span>
<span class="ltx_bibblock">
Hansi Hettiarachchi and Tharindu Ranasinghe. 2019.

</span>
<span class="ltx_bibblock">Emoji powered capsule network to detect type and target of offensive
posts in social media.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of RANLP</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar et al. (2018)</span>
<span class="ltx_bibblock">
Ritesh Kumar, Atul Kr Ojha, Shervin Malmasi, and Marcos Zampieri. 2018.

</span>
<span class="ltx_bibblock">Benchmarking Aggression Identification in Social Media.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of TRAC</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar et al. (2020)</span>
<span class="ltx_bibblock">
Ritesh Kumar, Atul Kr. Ojha, Shervin Malmasi, and Marcos Zampieri. 2020.

</span>
<span class="ltx_bibblock">Evaluating aggression identification in social media.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of TRAC</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lim et al. (2020)</span>
<span class="ltx_bibblock">
Wei Yang Bryan Lim, Nguyen Cong Luong, Dinh Thai Hoang, Yutao Jiao, Ying-Chang
Liang, Qiang Yang, Dusit Niyato, and Chunyan Miao. 2020.

</span>
<span class="ltx_bibblock">Federated learning in mobile edge networks: A comprehensive survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</em>, 22(3):2031–2063.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2022a)</span>
<span class="ltx_bibblock">
Bill Yuchen Lin, Chaoyang He, Chulin Xie, Fatemehsadat Mireshghallah, Ninareh
Mehrabi, Tian Li, Mahdi Soltanolkotabi, and Xiang Ren, editors.
2022a.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings FL4NLP</em>. ACL.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2022b)</span>
<span class="ltx_bibblock">
Bill Yuchen Lin, Chaoyang He, Zihang Ze, Hulin Wang, Yufen Hua, Christophe
Dupuy, Rahul Gupta, Mahdi Soltanolkotabi, Xiang Ren, and Salman Avestimehr.
2022b.

</span>
<span class="ltx_bibblock">Fednlp: Benchmarking federated learning methods for natural language
processing tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Findings of NAACL</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MacAvaney et al. (2019)</span>
<span class="ltx_bibblock">
Sean MacAvaney, Hao-Ren Yao, Eugene Yang, Katina Russell, Nazli Goharian, and
Ophir Frieder. 2019.

</span>
<span class="ltx_bibblock">Hate speech detection: Challenges and solutions.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">PloS one</em>, 14(8):e0221152.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malmasi and Zampieri (2017)</span>
<span class="ltx_bibblock">
Shervin Malmasi and Marcos Zampieri. 2017.

</span>
<span class="ltx_bibblock">Detecting Hate Speech in Social Media.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of RANLP</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malmasi and Zampieri (2018)</span>
<span class="ltx_bibblock">
Shervin Malmasi and Marcos Zampieri. 2018.

</span>
<span class="ltx_bibblock">Challenges in Discriminating Profanity from Hate Speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Journal of Experimental &amp; Theoretical Artificial
Intelligence</em>, 30:1–16.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mandl et al. (2020)</span>
<span class="ltx_bibblock">
Thomas Mandl, Sandip Modha, Anand Kumar M, and Bharathi Raja Chakravarthi.
2020.

</span>
<span class="ltx_bibblock">Overview of the hasoc track at fire 2020: Hate speech and offensive
language identification in tamil, malayalam, hindi, english and german.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of FIRE</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mathew et al. (2021)</span>
<span class="ltx_bibblock">
Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and
Animesh Mukherjee. 2021.

</span>
<span class="ltx_bibblock">HateXplain: A Benchmark Dataset for Explainable Hate Speech
Detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of AAAI</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas. 2017.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of AISTATS</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Melton et al. (2020)</span>
<span class="ltx_bibblock">
Joshua Melton, Arunkumar Bagavathi, and Siddharth Krishnan. 2020.

</span>
<span class="ltx_bibblock">Del-hate: a deep learning tunable ensemble for hate speech detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of ICMLA</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Modha et al. (2021)</span>
<span class="ltx_bibblock">
Sandip Modha, Thomas Mandl, Gautam Kishore Shahi, Hiren Madhu, Shrey Satapara,
Tharindu Ranasinghe, and Marcos Zampieri. 2021.

</span>
<span class="ltx_bibblock">Overview of the hasoc subtrack at fire 2021: Hate speech and
offensive content identification in english and indo-aryan languages and
conversational hate speech.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of FIRE</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mubarak et al. (2021)</span>
<span class="ltx_bibblock">
Hamdy Mubarak, Ammar Rashed, Kareem Darwish, Younes Samih, and Ahmed Abdelali.
2021.

</span>
<span class="ltx_bibblock">Arabic Offensive Language on Twitter: Analysis and Experiments.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of WANLP</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plaza-del Arco et al. (2021)</span>
<span class="ltx_bibblock">
Flor Miriam Plaza-del Arco, Arturo Montejo-Ráez, L Alfonso Urena Lopez, and
María-Teresa Martín-Valdivia. 2021.

</span>
<span class="ltx_bibblock">Offendes: A new corpus in spanish for offensive language research.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of RANLP</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raihan et al. (2023)</span>
<span class="ltx_bibblock">
Md Nishat Raihan, Umma Tanmoy, Anika Binte Islam, Kai North, Tharindu
Ranasinghe, Antonios Anastasopoulos, and Marcos Zampieri. 2023.

</span>
<span class="ltx_bibblock">Offensive language identification in transliterated and code-mixed
bangla.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of BLP</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ranasinghe and Zampieri (2020)</span>
<span class="ltx_bibblock">
Tharindu Ranasinghe and Marcos Zampieri. 2020.

</span>
<span class="ltx_bibblock">Multilingual Offensive Language Identification with Cross-lingual
Embeddings.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of EMNLP</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ranasinghe and Zampieri (2021)</span>
<span class="ltx_bibblock">
Tharindu Ranasinghe and Marcos Zampieri. 2021.

</span>
<span class="ltx_bibblock">MUDES: Multilingual Detection of Offensive Spans.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of NAACL</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ranasinghe et al. (2019)</span>
<span class="ltx_bibblock">
Tharindu Ranasinghe, Marcos Zampieri, and Hansi Hettiarachchi. 2019.

</span>
<span class="ltx_bibblock">BRUMS at HASOC 2019: Deep Learning Models for Multilingual Hate
Speech and Offensive Language Identification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of FIRE</em>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddi et al. (2021)</span>
<span class="ltx_bibblock">
Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
Jakub Konečný, Sanjiv Kumar, and Hugh Brendan McMahan. 2021.

</span>
<span class="ltx_bibblock">Adaptive federated optimization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of ICLR</em>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rosenthal et al. (2021)</span>
<span class="ltx_bibblock">
Sara Rosenthal, Pepa Atanasova, Georgi Karadzhov, Marcos Zampieri, and Preslav
Nakov. 2021.

</span>
<span class="ltx_bibblock">SOLID: A Large-Scale Weakly Supervised Dataset for Offensive
Language Identification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Findings of ACL</em>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sahu et al. (2019)</span>
<span class="ltx_bibblock">
Anit Kumar Sahu, Tian Li, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and
Virginia Smith. 2019.

</span>
<span class="ltx_bibblock">Federated optimization for heterogeneous networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of AMTL</em>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarkar et al. (2021)</span>
<span class="ltx_bibblock">
Diptanu Sarkar, Marcos Zampieri, Tharindu Ranasinghe, and Alexander Ororbia.
2021.

</span>
<span class="ltx_bibblock">fbert: A neural transformer for identifying offensive content.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Findings of EMNLP</em>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Satapara et al. (2022)</span>
<span class="ltx_bibblock">
Shrey Satapara, Prasenjit Majumder, Thomas Mandl, Sandip Modha, Hiren Madhu,
Tharindu Ranasinghe, Marcos Zampieri, Kai North, and Damith Premasiri. 2022.

</span>
<span class="ltx_bibblock">Overview of the hasoc subtrack at fire 2022: Hate speech and
offensive content identification in english and indo-aryan languages.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of FIRE</em>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silva et al. (2023)</span>
<span class="ltx_bibblock">
Andrew Silva, Pradyumna Tambwekar, and Matthew Gombolay. 2023.

</span>
<span class="ltx_bibblock">Fedperc: Federated learning for language generation with personal and
context preference embeddings.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Findings of EACL</em>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Yansheng Wang, Yongxin Tong, Dingyuan Shi, and Ke Xu. 2021.

</span>
<span class="ltx_bibblock">An efficient approach for crosssilo federated learning to rank.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of ICDE</em>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weerasooriya et al. (2023)</span>
<span class="ltx_bibblock">
Tharindu Weerasooriya, Sujan Dutta, Tharindu Ranasinghe, Marcos Zampieri,
Christopher Homan, and Ashiqur Khudabukhsh. 2023.

</span>
<span class="ltx_bibblock">Vicarious offense and noise audit of offensive speech classifiers:
Unifying human and machine disagreement on what is offensive.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of EMNLP</em>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. (2021)</span>
<span class="ltx_bibblock">
Bingjie Yan, Jun Wang, Jieren Cheng, Yize Zhou, Yixian Zhang, Yifan Yang,
Li Liu, Haojiang Zhao, Chunjuan Wang, and Boyi Liu. 2021.

</span>
<span class="ltx_bibblock">Experiments of federated learning for covid-19 chest x-ray images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of ICAIS</em>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zampieri et al. (2019a)</span>
<span class="ltx_bibblock">
Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra,
and Ritesh Kumar. 2019a.

</span>
<span class="ltx_bibblock">Predicting the type and target of offensive posts in social media.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of NAACL</em>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zampieri et al. (2019b)</span>
<span class="ltx_bibblock">
Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra,
and Ritesh Kumar. 2019b.

</span>
<span class="ltx_bibblock">SemEval-2019 Task 6: Identifying and Categorizing Offensive Language
in Social Media (OffensEval).

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of SemEval</em>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zampieri et al. (2020)</span>
<span class="ltx_bibblock">
Marcos Zampieri, Preslav Nakov, Sara Rosenthal, Pepa Atanasova, Georgi
Karadzhov, Hamdy Mubarak, Leon Derczynski, Zeses Pitenis, and
Çağrı Çöltekin. 2020.

</span>
<span class="ltx_bibblock">SemEval-2020 Task 12: Multilingual Offensive Language Identification
in Social Media (OffensEval 2020).

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of SemEval</em>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Zhuo Zhang, Xiangjing Hu, Jingyuan Zhang, Yating Zhang, Hui Wang, Lizhen Qu,
and Zenglin Xu. 2023.

</span>
<span class="ltx_bibblock">Fedlegal: The first real-world federated learning benchmark for legal
nlp.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of ACL</em>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zia et al. (2022)</span>
<span class="ltx_bibblock">
Haris Bin Zia, Ignacio Castro, Arkaitz Zubiaga, and Gareth Tyson. 2022.

</span>
<span class="ltx_bibblock">Improving zero-shot cross-lingual hate speech detection with
pseudo-label fine-tuning of transformer language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of ICWSM</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.11469" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.11470" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.11470">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.11470" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.11471" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 22:38:44 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
