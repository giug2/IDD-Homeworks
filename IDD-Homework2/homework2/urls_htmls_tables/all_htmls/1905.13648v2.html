<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1905.13648] Scene Text Visual Question Answering</title><meta property="og:description" content="Current visual question answering datasets do not consider the rich semantic information conveyed by text within an image. In this work, we present a new dataset, ST-VQA, that aims to highlight the importance of exploi…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Scene Text Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Scene Text Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1905.13648">

<!--Generated on Sat Mar  2 16:27:15 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Scene Text Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ali Furkan Biten <sup id="id11.11.id1" class="ltx_sup"><span id="id11.11.id1.1" class="ltx_text ltx_font_italic">,1</span></sup>     Rubèn Tito<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotemark: </span></span></span></span> <sup id="id12.12.id2" class="ltx_sup"><span id="id12.12.id2.1" class="ltx_text ltx_font_italic">,1</span></sup>     Andres Mafla<span id="footnotex2" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotemark: </span></span></span></span> <sup id="id13.13.id3" class="ltx_sup"><span id="id13.13.id3.1" class="ltx_text ltx_font_italic">,1</span></sup>     Lluis Gomez<sup id="id14.14.id4" class="ltx_sup"><span id="id14.14.id4.1" class="ltx_text ltx_font_italic">1</span></sup>
<br class="ltx_break">Marçal Rusiñol<sup id="id15.15.id5" class="ltx_sup"><span id="id15.15.id5.1" class="ltx_text ltx_font_italic">1</span></sup>     Ernest Valveny<sup id="id16.16.id6" class="ltx_sup"><span id="id16.16.id6.1" class="ltx_text ltx_font_italic">1</span></sup>     C.V. Jawahar<sup id="id17.17.id7" class="ltx_sup"><span id="id17.17.id7.1" class="ltx_text ltx_font_italic">2</span></sup>     Dimosthenis Karatzas<sup id="id18.18.id8" class="ltx_sup"><span id="id18.18.id8.1" class="ltx_text ltx_font_italic">1</span></sup>
<br class="ltx_break">
<br class="ltx_break"><sup id="id19.19.id9" class="ltx_sup">1</sup>Computer Vision Center, UAB, Spain     <sup id="id20.20.id10" class="ltx_sup">2</sup>CVIT, IIIT Hyderabad, India 
<br class="ltx_break"><span id="id21.21.id11" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{abiten, rperez, amafla, lgomez, marcal, dimos}@cvc.uab.es</span>
</span><span class="ltx_author_notes">Equal contribution.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id22.id1" class="ltx_p">Current visual question answering datasets do not consider the rich semantic information conveyed by text within an image. In this work, we present a new dataset, <span id="id22.id1.1" class="ltx_text">ST-VQA</span>, that aims to highlight the importance of exploiting high-level semantic information present in images as textual cues in the Visual Question Answering process.
We use this dataset to define a series of tasks of increasing difficulty for which reading the scene text in the context provided by the visual information is necessary to reason and generate an appropriate answer.
We propose a new evaluation metric for these tasks to account both for reasoning errors as well as shortcomings of the text recognition module. In addition we put forward a series of baseline methods, which provide further insight to the newly released dataset, and set the scene for further research.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Textual content in man-made environments conveys important high-level semantic information that is explicit and not available in any other form in the scene.
Interpreting written information in man-made environments is essential in order to perform most everyday tasks like making a purchase, using public transportation, finding a place in the city, getting an appointment, or checking whether a store is open or not, to mention just a few.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Text is present in about 50% of the images in large-scale datasets such as MS Common Objects in Context <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> and the percentage goes up sharply in urban environments. It is thus fundamental to design models that take advantage of these explicit cues.
Ensuring that scene text is properly accounted for is not a marginal research problem, but quite central for holistic scene interpretation models.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The research community on reading systems has made significant advances over the past decade <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. The current state of the art in scene text understanding allows endowing computer vision systems with basic reading capacity, although the community has not yet exploited this towards solving higher level problems.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">At the same time, current Visual Question Answering (VQA) datasets and models present serious limitations as a result of ignoring scene text content, with disappointing results on questions that require scene text understanding. We therefore consider it is timely to bring together these two research lines in the VQA domain.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<table id="S1.F1.4.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.F1.2.2.2" class="ltx_tr">
<td id="S1.F1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S1.F1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.F1.1.1.1.1.1.1" class="ltx_p" style="width:195.1pt;"><img src="/html/1905.13648/assets/COCO_train2014_000000009452.jpg" id="S1.F1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption"></span>
</span>
</td>
<td id="S1.F1.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S1.F1.2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.F1.2.2.2.2.1.1" class="ltx_p" style="width:195.1pt;"><img src="/html/1905.13648/assets/2410185.jpg" id="S1.F1.2.2.2.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption"></span>
</span>
</td>
</tr>
<tr id="S1.F1.4.4.5.1" class="ltx_tr">
<td id="S1.F1.4.4.5.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S1.F1.4.4.5.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.F1.4.4.5.1.1.1.1" class="ltx_p" style="width:195.1pt;"><span id="S1.F1.4.4.5.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q:<span id="S1.F1.4.4.5.1.1.1.1.1.1" class="ltx_text ltx_font_medium"> What is the price of the bananas per kg?</span></span></span>
<span id="S1.F1.4.4.5.1.1.1.2" class="ltx_p"><span id="S1.F1.4.4.5.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">A:<span id="S1.F1.4.4.5.1.1.1.2.1.1" class="ltx_text ltx_font_medium"> $11.98</span></span></span>
</span>
</td>
<td id="S1.F1.4.4.5.1.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S1.F1.4.4.5.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.F1.4.4.5.1.2.1.1" class="ltx_p" style="width:195.1pt;"><span id="S1.F1.4.4.5.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q:<span id="S1.F1.4.4.5.1.2.1.1.1.1" class="ltx_text ltx_font_medium"> What does the red sign say?</span></span></span>
<span id="S1.F1.4.4.5.1.2.1.2" class="ltx_p"><span id="S1.F1.4.4.5.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">A:<span id="S1.F1.4.4.5.1.2.1.2.1.1" class="ltx_text ltx_font_medium"> Stop</span></span></span>
</span>
</td>
</tr>
<tr id="S1.F1.4.4.4" class="ltx_tr">
<td id="S1.F1.3.3.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S1.F1.3.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.F1.3.3.3.1.1.1" class="ltx_p" style="width:195.1pt;"><img src="/html/1905.13648/assets/2416461.png" id="S1.F1.3.3.3.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption"></span>
</span>
</td>
<td id="S1.F1.4.4.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S1.F1.4.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.F1.4.4.4.2.1.1" class="ltx_p" style="width:195.1pt;"><img src="/html/1905.13648/assets/2321742.png" id="S1.F1.4.4.4.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption"></span>
</span>
</td>
</tr>
<tr id="S1.F1.4.4.6.2" class="ltx_tr">
<td id="S1.F1.4.4.6.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S1.F1.4.4.6.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.F1.4.4.6.2.1.1.1" class="ltx_p" style="width:195.1pt;"><span id="S1.F1.4.4.6.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q:<span id="S1.F1.4.4.6.2.1.1.1.1.1" class="ltx_text ltx_font_medium"> Where is this train going?</span></span></span>
<span id="S1.F1.4.4.6.2.1.1.2" class="ltx_p"><span id="S1.F1.4.4.6.2.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">A:<span id="S1.F1.4.4.6.2.1.1.2.1.1" class="ltx_text ltx_font_medium"> To New York 
<br class="ltx_break"></span>A:<span id="S1.F1.4.4.6.2.1.1.2.1.2" class="ltx_text ltx_font_medium"> New York</span></span></span>
</span>
</td>
<td id="S1.F1.4.4.6.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S1.F1.4.4.6.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.F1.4.4.6.2.2.1.1" class="ltx_p" style="width:195.1pt;"><span id="S1.F1.4.4.6.2.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q:<span id="S1.F1.4.4.6.2.2.1.1.1.1" class="ltx_text ltx_font_medium"> What is the exit number on the street sign?</span></span></span>
<span id="S1.F1.4.4.6.2.2.1.2" class="ltx_p"><span id="S1.F1.4.4.6.2.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">A:<span id="S1.F1.4.4.6.2.2.1.2.1.1" class="ltx_text ltx_font_medium"> 2 
<br class="ltx_break"></span>A:<span id="S1.F1.4.4.6.2.2.1.2.1.2" class="ltx_text ltx_font_medium"> Exit 2</span></span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Recognising and interpreting textual content is essential for scene understanding. In the Scene Text Visual Question Answering (ST-VQA) dataset leveraging textual information in the image is the only way to solve the QA task.</figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To move towards more human like reasoning, we contemplate that grounding question answering both on the visual and the textual information is necessary.
Integrating the textual modality in existing VQA pipelines is not trivial. On one hand, spotting <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">relevant</span> textual information in the scene requires performing complex reasoning about positions, colors, objects and semantics, to localise, recognise and eventually interpret the recognised text in the context of the visual content, or any other contextual information available.
On the other hand, current VQA models work mostly on the principle of classical <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and operant (instrumental) conditioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. Such models, display important dataset biases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> as well as failures in counting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, comparing and identifying attributes. These limitations make current models unsuitable to directly integrate scene text information which is often orthogonal and uncorrelated to the visual statistics of the image.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">To this end, in this work we propose a new dataset, called <span id="S1.p6.1.1" class="ltx_text ltx_font_italic">Scene Text Visual Question Answering</span> (ST-VQA) where the questions and answers are attained in a way that questions can only be answered based on the text present in the image. We consciously draw the majority (85.5%) of ST-VQA images from datasets that have generic question/answer pairs that can be combined with ST-VQA to establish a more generic, holistic VQA task.
Some sample images and questions from the collected dataset are shown in <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Scene Text Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Additionally, we introduce three tasks of increasing difficulty that simulate different degrees of availability of contextual information. Finally, we define a new evaluation metric to better discern the models’ answering ability, that employs the Levenshtein distance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> to account both for reasoning errors as well as shortcomings of the text recognition subsystem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. The dataset, as well as performance evaluation scripts and an online evaluation service are available through the ST-VQA Web portal<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://rrc.cvc.uab.es/?ch=11" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://rrc.cvc.uab.es/?ch=11</a></span></span></span>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The task of text detection and recognition in natural images sets the starting point for a generalized VQA system that can integrate textual cues towards complete scene understanding. The most common approach in the reading systems community consists of two steps, text detection and recognition. Several works have been proposed addressing text detection such as  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> which are mostly based on Fully Convolutional Neural Networks.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Text recognition methods such as the one presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> propose recognizing text at the word level as a classification problem (word spotting) from a <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="90K" display="inline"><semantics id="S2.p2.1.m1.1a"><mrow id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml"><mn id="S2.p2.1.m1.1.1.2" xref="S2.p2.1.m1.1.1.2.cmml">90</mn><mo lspace="0em" rspace="0em" id="S2.p2.1.m1.1.1.1" xref="S2.p2.1.m1.1.1.1.cmml">​</mo><mi id="S2.p2.1.m1.1.1.3" xref="S2.p2.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><apply id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"><times id="S2.p2.1.m1.1.1.1.cmml" xref="S2.p2.1.m1.1.1.1"></times><cn type="integer" id="S2.p2.1.m1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.2">90</cn><ci id="S2.p2.1.m1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">90K</annotation></semantics></math> English words vocabulary. Approaches that use Connectionist Temporal Classification have also been widely used in scene text recognition, in works such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, among others. Later works focus towards end-to-end architectures such as the ones presented by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, which mostly consist of an initial Convolutional Neural Network (CNN) that acts as an encoder and a Long Short Term Memory (LSTM) combined with attention that acts as the decoder.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Visual Question Answering (VQA) aims to come up with an answer to a given natural language question about the image. Since its introduction, VQA has received a lot of attention from the Computer Vision community <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> facilitated by access to large-scale datasets that allow the training of VQA models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. Despite VQA’s popularity, none of the existing datasets except TextVQA (reviewed separately next) consider textual content, while in our work, exploiting textual information found in the images is the only way to solve the VQA task.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Related to the task proposed in this paper, are the recent works of Kafle et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and Kahou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> on question answering for bar charts and diagrams, the work of Kise at al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> on QA for machine printed document images, and the work of Kembhavi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> on textbook question answering. The Textbook Question Answering (TQA) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> aims at answering multimodal questions given a context of text, diagrams and images, but textual information is provided in computer readable format. This is not the case for the diagrams and charts of the datasets proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, meaning that models require some sort of text recognition to solve such QA tasks. However, the text found on these datasets is rendered in standard font types and with good quality, and thus represents a less challenging setup than the scene text used in our work.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">TextVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> is a concurrent work to the one presented here. Similarly to ST-VQA, TextVQA proposes an alternative dataset for VQA which requires reading and reasoning about scene text. Additionally, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> also introduces a novel architecture that combines a standard VQA model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> and an independently trained OCR module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> with a “copy” mechanism, inspired by pointer networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, which allows to use OCR recognized words as predicted answers if needed. Both TextVQA and ST-VQA datasets are conceptually similar, although there are important differences in the implementation and design choices. We offer here a high-level summary of key differences, while section <a href="#S3.SS2" title="3.2 Analysis and Comparison with TextVQA ‣ 3 ST-VQA Dataset ‣ Scene Text Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> gives a quantitative comparison between the two datasets.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">In the case of ST-VQA, a number of different source image datasets were used, including scene text understanding ones, while in the case of TextVQA all images come from a single source, the Open Images dataset.
To select the images to annotate for the ST-VQA, we explicitly required a minimum amount of two text instances to be present, while in TextVQA images were sampled on a category basis, emphasizing categories that are expected to contain text.
In terms of the questions provided, ST-VQA focuses on questions that can be answered unambiguously directly using part of the image text as answer, while in TextVQA any question requiring reading the image text is allowed.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">Despite the differences, the two datasets are highly complementary as the image sources used do not intersect with each other, creating an opportunity for transfer learning between the two datasets and maybe combining data for training models with greater generalization capabilities.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>ST-VQA Dataset</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Collection</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In this section we describe the process for collecting images, questions and answers for the ST-VQA dataset, and offer an in-depth analysis of the collected data. Subsequently, we detail the proposed tasks and introduce the evaluation metric.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Images:</span> The ST-VQA dataset comprises <math id="S3.SS1.p2.1.m1.2" class="ltx_Math" alttext="23,038" display="inline"><semantics id="S3.SS1.p2.1.m1.2a"><mrow id="S3.SS1.p2.1.m1.2.3.2" xref="S3.SS1.p2.1.m1.2.3.1.cmml"><mn id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">23</mn><mo id="S3.SS1.p2.1.m1.2.3.2.1" xref="S3.SS1.p2.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS1.p2.1.m1.2.2" xref="S3.SS1.p2.1.m1.2.2.cmml">038</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.2b"><list id="S3.SS1.p2.1.m1.2.3.1.cmml" xref="S3.SS1.p2.1.m1.2.3.2"><cn type="integer" id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">23</cn><cn type="integer" id="S3.SS1.p2.1.m1.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2">038</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.2c">23,038</annotation></semantics></math> images sourced from a combination of public datasets that include both scene text understanding datasets as well as generic computer vision ones.
In total, we used six different datasets, namely: ICDAR 2013<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and ICDAR2015<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, VizWiz<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, IIIT Scene Text Retrieval<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> and COCO-Text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. A key benefit of combining images from various datasets is the reduction of dataset bias such as selection, capture and negative set bias which have been shown to exist in popular image datasets<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Consequently, the combination of datasets results in a greater variability of questions.
To automatically select images to define questions and answers, we use an end-to-end single shot text retrieval architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. We automatically select all images that contain at least 2 text instances thus ensuring that the proposed questions contain at least 2 possible options as an answer. The final number of images and questions per dataset can be found in <a href="#S3.T1" title="Table 1 ‣ 3.1 Data Collection ‣ 3 ST-VQA Dataset ‣ Scene Text Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S3.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.1.1.1" class="ltx_p">Original Dataset</span>
</span>
</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Images</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Questions</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t">
<span id="S3.T1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.1.1.1" class="ltx_p">Coco-text</span>
</span>
</th>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">7,520</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">10,854</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row">
<span id="S3.T1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.1.1.1" class="ltx_p">Visual Genome</span>
</span>
</th>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_right">8,490</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_right">11,195</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row">
<span id="S3.T1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.3.1.1.1" class="ltx_p">VizWiz</span>
</span>
</th>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_right">835</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_right">1,303</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.5.4.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row">
<span id="S3.T1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.4.1.1.1" class="ltx_p">ICDAR</span>
</span>
</th>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_right">1,088</td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_right">1,423</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<th id="S3.T1.1.6.5.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row">
<span id="S3.T1.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.5.1.1.1" class="ltx_p">ImageNet</span>
</span>
</th>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_right">3,680</td>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_right">5,165</td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<th id="S3.T1.1.7.6.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row">
<span id="S3.T1.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.6.1.1.1" class="ltx_p">IIIT-STR</span>
</span>
</th>
<td id="S3.T1.1.7.6.2" class="ltx_td ltx_align_right">1,425</td>
<td id="S3.T1.1.7.6.3" class="ltx_td ltx_align_right">1,890</td>
</tr>
<tr id="S3.T1.1.8.7" class="ltx_tr">
<th id="S3.T1.1.8.7.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_bb ltx_border_t">
<span id="S3.T1.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.7.1.1.1" class="ltx_p">Total</span>
</span>
</th>
<td id="S3.T1.1.8.7.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">23,038</td>
<td id="S3.T1.1.8.7.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">31,791</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Number of images and questions gathered per dataset.</figcaption>
</figure>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Question and Answers:</span> The ST-VQA dataset comprises <math id="S3.SS1.p3.1.m1.2" class="ltx_Math" alttext="31,791" display="inline"><semantics id="S3.SS1.p3.1.m1.2a"><mrow id="S3.SS1.p3.1.m1.2.3.2" xref="S3.SS1.p3.1.m1.2.3.1.cmml"><mn id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">31</mn><mo id="S3.SS1.p3.1.m1.2.3.2.1" xref="S3.SS1.p3.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS1.p3.1.m1.2.2" xref="S3.SS1.p3.1.m1.2.2.cmml">791</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.2b"><list id="S3.SS1.p3.1.m1.2.3.1.cmml" xref="S3.SS1.p3.1.m1.2.3.2"><cn type="integer" id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">31</cn><cn type="integer" id="S3.SS1.p3.1.m1.2.2.cmml" xref="S3.SS1.p3.1.m1.2.2">791</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.2c">31,791</annotation></semantics></math> questions. To gather the questions and answers of our dataset, we used the crowd-sourcing platform Amazon Mechanical Turk (AMT). During the collection of questions and answers, we encouraged workers to come up with closed-ended questions that can be unambiguously answered with text found in the image, prohibiting them to ask yes/no questions or questions that can be answered only based on the visual information.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">The process of collecting question and answer pairs consisted of two steps. First, the workers were given an image along with instructions asking them to come up with a question that can be answered using the text found in the image. The workers were asked to write up to three question and answer pairs. Then, as a verification step, we perform a second AMT task that consisted of providing different workers with the image and asking them to respond to the previously defined question.
We filtered the questions for which we did not obtain the same answer in both steps, in order to remove ambiguous questions. The ambiguous questions were checked by the authors and corrected if necessary, before being added to the dataset. In some cases both answers were deemed correct and accepted, therefore ST-VQA questions have up to two different valid answers.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.6" class="ltx_p">In total, the proposed ST-VQA dataset comprises <math id="S3.SS1.p5.1.m1.2" class="ltx_Math" alttext="23,038" display="inline"><semantics id="S3.SS1.p5.1.m1.2a"><mrow id="S3.SS1.p5.1.m1.2.3.2" xref="S3.SS1.p5.1.m1.2.3.1.cmml"><mn id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">23</mn><mo id="S3.SS1.p5.1.m1.2.3.2.1" xref="S3.SS1.p5.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS1.p5.1.m1.2.2" xref="S3.SS1.p5.1.m1.2.2.cmml">038</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.2b"><list id="S3.SS1.p5.1.m1.2.3.1.cmml" xref="S3.SS1.p5.1.m1.2.3.2"><cn type="integer" id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">23</cn><cn type="integer" id="S3.SS1.p5.1.m1.2.2.cmml" xref="S3.SS1.p5.1.m1.2.2">038</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.2c">23,038</annotation></semantics></math> images with <math id="S3.SS1.p5.2.m2.2" class="ltx_Math" alttext="31,791" display="inline"><semantics id="S3.SS1.p5.2.m2.2a"><mrow id="S3.SS1.p5.2.m2.2.3.2" xref="S3.SS1.p5.2.m2.2.3.1.cmml"><mn id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml">31</mn><mo id="S3.SS1.p5.2.m2.2.3.2.1" xref="S3.SS1.p5.2.m2.2.3.1.cmml">,</mo><mn id="S3.SS1.p5.2.m2.2.2" xref="S3.SS1.p5.2.m2.2.2.cmml">791</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.2b"><list id="S3.SS1.p5.2.m2.2.3.1.cmml" xref="S3.SS1.p5.2.m2.2.3.2"><cn type="integer" id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">31</cn><cn type="integer" id="S3.SS1.p5.2.m2.2.2.cmml" xref="S3.SS1.p5.2.m2.2.2">791</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.2c">31,791</annotation></semantics></math> questions/answers pair separated into <math id="S3.SS1.p5.3.m3.2" class="ltx_Math" alttext="19,027" display="inline"><semantics id="S3.SS1.p5.3.m3.2a"><mrow id="S3.SS1.p5.3.m3.2.3.2" xref="S3.SS1.p5.3.m3.2.3.1.cmml"><mn id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml">19</mn><mo id="S3.SS1.p5.3.m3.2.3.2.1" xref="S3.SS1.p5.3.m3.2.3.1.cmml">,</mo><mn id="S3.SS1.p5.3.m3.2.2" xref="S3.SS1.p5.3.m3.2.2.cmml">027</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.2b"><list id="S3.SS1.p5.3.m3.2.3.1.cmml" xref="S3.SS1.p5.3.m3.2.3.2"><cn type="integer" id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1">19</cn><cn type="integer" id="S3.SS1.p5.3.m3.2.2.cmml" xref="S3.SS1.p5.3.m3.2.2">027</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.2c">19,027</annotation></semantics></math> images - <math id="S3.SS1.p5.4.m4.2" class="ltx_Math" alttext="26,308" display="inline"><semantics id="S3.SS1.p5.4.m4.2a"><mrow id="S3.SS1.p5.4.m4.2.3.2" xref="S3.SS1.p5.4.m4.2.3.1.cmml"><mn id="S3.SS1.p5.4.m4.1.1" xref="S3.SS1.p5.4.m4.1.1.cmml">26</mn><mo id="S3.SS1.p5.4.m4.2.3.2.1" xref="S3.SS1.p5.4.m4.2.3.1.cmml">,</mo><mn id="S3.SS1.p5.4.m4.2.2" xref="S3.SS1.p5.4.m4.2.2.cmml">308</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.4.m4.2b"><list id="S3.SS1.p5.4.m4.2.3.1.cmml" xref="S3.SS1.p5.4.m4.2.3.2"><cn type="integer" id="S3.SS1.p5.4.m4.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1">26</cn><cn type="integer" id="S3.SS1.p5.4.m4.2.2.cmml" xref="S3.SS1.p5.4.m4.2.2">308</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.4.m4.2c">26,308</annotation></semantics></math> questions for training and <math id="S3.SS1.p5.5.m5.2" class="ltx_Math" alttext="2,993" display="inline"><semantics id="S3.SS1.p5.5.m5.2a"><mrow id="S3.SS1.p5.5.m5.2.3.2" xref="S3.SS1.p5.5.m5.2.3.1.cmml"><mn id="S3.SS1.p5.5.m5.1.1" xref="S3.SS1.p5.5.m5.1.1.cmml">2</mn><mo id="S3.SS1.p5.5.m5.2.3.2.1" xref="S3.SS1.p5.5.m5.2.3.1.cmml">,</mo><mn id="S3.SS1.p5.5.m5.2.2" xref="S3.SS1.p5.5.m5.2.2.cmml">993</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.5.m5.2b"><list id="S3.SS1.p5.5.m5.2.3.1.cmml" xref="S3.SS1.p5.5.m5.2.3.2"><cn type="integer" id="S3.SS1.p5.5.m5.1.1.cmml" xref="S3.SS1.p5.5.m5.1.1">2</cn><cn type="integer" id="S3.SS1.p5.5.m5.2.2.cmml" xref="S3.SS1.p5.5.m5.2.2">993</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.5.m5.2c">2,993</annotation></semantics></math> images - <math id="S3.SS1.p5.6.m6.2" class="ltx_Math" alttext="4,163" display="inline"><semantics id="S3.SS1.p5.6.m6.2a"><mrow id="S3.SS1.p5.6.m6.2.3.2" xref="S3.SS1.p5.6.m6.2.3.1.cmml"><mn id="S3.SS1.p5.6.m6.1.1" xref="S3.SS1.p5.6.m6.1.1.cmml">4</mn><mo id="S3.SS1.p5.6.m6.2.3.2.1" xref="S3.SS1.p5.6.m6.2.3.1.cmml">,</mo><mn id="S3.SS1.p5.6.m6.2.2" xref="S3.SS1.p5.6.m6.2.2.cmml">163</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.6.m6.2b"><list id="S3.SS1.p5.6.m6.2.3.1.cmml" xref="S3.SS1.p5.6.m6.2.3.2"><cn type="integer" id="S3.SS1.p5.6.m6.1.1.cmml" xref="S3.SS1.p5.6.m6.1.1">4</cn><cn type="integer" id="S3.SS1.p5.6.m6.2.2.cmml" xref="S3.SS1.p5.6.m6.2.2">163</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.6.m6.2c">4,163</annotation></semantics></math> questions for testing. We present examples of question and answers of our dataset in <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Scene Text Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Analysis and Comparison with TextVQA</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In <a href="#S3.F2" title="Figure 2 ‣ 3.2 Analysis and Comparison with TextVQA ‣ 3 ST-VQA Dataset ‣ Scene Text Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a> we provide the length distribution for the gathered questions and answers of the ST-VQA datasets, in comparison to the recently presented TextVQA. It can be observed that the length statistics of the two datasets are closely related.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/1905.13648/assets/x1.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="203" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Percentage of questions (top) and answers (bottom) that contain a specific number of words.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">To further explore the statistics of our dataset, <a href="#S3.F3" title="Figure 3 ‣ 3.2 Analysis and Comparison with TextVQA ‣ 3 ST-VQA Dataset ‣ Scene Text Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a> visualises how the ST-VQA questions are formed.
As it can be appreciated, our questions start with “What, Where, Which, How and Who”. A considerable percentage starts with “What” questions, as expected given the nature of the task. A critical point to realize however, is that the questions are not explicitly asking for specific text that appears in the scene; rather they are formulated in a way that requires to have certain prior world knowledge/experience. For example, some of the <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_italic">“what”</span> questions inquire about a brand, website, name, bus number, etc., which require some explicit knowledge about what a brand or website is.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">There has been a lot of effort to deal with the language prior inside the datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. One of the reasons for having language priors in datasets is the uneven distribution of answers in the dataset. In VQA v1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, since the dataset is formed from the images of MSCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, the answers to the question of “what sport …” are <span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_italic">tennis</span> and <span id="S3.SS2.p3.1.2" class="ltx_text ltx_font_italic">baseball</span> over 50%. Another example is the question “is there …”, having <span id="S3.SS2.p3.1.3" class="ltx_text ltx_font_italic">yes</span> as an answer in over <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mn id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">70</mn><mo id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">70\%</annotation></semantics></math> of the cases. As can be seen from <a href="#S3.F4" title="Figure 4 ‣ 3.2 Analysis and Comparison with TextVQA ‣ 3 ST-VQA Dataset ‣ Scene Text Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>, our dataset apart from the “sign” and “year” questions follows a uniform distribution for the answers, reducing the risk of language priors while having a big vocabulary for the answers.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/1905.13648/assets/x2.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="461" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Distribution of questions in the ST-VQA train set by their starting 4-grams (ordered from center to outwards). Words with a small contribution are not shown for better visualization.</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/1905.13648/assets/x3.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="146" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Distribution of answers for different types of questions in the ST-VQA train set. Each color represents a different unique answer.</figcaption>
</figure>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.3" class="ltx_p">To put ST-VQA in perspective, VQA 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, the biggest dataset in the community, contains 1.1 million questions out of which only 8k, corresponding to less than <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="1\%" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mrow id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mn id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">1</mn><mo id="S3.SS2.p4.1.m1.1.1.1" xref="S3.SS2.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">1\%</annotation></semantics></math> of the total questions, requires reading the text in the image. The TextVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> dataset on the other hand comprises <math id="S3.SS2.p4.2.m2.2" class="ltx_Math" alttext="28,408" display="inline"><semantics id="S3.SS2.p4.2.m2.2a"><mrow id="S3.SS2.p4.2.m2.2.3.2" xref="S3.SS2.p4.2.m2.2.3.1.cmml"><mn id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">28</mn><mo id="S3.SS2.p4.2.m2.2.3.2.1" xref="S3.SS2.p4.2.m2.2.3.1.cmml">,</mo><mn id="S3.SS2.p4.2.m2.2.2" xref="S3.SS2.p4.2.m2.2.2.cmml">408</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.2b"><list id="S3.SS2.p4.2.m2.2.3.1.cmml" xref="S3.SS2.p4.2.m2.2.3.2"><cn type="integer" id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">28</cn><cn type="integer" id="S3.SS2.p4.2.m2.2.2.cmml" xref="S3.SS2.p4.2.m2.2.2">408</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.2c">28,408</annotation></semantics></math> images paired with <math id="S3.SS2.p4.3.m3.2" class="ltx_Math" alttext="45,336" display="inline"><semantics id="S3.SS2.p4.3.m3.2a"><mrow id="S3.SS2.p4.3.m3.2.3.2" xref="S3.SS2.p4.3.m3.2.3.1.cmml"><mn id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">45</mn><mo id="S3.SS2.p4.3.m3.2.3.2.1" xref="S3.SS2.p4.3.m3.2.3.1.cmml">,</mo><mn id="S3.SS2.p4.3.m3.2.2" xref="S3.SS2.p4.3.m3.2.2.cmml">336</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.2b"><list id="S3.SS2.p4.3.m3.2.3.1.cmml" xref="S3.SS2.p4.3.m3.2.3.2"><cn type="integer" id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">45</cn><cn type="integer" id="S3.SS2.p4.3.m3.2.2.cmml" xref="S3.SS2.p4.3.m3.2.2">336</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.2c">45,336</annotation></semantics></math> questions.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.2" class="ltx_p">As a result of the different collection procedures followed, all ST-VQA questions can be answered unambiguously directly using the text in the image, while in the case of TextVQA reportedly <math id="S3.SS2.p5.1.m1.1" class="ltx_Math" alttext="39\%" display="inline"><semantics id="S3.SS2.p5.1.m1.1a"><mrow id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml"><mn id="S3.SS2.p5.1.m1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.2.cmml">39</mn><mo id="S3.SS2.p5.1.m1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.p5.1.m1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2">39</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">39\%</annotation></semantics></math> (<math id="S3.SS2.p5.2.m2.1" class="ltx_Math" alttext="18k" display="inline"><semantics id="S3.SS2.p5.2.m2.1a"><mrow id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml"><mn id="S3.SS2.p5.2.m2.1.1.2" xref="S3.SS2.p5.2.m2.1.1.2.cmml">18</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p5.2.m2.1.1.1" xref="S3.SS2.p5.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.p5.2.m2.1.1.3" xref="S3.SS2.p5.2.m2.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><apply id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1"><times id="S3.SS2.p5.2.m2.1.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1.1"></times><cn type="integer" id="S3.SS2.p5.2.m2.1.1.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2">18</cn><ci id="S3.SS2.p5.2.m2.1.1.3.cmml" xref="S3.SS2.p5.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">18k</annotation></semantics></math>) of the answers do not contain any of the OCR tokens <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Presentation of the TextVQA Challenge, CVPR 2019</span></span></span>. This might be either due to the type of the questions defined, or due to shortcomings of the employed text recognition engine.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.5" class="ltx_p">The fact that ST-VQA answers are explicitly grounded to the scene text, allows us to collect a single answer per question
To consider an answer as correct, we introduce a soft metric that requires it to have a small edit distance to the correct answer (see section <a href="#S3.SS4" title="3.4 Evaluation and Open Challenge ‣ 3 ST-VQA Dataset ‣ Scene Text Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>), factoring this way in the evaluation procedure the performance of the text recognition sub-system. In the case of TextVQA, <math id="S3.SS2.p6.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS2.p6.1.m1.1a"><mn id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><cn type="integer" id="S3.SS2.p6.1.m1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">10</annotation></semantics></math> answers are collected per question and any answer supported by at least three subjects is considered correct.
In order to better understand the effects of our approach compared to collecting multiple responses like in TextVQA, we performed an experiment collecting <math id="S3.SS2.p6.2.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS2.p6.2.m2.1a"><mn id="S3.SS2.p6.2.m2.1.1" xref="S3.SS2.p6.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.2.m2.1b"><cn type="integer" id="S3.SS2.p6.2.m2.1.1.cmml" xref="S3.SS2.p6.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.2.m2.1c">10</annotation></semantics></math> answers for a random subset of <math id="S3.SS2.p6.3.m3.2" class="ltx_Math" alttext="1,000" display="inline"><semantics id="S3.SS2.p6.3.m3.2a"><mrow id="S3.SS2.p6.3.m3.2.3.2" xref="S3.SS2.p6.3.m3.2.3.1.cmml"><mn id="S3.SS2.p6.3.m3.1.1" xref="S3.SS2.p6.3.m3.1.1.cmml">1</mn><mo id="S3.SS2.p6.3.m3.2.3.2.1" xref="S3.SS2.p6.3.m3.2.3.1.cmml">,</mo><mn id="S3.SS2.p6.3.m3.2.2" xref="S3.SS2.p6.3.m3.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.3.m3.2b"><list id="S3.SS2.p6.3.m3.2.3.1.cmml" xref="S3.SS2.p6.3.m3.2.3.2"><cn type="integer" id="S3.SS2.p6.3.m3.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1">1</cn><cn type="integer" id="S3.SS2.p6.3.m3.2.2.cmml" xref="S3.SS2.p6.3.m3.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.3.m3.2c">1,000</annotation></semantics></math> <span id="S3.SS2.p6.5.1" class="ltx_text">ST-VQA</span> questions. Our analysis showed that in <math id="S3.SS2.p6.4.m4.1" class="ltx_Math" alttext="84.1\%" display="inline"><semantics id="S3.SS2.p6.4.m4.1a"><mrow id="S3.SS2.p6.4.m4.1.1" xref="S3.SS2.p6.4.m4.1.1.cmml"><mn id="S3.SS2.p6.4.m4.1.1.2" xref="S3.SS2.p6.4.m4.1.1.2.cmml">84.1</mn><mo id="S3.SS2.p6.4.m4.1.1.1" xref="S3.SS2.p6.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.4.m4.1b"><apply id="S3.SS2.p6.4.m4.1.1.cmml" xref="S3.SS2.p6.4.m4.1.1"><csymbol cd="latexml" id="S3.SS2.p6.4.m4.1.1.1.cmml" xref="S3.SS2.p6.4.m4.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.p6.4.m4.1.1.2.cmml" xref="S3.SS2.p6.4.m4.1.1.2">84.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.4.m4.1c">84.1\%</annotation></semantics></math> of the cases there is agreement between the majority of subjects and the original answer. The same metric for TextVQA is <math id="S3.SS2.p6.5.m5.1" class="ltx_Math" alttext="80.3\%" display="inline"><semantics id="S3.SS2.p6.5.m5.1a"><mrow id="S3.SS2.p6.5.m5.1.1" xref="S3.SS2.p6.5.m5.1.1.cmml"><mn id="S3.SS2.p6.5.m5.1.1.2" xref="S3.SS2.p6.5.m5.1.1.2.cmml">80.3</mn><mo id="S3.SS2.p6.5.m5.1.1.1" xref="S3.SS2.p6.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.5.m5.1b"><apply id="S3.SS2.p6.5.m5.1.1.cmml" xref="S3.SS2.p6.5.m5.1.1"><csymbol cd="latexml" id="S3.SS2.p6.5.m5.1.1.1.cmml" xref="S3.SS2.p6.5.m5.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.p6.5.m5.1.1.2.cmml" xref="S3.SS2.p6.5.m5.1.1.2">80.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.5.m5.1c">80.3\%</annotation></semantics></math>, confirming that defining a single unambiguous answer results in similarly low ambiguity at evaluation time.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Tasks</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We define 3 novel tasks, suitable for the ST-VQA dataset, namely “strongly contextualised”, “weakly contextualised” and “open vocabulary”.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The proposed differentiation of tasks can be interpreted by how humans make use of prior knowledge to argue about their current situation. Such prior knowledge in <span id="S3.SS3.p2.1.1" class="ltx_text">ST-VQA</span> is provided as a dictionary, different for each task. Similar approaches using dynamic per-image dictionaries have been used for DVQA in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and for scene text understanding in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
Our formulation of the tasks is inspired by the previous notions and the difficulty per task increases gradually. In the strongly contextualised task we capture this prior knowledge by creating a dictionary <span id="S3.SS3.p2.1.2" class="ltx_text ltx_font_italic">per image</span> for the specific scenario depicted. In the weakly contextualised task we provide a <span id="S3.SS3.p2.1.3" class="ltx_text ltx_font_italic">single dictionary</span> comprising all the words in the answers of the dataset. Finally, for the open dictionary task, we treat the problem as tabula rasa where no a priori and no external information is available to the model.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">For the strongly contextualised task (1), following the standard practice used for end-to-end word spotting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, we create a dictionary per image that contains the words that appear in the answers defined for questions on that image, along with a series of distractors. The distractors are generated in two ways. On one hand, they comprise instances of scene text as returned by a text recogniser applied on the image. On the other hand, they comprise words obtained by exploiting the semantic understanding of the scene, in the form of the output of a dynamic lexicon generation model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
The dictionary for the strongly contextualised task is <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mn id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><cn type="integer" id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">100</annotation></semantics></math> words long and defined per image.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.3" class="ltx_p">In the weakly contextualised task (2), we provide a unique dictionary of <math id="S3.SS3.p4.1.m1.2" class="ltx_Math" alttext="30,000" display="inline"><semantics id="S3.SS3.p4.1.m1.2a"><mrow id="S3.SS3.p4.1.m1.2.3.2" xref="S3.SS3.p4.1.m1.2.3.1.cmml"><mn id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">30</mn><mo id="S3.SS3.p4.1.m1.2.3.2.1" xref="S3.SS3.p4.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS3.p4.1.m1.2.2" xref="S3.SS3.p4.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.2b"><list id="S3.SS3.p4.1.m1.2.3.1.cmml" xref="S3.SS3.p4.1.m1.2.3.2"><cn type="integer" id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">30</cn><cn type="integer" id="S3.SS3.p4.1.m1.2.2.cmml" xref="S3.SS3.p4.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.2c">30,000</annotation></semantics></math> words for all the datasets’ images which is formed by collecting all the <math id="S3.SS3.p4.2.m2.1" class="ltx_Math" alttext="22k" display="inline"><semantics id="S3.SS3.p4.2.m2.1a"><mrow id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml"><mn id="S3.SS3.p4.2.m2.1.1.2" xref="S3.SS3.p4.2.m2.1.1.2.cmml">22</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p4.2.m2.1.1.1" xref="S3.SS3.p4.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS3.p4.2.m2.1.1.3" xref="S3.SS3.p4.2.m2.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><apply id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1"><times id="S3.SS3.p4.2.m2.1.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1.1"></times><cn type="integer" id="S3.SS3.p4.2.m2.1.1.2.cmml" xref="S3.SS3.p4.2.m2.1.1.2">22</cn><ci id="S3.SS3.p4.2.m2.1.1.3.cmml" xref="S3.SS3.p4.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">22k</annotation></semantics></math> ground truth words plus <math id="S3.SS3.p4.3.m3.1" class="ltx_Math" alttext="8k" display="inline"><semantics id="S3.SS3.p4.3.m3.1a"><mrow id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml"><mn id="S3.SS3.p4.3.m3.1.1.2" xref="S3.SS3.p4.3.m3.1.1.2.cmml">8</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p4.3.m3.1.1.1" xref="S3.SS3.p4.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS3.p4.3.m3.1.1.3" xref="S3.SS3.p4.3.m3.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><apply id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1"><times id="S3.SS3.p4.3.m3.1.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1.1"></times><cn type="integer" id="S3.SS3.p4.3.m3.1.1.2.cmml" xref="S3.SS3.p4.3.m3.1.1.2">8</cn><ci id="S3.SS3.p4.3.m3.1.1.3.cmml" xref="S3.SS3.p4.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">8k</annotation></semantics></math> distractors generated in the same way as in the previous task. Finally for the open dictionary task (3), we provide no extra information thus we can consider it as an open-lexicon task.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">By proposing the aforementioned tasks the VQA problem is conceived in a novel manner that has certain advantages. First, it paves the way for research on automatically processing and generating such prior information, and its effect on the model design and performance. Second, it provides an interesting training ground for end-to-end reading systems, where the provided dictionaries can be used to prime text spotting methods.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Evaluation and Open Challenge</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Since the answers of our dataset are contained within the text found in the image, which is dependent on the accuracy of the OCR being employed, the classical evaluation metric of VQA tasks is not optimum for our dataset, e.g. if the model reasons properly about the answer but makes a mistake of a few characters in the recognition stage, like in <a href="#S4.F6" title="Figure 6 ‣ 4.1 Results ‣ 4 Baselines and Results ‣ Scene Text Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 6</span></a> (first row, third column), the typical accuracy score would be <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mn id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><cn type="integer" id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">0</cn></annotation-xml></semantics></math>. However, the metric we propose named Average Normalized Levenshtein Similarity (ANLS) would give an intermediate score between 0.5 and 1 that will softly penalise the OCR mistakes. Thus, a motivation of defining a metric that captures OCR accuracy as well as model reasoning is evident. To this end, in all 3 tasks we use the normalized Levenshtein similarity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> as an evaluation metric. More formally, we define ANLS as follows:</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\textrm{ANLS}=\frac{1}{N}\sum_{i=0}^{N}\left(\max_{j}s(a_{ij},o_{q_{i}})\right)\\
" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mtext id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3a.cmml">ANLS</mtext><mo id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mfrac id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml"><mn id="S3.E1.m1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.3.2.cmml">1</mn><mi id="S3.E1.m1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><munderover id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E1.m1.1.1.1.1.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E1.m1.1.1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2.3.2" xref="S3.E1.m1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E1.m1.1.1.1.1.2.2.3.1" xref="S3.E1.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E1.m1.1.1.1.1.2.2.3.3" xref="S3.E1.m1.1.1.1.1.2.2.3.3.cmml">0</mn></mrow><mi id="S3.E1.m1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.1.1.4.cmml"><munder id="S3.E1.m1.1.1.1.1.1.1.1.4.1" xref="S3.E1.m1.1.1.1.1.1.1.1.4.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.4.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.4.1.2.cmml">max</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.4.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.4.1.3.cmml">j</mi></munder><mo lspace="0.167em" id="S3.E1.m1.1.1.1.1.1.1.1.4a" xref="S3.E1.m1.1.1.1.1.1.1.1.4.cmml">⁡</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.4.2" xref="S3.E1.m1.1.1.1.1.1.1.1.4.2.cmml">s</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">a</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.2.2.4" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.cmml">o</mi><msub id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.2.cmml">q</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.3.cmml">i</mi></msub></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.2.2.5" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"></eq><ci id="S3.E1.m1.1.1.3a.cmml" xref="S3.E1.m1.1.1.3"><mtext id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3">ANLS</mtext></ci><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><times id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3"><divide id="S3.E1.m1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.3"></divide><cn type="integer" id="S3.E1.m1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.3.2">1</cn><ci id="S3.E1.m1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.3.3">𝑁</ci></apply><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2">subscript</csymbol><sum id="S3.E1.m1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2"></sum><apply id="S3.E1.m1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3"><eq id="S3.E1.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E1.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.2.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3.3">0</cn></apply></apply><ci id="S3.E1.m1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3">𝑁</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3"></times><apply id="S3.E1.m1.1.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.4"><apply id="S3.E1.m1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.4.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.4.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.4.1">subscript</csymbol><max id="S3.E1.m1.1.1.1.1.1.1.1.4.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.4.1.2"></max><ci id="S3.E1.m1.1.1.1.1.1.1.1.4.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.4.1.3">𝑗</ci></apply><ci id="S3.E1.m1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.4.2">𝑠</ci></apply><interval closure="open" id="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2"><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2">𝑎</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2">𝑖</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3">𝑗</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2">𝑜</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.2">𝑞</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.3">𝑖</ci></apply></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\textrm{ANLS}=\frac{1}{N}\sum_{i=0}^{N}\left(\max_{j}s(a_{ij},o_{q_{i}})\right)\\
</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.8" class="ltx_Math" alttext="s(a_{ij},o_{q_{i}})=\begin{cases}(1-NL(a_{ij},o_{q_{i}}))&amp;\text{if { $NL(a_{ij},o_{q_{i}})&lt;\tau$}}\\
$0$&amp;\text{if { $NL(a_{ij},o_{q_{i}})$ $\geqslant$ $\tau$}}\end{cases}" display="block"><semantics id="S3.Ex1.m1.8a"><mrow id="S3.Ex1.m1.8.8" xref="S3.Ex1.m1.8.8.cmml"><mrow id="S3.Ex1.m1.8.8.2" xref="S3.Ex1.m1.8.8.2.cmml"><mi id="S3.Ex1.m1.8.8.2.4" xref="S3.Ex1.m1.8.8.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.8.8.2.3" xref="S3.Ex1.m1.8.8.2.3.cmml">​</mo><mrow id="S3.Ex1.m1.8.8.2.2.2" xref="S3.Ex1.m1.8.8.2.2.3.cmml"><mo stretchy="false" id="S3.Ex1.m1.8.8.2.2.2.3" xref="S3.Ex1.m1.8.8.2.2.3.cmml">(</mo><msub id="S3.Ex1.m1.7.7.1.1.1.1" xref="S3.Ex1.m1.7.7.1.1.1.1.cmml"><mi id="S3.Ex1.m1.7.7.1.1.1.1.2" xref="S3.Ex1.m1.7.7.1.1.1.1.2.cmml">a</mi><mrow id="S3.Ex1.m1.7.7.1.1.1.1.3" xref="S3.Ex1.m1.7.7.1.1.1.1.3.cmml"><mi id="S3.Ex1.m1.7.7.1.1.1.1.3.2" xref="S3.Ex1.m1.7.7.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.7.7.1.1.1.1.3.1" xref="S3.Ex1.m1.7.7.1.1.1.1.3.1.cmml">​</mo><mi id="S3.Ex1.m1.7.7.1.1.1.1.3.3" xref="S3.Ex1.m1.7.7.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo id="S3.Ex1.m1.8.8.2.2.2.4" xref="S3.Ex1.m1.8.8.2.2.3.cmml">,</mo><msub id="S3.Ex1.m1.8.8.2.2.2.2" xref="S3.Ex1.m1.8.8.2.2.2.2.cmml"><mi id="S3.Ex1.m1.8.8.2.2.2.2.2" xref="S3.Ex1.m1.8.8.2.2.2.2.2.cmml">o</mi><msub id="S3.Ex1.m1.8.8.2.2.2.2.3" xref="S3.Ex1.m1.8.8.2.2.2.2.3.cmml"><mi id="S3.Ex1.m1.8.8.2.2.2.2.3.2" xref="S3.Ex1.m1.8.8.2.2.2.2.3.2.cmml">q</mi><mi id="S3.Ex1.m1.8.8.2.2.2.2.3.3" xref="S3.Ex1.m1.8.8.2.2.2.2.3.3.cmml">i</mi></msub></msub><mo stretchy="false" id="S3.Ex1.m1.8.8.2.2.2.5" xref="S3.Ex1.m1.8.8.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.8.8.3" xref="S3.Ex1.m1.8.8.3.cmml">=</mo><mrow id="S3.Ex1.m1.6.6" xref="S3.Ex1.m1.8.8.4.1.cmml"><mo id="S3.Ex1.m1.6.6.7" xref="S3.Ex1.m1.8.8.4.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.Ex1.m1.6.6.6" xref="S3.Ex1.m1.8.8.4.1.cmml"><mtr id="S3.Ex1.m1.6.6.6a" xref="S3.Ex1.m1.8.8.4.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.Ex1.m1.6.6.6b" xref="S3.Ex1.m1.8.8.4.1.cmml"><mrow id="S3.Ex1.m1.5.5.5.5.2.1.1" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.5.5.5.5.2.1.1.2" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.5.5.5.5.2.1.1.1" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.cmml"><mn id="S3.Ex1.m1.5.5.5.5.2.1.1.1.4" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.4.cmml">1</mn><mo id="S3.Ex1.m1.5.5.5.5.2.1.1.1.3" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.3.cmml">−</mo><mrow id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.cmml"><mi id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.4" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.3" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.3.cmml">​</mo><mi id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.5" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.5.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.3a" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.3.cmml">​</mo><mrow id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.3" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.3.cmml">(</mo><msub id="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.2.cmml">a</mi><mrow id="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.3.2" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.3.1" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.3.3" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.4" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.3.cmml">,</mo><msub id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.cmml"><mi id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.2" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.2.cmml">o</mi><msub id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.3" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.3.cmml"><mi id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.3.2" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.3.2.cmml">q</mi><mi id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.3.3" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.3.3.cmml">i</mi></msub></msub><mo stretchy="false" id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.5" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.Ex1.m1.5.5.5.5.2.1.1.3" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.cmml">)</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.Ex1.m1.6.6.6c" xref="S3.Ex1.m1.8.8.4.1.cmml"><mrow id="S3.Ex1.m1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1b.cmml"><mtext id="S3.Ex1.m1.1.1.1.1.1.1a" xref="S3.Ex1.m1.1.1.1.1.1.1b.cmml">if </mtext><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.cmml"><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.4" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.3.cmml">​</mo><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.5" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.5.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.3a" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.3.cmml">​</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.4" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.4.cmml"><mo stretchy="false" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.4.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.4.cmml">(</mo><msub id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.2.cmml">a</mi><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.4.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.4.cmml">,</mo><msub id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2.2.cmml">o</mi><msub id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2.3.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2.3.2.cmml">q</mi><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2.3.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2.3.3.cmml">i</mi></msub></msub><mo stretchy="false" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.4.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.4.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.3.cmml">&lt;</mo><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.4" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.4.cmml">τ</mi></mrow></mrow></mtd></mtr><mtr id="S3.Ex1.m1.6.6.6d" xref="S3.Ex1.m1.8.8.4.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.Ex1.m1.6.6.6e" xref="S3.Ex1.m1.8.8.4.1.cmml"><mtext id="S3.Ex1.m1.6.6.6.6.4.1" xref="S3.Ex1.m1.6.6.6.6.4.1a.cmml">0</mtext></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.Ex1.m1.6.6.6f" xref="S3.Ex1.m1.8.8.4.1.cmml"><mrow id="S3.Ex1.m1.4.4.4.4.3.3" xref="S3.Ex1.m1.4.4.4.4.3.3d.cmml"><mtext id="S3.Ex1.m1.4.4.4.4.3.3a" xref="S3.Ex1.m1.4.4.4.4.3.3d.cmml">if </mtext><mrow id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.cmml"><mi id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.4" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.3" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.3.cmml">​</mo><mi id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.5" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.5.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.3a" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.3.cmml">​</mo><mrow id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.4" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.4.cmml"><mo stretchy="false" id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.4.1" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.4.cmml">(</mo><msub id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.2" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.2.cmml">a</mi><mrow id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.3" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.3.cmml"><mi id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.3.2" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.3.1" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.3.3" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.4.2" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.4.cmml">,</mo><msub id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2.cmml"><mi id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2.2" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2.2.cmml">o</mi><msub id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2.3" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2.3.cmml"><mi id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2.3.2" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2.3.2.cmml">q</mi><mi id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2.3.3" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2.3.3.cmml">i</mi></msub></msub><mo stretchy="false" id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.4.3" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.4.cmml">)</mo></mrow></mrow><mtext id="S3.Ex1.m1.4.4.4.4.3.3b" xref="S3.Ex1.m1.4.4.4.4.3.3d.cmml"> </mtext><mo id="S3.Ex1.m1.3.3.3.3.2.2.2.2.m2.1.1" xref="S3.Ex1.m1.3.3.3.3.2.2.2.2.m2.1.1.cmml">⩾</mo><mtext id="S3.Ex1.m1.4.4.4.4.3.3c" xref="S3.Ex1.m1.4.4.4.4.3.3d.cmml"> </mtext><mi id="S3.Ex1.m1.4.4.4.4.3.3.3.3.m3.1.1" xref="S3.Ex1.m1.4.4.4.4.3.3.3.3.m3.1.1.cmml">τ</mi></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.8b"><apply id="S3.Ex1.m1.8.8.cmml" xref="S3.Ex1.m1.8.8"><eq id="S3.Ex1.m1.8.8.3.cmml" xref="S3.Ex1.m1.8.8.3"></eq><apply id="S3.Ex1.m1.8.8.2.cmml" xref="S3.Ex1.m1.8.8.2"><times id="S3.Ex1.m1.8.8.2.3.cmml" xref="S3.Ex1.m1.8.8.2.3"></times><ci id="S3.Ex1.m1.8.8.2.4.cmml" xref="S3.Ex1.m1.8.8.2.4">𝑠</ci><interval closure="open" id="S3.Ex1.m1.8.8.2.2.3.cmml" xref="S3.Ex1.m1.8.8.2.2.2"><apply id="S3.Ex1.m1.7.7.1.1.1.1.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.7.7.1.1.1.1.1.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.7.7.1.1.1.1.2.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.2">𝑎</ci><apply id="S3.Ex1.m1.7.7.1.1.1.1.3.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.3"><times id="S3.Ex1.m1.7.7.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.3.1"></times><ci id="S3.Ex1.m1.7.7.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.3.2">𝑖</ci><ci id="S3.Ex1.m1.7.7.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.3.3">𝑗</ci></apply></apply><apply id="S3.Ex1.m1.8.8.2.2.2.2.cmml" xref="S3.Ex1.m1.8.8.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.8.8.2.2.2.2.1.cmml" xref="S3.Ex1.m1.8.8.2.2.2.2">subscript</csymbol><ci id="S3.Ex1.m1.8.8.2.2.2.2.2.cmml" xref="S3.Ex1.m1.8.8.2.2.2.2.2">𝑜</ci><apply id="S3.Ex1.m1.8.8.2.2.2.2.3.cmml" xref="S3.Ex1.m1.8.8.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.8.8.2.2.2.2.3.1.cmml" xref="S3.Ex1.m1.8.8.2.2.2.2.3">subscript</csymbol><ci id="S3.Ex1.m1.8.8.2.2.2.2.3.2.cmml" xref="S3.Ex1.m1.8.8.2.2.2.2.3.2">𝑞</ci><ci id="S3.Ex1.m1.8.8.2.2.2.2.3.3.cmml" xref="S3.Ex1.m1.8.8.2.2.2.2.3.3">𝑖</ci></apply></apply></interval></apply><apply id="S3.Ex1.m1.8.8.4.1.cmml" xref="S3.Ex1.m1.6.6"><csymbol cd="latexml" id="S3.Ex1.m1.8.8.4.1.1.cmml" xref="S3.Ex1.m1.6.6.7">cases</csymbol><apply id="S3.Ex1.m1.5.5.5.5.2.1.1.1.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1"><minus id="S3.Ex1.m1.5.5.5.5.2.1.1.1.3.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.3"></minus><cn type="integer" id="S3.Ex1.m1.5.5.5.5.2.1.1.1.4.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.4">1</cn><apply id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2"><times id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.3.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.3"></times><ci id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.4.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.4">𝑁</ci><ci id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.5.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.5">𝐿</ci><interval closure="open" id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.3.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2"><apply id="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.2">𝑎</ci><apply id="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.3"><times id="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.3.1"></times><ci id="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.3.2">𝑖</ci><ci id="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.1.1.1.1.3.3">𝑗</ci></apply></apply><apply id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.1.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.2.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.2">𝑜</ci><apply id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.3.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.3.1.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.3">subscript</csymbol><ci id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.3.2.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.3.2">𝑞</ci><ci id="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.3.3.cmml" xref="S3.Ex1.m1.5.5.5.5.2.1.1.1.2.2.2.2.3.3">𝑖</ci></apply></apply></interval></apply></apply><ci id="S3.Ex1.m1.1.1.1.1.1.1b.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1"><mrow id="S3.Ex1.m1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1"><mtext id="S3.Ex1.m1.1.1.1.1.1.1a.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1">if </mtext><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2"><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.4.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.4">N</mi><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.3" lspace='0px' rspace='0px'></mo><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.5.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.5">L</mi><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.3a.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.3" lspace='0px' rspace='0px'></mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.4.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.4"><mo stretchy="false" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.4.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.4">(</mo><msub id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.2">a</mi><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3.2">i</mi><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3.1" lspace='0px' rspace='0px'></mo><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3.3">j</mi></mrow></msub><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.4.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.4">,</mo><msub id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2.2">o</mi><msub id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2.3"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2.3.2">q</mi><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.2.2.3.3">i</mi></msub></msub><mo stretchy="false" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.4.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.2.2.4">)</mo></mrow></mrow><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.3">&lt;</mo><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.4.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.m1.2.2.4">τ</mi></mrow></mrow></ci><ci id="S3.Ex1.m1.6.6.6.6.4.1a.cmml" xref="S3.Ex1.m1.6.6.6.6.4.1"><mtext id="S3.Ex1.m1.6.6.6.6.4.1.cmml" xref="S3.Ex1.m1.6.6.6.6.4.1">0</mtext></ci><ci id="S3.Ex1.m1.4.4.4.4.3.3d.cmml" xref="S3.Ex1.m1.4.4.4.4.3.3"><mrow id="S3.Ex1.m1.4.4.4.4.3.3.cmml" xref="S3.Ex1.m1.4.4.4.4.3.3"><mtext id="S3.Ex1.m1.4.4.4.4.3.3a.cmml" xref="S3.Ex1.m1.4.4.4.4.3.3">if </mtext><mrow id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2"><mi id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.4.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.4">N</mi><mo id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.3.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.3" lspace='0px' rspace='0px'></mo><mi id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.5.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.5">L</mi><mo id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.3a.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.3" lspace='0px' rspace='0px'></mo><mrow id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.4.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.4"><mo stretchy="false" id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.4.1.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.4">(</mo><msub id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1"><mi id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.2">a</mi><mrow id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.3"><mi id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.3.2">i</mi><mo id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.3.1" lspace='0px' rspace='0px'></mo><mi id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.1.1.1.1.1.3.3">j</mi></mrow></msub><mo id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.4.2.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.4">,</mo><msub id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2"><mi id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2.2">o</mi><msub id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2.3.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2.3"><mi id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2.3.2.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2.3.2">q</mi><mi id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2.3.3.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.2.2.3.3">i</mi></msub></msub><mo stretchy="false" id="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.4.3.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1.1.m1.2.2.2.4">)</mo></mrow></mrow><mtext id="S3.Ex1.m1.4.4.4.4.3.3b.cmml" xref="S3.Ex1.m1.4.4.4.4.3.3"> </mtext><mo id="S3.Ex1.m1.3.3.3.3.2.2.2.2.m2.1.1.cmml" xref="S3.Ex1.m1.3.3.3.3.2.2.2.2.m2.1.1">⩾</mo><mtext id="S3.Ex1.m1.4.4.4.4.3.3c.cmml" xref="S3.Ex1.m1.4.4.4.4.3.3"> </mtext><mi id="S3.Ex1.m1.4.4.4.4.3.3.3.3.m3.1.1.cmml" xref="S3.Ex1.m1.4.4.4.4.3.3.3.3.m3.1.1">τ</mi></mrow></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.8c">s(a_{ij},o_{q_{i}})=\begin{cases}(1-NL(a_{ij},o_{q_{i}}))&amp;\text{if { $NL(a_{ij},o_{q_{i}})&lt;\tau$}}\\
$0$&amp;\text{if { $NL(a_{ij},o_{q_{i}})$ $\geqslant$ $\tau$}}\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p3" class="ltx_para ltx_noindent">
<p id="S3.SS4.p3.18" class="ltx_p">where <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><mi id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><ci id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">N</annotation></semantics></math> is the total number of questions in the dataset, <math id="S3.SS4.p3.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS4.p3.2.m2.1a"><mi id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><ci id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">M</annotation></semantics></math> is the total number of GT answers per question, <math id="S3.SS4.p3.3.m3.1" class="ltx_Math" alttext="a_{ij}" display="inline"><semantics id="S3.SS4.p3.3.m3.1a"><msub id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml"><mi id="S3.SS4.p3.3.m3.1.1.2" xref="S3.SS4.p3.3.m3.1.1.2.cmml">a</mi><mrow id="S3.SS4.p3.3.m3.1.1.3" xref="S3.SS4.p3.3.m3.1.1.3.cmml"><mi id="S3.SS4.p3.3.m3.1.1.3.2" xref="S3.SS4.p3.3.m3.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.3.m3.1.1.3.1" xref="S3.SS4.p3.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p3.3.m3.1.1.3.3" xref="S3.SS4.p3.3.m3.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><apply id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.3.m3.1.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p3.3.m3.1.1.2.cmml" xref="S3.SS4.p3.3.m3.1.1.2">𝑎</ci><apply id="S3.SS4.p3.3.m3.1.1.3.cmml" xref="S3.SS4.p3.3.m3.1.1.3"><times id="S3.SS4.p3.3.m3.1.1.3.1.cmml" xref="S3.SS4.p3.3.m3.1.1.3.1"></times><ci id="S3.SS4.p3.3.m3.1.1.3.2.cmml" xref="S3.SS4.p3.3.m3.1.1.3.2">𝑖</ci><ci id="S3.SS4.p3.3.m3.1.1.3.3.cmml" xref="S3.SS4.p3.3.m3.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">a_{ij}</annotation></semantics></math> are the ground truth answers where <math id="S3.SS4.p3.4.m4.3" class="ltx_Math" alttext="i=\{0,...,N\}" display="inline"><semantics id="S3.SS4.p3.4.m4.3a"><mrow id="S3.SS4.p3.4.m4.3.4" xref="S3.SS4.p3.4.m4.3.4.cmml"><mi id="S3.SS4.p3.4.m4.3.4.2" xref="S3.SS4.p3.4.m4.3.4.2.cmml">i</mi><mo id="S3.SS4.p3.4.m4.3.4.1" xref="S3.SS4.p3.4.m4.3.4.1.cmml">=</mo><mrow id="S3.SS4.p3.4.m4.3.4.3.2" xref="S3.SS4.p3.4.m4.3.4.3.1.cmml"><mo stretchy="false" id="S3.SS4.p3.4.m4.3.4.3.2.1" xref="S3.SS4.p3.4.m4.3.4.3.1.cmml">{</mo><mn id="S3.SS4.p3.4.m4.1.1" xref="S3.SS4.p3.4.m4.1.1.cmml">0</mn><mo id="S3.SS4.p3.4.m4.3.4.3.2.2" xref="S3.SS4.p3.4.m4.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS4.p3.4.m4.2.2" xref="S3.SS4.p3.4.m4.2.2.cmml">…</mi><mo id="S3.SS4.p3.4.m4.3.4.3.2.3" xref="S3.SS4.p3.4.m4.3.4.3.1.cmml">,</mo><mi id="S3.SS4.p3.4.m4.3.3" xref="S3.SS4.p3.4.m4.3.3.cmml">N</mi><mo stretchy="false" id="S3.SS4.p3.4.m4.3.4.3.2.4" xref="S3.SS4.p3.4.m4.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.3b"><apply id="S3.SS4.p3.4.m4.3.4.cmml" xref="S3.SS4.p3.4.m4.3.4"><eq id="S3.SS4.p3.4.m4.3.4.1.cmml" xref="S3.SS4.p3.4.m4.3.4.1"></eq><ci id="S3.SS4.p3.4.m4.3.4.2.cmml" xref="S3.SS4.p3.4.m4.3.4.2">𝑖</ci><set id="S3.SS4.p3.4.m4.3.4.3.1.cmml" xref="S3.SS4.p3.4.m4.3.4.3.2"><cn type="integer" id="S3.SS4.p3.4.m4.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1">0</cn><ci id="S3.SS4.p3.4.m4.2.2.cmml" xref="S3.SS4.p3.4.m4.2.2">…</ci><ci id="S3.SS4.p3.4.m4.3.3.cmml" xref="S3.SS4.p3.4.m4.3.3">𝑁</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.3c">i=\{0,...,N\}</annotation></semantics></math>, and <math id="S3.SS4.p3.5.m5.3" class="ltx_Math" alttext="j=\{0,...,M\}" display="inline"><semantics id="S3.SS4.p3.5.m5.3a"><mrow id="S3.SS4.p3.5.m5.3.4" xref="S3.SS4.p3.5.m5.3.4.cmml"><mi id="S3.SS4.p3.5.m5.3.4.2" xref="S3.SS4.p3.5.m5.3.4.2.cmml">j</mi><mo id="S3.SS4.p3.5.m5.3.4.1" xref="S3.SS4.p3.5.m5.3.4.1.cmml">=</mo><mrow id="S3.SS4.p3.5.m5.3.4.3.2" xref="S3.SS4.p3.5.m5.3.4.3.1.cmml"><mo stretchy="false" id="S3.SS4.p3.5.m5.3.4.3.2.1" xref="S3.SS4.p3.5.m5.3.4.3.1.cmml">{</mo><mn id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml">0</mn><mo id="S3.SS4.p3.5.m5.3.4.3.2.2" xref="S3.SS4.p3.5.m5.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS4.p3.5.m5.2.2" xref="S3.SS4.p3.5.m5.2.2.cmml">…</mi><mo id="S3.SS4.p3.5.m5.3.4.3.2.3" xref="S3.SS4.p3.5.m5.3.4.3.1.cmml">,</mo><mi id="S3.SS4.p3.5.m5.3.3" xref="S3.SS4.p3.5.m5.3.3.cmml">M</mi><mo stretchy="false" id="S3.SS4.p3.5.m5.3.4.3.2.4" xref="S3.SS4.p3.5.m5.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.3b"><apply id="S3.SS4.p3.5.m5.3.4.cmml" xref="S3.SS4.p3.5.m5.3.4"><eq id="S3.SS4.p3.5.m5.3.4.1.cmml" xref="S3.SS4.p3.5.m5.3.4.1"></eq><ci id="S3.SS4.p3.5.m5.3.4.2.cmml" xref="S3.SS4.p3.5.m5.3.4.2">𝑗</ci><set id="S3.SS4.p3.5.m5.3.4.3.1.cmml" xref="S3.SS4.p3.5.m5.3.4.3.2"><cn type="integer" id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">0</cn><ci id="S3.SS4.p3.5.m5.2.2.cmml" xref="S3.SS4.p3.5.m5.2.2">…</ci><ci id="S3.SS4.p3.5.m5.3.3.cmml" xref="S3.SS4.p3.5.m5.3.3">𝑀</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.3c">j=\{0,...,M\}</annotation></semantics></math>, and <math id="S3.SS4.p3.6.m6.1" class="ltx_Math" alttext="o_{q_{i}}" display="inline"><semantics id="S3.SS4.p3.6.m6.1a"><msub id="S3.SS4.p3.6.m6.1.1" xref="S3.SS4.p3.6.m6.1.1.cmml"><mi id="S3.SS4.p3.6.m6.1.1.2" xref="S3.SS4.p3.6.m6.1.1.2.cmml">o</mi><msub id="S3.SS4.p3.6.m6.1.1.3" xref="S3.SS4.p3.6.m6.1.1.3.cmml"><mi id="S3.SS4.p3.6.m6.1.1.3.2" xref="S3.SS4.p3.6.m6.1.1.3.2.cmml">q</mi><mi id="S3.SS4.p3.6.m6.1.1.3.3" xref="S3.SS4.p3.6.m6.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.6.m6.1b"><apply id="S3.SS4.p3.6.m6.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.6.m6.1.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS4.p3.6.m6.1.1.2.cmml" xref="S3.SS4.p3.6.m6.1.1.2">𝑜</ci><apply id="S3.SS4.p3.6.m6.1.1.3.cmml" xref="S3.SS4.p3.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p3.6.m6.1.1.3.1.cmml" xref="S3.SS4.p3.6.m6.1.1.3">subscript</csymbol><ci id="S3.SS4.p3.6.m6.1.1.3.2.cmml" xref="S3.SS4.p3.6.m6.1.1.3.2">𝑞</ci><ci id="S3.SS4.p3.6.m6.1.1.3.3.cmml" xref="S3.SS4.p3.6.m6.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.6.m6.1c">o_{q_{i}}</annotation></semantics></math> is the network’s answer for the <math id="S3.SS4.p3.7.m7.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S3.SS4.p3.7.m7.1a"><msup id="S3.SS4.p3.7.m7.1.1" xref="S3.SS4.p3.7.m7.1.1.cmml"><mi id="S3.SS4.p3.7.m7.1.1.2" xref="S3.SS4.p3.7.m7.1.1.2.cmml">i</mi><mrow id="S3.SS4.p3.7.m7.1.1.3" xref="S3.SS4.p3.7.m7.1.1.3.cmml"><mi id="S3.SS4.p3.7.m7.1.1.3.2" xref="S3.SS4.p3.7.m7.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.7.m7.1.1.3.1" xref="S3.SS4.p3.7.m7.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p3.7.m7.1.1.3.3" xref="S3.SS4.p3.7.m7.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.7.m7.1b"><apply id="S3.SS4.p3.7.m7.1.1.cmml" xref="S3.SS4.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.7.m7.1.1.1.cmml" xref="S3.SS4.p3.7.m7.1.1">superscript</csymbol><ci id="S3.SS4.p3.7.m7.1.1.2.cmml" xref="S3.SS4.p3.7.m7.1.1.2">𝑖</ci><apply id="S3.SS4.p3.7.m7.1.1.3.cmml" xref="S3.SS4.p3.7.m7.1.1.3"><times id="S3.SS4.p3.7.m7.1.1.3.1.cmml" xref="S3.SS4.p3.7.m7.1.1.3.1"></times><ci id="S3.SS4.p3.7.m7.1.1.3.2.cmml" xref="S3.SS4.p3.7.m7.1.1.3.2">𝑡</ci><ci id="S3.SS4.p3.7.m7.1.1.3.3.cmml" xref="S3.SS4.p3.7.m7.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.7.m7.1c">i^{th}</annotation></semantics></math> question <math id="S3.SS4.p3.8.m8.1" class="ltx_Math" alttext="q_{i}" display="inline"><semantics id="S3.SS4.p3.8.m8.1a"><msub id="S3.SS4.p3.8.m8.1.1" xref="S3.SS4.p3.8.m8.1.1.cmml"><mi id="S3.SS4.p3.8.m8.1.1.2" xref="S3.SS4.p3.8.m8.1.1.2.cmml">q</mi><mi id="S3.SS4.p3.8.m8.1.1.3" xref="S3.SS4.p3.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.8.m8.1b"><apply id="S3.SS4.p3.8.m8.1.1.cmml" xref="S3.SS4.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.8.m8.1.1.1.cmml" xref="S3.SS4.p3.8.m8.1.1">subscript</csymbol><ci id="S3.SS4.p3.8.m8.1.1.2.cmml" xref="S3.SS4.p3.8.m8.1.1.2">𝑞</ci><ci id="S3.SS4.p3.8.m8.1.1.3.cmml" xref="S3.SS4.p3.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.8.m8.1c">q_{i}</annotation></semantics></math>. <math id="S3.SS4.p3.9.m9.2" class="ltx_Math" alttext="NL(a_{ij},o_{q_{i}})" display="inline"><semantics id="S3.SS4.p3.9.m9.2a"><mrow id="S3.SS4.p3.9.m9.2.2" xref="S3.SS4.p3.9.m9.2.2.cmml"><mi id="S3.SS4.p3.9.m9.2.2.4" xref="S3.SS4.p3.9.m9.2.2.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.9.m9.2.2.3" xref="S3.SS4.p3.9.m9.2.2.3.cmml">​</mo><mi id="S3.SS4.p3.9.m9.2.2.5" xref="S3.SS4.p3.9.m9.2.2.5.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.9.m9.2.2.3a" xref="S3.SS4.p3.9.m9.2.2.3.cmml">​</mo><mrow id="S3.SS4.p3.9.m9.2.2.2.2" xref="S3.SS4.p3.9.m9.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS4.p3.9.m9.2.2.2.2.3" xref="S3.SS4.p3.9.m9.2.2.2.3.cmml">(</mo><msub id="S3.SS4.p3.9.m9.1.1.1.1.1" xref="S3.SS4.p3.9.m9.1.1.1.1.1.cmml"><mi id="S3.SS4.p3.9.m9.1.1.1.1.1.2" xref="S3.SS4.p3.9.m9.1.1.1.1.1.2.cmml">a</mi><mrow id="S3.SS4.p3.9.m9.1.1.1.1.1.3" xref="S3.SS4.p3.9.m9.1.1.1.1.1.3.cmml"><mi id="S3.SS4.p3.9.m9.1.1.1.1.1.3.2" xref="S3.SS4.p3.9.m9.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.9.m9.1.1.1.1.1.3.1" xref="S3.SS4.p3.9.m9.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p3.9.m9.1.1.1.1.1.3.3" xref="S3.SS4.p3.9.m9.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo id="S3.SS4.p3.9.m9.2.2.2.2.4" xref="S3.SS4.p3.9.m9.2.2.2.3.cmml">,</mo><msub id="S3.SS4.p3.9.m9.2.2.2.2.2" xref="S3.SS4.p3.9.m9.2.2.2.2.2.cmml"><mi id="S3.SS4.p3.9.m9.2.2.2.2.2.2" xref="S3.SS4.p3.9.m9.2.2.2.2.2.2.cmml">o</mi><msub id="S3.SS4.p3.9.m9.2.2.2.2.2.3" xref="S3.SS4.p3.9.m9.2.2.2.2.2.3.cmml"><mi id="S3.SS4.p3.9.m9.2.2.2.2.2.3.2" xref="S3.SS4.p3.9.m9.2.2.2.2.2.3.2.cmml">q</mi><mi id="S3.SS4.p3.9.m9.2.2.2.2.2.3.3" xref="S3.SS4.p3.9.m9.2.2.2.2.2.3.3.cmml">i</mi></msub></msub><mo stretchy="false" id="S3.SS4.p3.9.m9.2.2.2.2.5" xref="S3.SS4.p3.9.m9.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.9.m9.2b"><apply id="S3.SS4.p3.9.m9.2.2.cmml" xref="S3.SS4.p3.9.m9.2.2"><times id="S3.SS4.p3.9.m9.2.2.3.cmml" xref="S3.SS4.p3.9.m9.2.2.3"></times><ci id="S3.SS4.p3.9.m9.2.2.4.cmml" xref="S3.SS4.p3.9.m9.2.2.4">𝑁</ci><ci id="S3.SS4.p3.9.m9.2.2.5.cmml" xref="S3.SS4.p3.9.m9.2.2.5">𝐿</ci><interval closure="open" id="S3.SS4.p3.9.m9.2.2.2.3.cmml" xref="S3.SS4.p3.9.m9.2.2.2.2"><apply id="S3.SS4.p3.9.m9.1.1.1.1.1.cmml" xref="S3.SS4.p3.9.m9.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.9.m9.1.1.1.1.1.1.cmml" xref="S3.SS4.p3.9.m9.1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p3.9.m9.1.1.1.1.1.2.cmml" xref="S3.SS4.p3.9.m9.1.1.1.1.1.2">𝑎</ci><apply id="S3.SS4.p3.9.m9.1.1.1.1.1.3.cmml" xref="S3.SS4.p3.9.m9.1.1.1.1.1.3"><times id="S3.SS4.p3.9.m9.1.1.1.1.1.3.1.cmml" xref="S3.SS4.p3.9.m9.1.1.1.1.1.3.1"></times><ci id="S3.SS4.p3.9.m9.1.1.1.1.1.3.2.cmml" xref="S3.SS4.p3.9.m9.1.1.1.1.1.3.2">𝑖</ci><ci id="S3.SS4.p3.9.m9.1.1.1.1.1.3.3.cmml" xref="S3.SS4.p3.9.m9.1.1.1.1.1.3.3">𝑗</ci></apply></apply><apply id="S3.SS4.p3.9.m9.2.2.2.2.2.cmml" xref="S3.SS4.p3.9.m9.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p3.9.m9.2.2.2.2.2.1.cmml" xref="S3.SS4.p3.9.m9.2.2.2.2.2">subscript</csymbol><ci id="S3.SS4.p3.9.m9.2.2.2.2.2.2.cmml" xref="S3.SS4.p3.9.m9.2.2.2.2.2.2">𝑜</ci><apply id="S3.SS4.p3.9.m9.2.2.2.2.2.3.cmml" xref="S3.SS4.p3.9.m9.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS4.p3.9.m9.2.2.2.2.2.3.1.cmml" xref="S3.SS4.p3.9.m9.2.2.2.2.2.3">subscript</csymbol><ci id="S3.SS4.p3.9.m9.2.2.2.2.2.3.2.cmml" xref="S3.SS4.p3.9.m9.2.2.2.2.2.3.2">𝑞</ci><ci id="S3.SS4.p3.9.m9.2.2.2.2.2.3.3.cmml" xref="S3.SS4.p3.9.m9.2.2.2.2.2.3.3">𝑖</ci></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.9.m9.2c">NL(a_{ij},o_{q_{i}})</annotation></semantics></math> is the normalized Levenshtein distance between the strings <math id="S3.SS4.p3.10.m10.1" class="ltx_Math" alttext="a_{ij}" display="inline"><semantics id="S3.SS4.p3.10.m10.1a"><msub id="S3.SS4.p3.10.m10.1.1" xref="S3.SS4.p3.10.m10.1.1.cmml"><mi id="S3.SS4.p3.10.m10.1.1.2" xref="S3.SS4.p3.10.m10.1.1.2.cmml">a</mi><mrow id="S3.SS4.p3.10.m10.1.1.3" xref="S3.SS4.p3.10.m10.1.1.3.cmml"><mi id="S3.SS4.p3.10.m10.1.1.3.2" xref="S3.SS4.p3.10.m10.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.10.m10.1.1.3.1" xref="S3.SS4.p3.10.m10.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p3.10.m10.1.1.3.3" xref="S3.SS4.p3.10.m10.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.10.m10.1b"><apply id="S3.SS4.p3.10.m10.1.1.cmml" xref="S3.SS4.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.10.m10.1.1.1.cmml" xref="S3.SS4.p3.10.m10.1.1">subscript</csymbol><ci id="S3.SS4.p3.10.m10.1.1.2.cmml" xref="S3.SS4.p3.10.m10.1.1.2">𝑎</ci><apply id="S3.SS4.p3.10.m10.1.1.3.cmml" xref="S3.SS4.p3.10.m10.1.1.3"><times id="S3.SS4.p3.10.m10.1.1.3.1.cmml" xref="S3.SS4.p3.10.m10.1.1.3.1"></times><ci id="S3.SS4.p3.10.m10.1.1.3.2.cmml" xref="S3.SS4.p3.10.m10.1.1.3.2">𝑖</ci><ci id="S3.SS4.p3.10.m10.1.1.3.3.cmml" xref="S3.SS4.p3.10.m10.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.10.m10.1c">a_{ij}</annotation></semantics></math> and <math id="S3.SS4.p3.11.m11.1" class="ltx_Math" alttext="o_{q_{i}}" display="inline"><semantics id="S3.SS4.p3.11.m11.1a"><msub id="S3.SS4.p3.11.m11.1.1" xref="S3.SS4.p3.11.m11.1.1.cmml"><mi id="S3.SS4.p3.11.m11.1.1.2" xref="S3.SS4.p3.11.m11.1.1.2.cmml">o</mi><msub id="S3.SS4.p3.11.m11.1.1.3" xref="S3.SS4.p3.11.m11.1.1.3.cmml"><mi id="S3.SS4.p3.11.m11.1.1.3.2" xref="S3.SS4.p3.11.m11.1.1.3.2.cmml">q</mi><mi id="S3.SS4.p3.11.m11.1.1.3.3" xref="S3.SS4.p3.11.m11.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.11.m11.1b"><apply id="S3.SS4.p3.11.m11.1.1.cmml" xref="S3.SS4.p3.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.11.m11.1.1.1.cmml" xref="S3.SS4.p3.11.m11.1.1">subscript</csymbol><ci id="S3.SS4.p3.11.m11.1.1.2.cmml" xref="S3.SS4.p3.11.m11.1.1.2">𝑜</ci><apply id="S3.SS4.p3.11.m11.1.1.3.cmml" xref="S3.SS4.p3.11.m11.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p3.11.m11.1.1.3.1.cmml" xref="S3.SS4.p3.11.m11.1.1.3">subscript</csymbol><ci id="S3.SS4.p3.11.m11.1.1.3.2.cmml" xref="S3.SS4.p3.11.m11.1.1.3.2">𝑞</ci><ci id="S3.SS4.p3.11.m11.1.1.3.3.cmml" xref="S3.SS4.p3.11.m11.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.11.m11.1c">o_{q_{i}}</annotation></semantics></math> (notice that the normalized Levenshtein distance is a value between <math id="S3.SS4.p3.12.m12.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.SS4.p3.12.m12.1a"><mn id="S3.SS4.p3.12.m12.1.1" xref="S3.SS4.p3.12.m12.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.12.m12.1b"><cn type="integer" id="S3.SS4.p3.12.m12.1.1.cmml" xref="S3.SS4.p3.12.m12.1.1">0</cn></annotation-xml></semantics></math> and <math id="S3.SS4.p3.13.m13.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS4.p3.13.m13.1a"><mn id="S3.SS4.p3.13.m13.1.1" xref="S3.SS4.p3.13.m13.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.13.m13.1b"><cn type="integer" id="S3.SS4.p3.13.m13.1.1.cmml" xref="S3.SS4.p3.13.m13.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.13.m13.1c">1</annotation></semantics></math>). We define a threshold <math id="S3.SS4.p3.14.m14.1" class="ltx_Math" alttext="\tau=0.5" display="inline"><semantics id="S3.SS4.p3.14.m14.1a"><mrow id="S3.SS4.p3.14.m14.1.1" xref="S3.SS4.p3.14.m14.1.1.cmml"><mi id="S3.SS4.p3.14.m14.1.1.2" xref="S3.SS4.p3.14.m14.1.1.2.cmml">τ</mi><mo id="S3.SS4.p3.14.m14.1.1.1" xref="S3.SS4.p3.14.m14.1.1.1.cmml">=</mo><mn id="S3.SS4.p3.14.m14.1.1.3" xref="S3.SS4.p3.14.m14.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.14.m14.1b"><apply id="S3.SS4.p3.14.m14.1.1.cmml" xref="S3.SS4.p3.14.m14.1.1"><eq id="S3.SS4.p3.14.m14.1.1.1.cmml" xref="S3.SS4.p3.14.m14.1.1.1"></eq><ci id="S3.SS4.p3.14.m14.1.1.2.cmml" xref="S3.SS4.p3.14.m14.1.1.2">𝜏</ci><cn type="float" id="S3.SS4.p3.14.m14.1.1.3.cmml" xref="S3.SS4.p3.14.m14.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.14.m14.1c">\tau=0.5</annotation></semantics></math> that penalizes metrics larger than this value, thus the final score will be <math id="S3.SS4.p3.15.m15.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.SS4.p3.15.m15.1a"><mn id="S3.SS4.p3.15.m15.1.1" xref="S3.SS4.p3.15.m15.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.15.m15.1b"><cn type="integer" id="S3.SS4.p3.15.m15.1.1.cmml" xref="S3.SS4.p3.15.m15.1.1">0</cn></annotation-xml></semantics></math> if the <math id="S3.SS4.p3.16.m16.1" class="ltx_Math" alttext="NL" display="inline"><semantics id="S3.SS4.p3.16.m16.1a"><mrow id="S3.SS4.p3.16.m16.1.1" xref="S3.SS4.p3.16.m16.1.1.cmml"><mi id="S3.SS4.p3.16.m16.1.1.2" xref="S3.SS4.p3.16.m16.1.1.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.16.m16.1.1.1" xref="S3.SS4.p3.16.m16.1.1.1.cmml">​</mo><mi id="S3.SS4.p3.16.m16.1.1.3" xref="S3.SS4.p3.16.m16.1.1.3.cmml">L</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.16.m16.1b"><apply id="S3.SS4.p3.16.m16.1.1.cmml" xref="S3.SS4.p3.16.m16.1.1"><times id="S3.SS4.p3.16.m16.1.1.1.cmml" xref="S3.SS4.p3.16.m16.1.1.1"></times><ci id="S3.SS4.p3.16.m16.1.1.2.cmml" xref="S3.SS4.p3.16.m16.1.1.2">𝑁</ci><ci id="S3.SS4.p3.16.m16.1.1.3.cmml" xref="S3.SS4.p3.16.m16.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.16.m16.1c">NL</annotation></semantics></math> is larger than <math id="S3.SS4.p3.17.m17.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.SS4.p3.17.m17.1a"><mi id="S3.SS4.p3.17.m17.1.1" xref="S3.SS4.p3.17.m17.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.17.m17.1b"><ci id="S3.SS4.p3.17.m17.1.1.cmml" xref="S3.SS4.p3.17.m17.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.17.m17.1c">\tau</annotation></semantics></math>. The intuition behind the threshold is that if an output has an edit distance of more than <math id="S3.SS4.p3.18.m18.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S3.SS4.p3.18.m18.1a"><mn id="S3.SS4.p3.18.m18.1.1" xref="S3.SS4.p3.18.m18.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.18.m18.1b"><cn type="float" id="S3.SS4.p3.18.m18.1.1.cmml" xref="S3.SS4.p3.18.m18.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.18.m18.1c">0.5</annotation></semantics></math> to an answer, meaning getting half of the answer wrong, we reason that the output is the wrong text selected from the options as an answer. Otherwise, the metric has a smooth response that can gracefully capture errors in text recognition.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">In addition, we provide an online service
where the open challenge was hosted <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, that researchers can use to evaluate their methods against a public validation/test dataset.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Baselines and Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The following section describes the baselines employed in this work as well as an analysis of the results obtained in the experiments conducted.
The proposed baselines help us to showcase the difficulty of the proposed dataset and its tasks. Aside from baselines designed to exploit all the information available (visual information, scene text, and the question), we have purposely included baselines that ignore one or more of the available pieces of information in order to establish lower bounds of performance. The following baselines are employed to evaluate the datasets:</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Random:</span>
As a way of assessing aimless chance, we return a random word from the dictionary provided for each task (see section <a href="#S3.SS3" title="3.3 Tasks ‣ 3 ST-VQA Dataset ‣ Scene Text Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> for more detail).</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Scene Text Retrieval:</span>
This baseline leverages a single shot CNN architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> that predicts at the same time bounding boxes and a Pyramidal Histogram Of Characters (PHOC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. The PHOC is a compact representation of a word that considers the spatial location of each character to construct the resulting encoding. This baseline ignores the question and any other visual information of the image.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">We have defined two approaches: the first (“STR retrieval”) uses the specific task dictionaries as queries to a given image, and the top-1 retrieved word is returned as the answer; the second one (“STR bbox”), follows the intuition that humans tend to formulate questions about the largest text instance in the image. We take the text representation from the biggest bounding box found and then find the nearest neighbor word in the corresponding dictionaries.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Scene Image OCR:</span>
A state of the art text recognition model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> is used to process the test set images. The detected text is ranked according to the confidence score and the closest match between the most confident text detection and the provided vocabularies for task 1 and task 2 is used as the answer. In task 3 the most confident text detection is adopted as the answer directly.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt"></th>
<th id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Task 1</th>
<th id="S4.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Task 2</th>
<th id="S4.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Task 3</th>
<th id="S4.T2.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Upper bound</th>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<th id="S4.T2.1.2.2.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_r">
<span id="S4.T2.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.2.2.1.1.1" class="ltx_p">Method with</span>
</span>
</th>
<th id="S4.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">OCR</th>
<th id="S4.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Q</th>
<th id="S4.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">V</th>
<th id="S4.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">ANLS</th>
<th id="S4.T2.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">Acc.</th>
<th id="S4.T2.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column">ANLS</th>
<th id="S4.T2.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column">Acc.</th>
<th id="S4.T2.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column">ANLS</th>
<th id="S4.T2.1.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column">Acc.</th>
<th id="S4.T2.1.2.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column">ANLS</th>
<th id="S4.T2.1.2.2.12" class="ltx_td ltx_align_center ltx_th ltx_th_column">Acc.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.3.1" class="ltx_tr">
<th id="S4.T2.1.3.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S4.T2.1.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.3.1.1.1.1" class="ltx_p">Random</span>
</span>
</th>
<td id="S4.T2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.3.1.2.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S4.T2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.3.1.3.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S4.T2.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.3.1.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S4.T2.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">0.015</td>
<td id="S4.T2.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">0.96</td>
<td id="S4.T2.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">0.001</td>
<td id="S4.T2.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">0.00</td>
<td id="S4.T2.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t">0.00</td>
<td id="S4.T2.1.3.1.10" class="ltx_td ltx_align_center ltx_border_t">0.00</td>
<td id="S4.T2.1.3.1.11" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.1.3.1.12" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T2.1.4.2" class="ltx_tr">
<th id="S4.T2.1.4.2.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.4.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.4.2.1.1.1" class="ltx_p">STR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> (retrieval)</span>
</span>
</th>
<td id="S4.T2.1.4.2.2" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T2.1.4.2.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.4.2.3.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S4.T2.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.4.2.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S4.T2.1.4.2.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.4.2.5.1" class="ltx_text ltx_font_bold">0.171</span></td>
<td id="S4.T2.1.4.2.6" class="ltx_td ltx_align_center"><span id="S4.T2.1.4.2.6.1" class="ltx_text ltx_font_bold">13.78</span></td>
<td id="S4.T2.1.4.2.7" class="ltx_td ltx_align_center">0.073</td>
<td id="S4.T2.1.4.2.8" class="ltx_td ltx_align_center">5.55</td>
<td id="S4.T2.1.4.2.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.4.2.10" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.4.2.11" class="ltx_td ltx_align_center">0.782</td>
<td id="S4.T2.1.4.2.12" class="ltx_td ltx_align_center">68.84</td>
</tr>
<tr id="S4.T2.1.5.3" class="ltx_tr">
<th id="S4.T2.1.5.3.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.5.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.5.3.1.1.1" class="ltx_p">STR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> (bbox)</span>
</span>
</th>
<td id="S4.T2.1.5.3.2" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T2.1.5.3.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.3.3.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S4.T2.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.5.3.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S4.T2.1.5.3.5" class="ltx_td ltx_align_center">0.130</td>
<td id="S4.T2.1.5.3.6" class="ltx_td ltx_align_center">7.32</td>
<td id="S4.T2.1.5.3.7" class="ltx_td ltx_align_center">0.118</td>
<td id="S4.T2.1.5.3.8" class="ltx_td ltx_align_center">6.89</td>
<td id="S4.T2.1.5.3.9" class="ltx_td ltx_align_center">0.128</td>
<td id="S4.T2.1.5.3.10" class="ltx_td ltx_align_center">7.21</td>
<td id="S4.T2.1.5.3.11" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.5.3.12" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T2.1.6.4" class="ltx_tr">
<th id="S4.T2.1.6.4.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.6.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.6.4.1.1.1" class="ltx_p">Scene Image OCR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite></span>
</span>
</th>
<td id="S4.T2.1.6.4.2" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T2.1.6.4.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.6.4.3.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S4.T2.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.6.4.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S4.T2.1.6.4.5" class="ltx_td ltx_align_center">0.145</td>
<td id="S4.T2.1.6.4.6" class="ltx_td ltx_align_center">8.89</td>
<td id="S4.T2.1.6.4.7" class="ltx_td ltx_align_center">0.132</td>
<td id="S4.T2.1.6.4.8" class="ltx_td ltx_align_center">8.69</td>
<td id="S4.T2.1.6.4.9" class="ltx_td ltx_align_center"><span id="S4.T2.1.6.4.9.1" class="ltx_text ltx_font_bold">0.140</span></td>
<td id="S4.T2.1.6.4.10" class="ltx_td ltx_align_center">8.60</td>
<td id="S4.T2.1.6.4.11" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.6.4.12" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T2.1.7.5" class="ltx_tr">
<th id="S4.T2.1.7.5.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.7.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.7.5.1.1.1" class="ltx_p">SAAA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> (1k cls)</span>
</span>
</th>
<td id="S4.T2.1.7.5.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.7.5.2.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S4.T2.1.7.5.3" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T2.1.7.5.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T2.1.7.5.5" class="ltx_td ltx_align_center">0.085</td>
<td id="S4.T2.1.7.5.6" class="ltx_td ltx_align_center">6.36</td>
<td id="S4.T2.1.7.5.7" class="ltx_td ltx_align_center">0.085</td>
<td id="S4.T2.1.7.5.8" class="ltx_td ltx_align_center">6.36</td>
<td id="S4.T2.1.7.5.9" class="ltx_td ltx_align_center">0.085</td>
<td id="S4.T2.1.7.5.10" class="ltx_td ltx_align_center">6.36</td>
<td id="S4.T2.1.7.5.11" class="ltx_td ltx_align_center">0.571</td>
<td id="S4.T2.1.7.5.12" class="ltx_td ltx_align_center">31.96</td>
</tr>
<tr id="S4.T2.1.8.6" class="ltx_tr">
<th id="S4.T2.1.8.6.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.8.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.8.6.1.1.1" class="ltx_p">SAAA+STR (1k cls)</span>
</span>
</th>
<td id="S4.T2.1.8.6.2" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T2.1.8.6.3" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T2.1.8.6.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T2.1.8.6.5" class="ltx_td ltx_align_center">0.091</td>
<td id="S4.T2.1.8.6.6" class="ltx_td ltx_align_center">6.66</td>
<td id="S4.T2.1.8.6.7" class="ltx_td ltx_align_center">0.091</td>
<td id="S4.T2.1.8.6.8" class="ltx_td ltx_align_center">6.66</td>
<td id="S4.T2.1.8.6.9" class="ltx_td ltx_align_center">0.091</td>
<td id="S4.T2.1.8.6.10" class="ltx_td ltx_align_center">6.66</td>
<td id="S4.T2.1.8.6.11" class="ltx_td ltx_align_center">0.571</td>
<td id="S4.T2.1.8.6.12" class="ltx_td ltx_align_center">31.96</td>
</tr>
<tr id="S4.T2.1.9.7" class="ltx_tr">
<th id="S4.T2.1.9.7.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.9.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.9.7.1.1.1" class="ltx_p">SAAA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> (5k cls)</span>
</span>
</th>
<td id="S4.T2.1.9.7.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.9.7.2.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S4.T2.1.9.7.3" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T2.1.9.7.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T2.1.9.7.5" class="ltx_td ltx_align_center">0.087</td>
<td id="S4.T2.1.9.7.6" class="ltx_td ltx_align_center">6.66</td>
<td id="S4.T2.1.9.7.7" class="ltx_td ltx_align_center">0.087</td>
<td id="S4.T2.1.9.7.8" class="ltx_td ltx_align_center">6.66</td>
<td id="S4.T2.1.9.7.9" class="ltx_td ltx_align_center">0.087</td>
<td id="S4.T2.1.9.7.10" class="ltx_td ltx_align_center">6.66</td>
<td id="S4.T2.1.9.7.11" class="ltx_td ltx_align_center">0.740</td>
<td id="S4.T2.1.9.7.12" class="ltx_td ltx_align_center">41.03</td>
</tr>
<tr id="S4.T2.1.10.8" class="ltx_tr">
<th id="S4.T2.1.10.8.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.10.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.10.8.1.1.1" class="ltx_p">SAAA+STR (5k cls)</span>
</span>
</th>
<td id="S4.T2.1.10.8.2" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T2.1.10.8.3" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T2.1.10.8.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T2.1.10.8.5" class="ltx_td ltx_align_center">0.096</td>
<td id="S4.T2.1.10.8.6" class="ltx_td ltx_align_center">7.41</td>
<td id="S4.T2.1.10.8.7" class="ltx_td ltx_align_center">0.096</td>
<td id="S4.T2.1.10.8.8" class="ltx_td ltx_align_center">7.41</td>
<td id="S4.T2.1.10.8.9" class="ltx_td ltx_align_center">0.096</td>
<td id="S4.T2.1.10.8.10" class="ltx_td ltx_align_center">7.41</td>
<td id="S4.T2.1.10.8.11" class="ltx_td ltx_align_center">0.740</td>
<td id="S4.T2.1.10.8.12" class="ltx_td ltx_align_center">41.03</td>
</tr>
<tr id="S4.T2.1.11.9" class="ltx_tr">
<th id="S4.T2.1.11.9.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.11.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.11.9.1.1.1" class="ltx_p">SAAA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> (19k cls)</span>
</span>
</th>
<td id="S4.T2.1.11.9.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.11.9.2.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S4.T2.1.11.9.3" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T2.1.11.9.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T2.1.11.9.5" class="ltx_td ltx_align_center">0.084</td>
<td id="S4.T2.1.11.9.6" class="ltx_td ltx_align_center">6.13</td>
<td id="S4.T2.1.11.9.7" class="ltx_td ltx_align_center">0.084</td>
<td id="S4.T2.1.11.9.8" class="ltx_td ltx_align_center">6.13</td>
<td id="S4.T2.1.11.9.9" class="ltx_td ltx_align_center">0.084</td>
<td id="S4.T2.1.11.9.10" class="ltx_td ltx_align_center">6.13</td>
<td id="S4.T2.1.11.9.11" class="ltx_td ltx_align_center">0.862</td>
<td id="S4.T2.1.11.9.12" class="ltx_td ltx_align_center">52.31</td>
</tr>
<tr id="S4.T2.1.12.10" class="ltx_tr">
<th id="S4.T2.1.12.10.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.12.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.12.10.1.1.1" class="ltx_p">SAAA+STR (19k cls)</span>
</span>
</th>
<td id="S4.T2.1.12.10.2" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T2.1.12.10.3" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T2.1.12.10.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T2.1.12.10.5" class="ltx_td ltx_align_center">0.087</td>
<td id="S4.T2.1.12.10.6" class="ltx_td ltx_align_center">6.36</td>
<td id="S4.T2.1.12.10.7" class="ltx_td ltx_align_center">0.087</td>
<td id="S4.T2.1.12.10.8" class="ltx_td ltx_align_center">6.36</td>
<td id="S4.T2.1.12.10.9" class="ltx_td ltx_align_center">0.087</td>
<td id="S4.T2.1.12.10.10" class="ltx_td ltx_align_center">6.36</td>
<td id="S4.T2.1.12.10.11" class="ltx_td ltx_align_center">0.862</td>
<td id="S4.T2.1.12.10.12" class="ltx_td ltx_align_center">52.31</td>
</tr>
<tr id="S4.T2.1.13.11" class="ltx_tr">
<th id="S4.T2.1.13.11.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.13.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.13.11.1.1.1" class="ltx_p">QA+STR (19k cls)</span>
</span>
</th>
<td id="S4.T2.1.13.11.2" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T2.1.13.11.3" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T2.1.13.11.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.13.11.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S4.T2.1.13.11.5" class="ltx_td ltx_align_center">0.069</td>
<td id="S4.T2.1.13.11.6" class="ltx_td ltx_align_center">4.65</td>
<td id="S4.T2.1.13.11.7" class="ltx_td ltx_align_center">0.069</td>
<td id="S4.T2.1.13.11.8" class="ltx_td ltx_align_center">4.65</td>
<td id="S4.T2.1.13.11.9" class="ltx_td ltx_align_center">0.069</td>
<td id="S4.T2.1.13.11.10" class="ltx_td ltx_align_center">4.65</td>
<td id="S4.T2.1.13.11.11" class="ltx_td ltx_align_center">0.862</td>
<td id="S4.T2.1.13.11.12" class="ltx_td ltx_align_center">52.31</td>
</tr>
<tr id="S4.T2.1.14.12" class="ltx_tr">
<th id="S4.T2.1.14.12.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.14.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.14.12.1.1.1" class="ltx_p">SAN(LSTM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> (5k cls)</span>
</span>
</th>
<td id="S4.T2.1.14.12.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.14.12.2.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S4.T2.1.14.12.3" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T2.1.14.12.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T2.1.14.12.5" class="ltx_td ltx_align_center">0.102</td>
<td id="S4.T2.1.14.12.6" class="ltx_td ltx_align_center">7.78</td>
<td id="S4.T2.1.14.12.7" class="ltx_td ltx_align_center">0.102</td>
<td id="S4.T2.1.14.12.8" class="ltx_td ltx_align_center">7.78</td>
<td id="S4.T2.1.14.12.9" class="ltx_td ltx_align_center">0.102</td>
<td id="S4.T2.1.14.12.10" class="ltx_td ltx_align_center">7.78</td>
<td id="S4.T2.1.14.12.11" class="ltx_td ltx_align_center">0.740</td>
<td id="S4.T2.1.14.12.12" class="ltx_td ltx_align_center">41.03</td>
</tr>
<tr id="S4.T2.1.15.13" class="ltx_tr">
<th id="S4.T2.1.15.13.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.15.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.15.13.1.1.1" class="ltx_p">SAN(LSTM)+STR (5k cls)</span>
</span>
</th>
<td id="S4.T2.1.15.13.2" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T2.1.15.13.3" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T2.1.15.13.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T2.1.15.13.5" class="ltx_td ltx_align_center">0.136</td>
<td id="S4.T2.1.15.13.6" class="ltx_td ltx_align_center">10.34</td>
<td id="S4.T2.1.15.13.7" class="ltx_td ltx_align_center"><span id="S4.T2.1.15.13.7.1" class="ltx_text ltx_font_bold">0.136</span></td>
<td id="S4.T2.1.15.13.8" class="ltx_td ltx_align_center"><span id="S4.T2.1.15.13.8.1" class="ltx_text ltx_font_bold">10.34</span></td>
<td id="S4.T2.1.15.13.9" class="ltx_td ltx_align_center">0.136</td>
<td id="S4.T2.1.15.13.10" class="ltx_td ltx_align_center"><span id="S4.T2.1.15.13.10.1" class="ltx_text ltx_font_bold">10.34</span></td>
<td id="S4.T2.1.15.13.11" class="ltx_td ltx_align_center">0.740</td>
<td id="S4.T2.1.15.13.12" class="ltx_td ltx_align_center">41.03</td>
</tr>
<tr id="S4.T2.1.16.14" class="ltx_tr">
<th id="S4.T2.1.16.14.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_bb ltx_border_r">
<span id="S4.T2.1.16.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.16.14.1.1.1" class="ltx_p">SAN(CNN)+STR (5k cls)</span>
</span>
</th>
<td id="S4.T2.1.16.14.2" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S4.T2.1.16.14.3" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S4.T2.1.16.14.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">✓</td>
<td id="S4.T2.1.16.14.5" class="ltx_td ltx_align_center ltx_border_bb">0.135</td>
<td id="S4.T2.1.16.14.6" class="ltx_td ltx_align_center ltx_border_bb">10.46</td>
<td id="S4.T2.1.16.14.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.16.14.7.1" class="ltx_text ltx_font_bold">0.135</span></td>
<td id="S4.T2.1.16.14.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.16.14.8.1" class="ltx_text ltx_font_bold">10.46</span></td>
<td id="S4.T2.1.16.14.9" class="ltx_td ltx_align_center ltx_border_bb">0.135</td>
<td id="S4.T2.1.16.14.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.16.14.10.1" class="ltx_text ltx_font_bold">10.46</span></td>
<td id="S4.T2.1.16.14.11" class="ltx_td ltx_align_center ltx_border_bb">0.740</td>
<td id="S4.T2.1.16.14.12" class="ltx_td ltx_align_center ltx_border_bb">41.03</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Baseline results comparison on the three tasks of ST-VQA dataset. We provide Average Normalized Levenshtein similarity (ANLS) and Accuracy for different methods that leverage OCR, Question (Q) and Visual (V) information.</figcaption>
</figure>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.4" class="ltx_p"><span id="S4.p6.4.1" class="ltx_text ltx_font_bold">Standard VQA models:</span>
We evaluate two standard VQA models. The first one, named “Show, Ask, Attend and Answer” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> (SAAA),
consists of a CNN-LSTM architecture. On one hand, a ResNet-152 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> is used to extract image features with dimension <math id="S4.p6.1.m1.1" class="ltx_Math" alttext="14\times 14\times 2048" display="inline"><semantics id="S4.p6.1.m1.1a"><mrow id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml"><mn id="S4.p6.1.m1.1.1.2" xref="S4.p6.1.m1.1.1.2.cmml">14</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p6.1.m1.1.1.1" xref="S4.p6.1.m1.1.1.1.cmml">×</mo><mn id="S4.p6.1.m1.1.1.3" xref="S4.p6.1.m1.1.1.3.cmml">14</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p6.1.m1.1.1.1a" xref="S4.p6.1.m1.1.1.1.cmml">×</mo><mn id="S4.p6.1.m1.1.1.4" xref="S4.p6.1.m1.1.1.4.cmml">2048</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.1b"><apply id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1"><times id="S4.p6.1.m1.1.1.1.cmml" xref="S4.p6.1.m1.1.1.1"></times><cn type="integer" id="S4.p6.1.m1.1.1.2.cmml" xref="S4.p6.1.m1.1.1.2">14</cn><cn type="integer" id="S4.p6.1.m1.1.1.3.cmml" xref="S4.p6.1.m1.1.1.3">14</cn><cn type="integer" id="S4.p6.1.m1.1.1.4.cmml" xref="S4.p6.1.m1.1.1.4">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">14\times 14\times 2048</annotation></semantics></math>, while the question is tokenized and embedded by using a multi-layer LSTM. On top of the combination of image features and the question embedding, multiple attention maps (glimpses) are obtained. The result of the attention glimpses over the image features and the last state of the LSTM is concatenated and fed into two fully connected layers to obtain the distribution of answer probabilities according to the classes. We optimize the model with the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> with a batch size of <math id="S4.p6.2.m2.1" class="ltx_Math" alttext="128" display="inline"><semantics id="S4.p6.2.m2.1a"><mn id="S4.p6.2.m2.1.1" xref="S4.p6.2.m2.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S4.p6.2.m2.1b"><cn type="integer" id="S4.p6.2.m2.1.1.cmml" xref="S4.p6.2.m2.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.2.m2.1c">128</annotation></semantics></math> for <math id="S4.p6.3.m3.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S4.p6.3.m3.1a"><mn id="S4.p6.3.m3.1.1" xref="S4.p6.3.m3.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S4.p6.3.m3.1b"><cn type="integer" id="S4.p6.3.m3.1.1.cmml" xref="S4.p6.3.m3.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.3.m3.1c">30</annotation></semantics></math> epochs. The starting learning rate is <math id="S4.p6.4.m4.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S4.p6.4.m4.1a"><mn id="S4.p6.4.m4.1.1" xref="S4.p6.4.m4.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S4.p6.4.m4.1b"><cn type="float" id="S4.p6.4.m4.1.1.cmml" xref="S4.p6.4.m4.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.4.m4.1c">0.001</annotation></semantics></math> which decays by half every 50<span id="S4.p6.4.2" class="ltx_text ltx_font_italic">K</span> iterations.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.5" class="ltx_p">The second model, named “Stacked Attention Networks” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> (SAN), uses a pre-trained VGGN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> CNN to obtain image features with shape <math id="S4.p7.1.m1.1" class="ltx_Math" alttext="14\times 14\times 512" display="inline"><semantics id="S4.p7.1.m1.1a"><mrow id="S4.p7.1.m1.1.1" xref="S4.p7.1.m1.1.1.cmml"><mn id="S4.p7.1.m1.1.1.2" xref="S4.p7.1.m1.1.1.2.cmml">14</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p7.1.m1.1.1.1" xref="S4.p7.1.m1.1.1.1.cmml">×</mo><mn id="S4.p7.1.m1.1.1.3" xref="S4.p7.1.m1.1.1.3.cmml">14</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p7.1.m1.1.1.1a" xref="S4.p7.1.m1.1.1.1.cmml">×</mo><mn id="S4.p7.1.m1.1.1.4" xref="S4.p7.1.m1.1.1.4.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p7.1.m1.1b"><apply id="S4.p7.1.m1.1.1.cmml" xref="S4.p7.1.m1.1.1"><times id="S4.p7.1.m1.1.1.1.cmml" xref="S4.p7.1.m1.1.1.1"></times><cn type="integer" id="S4.p7.1.m1.1.1.2.cmml" xref="S4.p7.1.m1.1.1.2">14</cn><cn type="integer" id="S4.p7.1.m1.1.1.3.cmml" xref="S4.p7.1.m1.1.1.3">14</cn><cn type="integer" id="S4.p7.1.m1.1.1.4.cmml" xref="S4.p7.1.m1.1.1.4">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.1.m1.1c">14\times 14\times 512</annotation></semantics></math>. Two question encoding methods are proposed, one that uses an LSTM and another that uses a CNN, both of them yielding similar results according to the evaluated dataset. The encoded question either by a CNN or LSTM is used along with the image features to compute two attention maps, which later are used with the image features to output a classification vector. We optimize the model with a batch size of <math id="S4.p7.2.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.p7.2.m2.1a"><mn id="S4.p7.2.m2.1.1" xref="S4.p7.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.p7.2.m2.1b"><cn type="integer" id="S4.p7.2.m2.1.1.cmml" xref="S4.p7.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.2.m2.1c">100</annotation></semantics></math> for <math id="S4.p7.3.m3.1" class="ltx_Math" alttext="150" display="inline"><semantics id="S4.p7.3.m3.1a"><mn id="S4.p7.3.m3.1.1" xref="S4.p7.3.m3.1.1.cmml">150</mn><annotation-xml encoding="MathML-Content" id="S4.p7.3.m3.1b"><cn type="integer" id="S4.p7.3.m3.1.1.cmml" xref="S4.p7.3.m3.1.1">150</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.3.m3.1c">150</annotation></semantics></math> epochs. The optimizer used is RMSProp with a starting learning rate of <math id="S4.p7.4.m4.1" class="ltx_Math" alttext="0.0003" display="inline"><semantics id="S4.p7.4.m4.1a"><mn id="S4.p7.4.m4.1.1" xref="S4.p7.4.m4.1.1.cmml">0.0003</mn><annotation-xml encoding="MathML-Content" id="S4.p7.4.m4.1b"><cn type="float" id="S4.p7.4.m4.1.1.cmml" xref="S4.p7.4.m4.1.1">0.0003</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.4.m4.1c">0.0003</annotation></semantics></math> and a decay value of <math id="S4.p7.5.m5.1" class="ltx_Math" alttext="0.9999" display="inline"><semantics id="S4.p7.5.m5.1a"><mn id="S4.p7.5.m5.1.1" xref="S4.p7.5.m5.1.1.cmml">0.9999</mn><annotation-xml encoding="MathML-Content" id="S4.p7.5.m5.1b"><cn type="float" id="S4.p7.5.m5.1.1.cmml" xref="S4.p7.5.m5.1.1">0.9999</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.5.m5.1c">0.9999</annotation></semantics></math>.</p>
</div>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.1" class="ltx_p">Overall, three different experiments are proposed according to the output classification vector. The first, is formed by selecting the most common 1k answer strings in the ST-VQA training set as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. For the second one, we selected the 5k most common answers so that we can see the effect of a gradual increase of the output vector in the two VQA models. In the third one, all the answers found in the training set are used (<math id="S4.p8.1.m1.2" class="ltx_Math" alttext="19,296" display="inline"><semantics id="S4.p8.1.m1.2a"><mrow id="S4.p8.1.m1.2.3.2" xref="S4.p8.1.m1.2.3.1.cmml"><mn id="S4.p8.1.m1.1.1" xref="S4.p8.1.m1.1.1.cmml">19</mn><mo id="S4.p8.1.m1.2.3.2.1" xref="S4.p8.1.m1.2.3.1.cmml">,</mo><mn id="S4.p8.1.m1.2.2" xref="S4.p8.1.m1.2.2.cmml">296</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p8.1.m1.2b"><list id="S4.p8.1.m1.2.3.1.cmml" xref="S4.p8.1.m1.2.3.2"><cn type="integer" id="S4.p8.1.m1.1.1.cmml" xref="S4.p8.1.m1.1.1">19</cn><cn type="integer" id="S4.p8.1.m1.2.2.cmml" xref="S4.p8.1.m1.2.2">296</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.1.m1.2c">19,296</annotation></semantics></math>) to replicate the wide range vocabulary of scene-text images and to capture all the answers found in the training set.</p>
</div>
<div id="S4.p9" class="ltx_para">
<p id="S4.p9.1" class="ltx_p"><span id="S4.p9.1.1" class="ltx_text ltx_font_bold">Fusing Modalities - Standard VQA Models + Scene Text Retrieval:</span>
Using the previously described VQA models, the purpose of this baseline is to combine textual features obtained from a scene text retrieval model with existing VQA pipelines. To achieve this, we use the model from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and we employ the output tensor before the non-maximal suppression step (NMS) is performed. The most confident PHOC predictions above a threshold are selected relative to a single grid cell. The selected features form a tensor of size <math id="S4.p9.1.m1.1" class="ltx_Math" alttext="14\times 14\times 609" display="inline"><semantics id="S4.p9.1.m1.1a"><mrow id="S4.p9.1.m1.1.1" xref="S4.p9.1.m1.1.1.cmml"><mn id="S4.p9.1.m1.1.1.2" xref="S4.p9.1.m1.1.1.2.cmml">14</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p9.1.m1.1.1.1" xref="S4.p9.1.m1.1.1.1.cmml">×</mo><mn id="S4.p9.1.m1.1.1.3" xref="S4.p9.1.m1.1.1.3.cmml">14</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p9.1.m1.1.1.1a" xref="S4.p9.1.m1.1.1.1.cmml">×</mo><mn id="S4.p9.1.m1.1.1.4" xref="S4.p9.1.m1.1.1.4.cmml">609</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p9.1.m1.1b"><apply id="S4.p9.1.m1.1.1.cmml" xref="S4.p9.1.m1.1.1"><times id="S4.p9.1.m1.1.1.1.cmml" xref="S4.p9.1.m1.1.1.1"></times><cn type="integer" id="S4.p9.1.m1.1.1.2.cmml" xref="S4.p9.1.m1.1.1.2">14</cn><cn type="integer" id="S4.p9.1.m1.1.1.3.cmml" xref="S4.p9.1.m1.1.1.3">14</cn><cn type="integer" id="S4.p9.1.m1.1.1.4.cmml" xref="S4.p9.1.m1.1.1.4">609</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.1.m1.1c">14\times 14\times 609</annotation></semantics></math>, which is concatenated with the image features before the attention maps are calculated on both previously described VQA baselines. Afterwards the attended features are used to output a probability distribution over the classification vector. The models are optimized using the same strategy described before.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/1905.13648/assets/x4.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="143" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Results of baseline methods in the open vocabulary task of ST-VQA by question type.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Results</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The results of all provided baselines according to the defined tasks are summarized in <a href="#S4.T2" title="Table 2 ‣ 4 Baselines and Results ‣ Scene Text Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 2</span></a>. As a way to compare the proposed Average Normalized Levenshtein Similarity (ANLS) metric, we also calculate the accuracy for each baseline. The accuracy is calculated by counting the exact matches between the model predictions and collected answers as is the standard practice in the VQA literature.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The last column in <a href="#S4.T2" title="Table 2 ‣ 4 Baselines and Results ‣ Scene Text Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 2</span></a>, upper bound, shows the maximum possible score that can be achieved depending on the method evaluated.
The upper bound accuracy for standard VQA models is the percentage of questions where the correct answer is part of the models’ output vocabulary, while the upper bound ANLS is calculated by taking as answer the closest word (output class) in terms of Levenshtein distance to the correct answer.
In the case of the Scene Text Retrieval (STR retrieval) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> model the upper bound is calculated by assuming that the correct answer is a single word and that this word is retrieved by the model as the top-1 among all the words in the provided vocabularies.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">In <a href="#S4.T2" title="Table 2 ‣ 4 Baselines and Results ‣ Scene Text Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 2</span></a> we appreciate that standard VQA models that disregard textual information from the image achieve similar scores, ranging between 0.085 to 0.102 ANLS, or 6.36% to 7.78% accuracy. One relevant point is that although in VQA v1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> the SAAA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> model is known to outperform SAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, in our dataset the effect found is the opposite, due to the fact that our dataset and task outline is different in its nature compared to VQA v1.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Another important point is that the SAAA model increases both its accuracy and ANLS score when using a larger classification vector size, from 1k to 5k classes; however, going from 5k to 19k classes the results are worse, suggesting that learning such a big vocabulary in a classification manner is not feasible.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">It is worth noting that the proposed ANLS metric generally tracks accuracy, which
indicates broad compatibility between the metrics.
But, in addition,
ANLS can deal with border cases (i.e. correct intended responses, but slightly wrong recognized text) where accuracy, being a hard metric based on exact matches, cannot. Such border cases are frequent due to errors at the text recognition stage. Examples of such behaviour can be seen in the qualitative results shown in <a href="#S4.F6" title="Figure 6 ‣ 4.1 Results ‣ 4 Baselines and Results ‣ Scene Text Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 6</span></a> for some of the answers (indicated in orange color).
This also explains why
the “Scene Image OCR” model
is better ranked in terms of ANLS than of accuracy in <a href="#S4.T2" title="Table 2 ‣ 4 Baselines and Results ‣ Scene Text Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p">Finally, we notice that standard VQA models, disregarding any textual information, perform worse or comparable at best to the “STR (retrieval)” or “Scene Image OCR” models, despite the fact that these heuristic methods do not take into account the question. This observation confirms the necessity of leveraging textual information as a way to improve performance in VQA models. We demonstrate this effect by slightly improving the results of VQA models (SAAA and SAN) by using a combination of visual features and PHOC-based textual features (see SAAA+STR and SAN+STR baselines descriptions for details).</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<table id="S4.F6.8.8" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F6.4.4.4" class="ltx_tr">
<td id="S4.F6.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F6.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F6.1.1.1.1.1.1" class="ltx_p" style="width:99.7pt;"><img src="/html/1905.13648/assets/2318631.jpg" id="S4.F6.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F6.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F6.2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F6.2.2.2.2.1.1" class="ltx_p" style="width:99.7pt;"><img src="/html/1905.13648/assets/img_000496.jpg" id="S4.F6.2.2.2.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F6.3.3.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F6.3.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F6.3.3.3.3.1.1" class="ltx_p" style="width:99.7pt;"><img src="/html/1905.13648/assets/x5.jpeg" id="S4.F6.3.3.3.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="664" height="498" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F6.4.4.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F6.4.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F6.4.4.4.4.1.1" class="ltx_p" style="width:99.7pt;"><img src="/html/1905.13648/assets/COCO_train2014_000000518855.jpg" id="S4.F6.4.4.4.4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption"></span>
</span>
</td>
</tr>
<tr id="S4.F6.8.8.9.1" class="ltx_tr">
<td id="S4.F6.8.8.9.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F6.8.8.9.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F6.8.8.9.1.1.1.1" class="ltx_p" style="width:99.7pt;"><span id="S4.F6.8.8.9.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q:<span id="S4.F6.8.8.9.1.1.1.1.1.1" class="ltx_text ltx_font_medium"> What brand are the machines?</span></span></span>
<span id="S4.F6.8.8.9.1.1.1.2" class="ltx_p"><span id="S4.F6.8.8.9.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">A:<span id="S4.F6.8.8.9.1.1.1.2.1.1" class="ltx_text ltx_font_medium"> bongard<span id="S4.F6.8.8.9.1.1.1.2.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.9.1.1.1.3" class="ltx_p"><span id="S4.F6.8.8.9.1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">SAN(CNN)+STR:<span id="S4.F6.8.8.9.1.1.1.3.1.1" class="ltx_text ltx_font_medium"> ray<span id="S4.F6.8.8.9.1.1.1.3.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.9.1.1.1.4" class="ltx_p"><span id="S4.F6.8.8.9.1.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">SAAA+STR:<span id="S4.F6.8.8.9.1.1.1.4.1.1" class="ltx_text ltx_font_medium"> ray<span id="S4.F6.8.8.9.1.1.1.4.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.9.1.1.1.5" class="ltx_p"><span id="S4.F6.8.8.9.1.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF8000;">Scene Image OCR:<span id="S4.F6.8.8.9.1.1.1.5.1.1" class="ltx_text ltx_font_medium"> zbongard<span id="S4.F6.8.8.9.1.1.1.5.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.9.1.1.1.6" class="ltx_p"><span id="S4.F6.8.8.9.1.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">STR (bbox):<span id="S4.F6.8.8.9.1.1.1.6.1.1" class="ltx_text ltx_font_medium"> 1</span></span></span>
</span>
</td>
<td id="S4.F6.8.8.9.1.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F6.8.8.9.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F6.8.8.9.1.2.1.1" class="ltx_p" style="width:99.7pt;"><span id="S4.F6.8.8.9.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q:<span id="S4.F6.8.8.9.1.2.1.1.1.1" class="ltx_text ltx_font_medium"> Where is the high court located?</span></span></span>
<span id="S4.F6.8.8.9.1.2.1.2" class="ltx_p"><span id="S4.F6.8.8.9.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">A:<span id="S4.F6.8.8.9.1.2.1.2.1.1" class="ltx_text ltx_font_medium"> delhi<span id="S4.F6.8.8.9.1.2.1.2.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.9.1.2.1.3" class="ltx_p"><span id="S4.F6.8.8.9.1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#00FF00;">SAN(CNN)+STR:<span id="S4.F6.8.8.9.1.2.1.3.1.1" class="ltx_text ltx_font_medium"> delhi</span></span></span>
<span id="S4.F6.8.8.9.1.2.1.4" class="ltx_p"><span id="S4.F6.8.8.9.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#00FF00;">SAAA+STR:<span id="S4.F6.8.8.9.1.2.1.4.1.1" class="ltx_text ltx_font_medium"> delhi<span id="S4.F6.8.8.9.1.2.1.4.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.9.1.2.1.5" class="ltx_p"><span id="S4.F6.8.8.9.1.2.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">Scene Image OCR:<span id="S4.F6.8.8.9.1.2.1.5.1.1" class="ltx_text ltx_font_medium"> high<span id="S4.F6.8.8.9.1.2.1.5.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.9.1.2.1.6" class="ltx_p"><span id="S4.F6.8.8.9.1.2.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#00FF00;">STR (bbox):<span id="S4.F6.8.8.9.1.2.1.6.1.1" class="ltx_text ltx_font_medium"> delhi</span></span></span>
</span>
</td>
<td id="S4.F6.8.8.9.1.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F6.8.8.9.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F6.8.8.9.1.3.1.1" class="ltx_p" style="width:99.7pt;"><span id="S4.F6.8.8.9.1.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q:<span id="S4.F6.8.8.9.1.3.1.1.1.1" class="ltx_text ltx_font_medium"> What does the black label say?</span></span></span>
<span id="S4.F6.8.8.9.1.3.1.2" class="ltx_p"><span id="S4.F6.8.8.9.1.3.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">A:<span id="S4.F6.8.8.9.1.3.1.2.1.1" class="ltx_text ltx_font_medium"> GemOro<span id="S4.F6.8.8.9.1.3.1.2.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.9.1.3.1.3" class="ltx_p"><span id="S4.F6.8.8.9.1.3.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">SAN(CNN)+STR:<span id="S4.F6.8.8.9.1.3.1.3.1.1" class="ltx_text ltx_font_medium"> st. george ct.<span id="S4.F6.8.8.9.1.3.1.3.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.9.1.3.1.4" class="ltx_p"><span id="S4.F6.8.8.9.1.3.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">SAAA+STR:<span id="S4.F6.8.8.9.1.3.1.4.1.1" class="ltx_text ltx_font_medium"> esplanade<span id="S4.F6.8.8.9.1.3.1.4.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.9.1.3.1.5" class="ltx_p"><span id="S4.F6.8.8.9.1.3.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF8000;">Scene Image OCR:<span id="S4.F6.8.8.9.1.3.1.5.1.1" class="ltx_text ltx_font_medium"> gemors<span id="S4.F6.8.8.9.1.3.1.5.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.9.1.3.1.6" class="ltx_p"><span id="S4.F6.8.8.9.1.3.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">STR (bbox):<span id="S4.F6.8.8.9.1.3.1.6.1.1" class="ltx_text ltx_font_medium"> genoa</span></span></span>
</span>
</td>
<td id="S4.F6.8.8.9.1.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F6.8.8.9.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F6.8.8.9.1.4.1.1" class="ltx_p" style="width:99.7pt;"><span id="S4.F6.8.8.9.1.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q:<span id="S4.F6.8.8.9.1.4.1.1.1.1" class="ltx_text ltx_font_medium"> What’s the street name?</span></span></span>
<span id="S4.F6.8.8.9.1.4.1.2" class="ltx_p"><span id="S4.F6.8.8.9.1.4.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">A:<span id="S4.F6.8.8.9.1.4.1.2.1.1" class="ltx_text ltx_font_medium"> place d’armes<span id="S4.F6.8.8.9.1.4.1.2.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.9.1.4.1.3" class="ltx_p"><span id="S4.F6.8.8.9.1.4.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">SAN(CNN)+STR:<span id="S4.F6.8.8.9.1.4.1.3.1.1" class="ltx_text ltx_font_medium"> 10th st<span id="S4.F6.8.8.9.1.4.1.3.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.9.1.4.1.4" class="ltx_p"><span id="S4.F6.8.8.9.1.4.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">SAAA+STR:<span id="S4.F6.8.8.9.1.4.1.4.1.1" class="ltx_text ltx_font_medium"> ramistrasse<span id="S4.F6.8.8.9.1.4.1.4.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.9.1.4.1.5" class="ltx_p"><span id="S4.F6.8.8.9.1.4.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF8000;">Scene Image OCR:<span id="S4.F6.8.8.9.1.4.1.5.1.1" class="ltx_text ltx_font_medium"> d’armes<span id="S4.F6.8.8.9.1.4.1.5.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.9.1.4.1.6" class="ltx_p"><span id="S4.F6.8.8.9.1.4.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">STR (bbox):<span id="S4.F6.8.8.9.1.4.1.6.1.1" class="ltx_text ltx_font_medium"> dames</span></span></span>
</span>
</td>
</tr>
<tr id="S4.F6.8.8.8" class="ltx_tr">
<td id="S4.F6.5.5.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F6.5.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F6.5.5.5.1.1.1" class="ltx_p" style="width:99.7pt;"><img src="/html/1905.13648/assets/2348387.jpg" id="S4.F6.5.5.5.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F6.6.6.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F6.6.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F6.6.6.6.2.1.1" class="ltx_p" style="width:99.7pt;"><img src="/html/1905.13648/assets/2376130.jpg" id="S4.F6.6.6.6.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F6.7.7.7.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F6.7.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F6.7.7.7.3.1.1" class="ltx_p" style="width:99.7pt;"><img src="/html/1905.13648/assets/COCO_train2014_000000225070.jpg" id="S4.F6.7.7.7.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption"></span>
</span>
</td>
<td id="S4.F6.8.8.8.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F6.8.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F6.8.8.8.4.1.1" class="ltx_p" style="width:99.7pt;"><img src="/html/1905.13648/assets/VizWiz_train_000000007657.jpg" id="S4.F6.8.8.8.4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption"></span>
</span>
</td>
</tr>
<tr id="S4.F6.8.8.10.2" class="ltx_tr">
<td id="S4.F6.8.8.10.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F6.8.8.10.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F6.8.8.10.2.1.1.1" class="ltx_p" style="width:99.7pt;"><span id="S4.F6.8.8.10.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q:<span id="S4.F6.8.8.10.2.1.1.1.1.1" class="ltx_text ltx_font_medium"> What is the route of the bus?</span></span></span>
<span id="S4.F6.8.8.10.2.1.1.2" class="ltx_p"><span id="S4.F6.8.8.10.2.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">A:<span id="S4.F6.8.8.10.2.1.1.2.1.1" class="ltx_text ltx_font_medium"> purple route<span id="S4.F6.8.8.10.2.1.1.2.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.10.2.1.1.3" class="ltx_p"><span id="S4.F6.8.8.10.2.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">SAN(CNN)+STR:<span id="S4.F6.8.8.10.2.1.1.3.1.1" class="ltx_text ltx_font_medium"> 66<span id="S4.F6.8.8.10.2.1.1.3.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.10.2.1.1.4" class="ltx_p"><span id="S4.F6.8.8.10.2.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">SAAA+STR:<span id="S4.F6.8.8.10.2.1.1.4.1.1" class="ltx_text ltx_font_medium"> 508<span id="S4.F6.8.8.10.2.1.1.4.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.10.2.1.1.5" class="ltx_p"><span id="S4.F6.8.8.10.2.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">Scene Image OCR:<span id="S4.F6.8.8.10.2.1.1.5.1.1" class="ltx_text ltx_font_medium"> 1208<span id="S4.F6.8.8.10.2.1.1.5.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.10.2.1.1.6" class="ltx_p"><span id="S4.F6.8.8.10.2.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF8000;">STR (bbox):<span id="S4.F6.8.8.10.2.1.1.6.1.1" class="ltx_text ltx_font_medium"> purple</span></span></span>
</span>
</td>
<td id="S4.F6.8.8.10.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F6.8.8.10.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F6.8.8.10.2.2.1.1" class="ltx_p" style="width:99.7pt;"><span id="S4.F6.8.8.10.2.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q:<span id="S4.F6.8.8.10.2.2.1.1.1.1" class="ltx_text ltx_font_medium"> What is the automobile sponsor of the event?</span></span></span>
<span id="S4.F6.8.8.10.2.2.1.2" class="ltx_p"><span id="S4.F6.8.8.10.2.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">A:<span id="S4.F6.8.8.10.2.2.1.2.1.1" class="ltx_text ltx_font_medium"> kia<span id="S4.F6.8.8.10.2.2.1.2.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.10.2.2.1.3" class="ltx_p"><span id="S4.F6.8.8.10.2.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#00FF00;">SAN(CNN)+STR:<span id="S4.F6.8.8.10.2.2.1.3.1.1" class="ltx_text ltx_font_medium"> kia<span id="S4.F6.8.8.10.2.2.1.3.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.10.2.2.1.4" class="ltx_p"><span id="S4.F6.8.8.10.2.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#00FF00;">SAAA+STR:<span id="S4.F6.8.8.10.2.2.1.4.1.1" class="ltx_text ltx_font_medium"> kia<span id="S4.F6.8.8.10.2.2.1.4.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.10.2.2.1.5" class="ltx_p"><span id="S4.F6.8.8.10.2.2.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF8000;">Scene Image OCR:<span id="S4.F6.8.8.10.2.2.1.5.1.1" class="ltx_text ltx_font_medium"> kin<span id="S4.F6.8.8.10.2.2.1.5.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.10.2.2.1.6" class="ltx_p"><span id="S4.F6.8.8.10.2.2.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">STR (bbox):<span id="S4.F6.8.8.10.2.2.1.6.1.1" class="ltx_text ltx_font_medium"> 0</span></span></span>
</span>
</td>
<td id="S4.F6.8.8.10.2.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F6.8.8.10.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F6.8.8.10.2.3.1.1" class="ltx_p" style="width:99.7pt;"><span id="S4.F6.8.8.10.2.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q:<span id="S4.F6.8.8.10.2.3.1.1.1.1" class="ltx_text ltx_font_medium"> Which dessert is showcased?</span></span></span>
<span id="S4.F6.8.8.10.2.3.1.2" class="ltx_p"><span id="S4.F6.8.8.10.2.3.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">A:<span id="S4.F6.8.8.10.2.3.1.2.1.1" class="ltx_text ltx_font_medium"> donut<span id="S4.F6.8.8.10.2.3.1.2.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.10.2.3.1.3" class="ltx_p"><span id="S4.F6.8.8.10.2.3.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">A:<span id="S4.F6.8.8.10.2.3.1.3.1.1" class="ltx_text ltx_font_medium"> Vegan Donut<span id="S4.F6.8.8.10.2.3.1.3.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.10.2.3.1.4" class="ltx_p"><span id="S4.F6.8.8.10.2.3.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">SAN(CNN)+STR:<span id="S4.F6.8.8.10.2.3.1.4.1.1" class="ltx_text ltx_font_medium"> t<span id="S4.F6.8.8.10.2.3.1.4.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.10.2.3.1.5" class="ltx_p"><span id="S4.F6.8.8.10.2.3.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF8000;">SAAA+STR:<span id="S4.F6.8.8.10.2.3.1.5.1.1" class="ltx_text ltx_font_medium"> Donuts<span id="S4.F6.8.8.10.2.3.1.5.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.10.2.3.1.6" class="ltx_p"><span id="S4.F6.8.8.10.2.3.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">Scene Image OCR:<span id="S4.F6.8.8.10.2.3.1.6.1.1" class="ltx_text ltx_font_medium"> 175<span id="S4.F6.8.8.10.2.3.1.6.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.10.2.3.1.7" class="ltx_p"><span id="S4.F6.8.8.10.2.3.1.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">STR (bbox):<span id="S4.F6.8.8.10.2.3.1.7.1.1" class="ltx_text ltx_font_medium"> north</span></span></span>
</span>
</td>
<td id="S4.F6.8.8.10.2.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.F6.8.8.10.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.F6.8.8.10.2.4.1.1" class="ltx_p" style="width:99.7pt;"><span id="S4.F6.8.8.10.2.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q:<span id="S4.F6.8.8.10.2.4.1.1.1.1" class="ltx_text ltx_font_medium"> What is preheat oven temperature?</span></span></span>
<span id="S4.F6.8.8.10.2.4.1.2" class="ltx_p"><span id="S4.F6.8.8.10.2.4.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">A:<span id="S4.F6.8.8.10.2.4.1.2.1.1" class="ltx_text ltx_font_medium"> 350<span id="S4.F6.8.8.10.2.4.1.2.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.10.2.4.1.3" class="ltx_p"><span id="S4.F6.8.8.10.2.4.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#00FF00;">SAN(CNN)+STR:<span id="S4.F6.8.8.10.2.4.1.3.1.1" class="ltx_text ltx_font_medium"> 350 <span id="S4.F6.8.8.10.2.4.1.3.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.10.2.4.1.4" class="ltx_p"><span id="S4.F6.8.8.10.2.4.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">SAAA+STR:<span id="S4.F6.8.8.10.2.4.1.4.1.1" class="ltx_text ltx_font_medium"> 0<span id="S4.F6.8.8.10.2.4.1.4.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.10.2.4.1.5" class="ltx_p"><span id="S4.F6.8.8.10.2.4.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">Scene Image OCR:<span id="S4.F6.8.8.10.2.4.1.5.1.1" class="ltx_text ltx_font_medium"> high<span id="S4.F6.8.8.10.2.4.1.5.1.1.1" class="ltx_text" style="color:#000000;"></span></span></span></span>
<span id="S4.F6.8.8.10.2.4.1.6" class="ltx_p"><span id="S4.F6.8.8.10.2.4.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">STR (bbox):<span id="S4.F6.8.8.10.2.4.1.6.1.1" class="ltx_text ltx_font_medium"> receivables</span></span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Qualitative results for different methods on task 1 (strongly contextualised) of the ST-VQA dataset. For each image we show the question (Q), ground-truth answer (blue), and the answers provided by different methods (green: correct answer, red: incorrect answer, orange: incorrect answer in terms of accuracy but partially correct in terms of ANLS (<math id="S4.F6.10.10.m1.1" class="ltx_Math" alttext="0.5\leq ANLS&lt;1" display="inline"><semantics id="S4.F6.10.10.m1.1b"><mrow id="S4.F6.10.10.m1.1.1" xref="S4.F6.10.10.m1.1.1.cmml"><mn id="S4.F6.10.10.m1.1.1.2" xref="S4.F6.10.10.m1.1.1.2.cmml">0.5</mn><mo id="S4.F6.10.10.m1.1.1.3" xref="S4.F6.10.10.m1.1.1.3.cmml">≤</mo><mrow id="S4.F6.10.10.m1.1.1.4" xref="S4.F6.10.10.m1.1.1.4.cmml"><mi id="S4.F6.10.10.m1.1.1.4.2" xref="S4.F6.10.10.m1.1.1.4.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.F6.10.10.m1.1.1.4.1" xref="S4.F6.10.10.m1.1.1.4.1.cmml">​</mo><mi id="S4.F6.10.10.m1.1.1.4.3" xref="S4.F6.10.10.m1.1.1.4.3.cmml">N</mi><mo lspace="0em" rspace="0em" id="S4.F6.10.10.m1.1.1.4.1b" xref="S4.F6.10.10.m1.1.1.4.1.cmml">​</mo><mi id="S4.F6.10.10.m1.1.1.4.4" xref="S4.F6.10.10.m1.1.1.4.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.F6.10.10.m1.1.1.4.1c" xref="S4.F6.10.10.m1.1.1.4.1.cmml">​</mo><mi id="S4.F6.10.10.m1.1.1.4.5" xref="S4.F6.10.10.m1.1.1.4.5.cmml">S</mi></mrow><mo id="S4.F6.10.10.m1.1.1.5" xref="S4.F6.10.10.m1.1.1.5.cmml">&lt;</mo><mn id="S4.F6.10.10.m1.1.1.6" xref="S4.F6.10.10.m1.1.1.6.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F6.10.10.m1.1c"><apply id="S4.F6.10.10.m1.1.1.cmml" xref="S4.F6.10.10.m1.1.1"><and id="S4.F6.10.10.m1.1.1a.cmml" xref="S4.F6.10.10.m1.1.1"></and><apply id="S4.F6.10.10.m1.1.1b.cmml" xref="S4.F6.10.10.m1.1.1"><leq id="S4.F6.10.10.m1.1.1.3.cmml" xref="S4.F6.10.10.m1.1.1.3"></leq><cn type="float" id="S4.F6.10.10.m1.1.1.2.cmml" xref="S4.F6.10.10.m1.1.1.2">0.5</cn><apply id="S4.F6.10.10.m1.1.1.4.cmml" xref="S4.F6.10.10.m1.1.1.4"><times id="S4.F6.10.10.m1.1.1.4.1.cmml" xref="S4.F6.10.10.m1.1.1.4.1"></times><ci id="S4.F6.10.10.m1.1.1.4.2.cmml" xref="S4.F6.10.10.m1.1.1.4.2">𝐴</ci><ci id="S4.F6.10.10.m1.1.1.4.3.cmml" xref="S4.F6.10.10.m1.1.1.4.3">𝑁</ci><ci id="S4.F6.10.10.m1.1.1.4.4.cmml" xref="S4.F6.10.10.m1.1.1.4.4">𝐿</ci><ci id="S4.F6.10.10.m1.1.1.4.5.cmml" xref="S4.F6.10.10.m1.1.1.4.5">𝑆</ci></apply></apply><apply id="S4.F6.10.10.m1.1.1c.cmml" xref="S4.F6.10.10.m1.1.1"><lt id="S4.F6.10.10.m1.1.1.5.cmml" xref="S4.F6.10.10.m1.1.1.5"></lt><share href="#S4.F6.10.10.m1.1.1.4.cmml" id="S4.F6.10.10.m1.1.1d.cmml" xref="S4.F6.10.10.m1.1.1"></share><cn type="integer" id="S4.F6.10.10.m1.1.1.6.cmml" xref="S4.F6.10.10.m1.1.1.6">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.10.10.m1.1d">0.5\leq ANLS&lt;1</annotation></semantics></math>)).</figcaption>
</figure>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p">For further analysis of the baseline models’ outputs and comparison between them, we provide in <a href="#S4.F5" title="Figure 5 ‣ 4 Baselines and Results ‣ Scene Text Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 5</span></a> two bar charts with specific results on different question types. In most of them the STR model is better than the “Scene Image OCR” (ST-OCR) in terms of ANLS. The effect of PHOC embedding is especially visible on the SAN model for correctly answering the question type such as “what year”, “what company” and “which”. Also, none of the models is capable of answering the questions regarding license plates, “who” and “what number”. This is an inherent limitation of models treating VQA as a pure classification problem, as they can not deal with out of vocabulary answers.
In this regard the importance of using PHOC features lies in their ability to capture the morphology of words rather than their semantics as in other text embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>; since several text instances and answers in the dataset may not have any representation in a pre-trained semantic model. The use of a morphological embedding like PHOC can provide a starting point for datasets that contain text and answers in several languages and out of dictionary words such as license plates, prices, directions, names, etc.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions and Future Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This work introduces a new and relevant dimension to the VQA domain. We presented a new dataset for Visual Question Answering, the Scene Text VQA, that aims to highlight the importance of properly exploiting the high-level semantic information present in images in the form of scene text to inform the VQA process. The dataset comprises questions and answers of high variability, and poses extremely difficult challenges for current VQA methods. We thoroughly analysed the ST-VQA dataset through performing as series of experiments with baseline methods, which established the lower performance bounds, and provided important insights. Although we demonstrate that adding textual information to generic VQA models leads to improvements, we also show that ad-hoc baselines (e.g. OCR-based, which do exploit the contextual words) can outperform them, reinforcing the need of different approaches. Existing VQA models usually address the problem as a classification task, but in the case of scene text based answers the number of possible classes is intractable. Dictionaries defined over single words are also limited. Instead, a generative pipeline such as the ones used in image captioning is required to capture multiple-word answers, and out of dictionary strings such as numbers, license plates or codes. The proposed metric, namely Average Normalized Levenshtein Similarity is better suited for generative models compared to evaluating classification performance, while at the same time, it has a smooth response to the text recognition performance.</p>
</div>
<section id="S5.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Acknowledgments</h3>

<div id="S5.SSx1.p1" class="ltx_para">
<p id="S5.SSx1.p1.1" class="ltx_p">This work has been supported by projects TIN2017-89779-P,
Marie-Curie (712949 TECNIOspring PLUS), aBSINTHE (Fundacion BBVA 2017), the CERCA Programme / Generalitat de Catalunya, a European Social Fund grant (CCI: 2014ES05SFOP007), NVIDIA Corporation and PhD scholarships from AGAUR (2019-FIB01233) and the UAB.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Manoj Acharya, Kushal Kafle, and Christopher Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Tallyqa: Answering complex counting questions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial
Intelligence</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, volume 33, pages 8076–8084, 2019.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Don’t just assume; look and answer: Overcoming priors for visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, pages 4971–4980, 2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Jon Almazán, Albert Gordo, Alicia Fornés, and Ernest Valveny.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Word spotting and recognition with embedded attributes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">,
36(12):2552–2566, 2014.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Vqa: Visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 2425–2433, 2015.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Ali Furkan Biten, Rubèn Tito, Andres Mafla, Lluis Gomez, Marçal
Rusiñol, Minesh Mathew, CV Jawahar, Ernest Valveny, and Dimosthenis
Karatzas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Icdar 2019 competition on scene text visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1907.00490</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Enriching word vectors with subword information.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Transactions of the Association for Computational Linguistics</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">,
5:135–146, 2017.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Fedor Borisyuk, Albert Gordo, and Viswanath Sivakumar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Rosetta: Large scale system for text detection and recognition in
images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 24th ACM SIGKDD International Conference
on Knowledge Discovery &amp; Data Mining</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 71–79. ACM, 2018.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Michal Busta, Lukas Neumann, and Jiri Matas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Deep textspotter: An end-to-end trainable scene text localization and
recognition framework.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 2204–2212, 2017.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Prithvijit Chattopadhyay, Ramakrishna Vedantam, Ramprasaath R Selvaraju, Dhruv
Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Counting everyday objects in everyday scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 1135–1144, 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">2009.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Are you talking to a machine? dataset and methods for multilingual
image question.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages
2296–2304, 2015.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Yunze Gao, Yingying Chen, Jinqiao Wang, and Hanqing Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Reading scene text with attention convolutional sequence modeling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1709.04303</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Lluís Gómez, Andrés Mafla, Marçal Rusinol, and Dimosthenis
Karatzas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Single shot scene text retrieval.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 700–715, 2018.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Lluis Gomez, Yash Patel, Marçal Rusiñol, Dimosthenis Karatzas, and CV
Jawahar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Self-supervised learning of visual features through embedding images
into text topic spaces.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 4230–4239, 2017.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Raul Gomez, Baoguang Shi, Lluis Gomez, Lukas Neumann, Andreas Veit, Jiri Matas,
Serge Belongie, and Dimosthenis Karatzas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Icdar2017 robust reading challenge on coco-text.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 14th IAPR International Conference on Document Analysis
and Recognition (ICDAR)</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, volume 1, pages 1435–1443. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 6904–6913, 2017.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Pointing the unknown words.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">54th Annual Meeting of the Association for Computational
Linguistics, ACL 2016</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 140–149. Association for Computational
Linguistics (ACL), 2016.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman,
Jiebo Luo, and Jeffrey P Bigham.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Vizwiz grand challenge: Answering visual questions from blind people.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 3608–3617, 2018.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Tong He, Zhi Tian, Weilin Huang, Chunhua Shen, Yu Qiao, and Changming Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">An end-to-end textspotter with explicit alignment and attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 5020–5029, 2018.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Wenhao He, Xu-Yao Zhang, Fei Yin, and Cheng-Lin Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Deep direct regression for multi-oriented scene text detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 745–753, 2017.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Reading text in the wild with convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 116(1):1–20, 2016.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. Lawrence Zitnick,
and R. Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Clevr: A diagnostic dataset for compositional language and elementary
visual reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 2901–2910, 2017.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Dvqa: Understanding data visualizations via question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 5648–5656, 2018.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Ákos
Kádár, Adam Trischler, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Figureqa: An annotated figure dataset for visual reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1710.07300</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh,
Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann,
Vijay Ramaseshan Chandrasekhar, Shijian Lu, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Icdar 2015 competition on robust reading.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2015 13th International Conference on Document Analysis and
Recognition (ICDAR)</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages 1156–1160. IEEE, 2015.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura,
Lluis Gomez i Bigorda, Sergi Robles Mestre, Joan Mas, David Fernandez Mota,
Jon Almazan Almazan, and Lluis Pere De Las Heras.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Icdar 2013 robust reading competition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2013 12th International Conference on Document Analysis and
Recognition</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 1484–1493. IEEE, 2013.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Vahid Kazemi and Ali Elqursh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Show, ask, attend, and answer: A strong baseline for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1704.03162</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi,
and Hannaneh Hajishirzi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Are you smarter than a sixth grader? textbook question answering for
multimodal machine comprehension.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 4999–5007, 2017.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Aditya Khosla, Tinghui Zhou, Tomasz Malisiewicz, Alexei A Efros, and Antonio
Torralba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Undoing the damage of dataset bias.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages 158–171.
Springer, 2012.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Diederik P Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1412.6980</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Koichi Kise, Shota Fukushima, and Keinosuke Matsumoto.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Document image retrieval for QA systems based on the density
distributions of successive terms.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEICE Transactions</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">, 88-D, 2005.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 123(1):32–73, 2017.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Vladimir I Levenshtein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Binary codes capable of correcting deletions, insertions, and
reversals.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Soviet physics doklady</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, volume 10, pages 707–710, 1966.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Minghui Liao, Baoguang Shi, and Xiang Bai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Textboxes++: A single-shot oriented scene text detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on image processing</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, 27(8):3676–3690, 2018.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Minghui Liao, Baoguang Shi, Xiang Bai, Xinggang Wang, and Wenyu Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Textboxes: A fast text detector with a single deep neural network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Thirty-First AAAI Conference on Artificial Intelligence</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">,
2017.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 740–755.
Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Xuebo Liu, Ding Liang, Shi Yan, Dagui Chen, Yu Qiao, and Junjie Yan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Fots: Fast oriented text spotting with a unified network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, pages 5676–5685, 2018.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Pengyuan Lyu, Minghui Liao, Cong Yao, Wenhao Wu, and Xiang Bai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Mask textspotter: An end-to-end trainable neural network for spotting
text with arbitrary shapes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, pages 67–83, 2018.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Mateusz Malinowski and Mario Fritz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">A multi-world approach to question answering about real-world scenes
based on uncertain input.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages
1682–1690, 2014.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Distributed representations of words and phrases and their
compositionality.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, pages
3111–3119, 2013.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
A. Mishra, K. Alahari, and C. V. Jawahar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Image retrieval using textual cues.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Yash Patel, Lluis Gomez, Marçal Rusinol, and Dimosthenis Karatzas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Dynamic lexicon generation for natural scene images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, pages 395–410.
Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Ivan P Pavlov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Conditioned reflex: An investigation of the physiological activity of
the cerebral cortex.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">1960.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Jeffrey Pennington, Richard Socher, and Christopher Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Glove: Global vectors for word representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2014 conference on empirical methods in
natural language processing (EMNLP)</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, pages 1532–1543, 2014.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Mengye Ren, Ryan Kiros, and Richard Zemel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Exploring models and data for image question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, pages
2953–2961, 2015.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Baoguang Shi, Xinggang Wang, Pengyuan Lyu, Cong Yao, and Xiang Bai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Robust scene text recognition with automatic rectification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, pages 4168–4176, 2016.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Karen Simonyan and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Very deep convolutional networks for large-scale image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1409.1556</span><span id="bib.bib48.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Amanpreet Singh, Vivek Natarajan, Yu Jiang, Xinlei Chen, Meet Shah, Marcus
Rohrbach, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Pythia-a platform for vision &amp; language research.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">SysML Workshop, NeurIPS</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, volume 2018, 2018.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv
Batra, Devi Parikh, and Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Towards vqa models that can read.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR (To appear)</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Burrhus F Skinner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Operant behavior.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">American psychologist</span><span id="bib.bib51.4.2" class="ltx_text" style="font-size:90%;">, 18(8):503, 1963.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel
Urtasun, and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Movieqa: Understanding stories in movies through question-answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, pages 4631–4640, 2016.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Coco-text: Dataset and benchmark for text detection and recognition
in natural images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1601.07140</span><span id="bib.bib53.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Pointer networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, pages
2692–2700, 2015.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Kai Wang and Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Word spotting in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">, pages 591–604.
Springer, 2010.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Stacked attention networks for image question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib56.5.3" class="ltx_text" style="font-size:90%;">, pages 21–29, 2016.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Xu-Cheng Yin, Wei-Yi Pei, Jun Zhang, and Hong-Wei Hao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Multi-orientation scene text detection with adaptive clustering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib57.4.2" class="ltx_text" style="font-size:90%;">,
37(9):1930–1937, 2015.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Licheng Yu, Eunbyung Park, Alexander C Berg, and Tamara L Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Visual madlibs: Fill in the blank description generation and question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the ieee international conference on computer
vision</span><span id="bib.bib58.5.3" class="ltx_text" style="font-size:90%;">, pages 2461–2469, 2015.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">Yin and yang: Balancing and answering binary visual questions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib59.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib59.5.3" class="ltx_text" style="font-size:90%;">, pages 5014–5022, 2016.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang Zhou, Weiran He, and Jiajun
Liang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">East: an efficient and accurate scene text detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib60.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib60.5.3" class="ltx_text" style="font-size:90%;">, pages 5551–5560, 2017.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1905.13647" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1905.13648" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1905.13648">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1905.13648" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1905.13649" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 16:27:15 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
