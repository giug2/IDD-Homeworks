<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2109.13139] Multimodal Integration of Human-Like Attention in Visual Question Answering</title><meta property="og:description" content="Human-like attention as a supervisory signal to guide neural attention has shown significant promise but is currently limited to unimodal integration – even for inherently multimodal tasks such as visual question answe…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multimodal Integration of Human-Like Attention in Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multimodal Integration of Human-Like Attention in Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2109.13139">

<!--Generated on Sat Mar  2 03:27:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on  %**** main.tex Line 75 **** .-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Multimodal Integration of Human-Like Attention
<br class="ltx_break">in Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ekta Sood<sup id="id9.9.id1" class="ltx_sup">1</sup>, Fabian Kögel<sup id="id10.10.id2" class="ltx_sup">1</sup>, Philipp Müller<sup id="id11.11.id3" class="ltx_sup">2</sup>, Dominike Thomas,<sup id="id12.12.id4" class="ltx_sup">1</sup>, Mihai Bâce<sup id="id13.13.id5" class="ltx_sup">1</sup>, Andreas Bulling<sup id="id14.14.id6" class="ltx_sup">1</sup>
<br class="ltx_break"><sup id="id15.15.id7" class="ltx_sup">1</sup>University of Stuttgart, Institute for Visualization and Interactive Systems (VIS), Germany
<br class="ltx_break"><sup id="id16.16.id8" class="ltx_sup">2</sup>German Research Center for Artificial Intelligence (DFKI)
<br class="ltx_break"><span id="id17.17.id9" class="ltx_text ltx_font_typewriter">{ekta.sood,fabian.koegel,dominike.thomas,mihai.bace,andreas.bulling}@vis.uni-stuttgart.de</span> 
<br class="ltx_break"><span id="id18.18.id10" class="ltx_text ltx_font_typewriter">{philipp.mueller}@dfki.de</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id19.id1" class="ltx_p">Human-like attention as a supervisory signal to guide neural attention has shown significant promise but is currently limited to unimodal integration – even for inherently multimodal tasks such as visual question answering (VQA).
We present the Multimodal Human-like Attention Network (MULAN) – the first method for multimodal integration of human-like attention on image and text during training of VQA models.
MULAN integrates attention predictions from two state-of-the-art text and image saliency models into neural self-attention layers of a recent transformer-based VQA model.
Through evaluations on the challenging VQAv2 dataset, we show that MULAN achieves a new state-of-the-art performance of 73.98% accuracy on test-std and 73.72% on test-dev and, at the same time, has approximately 80% fewer trainable parameters than prior work.
Overall, our work underlines the potential of integrating multimodal human-like and neural attention for VQA.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual question answering (VQA) is an important task at the intersection of natural language processing (NLP) and computer vision that has attracted significant research interest in recent years
 <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib6" title="" class="ltx_ref">2015</a>); Malinowski and Fritz (<a href="#bib.bib25" title="" class="ltx_ref">2014</a>); Malinowski et al. (<a href="#bib.bib26" title="" class="ltx_ref">2015</a>); Yu et al. (<a href="#bib.bib45" title="" class="ltx_ref">2019b</a>)</cite>. One of its key challenges is, by its very nature, to jointly analyze and understand the language and visual input computationally. State-of-the-art methods for VQA rely on neural attention mechanisms to encode relations between questions and images and, for example, focus processing on parts of the image that are particularly relevant for a given question <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib43" title="" class="ltx_ref">2017</a>, <a href="#bib.bib45" title="" class="ltx_ref">2019b</a>); Jiang et al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>); Anderson et al. (<a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite>. An increasing number of methods make use of multiple Transformer-based attention modules <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a href="#bib.bib41" title="" class="ltx_ref">2017</a>)</cite>: they enable attention-based text features to exert complex patterns of influence on attention processes on images <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib45" title="" class="ltx_ref">2019b</a>); Jiang et al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Simultaneously, an increasing number of works have demonstrated the effectiveness of integrating human-like attention into neural attention mechanisms across a wide range of tasks, including image captioning <cite class="ltx_cite ltx_citemacro_cite">Sugano and Bulling (<a href="#bib.bib39" title="" class="ltx_ref">2016</a>)</cite>, text comprehension tasks <cite class="ltx_cite ltx_citemacro_cite">Sood et al. (<a href="#bib.bib38" title="" class="ltx_ref">2020b</a>)</cite>, or sentiment analysis and grammatical error detection <cite class="ltx_cite ltx_citemacro_cite">Barrett et al. (<a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite>. Research on integrating human-like attention into neural attention mechanisms for VQA has also attracted increasing interest <cite class="ltx_cite ltx_citemacro_cite">Qiao et al. (<a href="#bib.bib29" title="" class="ltx_ref">2018</a>); Selvaraju et al. (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>); Wu and Mooney (<a href="#bib.bib42" title="" class="ltx_ref">2019</a>); Chen et al. (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>); Shrestha et al. (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite>. However, despite the inherent multimodality of the VQA task, these works only integrate human-like attention on images.
A method for predicting and integrating human-like attention on text, which has obtained state-of-the-art performance in downstream NLP tasks, has only recently been proposed <cite class="ltx_cite ltx_citemacro_cite">Sood et al. (<a href="#bib.bib38" title="" class="ltx_ref">2020b</a>)</cite> and multimodal integration of human-like attention remains unexplored.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We fill this gap by proposing the Multimodal Human-like Attention Network (MULAN) – the first method for multimodal integration of human-like attention in VQA.
In contrast to previous unimodal integration methods on images alone, our method allows human attention information to act as a link between text and image input.
MULAN integrates state-of-the-art human saliency models for text and images into the attention scoring functions of the self-attention layers of the MCAN VQA architecture <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib45" title="" class="ltx_ref">2019b</a>)</cite>.
This way, human attention acts as an inductive bias directly modifying neural attention processes.
To model human-like attention on text we make use of the recently proposed Text Saliency Model (TSM) <cite class="ltx_cite ltx_citemacro_cite">Sood et al. (<a href="#bib.bib38" title="" class="ltx_ref">2020b</a>)</cite> that we adapt to the VQA task while training the MCAN framework.
Human attention on images is integrated using the recent multi-duration saliency model <cite class="ltx_cite ltx_citemacro_cite">Fosco et al. (<a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite> that also models the temporal dynamics of attention. We train our model on the VQAv2 dataset <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib14" title="" class="ltx_ref">2017</a>)</cite> and achieve state-of-the-art performance of 73.98% accuracy on <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">test-std</em> and 73.72% <em id="S1.p3.1.2" class="ltx_emph ltx_font_italic">test-dev</em>.
Notably, given that our model is based on the MCAN small variant, we require significantly fewer trainable parameters than the large variant.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our contributions are three-fold:
First, we propose a novel method to jointly integrate human-like attention on text and image into the MCAN VQA framework <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib45" title="" class="ltx_ref">2019b</a>)</cite>.
Second, we evaluate our method on the challenging <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">test-std</em> VQAv2 benchmark <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib14" title="" class="ltx_ref">2017</a>)</cite> and
show that it
outperforms the state of the art on both <em id="S1.p4.1.2" class="ltx_emph ltx_font_italic">test-std</em> and <em id="S1.p4.1.3" class="ltx_emph ltx_font_italic">test-dev</em> while requiring about 80% fewer trainable parameters.
Finally, through detailed analysis of success and failure cases we provide insights into how MULAN makes use of human attention information to correctly answer
questions that are notoriously difficult, e.g. longer questions.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Our work is related to previous works on 1) visual question answering, 2) using neural attention mechanisms, and 3) using human-like attention as a supervisory signal.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Visual Question Answering.</h3>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Using natural language to answer a question based on a single image <cite class="ltx_cite ltx_citemacro_citep">(Antol et al., <a href="#bib.bib6" title="" class="ltx_ref">2015</a>)</cite> has been a topic of increasing interest in recent years <cite class="ltx_cite ltx_citemacro_cite">Malinowski and Fritz (<a href="#bib.bib25" title="" class="ltx_ref">2014</a>); Malinowski et al. (<a href="#bib.bib26" title="" class="ltx_ref">2015</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Antol et al. (<a href="#bib.bib6" title="" class="ltx_ref">2015</a>)</cite> built the first, large-scale VQA dataset
that provided open-ended, free-form questions created by humans.
Given that models have been shown to exploit bias in datasets <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al., <a href="#bib.bib1" title="" class="ltx_ref">2016</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Goyal et al. (<a href="#bib.bib14" title="" class="ltx_ref">2017</a>)</cite> expanded the VQA dataset by balancing it so that each question had two images, with two different answers to the same question.
Tests on this new dataset (VQAv2) obtained significantly reduced performance for current models, showing how prevalent answer bias was before.
Another challenge in VQA remains the lack of inconsistency in answer predictions <cite class="ltx_cite ltx_citemacro_citep">(Shrestha et al., <a href="#bib.bib35" title="" class="ltx_ref">2019</a>; Zhang et al., <a href="#bib.bib46" title="" class="ltx_ref">2016</a>; Selvaraju et al., <a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite> and reduced performance for compositional questions <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al., <a href="#bib.bib3" title="" class="ltx_ref">2017</a>; Andreas et al., <a href="#bib.bib5" title="" class="ltx_ref">2016</a>; Shrestha et al., <a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite> or linguistic variation <cite class="ltx_cite ltx_citemacro_citep">(Shah et al., <a href="#bib.bib33" title="" class="ltx_ref">2019</a>; Agrawal et al., <a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Neural Attention Mechanisms.</h3>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">To imbue models with more reasoning capabilities,
researchers started experimenting with human-inspired neural attention and showed that adding neural attention mechanisms improved performance for VQA.
<cite class="ltx_cite ltx_citemacro_citet">Shih et al. (<a href="#bib.bib34" title="" class="ltx_ref">2016</a>)</cite> added a region selection layer to pinpoint relevant areas of an image and improved from <cite class="ltx_cite ltx_citemacro_citet">Antol et al. (<a href="#bib.bib6" title="" class="ltx_ref">2015</a>)</cite> by 5%.
Similarly, <cite class="ltx_cite ltx_citemacro_citet">Anderson et al. (<a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite> demonstrated that using bottom-up attention was preferable to top-down, and won first place in the 2017 VQA Challenge. <cite class="ltx_cite ltx_citemacro_citet">Jiang et al. (<a href="#bib.bib17" title="" class="ltx_ref">2018</a>)</cite> further expanded on this work by optimizing the model architecture and won the 2018 VQA challenge.
Follow-up works combined learned visual and language attention in order to narrow down which part of the image and question are relevant, first with alternating attention <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib23" title="" class="ltx_ref">2016</a>)</cite>, dual attention <cite class="ltx_cite ltx_citemacro_citep">(Nam et al., <a href="#bib.bib27" title="" class="ltx_ref">2017</a>)</cite>, and finally multi-level attention <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a href="#bib.bib43" title="" class="ltx_ref">2017</a>)</cite>.
The success of Transformers <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a href="#bib.bib41" title="" class="ltx_ref">2017</a>)</cite> in NLP tasks also inspired new work in VQA.
<cite class="ltx_cite ltx_citemacro_citet">Yu et al. (<a href="#bib.bib45" title="" class="ltx_ref">2019b</a>)</cite> created the Transformer-inspired Modular Co-Attention Network (MCAN) that combines self-attention with guided-attention to leverage the interaction within and between modalities.
<cite class="ltx_cite ltx_citemacro_citet">Jiang et al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite> further built on this architecture and won the 2020 VQA Challenge.
<cite class="ltx_cite ltx_citemacro_citet">Tan and Bansal (<a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite> improved input encoding with a Transformer and transfer learning,
while <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite> modified input encodings by adding an object tag to help align images and text semantically.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Supervision Using Human Attention.</h3>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Despite its advantages,
it was also demonstrated that neural attention may focus on the wrong area of an image <cite class="ltx_cite ltx_citemacro_citep">(Das et al., <a href="#bib.bib10" title="" class="ltx_ref">2016</a>; Chen et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>.
To rectify this, human attention was brought in as an additional supervisory signal.
Researchers investigated differences between neural and human attention in VQA <cite class="ltx_cite ltx_citemacro_citep">(Das et al., <a href="#bib.bib10" title="" class="ltx_ref">2016</a>; Chen et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite> and created datasets containing human attention maps <cite class="ltx_cite ltx_citemacro_citep">(Fosco et al., <a href="#bib.bib12" title="" class="ltx_ref">2020</a>; Chen et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>; Das et al., <a href="#bib.bib10" title="" class="ltx_ref">2016</a>)</cite>.
At the same time, integrating human attention supervision showed to be promising in closely related computer vision <cite class="ltx_cite ltx_citemacro_citep">(Sugano and Bulling, <a href="#bib.bib39" title="" class="ltx_ref">2016</a>; Karessli et al., <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite> or NLP tasks <cite class="ltx_cite ltx_citemacro_citep">(Barrett et al., <a href="#bib.bib7" title="" class="ltx_ref">2018</a>; Sood et al., <a href="#bib.bib37" title="" class="ltx_ref">2020a</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Sood et al. (<a href="#bib.bib38" title="" class="ltx_ref">2020b</a>)</cite> proposed a novel text saliency model that, by combining a cognitive model of reading with human attention supervision, beat state-of-the-art results on paraphrase generation and sentence compression.
For VQA tasks, <cite class="ltx_cite ltx_citemacro_citet">Gan et al. (<a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite> combined human attention on images and semantic segmentation of questions. Using ground truth human attention, <cite class="ltx_cite ltx_citemacro_citet">Wu and Mooney (<a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite> penalized networks for focusing on the wrong area of an image, while <cite class="ltx_cite ltx_citemacro_citet">Selvaraju et al. (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite> guided neural networks to look at areas of an image that humans judged as particularly relevant for question answering. <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite> continued in this direction by using human attention to encourage reasoning behaviour from a model. Since obtaining ground truth human attention annotations is costly and time-consuming, <cite class="ltx_cite ltx_citemacro_citet">Qiao et al. (<a href="#bib.bib29" title="" class="ltx_ref">2018</a>)</cite> trained a network on the VQA-HAT dataset to automatically generate human-like attention on unseen images, then used these saliency maps to create the enhanced Human-Like ATention (HLAT) dataset.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2109.13139/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="137" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of the Multimodal Human-like Attention Network (MULAN). Our method proposes multimodal integration of human-like attention on questions as well as images during training of VQA models. MULAN leverages attention predictions from two state-of-the-art text <cite class="ltx_cite ltx_citemacro_cite">Sood et al. (<a href="#bib.bib38" title="" class="ltx_ref">2020b</a>)</cite> and image saliency models <cite class="ltx_cite ltx_citemacro_cite">Fosco et al. (<a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The core contribution of our work is to propose MULAN, the first multimodal method to integrate human-like attention on both the image and text for VQA (see <a href="#S3.F1" title="Figure 1 ‣ 3 Method ‣ Multimodal Integration of Human-Like Attention in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a> for an overview of the method).
At its core, our method builds on the recent MCAN model <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib45" title="" class="ltx_ref">2019b</a>, <a href="#bib.bib44" title="" class="ltx_ref">a</a>)</cite>, which won the 2019 VQA challenge as well as being the basis of the 2020 winning method utilizing grid features <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>. We adapted the open source implementation<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/MILVLG/openvqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/MILVLG/openvqa</a></span></span></span> and trained the small variant of the MCAN using the grid features<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/facebookresearch/grid-feats-vqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/grid-feats-vqa</a></span></span></span>.
We first introduce the feature representations and the MCAN framework and subsequently explain our novel multimodal integration method<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Our implementation and other supporting material will be made publicly available</span></span></span>.</p>
</div>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Feature Representations.</h3>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.4" class="ltx_p">We represent the input images by spatial grid features, following the extraction methodology of <cite class="ltx_cite ltx_citemacro_citet">Jiang et al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>. A Faster R-CNN with ResNet-50 backbone  <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib30" title="" class="ltx_ref">2015</a>); He et al. (<a href="#bib.bib15" title="" class="ltx_ref">2016</a>)</cite> is pre-trained on ImageNet  <cite class="ltx_cite ltx_citemacro_cite">Deng et al. (<a href="#bib.bib11" title="" class="ltx_ref">2009</a>)</cite> and VG <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. (<a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite> and then the object proposal and RoI pooling (used for region features in <cite class="ltx_cite ltx_citemacro_citet">Anderson et al. (<a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite>) is removed. The remaining ResNet directly outputs the grid features.
We obtain up to 19x32 features (depending on aspect ratio) per image. The final image representation is <math id="S3.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="X\in\mathbb{R}^{m\times d_{x}}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">X</mi><mo id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.1.cmml">×</mo><msub id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.3" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.3.cmml"><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.3.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.3.2.cmml">d</mi><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.3.3" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.3.3.cmml">x</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1"><in id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.1"></in><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2">𝑋</ci><apply id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3"><times id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.2">𝑚</ci><apply id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.3.2">𝑑</ci><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.3.3.3">𝑥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.1.m1.1c">X\in\mathbb{R}^{m\times d_{x}}</annotation></semantics></math> with <math id="S3.SS0.SSS0.Px1.p1.2.m2.2" class="ltx_Math" alttext="m\in[192,608]" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.2.m2.2a"><mrow id="S3.SS0.SSS0.Px1.p1.2.m2.2.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.cmml"><mi id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.2.cmml">m</mi><mo id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.1" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.1.cmml">∈</mo><mrow id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.2.1" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.1.cmml">[</mo><mn id="S3.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">192</mn><mo id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.2.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.1.cmml">,</mo><mn id="S3.SS0.SSS0.Px1.p1.2.m2.2.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2.cmml">608</mn><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.2.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.2.m2.2b"><apply id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3"><in id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.1"></in><ci id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.2">𝑚</ci><interval closure="closed" id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.3.2"><cn type="integer" id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1">192</cn><cn type="integer" id="S3.SS0.SSS0.Px1.p1.2.m2.2.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2">608</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.2.m2.2c">m\in[192,608]</annotation></semantics></math>, where <math id="S3.SS0.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.3.m3.1a"><mi id="S3.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.3.m3.1c">m</annotation></semantics></math> represents the number of features, and <math id="S3.SS0.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="d_{x}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.4.m4.1a"><msub id="S3.SS0.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.4.m4.1.1.2" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1.2.cmml">d</mi><mi id="S3.SS0.SSS0.Px1.p1.4.m4.1.1.3" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.4.m4.1b"><apply id="S3.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1.2">𝑑</ci><ci id="S3.SS0.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.4.m4.1c">d_{x}</annotation></semantics></math> the feature embedding dimension.</p>
</div>
<div id="S3.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p2.4" class="ltx_p">The input questions are represented as in MCAN: tokenized at word-level, trimmed to <math id="S3.SS0.SSS0.Px1.p2.1.m1.2" class="ltx_Math" alttext="n\in[1,14]" display="inline"><semantics id="S3.SS0.SSS0.Px1.p2.1.m1.2a"><mrow id="S3.SS0.SSS0.Px1.p2.1.m1.2.3" xref="S3.SS0.SSS0.Px1.p2.1.m1.2.3.cmml"><mi id="S3.SS0.SSS0.Px1.p2.1.m1.2.3.2" xref="S3.SS0.SSS0.Px1.p2.1.m1.2.3.2.cmml">n</mi><mo id="S3.SS0.SSS0.Px1.p2.1.m1.2.3.1" xref="S3.SS0.SSS0.Px1.p2.1.m1.2.3.1.cmml">∈</mo><mrow id="S3.SS0.SSS0.Px1.p2.1.m1.2.3.3.2" xref="S3.SS0.SSS0.Px1.p2.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px1.p2.1.m1.2.3.3.2.1" xref="S3.SS0.SSS0.Px1.p2.1.m1.2.3.3.1.cmml">[</mo><mn id="S3.SS0.SSS0.Px1.p2.1.m1.1.1" xref="S3.SS0.SSS0.Px1.p2.1.m1.1.1.cmml">1</mn><mo id="S3.SS0.SSS0.Px1.p2.1.m1.2.3.3.2.2" xref="S3.SS0.SSS0.Px1.p2.1.m1.2.3.3.1.cmml">,</mo><mn id="S3.SS0.SSS0.Px1.p2.1.m1.2.2" xref="S3.SS0.SSS0.Px1.p2.1.m1.2.2.cmml">14</mn><mo stretchy="false" id="S3.SS0.SSS0.Px1.p2.1.m1.2.3.3.2.3" xref="S3.SS0.SSS0.Px1.p2.1.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p2.1.m1.2b"><apply id="S3.SS0.SSS0.Px1.p2.1.m1.2.3.cmml" xref="S3.SS0.SSS0.Px1.p2.1.m1.2.3"><in id="S3.SS0.SSS0.Px1.p2.1.m1.2.3.1.cmml" xref="S3.SS0.SSS0.Px1.p2.1.m1.2.3.1"></in><ci id="S3.SS0.SSS0.Px1.p2.1.m1.2.3.2.cmml" xref="S3.SS0.SSS0.Px1.p2.1.m1.2.3.2">𝑛</ci><interval closure="closed" id="S3.SS0.SSS0.Px1.p2.1.m1.2.3.3.1.cmml" xref="S3.SS0.SSS0.Px1.p2.1.m1.2.3.3.2"><cn type="integer" id="S3.SS0.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p2.1.m1.1.1">1</cn><cn type="integer" id="S3.SS0.SSS0.Px1.p2.1.m1.2.2.cmml" xref="S3.SS0.SSS0.Px1.p2.1.m1.2.2">14</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p2.1.m1.2c">n\in[1,14]</annotation></semantics></math> tokens and represented using 300-D GloVe <cite class="ltx_cite ltx_citemacro_cite">Pennington et al. (<a href="#bib.bib28" title="" class="ltx_ref">2014</a>)</cite> word embeddings. The <math id="S3.SS0.SSS0.Px1.p2.2.m2.1" class="ltx_Math" alttext="n\times 300" display="inline"><semantics id="S3.SS0.SSS0.Px1.p2.2.m2.1a"><mrow id="S3.SS0.SSS0.Px1.p2.2.m2.1.1" xref="S3.SS0.SSS0.Px1.p2.2.m2.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p2.2.m2.1.1.2" xref="S3.SS0.SSS0.Px1.p2.2.m2.1.1.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS0.SSS0.Px1.p2.2.m2.1.1.1" xref="S3.SS0.SSS0.Px1.p2.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS0.SSS0.Px1.p2.2.m2.1.1.3" xref="S3.SS0.SSS0.Px1.p2.2.m2.1.1.3.cmml">300</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p2.2.m2.1b"><apply id="S3.SS0.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px1.p2.2.m2.1.1"><times id="S3.SS0.SSS0.Px1.p2.2.m2.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p2.2.m2.1.1.1"></times><ci id="S3.SS0.SSS0.Px1.p2.2.m2.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p2.2.m2.1.1.2">𝑛</ci><cn type="integer" id="S3.SS0.SSS0.Px1.p2.2.m2.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p2.2.m2.1.1.3">300</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p2.2.m2.1c">n\times 300</annotation></semantics></math> embeddings are further passed through a one-layer LSTM with hidden size <math id="S3.SS0.SSS0.Px1.p2.3.m3.1" class="ltx_Math" alttext="d_{y}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p2.3.m3.1a"><msub id="S3.SS0.SSS0.Px1.p2.3.m3.1.1" xref="S3.SS0.SSS0.Px1.p2.3.m3.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p2.3.m3.1.1.2" xref="S3.SS0.SSS0.Px1.p2.3.m3.1.1.2.cmml">d</mi><mi id="S3.SS0.SSS0.Px1.p2.3.m3.1.1.3" xref="S3.SS0.SSS0.Px1.p2.3.m3.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p2.3.m3.1b"><apply id="S3.SS0.SSS0.Px1.p2.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p2.3.m3.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p2.3.m3.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p2.3.m3.1.1.2">𝑑</ci><ci id="S3.SS0.SSS0.Px1.p2.3.m3.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p2.3.m3.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p2.3.m3.1c">d_{y}</annotation></semantics></math> and all intermediate hidden states form the final question representation matrix <math id="S3.SS0.SSS0.Px1.p2.4.m4.1" class="ltx_Math" alttext="Y\in\mathbb{R}^{n\times d_{y}}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p2.4.m4.1a"><mrow id="S3.SS0.SSS0.Px1.p2.4.m4.1.1" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.2" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.2.cmml">Y</mi><mo id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.1" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.2" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.cmml"><mi id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.2" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.1" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.1.cmml">×</mo><msub id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.3" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.3.cmml"><mi id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.3.2" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.3.2.cmml">d</mi><mi id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.3.3" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.3.3.cmml">y</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p2.4.m4.1b"><apply id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.cmml" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1"><in id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.1"></in><ci id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.2">𝑌</ci><apply id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.2">ℝ</ci><apply id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3"><times id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.1"></times><ci id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.2">𝑛</ci><apply id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.3.cmml" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.3.1.cmml" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.3">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.3.2.cmml" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.3.2">𝑑</ci><ci id="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.3.3.cmml" xref="S3.SS0.SSS0.Px1.p2.4.m4.1.1.3.3.3.3">𝑦</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p2.4.m4.1c">Y\in\mathbb{R}^{n\times d_{y}}</annotation></semantics></math>.
Both representations are zero-padded to accommodate the varying number of grid features and question words.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Base model.</h3>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p1.1" class="ltx_p">In general, an attention function computes an alignment score between a query and key-value pairs and uses the score to re-weight the values.
Attention methods differ in their choice of scoring function, whether they attend to the whole (global/soft) or only parts (local/hard) of the input, and whether queries and key-value pairs are projections from the same inputs (self-attention) or different inputs (guided attention).
The Deep Modular Co-Attention Network (MCAN) for VQA <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib45" title="" class="ltx_ref">2019b</a>)</cite> is a Transformer-based network <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a href="#bib.bib41" title="" class="ltx_ref">2017</a>)</cite> that runs multiple layers of multi-headed self-attention (SA) and guided-attention (GA) modules in an encoder-decoder architecture using the scaled dot-product score function.</p>
</div>
<div id="S3.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p2.5" class="ltx_p">A schematic of an SA module is shown in <a href="#S3.F2" title="Figure 2 ‣ Human-Like Attention Integration. ‣ 3 Method ‣ Multimodal Integration of Human-Like Attention in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a> in gray.
It consists of two sub-layers: the multi-headed attention and a feed-forward layer.
Both are encompassed by a residual connection and layer normalization.
The attention sub-layer projects the input feature embeddings into queries <math id="S3.SS0.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="Q\in\mathbb{R}^{n\times d}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.1.m1.1a"><mrow id="S3.SS0.SSS0.Px2.p2.1.m1.1.1" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.2" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.2.cmml">Q</mi><mo id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.1" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.2" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.cmml"><mi id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.2" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.1" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.3" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.1.m1.1b"><apply id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1"><in id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.1"></in><ci id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.2">𝑄</ci><apply id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3"><times id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.1"></times><ci id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.2">𝑛</ci><ci id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.1.m1.1c">Q\in\mathbb{R}^{n\times d}</annotation></semantics></math>, keys <math id="S3.SS0.SSS0.Px2.p2.2.m2.1" class="ltx_Math" alttext="K\in\mathbb{R}^{n\times d}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.2.m2.1a"><mrow id="S3.SS0.SSS0.Px2.p2.2.m2.1.1" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.2" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.2.cmml">K</mi><mo id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.1" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.2" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3.cmml"><mi id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3.2" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3.1" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3.3" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.2.m2.1b"><apply id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1"><in id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.1"></in><ci id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.2">𝐾</ci><apply id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3"><times id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3.1"></times><ci id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3.2">𝑛</ci><ci id="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3.3.cmml" xref="S3.SS0.SSS0.Px2.p2.2.m2.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.2.m2.1c">K\in\mathbb{R}^{n\times d}</annotation></semantics></math>, and values <math id="S3.SS0.SSS0.Px2.p2.3.m3.1" class="ltx_Math" alttext="V\in\mathbb{R}^{n\times d}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.3.m3.1a"><mrow id="S3.SS0.SSS0.Px2.p2.3.m3.1.1" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.2" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.2.cmml">V</mi><mo id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.1" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.2" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3.cmml"><mi id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3.2" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3.1" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3.3" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.3.m3.1b"><apply id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1"><in id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.1"></in><ci id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.2">𝑉</ci><apply id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3"><times id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3.1"></times><ci id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3.2">𝑛</ci><ci id="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3.3.cmml" xref="S3.SS0.SSS0.Px2.p2.3.m3.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.3.m3.1c">V\in\mathbb{R}^{n\times d}</annotation></semantics></math> with a common hidden dimension <math id="S3.SS0.SSS0.Px2.p2.4.m4.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.4.m4.1a"><mi id="S3.SS0.SSS0.Px2.p2.4.m4.1.1" xref="S3.SS0.SSS0.Px2.p2.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.4.m4.1b"><ci id="S3.SS0.SSS0.Px2.p2.4.m4.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.4.m4.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.4.m4.1c">d</annotation></semantics></math>. For a query <math id="S3.SS0.SSS0.Px2.p2.5.m5.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.5.m5.1a"><mi id="S3.SS0.SSS0.Px2.p2.5.m5.1.1" xref="S3.SS0.SSS0.Px2.p2.5.m5.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.5.m5.1b"><ci id="S3.SS0.SSS0.Px2.p2.5.m5.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.5.m5.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.5.m5.1c">q</annotation></semantics></math> the attended output is calculated with:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.4" class="ltx_Math" alttext="A(q,K,V)=\textrm{softmax}(\frac{qK^{T}}{\sqrt{d}})V" display="block"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.5" xref="S3.E1.m1.4.5.cmml"><mrow id="S3.E1.m1.4.5.2" xref="S3.E1.m1.4.5.2.cmml"><mi id="S3.E1.m1.4.5.2.2" xref="S3.E1.m1.4.5.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.5.2.1" xref="S3.E1.m1.4.5.2.1.cmml">​</mo><mrow id="S3.E1.m1.4.5.2.3.2" xref="S3.E1.m1.4.5.2.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.4.5.2.3.2.1" xref="S3.E1.m1.4.5.2.3.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">q</mi><mo id="S3.E1.m1.4.5.2.3.2.2" xref="S3.E1.m1.4.5.2.3.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">K</mi><mo id="S3.E1.m1.4.5.2.3.2.3" xref="S3.E1.m1.4.5.2.3.1.cmml">,</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">V</mi><mo stretchy="false" id="S3.E1.m1.4.5.2.3.2.4" xref="S3.E1.m1.4.5.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.5.1" xref="S3.E1.m1.4.5.1.cmml">=</mo><mrow id="S3.E1.m1.4.5.3" xref="S3.E1.m1.4.5.3.cmml"><mtext id="S3.E1.m1.4.5.3.2" xref="S3.E1.m1.4.5.3.2a.cmml">softmax</mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.5.3.1" xref="S3.E1.m1.4.5.3.1.cmml">​</mo><mrow id="S3.E1.m1.4.5.3.3.2" xref="S3.E1.m1.4.4.cmml"><mo stretchy="false" id="S3.E1.m1.4.5.3.3.2.1" xref="S3.E1.m1.4.4.cmml">(</mo><mfrac id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml"><mrow id="S3.E1.m1.4.4.2" xref="S3.E1.m1.4.4.2.cmml"><mi id="S3.E1.m1.4.4.2.2" xref="S3.E1.m1.4.4.2.2.cmml">q</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.2.1" xref="S3.E1.m1.4.4.2.1.cmml">​</mo><msup id="S3.E1.m1.4.4.2.3" xref="S3.E1.m1.4.4.2.3.cmml"><mi id="S3.E1.m1.4.4.2.3.2" xref="S3.E1.m1.4.4.2.3.2.cmml">K</mi><mi id="S3.E1.m1.4.4.2.3.3" xref="S3.E1.m1.4.4.2.3.3.cmml">T</mi></msup></mrow><msqrt id="S3.E1.m1.4.4.3" xref="S3.E1.m1.4.4.3.cmml"><mi id="S3.E1.m1.4.4.3.2" xref="S3.E1.m1.4.4.3.2.cmml">d</mi></msqrt></mfrac><mo stretchy="false" id="S3.E1.m1.4.5.3.3.2.2" xref="S3.E1.m1.4.4.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.5.3.1a" xref="S3.E1.m1.4.5.3.1.cmml">​</mo><mi id="S3.E1.m1.4.5.3.4" xref="S3.E1.m1.4.5.3.4.cmml">V</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.5.cmml" xref="S3.E1.m1.4.5"><eq id="S3.E1.m1.4.5.1.cmml" xref="S3.E1.m1.4.5.1"></eq><apply id="S3.E1.m1.4.5.2.cmml" xref="S3.E1.m1.4.5.2"><times id="S3.E1.m1.4.5.2.1.cmml" xref="S3.E1.m1.4.5.2.1"></times><ci id="S3.E1.m1.4.5.2.2.cmml" xref="S3.E1.m1.4.5.2.2">𝐴</ci><vector id="S3.E1.m1.4.5.2.3.1.cmml" xref="S3.E1.m1.4.5.2.3.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑞</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝐾</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝑉</ci></vector></apply><apply id="S3.E1.m1.4.5.3.cmml" xref="S3.E1.m1.4.5.3"><times id="S3.E1.m1.4.5.3.1.cmml" xref="S3.E1.m1.4.5.3.1"></times><ci id="S3.E1.m1.4.5.3.2a.cmml" xref="S3.E1.m1.4.5.3.2"><mtext id="S3.E1.m1.4.5.3.2.cmml" xref="S3.E1.m1.4.5.3.2">softmax</mtext></ci><apply id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.5.3.3.2"><divide id="S3.E1.m1.4.4.1.cmml" xref="S3.E1.m1.4.5.3.3.2"></divide><apply id="S3.E1.m1.4.4.2.cmml" xref="S3.E1.m1.4.4.2"><times id="S3.E1.m1.4.4.2.1.cmml" xref="S3.E1.m1.4.4.2.1"></times><ci id="S3.E1.m1.4.4.2.2.cmml" xref="S3.E1.m1.4.4.2.2">𝑞</ci><apply id="S3.E1.m1.4.4.2.3.cmml" xref="S3.E1.m1.4.4.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.2.3.1.cmml" xref="S3.E1.m1.4.4.2.3">superscript</csymbol><ci id="S3.E1.m1.4.4.2.3.2.cmml" xref="S3.E1.m1.4.4.2.3.2">𝐾</ci><ci id="S3.E1.m1.4.4.2.3.3.cmml" xref="S3.E1.m1.4.4.2.3.3">𝑇</ci></apply></apply><apply id="S3.E1.m1.4.4.3.cmml" xref="S3.E1.m1.4.4.3"><root id="S3.E1.m1.4.4.3a.cmml" xref="S3.E1.m1.4.4.3"></root><ci id="S3.E1.m1.4.4.3.2.cmml" xref="S3.E1.m1.4.4.3.2">𝑑</ci></apply></apply><ci id="S3.E1.m1.4.5.3.4.cmml" xref="S3.E1.m1.4.5.3.4">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">A(q,K,V)=\textrm{softmax}(\frac{qK^{T}}{\sqrt{d}})V</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS0.SSS0.Px2.p2.9" class="ltx_p">As in the Transformer <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a href="#bib.bib41" title="" class="ltx_ref">2017</a>)</cite>, this is calculated for multiple queries at once with <math id="S3.SS0.SSS0.Px2.p2.6.m1.1" class="ltx_Math" alttext="QK^{T}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.6.m1.1a"><mrow id="S3.SS0.SSS0.Px2.p2.6.m1.1.1" xref="S3.SS0.SSS0.Px2.p2.6.m1.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p2.6.m1.1.1.2" xref="S3.SS0.SSS0.Px2.p2.6.m1.1.1.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px2.p2.6.m1.1.1.1" xref="S3.SS0.SSS0.Px2.p2.6.m1.1.1.1.cmml">​</mo><msup id="S3.SS0.SSS0.Px2.p2.6.m1.1.1.3" xref="S3.SS0.SSS0.Px2.p2.6.m1.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px2.p2.6.m1.1.1.3.2" xref="S3.SS0.SSS0.Px2.p2.6.m1.1.1.3.2.cmml">K</mi><mi id="S3.SS0.SSS0.Px2.p2.6.m1.1.1.3.3" xref="S3.SS0.SSS0.Px2.p2.6.m1.1.1.3.3.cmml">T</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.6.m1.1b"><apply id="S3.SS0.SSS0.Px2.p2.6.m1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.6.m1.1.1"><times id="S3.SS0.SSS0.Px2.p2.6.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.6.m1.1.1.1"></times><ci id="S3.SS0.SSS0.Px2.p2.6.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p2.6.m1.1.1.2">𝑄</ci><apply id="S3.SS0.SSS0.Px2.p2.6.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p2.6.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p2.6.m1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p2.6.m1.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p2.6.m1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p2.6.m1.1.1.3.2">𝐾</ci><ci id="S3.SS0.SSS0.Px2.p2.6.m1.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p2.6.m1.1.1.3.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.6.m1.1c">QK^{T}</annotation></semantics></math> and the results of several heads with different projections for <math id="S3.SS0.SSS0.Px2.p2.7.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.7.m2.1a"><mi id="S3.SS0.SSS0.Px2.p2.7.m2.1.1" xref="S3.SS0.SSS0.Px2.p2.7.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.7.m2.1b"><ci id="S3.SS0.SSS0.Px2.p2.7.m2.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.7.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.7.m2.1c">Q</annotation></semantics></math>, <math id="S3.SS0.SSS0.Px2.p2.8.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.8.m3.1a"><mi id="S3.SS0.SSS0.Px2.p2.8.m3.1.1" xref="S3.SS0.SSS0.Px2.p2.8.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.8.m3.1b"><ci id="S3.SS0.SSS0.Px2.p2.8.m3.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.8.m3.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.8.m3.1c">K</annotation></semantics></math>, <math id="S3.SS0.SSS0.Px2.p2.9.m4.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS0.SSS0.Px2.p2.9.m4.1a"><mi id="S3.SS0.SSS0.Px2.p2.9.m4.1.1" xref="S3.SS0.SSS0.Px2.p2.9.m4.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.9.m4.1b"><ci id="S3.SS0.SSS0.Px2.p2.9.m4.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.9.m4.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.9.m4.1c">V</annotation></semantics></math> are combined.</p>
</div>
<div id="S3.SS0.SSS0.Px2.p3" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p3.1" class="ltx_p">The GA module is set up identically to the SA module except the queries and key-value pairs are provided by separate inputs.
In this way, text features can guide attention on image features.
Intuitively, the attention layer reconstructs the queries from a linear combination of the values, emphasizing interactions between them. The value space is projected from the input features, which in the GA case is a fusion space between the modalities.</p>
</div>
<div id="S3.SS0.SSS0.Px2.p4" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p4.4" class="ltx_p">The MCAN encoder stacks multiple layers of SA on text features <math id="S3.SS0.SSS0.Px2.p4.1.m1.1" class="ltx_Math" alttext="Y\in\mathbb{R}^{n_{y}\times d_{y}}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p4.1.m1.1a"><mrow id="S3.SS0.SSS0.Px2.p4.1.m1.1.1" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.2" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.2.cmml">Y</mi><mo id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.1" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.2" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.cmml"><msub id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.2" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.2.cmml"><mi id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.2.2" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.2.2.cmml">n</mi><mi id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.2.3" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.2.3.cmml">y</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.1" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.1.cmml">×</mo><msub id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.3" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.3.cmml"><mi id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.3.2" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.3.2.cmml">d</mi><mi id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.3.3" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.3.3.cmml">y</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p4.1.m1.1b"><apply id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1"><in id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.1"></in><ci id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.2">𝑌</ci><apply id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3"><times id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.1"></times><apply id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.2.1.cmml" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.2.2.cmml" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.2.2">𝑛</ci><ci id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.2.3.cmml" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.2.3">𝑦</ci></apply><apply id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.3.cmml" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.3.1.cmml" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.3.2.cmml" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.3.2">𝑑</ci><ci id="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.3.3.cmml" xref="S3.SS0.SSS0.Px2.p4.1.m1.1.1.3.3.3.3">𝑦</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p4.1.m1.1c">Y\in\mathbb{R}^{n_{y}\times d_{y}}</annotation></semantics></math> before feeding the result of the last layer into the decoder. The decoder stacks modules with SA on image features <math id="S3.SS0.SSS0.Px2.p4.2.m2.1" class="ltx_Math" alttext="X\in\mathbb{R}^{n_{x}\times d_{x}}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p4.2.m2.1a"><mrow id="S3.SS0.SSS0.Px2.p4.2.m2.1.1" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.2" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.2.cmml">X</mi><mo id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.1" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.2" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.cmml"><msub id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.2" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.2.cmml"><mi id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.2.2" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.2.2.cmml">n</mi><mi id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.2.3" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.2.3.cmml">x</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.1" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.1.cmml">×</mo><msub id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.3" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.3.cmml"><mi id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.3.2" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.3.2.cmml">d</mi><mi id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.3.3" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.3.3.cmml">x</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p4.2.m2.1b"><apply id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1"><in id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.1"></in><ci id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.2">𝑋</ci><apply id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3"><times id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.1"></times><apply id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.2.1.cmml" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.2.2.cmml" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.2.2">𝑛</ci><ci id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.2.3.cmml" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.2.3">𝑥</ci></apply><apply id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.3.cmml" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.3.1.cmml" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.3">subscript</csymbol><ci id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.3.2.cmml" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.3.2">𝑑</ci><ci id="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.3.3.cmml" xref="S3.SS0.SSS0.Px2.p4.2.m2.1.1.3.3.3.3">𝑥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p4.2.m2.1c">X\in\mathbb{R}^{n_{x}\times d_{x}}</annotation></semantics></math> and GA between the encoder result and the SA output. After the last layer, the resulting feature matrices from both encoder and decoder are flattened to obtain the attended features <math id="S3.SS0.SSS0.Px2.p4.3.m3.1" class="ltx_Math" alttext="\tilde{y}\in\mathbb{R}^{d}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p4.3.m3.1a"><mrow id="S3.SS0.SSS0.Px2.p4.3.m3.1.1" xref="S3.SS0.SSS0.Px2.p4.3.m3.1.1.cmml"><mover accent="true" id="S3.SS0.SSS0.Px2.p4.3.m3.1.1.2" xref="S3.SS0.SSS0.Px2.p4.3.m3.1.1.2.cmml"><mi id="S3.SS0.SSS0.Px2.p4.3.m3.1.1.2.2" xref="S3.SS0.SSS0.Px2.p4.3.m3.1.1.2.2.cmml">y</mi><mo id="S3.SS0.SSS0.Px2.p4.3.m3.1.1.2.1" xref="S3.SS0.SSS0.Px2.p4.3.m3.1.1.2.1.cmml">~</mo></mover><mo id="S3.SS0.SSS0.Px2.p4.3.m3.1.1.1" xref="S3.SS0.SSS0.Px2.p4.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS0.SSS0.Px2.p4.3.m3.1.1.3" xref="S3.SS0.SSS0.Px2.p4.3.m3.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px2.p4.3.m3.1.1.3.2" xref="S3.SS0.SSS0.Px2.p4.3.m3.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS0.SSS0.Px2.p4.3.m3.1.1.3.3" xref="S3.SS0.SSS0.Px2.p4.3.m3.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p4.3.m3.1b"><apply id="S3.SS0.SSS0.Px2.p4.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px2.p4.3.m3.1.1"><in id="S3.SS0.SSS0.Px2.p4.3.m3.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p4.3.m3.1.1.1"></in><apply id="S3.SS0.SSS0.Px2.p4.3.m3.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p4.3.m3.1.1.2"><ci id="S3.SS0.SSS0.Px2.p4.3.m3.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px2.p4.3.m3.1.1.2.1">~</ci><ci id="S3.SS0.SSS0.Px2.p4.3.m3.1.1.2.2.cmml" xref="S3.SS0.SSS0.Px2.p4.3.m3.1.1.2.2">𝑦</ci></apply><apply id="S3.SS0.SSS0.Px2.p4.3.m3.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p4.3.m3.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p4.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p4.3.m3.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p4.3.m3.1.1.3.2">ℝ</ci><ci id="S3.SS0.SSS0.Px2.p4.3.m3.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p4.3.m3.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p4.3.m3.1c">\tilde{y}\in\mathbb{R}^{d}</annotation></semantics></math> and <math id="S3.SS0.SSS0.Px2.p4.4.m4.1" class="ltx_Math" alttext="\tilde{x}\in\mathbb{R}^{d}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p4.4.m4.1a"><mrow id="S3.SS0.SSS0.Px2.p4.4.m4.1.1" xref="S3.SS0.SSS0.Px2.p4.4.m4.1.1.cmml"><mover accent="true" id="S3.SS0.SSS0.Px2.p4.4.m4.1.1.2" xref="S3.SS0.SSS0.Px2.p4.4.m4.1.1.2.cmml"><mi id="S3.SS0.SSS0.Px2.p4.4.m4.1.1.2.2" xref="S3.SS0.SSS0.Px2.p4.4.m4.1.1.2.2.cmml">x</mi><mo id="S3.SS0.SSS0.Px2.p4.4.m4.1.1.2.1" xref="S3.SS0.SSS0.Px2.p4.4.m4.1.1.2.1.cmml">~</mo></mover><mo id="S3.SS0.SSS0.Px2.p4.4.m4.1.1.1" xref="S3.SS0.SSS0.Px2.p4.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS0.SSS0.Px2.p4.4.m4.1.1.3" xref="S3.SS0.SSS0.Px2.p4.4.m4.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px2.p4.4.m4.1.1.3.2" xref="S3.SS0.SSS0.Px2.p4.4.m4.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS0.SSS0.Px2.p4.4.m4.1.1.3.3" xref="S3.SS0.SSS0.Px2.p4.4.m4.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p4.4.m4.1b"><apply id="S3.SS0.SSS0.Px2.p4.4.m4.1.1.cmml" xref="S3.SS0.SSS0.Px2.p4.4.m4.1.1"><in id="S3.SS0.SSS0.Px2.p4.4.m4.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p4.4.m4.1.1.1"></in><apply id="S3.SS0.SSS0.Px2.p4.4.m4.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p4.4.m4.1.1.2"><ci id="S3.SS0.SSS0.Px2.p4.4.m4.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px2.p4.4.m4.1.1.2.1">~</ci><ci id="S3.SS0.SSS0.Px2.p4.4.m4.1.1.2.2.cmml" xref="S3.SS0.SSS0.Px2.p4.4.m4.1.1.2.2">𝑥</ci></apply><apply id="S3.SS0.SSS0.Px2.p4.4.m4.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p4.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p4.4.m4.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p4.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p4.4.m4.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p4.4.m4.1.1.3.2">ℝ</ci><ci id="S3.SS0.SSS0.Px2.p4.4.m4.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p4.4.m4.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p4.4.m4.1c">\tilde{x}\in\mathbb{R}^{d}</annotation></semantics></math>
and fused by projecting them into the same space and adding them. The VQA task is formulated as classification, so a final projection into the answer dimension and a sigmoid function conclude the network.
<cite class="ltx_cite ltx_citemacro_citet">Jiang et al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite> improved the performance of the original MCAN by replacing the region image features with spatial grid features. We use their model as a baseline for our experiments.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px3" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Human-Like Attention Integration.</h3>

<div id="S3.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.p1.1" class="ltx_p">Although the importance of fusing both modalities has been underlined by many previous works and is the driving idea behind co-attention, the integration of external guidance has only been explored in the image domain  <cite class="ltx_cite ltx_citemacro_cite">Gan et al. (<a href="#bib.bib13" title="" class="ltx_ref">2017</a>); Qiao et al. (<a href="#bib.bib29" title="" class="ltx_ref">2018</a>); Selvaraju et al. (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>); Wu and Mooney (<a href="#bib.bib42" title="" class="ltx_ref">2019</a>); Chen et al. (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>); Shrestha et al. (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2109.13139/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="346" height="390" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Schematic of a self-attention (SA) layer. The vanilla SA layer is shown in gray, while our human attention integration approach in red.</figcaption>
</figure>
<div id="S3.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.p2.6" class="ltx_p">We integrate human-like attention into both the text and image streams of the MCAN base model. For both streams, we use the same basic principle of integration into SA modules (see <a href="#S3.F2" title="Figure 2 ‣ Human-Like Attention Integration. ‣ 3 Method ‣ Multimodal Integration of Human-Like Attention in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>).
We propose a new attention function <math id="S3.SS0.SSS0.Px3.p2.1.m1.1" class="ltx_Math" alttext="A_{H}" display="inline"><semantics id="S3.SS0.SSS0.Px3.p2.1.m1.1a"><msub id="S3.SS0.SSS0.Px3.p2.1.m1.1.1" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.cmml"><mi id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.2" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.2.cmml">A</mi><mi id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.cmml">H</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.1.m1.1b"><apply id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.2">𝐴</ci><ci id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.3">𝐻</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.1.m1.1c">A_{H}</annotation></semantics></math> for the SA layer, multiplying human-like attention weights <math id="S3.SS0.SSS0.Px3.p2.2.m2.1" class="ltx_Math" alttext="\alpha\in\mathbb{R}^{n}" display="inline"><semantics id="S3.SS0.SSS0.Px3.p2.2.m2.1a"><mrow id="S3.SS0.SSS0.Px3.p2.2.m2.1.1" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1.cmml"><mi id="S3.SS0.SSS0.Px3.p2.2.m2.1.1.2" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1.2.cmml">α</mi><mo id="S3.SS0.SSS0.Px3.p2.2.m2.1.1.1" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS0.SSS0.Px3.p2.2.m2.1.1.3" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px3.p2.2.m2.1.1.3.2" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS0.SSS0.Px3.p2.2.m2.1.1.3.3" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1.3.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.2.m2.1b"><apply id="S3.SS0.SSS0.Px3.p2.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1"><in id="S3.SS0.SSS0.Px3.p2.2.m2.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1.1"></in><ci id="S3.SS0.SSS0.Px3.p2.2.m2.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1.2">𝛼</ci><apply id="S3.SS0.SSS0.Px3.p2.2.m2.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p2.2.m2.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px3.p2.2.m2.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1.3.2">ℝ</ci><ci id="S3.SS0.SSS0.Px3.p2.2.m2.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.2.m2.1c">\alpha\in\mathbb{R}^{n}</annotation></semantics></math> (<math id="S3.SS0.SSS0.Px3.p2.3.m3.1" class="ltx_Math" alttext="\mathbb{R}^{m}" display="inline"><semantics id="S3.SS0.SSS0.Px3.p2.3.m3.1a"><msup id="S3.SS0.SSS0.Px3.p2.3.m3.1.1" xref="S3.SS0.SSS0.Px3.p2.3.m3.1.1.cmml"><mi id="S3.SS0.SSS0.Px3.p2.3.m3.1.1.2" xref="S3.SS0.SSS0.Px3.p2.3.m3.1.1.2.cmml">ℝ</mi><mi id="S3.SS0.SSS0.Px3.p2.3.m3.1.1.3" xref="S3.SS0.SSS0.Px3.p2.3.m3.1.1.3.cmml">m</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.3.m3.1b"><apply id="S3.SS0.SSS0.Px3.p2.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p2.3.m3.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.3.m3.1.1">superscript</csymbol><ci id="S3.SS0.SSS0.Px3.p2.3.m3.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p2.3.m3.1.1.2">ℝ</ci><ci id="S3.SS0.SSS0.Px3.p2.3.m3.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.p2.3.m3.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.3.m3.1c">\mathbb{R}^{m}</annotation></semantics></math> for image features) into the attention score.
For the query <math id="S3.SS0.SSS0.Px3.p2.4.m4.1" class="ltx_Math" alttext="q_{i}" display="inline"><semantics id="S3.SS0.SSS0.Px3.p2.4.m4.1a"><msub id="S3.SS0.SSS0.Px3.p2.4.m4.1.1" xref="S3.SS0.SSS0.Px3.p2.4.m4.1.1.cmml"><mi id="S3.SS0.SSS0.Px3.p2.4.m4.1.1.2" xref="S3.SS0.SSS0.Px3.p2.4.m4.1.1.2.cmml">q</mi><mi id="S3.SS0.SSS0.Px3.p2.4.m4.1.1.3" xref="S3.SS0.SSS0.Px3.p2.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.4.m4.1b"><apply id="S3.SS0.SSS0.Px3.p2.4.m4.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p2.4.m4.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.p2.4.m4.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p2.4.m4.1.1.2">𝑞</ci><ci id="S3.SS0.SSS0.Px3.p2.4.m4.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.p2.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.4.m4.1c">q_{i}</annotation></semantics></math> corresponding to the <math id="S3.SS0.SSS0.Px3.p2.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS0.SSS0.Px3.p2.5.m5.1a"><mi id="S3.SS0.SSS0.Px3.p2.5.m5.1.1" xref="S3.SS0.SSS0.Px3.p2.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.5.m5.1b"><ci id="S3.SS0.SSS0.Px3.p2.5.m5.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.5.m5.1c">i</annotation></semantics></math>-th input feature embedding, we calculate the <math id="S3.SS0.SSS0.Px3.p2.6.m6.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS0.SSS0.Px3.p2.6.m6.1a"><mi id="S3.SS0.SSS0.Px3.p2.6.m6.1.1" xref="S3.SS0.SSS0.Px3.p2.6.m6.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.6.m6.1b"><ci id="S3.SS0.SSS0.Px3.p2.6.m6.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.6.m6.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.6.m6.1c">i</annotation></semantics></math>-th attended embedding:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.5" class="ltx_Math" alttext="A_{H}(q,K,V,\alpha)=\textrm{softmax}(\frac{q_{i}K^{T}\cdot\alpha_{i}}{\sqrt{d}})V" display="block"><semantics id="S3.E2.m1.5a"><mrow id="S3.E2.m1.5.6" xref="S3.E2.m1.5.6.cmml"><mrow id="S3.E2.m1.5.6.2" xref="S3.E2.m1.5.6.2.cmml"><msub id="S3.E2.m1.5.6.2.2" xref="S3.E2.m1.5.6.2.2.cmml"><mi id="S3.E2.m1.5.6.2.2.2" xref="S3.E2.m1.5.6.2.2.2.cmml">A</mi><mi id="S3.E2.m1.5.6.2.2.3" xref="S3.E2.m1.5.6.2.2.3.cmml">H</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.6.2.1" xref="S3.E2.m1.5.6.2.1.cmml">​</mo><mrow id="S3.E2.m1.5.6.2.3.2" xref="S3.E2.m1.5.6.2.3.1.cmml"><mo stretchy="false" id="S3.E2.m1.5.6.2.3.2.1" xref="S3.E2.m1.5.6.2.3.1.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">q</mi><mo id="S3.E2.m1.5.6.2.3.2.2" xref="S3.E2.m1.5.6.2.3.1.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">K</mi><mo id="S3.E2.m1.5.6.2.3.2.3" xref="S3.E2.m1.5.6.2.3.1.cmml">,</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">V</mi><mo id="S3.E2.m1.5.6.2.3.2.4" xref="S3.E2.m1.5.6.2.3.1.cmml">,</mo><mi id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">α</mi><mo stretchy="false" id="S3.E2.m1.5.6.2.3.2.5" xref="S3.E2.m1.5.6.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.5.6.1" xref="S3.E2.m1.5.6.1.cmml">=</mo><mrow id="S3.E2.m1.5.6.3" xref="S3.E2.m1.5.6.3.cmml"><mtext id="S3.E2.m1.5.6.3.2" xref="S3.E2.m1.5.6.3.2a.cmml">softmax</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.6.3.1" xref="S3.E2.m1.5.6.3.1.cmml">​</mo><mrow id="S3.E2.m1.5.6.3.3.2" xref="S3.E2.m1.5.5.cmml"><mo stretchy="false" id="S3.E2.m1.5.6.3.3.2.1" xref="S3.E2.m1.5.5.cmml">(</mo><mfrac id="S3.E2.m1.5.5" xref="S3.E2.m1.5.5.cmml"><mrow id="S3.E2.m1.5.5.2" xref="S3.E2.m1.5.5.2.cmml"><mrow id="S3.E2.m1.5.5.2.2" xref="S3.E2.m1.5.5.2.2.cmml"><msub id="S3.E2.m1.5.5.2.2.2" xref="S3.E2.m1.5.5.2.2.2.cmml"><mi id="S3.E2.m1.5.5.2.2.2.2" xref="S3.E2.m1.5.5.2.2.2.2.cmml">q</mi><mi id="S3.E2.m1.5.5.2.2.2.3" xref="S3.E2.m1.5.5.2.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.2.2.1" xref="S3.E2.m1.5.5.2.2.1.cmml">​</mo><msup id="S3.E2.m1.5.5.2.2.3" xref="S3.E2.m1.5.5.2.2.3.cmml"><mi id="S3.E2.m1.5.5.2.2.3.2" xref="S3.E2.m1.5.5.2.2.3.2.cmml">K</mi><mi id="S3.E2.m1.5.5.2.2.3.3" xref="S3.E2.m1.5.5.2.2.3.3.cmml">T</mi></msup></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.5.5.2.1" xref="S3.E2.m1.5.5.2.1.cmml">⋅</mo><msub id="S3.E2.m1.5.5.2.3" xref="S3.E2.m1.5.5.2.3.cmml"><mi id="S3.E2.m1.5.5.2.3.2" xref="S3.E2.m1.5.5.2.3.2.cmml">α</mi><mi id="S3.E2.m1.5.5.2.3.3" xref="S3.E2.m1.5.5.2.3.3.cmml">i</mi></msub></mrow><msqrt id="S3.E2.m1.5.5.3" xref="S3.E2.m1.5.5.3.cmml"><mi id="S3.E2.m1.5.5.3.2" xref="S3.E2.m1.5.5.3.2.cmml">d</mi></msqrt></mfrac><mo stretchy="false" id="S3.E2.m1.5.6.3.3.2.2" xref="S3.E2.m1.5.5.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.6.3.1a" xref="S3.E2.m1.5.6.3.1.cmml">​</mo><mi id="S3.E2.m1.5.6.3.4" xref="S3.E2.m1.5.6.3.4.cmml">V</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.5b"><apply id="S3.E2.m1.5.6.cmml" xref="S3.E2.m1.5.6"><eq id="S3.E2.m1.5.6.1.cmml" xref="S3.E2.m1.5.6.1"></eq><apply id="S3.E2.m1.5.6.2.cmml" xref="S3.E2.m1.5.6.2"><times id="S3.E2.m1.5.6.2.1.cmml" xref="S3.E2.m1.5.6.2.1"></times><apply id="S3.E2.m1.5.6.2.2.cmml" xref="S3.E2.m1.5.6.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.6.2.2.1.cmml" xref="S3.E2.m1.5.6.2.2">subscript</csymbol><ci id="S3.E2.m1.5.6.2.2.2.cmml" xref="S3.E2.m1.5.6.2.2.2">𝐴</ci><ci id="S3.E2.m1.5.6.2.2.3.cmml" xref="S3.E2.m1.5.6.2.2.3">𝐻</ci></apply><vector id="S3.E2.m1.5.6.2.3.1.cmml" xref="S3.E2.m1.5.6.2.3.2"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑞</ci><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝐾</ci><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝑉</ci><ci id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">𝛼</ci></vector></apply><apply id="S3.E2.m1.5.6.3.cmml" xref="S3.E2.m1.5.6.3"><times id="S3.E2.m1.5.6.3.1.cmml" xref="S3.E2.m1.5.6.3.1"></times><ci id="S3.E2.m1.5.6.3.2a.cmml" xref="S3.E2.m1.5.6.3.2"><mtext id="S3.E2.m1.5.6.3.2.cmml" xref="S3.E2.m1.5.6.3.2">softmax</mtext></ci><apply id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.6.3.3.2"><divide id="S3.E2.m1.5.5.1.cmml" xref="S3.E2.m1.5.6.3.3.2"></divide><apply id="S3.E2.m1.5.5.2.cmml" xref="S3.E2.m1.5.5.2"><ci id="S3.E2.m1.5.5.2.1.cmml" xref="S3.E2.m1.5.5.2.1">⋅</ci><apply id="S3.E2.m1.5.5.2.2.cmml" xref="S3.E2.m1.5.5.2.2"><times id="S3.E2.m1.5.5.2.2.1.cmml" xref="S3.E2.m1.5.5.2.2.1"></times><apply id="S3.E2.m1.5.5.2.2.2.cmml" xref="S3.E2.m1.5.5.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.2.2.2.1.cmml" xref="S3.E2.m1.5.5.2.2.2">subscript</csymbol><ci id="S3.E2.m1.5.5.2.2.2.2.cmml" xref="S3.E2.m1.5.5.2.2.2.2">𝑞</ci><ci id="S3.E2.m1.5.5.2.2.2.3.cmml" xref="S3.E2.m1.5.5.2.2.2.3">𝑖</ci></apply><apply id="S3.E2.m1.5.5.2.2.3.cmml" xref="S3.E2.m1.5.5.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.2.2.3.1.cmml" xref="S3.E2.m1.5.5.2.2.3">superscript</csymbol><ci id="S3.E2.m1.5.5.2.2.3.2.cmml" xref="S3.E2.m1.5.5.2.2.3.2">𝐾</ci><ci id="S3.E2.m1.5.5.2.2.3.3.cmml" xref="S3.E2.m1.5.5.2.2.3.3">𝑇</ci></apply></apply><apply id="S3.E2.m1.5.5.2.3.cmml" xref="S3.E2.m1.5.5.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.2.3.1.cmml" xref="S3.E2.m1.5.5.2.3">subscript</csymbol><ci id="S3.E2.m1.5.5.2.3.2.cmml" xref="S3.E2.m1.5.5.2.3.2">𝛼</ci><ci id="S3.E2.m1.5.5.2.3.3.cmml" xref="S3.E2.m1.5.5.2.3.3">𝑖</ci></apply></apply><apply id="S3.E2.m1.5.5.3.cmml" xref="S3.E2.m1.5.5.3"><root id="S3.E2.m1.5.5.3a.cmml" xref="S3.E2.m1.5.5.3"></root><ci id="S3.E2.m1.5.5.3.2.cmml" xref="S3.E2.m1.5.5.3.2">𝑑</ci></apply></apply><ci id="S3.E2.m1.5.6.3.4.cmml" xref="S3.E2.m1.5.6.3.4">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.5c">A_{H}(q,K,V,\alpha)=\textrm{softmax}(\frac{q_{i}K^{T}\cdot\alpha_{i}}{\sqrt{d}})V</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS0.SSS0.Px3.p3" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.p3.1" class="ltx_p">We integrate human-like attention on the question text into the first SA module in the encoder part of MCAN (see <a href="#S3.F1" title="Figure 1 ‣ 3 Method ‣ Multimodal Integration of Human-Like Attention in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>) and on the image after the first GA module that integrates text and image.
This early integration is motivated by <cite class="ltx_cite ltx_citemacro_citet">Brunner et al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> who investigated the token mixing that occurs in self-attention layers.
They found that the contribution of the original input token to the embedding at the same position quickly decreases after the first layer, making the integration of re-weighting attention weights less targeted at later layers.
We opted to integrate human-like image attention in the SA module <em id="S3.SS0.SSS0.Px3.p3.1.1" class="ltx_emph ltx_font_italic">after</em> the first GA module (as opposed to before) because this allows text-attention dependent features to interact during integration of human-like attention on the image. To obtain human-like attention scores for questions and images, we make use of domain-specific attention networks that we discuss in the following.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px4" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Text Attention Model.</h3>

<div id="S3.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px4.p1.1" class="ltx_p">For text, we make use of the recently introduced Text Saliency Model (TSM) <cite class="ltx_cite ltx_citemacro_cite">Sood et al. (<a href="#bib.bib38" title="" class="ltx_ref">2020b</a>)</cite> that yields an attention weight for every token in the question.
TSM is pre-trained on synthetic data obtained from a cognitive reading model as well as on real human gaze data.
<cite class="ltx_cite ltx_citemacro_citet">Sood et al. (<a href="#bib.bib38" title="" class="ltx_ref">2020b</a>)</cite> proposed a joint training approach in which TSM predictions are integrated into the Luong attention layer <cite class="ltx_cite ltx_citemacro_cite">Luong et al. (<a href="#bib.bib24" title="" class="ltx_ref">2015</a>)</cite> of a downstream NLP task and fine-tuned while training for this downstream task.
We follow a similar methodology and fine-tune the TSM while training our VQA network.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px5" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Image Attention Model.</h3>

<div id="S3.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px5.p1.1" class="ltx_p">For images, we obtain human-like attention using the state-of-the-art Multi-Duration Saliency (MDS) method <cite class="ltx_cite ltx_citemacro_cite">Fosco et al. (<a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>.
MDS predicts human attention allocation for viewing durations of 0.5, 3, and 5 seconds.
Because our integration approach requires a single attention map per image, we use the output of MDS for the 3 second viewing duration as suggested by the original authors.
The input images are of different aspect ratios which leads to black borders in the obtained fixed-size MDS maps after re-scaling.
However, the grid features are extracted from images with their original aspect-ratios.
Therefore, to obtain a single attention weight per feature, we remove the borders from the MDS maps, overlay the grid features and sum the pixel values in every grid cell.
The values are then normalized over the total sum to produce a distribution.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_right ltx_border_tt">Model</td>
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_right ltx_border_tt"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_italic">test-std</span></td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_right ltx_border_tt"><span id="S3.T1.1.1.1.3.1" class="ltx_text ltx_font_italic">test-dev</span></td>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.2.2.1" class="ltx_td ltx_align_right ltx_border_t">MULAN (multimodal)</td>
<td id="S3.T1.1.2.2.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.2.2.2.1" class="ltx_text ltx_font_bold">73.98%</span></td>
<td id="S3.T1.1.2.2.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.2.2.3.1" class="ltx_text ltx_font_bold">73.72%</span></td>
</tr>
<tr id="S3.T1.1.3.3" class="ltx_tr">
<td id="S3.T1.1.3.3.1" class="ltx_td ltx_align_right">Text only (TSM)</td>
<td id="S3.T1.1.3.3.2" class="ltx_td ltx_align_right">73.77%</td>
<td id="S3.T1.1.3.3.3" class="ltx_td ltx_align_right">73.52%</td>
</tr>
<tr id="S3.T1.1.4.4" class="ltx_tr">
<td id="S3.T1.1.4.4.1" class="ltx_td ltx_align_right">Image only (MDS)</td>
<td id="S3.T1.1.4.4.2" class="ltx_td ltx_align_right">73.67%</td>
<td id="S3.T1.1.4.4.3" class="ltx_td ltx_align_right">73.39%</td>
</tr>
<tr id="S3.T1.1.5.5" class="ltx_tr">
<td id="S3.T1.1.5.5.1" class="ltx_td ltx_align_right">No Integration</td>
<td id="S3.T1.1.5.5.2" class="ltx_td ltx_align_right">73.65%</td>
<td id="S3.T1.1.5.5.3" class="ltx_td ltx_align_right">73.39%</td>
</tr>
<tr id="S3.T1.1.6.6" class="ltx_tr">
<td id="S3.T1.1.6.6.1" class="ltx_td ltx_align_right ltx_border_t">Li et al. (2020)</td>
<td id="S3.T1.1.6.6.2" class="ltx_td ltx_align_right ltx_border_t">73.82%</td>
<td id="S3.T1.1.6.6.3" class="ltx_td ltx_align_right ltx_border_t">73.61%</td>
</tr>
<tr id="S3.T1.1.7.7" class="ltx_tr">
<td id="S3.T1.1.7.7.1" class="ltx_td ltx_align_right ltx_border_bb">Jiang et al. (2020)</td>
<td id="S3.T1.1.7.7.2" class="ltx_td ltx_align_right ltx_border_bb">72.71%</td>
<td id="S3.T1.1.7.7.3" class="ltx_td ltx_align_right ltx_border_bb">72.59%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results showing <span id="S3.T1.5.1" class="ltx_text ltx_font_italic">test-std</span> and <span id="S3.T1.6.2" class="ltx_text ltx_font_italic">test-dev</span> accuracy scores of our model (trained on <span id="S3.T1.7.3" class="ltx_text ltx_font_italic">train+val+vg</span>) and ablated versions over different datasets. MULAN achieves state-of-the-art on both benchmarks.</figcaption>
</figure>
</section>
<section id="S3.SS0.SSS0.Px6" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Implementation Details.</h3>

<div id="S3.SS0.SSS0.Px6.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px6.p1.3" class="ltx_p">We trained the network using the basic configuration and hyperparameters of MCAN_small. The input features were set to <math id="S3.SS0.SSS0.Px6.p1.1.m1.1" class="ltx_Math" alttext="d_{y}=512" display="inline"><semantics id="S3.SS0.SSS0.Px6.p1.1.m1.1a"><mrow id="S3.SS0.SSS0.Px6.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.cmml"><msub id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.2" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.2.cmml"><mi id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.2.2" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.2.2.cmml">d</mi><mi id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.2.3" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.2.3.cmml">y</mi></msub><mo id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.1" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px6.p1.1.m1.1b"><apply id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1"><eq id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.1"></eq><apply id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.2.2.cmml" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.2.2">𝑑</ci><ci id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.2.3.cmml" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.2.3">𝑦</ci></apply><cn type="integer" id="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px6.p1.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px6.p1.1.m1.1c">d_{y}=512</annotation></semantics></math> and after the change to grid features, <math id="S3.SS0.SSS0.Px6.p1.2.m2.1" class="ltx_Math" alttext="d_{x}=2048" display="inline"><semantics id="S3.SS0.SSS0.Px6.p1.2.m2.1a"><mrow id="S3.SS0.SSS0.Px6.p1.2.m2.1.1" xref="S3.SS0.SSS0.Px6.p1.2.m2.1.1.cmml"><msub id="S3.SS0.SSS0.Px6.p1.2.m2.1.1.2" xref="S3.SS0.SSS0.Px6.p1.2.m2.1.1.2.cmml"><mi id="S3.SS0.SSS0.Px6.p1.2.m2.1.1.2.2" xref="S3.SS0.SSS0.Px6.p1.2.m2.1.1.2.2.cmml">d</mi><mi id="S3.SS0.SSS0.Px6.p1.2.m2.1.1.2.3" xref="S3.SS0.SSS0.Px6.p1.2.m2.1.1.2.3.cmml">x</mi></msub><mo id="S3.SS0.SSS0.Px6.p1.2.m2.1.1.1" xref="S3.SS0.SSS0.Px6.p1.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS0.SSS0.Px6.p1.2.m2.1.1.3" xref="S3.SS0.SSS0.Px6.p1.2.m2.1.1.3.cmml">2048</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px6.p1.2.m2.1b"><apply id="S3.SS0.SSS0.Px6.p1.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px6.p1.2.m2.1.1"><eq id="S3.SS0.SSS0.Px6.p1.2.m2.1.1.1.cmml" xref="S3.SS0.SSS0.Px6.p1.2.m2.1.1.1"></eq><apply id="S3.SS0.SSS0.Px6.p1.2.m2.1.1.2.cmml" xref="S3.SS0.SSS0.Px6.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px6.p1.2.m2.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px6.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px6.p1.2.m2.1.1.2.2.cmml" xref="S3.SS0.SSS0.Px6.p1.2.m2.1.1.2.2">𝑑</ci><ci id="S3.SS0.SSS0.Px6.p1.2.m2.1.1.2.3.cmml" xref="S3.SS0.SSS0.Px6.p1.2.m2.1.1.2.3">𝑥</ci></apply><cn type="integer" id="S3.SS0.SSS0.Px6.p1.2.m2.1.1.3.cmml" xref="S3.SS0.SSS0.Px6.p1.2.m2.1.1.3">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px6.p1.2.m2.1c">d_{x}=2048</annotation></semantics></math>. The hidden dimension <math id="S3.SS0.SSS0.Px6.p1.3.m3.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS0.SSS0.Px6.p1.3.m3.1a"><mi id="S3.SS0.SSS0.Px6.p1.3.m3.1.1" xref="S3.SS0.SSS0.Px6.p1.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px6.p1.3.m3.1b"><ci id="S3.SS0.SSS0.Px6.p1.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px6.p1.3.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px6.p1.3.m3.1c">d</annotation></semantics></math> inside the Transformer heads was kept at 512. The increased dimensions in the MCAN_large configuration did not bring performance advantages in our preliminary experiments, so we opted for fewer parameters.
In the added TSM model we set the hidden dimension for both BiLSTM and Transformer heads to 128. We used 4 heads and one layer.
Even including the trainable parameters added by the TSM model, our full MULAN model has significantly fewer parameters than MCAN_large (MULAN: 58M, MCAN_large: 203M).
We kept the MCAN Adam Solver and the corresponding learning rate schedule and trained over 12 epochs with batch size 64. The pre-trained TSM model is trained jointly with the MCAN.
Results on <span id="S3.SS0.SSS0.Px6.p1.3.1" class="ltx_text ltx_font_italic">test-std</span> were obtained after training on <span id="S3.SS0.SSS0.Px6.p1.3.2" class="ltx_text ltx_font_italic">train</span> and <span id="S3.SS0.SSS0.Px6.p1.3.3" class="ltx_text ltx_font_italic">val</span> splits and a subset of Visual Genome <span id="S3.SS0.SSS0.Px6.p1.3.4" class="ltx_text ltx_font_italic">vg</span>, for results on <span id="S3.SS0.SSS0.Px6.p1.3.5" class="ltx_text ltx_font_italic">val</span> we trained on <span id="S3.SS0.SSS0.Px6.p1.3.6" class="ltx_text ltx_font_italic">train</span>.
We trained on single Nvidia Tesla V100-SXM2 GPUs with 32GB RAM. Average runtime was 2.6h for models trained on <span id="S3.SS0.SSS0.Px6.p1.3.7" class="ltx_text ltx_font_italic">train</span> (36h for convergence) and 5.3h for models trained on <span id="S3.SS0.SSS0.Px6.p1.3.8" class="ltx_text ltx_font_italic">train+val+vg</span> (68h for convergence).</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We used VQAv2<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://visualqa.org/download.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://visualqa.org/download.html</a></span></span></span>, the balanced version <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib14" title="" class="ltx_ref">2017</a>)</cite> of the VQA dataset <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib6" title="" class="ltx_ref">2015</a>)</cite>, for all our experiments.
VQAv2 is among the most popular benchmark datasets in the field and contains 1.1M human-annotated questions on 200K images from MS COCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib22" title="" class="ltx_ref">2014</a>)</cite>, split into <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">train</span>, <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">val</span>, and <span id="S4.p1.1.3" class="ltx_text ltx_font_italic">test-dev</span>/<span id="S4.p1.1.4" class="ltx_text ltx_font_italic">test-std</span> sets.
The annotations for the test splits have been held back for online evaluation of models submitted to the annual challenge.
The standard evaluation metric<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://visualqa.org/evaluation.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://visualqa.org/evaluation.html</a></span></span></span> is simple overall accuracy, for which agreement with three out of the 10 available annotator answers is considered as achieving an accuracy of 100%. In practice the machine accuracy is calculated as mean over all 9 out of 10 subsets.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The standard evaluation is misleading because the same overall accuracy can be achieved by answering very different sets of questions.
Hence, we evaluated accuracy overall for comparison with other work, but also binned “per question-type” as proposed by <cite class="ltx_cite ltx_citemacro_citet">Kafle and Kanan (<a href="#bib.bib18" title="" class="ltx_ref">2017</a>)</cite> to compensate for class imbalance and answer bias skewing the evaluation.
For this, we used their question types that categorize questions by the task they solve and added a <span id="S4.p2.1.1" class="ltx_text ltx_font_italic">reading</span> category for questions that are answered by text on the image. Because they label only about 8% of VQAv2 <span id="S4.p2.1.2" class="ltx_text ltx_font_italic">val</span>, we annotated the full <span id="S4.p2.1.3" class="ltx_text ltx_font_italic">val</span> set using a BiLSTM network pre-trained on TDIUC (1.6M samples) and hand-crafted regular expressions (245K samples).
These additional annotations allowed us to assess the performance changes of our model in more detail.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">We performed a series of experiments to evaluate the performance of our proposed method.
To shed more light on the importance of multimodal integration, we first compared different ablated versions of our method on <span id="S4.p3.1.1" class="ltx_text ltx_font_italic">test-dev</span> and <span id="S4.p3.1.2" class="ltx_text ltx_font_italic">test-std</span>.
Specifically, we compared multimodal with text-only, image-only, and integration of human-like attention.
Afterwards, we evaluated performance of the multimodal method when integrating human-like attention at different layers of the Transformer network.
Finally, we evaluated more fine-grained performance values of the multimodal, unimodal, and no attention integration method for the different question types.
For all of these experiments, we report accuracy on <span id="S4.p3.1.3" class="ltx_text ltx_font_italic">test-std</span> with training on the union of <span id="S4.p3.1.4" class="ltx_text ltx_font_italic">train</span>, <span id="S4.p3.1.5" class="ltx_text ltx_font_italic">val</span> and <span id="S4.p3.1.6" class="ltx_text ltx_font_italic">vg</span> sets.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2109.13139/assets/figures/seq_len_acc.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="411" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Performance improvements of our multimodal integration method (MULAN) relative to the baseline (<span id="S4.F3.2.1" class="ltx_text ltx_font_italic">No Integration</span>), depending on the question length. Results show a significant increase in accuracy for longer questions, in particular when questions have seven or more tokens.
</figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>

<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<td id="S5.T2.1.1.1.1" class="ltx_td ltx_align_right ltx_border_tt">TSM</td>
<td id="S5.T2.1.1.1.2" class="ltx_td ltx_align_right ltx_border_tt">MDS</td>
<td id="S5.T2.1.1.1.3" class="ltx_td ltx_align_right ltx_border_tt"><span id="S5.T2.1.1.1.3.1" class="ltx_text ltx_font_italic">test-std</span></td>
</tr>
<tr id="S5.T2.1.2.2" class="ltx_tr">
<td id="S5.T2.1.2.2.1" class="ltx_td ltx_align_right ltx_border_t">1</td>
<td id="S5.T2.1.2.2.2" class="ltx_td ltx_align_right ltx_border_t">2</td>
<td id="S5.T2.1.2.2.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T2.1.2.2.3.1" class="ltx_text ltx_font_bold">(ours) 73.98%</span></td>
</tr>
<tr id="S5.T2.1.3.3" class="ltx_tr">
<td id="S5.T2.1.3.3.1" class="ltx_td ltx_align_right">2</td>
<td id="S5.T2.1.3.3.2" class="ltx_td ltx_align_right">2</td>
<td id="S5.T2.1.3.3.3" class="ltx_td ltx_align_right">73.64%</td>
</tr>
<tr id="S5.T2.1.4.4" class="ltx_tr">
<td id="S5.T2.1.4.4.1" class="ltx_td ltx_align_right">1, 3, 5</td>
<td id="S5.T2.1.4.4.2" class="ltx_td ltx_align_right">2</td>
<td id="S5.T2.1.4.4.3" class="ltx_td ltx_align_right">73.73%</td>
</tr>
<tr id="S5.T2.1.5.5" class="ltx_tr">
<td id="S5.T2.1.5.5.1" class="ltx_td ltx_align_right">1</td>
<td id="S5.T2.1.5.5.2" class="ltx_td ltx_align_right">1–6</td>
<td id="S5.T2.1.5.5.3" class="ltx_td ltx_align_right">71.55%</td>
</tr>
<tr id="S5.T2.1.6.6" class="ltx_tr">
<td id="S5.T2.1.6.6.1" class="ltx_td ltx_align_right">1–3</td>
<td id="S5.T2.1.6.6.2" class="ltx_td ltx_align_right">2</td>
<td id="S5.T2.1.6.6.3" class="ltx_td ltx_align_right">73.49%</td>
</tr>
<tr id="S5.T2.1.7.7" class="ltx_tr">
<td id="S5.T2.1.7.7.1" class="ltx_td ltx_align_right">1–6</td>
<td id="S5.T2.1.7.7.2" class="ltx_td ltx_align_right">2</td>
<td id="S5.T2.1.7.7.3" class="ltx_td ltx_align_right">73.73%</td>
</tr>
<tr id="S5.T2.1.8.8" class="ltx_tr">
<td id="S5.T2.1.8.8.1" class="ltx_td ltx_align_right ltx_border_bb">1–6</td>
<td id="S5.T2.1.8.8.2" class="ltx_td ltx_align_right ltx_border_bb">2–6</td>
<td id="S5.T2.1.8.8.3" class="ltx_td ltx_align_right ltx_border_bb">73.50%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Layer-wise integration ablation study results, on <span id="S5.T2.3.1" class="ltx_text ltx_font_italic">test-std</span>. We integrate human-like attention at different layer combinations. TSM question attention weights are integrated into encoder SA modules, MDS image attention weights into decoder SA modules.
</figcaption>
</figure>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.5.1.1" class="ltx_tr">
<th id="S5.T3.5.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt">Question type</th>
<th id="S5.T3.5.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Bin Size</th>
<th id="S5.T3.5.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">No Integration</th>
<th id="S5.T3.5.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">text-only</th>
<th id="S5.T3.5.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">image-only</th>
<th id="S5.T3.5.1.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.5.1.1.6.1" class="ltx_text ltx_font_bold">MULAN</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.5.2.1" class="ltx_tr">
<th id="S5.T3.5.2.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">reading</th>
<th id="S5.T3.5.2.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t">31 K</th>
<td id="S5.T3.5.2.1.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T3.5.2.1.3.1" class="ltx_text ltx_font_bold">42.46</span></td>
<td id="S5.T3.5.2.1.4" class="ltx_td ltx_align_right ltx_border_t">42.28</td>
<td id="S5.T3.5.2.1.5" class="ltx_td ltx_align_right ltx_border_t">42.40</td>
<td id="S5.T3.5.2.1.6" class="ltx_td ltx_align_right ltx_border_t">42.30</td>
</tr>
<tr id="S5.T3.5.3.2" class="ltx_tr">
<th id="S5.T3.5.3.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">activity recognition</th>
<th id="S5.T3.5.3.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">15 K</th>
<td id="S5.T3.5.3.2.3" class="ltx_td ltx_align_right">74.55</td>
<td id="S5.T3.5.3.2.4" class="ltx_td ltx_align_right">74.72</td>
<td id="S5.T3.5.3.2.5" class="ltx_td ltx_align_right">74.59</td>
<td id="S5.T3.5.3.2.6" class="ltx_td ltx_align_right"><span id="S5.T3.5.3.2.6.1" class="ltx_text ltx_font_bold">75.01</span></td>
</tr>
<tr id="S5.T3.5.4.3" class="ltx_tr">
<th id="S5.T3.5.4.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">positional reasoning</th>
<th id="S5.T3.5.4.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">26 K</th>
<td id="S5.T3.5.4.3.3" class="ltx_td ltx_align_right">61.74</td>
<td id="S5.T3.5.4.3.4" class="ltx_td ltx_align_right">61.97</td>
<td id="S5.T3.5.4.3.5" class="ltx_td ltx_align_right">61.85</td>
<td id="S5.T3.5.4.3.6" class="ltx_td ltx_align_right"><span id="S5.T3.5.4.3.6.1" class="ltx_text ltx_font_bold">62.01</span></td>
</tr>
<tr id="S5.T3.5.5.4" class="ltx_tr">
<th id="S5.T3.5.5.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">object recognition</th>
<th id="S5.T3.5.5.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">28 K</th>
<td id="S5.T3.5.5.4.3" class="ltx_td ltx_align_right">82.59</td>
<td id="S5.T3.5.5.4.4" class="ltx_td ltx_align_right">82.50</td>
<td id="S5.T3.5.5.4.5" class="ltx_td ltx_align_right">82.49</td>
<td id="S5.T3.5.5.4.6" class="ltx_td ltx_align_right"><span id="S5.T3.5.5.4.6.1" class="ltx_text ltx_font_bold">82.68</span></td>
</tr>
<tr id="S5.T3.5.6.5" class="ltx_tr">
<th id="S5.T3.5.6.5.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">counting</th>
<th id="S5.T3.5.6.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">24 K</th>
<td id="S5.T3.5.6.5.3" class="ltx_td ltx_align_right">59.77</td>
<td id="S5.T3.5.6.5.4" class="ltx_td ltx_align_right">59.70</td>
<td id="S5.T3.5.6.5.5" class="ltx_td ltx_align_right">59.44</td>
<td id="S5.T3.5.6.5.6" class="ltx_td ltx_align_right"><span id="S5.T3.5.6.5.6.1" class="ltx_text ltx_font_bold">59.82</span></td>
</tr>
<tr id="S5.T3.5.7.6" class="ltx_tr">
<th id="S5.T3.5.7.6.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">object presence</th>
<th id="S5.T3.5.7.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">17 K</th>
<td id="S5.T3.5.7.6.3" class="ltx_td ltx_align_right">86.45</td>
<td id="S5.T3.5.7.6.4" class="ltx_td ltx_align_right">86.47</td>
<td id="S5.T3.5.7.6.5" class="ltx_td ltx_align_right"><span id="S5.T3.5.7.6.5.1" class="ltx_text ltx_font_bold">86.59</span></td>
<td id="S5.T3.5.7.6.6" class="ltx_td ltx_align_right">86.57</td>
</tr>
<tr id="S5.T3.5.8.7" class="ltx_tr">
<th id="S5.T3.5.8.7.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">scene recognition</th>
<th id="S5.T3.5.8.7.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">15 K</th>
<td id="S5.T3.5.8.7.3" class="ltx_td ltx_align_right">79.19</td>
<td id="S5.T3.5.8.7.4" class="ltx_td ltx_align_right">79.10</td>
<td id="S5.T3.5.8.7.5" class="ltx_td ltx_align_right"><span id="S5.T3.5.8.7.5.1" class="ltx_text ltx_font_bold">79.20</span></td>
<td id="S5.T3.5.8.7.6" class="ltx_td ltx_align_right">79.19</td>
</tr>
<tr id="S5.T3.5.9.8" class="ltx_tr">
<th id="S5.T3.5.9.8.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">sentiment understanding</th>
<th id="S5.T3.5.9.8.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">14 K</th>
<td id="S5.T3.5.9.8.3" class="ltx_td ltx_align_right">83.59</td>
<td id="S5.T3.5.9.8.4" class="ltx_td ltx_align_right">83.77</td>
<td id="S5.T3.5.9.8.5" class="ltx_td ltx_align_right">83.53</td>
<td id="S5.T3.5.9.8.6" class="ltx_td ltx_align_right"><span id="S5.T3.5.9.8.6.1" class="ltx_text ltx_font_bold">83.92</span></td>
</tr>
<tr id="S5.T3.5.10.9" class="ltx_tr">
<th id="S5.T3.5.10.9.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">color</th>
<th id="S5.T3.5.10.9.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">25 K</th>
<td id="S5.T3.5.10.9.3" class="ltx_td ltx_align_right"><span id="S5.T3.5.10.9.3.1" class="ltx_text ltx_font_bold">80.56</span></td>
<td id="S5.T3.5.10.9.4" class="ltx_td ltx_align_right">80.52</td>
<td id="S5.T3.5.10.9.5" class="ltx_td ltx_align_right">80.31</td>
<td id="S5.T3.5.10.9.6" class="ltx_td ltx_align_right"><span id="S5.T3.5.10.9.6.1" class="ltx_text ltx_font_bold">80.56</span></td>
</tr>
<tr id="S5.T3.5.11.10" class="ltx_tr">
<th id="S5.T3.5.11.10.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">attribute</th>
<th id="S5.T3.5.11.10.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">4 K</th>
<td id="S5.T3.5.11.10.3" class="ltx_td ltx_align_right">69.36</td>
<td id="S5.T3.5.11.10.4" class="ltx_td ltx_align_right">69.09</td>
<td id="S5.T3.5.11.10.5" class="ltx_td ltx_align_right">69.37</td>
<td id="S5.T3.5.11.10.6" class="ltx_td ltx_align_right"><span id="S5.T3.5.11.10.6.1" class="ltx_text ltx_font_bold">69.65</span></td>
</tr>
<tr id="S5.T3.5.12.11" class="ltx_tr">
<th id="S5.T3.5.12.11.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">utility affordance</th>
<th id="S5.T3.5.12.11.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">11 K</th>
<td id="S5.T3.5.12.11.3" class="ltx_td ltx_align_right">66.33</td>
<td id="S5.T3.5.12.11.4" class="ltx_td ltx_align_right">66.40</td>
<td id="S5.T3.5.12.11.5" class="ltx_td ltx_align_right"><span id="S5.T3.5.12.11.5.1" class="ltx_text ltx_font_bold">66.64</span></td>
<td id="S5.T3.5.12.11.6" class="ltx_td ltx_align_right">66.42</td>
</tr>
<tr id="S5.T3.5.13.12" class="ltx_tr">
<th id="S5.T3.5.13.12.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">sport recognition</th>
<th id="S5.T3.5.13.12.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r">6 K</th>
<td id="S5.T3.5.13.12.3" class="ltx_td ltx_align_right">85.39</td>
<td id="S5.T3.5.13.12.4" class="ltx_td ltx_align_right">85.38</td>
<td id="S5.T3.5.13.12.5" class="ltx_td ltx_align_right"><span id="S5.T3.5.13.12.5.1" class="ltx_text ltx_font_bold">85.93</span></td>
<td id="S5.T3.5.13.12.6" class="ltx_td ltx_align_right">85.60</td>
</tr>
<tr id="S5.T3.5.14.13" class="ltx_tr">
<th id="S5.T3.5.14.13.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t">Overall VQAv2 <span id="S5.T3.5.14.13.1.1" class="ltx_text ltx_font_italic">val</span> Accuracy:</th>
<th id="S5.T3.5.14.13.2" class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t"></th>
<td id="S5.T3.5.14.13.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">70.06</td>
<td id="S5.T3.5.14.13.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">70.09</td>
<td id="S5.T3.5.14.13.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">70.03</td>
<td id="S5.T3.5.14.13.6" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S5.T3.5.14.13.6.1" class="ltx_text ltx_font_bold">70.28*</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance on VQAv2 <span id="S5.T3.12.1" class="ltx_text ltx_font_italic">val</span>
split in terms of per-question-type accuracy of the proposed multimodal integration method (MULAN) and the unimodal ablations <span id="S5.T3.13.2" class="ltx_text ltx_font_italic">text-only</span> or <span id="S5.T3.14.3" class="ltx_text ltx_font_italic">image-only</span> and no integration of human attention (<span id="S5.T3.15.4" class="ltx_text ltx_font_italic">No Integration</span>). Because the online evaluation of VQAv2 only returns overall accuracy, we cannot obtain fine-grained accuracy for <span id="S5.T3.16.5" class="ltx_text ltx_font_italic">test-std</span> or <span id="S5.T3.17.6" class="ltx_text ltx_font_italic">test-dev</span>. A star indicates statistically significant <math id="S5.T3.3.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S5.T3.3.m1.1b"><mi id="S5.T3.3.m1.1.1" xref="S5.T3.3.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.T3.3.m1.1c"><ci id="S5.T3.3.m1.1.1.cmml" xref="S5.T3.3.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.m1.1d">p</annotation></semantics></math> at <math id="S5.T3.4.m2.1" class="ltx_Math" alttext="p&lt;0.05" display="inline"><semantics id="S5.T3.4.m2.1b"><mrow id="S5.T3.4.m2.1.1" xref="S5.T3.4.m2.1.1.cmml"><mi id="S5.T3.4.m2.1.1.2" xref="S5.T3.4.m2.1.1.2.cmml">p</mi><mo id="S5.T3.4.m2.1.1.1" xref="S5.T3.4.m2.1.1.1.cmml">&lt;</mo><mn id="S5.T3.4.m2.1.1.3" xref="S5.T3.4.m2.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.4.m2.1c"><apply id="S5.T3.4.m2.1.1.cmml" xref="S5.T3.4.m2.1.1"><lt id="S5.T3.4.m2.1.1.1.cmml" xref="S5.T3.4.m2.1.1.1"></lt><ci id="S5.T3.4.m2.1.1.2.cmml" xref="S5.T3.4.m2.1.1.2">𝑝</ci><cn type="float" id="S5.T3.4.m2.1.1.3.cmml" xref="S5.T3.4.m2.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.m2.1d">p&lt;0.05</annotation></semantics></math>.</figcaption>
</figure>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2109.13139/assets/x3.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="159" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
Visualization of the weights in the attention reduction modules for text and image features. We compared MULAN and the baseline model (No Integration) at epochs 1, 8, and 13. The input question was “What is the child digging around in?”, the correct answer is “fridge”. Classification outputs are given for each considered epoch.
</figcaption>
</figure>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Overall Performance.</h3>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p"><a href="#S3.T1" title="Table 1 ‣ Image Attention Model. ‣ 3 Method ‣ Multimodal Integration of Human-Like Attention in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 1</span></a> shows results of our MULAN model along with current state-of-the-art approaches and ablations of our method.
Our method outperforms the current state of the art <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite>, reaching accuracy scores of 73.98% on <em id="S5.SS0.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">test-std</em> and 73.72% on <em id="S5.SS0.SSS0.Px1.p1.1.2" class="ltx_emph ltx_font_italic">test-dev</em>, as compared to 73.82% and 73.61%.
Our model also uses approximately 80% less trainable parameters than  <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite>.
Notably, we observe a systematic increase in performance as a result of human-like attention integration.
Our base model without any integration reaches 73.65% on <em id="S5.SS0.SSS0.Px1.p1.1.3" class="ltx_emph ltx_font_italic">test-std</em>.
While the integration of human-like attention on only images (73.67%) or text (73.77% on <em id="S5.SS0.SSS0.Px1.p1.1.4" class="ltx_emph ltx_font_italic">test-std</em>) can lead to an increase in performance, our full MULAN model employing multimodal integration is the best performing approach. This further underlines the importance of jointly integrating human-like attention on both text and images for the VQA task, which is inherently multimodal.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Layer-Wise Integration Experiments.</h3>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">We evaluated the integration of human-like attention on questions and images for different layers in the MCAN encoder-decoder architecture (see <a href="#S5.T2" title="Table 2 ‣ 5 Results and Discussion ‣ Multimodal Integration of Human-Like Attention in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 2</span></a>). We investigated the integration of TSM outputs into different SA layers of the encoder, and the integration of MDS outputs into different SA modules of the decoder.
Among all investigated combinations, the initial integration into the first layer of the encoder and the second layer of the decoder performed best (73.98% accuracy).
Integrating TSM predictions into the second encoder layer decreased the overall accuracy to 73.64%, which is in line with the reasoning discussed in <cite class="ltx_cite ltx_citemacro_citet">Brunner et al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite>, where with layer depth feature embeddings are increasingly mixed and therefore less attributable to the input word token at the same position. The TSM predicts attention weights for specific word tokens.
We further investigated the integration of TSM and MDS predictions at multiple layers in the MCAN architecture. However all options resulted in decreased performance in comparison to MULAN.
Our results indicate that early integration of human-like attention at a single point for both text and image is optimal for the VQA task.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px3" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Category-Specific Performance.</h3>

<div id="S5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p1.1" class="ltx_p">To obtain a deeper understanding of our improvements over baseline approaches, we categorized question types into 12 fine-grained bins, similarly to <cite class="ltx_cite ltx_citemacro_citet">Kafle and Kanan (<a href="#bib.bib18" title="" class="ltx_ref">2017</a>)</cite>.
<a href="#S5.T3" title="Table 3 ‣ 5 Results and Discussion ‣ Multimodal Integration of Human-Like Attention in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 3</span></a> shows a detailed breakdown of accuracy results by category type.
We used the validation set, rather than the test set, since we needed access to the ground truth annotations to calculate the per-category accuracy. For the same reason, we can only perform the paired t-test on the full validation set. As can be seen, all ablated models obtain inferior performance to our full model on the validation set (statistically significant at the 0.05 level).
For most categories, MULAN achieves the highest accuracy.
Moreover, in comparison to the baseline, our method is the best performing one in 10 out of 12 categories with especially clear improvements in activity recognition and sentiment analysis categories.
MULAN expectedly reduces accuracy on reading questions that other models can most likely only answer by bias exploitation, and improves on small bins like attribute.
The distances between the models are small in absolute terms, but given the vastly different bin sizes the relative improvements are large.
This underlines the robustness of improvements with human-like attention integration and, in particular, multimodal integration.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px4" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Sequence-Length Analysis.</h3>

<div id="S5.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px4.p1.1" class="ltx_p">Previous works have shown that VQA models often converge to the answer after processing only the first words of a question, a behavior that has been characterized as “jumping to conclusions” <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib1" title="" class="ltx_ref">2016</a>)</cite>.
Human-like attention integration might be useful to combat this effect as the TSM was previously shown to successfully predict human-like attention distributions across all salient words in the input sequence <cite class="ltx_cite ltx_citemacro_cite">Sood et al. (<a href="#bib.bib38" title="" class="ltx_ref">2020b</a>)</cite>.
As this effect might be especially pronounced for longer questions,
we investigated whether human-like attention integration in MULAN can especially improve on those questions <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib1" title="" class="ltx_ref">2016</a>)</cite>.
<a href="#S4.F3" title="Figure 3 ‣ 4 Experiments ‣ Multimodal Integration of Human-Like Attention in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a> shows the results of an evaluation where we analyzed the improvements of our system relative to the baseline model, depending on the question length. We find that while MULAN improves for all questions independent of their length, its advantage is especially significant for questions that contain seven tokens or more (relative improvements of 0.3% or more), indicating that MULAN can improve upon the above-described challenge.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px5" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Attention Visualizations.</h3>

<div id="S5.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px5.p1.1" class="ltx_p">To further investigate MULAN’s ability to answer longer questions than the baseline model, we visualized the attention weights in the attention reduction modules after the encoder/decoder, which merge the text and image features to one attended feature vector each.
These weights represent the final impact of the transformed features.
<a href="#S5.F4" title="Figure 4 ‣ 5 Results and Discussion ‣ Multimodal Integration of Human-Like Attention in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 4</span></a> shows examples from the validation set with the corresponding predictions comparing our method to the baseline at epochs 1, 8 and 13. The input question was “What is the child digging around in?” and the correct answer is “fridge”. Our method it able to correctly predict that the child is digging in the fridge as opposed to the baseline that outputs “nothing”.
MULAN focuses on both the token “digging” as well as the location, which is in front of the child.
In contrast, the attention of the baseline model is more spread out, failing to focus on the relevant cues. Interestingly, the focus of attention in the baseline evolved over several epochs of training, unlike MULAN which quickly converged to a stable attention distribution. This indicates that initial human-like attention maps on the image are indeed adapted using attention-based information extracted from the question text.
<a href="#S5.F5" title="Figure 5 ‣ Attention Visualizations. ‣ 5 Results and Discussion ‣ Multimodal Integration of Human-Like Attention in Visual Question Answering" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 5</span></a> shows three additional examples of our method compared to the baseline from the final epoch. The top and middle examples show how our method is able to correctly answer the question, while the baseline fails. The bottom example shows an error case of our method.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2109.13139/assets/x4.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="415" height="577" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
Visualization of attention distributions for MULAN and the baseline (No Integration). The upper examples show improvements of MULAN over the baseline, while the bottom shows a failure case.
</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we propose the first method for multimodal integration of human-like attention on both image and text for visual question answering.
Our Multimodal Human-like Attention Network (MULAN) method integrates state-of-the-art text and image saliency models into neural self-attention layers by modifying attention scoring functions of transformer-based self-attention modules.
Evaluations on the challenging VQAv2 dataset show that our method not only achieves state-of-the-art performance (73.98% on <span id="S6.p1.1.1" class="ltx_text ltx_font_italic">test-std</span> and 73.72% on <span id="S6.p1.1.2" class="ltx_text ltx_font_italic">test-dev</span>) but also does so with fewer trainable parameters than current models.
As such, our work provides further evidence for the potential of integrating human-like attention as a supervisory signal in neural attention mechanisms.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2016)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, and Devi Parikh. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D16-1203" title="" class="ltx_ref ltx_href">Analyzing the
Behavior of Visual Question Answering Models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing (EMNLP)</em>, pages 1955–1960.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2018)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Agrawal_Dont_Just_Assume_CVPR_2018_paper.html" title="" class="ltx_ref ltx_href">Don’t Just Assume; Look and Answer: Overcoming Priors for
Visual Question Answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2017)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Aniruddha Kembhavi, Dhruv Batra, and Devi Parikh. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1704.08243" title="" class="ltx_ref ltx_href">C-VQA: A Compositional
Split of the Visual Question Answering (VQA) v1.0 Dataset</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv:1704.08243</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al. (2018)</span>
<span class="ltx_bibblock">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2018.00636" title="" class="ltx_ref ltx_href">Bottom-Up and
Top-Down Attention for Image Captioning and Visual Question
Answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, volume 1, pages
6077–6086.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andreas et al. (2016)</span>
<span class="ltx_bibblock">
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_cvpr_2016/html/Andreas_Neural_Module_Networks_CVPR_2016_paper.html" title="" class="ltx_ref ltx_href">Neural Module Networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, pages 39–48.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV.2015.279" title="" class="ltx_ref ltx_href">VQA: Visual
Question Answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV)</em>, pages 2425–2433.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrett et al. (2018)</span>
<span class="ltx_bibblock">
Maria Barrett, Joachim Bingel, Nora Hollenstein, Marek Rei, and Anders
Søgaard. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/http://dx.doi.org/10.18653/v1/K18-1030" title="" class="ltx_ref ltx_href">Sequence Classification with Human Attention</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACL SIGNLL Conference on Computational
Natural Language Learning (CoNLL)</em>, pages 302–312.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brunner et al. (2020)</span>
<span class="ltx_bibblock">
Gino Brunner, Yang Liu, Damián Pascual, Oliver Richter, Massimiliano
Ciaramita, and Roger Wattenhofer. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=BJg1f6EFDB" title="" class="ltx_ref ltx_href">On
Identifiability in Transformers</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations
(ICLR)</em>, pages 1–35.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Shi Chen, Ming Jiang, Jinhui Yang, and Qi Zhao. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1007/978-3-030-58452-8_6" title="" class="ltx_ref ltx_href">AiR: Attention with Reasoning Capability</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision (ECCV)</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das et al. (2016)</span>
<span class="ltx_bibblock">
Abhishek Das, Harsh Agrawal, C. Lawrence Zitnick, Devi Parikh, and Dhruv Batra.
2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D16-1092" title="" class="ltx_ref ltx_href">Human Attention in
Visual Question Answering: Do Humans and Deep Networks Look
at the Same Regions?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing (EMNLP)</em>, pages 932–937.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2009)</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2009.5206848" title="" class="ltx_ref ltx_href">Imagenet: A
Large-Scale Hierarchical Image Database</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2009 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, pages 248–255.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fosco et al. (2020)</span>
<span class="ltx_bibblock">
Camilo Fosco, Anelise Newman, Pat Sukhum, Yun Bin Zhang, Nanxuan Zhao, Aude
Oliva, and Zoya Bylinskii. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR42600.2020.00453" title="" class="ltx_ref ltx_href">How Much
Time Do You Have? Modeling Multi-Duration Saliency</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, volume 1, pages
4473–4482.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan et al. (2017)</span>
<span class="ltx_bibblock">
Chuang Gan, Yandong Li, Haoxiang Li, Chen Sun, and Boqing Gong. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV.2017.201" title="" class="ltx_ref ltx_href">VQS: Linking
Segmentations to Questions and Answers for Supervised Attention in
VQA and Question-Focused Semantic Segmentation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV)</em>, pages 1811–1820.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2017.670" title="" class="ltx_ref ltx_href">Making the v in
VQA Matter: Elevating the Role of Image Understanding in Visual
Question Answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, pages 6325–6334.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2016.90" title="" class="ltx_ref ltx_href">Deep Residual
Learning for Image Recognition</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2020)</span>
<span class="ltx_bibblock">
Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-Miller, and Xinlei
Chen. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_In_Defense_of_Grid_Features_for_Visual_Question_Answering_CVPR_2020_paper.pdf" title="" class="ltx_ref ltx_href">In Defense of Grid Features for Visual Question Answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2018)</span>
<span class="ltx_bibblock">
Yu Jiang, Vivek Natarajan, Xinlei Chen, Marcus Rohrbach, Dhruv Batra, and Devi
Parikh. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1807.09956" title="" class="ltx_ref ltx_href">Pythia v0.1: the Winning
Entry to the VQA Challenge 2018</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kafle and Kanan (2017)</span>
<span class="ltx_bibblock">
Kushal Kafle and Christopher Kanan. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV.2017.217" title="" class="ltx_ref ltx_href">An Analysis of
Visual Question Answering Algorithms</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV)</em>, pages 1983–1991.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karessli et al. (2017)</span>
<span class="ltx_bibblock">
Nour Karessli, Zeynep Akata, Bernt Schiele, and Andreas Bulling. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Karessli_Gaze_Embeddings_for_CVPR_2017_paper.pdf" title="" class="ltx_ref ltx_href">Gaze Embeddings for Zero-Shot Image Classification</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, pages 4525–4534.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. (2016)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalanditis, Li-Jia Li, David A Shamma,
Michael Bernstein, and Li Fei-Fei. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/s11263-016-0981-7" title="" class="ltx_ref ltx_href">Visual Genome:
Connecting Language and Vision Using Crowdsourced Dense Image
Annotations</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020)</span>
<span class="ltx_bibblock">
Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan
Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-030-58577-8_8" title="" class="ltx_ref ltx_href">Oscar:
Object-Semantics Aligned Pre-training for Vision-Language
Tasks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision (ECCV)</em>, volume
12375, pages 121–137.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48" title="" class="ltx_ref ltx_href">Microsoft
COCO: Common Objects in Context</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision (ECCV)</em>, pages
740–755.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2016)</span>
<span class="ltx_bibblock">
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://papers.nips.cc/paper/2016/hash/9dcb88e0137649590b755372b040afad-Abstract.html" title="" class="ltx_ref ltx_href">Hierarchical Question-Image Co-Attention for Visual Question
Answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 29
(NeurIPS)</em>, pages 1–9.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luong et al. (2015)</span>
<span class="ltx_bibblock">
Minh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D15-1166" title="" class="ltx_ref ltx_href">Effective Approaches
to Attention-Based Neural Machine Translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing (EMNLP)</em>, pages 1412–1421.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski and Fritz (2014)</span>
<span class="ltx_bibblock">
Mateusz Malinowski and Mario Fritz. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2014/file/d516b13671a4179d9b7b458a6ebdeb92-Paper.pdf" title="" class="ltx_ref ltx_href">A Multi-World Approach to Question Answering about Real-World
Scenes based on Uncertain Input</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems
(NeurIPS)</em>, volume 27, pages 1682–1690.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski et al. (2015)</span>
<span class="ltx_bibblock">
Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV.2015.9" title="" class="ltx_ref ltx_href">Ask Your Neurons:
A Neural-Based Approach to Answering Questions about Images</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV)</em>, pages 1–9.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nam et al. (2017)</span>
<span class="ltx_bibblock">
Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2017.232" title="" class="ltx_ref ltx_href">Dual Attention
Networks for Multimodal Reasoning and Matching</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, volume 1, pages
2156–2164.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington et al. (2014)</span>
<span class="ltx_bibblock">
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://www.aclweb.org/anthology/D14-1162" title="" class="ltx_ref ltx_href">GloVe: Global
Vectors for Word Representation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP)</em>, pages 1532–1543.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiao et al. (2018)</span>
<span class="ltx_bibblock">
Tingting Qiao, Jianfeng Dong, and Duanqing Xu. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16485" title="" class="ltx_ref ltx_href">Exploring
Human-Like Attention Supervision in Visual Question Answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Thirty-Second AAAI Conference on Artificial Intelligence
(AAAI)</em>, pages 7300–7307.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2015)</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf" title="" class="ltx_ref ltx_href">Faster R-CNN: Towards Real-Time Object Detection with Region
Proposal Networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volume 28, pages 91–99.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Selvaraju et al. (2019)</span>
<span class="ltx_bibblock">
Ramprasaath R. Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh,
Larry Heck, Dhruv Batra, and Devi Parikh. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV.2019.00268" title="" class="ltx_ref ltx_href">Taking a HINT:
Leveraging Explanations to Make Vision and Language Models More
Grounded</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV)</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Selvaraju et al. (2020)</span>
<span class="ltx_bibblock">
Ramprasaath R. Selvaraju, Purva Tendulkar, Devi Parikh, Eric Horvitz,
Marco Tulio Ribeiro, Besmira Nushi, and Ece Kamar. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_CVPR_2020/html/Selvaraju_SQuINTing_at_VQA_Models_Introspecting_VQA_Models_With_Sub-Questions_CVPR_2020_paper.html" title="" class="ltx_ref ltx_href">SQuINTing at VQA Models: Introspecting VQA Models with
Sub-Questions</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, pages
10003–10011.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah et al. (2019)</span>
<span class="ltx_bibblock">
Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2019.00681" title="" class="ltx_ref ltx_href">Cycle-Consistency
for Robust Visual Question Answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, pages 6642–6651.
IEEE.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shih et al. (2016)</span>
<span class="ltx_bibblock">
Kevin J Shih, Saurabh Singh, and Derek Hoiem. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2016.499" title="" class="ltx_ref ltx_href">Where to Look:
Focus Regions for Visual Question Answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, pages 4613–4621.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shrestha et al. (2019)</span>
<span class="ltx_bibblock">
Robik Shrestha, Kushal Kafle, and Christopher Kanan. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1903.00366" title="" class="ltx_ref ltx_href">Answer Them All!
Toward Universal Visual Question Answering Models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv:1903.00366</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shrestha et al. (2020)</span>
<span class="ltx_bibblock">
Robik Shrestha, Kushal Kafle, and Christopher Kanan. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.727" title="" class="ltx_ref ltx_href">A Negative
Case Analysis of Visual Grounding Methods for VQA</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 8172–8181. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sood et al. (2020a)</span>
<span class="ltx_bibblock">
Ekta Sood, Simon Tannert, Diego Frassinelli, Andreas Bulling, and Ngoc Thang
Vu. 2020a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P17" title="" class="ltx_ref ltx_href">Interpreting Attention
Models with Human Visual Attention in Machine Reading
Comprehension</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACL SIGNLL Conference on Computational
Natural Language Learning (CoNLL)</em>, pages 12–25.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sood et al. (2020b)</span>
<span class="ltx_bibblock">
Ekta Sood, Simon Tannert, Philipp Müller, and Andreas Bulling.
2020b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/460191c72f67e90150a093b4585e7eb4-Abstract.html" title="" class="ltx_ref ltx_href">Improving Natural Language Processing Tasks with Human
Gaze-Guided Neural Attention</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems
(NeurIPS)</em>, pages 1–15.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sugano and Bulling (2016)</span>
<span class="ltx_bibblock">
Yusuke Sugano and Andreas Bulling. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1608.05203" title="" class="ltx_ref ltx_href">Seeing with Humans:
Gaze-Assisted Neural Image Captioning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv:1608.05203</em>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Bansal (2019)</span>
<span class="ltx_bibblock">
Hao Tan and Mohit Bansal. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1514" title="" class="ltx_ref ltx_href">LXMERT: Learning
Cross-Modality Encoder Representations from Transformers</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing (EMNLP)</em>, pages 5100–5111.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="" class="ltx_ref ltx_href">Attention is All you Need</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems
(NeurIPS)</em>, volume 30, pages 5998–6008.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Mooney (2019)</span>
<span class="ltx_bibblock">
Jialin Wu and Raymond J. Mooney. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://papers.nips.cc/paper/2019/file/33b879e7ab79f56af1e88359f9314a10-Paper.pdf" title="" class="ltx_ref ltx_href">Self-Critical Reasoning for Robust Visual Question Answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems
(NeurIPS)</em>, pages 1–11.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2017)</span>
<span class="ltx_bibblock">
Dongfei Yu, Jianlong Fu, Tao Mei, and Yong Rui. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2017.446" title="" class="ltx_ref ltx_href">Multi-Level
Attention Networks for Visual Question Answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, pages 4187–4195.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2019a)</span>
<span class="ltx_bibblock">
Zhou Yu, Yuhao Cui, Zhenwei Shao, Pengbing Gao, and Jun Yu. 2019a.

</span>
<span class="ltx_bibblock">OpenVQA.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/MILVLG/openvqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/MILVLG/openvqa</a>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2019b)</span>
<span class="ltx_bibblock">
Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. 2019b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Deep_Modular_Co-Attention_Networks_for_Visual_Question_Answering_CVPR_2019_paper.html" title="" class="ltx_ref ltx_href">Deep Modular Co-Attention Networks for Visual Question
Answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, pages 6281–6290.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2016)</span>
<span class="ltx_bibblock">
Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Yin_and_Yang_CVPR_2016_paper.html" title="" class="ltx_ref ltx_href">Yin and Yang: Balancing and Answering Binary Visual Questions</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, pages 5014–5022.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2109.13138" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2109.13139" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2109.13139">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2109.13139" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2109.13140" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 03:27:21 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
